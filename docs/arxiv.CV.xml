<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>

<item>
<title>RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects</title>
<link>https://arxiv.org/abs/2504.18468</link>
<guid>https://arxiv.org/abs/2504.18468</guid>
<content:encoded><![CDATA[
<div> Gaussian surfel representation, inverse rendering, relighting, deferred shading pipeline, high-quality reconstruction

Summary:
RGS-DR introduces a novel method for inverse rendering of glossy and reflective objects, addressing view-dependent issues by utilizing a 2D Gaussian surfel representation for accurate geometry and normals estimation. The approach incorporates learnable primitives in a deferred shading pipeline to reduce rendering artifacts and preserve sharp reflections. The use of multi-level cube mipmaps aids in approximating environment lighting integrals for high-quality reconstruction and relighting. Additionally, a residual pass with spherical-mipmap-based directional encoding refines appearance modeling. Experimentation shows that RGS-DR excels in reconstructing and rendering shiny objects, surpassing existing methods in quality and allowing for relighting capabilities. <br><br>Summary: <div>
arXiv:2504.18468v3 Announce Type: replace 
Abstract: We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D Gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. Our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. By employing a multi-level cube mipmap, RGS-DR accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. A residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Multi-party Collaborative Attention Control for Image Customization</title>
<link>https://arxiv.org/abs/2505.01428</link>
<guid>https://arxiv.org/abs/2505.01428</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, image generation, attention control, subject localization, customization

Summary:
MCA-Ctrl is introduced as a method for high-quality image customization using text and visual conditions. It addresses limitations of current methods by allowing customization with both text and complex visual conditions, coordinating multiple diffusion processes and guiding image generation. MCA-Ctrl captures specific subjects' content and appearance while maintaining semantic consistency. A Subject Localization Module is introduced to extract precise subject and editable image layers based on user instructions, reducing subject leakage and confusion in complex visual scenarios. Extensive experiments show MCA-Ctrl outperforms existing methods in zero-shot image customization, resolving issues related to inconsistent backgrounds and high computational costs. MCA-Ctrl offers a tuning-free approach for customized image generation with improved quality and efficiency.<br /><br />Summary: <div>
arXiv:2505.01428v1 Announce Type: new 
Abstract: The rapid advancement of diffusion models has increased the need for customized image generation. However, current customization methods face several limitations: 1) typically accept either image or text conditions alone; 2) customization in complex visual scenarios often leads to subject leakage or confusion; 3) image-conditioned outputs tend to suffer from inconsistent backgrounds; and 4) high computational costs. To address these issues, this paper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a tuning-free method that enables high-quality image customization using both text and complex visual conditions. Specifically, MCA-Ctrl leverages two key operations within the self-attention layer to coordinate multiple parallel diffusion processes and guide the target image generation. This approach allows MCA-Ctrl to capture the content and appearance of specific subjects while maintaining semantic consistency with the conditional input. Additionally, to mitigate subject leakage and confusion issues common in complex visual scenarios, we introduce a Subject Localization Module that extracts precise subject and editable image layers based on user instructions. Extensive quantitative and human evaluation experiments show that MCA-Ctrl outperforms existing methods in zero-shot image customization, effectively resolving the mentioned issues.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI-Driven Detection of Human Monkeypox Using Deep Learning and Vision Transformers: A Comprehensive Analysis</title>
<link>https://arxiv.org/abs/2505.01429</link>
<guid>https://arxiv.org/abs/2505.01429</guid>
<content:encoded><![CDATA[
<div> zoonotic viral illness, mpox, deep learning, medical imaging, transfer learning <br />
Summary: <br />Since mpox is a zoonotic viral illness with symptoms similar to measles and chickenpox, early clinical diagnosis is challenging. This study explores the use of deep learning and vision transformer-based models to analyze skin lesion images for disease detection. Training models from scratch with limited datasets proved to be a drawback, leading to the use of transfer learning with pre-trained models for better classification. The MobileNet-v2 model showed the highest accuracy at 93.15%, followed by ViT B16 at 92.12% and ResNet-50 at 86.21%. Explainable AI techniques were used to validate model performance, demonstrating the potential of using deep learning in medical image analysis for disease detection and diagnosis. <br /> <div>
arXiv:2505.01429v1 Announce Type: new 
Abstract: Since mpox can spread from person to person, it is a zoonotic viral illness that poses a significant public health concern. It is difficult to make an early clinical diagnosis because of how closely its symptoms match those of measles and chickenpox. Medical imaging combined with deep learning (DL) techniques has shown promise in improving disease detection by analyzing affected skin areas. Our study explore the feasibility to train deep learning and vision transformer-based models from scratch with publicly available skin lesion image dataset. Our experimental results show dataset limitation as a major drawback to build better classifier models trained from scratch. We used transfer learning with the help of pre-trained models to get a better classifier. The MobileNet-v2 outperformed other state of the art pre-trained models with 93.15% accuracy and 93.09% weighted average F1 score. ViT B16 and ResNet-50 also achieved satisfactory performance compared to already available studies with accuracy 92.12% and 86.21% respectively. To further validate the performance of the models, we applied explainable AI techniques.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models</title>
<link>https://arxiv.org/abs/2505.01430</link>
<guid>https://arxiv.org/abs/2505.01430</guid>
<content:encoded><![CDATA[
<div> metric, cultural biases, image generation, data imbalance, AI systems
Summary:
The paper introduces the Component Inclusion Score (CIS) as a metric to evaluate cultural biases in text-to-image (T2I) models. Through analysis of 2,400 images, biases in compositional fragility and contextual misalignment for Western and non-Western cultural prompts are identified. The study highlights the impact of data imbalance, attention entropy, and embedding superposition on model fairness. By benchmarking models like Stable Diffusion with CIS, potential interventions for enhancing cultural inclusivity in AI-generated imagery are explored. The research aims to diagnose and mitigate biases in T2I generation, advocating for more equitable AI systems. <div>
arXiv:2505.01430v1 Announce Type: new 
Abstract: The transformative potential of text-to-image (T2I) models hinges on their ability to synthesize culturally diverse, photorealistic images from textual prompts. However, these models often perpetuate cultural biases embedded within their training data, leading to systemic misrepresentations. This paper benchmarks the Component Inclusion Score (CIS), a metric designed to evaluate the fidelity of image generation across cultural contexts. Through extensive analysis involving 2,400 images, we quantify biases in terms of compositional fragility and contextual misalignment, revealing significant performance gaps between Western and non-Western cultural prompts. Our findings underscore the impact of data imbalance, attention entropy, and embedding superposition on model fairness. By benchmarking models like Stable Diffusion with CIS, we provide insights into architectural and data-centric interventions for enhancing cultural inclusivity in AI-generated imagery. This work advances the field by offering a comprehensive tool for diagnosing and mitigating biases in T2I generation, advocating for more equitable AI systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZS-VCOS: Zero-Shot Outperforms Supervised Video Camouflaged Object Segmentation</title>
<link>https://arxiv.org/abs/2505.01431</link>
<guid>https://arxiv.org/abs/2505.01431</guid>
<content:encoded><![CDATA[
<div> Keywords: camouflaged object segmentation, zero-shot approach, optical flow, vision-language model, MoCA-Mask dataset 

Summary:
Camouflaged object segmentation is a challenging task due to the similarity between objects and backgrounds. Existing methods have focused on supervised or unsupervised pre-training, neglecting zero-shot approaches. This study proposes a novel method that integrates optical flow, vision-language models, and the Segment Anything Model (SAM) 2 in a sequential pipeline. The approach achieves significant performance improvements on the MoCA-Mask dataset, surpassing existing zero-shot and supervised methods in terms of F-measure. Evaluation on the MoCA-Filter dataset also demonstrates superior results compared to the FlowSAM method. A thorough ablation study confirms the individual contributions of each component. The proposed method is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2505.01431v1 Announce Type: new 
Abstract: Camouflaged object segmentation presents unique challenges compared to traditional segmentation tasks, primarily due to the high similarity in patterns and colors between camouflaged objects and their backgrounds. Effective solutions to this problem have significant implications in critical areas such as pest control, defect detection, and lesion segmentation in medical imaging. Prior research has predominantly emphasized supervised or unsupervised pre-training methods, leaving zero-shot approaches significantly underdeveloped. Existing zero-shot techniques commonly utilize the Segment Anything Model (SAM) in automatic mode or rely on vision-language models to generate cues for segmentation; however, their performances remain unsatisfactory, likely due to the similarity of the camouflaged object and the background. Optical flow, commonly utilized for detecting moving objects, has demonstrated effectiveness even with camouflaged entities. Our method integrates optical flow, a vision-language model, and SAM 2 into a sequential pipeline. Evaluated on the MoCA-Mask dataset, our approach achieves outstanding performance improvements, significantly outperforming existing zero-shot methods by raising the F-measure ($F_\beta^w$) from 0.296 to 0.628. Remarkably, our approach also surpasses supervised methods, increasing the F-measure from 0.476 to 0.628. Additionally, evaluation on the MoCA-Filter dataset demonstrates an increase in the success rate from 0.628 to 0.697 when compared with FlowSAM, a supervised transfer method. A thorough ablation study further validates the individual contributions of each component. More details can be found on https://github.com/weathon/vcos.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos</title>
<link>https://arxiv.org/abs/2505.01481</link>
<guid>https://arxiv.org/abs/2505.01481</guid>
<content:encoded><![CDATA[
<div> VideoHallu, synthetic video generation, foundation models, common sense, physical laws <br />
Summary:
The article introduces VideoHallu, a benchmark for evaluating synthetic videos generated by foundation models like Veo2, Sora, and Kling. Existing metrics like VideoScore do not capture abnormalities in synthetic videos. Multi-modal large language models (MLLMs) are used as interpretable evaluators to detect violations of common sense and physical laws. State-of-the-art MLLMs like GPT-4o and Gemini-2.5-Pro still struggle with basic reasoning tasks in synthetic videos. Fine-tuning these models using Group Relative Policy Optimization (GRPO) on real and synthetic data improves their accuracy, especially with the integration of counterexamples. The study highlights the challenge of hallucination in synthetic video generation and showcases the potential for enhancing MLLMs' reasoning abilities in this context.<br /><br />Summary: <div>
arXiv:2505.01481v1 Announce Type: new 
Abstract: Synthetic video generation with foundation models has gained attention for its realism and wide applications. While these models produce high-quality frames, they often fail to respect common sense and physical laws, resulting in abnormal content. Existing metrics like VideoScore emphasize general quality but ignore such violations and lack interpretability. A more insightful approach is using multi-modal large language models (MLLMs) as interpretable evaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities in synthetic videos remains underexplored. To address this, we introduce VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora, and Kling, paired with expert-designed QA tasks solvable via human-level reasoning across various categories. We assess several SoTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat, these models still hallucinate on basic commonsense and physics tasks in synthetic settings, underscoring the challenge of hallucination. We further fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real and synthetic commonsense/physics data. Results show notable accuracy gains, especially with counterexample integration, advancing MLLMs' reasoning capabilities. Our data is available at https://github.com/zli12321/VideoHallu.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldGenBench: A World-Knowledge-Integrated Benchmark for Reasoning-Driven Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.01490</link>
<guid>https://arxiv.org/abs/2505.01490</guid>
<content:encoded><![CDATA[
<div> Keyword: text-to-image generation, world knowledge, implicit reasoning, benchmark, diffusion models<br />
Summary:<br />
The article introduces a new benchmark called WorldGenBench for evaluating text-to-image generation models based on their world knowledge grounding and implicit inferential capabilities. The benchmark covers various domains such as humanities and nature. The Knowledge Checklist Score is proposed as a metric to measure how well generated images meet key semantic expectations. Experiments involving 21 state-of-the-art models show that diffusion models perform well among open-source methods, but proprietary auto-regressive models like GPT-4o show stronger reasoning and knowledge integration. The findings emphasize the importance of deeper understanding and inference capabilities in enhancing the performance of text-to-image generation systems.<br /> <div>
arXiv:2505.01490v1 Announce Type: new 
Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive results, yet existing models still struggle with prompts that require rich world knowledge and implicit reasoning: both of which are critical for producing semantically accurate, coherent, and contextually appropriate images in real-world scenarios. To address this gap, we introduce \textbf{WorldGenBench}, a benchmark designed to systematically evaluate T2I models' world knowledge grounding and implicit inferential capabilities, covering both the humanities and nature domains. We propose the \textbf{Knowledge Checklist Score}, a structured metric that measures how well generated images satisfy key semantic expectations. Experiments across 21 state-of-the-art models reveal that while diffusion models lead among open-source methods, proprietary auto-regressive models like GPT-4o exhibit significantly stronger reasoning and knowledge integration. Our findings highlight the need for deeper understanding and inference capabilities in next-generation T2I systems. Project Page: \href{https://dwanzhang-ai.github.io/WorldGenBench/}{https://dwanzhang-ai.github.io/WorldGenBench/}
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer</title>
<link>https://arxiv.org/abs/2505.01530</link>
<guid>https://arxiv.org/abs/2505.01530</guid>
<content:encoded><![CDATA[
<div> Keywords: structured information extraction, deep learning, engineering drawings, oriented bounding box detection, transformer-based document parsing<br />
Summary:<br />
- Accurate extraction of key information from engineering drawings is vital for high-precision manufacturing. Manual extraction is time-consuming and error-prone, while traditional OCR techniques face challenges with complex layouts.
- A hybrid deep learning framework is proposed, integrating an OBB detection model with a transformer-based document parsing model for structured information extraction.
- YOLOv11 is trained to detect key categories like GD&amp;T and Tolerances, which are then labeled and used to fine-tune Donut for structured JSON output.
- The single model approach outperforms category-specific models, achieving high precision, recall, and F1 score while reducing hallucination.
- The framework enhances accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.<br />  
Summary: <div>
arXiv:2505.01530v1 Announce Type: new 
Abstract: Accurate extraction of key information from 2D engineering drawings is crucial for high-precision manufacturing. Manual extraction is time-consuming and error-prone, while traditional Optical Character Recognition (OCR) techniques often struggle with complex layouts and overlapping symbols, resulting in unstructured outputs. To address these challenges, this paper proposes a novel hybrid deep learning framework for structured information extraction by integrating an oriented bounding box (OBB) detection model with a transformer-based document parsing model (Donut). An in-house annotated dataset is used to train YOLOv11 for detecting nine key categories: Geometric Dimensioning and Tolerancing (GD&amp;T), General Tolerances, Measures, Materials, Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are cropped into images and labeled to fine-tune Donut for structured JSON output. Fine-tuning strategies include a single model trained across all categories and category-specific models. Results show that the single model consistently outperforms category-specific ones across all evaluation metrics, achieving higher precision (94.77% for GD&amp;T), recall (100% for most), and F1 score (97.3%), while reducing hallucination (5.23%). The proposed framework improves accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation</title>
<link>https://arxiv.org/abs/2505.01548</link>
<guid>https://arxiv.org/abs/2505.01548</guid>
<content:encoded><![CDATA[
<div> Sparse Event Camera, RGB-Event Fusion, Event Representation, Temporal Correlations, Semantic Segmentation
Summary:
The paper introduces a novel approach for RGB-Event fusion using a Motion-enhanced Event Tensor (MET) representation to address temporal, spatial, and modal misalignments. The MET transforms sparse event voxels into a dense and temporally coherent form by incorporating dense optical flows and event temporal features. The proposed Frequency-aware Bidirectional Flow Aggregation Module (BFAM) utilizes the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms handle spatiotemporal misalignment. Experimental results on large-scale datasets demonstrate superior performance compared to existing approaches in RGB-Event semantic segmentation tasks. The code for the framework is publicly available at the provided GitHub repository. 
<br /><br />Summary: <div>
arXiv:2505.01548v1 Announce Type: new 
Abstract: Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning</title>
<link>https://arxiv.org/abs/2505.01558</link>
<guid>https://arxiv.org/abs/2505.01558</guid>
<content:encoded><![CDATA[
<div> satellite technology, remote sensing, segmentation models, domain adaptation, geospatial

Summary:<br />
Remote sensing plays a crucial role in various applications such as land cover mapping and crop yield prediction. However, the accuracy of segmentation models is often limited by the scarcity and variability of labeled data across different sources and conditions. To address this issue, this study proposes a domain generalization approach that combines soft-alignment pseudo-labeling with generative pre-training from the source to the target domain. The research also introduces mathematical insights into generative learning based on Mean Absolute Error for domain-invariant feature learning. Experimental results using hyperspectral and multispectral remote sensing datasets demonstrate the effectiveness of the proposed method in improving adaptability and segmentation accuracy. <div>
arXiv:2505.01558v1 Announce Type: new 
Abstract: Remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. Advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. Domain adaptation offers a promising solution to improve model generalization. This paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. We further provide new mathematical insights into MAE-based generative learning for domain-invariant feature learning. Experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PainFormer: a Vision Foundation Model for Automatic Pain Assessment</title>
<link>https://arxiv.org/abs/2505.01571</link>
<guid>https://arxiv.org/abs/2505.01571</guid>
<content:encoded><![CDATA[
<div> Keywords: PainFormer, multi-task learning, vision foundation model, automatic pain assessment, multimodal inputs

Summary:
PainFormer is introduced as a vision foundation model for automatic pain assessment, based on multi-task learning principles and trained on 14 tasks/datasets with 10.9 million samples. It serves as an embedding extractor for various input modalities, providing feature representations to the Embedding-Mixer for final pain assessment. The model was extensively tested on behavioral and physiological modalities, showing effective extraction of high-quality embeddings. Evaluations on BioVid and AI4Pain datasets compared PainFormer to 73 other methodologies, demonstrating state-of-the-art performance across modalities. The framework shows promise for general-purpose models in pain assessment, aiming to improve monitoring and decision-making processes for pain management protocols. <br /><br />Summary: <div>
arXiv:2505.01571v1 Announce Type: new 
Abstract: Pain is a manifold condition that impacts a significant percentage of the population. Accurate and reliable pain evaluation for the people suffering is crucial to developing effective and advanced pain management protocols. Automatic pain assessment systems provide continuous monitoring and support decision-making processes, ultimately aiming to alleviate distress and prevent functionality decline. This study introduces PainFormer, a vision foundation model based on multi-task learning principles trained simultaneously on 14 tasks/datasets with a total of 10.9 million samples. Functioning as an embedding extractor for various input modalities, the foundation model provides feature representations to the Embedding-Mixer, a transformer-based module that performs the final pain assessment. Extensive experiments employing behavioral modalities-including RGB, synthetic thermal, and estimated depth videos-and physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that PainFormer effectively extracts high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 73 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Task Assistance with Multimodal Cues from a Single Demonstration</title>
<link>https://arxiv.org/abs/2505.01578</link>
<guid>https://arxiv.org/abs/2505.01578</guid>
<content:encoded><![CDATA[
<div> Assistance, Multimodal, Contextual, Gaze, Speech
<br />
Summary: 
The article introduces MICA, a framework for enhancing conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into sub-tasks, extracting keyframes and captions to provide richer contextual grounding for visual question answering. Evaluations show that multimodal cues significantly improve response quality over frame-based retrieval. Gaze cues alone achieve 93% of speech performance, with the combination yielding the highest accuracy. The effectiveness of implicit (gaze) vs. explicit (speech) cues depends on the task type, highlighting the need for adaptable multimodal models. The results showcase the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance. 
<br /> <div>
arXiv:2505.01578v1 Announce Type: new 
Abstract: A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action</title>
<link>https://arxiv.org/abs/2505.01583</link>
<guid>https://arxiv.org/abs/2505.01583</guid>
<content:encoded><![CDATA[
<div> masked event prediction, causal reasoning, video temporal understanding, video segmentation, dense captioning

Summary:
TEMPURA is a new framework designed to improve video temporal understanding by incorporating causal reasoning and fine-grained event segmentation. The two-stage training approach of TEMPURA involves masked event prediction reasoning to reconstruct missing events and generate causal explanations, followed by video segmentation and dense captioning to decompose videos into non-overlapping events with detailed descriptions. Trained on the VER dataset with temporally aligned event descriptions and reasoning steps, TEMPURA outperforms baseline models in temporal grounding and highlight detection benchmarks. By integrating causal reasoning with fine-grained temporal segmentation, TEMPURA demonstrates enhanced video understanding capabilities. <div>
arXiv:2505.01583v1 Announce Type: new 
Abstract: Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation</title>
<link>https://arxiv.org/abs/2505.01615</link>
<guid>https://arxiv.org/abs/2505.01615</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal sensor fusion, autonomous marine navigation, cross attention transformer, LiDAR point clouds, birds eye view <br />
Summary: <br />
The study introduces a novel approach for multimodal sensor fusion using a cross attention transformer to enhance autonomous marine navigation safety. The method combines RGB images, long wave infrared images, and sparse LiDAR point clouds to create a comprehensive birds eye view of a vessel's surroundings. Training incorporates X band radar and electronic chart data for more informed predictions. The resulting view offers a detailed and reliable representation of the scene, improving navigational accuracy and robustness. Real-world sea trials demonstrate the effectiveness of the method, even in challenging weather conditions and complex maritime environments. <div>
arXiv:2505.01615v1 Announce Type: new 
Abstract: We propose a cross attention transformer based method for multimodal sensor fusion to build a birds eye view of a vessels surroundings supporting safer autonomous marine navigation. The model deeply fuses multiview RGB and long wave infrared images with sparse LiDAR point clouds. Training also integrates X band radar and electronic chart data to inform predictions. The resulting view provides a detailed reliable scene representation improving navigational accuracy and robustness. Real world sea trials confirm the methods effectiveness even in adverse weather and complex maritime settings.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Onboard AI-Enabled Solutions to Space Object Detection for Space Sustainability</title>
<link>https://arxiv.org/abs/2505.01650</link>
<guid>https://arxiv.org/abs/2505.01650</guid>
<content:encoded><![CDATA[
<div> Keywords: low-Earth orbit satellites, space object detection, deep learning models, collision assessment, vision sensors<br />
Summary:<br />
The paper delves into the importance of effective space object detection (SOD) for collision assessment and avoidance in the rapidly expanding low-Earth orbit (LEO) satellite field. It explores utilizing vision sensors and deep learning models, such as Squeeze-and-Excitation (SE), Vision Transformer (ViT), and Generalized Efficient Layer Aggregation Network (GELAN), for SOD tasks. Experimental results showcase promising performance metrics, with the proposed models achieving high mean average precision scores. The GELAN-ViT-SE model stands out by enhancing mAP50 and mAP50:95 scores compared to the baseline model, while also reducing GFLOPs and peak power consumption. This research highlights the potential of advanced technologies in enhancing SOD capabilities and optimizing satellite operations for future space missions. <br /><br /> <div>
arXiv:2505.01650v1 Announce Type: new 
Abstract: The rapid expansion of advanced low-Earth orbit (LEO) satellites in large constellations is positioning space assets as key to the future, enabling global internet access and relay systems for deep space missions. A solution to the challenge is effective space object detection (SOD) for collision assessment and avoidance. In SOD, an LEO satellite must detect other satellites and objects with high precision and minimal delay. This paper investigates the feasibility and effectiveness of employing vision sensors for SOD tasks based on deep learning (DL) models. It introduces models based on the Squeeze-and-Excitation (SE) layer, Vision Transformer (ViT), and the Generalized Efficient Layer Aggregation Network (GELAN) and evaluates their performance under SOD scenarios. Experimental results show that the proposed models achieve mean average precision at intersection over union threshold 0.5 (mAP50) scores of up to 0.751 and mean average precision averaged over intersection over union thresholds from 0.5 to 0.95 (mAP50:95) scores of up to 0.280. Compared to the baseline GELAN-t model, the proposed GELAN-ViT-SE model increases the average mAP50 from 0.721 to 0.751, improves the mAP50:95 from 0.266 to 0.274, reduces giga floating point operations (GFLOPs) from 7.3 to 5.6, and lowers peak power consumption from 2080.7 mW to 2028.7 mW by 2.5\%.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel WaveInst-based Network for Tree Trunk Structure Extraction and Pattern Analysis in Forest Inventory</title>
<link>https://arxiv.org/abs/2505.01656</link>
<guid>https://arxiv.org/abs/2505.01656</guid>
<content:encoded><![CDATA[
<div> Keywords: tree structure extraction, instance segmentation, wavelet transform, phenotypic dataset, precision forestry

Summary: 
The study introduces a novel WaveInst instance segmentation framework for accurate tree structure extraction, utilizing a discrete wavelet transform to enhance multi-scale edge information. The method outperforms existing techniques on various datasets, including the newly introduced PoplarDataset for phenotypic analysis of artificial forests. The proposed model achieves superior performance in extracting tree structures, surpassing the state-of-the-art method. By integrating the segmentation model with a regression model, accurate tree growth parameters such as tree location, diameter-at-breast-height, and plant height can be obtained directly from 2D images. This research contributes valuable data for tree structure analysis in phenotype research, with applications in precision forestry, ecological monitoring, and intelligent breeding.<br /><br />Summary: <div>
arXiv:2505.01656v1 Announce Type: new 
Abstract: The pattern analysis of tree structure holds significant scientific value for genetic breeding and forestry management. The current trunk and branch extraction technologies are mainly LiDAR-based or UAV-based. The former approaches obtain high-precision 3D data, but its equipment cost is high and the three-dimensional (3D) data processing is complex. The latter approaches efficiently capture canopy information, but they miss the 3-D structure of trees. In order to deal with the branch information extraction from the complex background interference and occlusion, this work proposes a novel WaveInst instance segmentation framework, involving a discrete wavelet transform, to enhance multi-scale edge information for accurately improving tree structure extraction. Experimental results of the proposed model show superior performance on SynthTree43k, CaneTree100, Urban Street and our PoplarDataset. Moreover, we present a new Phenotypic dataset PoplarDataset, which is dedicated to extract tree structure and pattern analysis from artificial forest. The proposed method achieves a mean average precision of 49.6 and 24.3 for the structure extraction of mature and juvenile trees, respectively, surpassing the existing state-of-the-art method by 9.9. Furthermore, by in tegrating the segmentation model within the regression model, we accurately achieve significant tree grown parameters, such as the location of trees, the diameter-at-breast-height of individual trees, and the plant height, from 2D images directly. This study provides a scientific and plenty of data for tree structure analysis in related to the phenotype research, offering a platform for the significant applications in precision forestry, ecological monitoring, and intelligent breeding.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.01664</link>
<guid>https://arxiv.org/abs/2505.01664</guid>
<content:encoded><![CDATA[
<div> domain adaptation, partial domain adaptation, optimal transport, neural network, class-conditional distribution matching 

Summary: 
The paper introduces a Soft-masked Semi-dual Optimal Transport (SSOT) method to address the challenges of partial domain adaptation (PDA) in visual domain adaptation. The method estimates class weights of domains to construct a reweighed source domain for class-conditional distribution matching with the target domain. A soft-masked transport distance matrix enhances the representation ability of optimal transport in the shared feature space. The semi-dual formulation of the entropy-regularized Kantorovich problem is utilized for large-scale optimal transport problems, optimized using gradient-based algorithms. A neural network is used to approximate the Kantorovich potential, allowing generalization of the dual variable. The SSOT model, based on neural networks, can be optimized in an end-to-end manner. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed SSOT method in partial domain adaptation scenarios. 

<br /><br />Summary: <div>
arXiv:2505.01664v1 Announce Type: new 
Abstract: Visual domain adaptation aims to learn discriminative and domain-invariant representation for an unlabeled target domain by leveraging knowledge from a labeled source domain. Partial domain adaptation (PDA) is a general and practical scenario in which the target label space is a subset of the source one. The challenges of PDA exist due to not only domain shift but also the non-identical label spaces of domains. In this paper, a Soft-masked Semi-dual Optimal Transport (SSOT) method is proposed to deal with the PDA problem. Specifically, the class weights of domains are estimated, and then a reweighed source domain is constructed, which is favorable in conducting class-conditional distribution matching with the target domain. A soft-masked transport distance matrix is constructed by category predictions, which will enhance the class-oriented representation ability of optimal transport in the shared feature space. To deal with large-scale optimal transport problems, the semi-dual formulation of the entropy-regularized Kantorovich problem is employed since it can be optimized by gradient-based algorithms. Further, a neural network is exploited to approximate the Kantorovich potential due to its strong fitting ability. This network parametrization also allows the generalization of the dual variable outside the supports of the input distribution. The SSOT model is built upon neural networks, which can be optimized alternately in an end-to-end manner. Extensive experiments are conducted on four benchmark datasets to demonstrate the effectiveness of SSOT.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study</title>
<link>https://arxiv.org/abs/2505.01680</link>
<guid>https://arxiv.org/abs/2505.01680</guid>
<content:encoded><![CDATA[
<div> Keywords: ARAT, upper extremity assessment, stroke rehabilitation, automated scoring system, multimodal video analysis

Summary:
An automated scoring system for the Action Research Arm Test (ARAT) in stroke rehabilitation has been developed in this study. The system integrates multimodal video analysis using advanced models like SlowFast, I3D, and Transformer-based models, incorporating OpenPose keypoints and object locations. It utilizes multi-view data and applies early and late fusion techniques to combine features across views and models. Hierarchical Bayesian Models (HBMs) are employed to infer movement quality components, enhancing interpretability. The system includes a clinician dashboard displaying task scores, execution times, and quality assessments. Clinicians reviewed the system's generated video ratings and provided feedback on its accuracy and usability. Validation on a stroke rehabilitation dataset showed the framework achieving 89.0% accuracy with late fusion, and the HBMs closely aligned with manual assessments. This work offers a scalable and interpretable solution for automated rehabilitation assessments, advancing the field with clinical validation. 

<br /><br />Summary: <div>
arXiv:2505.01680v1 Announce Type: new 
Abstract: Manual scoring of the Action Research Arm Test (ARAT) for upper extremity assessment in stroke rehabilitation is time-intensive and variable. We propose an automated ARAT scoring system integrating multimodal video analysis with SlowFast, I3D, and Transformer-based models using OpenPose keypoints and object locations. Our approach employs multi-view data (ipsilateral, contralateral, and top perspectives), applying early and late fusion to combine features across views and models. Hierarchical Bayesian Models (HBMs) infer movement quality components, enhancing interpretability. A clinician dashboard displays task scores, execution times, and quality assessments. We conducted a study with five clinicians who reviewed 500 video ratings generated by our system, providing feedback on its accuracy and usability. Evaluated on a stroke rehabilitation dataset, our framework achieves 89.0% validation accuracy with late fusion, with HBMs aligning closely with manual assessments. This work advances automated rehabilitation by offering a scalable, interpretable solution with clinical validation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Aware CLIP Few-Shot Learning</title>
<link>https://arxiv.org/abs/2505.01694</link>
<guid>https://arxiv.org/abs/2505.01694</guid>
<content:encoded><![CDATA[
<div> Topological information, Vision-Language Models, Few-shot learning, Representation Topology Divergence, Task Residual<br />
<br />
Summary: 
In this new article, the authors propose a topology-aware tuning approach for adapting large Vision-Language Models (VLMs) like CLIP for few-shot learning. By integrating Representation Topology Divergence (RTD) into the Task Residual (TR) framework, they align the topological structures of visual and text representations while freezing base VLM encoders. This method aims to balance pre-trained knowledge retention and task-specific adaptation, optimizing only lightweight Task Residual parameters. Across 6 benchmark datasets, the approach shows significant gains in few-shot settings, with an average accuracy improvement of 1-2% over baseline methods. This strategy effectively boosts VLM few-shot capabilities by incorporating topological alignment.<br /><br />Summary: <div>
arXiv:2505.01694v1 Announce Type: new 
Abstract: Efficiently adapting large Vision-Language Models (VLMs) like CLIP for few-shot learning poses challenges in balancing pre-trained knowledge retention and task-specific adaptation. Existing methods often overlook valuable structural information within the VLM's latent space. We introduce a topology-aware tuning approach integrating Representation Topology Divergence (RTD) into the Task Residual (TR) framework. By explicitly aligning the topological structures of visual and text representations using a combined RTD and Cross-Entropy loss, while freezing base VLM encoders, our method enhances few-shot performance. We optimize only lightweight Task Residual parameters, effectively leveraging topological information. Across 6 diverse benchmark datasets, our approach demonstrates significant gains, achieving an average accuracy improvement of 1-2\% over relevant baseline methods in few-shot settings. This work presents an effective strategy to boost VLM few-shot capabilities by incorporating topological alignment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning</title>
<link>https://arxiv.org/abs/2505.01699</link>
<guid>https://arxiv.org/abs/2505.01699</guid>
<content:encoded><![CDATA[
<div> Keywords: face recognition, fairness, bias mitigation, Bayesian Network, meta-learning

Summary:
In a new study, researchers focus on face component fairness, a concept that addresses bias in individual biological face features. They introduce a novel approach called Bayesian Network-informed Meta Reweighting (BNMR) to mitigate bias in face attribute prediction at the biological feature level. The study identifies challenges such as attribute label scarcity and inter-dependencies, which limit the effectiveness of bias mitigation. BNMR incorporates a Bayesian Network calibrator to guide a meta-learning-based sample reweighting process, dynamically tracking model bias and encoding prior probabilities for face component attributes. Experimental results show that BNMR outperforms recent bias mitigation baselines and positively impacts demographic fairness. The findings suggest that face component fairness could be a surrogate objective for demographic fairness, opening up new research avenues in this area. The code for the approach is publicly available for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.01699v1 Announce Type: new 
Abstract: The widespread integration of face recognition technologies into various applications (e.g., access control and personalized advertising) necessitates a critical emphasis on fairness. While previous efforts have focused on demographic fairness, the fairness of individual biological face components remains unexplored. In this paper, we focus on face component fairness, a fairness notion defined by biological face features. To our best knowledge, our work is the first work to mitigate bias of face attribute prediction at the biological feature level. In this work, we identify two key challenges in optimizing face component fairness: attribute label scarcity and attribute inter-dependencies, both of which limit the effectiveness of bias mitigation from previous approaches. To address these issues, we propose \textbf{B}ayesian \textbf{N}etwork-informed \textbf{M}eta \textbf{R}eweighting (BNMR), which incorporates a Bayesian Network calibrator to guide an adaptive meta-learning-based sample reweighting process. During the training process of our approach, the Bayesian Network calibrator dynamically tracks model bias and encodes prior probabilities for face component attributes to overcome the above challenges. To demonstrate the efficacy of our approach, we conduct extensive experiments on a large-scale real-world human face dataset. Our results show that BNMR is able to consistently outperform recent face bias mitigation baselines. Moreover, our results suggest a positive impact of face component fairness on the commonly considered demographic fairness (e.g., \textit{gender}). Our findings pave the way for new research avenues on face component fairness, suggesting that face component fairness could serve as a potential surrogate objective for demographic fairness. The code for our work is publicly available~\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings</title>
<link>https://arxiv.org/abs/2505.01711</link>
<guid>https://arxiv.org/abs/2505.01711</guid>
<content:encoded><![CDATA[
<div> Framework, medical imaging, language models, clinical reasoning, dataset 

Summary: 
The paper introduces CXR-TextInter, a framework that utilizes text-centric large language models for automated interpretation of chest X-rays (CXR). By operating on structured textual representations of image content, generated by an image analysis pipeline, the framework enhances clinical reasoning with an integrated medical knowledge module. The development of the MediInstruct-CXR dataset, containing structured image representations paired with diverse instruction-response examples, facilitates training and evaluation. The CXR-TextInter framework achieves state-of-the-art performance in pathology detection, report generation, and visual question answering on the CXR-ClinEval benchmark, surpassing existing multimodal models. Ablation studies confirm the importance of the knowledge integration module, and blinded human evaluation by radiologists shows a preference for the clinical quality of CXR-TextInter outputs. This work validates a novel approach in medical image AI, demonstrating the effectiveness of leveraging advanced large language models when visual information is structured and domain knowledge is integrated.<br /><br /> <div>
arXiv:2505.01711v1 Announce Type: new 
Abstract: Automated interpretation of chest X-rays (CXR) is a critical task with the potential to significantly improve clinical workflow and patient care. While recent advances in multimodal foundation models have shown promise, effectively leveraging the full power of large language models (LLMs) for this visual task remains an underexplored area. This paper introduces CXR-TextInter, a novel framework that repurposes powerful text-centric LLMs for CXR interpretation by operating solely on a rich, structured textual representation of the image content, generated by an upstream image analysis pipeline. We augment this LLM-centric approach with an integrated medical knowledge module to enhance clinical reasoning. To facilitate training and evaluation, we developed the MediInstruct-CXR dataset, containing structured image representations paired with diverse, clinically relevant instruction-response examples, and the CXR-ClinEval benchmark for comprehensive assessment across various interpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that CXR-TextInter achieves state-of-the-art quantitative performance across pathology detection, report generation, and visual question answering, surpassing existing multimodal foundation models. Ablation studies confirm the critical contribution of the knowledge integration module. Furthermore, blinded human evaluation by board-certified radiologists shows a significant preference for the clinical quality of outputs generated by CXR-TextInter. Our work validates an alternative paradigm for medical image AI, showcasing the potential of harnessing advanced LLM capabilities when visual information is effectively structured and domain knowledge is integrated.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision and Intention Boost Large Language Model in Long-Term Action Anticipation</title>
<link>https://arxiv.org/abs/2505.01713</link>
<guid>https://arxiv.org/abs/2505.01713</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-term action anticipation, Vision-language model, Intention-conditioned, Multi-modality fusion, Example selection strategy

Summary:
The study introduces a novel Intention-Conditioned Vision-Language (ICVL) model for long-term action anticipation. By combining visual data with the reasoning capabilities of large language models (LLMs), the ICVL model extracts behavioral intentions from video inputs using a vision-language model. These intentions are fused with visual features to enhance visual representations, which are then inputted into LLM for future action anticipation. Additionally, an effective example selection strategy that considers visual and textual similarities is proposed to improve in-context learning. Experimental results on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+ datasets demonstrate the superiority of the ICVL model in terms of performance. The novel approach addresses limitations of single-modality methods by leveraging the rich semantic information of visual data and the powerful reasoning capabilities of LLMs, thereby enhancing the accuracy of long-term action anticipation. 

<br /><br />Summary: <div>
arXiv:2505.01713v1 Announce Type: new 
Abstract: Long-term action anticipation (LTA) aims to predict future actions over an extended period. Previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. Recent researches leverage large language models (LLMs) by utilizing text-based inputs which suffer severe information loss. To tackle these limitations single-modality methods face, we propose a novel Intention-Conditioned Vision-Language (ICVL) model in this study that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of LLMs. Considering intention as a high-level concept guiding the evolution of actions, we first propose to employ a vision-language model (VLM) to infer behavioral intentions as comprehensive textual features directly from video inputs. The inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. These enhanced visual representations, along with textual prompts, are fed into LLM for future action anticipation. Furthermore, we propose an effective example selection strategy jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. Extensive experiments with state-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+ datasets fully demonstrate the effectiveness and superiority of the proposed method.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes</title>
<link>https://arxiv.org/abs/2505.01726</link>
<guid>https://arxiv.org/abs/2505.01726</guid>
<content:encoded><![CDATA[
<div> probabilistic framework, 3D segmentation, interactive segmentation, Neural Processes, uncertainty estimation
Summary:<br />
- NPISeg3D introduces a probabilistic framework for interactive 3D segmentation that effectively generalizes from sparse user clicks to produce accurate segmentation. <br />
- It incorporates a hierarchical latent variable structure with scene-specific and object-specific latent variables to capture global context and object-specific characteristics for enhanced few-shot generalization. <br />
- The model utilizes a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables to improve object-aware context understanding and quantify predictive uncertainty. <br />
- Experimental results on four 3D point cloud datasets demonstrate that NPISeg3D outperforms existing methods in segmentation performance, requiring fewer clicks while providing reliable uncertainty estimations. <br /> <div>
arXiv:2505.01726v1 Announce Type: new 
Abstract: Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model's ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth</title>
<link>https://arxiv.org/abs/2505.01729</link>
<guid>https://arxiv.org/abs/2505.01729</guid>
<content:encoded><![CDATA[
<div> framework, camera pose control, generative world models, self-supervised depth estimation, viewpoint synthesis <br />
Summary: <br />
The paper introduces PosePilot, a framework that enhances camera pose control in generative world models. It leverages self-supervised depth estimation principles to tightly couple camera pose and video generation. By incorporating self-supervised depth and pose readouts, the model can infer depth and relative camera motion, driving pose-aware frame warping with a photometric warping loss. A reverse warping step and pose regression loss further refine camera pose estimation. PosePilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. It sets a new benchmark for pose controllability by steering camera pose with self-supervised depth, enabling physically consistent and reliable viewpoint synthesis. Extensive experiments on autonomous driving and general-domain video datasets demonstrate the effectiveness of PosePilot in achieving precise and flexible camera pose control. <div>
arXiv:2505.01729v1 Announce Type: new 
Abstract: Recent advancements in autonomous driving (AD) systems have highlighted the potential of world models in achieving robust and generalizable performance across both ordinary and challenging driving conditions. However, a key challenge remains: precise and flexible camera pose control, which is crucial for accurate viewpoint transformation and realistic simulation of scene dynamics. In this paper, we introduce PosePilot, a lightweight yet powerful framework that significantly enhances camera pose controllability in generative world models. Drawing inspiration from self-supervised depth estimation, PosePilot leverages structure-from-motion principles to establish a tight coupling between camera pose and video generation. Specifically, we incorporate self-supervised depth and pose readouts, allowing the model to infer depth and relative camera motion directly from video sequences. These outputs drive pose-aware frame warping, guided by a photometric warping loss that enforces geometric consistency across synthesized frames. To further refine camera pose estimation, we introduce a reverse warping step and a pose regression loss, improving viewpoint precision and adaptability. Extensive experiments on autonomous driving and general-domain video datasets demonstrate that PosePilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. By steering camera pose with self-supervised depth, PosePilot sets a new benchmark for pose controllability, enabling physically consistent, reliable viewpoint synthesis in generative world models.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Multi-frame and Monocular Prior for Estimating Geometry in Dynamic Scenes</title>
<link>https://arxiv.org/abs/2505.01737</link>
<guid>https://arxiv.org/abs/2505.01737</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular videos, dynamic scenes, 3D geometry, feed-forward prediction, Siamese architecture

Summary: 
MMP, a new model for estimating 3D geometry in monocular videos of dynamic scenes, is introduced. Existing models struggle with object motion, providing only partial attributes over a pair of frames. These attributes are often noisy and require global optimizations during test time, leading to potential failure and heavy inference costs. MMP addresses this challenge by predicting geometry in a feed-forward manner, evolving a dynamic pointmap representation over multiple frames. The model incorporates a trajectory encoding module within a Siamese architecture, enhancing expressiveness for dynamic scenes. Experimental results show MMP achieves state-of-the-art quality in feed-forward pointmap prediction, with a 15.1% improvement in regression error. <br /><br />Summary: <div>
arXiv:2505.01737v1 Announce Type: new 
Abstract: In monocular videos that capture dynamic scenes, estimating the 3D geometry of video contents has been a fundamental challenge in computer vision. Specifically, the task is significantly challenged by the object motion, where existing models are limited to predict only partial attributes of the dynamic scenes, such as depth or pointmaps spanning only over a pair of frames. Since these attributes are inherently noisy under multiple frames, test-time global optimizations are often employed to fully recover the geometry, which is liable to failure and incurs heavy inference costs. To address the challenge, we present a new model, coined MMP, to estimate the geometry in a feed-forward manner, which produces a dynamic pointmap representation that evolves over multiple frames. Specifically, based on the recent Siamese architecture, we introduce a new trajectory encoding module to project point-wise dynamics on the representation for each frame, which can provide significantly improved expressiveness for dynamic scenes. In our experiments, we find MMP can achieve state-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1% enhancement in the regression error.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding</title>
<link>https://arxiv.org/abs/2505.01743</link>
<guid>https://arxiv.org/abs/2505.01743</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision Language Models, Low-resolution data, Human Behavior Understanding, Contrastive-Oriented Data Labeler, Physical-Knowledge Guided Captioner

Summary:
The paper introduces a novel system, Llambda, aimed at enhancing the understanding of low-resolution human behavior in vision systems. The system leverages limited labeled data and a large amount of unlabeled data to guide Large Vision Language Models (LVLMs) in generating informative captions for low-resolution videos. The system comprises two main components: the Contrastive-Oriented Data Labeler, which generates high-quality pseudo labels for unlabeled data, and the Physical-Knowledge Guided Captioner, which improves LVLMs' understanding of sequential data and generates high-quality video captions. To ensure deployability on devices, efficient fine-tuning techniques are employed using LoRA-based methods. Experimental results demonstrate that Llambda outperforms existing LVLM systems by up to 40.03% on average Bert-Score when tested on real-world datasets. 

<br /><br />Summary: 
1. Introduction of Llambda system for enhancing low-resolution human behavior understanding in vision systems
2. Leveraging limited labeled data and a large amount of unlabeled data to guide LVLMs in generating informative captions
3. Components of Llambda including Contrastive-Oriented Data Labeler and Physical-Knowledge Guided Captioner
4. Efficient fine-tuning techniques using LoRA-based methods for on-device deployability
5. Experimental results showing Llambda outperforming existing LVLM systems by up to 40.03% on average Bert-Score <div>
arXiv:2505.01743v1 Announce Type: new 
Abstract: The rapid advancements in Large Vision Language Models (LVLMs) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (HBU) in low-resolution vision systems, such as depth, thermal, and infrared. However, existing large vision language model (LVLM) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as RGB images. A quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. In this paper, we propose a novel, labor-saving system, Llambda, designed to support low-resolution HBU. The core idea is to leverage limited labeled data and a large amount of unlabeled data to guide LLMs in generating informative captions, which can be combined with raw data to effectively fine-tune LVLM models for understanding low-resolution videos in HBU. First, we propose a Contrastive-Oriented Data Labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. Second, we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. Therefore, it can improve LLMs' understanding of sequential data and then generate high-quality video captions. Finally, to ensure on-device deployability, we employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data. We evaluate Llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that Llambda outperforms several state-of-the-art LVLM systems up to $40.03\%$ on average Bert-Score.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</title>
<link>https://arxiv.org/abs/2505.01746</link>
<guid>https://arxiv.org/abs/2505.01746</guid>
<content:encoded><![CDATA[
<div> Keywords: gesture synthesis, concurrent co-speech gestures, dataset, temporal interaction module, mutual attention mechanism

Summary: 
The article introduces a new method for generating gestures from human speech in interactive conversations. The research addresses the challenge of synthesizing concurrent co-speech gestures in two-person dialogues, which has not been adequately explored in existing methods. The authors have developed a large-scale dataset called GES-Inter, containing over 7 million frames of diverse two-person interactive posture sequences. They propose the Co$^3$Gesture framework, which utilizes a Temporal Interaction Module (TIM) to enhance coordination between speakers' gestures. A mutual attention mechanism is employed to improve the learning of dependencies between interacted concurrent motions, resulting in more realistic and coherent gesture generation. Experimental results show that the proposed method outperforms existing models on the GES-Inter dataset. The dataset and source code are publicly available for further research and development.<br /><br />Summary: <div>
arXiv:2505.01746v1 Announce Type: new 
Abstract: Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by individual self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed GES-Inter. Additionally, we propose Co$^3$Gesture, a novel framework that enables coherent concurrent co-speech gesture synthesis including two-person interactive movements. Considering the asymmetric body dynamics of two speakers, our framework is built upon two cooperative generation branches conditioned on separated speaker audio. Specifically, to enhance the coordination of human postures with respect to corresponding speaker audios while interacting with the conversational partner, we present a Temporal Interaction Module (TIM). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further holistically boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset. The dataset and source code are publicly available at \href{https://mattie-e.github.io/Co3/}{\textit{https://mattie-e.github.io/Co3/}}.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement</title>
<link>https://arxiv.org/abs/2505.01766</link>
<guid>https://arxiv.org/abs/2505.01766</guid>
<content:encoded><![CDATA[
<div> keywords: surgical workflow recognition, multimodal approach, graph representation network, adversarial feature disentanglement, data corruption

Summary:
Surgical workflow recognition is crucial for improving patient safety and standardizing procedures. This study proposes a robust graph-based multimodal approach, called GRAD, to integrate vision and kinematic data for accurate recognition in challenging scenarios. By leveraging a Multimodal Disentanglement Graph Network and Vision-Kinematic Adversarial framework, the model captures fine-grained visual information and aligns feature spaces across modalities. A Contextual Calibrated Decoder enhances robustness against domain shifts and data corruption. Extensive experiments demonstrate the effectiveness and stability of the proposed method in handling data corruption during storage and transmission. This approach aims to advance automated surgical workflow recognition by addressing the complexities and dynamism inherent in surgical procedures.

Summary: <div>
arXiv:2505.01766v1 Announce Type: new 
Abstract: Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos</title>
<link>https://arxiv.org/abs/2505.01790</link>
<guid>https://arxiv.org/abs/2505.01790</guid>
<content:encoded><![CDATA[
<div> Keywords: web-based educational videos, question generation, vision-language models, fine-tuning, multimodal datasets<br />
Summary:<br />
1. The study explores the use of vision-language models for generating learning-oriented questions for educational videos, which can enhance user engagement and knowledge retention.<br />
2. The researchers evaluate the performance of out-of-the-box models and the effects of fine-tuning on content-specific question generation.<br />
3. Different video modalities are examined for their impact on question quality, highlighting the importance of considering various factors in question generation.<br />
4. A qualitative study assesses the relevance, answerability, and difficulty levels of the generated questions, revealing challenges in question diversity and relevance that need to be addressed.<br />
5. The findings suggest the need for future multimodal datasets and point towards promising research directions in the field of question generation for educational videos.<br /> 
<br />Summary: <div>
arXiv:2505.01790v1 Announce Type: new 
Abstract: Web-based educational videos offer flexible learning opportunities and are becoming increasingly popular. However, improving user engagement and knowledge retention remains a challenge. Automatically generated questions can activate learners and support their knowledge acquisition. Further, they can help teachers and learners assess their understanding. While large language and vision-language models have been employed in various tasks, their application to question generation for educational videos remains underexplored. In this paper, we investigate the capabilities of current vision-language models for generating learning-oriented questions for educational video content. We assess (1) out-of-the-box models' performance; (2) fine-tuning effects on content-specific question generation; (3) the impact of different video modalities on question quality; and (4) in a qualitative study, question relevance, answerability, and difficulty levels of generated questions. Our findings delineate the capabilities of current vision-language models, highlighting the need for fine-tuning and addressing challenges in question diversity and relevance. We identify requirements for future multimodal datasets and outline promising research directions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.01799</link>
<guid>https://arxiv.org/abs/2505.01799</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater scene reconstruction, SeaThru algorithm, multi-view stereo, Neural Radiance Fields, Gaussian Splatting

Summary:
AquaGS is introduced as an SfM-free underwater scene reconstruction model based on the SeaThru algorithm. It utilizes state-of-the-art multi-view stereo technology for initializing Gaussians and employs Neural Radiance Fields for rendering translucent media. The model also incorporates the explicit 3D Gaussian Splatting technique for rendering object surfaces accurately. This approach addresses the limitations of traditional methods and effectively simulates underwater optical phenomena. Experimental results demonstrate that AquaGS can achieve high-precision reconstruction in just 30 seconds with only 3 image inputs, enhancing its applicability in real-time scenarios and robotic platforms. This innovation represents a significant advancement in underwater scene reconstruction technology. 

<br /><br />Summary: <div>
arXiv:2505.01799v1 Announce Type: new 
Abstract: Underwater scene reconstruction is a critical tech-nology for underwater operations, enabling the generation of 3D models from images captured by underwater platforms. However, the quality of underwater images is often degraded due to medium interference, which limits the effectiveness of Structure-from-Motion (SfM) pose estimation, leading to subsequent reconstruction failures. Additionally, SfM methods typically operate at slower speeds, further hindering their applicability in real-time scenarios. In this paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model based on the SeaThru algorithm, which facilitates rapid and accurate separation of scene details and medium features. Our approach initializes Gaussians by integrating state-of-the-art multi-view stereo (MVS) technology, employs implicit Neural Radiance Fields (NeRF) for rendering translucent media and utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render object surfaces, which effectively addresses the limitations of traditional methods and accurately simulates underwater optical phenomena. Experimental results on the data set and the robot platform show that our model can complete high-precision reconstruction in 30 seconds with only 3 image inputs, significantly enhancing the practical application of the algorithm in robotic platforms.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows</title>
<link>https://arxiv.org/abs/2505.01802</link>
<guid>https://arxiv.org/abs/2505.01802</guid>
<content:encoded><![CDATA[
<div> Neural Network Models, 3D Full-Body Reconstruction, AR/VR Applications, Multi-Layer Perceptron, Efficient Generation<br />
<br />
Summary:<br />
Efficient and effective Neural Network models are crucial for a seamless user experience in AR/VR applications, especially for generating missing body parts in a virtual environment. Current models are computationally expensive and rely on long sequences of tracking inputs, leading to increased computation overhead and noise in longer temporal dependencies. In this study, a novel Multi-Layer Perceptron-based approach is proposed to improve 3D full-body generation performance while balancing computational costs and memory overhead. The method divides input sequences into smaller temporal windows and merges current motion with past context through latent representations. Experimental results show a significant improvement in generation accuracy compared to existing methods, with reduced resource requirements, making it suitable for resource-constrained devices. <br /> <div>
arXiv:2505.01802v1 Announce Type: new 
Abstract: To have a seamless user experience on immersive AR/VR applications, the importance of efficient and effective Neural Network (NN) models is undeniable, since missing body parts that cannot be captured by limited sensors should be generated using these models for a complete 3D full-body reconstruction in virtual environment. However, the state-of-the-art NN-models are typically computational expensive and they leverage longer sequences of sparse tracking inputs to generate full-body movements by capturing temporal context. Inevitably, longer sequences increase the computation overhead and introduce noise in longer temporal dependencies that adversely affect the generation performance. In this paper, we propose a novel Multi-Layer Perceptron (MLP)-based method that enhances the overall performance while balancing the computational cost and memory overhead for efficient 3D full-body generation. Precisely, we introduce a NN-mechanism that divides the longer sequence of inputs into smaller temporal windows. Later, the current motion is merged with the information from these windows through latent representations to utilize the past context for the generation. Our experiments demonstrate that generation accuracy of our method with this NN-mechanism is significantly improved compared to the state-of-the-art methods while greatly reducing computational costs and memory overhead, making our method suitable for resource-constrained devices.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Every Tree Is a Forest: Benchmarking Forest Types from Satellite Remote Sensing</title>
<link>https://arxiv.org/abs/2505.01805</link>
<guid>https://arxiv.org/abs/2505.01805</guid>
<content:encoded><![CDATA[
<div> forest types mapping, benchmark, multi-temporal satellite data, convolutional neural networks, transformer-based models

Summary:

The article introduces ForTy, a benchmark dataset for global forest types mapping using multi-temporal satellite data. It includes 200,000 time series image patches with Sentinel-2, Sentinel-1, climate, and elevation data, capturing monthly or seasonal variations. Per-pixel annotations differentiate between natural forest, planted forest, and tree crops. Unlike existing products that lump forests into one class, this benchmark provides distinct forest types classes. The dataset covers the globe and is evaluated using baseline models like CNNs and transformers. A new transformer-based model tailored for multi-modal satellite data outperforms these baselines. This work is essential for accurate forest mapping to aid conservation efforts and comply with regulations like the European Union Deforestation Regulation. 

<br /><br />Summary: <div>
arXiv:2505.01805v1 Announce Type: new 
Abstract: Developing accurate and reliable models for forest types mapping is critical to support efforts for halting deforestation and for biodiversity conservation (such as European Union Deforestation Regulation (EUDR)). This work introduces ForTy, a benchmark for global-scale FORest TYpes mapping using multi-temporal satellite data1. The benchmark comprises 200,000 time series of image patches, each consisting of Sentinel-2, Sentinel-1, climate, and elevation data. Each time series captures variations at monthly or seasonal cadence. Per-pixel annotations, including forest types and other land use classes, support image segmentation tasks. Unlike most existing land use products that often categorize all forest areas into a single class, our benchmark differentiates between three forest types classes: natural forest, planted forest, and tree crops. By leveraging multiple public data sources, we achieve global coverage with this benchmark. We evaluate the forest types dataset using several baseline models, including convolution neural networks and transformer-based models. Additionally, we propose a novel transformer-based model specifically designed to handle multi-modal, multi-temporal satellite data for forest types mapping. Our experimental results demonstrate that the proposed model surpasses the baseline models in performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment</title>
<link>https://arxiv.org/abs/2505.01809</link>
<guid>https://arxiv.org/abs/2505.01809</guid>
<content:encoded><![CDATA[
<div> 3D weakly-supervised visual grounding, oriented 3D boxes, point clouds, natural language descriptions, category-level ambiguity

Summary: 
The article introduces a novel weakly-supervised visual grounding approach for localizing 3D boxes in point clouds based on natural language descriptions. The method addresses two main challenges: category-level ambiguity and instance-level complexity. It differentiates between categories and instances by leveraging category knowledge from a pre-trained detector and utilizing spatial relationship descriptions from language queries. This enhances category awareness and enables clear differentiation among objects, leading to improved performance on benchmarks. The approach outperforms previous methods on Nr3D, Sr3D, and ScanRef datasets. This advancement in weakly-supervised visual grounding showcases the potential for accurately identifying target-category objects and distinguishing instances within the same category. <div>
arXiv:2505.01809v1 Announce Type: new 
Abstract: The 3D weakly-supervised visual grounding task aims to localize oriented 3D boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. This setting presents two primary challenges: category-level ambiguity and instance-level complexity. Category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. Instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. To address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. In the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. In the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. These designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. Compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach</title>
<link>https://arxiv.org/abs/2505.01823</link>
<guid>https://arxiv.org/abs/2505.01823</guid>
<content:encoded><![CDATA[
<div> Generative models, crop disease images, synthetic samples, computational benchmarking, text-to-image approach <br />
Summary: <br />
This research explores a multi-modal text-to-image approach for generating synthetic crop disease images using Stable Diffusion models. Three variants were trained and fine-tuned to enhance generalization, with SD3.5M performing the best. It showed efficient generation of 500 synthetic images from just 36 in-field samples in 1.5 hours, with low memory usage, power consumption, and total energy use during the inference task. The average memory usage was 18 GB, power consumption 180 W, and total energy use of 1.02 kWh/500 images (0.002 kWh per image). These results highlight the effectiveness of SD3.5M for generating crop disease data in an efficient and timely manner. <br /> <div>
arXiv:2505.01823v1 Announce Type: new 
Abstract: Collecting large-scale crop disease images in the field is labor-intensive and time-consuming. Generative models (GMs) offer an alternative by creating synthetic samples that resemble real-world images. However, existing research primarily relies on Generative Adversarial Networks (GANs)-based image-to-image translation and lack a comprehensive analysis of computational requirements in agriculture. Therefore, this research explores a multi-modal text-to-image approach for generating synthetic crop disease images and is the first to provide computational benchmarking in this context. We trained three Stable Diffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and fine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning techniques to enhance generalization. SD3.5M outperformed the others, with an average memory usage of 18 GB, power consumption of 180 W, and total energy use of 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results demonstrate SD3.5M's ability to generate 500 synthetic images from just 36 in-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease data generation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CVVNet: A Cross-Vertical-View Network for Gait Recognition</title>
<link>https://arxiv.org/abs/2505.01837</link>
<guid>https://arxiv.org/abs/2505.01837</guid>
<content:encoded><![CDATA[
<div> CVVNet, Gait recognition, cross-vertical-view scenarios, frequency aggregation architecture, multi-frequency feature extraction<br />
<br />
Summary:<br />
Gait recognition is a valuable tool for contact-free person identification, but existing methods struggle in scenarios where surveillance angles vary significantly in elevation. CVVNet is introduced as a solution, utilizing a High-Low Frequency Extraction module for multi-frequency feature extraction from input silhouettes. The Dynamic Gated Aggregation mechanism adjusts the fusion ratio of high and low-frequency features, while the Multi-Scale Attention Gated Aggregation module enhances recognition robustness across different vertical views. Experimental results demonstrate that CVVNet achieves state-of-the-art performance, outperforming existing methods on datasets like DroneGait and Gait3D, showcasing an 8.6% improvement and 2% improvement respectively. <div>
arXiv:2505.01837v1 Announce Type: new 
Abstract: Gait recognition enables contact-free, long-range person identification that is robust to clothing variations and non-cooperative scenarios. While existing methods perform well in controlled indoor environments, they struggle with cross-vertical view scenarios, where surveillance angles vary significantly in elevation. Our experiments show up to 60\% accuracy degradation in low-to-high vertical view settings due to severe deformations and self-occlusions of key anatomical features. Current CNN and self-attention-based methods fail to effectively handle these challenges, due to their reliance on single-scale convolutions or simplistic attention mechanisms that lack effective multi-frequency feature integration. To tackle this challenge, we propose CVVNet (Cross-Vertical-View Network), a frequency aggregation architecture specifically designed for robust cross-vertical-view gait recognition. CVVNet employs a High-Low Frequency Extraction module (HLFE) that adopts parallel multi-scale convolution/max-pooling path and self-attention path as high- and low-frequency mixers for effective multi-frequency feature extraction from input silhouettes. We also introduce the Dynamic Gated Aggregation (DGA) mechanism to adaptively adjust the fusion ratio of high- and low-frequency features. The integration of our core Multi-Scale Attention Gated Aggregation (MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions from view changes, significantly improving the recognition robustness across different vertical views. Experimental results show that our CVVNet achieves state-of-the-art performance, with $8.6\%$ improvement on DroneGait and $2\%$ on Gait3D compared with the best existing methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization</title>
<link>https://arxiv.org/abs/2505.01838</link>
<guid>https://arxiv.org/abs/2505.01838</guid>
<content:encoded><![CDATA[
<div> Large-scale dataset, 3D vision, human-centric tasks, multi-view human action sequences, MVHumanNet++

Summary:
The article introduces MVHumanNet++, a dataset focusing on multi-view human action sequences of 4,500 human identities with diverse everyday clothing, totaling 9,000 outfits, 60,000 motion sequences, and 645 million frames. The dataset includes annotations like human masks, camera parameters, 2D and 3D keypoints, and textual descriptions. Newly processed normal maps and depth maps enhance its utility for advanced research. Pilot studies demonstrate performance improvements in various visual tasks, showcasing the dataset's potential. MVHumanNet++ aims to advance 3D human-centric research with its scale, being the current largest of its kind. The dataset is publicly available to foster innovations in this domain. Find more information at https://kevinlee09.github.io/research/MVHumanNet++/.

<br /><br />Summary: <div>
arXiv:2505.01838v1 Announce Type: new 
Abstract: In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while significant progress has been achieved in object-centric tasks through large-scale datasets like Objaverse and MVImgNet, human-centric tasks have seen limited advancement, largely due to the absence of a comparable large-scale human dataset. To bridge this gap, we present MVHumanNet++, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using multi-view human capture systems, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. Additionally, the proposed MVHumanNet++ dataset is enhanced with newly processed normal maps and depth maps, significantly expanding its applicability and utility for advanced human-centric research. To explore the potential of our proposed MVHumanNet++ dataset in various 2D and 3D visual tasks, we conducted several pilot studies to demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet++. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet++ dataset with annotations will foster further innovations in the domain of 3D human-centric tasks at scale. MVHumanNet++ is publicly available at https://kevinlee09.github.io/research/MVHumanNet++/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Group-Level Fairness Disparities in Federated Visual Language Models</title>
<link>https://arxiv.org/abs/2505.01851</link>
<guid>https://arxiv.org/abs/2505.01851</guid>
<content:encoded><![CDATA[
<div> fairness, visual language models, federated learning, demographic bias, multimodal tasks 

Summary:
Cross-Layer Demographic Fair Prompting (CDFP) adjusts biased embeddings through counterfactual regularization. Demographic Subspace Orthogonal Projection (DSOP) removes demographic bias in image representations by mapping fair prompt text to group subspaces. Fair-aware Prompt Fusion (FPF) dynamically balances client contributions based on performance and fairness metrics. Evaluations show a 45% reduction in demographic disparity compared to standard FL approaches while maintaining task performance within 6% of state-of-the-art results. FVL-FP efficiently addresses challenges of non-IID data distributions in federated settings with minimal computational overhead, providing significant fairness benefits. The framework, FVL-FP, offers a parameter-efficient solution to ensuring equitable performance across demographic groups in privacy-preserving multimodal systems.<br /><br />Summary: <div>
arXiv:2505.01851v1 Announce Type: new 
Abstract: Visual language models (VLMs) have shown remarkable capabilities in multimodal tasks but face challenges in maintaining fairness across demographic groups, particularly when deployed in federated learning (FL) environments. This paper addresses the critical issue of group fairness in federated VLMs by introducing FVL-FP, a novel framework that combines FL with fair prompt tuning techniques. We focus on mitigating demographic biases while preserving model performance through three innovative components: (1) Cross-Layer Demographic Fair Prompting (CDFP), which adjusts potentially biased embeddings through counterfactual regularization; (2) Demographic Subspace Orthogonal Projection (DSOP), which removes demographic bias in image representations by mapping fair prompt text to group subspaces; and (3) Fair-aware Prompt Fusion (FPF), which dynamically balances client contributions based on both performance and fairness metrics. Extensive evaluations across four benchmark datasets demonstrate that our approach reduces demographic disparity by an average of 45\% compared to standard FL approaches, while maintaining task performance within 6\% of state-of-the-art results. FVL-FP effectively addresses the challenges of non-IID data distributions in federated settings and introduces minimal computational overhead while providing significant fairness benefits. Our work presents a parameter-efficient solution to the critical challenge of ensuring equitable performance across demographic groups in privacy-preserving multimodal systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion</title>
<link>https://arxiv.org/abs/2505.01857</link>
<guid>https://arxiv.org/abs/2505.01857</guid>
<content:encoded><![CDATA[
<div> Keywords: driving scene reconstruction, multi-modal information, conditional diffusion model, semantic fusion attention, FGM loss

Summary:
DualDiff is a new approach for driving scene reconstruction that improves accuracy and fidelity by leveraging multi-modal information. It introduces Occupancy Ray Sampling (ORS) and a numerical driving scene representation for better foreground and background control. The model also includes a Semantic Fusion Attention (SFA) mechanism to align and fuse features across modalities, enhancing cross-modal information integration. Additionally, a foreground-aware masked (FGM) loss is used to improve the generation of tiny objects in the scene. DualDiff outperforms existing methods in FID score and achieves better results in downstream tasks such as BEV segmentation and 3D object detection. Overall, DualDiff offers a comprehensive solution for high-quality driving scene generation by capturing the complexity of the scene and effectively integrating multi-modal information.<br /><br />Summary: <div>
arXiv:2505.01857v1 Announce Type: new 
Abstract: Accurate and high-fidelity driving scene reconstruction relies on fully leveraging scene information as conditioning. However, existing approaches, which primarily use 3D bounding boxes and binary maps for foreground and background control, fall short in capturing the complexity of the scene and integrating multi-modal information. In this paper, we propose DualDiff, a dual-branch conditional diffusion model designed to enhance multi-view driving scene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3D representation, alongside numerical driving scene representation, for comprehensive foreground and background control. To improve cross-modal information integration, we propose a Semantic Fusion Attention (SFA) mechanism that aligns and fuses features across modalities. Furthermore, we design a foreground-aware masked (FGM) loss to enhance the generation of tiny objects. DualDiff achieves state-of-the-art performance in FID score, as well as consistently better results in downstream BEV segmentation and 3D object detection tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual enhancement and 3D representation for underwater scenes: a review</title>
<link>https://arxiv.org/abs/2505.01869</link>
<guid>https://arxiv.org/abs/2505.01869</guid>
<content:encoded><![CDATA[
<div> keywords: Underwater visual enhancement, underwater 3D reconstruction, computer vision, AI-based tasks, benchmark datasets

Summary:
Underwater visual enhancement and underwater 3D reconstruction present challenges in computer vision due to complex imaging conditions in aquatic environments. The paper provides a comprehensive review, covering physical models and highlighting challenges unique to underwater scenarios. Advanced methods for visual enhancement and 3D reconstruction tailored for underwater use are surveyed, including non-learning techniques and data-driven approaches like Neural Radiance Fields and 3D Gaussian Splatting. The effectiveness of these methods in handling underwater distortions is discussed. Quantitative and qualitative evaluations of state-of-the-art algorithms on benchmark datasets are presented. The review concludes by identifying key research directions for future advancements in underwater vision. 

Summary: <div>
arXiv:2505.01869v1 Announce Type: new 
Abstract: Underwater visual enhancement (UVE) and underwater 3D reconstruction pose significant challenges in
  computer vision and AI-based tasks due to complex imaging conditions in aquatic environments. Despite
  the development of numerous enhancement algorithms, a comprehensive and systematic review covering both
  UVE and underwater 3D reconstruction remains absent. To advance research in these areas, we present an
  in-depth review from multiple perspectives. First, we introduce the fundamental physical models, highlighting the
  peculiarities that challenge conventional techniques. We survey advanced methods for visual enhancement and
  3D reconstruction specifically designed for underwater scenarios. The paper assesses various approaches from
  non-learning methods to advanced data-driven techniques, including Neural Radiance Fields and 3D Gaussian
  Splatting, discussing their effectiveness in handling underwater distortions. Finally, we conduct both quantitative
  and qualitative evaluations of state-of-the-art UVE and underwater 3D reconstruction algorithms across multiple
  benchmark datasets. Finally, we highlight key research directions for future advancements in underwater vision.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications</title>
<link>https://arxiv.org/abs/2505.01881</link>
<guid>https://arxiv.org/abs/2505.01881</guid>
<content:encoded><![CDATA[
<div> Keywords: navigation, sensor fusion, vision-language models, autonomous systems, semantic reasoning

Summary: 
PhysNav-DG is a new framework that combines sensor fusion with vision-language models to enable robust navigation in various environments. The architecture integrates classical sensor fusion with semantic insights from models like LLaMA 3.2 11B and BLIP-2 to predict navigation actions and generate detailed explanations. A modified Adaptive Kalman Filter adjusts noise parameters based on environmental context. The MD-NEX Benchmark dataset is introduced to evaluate the approach across indoor navigation, autonomous driving, and social navigation tasks with human-validated explanations. Experimental results show a navigation success rate improvement of over 20% and high efficiency. PhysNav-DG bridges high-level semantic reasoning and geometric planning to create safer and more trustworthy autonomous systems. 

<br /><br />Summary: <div>
arXiv:2505.01881v1 Announce Type: new 
Abstract: Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMAWRNet: Multiple Adverse Weather Removal via a Unified Quaternion Neural Architecture</title>
<link>https://arxiv.org/abs/2505.01882</link>
<guid>https://arxiv.org/abs/2505.01882</guid>
<content:encoded><![CDATA[
<div> quaternion neural architecture, adverse weather removal, deep-learning methods, multiple weather artifacts, object detection<br />
Summary:<br />
The article introduces CMAWRNet, a novel solution for removing multiple adverse weather conditions in images using a quaternion neural architecture. Existing methods struggle with combinations of degradations like haze and rain, but CMAWRNet offers a unified approach. It includes a texture-structure decomposition block, an encoder-decoder quaternion transformer architecture, and an attentive fusion block with low-light correction. Additionally, a quaternion similarity loss function is used to preserve color information effectively. The proposed method outperforms current state-of-the-art approaches in removing multiple weather artifacts, as validated through quantitative and qualitative evaluations on benchmarking datasets and real-world images. The study also demonstrates the improved performance of downstream applications such as object detection. This innovative approach of decomposition is a significant advancement in the field of universal weather removal. <br /><br /> <div>
arXiv:2505.01882v1 Announce Type: new 
Abstract: Images used in real-world applications such as image or video retrieval, outdoor surveillance, and autonomous driving suffer from poor weather conditions. When designing robust computer vision systems, removing adverse weather such as haze, rain, and snow is a significant problem. Recently, deep-learning methods offered a solution for a single type of degradation. Current state-of-the-art universal methods struggle with combinations of degradations, such as haze and rain-streak. Few algorithms have been developed that perform well when presented with images containing multiple adverse weather conditions. This work focuses on developing an efficient solution for multiple adverse weather removal using a unified quaternion neural architecture called CMAWRNet. It is based on a novel texture-structure decomposition block, a novel lightweight encoder-decoder quaternion transformer architecture, and an attentive fusion block with low-light correction. We also introduce a quaternion similarity loss function to preserve color information better. The quantitative and qualitative evaluation of the current state-of-the-art benchmarking datasets and real-world images shows the performance advantages of the proposed CMAWRNet compared to other state-of-the-art weather removal approaches dealing with multiple weather artifacts. Extensive computer simulations validate that CMAWRNet improves the performance of downstream applications such as object detection. This is the first time the decomposition approach has been applied to the universal weather removal task.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Score Distilling Sampling for 3D Editing and Generation</title>
<link>https://arxiv.org/abs/2505.01888</link>
<guid>https://arxiv.org/abs/2505.01888</guid>
<content:encoded><![CDATA[
<div> Keywords: Score Distillation Sampling, 3D generation, text-to-3D, Unified Distillation Sampling, editing tasks

Summary:
Unified Distillation Sampling (UDS) is introduced as a method that integrates both generation and editing capabilities in 3D asset creation. By refining gradient terms used in existing methods like Score Distillation Sampling (SDS), UDS bridges the gap between generating new 3D assets and editing existing ones effectively. The underlying gradient terms for generation and editing tasks are unified in UDS, allowing for superior performance in both tasks. Experimental results show that UDS outperforms baseline methods in generating detailed 3D assets while also excelling in editing tasks. This approach significantly improves the versatility and efficiency of 3D asset creation processes. The code for UDS is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2505.01888v1 Announce Type: new 
Abstract: Score Distillation Sampling (SDS) has emerged as a prominent method for text-to-3D generation by leveraging the strengths of 2D diffusion models. However, SDS is limited to generation tasks and lacks the capability to edit existing 3D assets. Conversely, variants of SDS that introduce editing capabilities often can not generate new 3D assets effectively. In this work, we observe that the processes of generation and editing within SDS and its variants have unified underlying gradient terms. Building on this insight, we propose Unified Distillation Sampling (UDS), a method that seamlessly integrates both the generation and editing of 3D assets. Essentially, UDS refines the gradient terms used in vanilla SDS methods, unifying them to support both tasks. Extensive experiments demonstrate that UDS not only outperforms baseline methods in generating 3D assets with richer details but also excels in editing tasks, thereby bridging the gap between 3D generation and editing. The code is available on: https://github.com/xingy038/UDS.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.01928</link>
<guid>https://arxiv.org/abs/2505.01928</guid>
<content:encoded><![CDATA[
<div> Keywords: GenSync, multi-identity lip-synced video synthesis, 3D Gaussian Splatting, Disentanglement Module, training efficiency

Summary:<br />
GenSync is a new framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike existing methods, it learns a single network for synthesizing lip-synced videos for multiple speakers, reducing computational overhead and achieving 6.8x faster training. The incorporation of a Disentanglement Module enables efficient separation of identity-specific features from audio representations, improving multi-identity video synthesis. The approach maintains high lip-sync accuracy and visual quality while enhancing training efficiency. GenSync's unified network approach streamlines the process and enhances performance, making it a promising advancement in the field of video synthesis.<br /> <div>
arXiv:2505.01928v1 Announce Type: new 
Abstract: We introduce GenSync, a novel framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that require training a new model for each identity , GenSync learns a unified network that synthesizes lip-synced videos for multiple speakers. By incorporating a Disentanglement Module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. This design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GauS-SLAM: Dense RGB-D SLAM with Gaussian Surfels</title>
<link>https://arxiv.org/abs/2505.01934</link>
<guid>https://arxiv.org/abs/2505.01934</guid>
<content:encoded><![CDATA[
<div> GauS-SLAM, RGB-D SLAM, 2D Gaussian surfels, geometry distortion, depth modeling, Surface-aware Depth Rendering<br />
<br />
Summary:<br />
The article introduces GauS-SLAM, a dense RGB-D SLAM system utilizing 2D Gaussian surfels for robust tracking and high-fidelity mapping. It addresses the issue of geometry distortion in Gaussian-based scene representations caused by depth modeling and surface interference during depth blending. GauS-SLAM implements an incremental reconstruction strategy and a Surface-aware Depth Rendering mechanism to improve geometry accuracy and multi-view consistency. The system's local map design dynamically isolates visible surfaces during tracking, enhancing alignment and computational efficiency. Extensive experiments show that GauS-SLAM outperforms similar methods in tracking precision and rendering fidelity, making it a promising solution for dense RGB-D SLAM applications. The project page for GauS-SLAM can be accessed at https://gaus-slam.github.io.<br /> <div>
arXiv:2505.01934v1 Announce Type: new 
Abstract: We propose GauS-SLAM, a dense RGB-D SLAM system that leverages 2D Gaussian surfels to achieve robust tracking and high-fidelity mapping. Our investigations reveal that Gaussian-based scene representations exhibit geometry distortion under novel viewpoints, which significantly degrades the accuracy of Gaussian-based tracking methods. These geometry inconsistencies arise primarily from the depth modeling of Gaussian primitives and the mutual interference between surfaces during the depth blending. To address these, we propose a 2D Gaussian-based incremental reconstruction strategy coupled with a Surface-aware Depth Rendering mechanism, which significantly enhances geometry accuracy and multi-view consistency. Additionally, the proposed local map design dynamically isolates visible surfaces during tracking, mitigating misalignment caused by occluded regions in global maps while maintaining computational efficiency with increasing Gaussian density. Extensive experiments across multiple datasets demonstrate that GauS-SLAM outperforms comparable methods, delivering superior tracking precision and rendering fidelity. The project page will be made available at https://gaus-slam.github.io.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder</title>
<link>https://arxiv.org/abs/2505.01938</link>
<guid>https://arxiv.org/abs/2505.01938</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, compression, HybridGS, point cloud data encoding, rate control <br />
Summary: 
HybridGS is a new compression framework for 3D Gaussian Splatting (3DGS) that combines compact generation with standardized point cloud data encoding. It introduces dual-channel sparse representation for primitive position and feature bit depth supervision, followed by a canonical point cloud encoder for further compression. The framework includes a rate control scheme for data compression. While currently lacking quality improvement modules, HybridGS demonstrates comparable reconstruction performance to state-of-the-art methods, with significantly faster encoding and decoding speeds. The code for HybridGS is available on GitHub for public use. <br /> <div>
arXiv:2505.01938v1 Announce Type: new 
Abstract: Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on producing compact 3DGS representation via implicit data embedding. They have long coding times and highly customized data format, making it difficult for widespread deployment. This paper presents a new 3DGS compression framework called HybridGS, which takes advantage of both compact generation and standardized point cloud data encoding. HybridGS first generates compact and explicit 3DGS data. A dual-channel sparse representation is introduced to supervise the primitive position and feature bit depth. It then utilizes a canonical point cloud encoder to perform further data compression and form standard output bitstreams. A simple and effective rate control scheme is proposed to pivot the interpretable data compression scheme. At the current stage, HybridGS does not include any modules aimed at improving 3DGS quality during generation. But experiment results show that it still provides comparable reconstruction performance against state-of-the-art methods, with evidently higher encoding and decoding speed. The code is publicly available at https://github.com/Qi-Yangsjtu/HybridGS.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment Any RGB-Thermal Model with Language-aided Distillation</title>
<link>https://arxiv.org/abs/2505.01950</link>
<guid>https://arxiv.org/abs/2505.01950</guid>
<content:encoded><![CDATA[
<div> Keywords: SAM, RGB-thermal, semantic segmentation, LoRA layers, Cross-Modal Knowledge Distillation

Summary:<br />
The paper introduces the SARTM framework, a modified version of the Segment Anything Model (SAM) tailored for RGB-thermal (RGB-T) semantic segmentation. SARTM incorporates extra LoRA layers to preserve SAM's strong generalization capabilities and introduces language guidance for training. A Cross-Modal Knowledge Distillation module is utilized to address cross-modal inconsistencies, facilitating modality adaptation while maintaining generalization. The framework enhances segmentation performance by adjusting SAM's segmentation head and incorporating an auxiliary semantic segmentation head for multi-scale feature fusion. Extensive experiments on three RGBT semantic segmentation benchmarks show that SARTM outperforms state-of-the-art approaches across various conditions, both quantitatively and qualitatively.<br />
Summary: <div>
arXiv:2505.01950v1 Announce Type: new 
Abstract: The recent Segment Anything Model (SAM) demonstrates strong instance segmentation performance across various downstream tasks. However, SAM is trained solely on RGB data, limiting its direct applicability to RGB-thermal (RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low light and overexposure, we propose a novel framework, SARTM, which customizes the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash the potential of SAM while introduce semantic understanding modules for RGB-T data pairs. Specifically, our framework first involves fine tuning the original SAM by adding extra LoRA layers, aiming at preserving SAM's strong generalization and segmentation capabilities for downstream tasks. Secondly, we introduce language information as guidance for training our SARTM. To address cross-modal inconsistencies, we introduce a Cross-Modal Knowledge Distillation(CMKD) module that effectively achieves modality adaptation while maintaining its generalization capabilities. This semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. Furthermore, we enhance the segmentation performance by adjusting the segmentation head of SAM and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. Extensive experiments are conducted across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900, and FMB. Both quantitative and qualitative results consistently demonstrate that the proposed SARTM significantly outperforms state-of-the-art approaches across a variety of conditions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.01958</link>
<guid>https://arxiv.org/abs/2505.01958</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision-Language Models, visual object hallucination, mitigation methods, problematic components, hallucination benchmarks 

Summary:
Large Vision-Language Models (LVLMs) have shown great potential in multimodal tasks but often suffer from visual object hallucination, where inaccurate information is generated based on query inputs. This can lead to misinformation and concerns about reliability. This paper analyzes the components of LVLMs  language model, vision backbone, and projector  to identify sources of error and proposes mitigation methods for each. The authors introduce two new hallucination benchmarks, QA-VisualGenome and QA-FB15k, to evaluate attribute, relation, and cognition-based hallucinations. By understanding and addressing the underlying causes of visual hallucinations in LVLMs, this research aims to improve the accuracy and reliability of these models in multimodal tasks.<br /><br />Summary: <div>
arXiv:2505.01958v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.01969</link>
<guid>https://arxiv.org/abs/2505.01969</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D anomaly detection, multi-category, geometry-aware information, masked attention, reconstruction

Summary:
The paper introduces a novel unified model, MC3D-AD, for Multi-Category 3D Anomaly Detection. It addresses the limitations of existing methods by incorporating both local and global geometry-aware information to reconstruct normal representations of all categories. The model utilizes an adaptive geometry-aware masked attention module to extract geometry variation information for robust and generalized feature learning across different categories. A local geometry-aware encoder and a global query decoder are introduced to encode group-level feature tokens and improve the decoding process, respectively. Evaluation on Real3D-AD and Anomaly-ShapeNet datasets shows superior performance compared to current state-of-the-art single-category methods, with significant improvements in object-level AUROC. The proposed model achieves a 3.1% improvement in Real3D-AD and a 9.3% improvement in Anomaly-ShapeNet. The source code will be released upon acceptance. 

<br /><br />Summary: <div>
arXiv:2505.01969v1 Announce Type: new 
Abstract: 3D Anomaly Detection (AD) is a promising means of controlling the quality of manufactured products. However, existing methods typically require carefully training a task-specific model for each category independently, leading to high cost, low efficiency, and weak generalization. Therefore, this paper presents a novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. First, to learn robust and generalized features of different categories, we propose an adaptive geometry-aware masked attention module that extracts geometry variation information to guide mask attention. Then, we introduce a local geometry-aware encoder reinforced by the improved mask attention to encode group-level feature tokens. Finally, we design a global query decoder that utilizes point cloud position embeddings to improve the decoding process and reconstruction ability. This leads to local and global geometry-aware reconstructed feature tokens for the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and Anomaly-ShapeNet datasets, and exhibits significant superiority over current state-of-the-art single-category methods, achieving 3.1\% and 9.3\% improvement in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The source code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques</title>
<link>https://arxiv.org/abs/2505.01973</link>
<guid>https://arxiv.org/abs/2505.01973</guid>
<content:encoded><![CDATA[
<div> Keywords: distracted driving, machine learning, deep learning, multimodal, driver behavior

Summary: 
This systematic review evaluates studies from 2019 to 2024 on distracted driving detection using machine learning and deep learning techniques. Visual-only models like CNNs are prevalent but lack generalizability in real-world scenarios. Sensor-based and physiological models capture internal states and vehicle dynamics effectively. Emerging modalities like auditory sensing and RF methods offer privacy-aware alternatives. Multimodal architectures, combining visual, physiological, and vehicular data, outperform unimodal models in robustness and scalability. Future research should focus on lightweight, deployable multimodal frameworks, personalized baselines, and cross-modality benchmarks for reliable ADAS and road safety interventions. 

<br /><br />Summary: <div>
arXiv:2505.01973v1 Announce Type: new 
Abstract: Distracted driving continues to be a significant cause of road traffic injuries and fatalities worldwide, even with advancements in driver monitoring technologies. Recent developments in machine learning (ML) and deep learning (DL) have primarily focused on visual data to detect distraction, often neglecting the complex, multimodal nature of driver behavior. This systematic review assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ML/DL techniques for distracted driving detection across visual, sensor-based, multimodal, and emerging modalities. The review highlights a significant prevalence of visual-only models, particularly convolutional neural networks (CNNs) and temporal architectures, which achieve high accuracy but show limited generalizability in real-world scenarios. Sensor-based and physiological models provide complementary strengths by capturing internal states and vehicle dynamics, while emerging techniques, such as auditory sensing and radio frequency (RF) methods, offer privacy-aware alternatives. Multimodal architecture consistently surpasses unimodal baselines, demonstrating enhanced robustness, context awareness, and scalability by integrating diverse data streams. These findings emphasize the need to move beyond visual-only approaches and adopt multimodal systems that combine visual, physiological, and vehicular cues while keeping in checking the need to balance computational requirements. Future research should focus on developing lightweight, deployable multimodal frameworks, incorporating personalized baselines, and establishing cross-modality benchmarks to ensure real-world reliability in advanced driver assistance systems (ADAS) and road safety interventions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation</title>
<link>https://arxiv.org/abs/2505.01984</link>
<guid>https://arxiv.org/abs/2505.01984</guid>
<content:encoded><![CDATA[
<div> Keywords: Whole Slide Images, Lifelong Learning, ADaFGrad, Cancer Diagnosis, Computational Tool 

Summary: 
ADaFGrad is introduced as a method to enhance lifelong learning for whole-slide image (WSI) analysis. By leveraging pathology vision-language foundation models, it enables interaction between a slide's regional tissue features and text-based prototype buffer, with a gradient-distillation mechanism to mimic logit gradients for continual learning. Testing on six TCGA datasets shows ADaFGrad outperforming state-of-the-art WSI-specific and conventional continual learning methods, achieving up to 5.068% higher accuracy and exhibiting minimal forgetting. It surpasses baselines by up to 40.084% in accuracy, showcasing the effectiveness of its proposed modules.

<br /><br />Summary: <div>
arXiv:2505.01984v1 Announce Type: new 
Abstract: Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosis and prognosis, as they provide tissue details at the cellular level. However, the rapid growth of computational tasks involving WSIs poses significant challenges. Given that WSIs are gigapixels in size, they present difficulties in terms of storage, processing, and model training. Therefore, it is essential to develop lifelong learning approaches for WSI analysis. In scenarios where slides are distributed across multiple institutes, we aim to leverage them to develop a unified online model as a computational tool for cancer diagnosis in clinical and hospital settings. In this study, we introduce ADaFGrad, a method designed to enhance lifelong learning for whole-slide image (WSI) analysis. First, we leverage pathology vision-language foundation models to develop a framework that enables interaction between a slide's regional tissue features and a predefined text-based prototype buffer. Additionally, we propose a gradient-distillation mechanism that mimics the gradient of a logit with respect to the classification-head parameters across past and current iterations in a continual-learning setting. We construct a sequence of six TCGA datasets for training and evaluation. Experimental results show that ADaFGrad outperforms both state-of-the-art WSI-specific and conventional continual-learning methods after only a few training epochs, exceeding them by up to +5.068% in the class-incremental learning scenario while exhibiting the least forgetting (i.e., retaining the most knowledge from previous tasks). Moreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy, further demonstrating the effectiveness of the proposed modules.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Drug classification based on X-ray spectroscopy combined with machine learning</title>
<link>https://arxiv.org/abs/2505.01986</link>
<guid>https://arxiv.org/abs/2505.01986</guid>
<content:encoded><![CDATA[
<div> X-ray absorption spectroscopy, CNN, SVM, PSO, drug detection<br />
<br />
Summary: <br />
The study focuses on the development of a classification model using X-ray absorption spectroscopy combined with CNN, SVM, and PSO for drug detection. Traditional methods have complex operations, but this model offers ease of use, penetrative observation, and strong substance differentiation capabilities. The experiments involved 14 chemical reagent samples, with the model achieving a high classification accuracy of 99.14%. By extracting features with CNN and training the SVM model, the system showed faster execution speed and improved efficiency. The combination of these techniques provides a rapid, accurate, and reliable method for drug classification and identification, promising widespread application in the field of drug detection. <br /> <div>
arXiv:2505.01986v1 Announce Type: new 
Abstract: The proliferation of new types of drugs necessitates the urgent development of faster and more accurate detection methods. Traditional detection methods have high requirements for instruments and environments, making the operation complex. X-ray absorption spectroscopy, a non-destructive detection technique, offers advantages such as ease of operation, penetrative observation, and strong substance differentiation capabilities, making it well-suited for application in the field of drug detection and identification. In this study, we constructed a classification model using Convolutional Neural Networks (CNN), Support Vector Machines (SVM), and Particle Swarm Optimization (PSO) to classify and identify drugs based on their X-ray spectral profiles. In the experiments, we selected 14 chemical reagents with chemical formulas similar to drugs as samples. We utilized CNN to extract features from the spectral data of these 14 chemical reagents and used the extracted features to train an SVM model. We also utilized PSO to optimize two critical initial parameters of the SVM. The experimental results demonstrate that this model achieved higher classification accuracy compared to two other common methods, with a prediction accuracy of 99.14%. Additionally, the model exhibited fast execution speed, mitigating the drawback of a drastic increase in running time and efficiency reduction that may result from the direct fusion of PSO and SVM. Therefore, the combined approach of X-ray absorption spectroscopy with CNN, PSO, and SVM provides a rapid, highly accurate, and reliable classification and identification method for the field of drug detection, holding promising prospects for widespread application.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields</title>
<link>https://arxiv.org/abs/2505.02005</link>
<guid>https://arxiv.org/abs/2505.02005</guid>
<content:encoded><![CDATA[
arXiv:2505.02005v1 Announce Type: new 
Abstract: Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Noise Calculation in Deep Learning-based MRI Reconstructions</title>
<link>https://arxiv.org/abs/2505.02007</link>
<guid>https://arxiv.org/abs/2505.02007</guid>
<content:encoded><![CDATA[
arXiv:2505.02007v1 Announce Type: new 
Abstract: Accelerated MRI reconstruction involves solving an ill-posed inverse problem where noise in acquired data propagates to the reconstructed images. Noise analyses are central to MRI reconstruction for providing an explicit measure of solution fidelity and for guiding the design and deployment of novel reconstruction methods. However, deep learning (DL)-based reconstruction methods have often overlooked noise propagation due to inherent analytical and computational challenges, despite its critical importance. This work proposes a theoretically grounded, memory-efficient technique to calculate voxel-wise variance for quantifying uncertainty due to acquisition noise in accelerated MRI reconstructions. Our approach approximates noise covariance using the DL network's Jacobian, which is intractable to calculate. To circumvent this, we derive an unbiased estimator for the diagonal of this covariance matrix (voxel-wise variance) and introduce a Jacobian sketching technique to efficiently implement it. We evaluate our method on knee and brain MRI datasets for both data- and physics-driven networks trained in supervised and unsupervised manners. Compared to empirical references obtained via Monte Carlo simulations, our technique achieves near-equivalent performance while reducing computational and memory demands by an order of magnitude or more. Furthermore, our method is robust across varying input noise levels, acceleration factors, and diverse undersampling schemes, highlighting its broad applicability. Our work reintroduces accurate and efficient noise analysis as a central tenet of reconstruction algorithms, holding promise to reshape how we evaluate and deploy DL-based MRI. Our code will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLLM-Enhanced Face Forgery Detection: A Vision-Language Fusion Solution</title>
<link>https://arxiv.org/abs/2505.02013</link>
<guid>https://arxiv.org/abs/2505.02013</guid>
<content:encoded><![CDATA[
arXiv:2505.02013v1 Announce Type: new 
Abstract: Reliable face forgery detection algorithms are crucial for countering the growing threat of deepfake-driven disinformation. Previous research has demonstrated the potential of Multimodal Large Language Models (MLLMs) in identifying manipulated faces. However, existing methods typically depend on either the Large Language Model (LLM) alone or an external detector to generate classification results, which often leads to sub-optimal integration of visual and textual modalities. In this paper, we propose VLF-FFD, a novel Vision-Language Fusion solution for MLLM-enhanced Face Forgery Detection. Our key contributions are twofold. First, we present EFF++, a frame-level, explainability-driven extension of the widely used FaceForensics++ (FF++) dataset. In EFF++, each manipulated video frame is paired with a textual annotation that describes both the forgery artifacts and the specific manipulation technique applied, enabling more effective and informative MLLM training. Second, we design a Vision-Language Fusion Network (VLF-Net) that promotes bidirectional interaction between visual and textual features, supported by a three-stage training pipeline to fully leverage its potential. VLF-FFD achieves state-of-the-art (SOTA) performance in both cross-dataset and intra-dataset evaluations, underscoring its exceptional effectiveness in face forgery detection.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM &amp; MLLM Complex Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2505.02018</link>
<guid>https://arxiv.org/abs/2505.02018</guid>
<content:encoded><![CDATA[
arXiv:2505.02018v1 Announce Type: new 
Abstract: Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark, dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of both language and multimodal models. RBench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both English and Chinese. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an Olympiad-level multi-disciplinary benchmark. We evaluate widely used models, including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available at here.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Birotation Solution for Relative Pose Problems</title>
<link>https://arxiv.org/abs/2505.02025</link>
<guid>https://arxiv.org/abs/2505.02025</guid>
<content:encoded><![CDATA[
arXiv:2505.02025v1 Announce Type: new 
Abstract: Relative pose estimation, a fundamental computer vision problem, has been extensively studied for decades. Existing methods either estimate and decompose the essential matrix or directly estimate the rotation and translation to obtain the solution. In this article, we break the mold by tackling this traditional problem with a novel birotation solution. We first introduce three basis transformations, each associated with a geometric metric to quantify the distance between the relative pose to be estimated and its corresponding basis transformation. Three energy functions, designed based on these metrics, are then minimized on the Riemannian manifold $\mathrm{SO(3)}$ by iteratively updating the two rotation matrices. The two rotation matrices and the basis transformation corresponding to the minimum energy are ultimately utilized to recover the relative pose. Extensive quantitative and qualitative evaluations across diverse relative pose estimation tasks demonstrate the superior performance of our proposed birotation solution. Source code, demo video, and datasets will be available at \href{https://mias.group/birotation-solution}{mias.group/birotation-solution} upon publication.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction</title>
<link>https://arxiv.org/abs/2505.02043</link>
<guid>https://arxiv.org/abs/2505.02043</guid>
<content:encoded><![CDATA[
arXiv:2505.02043v1 Announce Type: new 
Abstract: Recovering CAD models from point clouds, especially the sketch-extrusion process, can be seen as the process of rebuilding the topology and extrusion primitives. Previous methods utilize implicit fields for sketch representation, leading to shape reconstruction of curved edges. In this paper, we proposed a CAD reconstruction network that produces editable CAD models from input point clouds (Point2Primitive) by directly predicting every element of the extrusion primitives. Point2Primitive can directly detect and predict sketch curves (type and parameter) from point clouds based on an improved transformer. The sketch curve parameters are formulated as position queries and optimized in an autoregressive way, leading to high parameter accuracy. The topology is rebuilt by extrusion segmentation, and each extrusion parameter (sketch and extrusion operation) is recovered by combining the predicted curves and the computed extrusion operation. Extensive experiments demonstrate that our method is superior in primitive prediction accuracy and CAD reconstruction. The reconstructed shapes are of high geometrical fidelity.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A UNet Model for Accelerated Preprocessing of CRISM Hyperspectral Data for Mineral Identification on Mars</title>
<link>https://arxiv.org/abs/2505.02046</link>
<guid>https://arxiv.org/abs/2505.02046</guid>
<content:encoded><![CDATA[
arXiv:2505.02046v1 Announce Type: new 
Abstract: Accurate mineral identification on the Martian surface is critical for understanding the planet's geological history. This paper presents a UNet-based autoencoder model for efficient spectral preprocessing of CRISM MTRDR hyperspectral data, addressing the limitations of traditional methods that are computationally intensive and time-consuming. The proposed model automates key preprocessing steps, such as smoothing and continuum removal, while preserving essential mineral absorption features. Trained on augmented spectra from the MICA spectral library, the model introduces realistic variability to simulate MTRDR data conditions. By integrating this framework, preprocessing time for an 800x800 MTRDR scene is reduced from 1.5 hours to just 5 minutes on an NVIDIA T1600 GPU. The preprocessed spectra are subsequently classified using MICAnet, a deep learning model for Martian mineral identification. Evaluation on labeled CRISM TRDR data demonstrates that the proposed approach achieves competitive accuracy while significantly enhancing preprocessing efficiency. This work highlights the potential of the UNet-based preprocessing framework to improve the speed and reliability of mineral mapping on Mars.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regression s all you need for medical image translation</title>
<link>https://arxiv.org/abs/2505.02048</link>
<guid>https://arxiv.org/abs/2505.02048</guid>
<content:encoded><![CDATA[
arXiv:2505.02048v1 Announce Type: new 
Abstract: The acquisition of information-rich images within a limited time budget is crucial in medical imaging. Medical image translation (MIT) can help enhance and supplement existing datasets by generating synthetic images from acquired data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. In fact, the imitation of acquisition noise or content hallucination hinder clinical utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel 2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and regression paradigms to produce realistic or noise-free outputs. Furthermore, we propose Expectation-Approximation (ExpA) DM sampling, which draws inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. Through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and regression sampling yield similar results in practice. As such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. Building on these insights, we demonstrate that YODA outperforms several state-of-the-art GAN and DM methods. Notably, YODA-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. Our findings challenge the presumed advantages of DMs in MIT and pave the way for the practical application of MIT in medical imaging.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin</title>
<link>https://arxiv.org/abs/2505.02056</link>
<guid>https://arxiv.org/abs/2505.02056</guid>
<content:encoded><![CDATA[
arXiv:2505.02056v1 Announce Type: new 
Abstract: Adapting vision-language models (VLMs) to downstream tasks with pseudolabels has gained increasing attention. A major obstacle is that the pseudolabels generated by VLMs tend to be imbalanced, leading to inferior performance. While existing methods have explored various strategies to address this, the underlying causes of imbalance remain insufficiently investigated. To fill this gap, we delve into imbalanced pseudolabels and identify two primary contributing factors: concept mismatch and concept confusion. To mitigate these two issues, we propose a novel framework incorporating concept alignment and confusion-aware calibrated margin mechanisms. The core of our approach lies in enhancing underperforming classes and promoting balanced predictions across categories, thus mitigating imbalance. Extensive experiments on six benchmark datasets with three learning paradigms demonstrate that the proposed method effectively enhances the accuracy and balance of pseudolabels, achieving a relative improvement of 6.29% over the SoTA method. Our code is avaliable at https://anonymous.4open.science/r/CAP-C642/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming faces into video stories -- VideoFace2.0</title>
<link>https://arxiv.org/abs/2505.02060</link>
<guid>https://arxiv.org/abs/2505.02060</guid>
<content:encoded><![CDATA[
arXiv:2505.02060v1 Announce Type: new 
Abstract: Face detection and face recognition have been in the focus of vision community since the very beginnings. Inspired by the success of the original Videoface digitizer, a pioneering device that allowed users to capture video signals from any source, we have designed an advanced video analytics tool to efficiently create structured video stories, i.e. identity-based information catalogs. VideoFace2.0 is the name of the developed system for spatial and temporal localization of each unique face in the input video, i.e. face re-identification (ReID), which also allows their cataloging, characterization and creation of structured video outputs for later downstream tasks. Developed near real-time solution is primarily designed to be utilized in application scenarios involving TV production, media analysis, and as an efficient tool for creating large video datasets necessary for training machine learning (ML) models in challenging vision tasks such as lip reading and multimodal speech recognition. Conducted experiments confirm applicability of the proposed face ReID algorithm that is combining the concepts of face detection, face recognition and passive tracking-by-detection in order to achieve robust and efficient face ReID. The system is envisioned as a compact and modular extensions of the existing video production equipment. We hope that the presented work and shared code will stimulate further interest in development of similar, application specific video analysis tools, and lower the entry barrier for production of high-quality multi-modal ML datasets in the future.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video</title>
<link>https://arxiv.org/abs/2505.02064</link>
<guid>https://arxiv.org/abs/2505.02064</guid>
<content:encoded><![CDATA[
arXiv:2505.02064v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) increasingly excel at perception, understanding, and reasoning. However, current benchmarks inadequately evaluate their ability to perform these tasks continuously in dynamic, real-world environments. To bridge this gap, we introduce RTV-Bench, a fine-grained benchmark for MLLM real-time video analysis. RTV-Bench uses three key principles: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve with scene changes; (2) Hierarchical Question Structure, combining basic and advanced queries; and (3) Multi-dimensional Evaluation, assessing the ability of continuous perception, understanding, and reasoning. RTV-Bench contains 552 diverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated leading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline (Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5, InternLM-XComposer2.5-OmniLive) models. Experiment results show open-source real-time models largely outperform offline ones but still trail top proprietary models. Our analysis also reveals that larger model size or higher frame sampling rates do not significantly boost RTV-Bench performance, sometimes causing slight decreases. This underscores the need for better model architectures optimized for video stream processing and long sequences to advance real-time video analysis with MLLMs. Our benchmark toolkit is available at: https://github.com/LJungang/RTV-Bench.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning</title>
<link>https://arxiv.org/abs/2505.02071</link>
<guid>https://arxiv.org/abs/2505.02071</guid>
<content:encoded><![CDATA[
arXiv:2505.02071v1 Announce Type: new 
Abstract: We propose the Compact Clustering Attention (COCA) layer, an effective building block that introduces a hierarchical strategy for object-centric representation learning, while solving the unsupervised object discovery task on single images. COCA is an attention-based clustering module capable of extracting object-centric representations from multi-object scenes, when cascaded into a bottom-up hierarchical network architecture, referred to as COCA-Net. At its core, COCA utilizes a novel clustering algorithm that leverages the physical concept of compactness, to highlight distinct object centroids in a scene, providing a spatial inductive bias. Thanks to this strategy, COCA-Net generates high-quality segmentation masks on both the decoder side and, notably, the encoder side of its pipeline. Additionally, COCA-Net is not bound by a predetermined number of object masks that it generates and handles the segmentation of background elements better than its competitors. We demonstrate COCA-Net's segmentation performance on six widely adopted datasets, achieving superior or competitive results against the state-of-the-art models across nine different evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation</title>
<link>https://arxiv.org/abs/2505.02075</link>
<guid>https://arxiv.org/abs/2505.02075</guid>
<content:encoded><![CDATA[
arXiv:2505.02075v1 Announce Type: new 
Abstract: Vision Foundation Models (VFMs) are large-scale, pre-trained models that serve as general-purpose backbones for various computer vision tasks. As VFMs' popularity grows, there is an increasing interest in understanding their effectiveness for dense prediction tasks. However, VFMs typically produce low-resolution features, limiting their direct applicability in this context. One way to tackle this limitation is by employing a task-agnostic feature upsampling module that refines VFM features resolution. To assess the effectiveness of this approach, we investigate Interactive Segmentation (IS) as a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its inherent multimodal input, consisting of an image and a set of user-defined clicks, as well as its dense mask output, IS creates a challenging environment that demands comprehensive visual scene understanding. Our benchmarking experiments show that selecting appropriate upsampling strategies significantly improves VFM features quality. The code is released at https://github.com/havrylovv/iSegProbe
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HandOcc: NeRF-based Hand Rendering with Occupancy Networks</title>
<link>https://arxiv.org/abs/2505.02079</link>
<guid>https://arxiv.org/abs/2505.02079</guid>
<content:encoded><![CDATA[
arXiv:2505.02079v1 Announce Type: new 
Abstract: We propose HandOcc, a novel framework for hand rendering based upon occupancy. Popular rendering methods such as NeRF are often combined with parametric meshes to provide deformable hand models. However, in doing so, such approaches present a trade-off between the fidelity of the mesh and the complexity and dimensionality of the parametric model. The simplicity of parametric mesh structures is appealing, but the underlying issue is that it binds methods to mesh initialization, making it unable to generalize to objects where a parametric model does not exist. It also means that estimation is tied to mesh resolution and the accuracy of mesh fitting. This paper presents a pipeline for meshless 3D rendering, which we apply to the hands. By providing only a 3D skeleton, the desired appearance is extracted via a convolutional model. We do this by exploiting a NeRF renderer conditioned upon an occupancy-based representation. The approach uses the hand occupancy to resolve hand-to-hand interactions further improving results, allowing fast rendering, and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset, we achieved state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignSplat: Rendering Sign Language via Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.02108</link>
<guid>https://arxiv.org/abs/2505.02108</guid>
<content:encoded><![CDATA[
arXiv:2505.02108v1 Announce Type: new 
Abstract: State-of-the-art approaches for conditional human body rendering via Gaussian splatting typically focus on simple body motions captured from many views. This is often in the context of dancing or walking. However, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. The problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. The solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. Nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. We focus on how to achieve this, constraining mesh parameters to build an accurate Gaussian splatting framework from few views capable of modelling subtle human motion. We leverage regularization techniques on the Gaussian parameters to mitigate overfitting and rendering artifacts. Additionally, we propose a new adaptive control method to densify Gaussians and prune splat points on the mesh surface. To demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. On benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance</title>
<link>https://arxiv.org/abs/2505.02109</link>
<guid>https://arxiv.org/abs/2505.02109</guid>
<content:encoded><![CDATA[
arXiv:2505.02109v1 Announce Type: new 
Abstract: Hyperspectral images super-resolution aims to improve the spatial resolution, yet its performance is often limited at high-resolution ratios. The recent adoption of high-resolution reference images for super-resolution is driven by the poor spatial detail found in low-resolution HSIs, presenting it as a favorable method. However, these approaches cannot effectively utilize information from the reference image, due to the inaccuracy of alignment and its inadequate interaction between alignment and fusion modules. In this paper, we introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution (SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the issues of inaccurate alignment and poor interactivity of the previous approaches. Specifically, to ensure spatial concordance, i.e., align images more accurately across resolutions and refine textures, we construct a Two-Stage Image Alignment with a synthetic generation pipeline in the image alignment module, where the fine-tuned optical flow model can produce a more accurate optical flow in the first stage and warp model can refine damaged textures in the second stage. To enhance the interaction between alignment and fusion modules and ensure spectral concordance during reconstruction, we propose a Feature Aggregation module and an Attention Fusion module. In the feature aggregation module, we introduce an Iterative Deformable Feature Aggregation block to achieve significant feature matching and texture aggregation with the fusion multi-scale results guidance, iteratively generating learnable offset. Besides, we introduce two basic spectral-wise attention blocks in the attention fusion module to model the inter-spectra interactions. Extensive experiments on three natural or remote-sensing datasets show that our method outperforms state-of-the-art approaches on both quantitative and qualitative evaluations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction</title>
<link>https://arxiv.org/abs/2505.02126</link>
<guid>https://arxiv.org/abs/2505.02126</guid>
<content:encoded><![CDATA[
arXiv:2505.02126v1 Announce Type: new 
Abstract: Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiLLIE: Human-in-the-Loop Training for Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2505.02134</link>
<guid>https://arxiv.org/abs/2505.02134</guid>
<content:encoded><![CDATA[
arXiv:2505.02134v1 Announce Type: new 
Abstract: Developing effective approaches to generate enhanced results that align well with human visual preferences for high-quality well-lit images remains a challenge in low-light image enhancement (LLIE). In this paper, we propose a human-in-the-loop LLIE training framework that improves the visual quality of unsupervised LLIE model outputs through iterative training stages, named HiLLIE. At each stage, we introduce human guidance into the training process through efficient visual quality annotations of enhanced outputs. Subsequently, we employ a tailored image quality assessment (IQA) model to learn human visual preferences encoded in the acquired labels, which is then utilized to guide the training process of an enhancement model. With only a small amount of pairwise ranking annotations required at each stage, our approach continually improves the IQA model's capability to simulate human visual assessment of enhanced outputs, thus leading to visually appealing LLIE results. Extensive experiments demonstrate that our approach significantly improves unsupervised LLIE model performance in terms of both quantitative and qualitative performance. The code and collected ranking dataset will be available at https://github.com/LabShuHangGU/HiLLIE.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.02148</link>
<guid>https://arxiv.org/abs/2505.02148</guid>
<content:encoded><![CDATA[
arXiv:2505.02148v1 Announce Type: new 
Abstract: To operate safely, autonomous vehicles (AVs) need to detect and handle unexpected objects or anomalies on the road. While significant research exists for anomaly detection and segmentation in 2D, research progress in 3D is underexplored. Existing datasets lack high-quality multimodal data that are typically found in AVs. This paper presents a novel dataset for anomaly segmentation in driving scenarios. To the best of our knowledge, it is the first publicly available dataset focused on road anomaly segmentation with dense 3D semantic labeling, incorporating both LiDAR and camera data, as well as sequential information to enable anomaly detection across various ranges. This capability is critical for the safe navigation of autonomous vehicles. We adapted and evaluated several baseline models for 3D segmentation, highlighting the challenges of 3D anomaly detection in driving environments. Our dataset and evaluation code will be openly available, facilitating the testing and performance comparison of different approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Clips, Big Gains: Learning Long-Range Refocused Temporal Information for Video Super-Resolution</title>
<link>https://arxiv.org/abs/2505.02159</link>
<guid>https://arxiv.org/abs/2505.02159</guid>
<content:encoded><![CDATA[
arXiv:2505.02159v1 Announce Type: new 
Abstract: Video super-resolution (VSR) can achieve better performance compared to single image super-resolution by additionally leveraging temporal information. In particular, the recurrent-based VSR model exploits long-range temporal information during inference and achieves superior detail restoration. However, effectively learning these long-term dependencies within long videos remains a key challenge. To address this, we propose LRTI-VSR, a novel training framework for recurrent VSR that efficiently leverages Long-Range Refocused Temporal Information. Our framework includes a generic training strategy that utilizes temporal propagation features from long video clips while training on shorter video clips. Additionally, we introduce a refocused intra&amp;inter-frame transformer block which allows the VSR model to selectively prioritize useful temporal information through its attention module while further improving inter-frame information utilization in the FFN module. We evaluate LRTI-VSR on both CNN and transformer-based VSR architectures, conducting extensive ablation studies to validate the contribution of each component. Experiments on long-video test sets demonstrate that LRTI-VSR achieves state-of-the-art performance while maintaining training and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focus What Matters: Matchability-Based Reweighting for Local Feature Matching</title>
<link>https://arxiv.org/abs/2505.02161</link>
<guid>https://arxiv.org/abs/2505.02161</guid>
<content:encoded><![CDATA[
arXiv:2505.02161v1 Announce Type: new 
Abstract: Since the rise of Transformers, many semi-dense matching methods have adopted attention mechanisms to extract feature descriptors. However, the attention weights, which capture dependencies between pixels or keypoints, are often learned from scratch. This approach can introduce redundancy and noisy interactions from irrelevant regions, as it treats all pixels or keypoints equally. Drawing inspiration from keypoint selection processes, we propose to first classify all pixels into two categories: matchable and non-matchable. Matchable pixels are expected to receive higher attention weights, while non-matchable ones are down-weighted. In this work, we propose a novel attention reweighting mechanism that simultaneously incorporates a learnable bias term into the attention logits and applies a matchability-informed rescaling to the input value features. The bias term, injected prior to the softmax operation, selectively adjusts attention scores based on the confidence of query-key interactions. Concurrently, the feature rescaling acts post-attention by modulating the influence of each value vector in the final output. This dual design allows the attention mechanism to dynamically adjust both its internal weighting scheme and the magnitude of its output representations. Extensive experiments conducted on three benchmark datasets validate the effectiveness of our method, consistently outperforming existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.02175</link>
<guid>https://arxiv.org/abs/2505.02175</guid>
<content:encoded><![CDATA[
arXiv:2505.02175v1 Announce Type: new 
Abstract: Recovering 3D information from scenes via multi-view stereo reconstruction (MVS) and novel view synthesis (NVS) is inherently challenging, particularly in scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting (3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3D scene reconstruction while maintaining real-time performance. Recent approaches have tackled the problem of sparse real-time NVS using 3DGS within a generalizable, MVS-based learning framework to regress 3D Gaussian parameters. Our work extends this line of research by addressing the challenge of generalizable sparse 3D reconstruction and NVS jointly, and manages to perform successfully at both tasks. We propose an MVS-based learning pipeline that regresses 2DGS surface element parameters in a feed-forward fashion to perform 3D shape reconstruction and NVS from sparse-view images. We further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. The resulting model attains the state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also demonstrates strong generalization on the BlendedMVS and Tanks and Temples datasets. We note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saliency-Guided Training for Fingerprint Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2505.02176</link>
<guid>https://arxiv.org/abs/2505.02176</guid>
<content:encoded><![CDATA[
arXiv:2505.02176v1 Announce Type: new 
Abstract: Saliency-guided training, which directs model learning to important regions of images, has demonstrated generalization improvements across various biometric presentation attack detection (PAD) tasks. This paper presents its first application to fingerprint PAD. We conducted a 50-participant study to create a dataset of 800 human-annotated fingerprint perceptually-important maps, explored alongside algorithmically-generated "pseudosaliency," including minutiae-based, image quality-based, and autoencoder-based saliency maps. Evaluating on the 2021 Fingerprint Liveness Detection Competition testing set, we explore various configurations within five distinct training scenarios to assess the impact of saliency-guided training on accuracy and generalization. Our findings demonstrate the effectiveness of saliency-guided training for fingerprint PAD in both limited and large data contexts, and we present a configuration capable of earning the first place on the LivDet-2021 benchmark. Our results highlight saliency-guided training's promise for increased model generalization capabilities, its effectiveness when data is limited, and its potential to scale to larger datasets in fingerprint PAD. All collected saliency data and trained models are released with the paper to support reproducible research.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparfels: Fast Reconstruction from Sparse Unposed Imagery</title>
<link>https://arxiv.org/abs/2505.02178</link>
<guid>https://arxiv.org/abs/2505.02178</guid>
<content:encoded><![CDATA[
arXiv:2505.02178v1 Announce Type: new 
Abstract: We present a method for Sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications</title>
<link>https://arxiv.org/abs/2505.02179</link>
<guid>https://arxiv.org/abs/2505.02179</guid>
<content:encoded><![CDATA[
arXiv:2505.02179v1 Announce Type: new 
Abstract: Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance Learning (MIL) suffers from label ambiguity, hindering discriminative feature learning. We propose ProDisc-VAD, an efficient framework tackling this via two synergistic components. The Prototype Interaction Layer (PIL) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M parameters, over 800x fewer than recent ViT-based methods like VadCLIP, demonstrating exceptional efficiency alongside state-of-the-art performance. Code is available at https://github.com/modadundun/ProDisc-VAD.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust AI-Generated Face Detection with Imbalanced Data</title>
<link>https://arxiv.org/abs/2505.02182</link>
<guid>https://arxiv.org/abs/2505.02182</guid>
<content:encoded><![CDATA[
arXiv:2505.02182v1 Announce Type: new 
Abstract: Deepfakes, created using advanced AI techniques such as Variational Autoencoder and Generative Adversarial Networks, have evolved from research and entertainment applications into tools for malicious activities, posing significant threats to digital trust. Current deepfake detection techniques have evolved from CNN-based methods focused on local artifacts to more advanced approaches using vision transformers and multimodal models like CLIP, which capture global anomalies and improve cross-domain generalization. Despite recent progress, state-of-the-art deepfake detectors still face major challenges in handling distribution shifts from emerging generative models and addressing severe class imbalance between authentic and fake samples in deepfake datasets, which limits their robustness and detection accuracy. To address these challenges, we propose a framework that combines dynamic loss reweighting and ranking-based optimization, which achieves superior generalization and performance under imbalanced dataset conditions. The code is available at https://github.com/Purdue-M2/SP_CUP.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization</title>
<link>https://arxiv.org/abs/2505.02192</link>
<guid>https://arxiv.org/abs/2505.02192</guid>
<content:encoded><![CDATA[
arXiv:2505.02192v1 Announce Type: new 
Abstract: Customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention through focusing on identity and motion consistency. Existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. However, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrades. To address this, we introduce DualReal, a novel framework that, employs adaptive joint training to collaboratively construct interdependencies between dimensions. Specifically, DualReal is composed of two units: (1) Dual-aware Adaptation dynamically selects a training phase (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) StageBlender Controller leverages the denoising stages and Diffusion Transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. We constructed a more comprehensive benchmark than existing methods. The experimental results show that DualReal improves CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion quality metrics.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Physical Object State Representation in Text-to-Image Generative Systems</title>
<link>https://arxiv.org/abs/2505.02236</link>
<guid>https://arxiv.org/abs/2505.02236</guid>
<content:encoded><![CDATA[
arXiv:2505.02236v1 Announce Type: new 
Abstract: Current text-to-image generative models struggle to accurately represent object states (e.g., "a table without a bottle," "an empty tumbler"). In this work, we first design a fully-automatic pipeline to generate high-quality synthetic data that accurately captures objects in varied states. Next, we fine-tune several open-source text-to-image models on this synthetic data. We evaluate the performance of the fine-tuned models by quantifying the alignment of the generated images to their prompts using GPT4o-mini, and achieve an average absolute improvement of 8+% across four models on the public GenAI-Bench dataset. We also curate a collection of 200 prompts with a specific focus on common objects in various physical states. We demonstrate a significant improvement of an average of 24+% over the baseline on this dataset. We release all evaluation prompts and code.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantizing Diffusion Models from a Sampling-Aware Perspective</title>
<link>https://arxiv.org/abs/2505.02242</link>
<guid>https://arxiv.org/abs/2505.02242</guid>
<content:encoded><![CDATA[
arXiv:2505.02242v1 Announce Type: new 
Abstract: Diffusion models have recently emerged as the dominant approach in visual generation tasks. However, the lengthy denoising chains and the computationally intensive noise estimation networks hinder their applicability in low-latency and resource-limited environments. Previous research has endeavored to address these limitations in a decoupled manner, utilizing either advanced samplers or efficient model quantization techniques. In this study, we uncover that quantization-induced noise disrupts directional estimation at each sampling step, further distorting the precise directional estimations of higher-order samplers when solving the sampling equations through discretized numerical methods, thereby altering the optimal sampling trajectory. To attain dual acceleration with high fidelity, we propose a sampling-aware quantization strategy, wherein a Mixed-Order Trajectory Alignment technique is devised to impose a more stringent constraint on the error bounds at each sampling step, facilitating a more linear probability flow. Extensive experiments on sparse-step fast sampling across multiple datasets demonstrate that our approach preserves the rapid convergence characteristics of high-speed samplers while maintaining superior generation quality. Code will be made publicly available soon.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cricket: A Self-Powered Chirping Pixel</title>
<link>https://arxiv.org/abs/2505.02246</link>
<guid>https://arxiv.org/abs/2505.02246</guid>
<content:encoded><![CDATA[
arXiv:2505.02246v1 Announce Type: new 
Abstract: We present a sensor that can measure light and wirelessly communicate the measurement, without the need for an external power source or a battery. Our sensor, called cricket, harvests energy from incident light. It is asleep for most of the time and transmits a short and strong radio frequency chirp when its harvested energy reaches a specific level. The carrier frequency of each cricket is fixed and reveals its identity, and the duration between consecutive chirps is a measure of the incident light level. We have characterized the radiometric response function, signal-to-noise ratio and dynamic range of cricket. We have experimentally verified that cricket can be miniaturized at the expense of increasing the duration between chirps. We show that a cube with a cricket on each of its sides can be used to estimate the centroid of any complex illumination, which has value in applications such as solar tracking. We also demonstrate the use of crickets for creating untethered sensor arrays that can produce video and control lighting for energy conservation. Finally, we modified cricket's circuit to develop battery-free electronic sunglasses that can instantly adapt to environmental illumination.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset</title>
<link>https://arxiv.org/abs/2505.02255</link>
<guid>https://arxiv.org/abs/2505.02255</guid>
<content:encoded><![CDATA[
arXiv:2505.02255v1 Announce Type: new 
Abstract: This study presents a novel approach to enhance the cost-to-quality ratio of image generation with diffusion models. We hypothesize that differences between distilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are consistent and, therefore, learnable within a specialized domain, like portrait generation. We generate a synthetic paired dataset and train a fast image-to-image translation head. Using two sets of low- and high-quality synthetic images, our model is trained to refine the output of a distilled generator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like FLUX.1-dev, which is more computationally intensive. Our results show that the pipeline, which combines a distilled version of a large generative model with our enhancement layer, delivers similar photorealistic portraits to the baseline version with up to an 82% decrease in computational cost compared to FLUX.1-dev. This study demonstrates the potential for improving the efficiency of AI solutions involving large-scale image generation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compositional Image-Text Matching and Retrieval by Grounding Entities</title>
<link>https://arxiv.org/abs/2505.02278</link>
<guid>https://arxiv.org/abs/2505.02278</guid>
<content:encoded><![CDATA[
arXiv:2505.02278v1 Announce Type: new 
Abstract: Vision-language pretraining on large datasets of images-text pairs is one of the main building blocks of current Vision-Language Models. While with additional training, these models excel in various downstream tasks, including visual question answering, image captioning, and visual commonsense reasoning. However, a notable weakness of pretrained models like CLIP, is their inability to perform entity grounding and compositional image and text matching~\cite{Jiang2024ComCLIP, yang2023amc, Rajabi2023GroundedVSR, learninglocalizeCVPR24}. In this work we propose a novel learning-free zero-shot augmentation of CLIP embeddings that has favorable compositional properties. We compute separate embeddings of sub-images of object entities and relations that are localized by the state of the art open vocabulary detectors and dynamically adjust the baseline global image embedding. % The final embedding is obtained by computing a weighted combination of the sub-image embeddings. The resulting embedding is then utilized for similarity computation with text embedding, resulting in a average 1.5\% improvement in image-text matching accuracy on the Visual Genome and SVO Probes datasets~\cite{krishna2017visualgenome, svo}. Notably, the enhanced embeddings demonstrate superior retrieval performance, thus achieving significant gains on the Flickr30K and MS-COCO retrieval benchmarks~\cite{flickr30ke, mscoco}, improving the state-of-the-art Recall@1 by 12\% and 0.4\%, respectively. Our code is available at https://github.com/madhukarreddyvongala/GroundingCLIP.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Normalizing Flows for Uncertainty-Aware Human Pose Estimation</title>
<link>https://arxiv.org/abs/2505.02287</link>
<guid>https://arxiv.org/abs/2505.02287</guid>
<content:encoded><![CDATA[
arXiv:2505.02287v1 Announce Type: new 
Abstract: Human Pose Estimation (HPE) is increasingly important for applications like virtual reality and motion analysis, yet current methods struggle with balancing accuracy, computational efficiency, and reliable uncertainty quantification (UQ). Traditional regression-based methods assume fixed distributions, which might lead to poor UQ. Heatmap-based methods effectively model the output distribution using likelihood heatmaps, however, they demand significant resources. To address this, we propose Continuous Flow Residual Estimation (CFRE), an integration of Continuous Normalizing Flows (CNFs) into regression-based models, which allows for dynamic distribution adaptation. Through extensive experiments, we show that CFRE leads to better accuracy and uncertainty quantification with retained computational efficiency on both 2D and 3D human pose estimation tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment</title>
<link>https://arxiv.org/abs/2505.02325</link>
<guid>https://arxiv.org/abs/2505.02325</guid>
<content:encoded><![CDATA[
arXiv:2505.02325v1 Announce Type: new 
Abstract: Learning discriminative 3D representations that generalize well to unknown testing categories is an emerging requirement for many real-world 3D applications. Existing well-established methods often struggle to attain this goal due to insufficient 3D training data from broader concepts. Meanwhile, pre-trained large vision-language models (e.g., CLIP) have shown remarkable zero-shot generalization capabilities. Yet, they are limited in extracting suitable 3D representations due to substantial gaps between their 2D training and 3D testing distributions. To address these challenges, we propose Testing-time Distribution Alignment (TeDA), a novel framework that adapts a pretrained 2D vision-language model CLIP for unknown 3D object retrieval at test time. To our knowledge, it is the first work that studies the test-time adaptation of a vision-language model for 3D feature learning. TeDA projects 3D objects into multi-view images, extracts features using CLIP, and refines 3D query embeddings with an iterative optimization strategy by confident query-target sample pairs in a self-boosting manner. Additionally, TeDA integrates textual descriptions generated by a multimodal language model (InternVL) to enhance 3D object understanding, leveraging CLIP's aligned feature space to fuse visual and textual cues. Extensive experiments on four open-set 3D object retrieval benchmarks demonstrate that TeDA greatly outperforms state-of-the-art methods, even those requiring extensive training. We also experimented with depth maps on Objaverse-LVIS, further validating its effectiveness. Code is available at https://github.com/wangzhichuan123/TeDA.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection</title>
<link>https://arxiv.org/abs/2505.02331</link>
<guid>https://arxiv.org/abs/2505.02331</guid>
<content:encoded><![CDATA[
arXiv:2505.02331v1 Announce Type: new 
Abstract: Audiovisual emotion recognition (AVER) aims to infer human emotions from nonverbal visual-audio (VA) cues, offering modality-complementary and language-agnostic advantages. However, AVER remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. Recent self-supervised AVER approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. To address these issues, we propose VAEmo, an efficient two-stage framework for emotion-centric joint VA representation learning with external knowledge injection. In Stage 1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric VA corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. In Stage 2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of VA samples; these rich textual semantics are then injected by aligning their corresponding embeddings with VA representations through dual-path contrastive learning, further bridging the emotion gap. Extensive experiments on multiple downstream AVER benchmarks show that VAEmo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable VA emotion representations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>6D Pose Estimation on Spoons and Hands</title>
<link>https://arxiv.org/abs/2505.02335</link>
<guid>https://arxiv.org/abs/2505.02335</guid>
<content:encoded><![CDATA[
arXiv:2505.02335v1 Announce Type: new 
Abstract: Accurate dietary monitoring is essential for promoting healthier eating habits. A key area of research is how people interact and consume food using utensils and hands. By tracking their position and orientation, it is possible to estimate the volume of food being consumed, or monitor eating behaviours, highly useful insights into nutritional intake that can be more reliable than popular methods such as self-reporting. Hence, this paper implements a system that analyzes stationary video feed of people eating, using 6D pose estimation to track hand and spoon movements to capture spatial position and orientation. In doing so, we examine the performance of two state-of-the-art (SOTA) video object segmentation (VOS) models, both quantitatively and qualitatively, and identify main sources of error within the system.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaternion Infrared Visible Image Fusion</title>
<link>https://arxiv.org/abs/2505.02364</link>
<guid>https://arxiv.org/abs/2505.02364</guid>
<content:encoded><![CDATA[
arXiv:2505.02364v1 Announce Type: new 
Abstract: Visible images provide rich details and color information only under well-lighted conditions while infrared images effectively highlight thermal targets under challenging conditions such as low visibility and adverse weather. Infrared-visible image fusion aims to integrate complementary information from infrared and visible images to generate a high-quality fused image. Existing methods exhibit critical limitations such as neglecting color structure information in visible images and performance degradation when processing low-quality color-visible inputs. To address these issues, we propose a quaternion infrared-visible image fusion (QIVIF) framework to generate high-quality fused images completely in the quaternion domain. QIVIF proposes a quaternion low-visibility feature learning model to adaptively extract salient thermal targets and fine-grained texture details from input infrared and visible images respectively under diverse degraded conditions. QIVIF then develops a quaternion adaptive unsharp masking method to adaptively improve high-frequency feature enhancement with balanced illumination. QIVIF further proposes a quaternion hierarchical Bayesian fusion model to integrate infrared saliency and enhanced visible details to obtain high-quality fused images. Extensive experiments across diverse datasets demonstrate that our QIVIF surpasses state-of-the-art methods under challenging low-visibility conditions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaternion Multi-focus Color Image Fusion</title>
<link>https://arxiv.org/abs/2505.02365</link>
<guid>https://arxiv.org/abs/2505.02365</guid>
<content:encoded><![CDATA[
arXiv:2505.02365v1 Announce Type: new 
Abstract: Multi-focus color image fusion refers to integrating multiple partially focused color images to create a single all-in-focus color image. However, existing methods struggle with complex real-world scenarios due to limitations in handling color information and intricate textures. To address these challenges, this paper proposes a quaternion multi-focus color image fusion framework to perform high-quality color image fusion completely in the quaternion domain. This framework introduces 1) a quaternion sparse decomposition model to jointly learn fine-scale image details and structure information of color images in an iterative fashion for high-precision focus detection, 2) a quaternion base-detail fusion strategy to individually fuse base-scale and detail-scale results across multiple color images for preserving structure and detail information, and 3) a quaternion structural similarity refinement strategy to adaptively select optimal patches from initial fusion results and obtain the final fused result for preserving fine details and ensuring spatially consistent outputs. Extensive experiments demonstrate that the proposed framework outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing</title>
<link>https://arxiv.org/abs/2505.02370</link>
<guid>https://arxiv.org/abs/2505.02370</guid>
<content:encoded><![CDATA[
arXiv:2505.02370v1 Announce Type: new 
Abstract: Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans</title>
<link>https://arxiv.org/abs/2505.02388</link>
<guid>https://arxiv.org/abs/2505.02388</guid>
<content:encoded><![CDATA[
arXiv:2505.02388v1 Announce Type: new 
Abstract: Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.02393</link>
<guid>https://arxiv.org/abs/2505.02393</guid>
<content:encoded><![CDATA[
arXiv:2505.02393v1 Announce Type: new 
Abstract: Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Coordinated Prompt Attention is Needed for Visual Prompting</title>
<link>https://arxiv.org/abs/2505.02406</link>
<guid>https://arxiv.org/abs/2505.02406</guid>
<content:encoded><![CDATA[
arXiv:2505.02406v1 Announce Type: new 
Abstract: Visual prompting techniques are widely used to efficiently fine-tune pretrained Vision Transformers (ViT) by learning a small set of shared prompts for all tokens. However, existing methods overlook the unique roles of different tokens in conveying discriminative information and interact with all tokens using the same prompts, thereby limiting the representational capacity of ViT. This often leads to indistinguishable and biased prompt-extracted features, hindering performance. To address this issue, we propose a plug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns specific coordinated prompts to different tokens for attention-based interactions. Firstly, recognizing the distinct functions of CLS and image tokens-global information aggregation and local feature extraction, we disentangle the prompts into CLS Prompts and Image Prompts, which interact exclusively with CLS tokens and image tokens through attention mechanisms. This enhances their respective discriminative abilities. Furthermore, as different image tokens correspond to distinct image patches and contain diverse information, we employ a matching function to automatically assign coordinated prompts to individual tokens. This enables more precise attention interactions, improving the diversity and representational capacity of the extracted features. Extensive experiments across various benchmarks demonstrate that TCPA significantly enhances the diversity and discriminative power of the extracted features. The code is available at https://github.com/zhoujiahuan1991/ICML2025-TCPA.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey</title>
<link>https://arxiv.org/abs/2505.02448</link>
<guid>https://arxiv.org/abs/2505.02448</guid>
<content:encoded><![CDATA[
arXiv:2505.02448v1 Announce Type: new 
Abstract: Out-of-distribution detection (OOD) is a pivotal task for real-world applications that trains models to identify samples that are distributionally different from the in-distribution (ID) data during testing. Recent advances in AI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized OOD detection by shifting from traditional unimodal image detectors to multimodal image-text detectors. This shift has inspired extensive research; however, existing categorization schemes (e.g., few- or zero-shot types) still rely solely on the availability of ID images, adhering to a unimodal paradigm. To better align with CLIP's cross-modal nature, we propose a new categorization framework rooted in both image and text modalities. Specifically, we categorize existing methods based on how visual and textual information of OOD data is utilized within image + text modalities, and further divide them into four groups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e., learnable vectors or class names) Known or Unknown, across two training strategies (i.e., train-free or training-required). More importantly, we discuss open problems in CLIP-like OOD detection and highlight promising directions for future research, including cross-domain integration, practical applications, and theoretical understanding.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging</title>
<link>https://arxiv.org/abs/2505.02467</link>
<guid>https://arxiv.org/abs/2505.02467</guid>
<content:encoded><![CDATA[
arXiv:2505.02467v1 Announce Type: new 
Abstract: Multimodal deep learning harnesses diverse imaging modalities, such as MRI sequences, to enhance diagnostic accuracy in medical imaging. A key challenge is determining the optimal timing for integrating these modalities-specifically, identifying the network layers where fusion modules should be inserted. Current approaches often rely on manual tuning or exhaustive search, which are computationally expensive without any guarantee of converging to optimal results. We propose a sequential forward search algorithm that incrementally activates and evaluates candidate fusion modules at different layers of a multimodal network. At each step, the algorithm retrains from previously learned weights and compares validation loss to identify the best-performing configuration. This process systematically reduces the search space, enabling efficient identification of the optimal fusion timing without exhaustively testing all possible module placements. The approach is validated on two multimodal MRI datasets, each addressing different classification tasks. Our algorithm consistently identified configurations that outperformed unimodal baselines, late fusion, and a brute-force ensemble of all potential fusion placements. These architectures demonstrated superior accuracy, F-score, and specificity while maintaining competitive or improved AUC values. Furthermore, the sequential nature of the search significantly reduced computational overhead, making the optimization process more practical. By systematically determining the optimal timing to fuse imaging modalities, our method advances multimodal deep learning for medical imaging. It provides an efficient and robust framework for fusion optimization, paving the way for improved clinical decision-making and more adaptable, scalable architectures in medical AI applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction</title>
<link>https://arxiv.org/abs/2505.02471</link>
<guid>https://arxiv.org/abs/2505.02471</guid>
<content:encoded><![CDATA[
arXiv:2505.02471v1 Announce Type: new 
Abstract: We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finger Pose Estimation for Under-screen Fingerprint Sensor</title>
<link>https://arxiv.org/abs/2505.02481</link>
<guid>https://arxiv.org/abs/2505.02481</guid>
<content:encoded><![CDATA[
arXiv:2505.02481v1 Announce Type: new 
Abstract: Two-dimensional pose estimation plays a crucial role in fingerprint recognition by facilitating global alignment and reduce pose-induced variations. However, existing methods are still unsatisfactory when handling with large angle or small area inputs. These limitations are particularly pronounced on fingerprints captured by under-screen fingerprint sensors in smartphones. In this paper, we present a novel dual-modal input based network for under-screen fingerprint pose estimation. Our approach effectively integrates two distinct yet complementary modalities: texture details extracted from ridge patches through the under-screen fingerprint sensor, and rough contours derived from capacitive images obtained via the touch screen. This collaborative integration endows our network with more comprehensive and discriminative information, substantially improving the accuracy and stability of pose estimation. A decoupled probability distribution prediction task is designed, instead of the traditional supervised forms of numerical regression or heatmap voting, to facilitate the training process. Additionally, we incorporate a Mixture of Experts (MoE) based feature fusion mechanism and a relationship driven cross-domain knowledge transfer strategy to further strengthen feature extraction and fusion capabilities. Extensive experiments are conducted on several public datasets and two private datasets. The results indicate that our method is significantly superior to previous state-of-the-art (SOTA) methods and remarkably boosts the recognition ability of fingerprint recognition algorithms. Our code is available at https://github.com/XiongjunGuan/DRACO.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions</title>
<link>https://arxiv.org/abs/2505.02501</link>
<guid>https://arxiv.org/abs/2505.02501</guid>
<content:encoded><![CDATA[
arXiv:2505.02501v1 Announce Type: new 
Abstract: We introduce Corr2Distrib, the first correspondence-based method which estimates a 6D camera pose distribution from an RGB image, explaining the observations. Indeed, symmetries and occlusions introduce visual ambiguities, leading to multiple valid poses. While a few recent methods tackle this problem, they do not rely on local correspondences which, according to the BOP Challenge, are currently the most effective way to estimate a single 6DoF pose solution. Using correspondences to estimate a pose distribution is not straightforward, since ambiguous correspondences induced by visual ambiguities drastically decrease the performance of PnP. With Corr2Distrib, we turn these ambiguities into an advantage to recover all valid poses. Corr2Distrib first learns a symmetry-aware representation for each 3D point on the object's surface, characterized by a descriptor and a local frame. This representation enables the generation of 3DoF rotation hypotheses from single 2D-3D correspondences. Next, we refine these hypotheses into a 6DoF pose distribution using PnP and pose scoring. Our experimental evaluations on complex non-synthetic scenes show that Corr2Distrib outperforms state-of-the-art solutions for both pose distribution estimation and single pose estimation from an RGB image, demonstrating the potential of correspondences-based approaches.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text to Image Generation and Editing: A Survey</title>
<link>https://arxiv.org/abs/2505.02527</link>
<guid>https://arxiv.org/abs/2505.02527</guid>
<content:encoded><![CDATA[
arXiv:2505.02527v1 Announce Type: new 
Abstract: Text-to-image generation (T2I) refers to the text-guided generation of high-quality images. In the past few years, T2I has attracted widespread attention and numerous works have emerged. In this survey, we comprehensively review 141 works conducted from 2021 to 2024. First, we introduce four foundation model architectures of T2I (autoregression, non-autoregression, GAN and diffusion) and the commonly used key technologies (autoencoder, attention and classifier-free guidance). Secondly, we systematically compare the methods of these studies in two directions, T2I generation and T2I editing, including the encoders and the key technologies they use. In addition, we also compare the performance of these researches side by side in terms of datasets, evaluation metrics, training resources, and inference speed. In addition to the four foundation models, we survey other works on T2I, such as energy-based models and recent Mamba and multimodality. We also investigate the potential social impact of T2I and provide some solutions. Finally, we propose unique insights of improving the performance of T2I models and possible future development directions. In summary, this survey is the first systematic and comprehensive overview of T2I, aiming to provide a valuable guide for future researchers and stimulate continued progress in this field.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction</title>
<link>https://arxiv.org/abs/2505.02529</link>
<guid>https://arxiv.org/abs/2505.02529</guid>
<content:encoded><![CDATA[
arXiv:2505.02529v1 Announce Type: new 
Abstract: Cancer survival prediction using multi-modal medical imaging presents a critical challenge in oncology, mainly due to the vulnerability of deep learning models to noise and protocol variations across imaging centers. Current approaches struggle to extract consistent features from heterogeneous CT and PET images, limiting their clinical applicability. We address these challenges by introducing RobSurv, a robust deep-learning framework that leverages vector quantization for resilient multi-modal feature learning. The key innovation of our approach lies in its dual-path architecture: one path maps continuous imaging features to learned discrete codebooks for noise-resistant representation, while the parallel path preserves fine-grained details through continuous feature processing. This dual representation is integrated through a novel patch-wise fusion mechanism that maintains local spatial relationships while capturing global context via Transformer-based processing. In extensive evaluations across three diverse datasets (HECKTOR, H\&amp;N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance, achieving concordance index of 0.771, 0.742, and 0.734 respectively - significantly outperforming existing methods. Most notably, our model maintains robust performance even under severe noise conditions, with performance degradation of only 3.8-4.5\% compared to 8-12\% in baseline methods. These results, combined with strong generalization across different cancer types and imaging protocols, establish RobSurv as a promising solution for reliable clinical prognosis that can enhance treatment planning and patient care.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D Reconstruction</title>
<link>https://arxiv.org/abs/2505.02539</link>
<guid>https://arxiv.org/abs/2505.02539</guid>
<content:encoded><![CDATA[
arXiv:2505.02539v1 Announce Type: new 
Abstract: Accurate 3D reconstruction using multi-camera RGB-D systems critically depends on precise extrinsic calibration to achieve proper alignment between captured views. In this paper, we introduce an iterative extrinsic calibration method that leverages the geometric constraints provided by a three-dimensional marker to significantly improve calibration accuracy. Our proposed approach systematically segments and refines marker planes through clustering, regression analysis, and iterative reassignment techniques, ensuring robust geometric correspondence across camera views. We validate our method comprehensively in both controlled environments and practical real-world settings within the Tech4Diet project, aimed at modeling the physical progression of patients undergoing nutritional treatments. Experimental results demonstrate substantial reductions in alignment errors, facilitating accurate and reliable 3D reconstructions.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identfication</title>
<link>https://arxiv.org/abs/2505.02549</link>
<guid>https://arxiv.org/abs/2505.02549</guid>
<content:encoded><![CDATA[
arXiv:2505.02549v1 Announce Type: new 
Abstract: Unsupervised visible-infrared person re-identification (UVI-ReID) aims to retrieve pedestrian images across different modalities without costly annotations, but faces challenges due to the modality gap and lack of supervision. Existing methods often adopt self-training with clustering-generated pseudo-labels but implicitly assume these labels are always correct. In practice, however, this assumption fails due to inevitable pseudo-label noise, which hinders model learning. To address this, we introduce a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN), characterized by three key challenges: noise overfitting, error accumulation, and noisy cluster correspondence. To this end, we propose a novel Robust Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning mechanism (RAL) is proposed to dynamically emphasize clean samples while down-weighting noisy ones. Second, to alleviate error accumulation-where the model reinforces its own mistakes-RoDE employs dual distinct models that are alternately trained using pseudo-labels from each other, encouraging diversity and preventing collapse. However, this dual-model strategy introduces misalignment between clusters across models and modalities, creating noisy cluster correspondence. To resolve this, we introduce Cluster Consistency Matching (CCM), which aligns clusters across models and modalities by measuring cross-cluster similarity. Extensive experiments on three benchmarks demonstrate the effectiveness of RoDE.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2505.02567</link>
<guid>https://arxiv.org/abs/2505.02567</guid>
<content:encoded><![CDATA[
arXiv:2505.02567v1 Announce Type: new 
Abstract: Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey will be available on GitHub soon.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet</title>
<link>https://arxiv.org/abs/2505.02586</link>
<guid>https://arxiv.org/abs/2505.02586</guid>
<content:encoded><![CDATA[
arXiv:2505.02586v1 Announce Type: new 
Abstract: This work introduces RGBX-DiffusionDet, an object detection framework extending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB imagery via an adaptive multimodal encoder. To enable cross-modal interaction, we design the dynamic channel reduction within a convolutional block attention module (DCR-CBAM), which facilitates cross-talk between subnetworks by dynamically highlighting salient channel features. Furthermore, the dynamic multi-level aggregation block (DMLAB) is proposed to refine spatial feature representations through adaptive multiscale fusion. Finally, novel regularization losses that enforce channel saliency and spatial selectivity are introduced, leading to compact and discriminative feature embeddings. Extensive experiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric dataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We demonstrate consistent superiority of the proposed approach over the baseline RGB-only DiffusionDet. The modular architecture maintains the original decoding complexity, ensuring efficiency. These results establish the proposed RGBX-DiffusionDet as a flexible multimodal object detection approach, providing new insights into integrating diverse 2D sensing modalities into diffusion-based detection pipelines.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DELTA: Dense Depth from Events and LiDAR using Transformer's Attention</title>
<link>https://arxiv.org/abs/2505.02593</link>
<guid>https://arxiv.org/abs/2505.02593</guid>
<content:encoded><![CDATA[
arXiv:2505.02593v1 Announce Type: new 
Abstract: Event cameras and LiDARs provide complementary yet distinct data: respectively, asynchronous detections of changes in lighting versus sparse but accurate depth information at a fixed rate. To this day, few works have explored the combination of these two modalities. In this article, we propose a novel neural-network-based method for fusing event and LiDAR data in order to estimate dense depth maps. Our architecture, DELTA, exploits the concepts of self- and cross-attention to model the spatial and temporal relations within and between the event and LiDAR data. Following a thorough evaluation, we demonstrate that DELTA sets a new state of the art in the event-based depth estimation problem, and that it is able to reduce the errors up to four times for close ranges compared to the previous SOTA.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2505.02626</link>
<guid>https://arxiv.org/abs/2505.02626</guid>
<content:encoded><![CDATA[
arXiv:2505.02626v1 Announce Type: new 
Abstract: Recent advances in visual industrial anomaly detection have demonstrated exceptional performance in identifying and segmenting anomalous regions while maintaining fast inference speeds. However, anomaly classification-distinguishing different types of anomalies-remains largely unexplored despite its critical importance in real-world inspection tasks. To address this gap, we propose VELM, a novel LLM-based pipeline for anomaly classification. Given the critical importance of inference speed, we first apply an unsupervised anomaly detection method as a vision expert to assess the normality of an observation. If an anomaly is detected, the LLM then classifies its type. A key challenge in developing and evaluating anomaly classification models is the lack of precise annotations of anomaly classes in existing datasets. To address this limitation, we introduce MVTec-AC and VisA-AC, refined versions of the widely used MVTec-AD and VisA datasets, which include accurate anomaly class labels for rigorous evaluation. Our approach achieves a state-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD, exceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the effectiveness of VELM in understanding and categorizing anomalies. We hope our methodology and benchmark inspire further research in anomaly classification, helping bridge the gap between detection and comprehensive anomaly characterization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.02648</link>
<guid>https://arxiv.org/abs/2505.02648</guid>
<content:encoded><![CDATA[
arXiv:2505.02648v1 Announce Type: new 
Abstract: Diffusion models have shown excellent performance in text-to-image generation. Nevertheless, existing methods often suffer from performance bottlenecks when handling complex prompts that involve multiple objects, characteristics, and relations. Therefore, we propose a Multi-agent Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation for complex scenes. Specifically, we design a multi-agent collaboration-based scene parsing module that generates an agent system comprising multiple agents with distinct tasks, utilizing MLLMs to extract various scene elements effectively. In addition, Hierarchical Compositional diffusion utilizes a Gaussian mask and filtering to refine bounding box regions and enhance objects through region enhancement, resulting in the accurate and high-fidelity generation of complex scenes. Comprehensive experiments demonstrate that our MCCD significantly improves the performance of the baseline models in a training-free manner, providing a substantial advantage in complex scene generation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sim2Real in endoscopy segmentation with a novel structure aware image translation</title>
<link>https://arxiv.org/abs/2505.02654</link>
<guid>https://arxiv.org/abs/2505.02654</guid>
<content:encoded><![CDATA[
arXiv:2505.02654v1 Announce Type: new 
Abstract: Automatic segmentation of anatomical landmarks in endoscopic images can provide assistance to doctors and surgeons for diagnosis, treatments or medical training. However, obtaining the annotations required to train commonly used supervised learning methods is a tedious and difficult task, in particular for real images. While ground truth annotations are easier to obtain for synthetic data, models trained on such data often do not generalize well to real data. Generative approaches can add realistic texture to it, but face difficulties to maintain the structure of the original scene. The main contribution in this work is a novel image translation model that adds realistic texture to simulated endoscopic images while keeping the key scene layout information. Our approach produces realistic images in different endoscopy scenarios. We demonstrate these images can effectively be used to successfully train a model for a challenging end task without any real labeled data. In particular, we demonstrate our approach for the task of fold segmentation in colonoscopy images. Folds are key anatomical landmarks that can occlude parts of the colon mucosa and possible polyps. Our approach generates realistic images maintaining the shape and location of the original folds, after the image-style-translation, better than existing methods. We run experiments both on a novel simulated dataset for fold segmentation, and real data from the EndoMapper (EM) dataset. All our new generated data and new EM metadata is being released to facilitate further research, as no public benchmark is currently available for the task of fold segmentation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data</title>
<link>https://arxiv.org/abs/2505.02677</link>
<guid>https://arxiv.org/abs/2505.02677</guid>
<content:encoded><![CDATA[
arXiv:2505.02677v1 Announce Type: new 
Abstract: Stroke is a major public health problem, affecting millions worldwide. Deep learning has recently demonstrated promise for enhancing the diagnosis and risk prediction of stroke. However, existing methods rely on costly medical imaging modalities, such as computed tomography. Recent studies suggest that retinal imaging could offer a cost-effective alternative for cerebrovascular health assessment due to the shared clinical pathways between the retina and the brain. Hence, this study explores the impact of leveraging retinal images and clinical data for stroke detection and risk prediction. We propose a multimodal deep neural network that processes Optical Coherence Tomography (OCT) and infrared reflectance retinal scans, combined with clinical data, such as demographics, vital signs, and diagnosis codes. We pretrained our model using a self-supervised learning framework using a real-world dataset consisting of $37$ k scans, and then fine-tuned and evaluated the model using a smaller labeled subset. Our empirical findings establish the predictive ability of the considered modalities in detecting lasting effects in the retina associated with acute stroke and forecasting future risk within a specific time horizon. The experimental results demonstrate the effectiveness of our proposed framework by achieving $5$\% AUROC improvement as compared to the unimodal image-only baseline, and $8$\% improvement compared to an existing state-of-the-art foundation model. In conclusion, our study highlights the potential of retinal imaging in identifying high-risk patients and improving long-term outcomes.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dance of Fireworks: An Interactive Broadcast Gymnastics Training System Based on Pose Estimation</title>
<link>https://arxiv.org/abs/2505.02690</link>
<guid>https://arxiv.org/abs/2505.02690</guid>
<content:encoded><![CDATA[
arXiv:2505.02690v1 Announce Type: new 
Abstract: This study introduces Dance of Fireworks, an interactive system designed to combat sedentary health risks by enhancing engagement in radio calisthenics. Leveraging mobile device cameras and lightweight pose estimation (PoseNet/TensorFlow Lite), the system extracts body keypoints, computes joint angles, and compares them with standardized motions to deliver real-time corrective feedback. To incentivize participation, it dynamically maps users' movements (such as joint angles and velocity) to customizable fireworks animations, rewarding improved accuracy with richer visual effects.
Experiments involving 136 participants demonstrated a significant reduction in average joint angle errors from 21.3 degrees to 9.8 degrees (p < 0.01) over four sessions, with 93.4 percent of users affirming its exercise-promoting efficacy and 85.4 percent praising its entertainment value. The system operates without predefined motion templates or specialised hardware, enabling seamless integration into office environments. Future enhancements will focus on improving pose recognition accuracy, reducing latency, and adding features such as multiplayer interaction and music synchronisation. This work presents a cost-effective, engaging solution to promote physical activity in sedentary populations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure Causal Models and LLMs Integration in Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.02703</link>
<guid>https://arxiv.org/abs/2505.02703</guid>
<content:encoded><![CDATA[
arXiv:2505.02703v1 Announce Type: new 
Abstract: Medical Visual Question Answering (MedVQA) aims to answer medical questions according to medical images. However, the complexity of medical data leads to confounders that are difficult to observe, so bias between images and questions is inevitable. Such cross-modal bias makes it challenging to infer medically meaningful answers. In this work, we propose a causal inference framework for the MedVQA task, which effectively eliminates the relative confounding effect between the image and the question to ensure the precision of the question-answering (QA) session. We are the first to introduce a novel causal graph structure that represents the interaction between visual and textual elements, explicitly capturing how different questions influence visual features. During optimization, we apply the mutual information to discover spurious correlations and propose a multi-variable resampling front-door adjustment method to eliminate the relative confounding effect, which aims to align features based on their true causal relevance to the question-answering task. In addition, we also introduce a prompt strategy that combines multiple prompt forms to improve the model's ability to understand complex medical data and answer accurately. Extensive experiments on three MedVQA datasets demonstrate that 1) our method significantly improves the accuracy of MedVQA, and 2) our method achieves true causal correlations in the face of complex medical data.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery</title>
<link>https://arxiv.org/abs/2505.02704</link>
<guid>https://arxiv.org/abs/2505.02704</guid>
<content:encoded><![CDATA[
arXiv:2505.02704v1 Announce Type: new 
Abstract: We propose a robust method for monocular depth scale recovery. Monocular depth estimation can be divided into two main directions: (1) relative depth estimation, which provides normalized or inverse depth without scale information, and (2) metric depth estimation, which involves recovering depth with absolute scale. To obtain absolute scale information for practical downstream tasks, utilizing textual information to recover the scale of a relative depth map is a highly promising approach. However, since a single image can have multiple descriptions from different perspectives or with varying styles, it has been shown that different textual descriptions can significantly affect the scale recovery process. To address this issue, our method, VGLD, stabilizes the influence of textual information by incorporating high-level semantic information from the corresponding image alongside the textual description. This approach resolves textual ambiguities and robustly outputs a set of linear transformation parameters (scalars) that can be globally applied to the relative depth map, ultimately generating depth predictions with metric-scale accuracy. We validate our method across several popular relative depth models(MiDas, DepthAnything), using both indoor scenes (NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions as a universal alignment module when trained on multiple datasets, achieving strong performance even in zero-shot scenarios. Code is available at: https://github.com/pakinwu/VGLD.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rate-Quality Model for Learned Video Coding</title>
<link>https://arxiv.org/abs/2505.02720</link>
<guid>https://arxiv.org/abs/2505.02720</guid>
<content:encoded><![CDATA[
arXiv:2505.02720v1 Announce Type: new 
Abstract: Learned video coding (LVC) has recently achieved superior coding performance. In this paper, we model the rate-quality (R-Q) relationship for learned video coding by a parametric function. We learn a neural network, termed RQNet, to characterize the relationship between the bitrate and quality level according to video content and coding context. The predicted (R,Q) results are further integrated with those from previously coded frames using the least-squares method to determine the parameters of our R-Q model on-the-fly. Compared to the conventional approaches, our method accurately estimates the R-Q relationship, enabling the online adaptation of model parameters to enhance both flexibility and precision. Experimental results show that our R-Q model achieves significantly smaller bitrate deviations than the baseline method on commonly used datasets with minimal additional complexity.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Knowledge Graphs to harvest datasets for efficient CLIP model training</title>
<link>https://arxiv.org/abs/2505.02746</link>
<guid>https://arxiv.org/abs/2505.02746</guid>
<content:encoded><![CDATA[
arXiv:2505.02746v1 Announce Type: new 
Abstract: Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Platelet enumeration in dense aggregates</title>
<link>https://arxiv.org/abs/2505.02751</link>
<guid>https://arxiv.org/abs/2505.02751</guid>
<content:encoded><![CDATA[
arXiv:2505.02751v1 Announce Type: new 
Abstract: Identifying and counting blood components such as red blood cells, various types of white blood cells, and platelets is a critical task for healthcare practitioners. Deep learning approaches, particularly convolutional neural networks (CNNs) using supervised learning strategies, have shown considerable success for such tasks. However, CNN based architectures such as U-Net, often struggles to accurately identify platelets due to their sizes and high variability of features. To address these challenges, researchers have commonly employed strategies such as class weighted loss functions, which have demonstrated some success. However, this does not address the more significant challenge of platelet variability in size and tendency to form aggregates and associations with other blood components. In this study, we explored an alternative approach by investigating the role of convolutional kernels in mitigating these issues. We also assigned separate classes to singular platelets and platelet aggregates and performed semantic segmentation using various U-Net architectures for identifying platelets. We then evaluated and compared two common methods (pixel area method and connected component analysis) for counting platelets and proposed an alternative approach specialized for single platelets and platelet aggregates. Our experiments provided results that showed significant improvements in the identification of platelets, highlighting the importance of optimizing convolutional operations and class designations. We show that the common practice of pixel area-based counting often over estimate platelet counts, whereas the proposed method presented in this work offers significant improvements. We discuss in detail about these methods from segmentation masks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models</title>
<link>https://arxiv.org/abs/2505.02753</link>
<guid>https://arxiv.org/abs/2505.02753</guid>
<content:encoded><![CDATA[
arXiv:2505.02753v1 Announce Type: new 
Abstract: We explore Generalizable Tumor Segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. Existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. In this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. To further improve and refine anomaly segmentation masks, DiffuGTS leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. Comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. Codes are available at https://github.com/Yankai96/DiffuGTS.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Deep Learning-based Keypoint Localization Estimating Descriptor Matching Performance</title>
<link>https://arxiv.org/abs/2505.02779</link>
<guid>https://arxiv.org/abs/2505.02779</guid>
<content:encoded><![CDATA[
arXiv:2505.02779v1 Announce Type: new 
Abstract: Retinal image registration, particularly for color fundus images, is a challenging yet essential task with diverse clinical applications. Existing registration methods for color fundus images typically rely on keypoints and descriptors for alignment; however, a significant limitation is their reliance on labeled data, which is particularly scarce in the medical domain.
  In this work, we present a novel unsupervised registration pipeline that entirely eliminates the need for labeled data. Our approach is based on the principle that locations with distinctive descriptors constitute reliable keypoints. This fully inverts the conventional state-of-the-art approach, conditioning the detector on the descriptor rather than the opposite.
  First, we propose an innovative descriptor learning method that operates without keypoint detection or any labels, generating descriptors for arbitrary locations in retinal images. Next, we introduce a novel, label-free keypoint detector network which works by estimating descriptor performance directly from the input image.
  We validate our method through a comprehensive evaluation on four hold-out datasets, demonstrating that our unsupervised descriptor outperforms state-of-the-art supervised descriptors and that our unsupervised detector significantly outperforms existing unsupervised detection methods. Finally, our full registration pipeline achieves performance comparable to the leading supervised methods, while not employing any labeled data. Additionally, the label-free nature and design of our method enable direct adaptation to other domains and modalities.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge</title>
<link>https://arxiv.org/abs/2505.02784</link>
<guid>https://arxiv.org/abs/2505.02784</guid>
<content:encoded><![CDATA[
arXiv:2505.02784v1 Announce Type: new 
Abstract: Accurate fetal brain tissue segmentation and biometric analysis are essential for studying brain development in utero. The FeTA Challenge 2024 advanced automated fetal brain MRI analysis by introducing biometry prediction as a new task alongside tissue segmentation. For the first time, our diverse multi-centric test set included data from a new low-field (0.55T) MRI dataset. Evaluation metrics were also expanded to include the topology-specific Euler characteristic difference (ED). Sixteen teams submitted segmentation methods, most of which performed consistently across both high- and low-field scans. However, longitudinal trends indicate that segmentation accuracy may be reaching a plateau, with results now approaching inter-rater variability. The ED metric uncovered topological differences that were missed by conventional metrics, while the low-field dataset achieved the highest segmentation scores, highlighting the potential of affordable imaging systems when paired with high-quality reconstruction. Seven teams participated in the biometry task, but most methods failed to outperform a simple baseline that predicted measurements based solely on gestational age, underscoring the challenge of extracting reliable biometric estimates from image data alone. Domain shift analysis identified image quality as the most significant factor affecting model generalization, with super-resolution pipelines also playing a substantial role. Other factors, such as gestational age, pathology, and acquisition site, had smaller, though still measurable, effects. Overall, FeTA 2024 offers a comprehensive benchmark for multi-class segmentation and biometry estimation in fetal brain MRI, underscoring the need for data-centric approaches, improved topological evaluation, and greater dataset diversity to enable clinically robust and generalizable AI tools.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration</title>
<link>https://arxiv.org/abs/2505.02787</link>
<guid>https://arxiv.org/abs/2505.02787</guid>
<content:encoded><![CDATA[
arXiv:2505.02787v1 Announce Type: new 
Abstract: Current color fundus image registration approaches are limited, among other things, by the lack of labeled data, which is even more significant in the medical domain, motivating the use of unsupervised learning. Therefore, in this work, we develop a novel unsupervised descriptor learning method that does not rely on keypoint detection. This enables the resulting descriptor network to be agnostic to the keypoint detector used during the registration inference.
  To validate this approach, we perform an extensive and comprehensive comparison on the reference public retinal image registration dataset. Additionally, we test our method with multiple keypoint detectors of varied nature, even proposing some novel ones. Our results demonstrate that the proposed approach offers accurate registration, not incurring in any performance loss versus supervised methods. Additionally, it demonstrates accurate performance regardless of the keypoint detector used. Thus, this work represents a notable step towards leveraging unsupervised learning in the medical domain.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPNet: Dynamic Pooling Network for Tiny Object Detection</title>
<link>https://arxiv.org/abs/2505.02797</link>
<guid>https://arxiv.org/abs/2505.02797</guid>
<content:encoded><![CDATA[
arXiv:2505.02797v1 Announce Type: new 
Abstract: In unmanned aerial systems, especially in complex environments, accurately detecting tiny objects is crucial. Resizing images is a common strategy to improve detection accuracy, particularly for small objects. However, simply enlarging images significantly increases computational costs and the number of negative samples, severely degrading detection performance and limiting its applicability. This paper proposes a Dynamic Pooling Network (DPNet) for tiny object detection to mitigate these issues. DPNet employs a flexible down-sampling strategy by introducing a factor (df) to relax the fixed downsampling process of the feature map to an adjustable one. Furthermore, we design a lightweight predictor to predict df for each input image, which is used to decrease the resolution of feature maps in the backbone. Thus, we achieve input-aware downsampling. We also design an Adaptive Normalization Module (ANM) to make a unified detector compatible with different dfs. A guidance loss supervises the predictor's training. DPNet dynamically allocates computing resources to trade off between detection accuracy and efficiency. Experiments on the TinyCOCO and TinyPerson datasets show that DPNet can save over 35% and 25% GFLOPs, respectively, while maintaining comparable detection performance. The code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Database-Agnostic Gait Enrollment using SetTransformers</title>
<link>https://arxiv.org/abs/2505.02815</link>
<guid>https://arxiv.org/abs/2505.02815</guid>
<content:encoded><![CDATA[
arXiv:2505.02815v1 Announce Type: new 
Abstract: Gait recognition has emerged as a powerful tool for unobtrusive and long-range identity analysis, with growing relevance in surveillance and monitoring applications. Although recent advances in deep learning and large-scale datasets have enabled highly accurate recognition under closed-set conditions, real-world deployment demands open-set gait enrollment, which means determining whether a new gait sample corresponds to a known identity or represents a previously unseen individual. In this work, we introduce a transformer-based framework for open-set gait enrollment that is both dataset-agnostic and recognition-architecture-agnostic. Our method leverages a SetTransformer to make enrollment decisions based on the embedding of a probe sample and a context set drawn from the gallery, without requiring task-specific thresholds or retraining for new environments. By decoupling enrollment from the main recognition pipeline, our model is generalized across different datasets, gallery sizes, and identity distributions. We propose an evaluation protocol that uses existing datasets in different ratios of identities and walks per identity. We instantiate our method using skeleton-based gait representations and evaluate it on two benchmark datasets (CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition models (GaitGraph, GaitFormer, and GaitPT). We show that our method is flexible, is able to accurately perform enrollment in different scenarios, and scales better with data compared to traditional approaches. We will make the code and dataset scenarios publicly available.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing</title>
<link>https://arxiv.org/abs/2505.02823</link>
<guid>https://arxiv.org/abs/2505.02823</guid>
<content:encoded><![CDATA[
arXiv:2505.02823v1 Announce Type: new 
Abstract: Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2505.02824</link>
<guid>https://arxiv.org/abs/2505.02824</guid>
<content:encoded><![CDATA[
arXiv:2505.02824v1 Announce Type: new 
Abstract: Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology</title>
<link>https://arxiv.org/abs/2505.02825</link>
<guid>https://arxiv.org/abs/2505.02825</guid>
<content:encoded><![CDATA[
arXiv:2505.02825v1 Announce Type: new 
Abstract: Computer vision methods have demonstrated considerable potential to streamline ecological and biological workflows, with a growing number of datasets and models becoming available to the research community. However, these resources focus predominantly on evaluation using machine learning metrics, with relatively little emphasis on how their application impacts downstream analysis. We argue that models should be evaluated using application-specific metrics that directly represent model performance in the context of its final use case. To support this argument, we present two disparate case studies: (1) estimating chimpanzee abundance and density with camera trap distance sampling when using a video-based behaviour classifier and (2) estimating head rotation in pigeons using a 3D posture estimator. We show that even models with strong machine learning performance (e.g., 87% mAP) can yield data that leads to discrepancies in abundance estimates compared to expert-derived data. Similarly, the highest-performing models for posture estimation do not produce the most accurate inferences of gaze direction in pigeons. Motivated by these findings, we call for researchers to integrate application-specific metrics in ecological/biological datasets, allowing for models to be benchmarked in the context of their downstream application and to facilitate better integration of models into application workflows.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation</title>
<link>https://arxiv.org/abs/2505.02830</link>
<guid>https://arxiv.org/abs/2505.02830</guid>
<content:encoded><![CDATA[
arXiv:2505.02830v1 Announce Type: new 
Abstract: Chest X-rays (CXRs) are the most frequently performed imaging examinations in clinical settings. Recent advancements in Large Multimodal Models (LMMs) have enabled automated CXR interpretation, enhancing diagnostic accuracy and efficiency. However, despite their strong visual understanding, current Medical LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level understanding and interaction, and (2) Limited accuracy and interpretability due to single-step reasoning. In this paper, we empower MLMMs with anatomy-centric reasoning capabilities to enhance their interactivity and explainability. Specifically, we first propose an Anatomical Ontology-Guided Reasoning (AOR) framework, which centers on cross-modal region-level information to facilitate multi-step reasoning. Next, under the guidance of expert physicians, we develop AOR-Instruction, a large instruction dataset for MLMMs training. Our experiments demonstrate AOR's superior performance in both VQA and report generation tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</title>
<link>https://arxiv.org/abs/2505.02831</link>
<guid>https://arxiv.org/abs/2505.02831</guid>
<content:encoded><![CDATA[
arXiv:2505.02831v1 Announce Type: new 
Abstract: Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance generation quality of the diffusion transformers. However, existing approaches necessitate to either introduce an additional and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation A}lignment (SRA), a simple yet straightforward method that obtain representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in earlier layer with higher noise to that in later layer with lower noise to progressively enhance the overall representation learning during only generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that heavily dependent on powerful external representation priors.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.02835</link>
<guid>https://arxiv.org/abs/2505.02835</guid>
<content:encoded><![CDATA[
arXiv:2505.02835v1 Announce Type: new 
Abstract: Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation</title>
<link>https://arxiv.org/abs/2505.02836</link>
<guid>https://arxiv.org/abs/2505.02836</guid>
<content:encoded><![CDATA[
arXiv:2505.02836v1 Announce Type: new 
Abstract: Synthesizing interactive 3D scenes from text is essential for gaming, virtual reality, and embodied AI. However, existing methods face several challenges. Learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. While large language models (LLMs) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. Our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that LLMs lack. To this end, we introduce Scenethesis, a training-free agentic framework that integrates LLM-based scene planning with vision-guided layout refinement. Given a text prompt, Scenethesis first employs an LLM to draft a coarse layout. A vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. Next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. Finally, a judge module verifies spatial coherence. Comprehensive experiments show that Scenethesis generates diverse, realistic, and physically plausible 3D interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied AI research.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation</title>
<link>https://arxiv.org/abs/2505.01456</link>
<guid>https://arxiv.org/abs/2505.01456</guid>
<content:encoded><![CDATA[
arXiv:2505.01456v1 Announce Type: cross 
Abstract: LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Granularity Multimodal Retrieval Framework for Multimodal Document Tasks</title>
<link>https://arxiv.org/abs/2505.01457</link>
<guid>https://arxiv.org/abs/2505.01457</guid>
<content:encoded><![CDATA[
arXiv:2505.01457v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) systems have predominantly focused on text-based retrieval, limiting their effectiveness in handling visually-rich documents that encompass text, images, tables, and charts. To bridge this gap, we propose a unified multi-granularity multimodal retrieval framework tailored for two benchmark tasks: MMDocIR and M2KR. Our approach integrates hierarchical encoding strategies, modality-aware retrieval mechanisms, and reranking modules to effectively capture and utilize the complex interdependencies between textual and visual modalities. By leveraging off-the-shelf vision-language models and implementing a training-free hybridretrieval strategy, our framework demonstrates robust performance without the need for task-specific fine-tuning. Experimental evaluations reveal that incorporating layout-aware search and reranking modules significantly enhances retrieval accuracy, achieving a top performance score of 65.56. This work underscores the potential of scalable and reproducible solutions in advancing multimodal document retrieval systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering</title>
<link>https://arxiv.org/abs/2505.01476</link>
<guid>https://arxiv.org/abs/2505.01476</guid>
<content:encoded><![CDATA[
arXiv:2505.01476v1 Announce Type: cross 
Abstract: Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an input image with respect to normal samples. Either by reconstructing normal counterparts (reconstruction-based) or by learning an image feature embedding space (embedding-based), existing approaches fundamentally rely on image-level or feature-level matching to derive anomaly scores. Often, such a matching process is inaccurate yet overlooked, leading to sub-optimal detection. To address this issue, we introduce the concept of cost filtering, borrowed from classical matching tasks, such as depth and flow estimation, into the UAD problem. We call this approach {\em CostFilter-AD}. Specifically, we first construct a matching cost volume between the input and normal samples, comprising two spatial dimensions and one matching dimension that encodes potential matches. To refine this, we propose a cost volume filtering network, guided by the input observation as an attention query across multiple feature layers, which effectively suppresses matching noise while preserving edge structures and capturing subtle anomalies. Designed as a generic post-processing plug-in, CostFilter-AD can be integrated with either reconstruction-based or embedding-based methods. Extensive experiments on MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for both single- and multi-class UAD tasks. Code and models will be released at https://github.com/ZHE-SAPI/CostFilter-AD.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth</title>
<link>https://arxiv.org/abs/2505.01638</link>
<guid>https://arxiv.org/abs/2505.01638</guid>
<content:encoded><![CDATA[
arXiv:2505.01638v1 Announce Type: cross 
Abstract: High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs) typically requires multimodal sensing - especially RGB and thermal imagery - which increases hardware cost and power consumption. This paper introduces SAM-TIFF, a novel teacher-student distillation framework for pixel-level wildfire temperature prediction and segmentation using RGB input only. A multimodal teacher network trained on paired RGB-Thermal imagery and radiometric TIFF ground truth distills knowledge to a unimodal RGB student network, enabling thermal-sensor-free inference. Segmentation supervision is generated using a hybrid approach of segment anything (SAM)-guided mask generation, and selection via TOPSIS, along with Canny edge detection and Otsu's thresholding pipeline for automatic point prompt selection. Our method is the first to perform per-pixel temperature regression from RGB UAV data, demonstrating strong generalization on the recent FLAME 3 dataset. This work lays the foundation for lightweight, cost-effective UAV-based wildfire monitoring systems without thermal sensors.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual-Task Synergy-Driven Generalization Framework for Pancreatic Cancer Segmentation in CT Scans</title>
<link>https://arxiv.org/abs/2505.01644</link>
<guid>https://arxiv.org/abs/2505.01644</guid>
<content:encoded><![CDATA[
arXiv:2505.01644v1 Announce Type: cross 
Abstract: Pancreatic cancer, characterized by its notable prevalence and mortality rates, demands accurate lesion delineation for effective diagnosis and therapeutic interventions. The generalizability of extant methods is frequently compromised due to the pronounced variability in imaging and the heterogeneous characteristics of pancreatic lesions, which may mimic normal tissues and exhibit significant inter-patient variability. Thus, we propose a generalization framework that synergizes pixel-level classification and regression tasks, to accurately delineate lesions and improve model stability. This framework not only seeks to align segmentation contours with actual lesions but also uses regression to elucidate spatial relationships between diseased and normal tissues, thereby improving tumor localization and morphological characterization. Enhanced by the reciprocal transformation of task outputs, our approach integrates additional regression supervision within the segmentation context, bolstering the model's generalization ability from a dual-task perspective. Besides, dual self-supervised learning in feature spaces and output spaces augments the model's representational capability and stability across different imaging views. Experiments on 594 samples composed of three datasets with significant imaging differences demonstrate that our generalized pancreas segmentation results comparable to mainstream in-domain validation performance (Dice: 84.07%). More importantly, it successfully improves the results of the highly challenging cross-lesion generalized pancreatic cancer segmentation task by 9.51%. Thus, our model constitutes a resilient and efficient foundational technological support for pancreatic disease management and wider medical applications. The codes will be released at https://github.com/SJTUBME-QianLab/Dual-Task-Seg.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAGAR: Retrieval Augment Personalized Image Generation Guided by Recommendation</title>
<link>https://arxiv.org/abs/2505.01657</link>
<guid>https://arxiv.org/abs/2505.01657</guid>
<content:encoded><![CDATA[
arXiv:2505.01657v1 Announce Type: cross 
Abstract: Personalized image generation is crucial for improving the user experience, as it renders reference images into preferred ones according to user visual preferences. Although effective, existing methods face two main issues. First, existing methods treat all items in the user historical sequence equally when extracting user preferences, overlooking the varying semantic similarities between historical items and the reference item. Disproportionately high weights for low-similarity items distort users' visual preferences for the reference item. Second, existing methods heavily rely on consistency between generated and reference images to optimize the generation, which leads to underfitting user preferences and hinders personalization. To address these issues, we propose Retrieval Augment Personalized Image GenerAtion guided by Recommendation (RAGAR). Our approach uses a retrieval mechanism to assign different weights to historical items according to their similarities to the reference item, thereby extracting more refined users' visual preferences for the reference item. Then we introduce a novel rank task based on the multi-modal ranking model to optimize the personalization of the generated images instead of forcing depend on consistency. Extensive experiments and human evaluations on three real-world datasets demonstrate that RAGAR achieves significant improvements in both personalization and semantic metrics compared to five baselines.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations</title>
<link>https://arxiv.org/abs/2505.01670</link>
<guid>https://arxiv.org/abs/2505.01670</guid>
<content:encoded><![CDATA[
arXiv:2505.01670v1 Announce Type: cross 
Abstract: This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that the brain signals of the subjects can be aligned in this common space during training to form a semantically aligned common brain. This is leveraged to demonstrate that aligning subject-specific lightweight modules to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios. We evaluate our methods on different datasets, demonstrating that the common space is subject and dataset-agnostic.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.01709</link>
<guid>https://arxiv.org/abs/2505.01709</guid>
<content:encoded><![CDATA[
arXiv:2505.01709v1 Announce Type: cross 
Abstract: Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLOG-CD: Curriculum Learning based on Oscillating Granularity of Class Decomposed Medical Image Classification</title>
<link>https://arxiv.org/abs/2505.01741</link>
<guid>https://arxiv.org/abs/2505.01741</guid>
<content:encoded><![CDATA[
arXiv:2505.01741v1 Announce Type: cross 
Abstract: Curriculum learning strategies have been proven to be effective in various applications and have gained significant interest in the field of machine learning. It has the ability to improve the final model's performance and accelerate the training process. However, in the medical imaging domain, data irregularities can make the recognition task more challenging and usually result in misclassification between the different classes in the dataset. Class-decomposition approaches have shown promising results in solving such a problem by learning the boundaries within the classes of the data set. In this paper, we present a novel convolutional neural network (CNN) training method based on the curriculum learning strategy and the class decomposition approach, which we call CLOG-CD, to improve the performance of medical image classification. We evaluated our method on four different imbalanced medical image datasets, such as Chest X-ray (CXR), brain tumour, digital knee X-ray, and histopathology colorectal cancer (CRC). CLOG-CD utilises the learnt weights from the decomposition granularity of the classes, and the training is accomplished from descending to ascending order (i.e., anti-curriculum technique). We also investigated the classification performance of our proposed method based on different acceleration factors and pace function curricula. We used two pre-trained networks, ResNet-50 and DenseNet-121, as the backbone for CLOG-CD. The results with ResNet-50 show that CLOG-CD has the ability to improve classification performance with an accuracy of 96.08% for the CXR dataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee X-ray, and 99.17% for the CRC dataset, compared to other training strategies. In addition, with DenseNet-121, CLOG-CD has achieved 94.86%, 94.63%, 76.19%, and 99.45% for CXR, brain tumour, digital knee X-ray, and CRC datasets, respectively
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LensNet: An End-to-End Learning Framework for Empirical Point Spread Function Modeling and Lensless Imaging Reconstruction</title>
<link>https://arxiv.org/abs/2505.01755</link>
<guid>https://arxiv.org/abs/2505.01755</guid>
<content:encoded><![CDATA[
arXiv:2505.01755v1 Announce Type: cross 
Abstract: Lensless imaging stands out as a promising alternative to conventional lens-based systems, particularly in scenarios demanding ultracompact form factors and cost-effective architectures. However, such systems are fundamentally governed by the Point Spread Function (PSF), which dictates how a point source contributes to the final captured signal. Traditional lensless techniques often require explicit calibrations and extensive pre-processing, relying on static or approximate PSF models. These rigid strategies can result in limited adaptability to real-world challenges, including noise, system imperfections, and dynamic scene variations, thus impeding high-fidelity reconstruction. In this paper, we propose LensNet, an end-to-end deep learning framework that integrates spatial-domain and frequency-domain representations in a unified pipeline. Central to our approach is a learnable Coded Mask Simulator (CMS) that enables dynamic, data-driven estimation of the PSF during training, effectively mitigating the shortcomings of fixed or sparsely calibrated kernels. By embedding a Wiener filtering component, LensNet refines global structure and restores fine-scale details, thus alleviating the dependency on multiple handcrafted pre-processing steps. Extensive experiments demonstrate LensNet's robust performance and superior reconstruction quality compared to state-of-the-art methods, particularly in preserving high-frequency details and attenuating noise. The proposed framework establishes a novel convergence between physics-based modeling and data-driven learning, paving the way for more accurate, flexible, and practical lensless imaging solutions for applications ranging from miniature sensors to medical diagnostics. The link of code is https://github.com/baijiesong/Lensnet.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Filtered Backprojection by Learnable Interpolation Network</title>
<link>https://arxiv.org/abs/2505.01768</link>
<guid>https://arxiv.org/abs/2505.01768</guid>
<content:encoded><![CDATA[
arXiv:2505.01768v1 Announce Type: cross 
Abstract: Accurate reconstruction of computed tomography (CT) images is crucial in medical imaging field. However, there are unavoidable interpolation errors in the backprojection step of the conventional reconstruction methods, i.e., filtered-back-projection based methods, which are detrimental to the accurate reconstruction. In this study, to address this issue, we propose a novel deep learning model, named Leanable-Interpolation-based FBP or LInFBP shortly, to enhance the reconstructed CT image quality, which achieves learnable interpolation in the backprojection step of filtered backprojection (FBP) and alleviates the interpolation errors. Specifically, in the proposed LInFBP, we formulate every local piece of the latent continuous function of discrete sinogram data as a linear combination of selected basis functions, and learn this continuous function by exploiting a deep network to predict the linear combination coefficients. Then, the learned latent continuous function is exploited for interpolation in backprojection step, which first time takes the advantage of deep learning for the interpolation in FBP. Extensive experiments, which encompass diverse CT scenarios, demonstrate the effectiveness of the proposed LInFBP in terms of enhanced reconstructed image quality, plug-and-play ability and generalization capability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement</title>
<link>https://arxiv.org/abs/2505.01831</link>
<guid>https://arxiv.org/abs/2505.01831</guid>
<content:encoded><![CDATA[
arXiv:2505.01831v1 Announce Type: cross 
Abstract: High-quality fundus images provide essential anatomical information for clinical screening and ophthalmic disease diagnosis. Yet, due to hardware limitations, operational variability, and patient compliance, fundus images often suffer from low resolution and signal-to-noise ratio. Recent years have witnessed promising progress in fundus image enhancement. However, existing works usually focus on restoring structural details or global characteristics of fundus images, lacking a unified image enhancement framework to recover comprehensive multi-scale information. Moreover, few methods pinpoint the target of image enhancement, e.g., lesions, which is crucial for medical image-based diagnosis. To address these challenges, we propose a multi-scale target-aware representation learning framework (MTRL-FIE) for efficient fundus image enhancement. Specifically, we propose a multi-scale feature encoder (MFE) that employs wavelet decomposition to embed both low-frequency structural information and high-frequency details. Next, we design a structure-preserving hierarchical decoder (SHD) to fuse multi-scale feature embeddings for real fundus image restoration. SHD integrates hierarchical fusion and group attention mechanisms to achieve adaptive feature fusion while retaining local structural smoothness. Meanwhile, a target-aware feature aggregation (TFA) module is used to enhance pathological regions and reduce artifacts. Experimental results on multiple fundus image datasets demonstrate the effectiveness and generalizability of MTRL-FIE for fundus image enhancement. Compared to state-of-the-art methods, MTRL-FIE achieves superior enhancement performance with a more lightweight architecture. Furthermore, our approach generalizes to other ophthalmic image processing tasks without supervised fine-tuning, highlighting its potential for clinical applications.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2</title>
<link>https://arxiv.org/abs/2505.01854</link>
<guid>https://arxiv.org/abs/2505.01854</guid>
<content:encoded><![CDATA[
arXiv:2505.01854v1 Announce Type: cross 
Abstract: Manual annotation of volumetric medical images, such as magnetic resonance imaging (MRI) and computed tomography (CT), is a labor-intensive and time-consuming process. Recent advancements in foundation models for video object segmentation, such as Segment Anything Model 2 (SAM 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. However, the performance of SAM 2 in this context varies. Our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. To address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. We evaluate SLM-SAM 2 on three public datasets covering organs, bones, and muscles across MRI and CT modalities. We show that the proposed method markedly outperforms the default SAM 2, achieving average Dice Similarity Coefficient improvement of 0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger resistance to over-propagation, making a notable step toward more accurate automated annotation of medical images for segmentation model development.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network</title>
<link>https://arxiv.org/abs/2505.01880</link>
<guid>https://arxiv.org/abs/2505.01880</guid>
<content:encoded><![CDATA[
arXiv:2505.01880v1 Announce Type: cross 
Abstract: Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. Specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. In this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. In addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. Finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. Extensive experiments show that the proposed LOCO achieves SOTA performance on three public benchmarks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images</title>
<link>https://arxiv.org/abs/2505.01884</link>
<guid>https://arxiv.org/abs/2505.01884</guid>
<content:encoded><![CDATA[
arXiv:2505.01884v1 Announce Type: cross 
Abstract: Inland water body segmentation from Synthetic Aperture Radar (SAR) images is an important task needed for several applications, such as flood mapping. While SAR sensors capture data in all-weather conditions as high-resolution images, differentiating water and water-like surfaces from SAR images is not straightforward. Inland water bodies, such as large river basins, have complex geometry, which adds to the challenge of segmentation. U-Net is a widely used deep learning model for land-water segmentation of SAR images. In practice, manual annotation is often used to generate the corresponding water masks as ground truth. Manual annotation of the images is prone to label noise owing to data poisoning attacks, especially due to complex geometry. In this work, we simulate manual errors in the form of adversarial attacks on the U-Net model and study the robustness of the model to human errors in annotation. Our results indicate that U-Net can tolerate a certain level of corruption before its performance drops significantly. This finding highlights the crucial role that the quality of manual annotations plays in determining the effectiveness of the segmentation model. The code and the new dataset, along with adversarial examples for robust training, are publicly available. (Github link - https://github.com/GVCL/IWSeg-SAR-Poison.git)
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OT-Talk: Animating 3D Talking Head with Optimal Transportation</title>
<link>https://arxiv.org/abs/2505.01932</link>
<guid>https://arxiv.org/abs/2505.01932</guid>
<content:encoded><![CDATA[
arXiv:2505.01932v1 Announce Type: cross 
Abstract: Animating 3D head meshes using audio inputs has significant applications in AR/VR, gaming, and entertainment through 3D avatars. However, bridging the modality gap between speech signals and facial dynamics remains a challenge, often resulting in incorrect lip syncing and unnatural facial movements. To address this, we propose OT-Talk, the first approach to leverage optimal transportation to optimize the learning model in talking head animation. Building on existing learning frameworks, we utilize a pre-trained Hubert model to extract audio features and a transformer model to process temporal sequences. Unlike previous methods that focus solely on vertex coordinates or displacements, we introduce Chebyshev Graph Convolution to extract geometric features from triangulated meshes. To measure mesh dissimilarities, we go beyond traditional mesh reconstruction errors and velocity differences between adjacent frames. Instead, we represent meshes as probability measures and approximate their surfaces. This allows us to leverage the sliced Wasserstein distance for modeling mesh variations. This approach facilitates the learning of smooth and accurate facial motions, resulting in coherent and natural facial animations. Our experiments on two public audio-mesh datasets demonstrate that our method outperforms state-of-the-art techniques both quantitatively and qualitatively in terms of mesh reconstruction accuracy and temporal alignment. In addition, we conducted a user perception study with 20 volunteers to further assess the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Always Skip Attention</title>
<link>https://arxiv.org/abs/2505.01996</link>
<guid>https://arxiv.org/abs/2505.01996</guid>
<content:encoded><![CDATA[
arXiv:2505.01996v1 Announce Type: cross 
Abstract: We highlight a curious empirical result within modern Vision Transformers (ViTs). Specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. This is in contrast to other elements of a ViT that continue to exhibit good performance (albeit suboptimal) when skip connections are removed. Further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (\eg, CNNs) exhibiting good performance in their absence. In this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. Additionally, we propose Token Graying -- a simple yet effective complement (to skip connections) that further improves the condition of input tokens. We validate our approach in both supervised and self-supervised training methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Image Resolution Quality Metric (HIRQM):A Comprehensive Perceptual Image Quality Assessment Framework</title>
<link>https://arxiv.org/abs/2505.02001</link>
<guid>https://arxiv.org/abs/2505.02001</guid>
<content:encoded><![CDATA[
arXiv:2505.02001v1 Announce Type: cross 
Abstract: Traditional image quality assessment metrics like Mean Squared Error and Structural Similarity Index often fail to reflect perceptual quality under complex distortions. We propose the Hybrid Image Resolution Quality Metric (HIRQM), integrating statistical, multi-scale, and deep learning-based methods for a comprehensive quality evaluation. HIRQM combines three components: Probability Density Function for local pixel distribution analysis, Multi-scale Feature Similarity for structural integrity across resolutions, and Hierarchical Deep Image Features using a pre-trained VGG16 network for semantic alignment with human perception. A dynamic weighting mechanism adapts component contributions based on image characteristics like brightness and variance, enhancing flexibility across distortion types. Our contributions include a unified metric and dynamic weighting for better perceptual alignment. Evaluated on TID2013 and LIVE datasets, HIRQM achieves Pearson and Spearman correlations of 0.92 and 0.90, outperforming traditional metrics. It excels in handling noise, blur, and compression artifacts, making it valuable for image processing applications like compression and restoration.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.02052</link>
<guid>https://arxiv.org/abs/2505.02052</guid>
<content:encoded><![CDATA[
arXiv:2505.02052v1 Announce Type: cross 
Abstract: Sensor-based human activity recognition (HAR) has predominantly focused on Inertial Measurement Units and vision data, often overlooking the capabilities unique to pressure sensors, which capture subtle body dynamics and shifts in the center of mass. Despite their potential for postural and balance-based activities, pressure sensors remain underutilized in the HAR domain due to limited datasets. To bridge this gap, we propose to exploit generative foundation models with pressure-specific HAR techniques. Specifically, we present a bidirectional Text$\times$Pressure model that uses generative foundation models to interpret pressure data as natural language. TxP accomplishes two tasks: (1) Text2Pressure, converting activity text descriptions into pressure sequences, and (2) Pressure2Text, generating activity descriptions and classifications from dynamic pressure maps. Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on our synthetic PressLang dataset, containing over 81,100 text-pressure pairs. Validated on real-world data for activities such as yoga and daily tasks, TxP provides novel approaches to data augmentation and classification grounded in atomic actions. This consequently improved HAR performance by up to 12.4\% in macro F1 score compared to the state-of-the-art, advancing pressure-based HAR with broader applications and deeper insights into human movement.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations</title>
<link>https://arxiv.org/abs/2505.02094</link>
<guid>https://arxiv.org/abs/2505.02094</guid>
<content:encoded><![CDATA[
arXiv:2505.02094v1 Announce Type: cross 
Abstract: We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora</title>
<link>https://arxiv.org/abs/2505.02147</link>
<guid>https://arxiv.org/abs/2505.02147</guid>
<content:encoded><![CDATA[
arXiv:2505.02147v1 Announce Type: cross 
Abstract: Herb classification presents a critical challenge in botanical research, particularly in regions with rich biodiversity such as Nepal. This study introduces a novel deep learning approach for classifying 60 different herb species using Convolutional Neural Networks (CNNs) and transfer learning techniques. Using a manually curated dataset of 12,000 herb images, we developed a robust machine learning model that addresses existing limitations in herb recognition methodologies. Our research employed multiple model architectures, including DenseNet121, 50-layer Residual Network (ResNet50), 16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2, and Vision Transformer (VIT), with DenseNet121 ultimately demonstrating superior performance. Data augmentation and regularization techniques were applied to mitigate overfitting and enhance the generalizability of the model. This work advances herb classification techniques, preserving traditional botanical knowledge and promoting sustainable herb utilization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid Carcinoma Classification in Ultrasound Images</title>
<link>https://arxiv.org/abs/2505.02211</link>
<guid>https://arxiv.org/abs/2505.02211</guid>
<content:encoded><![CDATA[
arXiv:2505.02211v1 Announce Type: cross 
Abstract: Heterogeneous morphological features and data imbalance pose significant challenges in rare thyroid carcinoma classification using ultrasound imaging. To address this issue, we propose a novel multitask learning framework, Channel-Spatial Attention Synergy Network (CSASN), which integrates a dual-branch feature extractor - combining EfficientNet for local spatial encoding and ViT for global semantic modeling, with a cascaded channel-spatial attention refinement module. A residual multiscale classifier and dynamically weighted loss function further enhance classification stability and accuracy. Trained on a multicenter dataset comprising more than 2000 patients from four clinical institutions, our framework leverages a residual multiscale classifier and dynamically weighted loss function to enhance classification stability and accuracy. Extensive ablation studies demonstrate that each module contributes significantly to model performance, particularly in recognizing rare subtypes such as FTC and MTC carcinomas. Experimental results show that CSASN outperforms existing single-stream CNN or Transformer-based models, achieving a superior balance between precision and recall under class-imbalanced conditions. This framework provides a promising strategy for AI-assisted thyroid cancer diagnosis.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition</title>
<link>https://arxiv.org/abs/2505.02304</link>
<guid>https://arxiv.org/abs/2505.02304</guid>
<content:encoded><![CDATA[
arXiv:2505.02304v1 Announce Type: cross 
Abstract: Sign language recognition (SLR) faces fundamental challenges in creating accurate annotations due to the inherent complexity of simultaneous manual and non-manual signals. To the best of our knowledge, this is the first work to integrate generative large language models (LLMs) into SLR tasks. We propose a novel Generative Sign-description Prompts Multi-positive Contrastive learning (GSP-MC) method that leverages retrieval-augmented generation (RAG) with domain-specific LLMs, incorporating multi-step prompt engineering and expert-validated sign language corpora to produce precise multipart descriptions. The GSP-MC method also employs a dual-encoder architecture to bidirectionally align hierarchical skeleton features with multiple text descriptions (global, synonym, and part level) through probabilistic matching. Our approach combines global and part-level losses, optimizing KL divergence to ensure robust alignment across all relevant text-skeleton pairs while capturing both sign-level semantics and detailed part dynamics. Experiments demonstrate state-of-the-art performance against existing methods on the Chinese SLR500 (reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's cross-lingual effectiveness highlight its potential for developing inclusive communication technologies.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation</title>
<link>https://arxiv.org/abs/2505.02350</link>
<guid>https://arxiv.org/abs/2505.02350</guid>
<content:encoded><![CDATA[
arXiv:2505.02350v1 Announce Type: cross 
Abstract: Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using sparse ellipsoidal radial basis function networks, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represent the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding code is publicly available at https://github.com/lianbobo/SE-RBFNet.git.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks</title>
<link>https://arxiv.org/abs/2505.02369</link>
<guid>https://arxiv.org/abs/2505.02369</guid>
<content:encoded><![CDATA[
arXiv:2505.02369v1 Announce Type: cross 
Abstract: Generalizing well in deep neural networks remains a core challenge, particularly due to their tendency to converge to sharp minima that degrade robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking flatter minima but perturbs parameters using the full gradient, which can include statistically insignificant directions. We propose ZSharp, a simple yet effective extension to SAM that applies layer-wise Z-score normalization followed by percentile-based filtering to retain only statistically significant gradient components. This selective perturbation aligns updates with curvature-sensitive directions, enhancing generalization without requiring architectural changes. ZSharp introduces only one additional hyperparameter, the percentile threshold, and remains fully compatible with existing SAM variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet, VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and its variants in test accuracy, particularly on deeper and transformer-based models. These results demonstrate that ZSharp is a principled and lightweight improvement for sharpness-aware optimization.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Arbitrary-Modal Fusion Network for Volumetric Cranial Nerves Tract Segmentation</title>
<link>https://arxiv.org/abs/2505.02385</link>
<guid>https://arxiv.org/abs/2505.02385</guid>
<content:encoded><![CDATA[
arXiv:2505.02385v1 Announce Type: cross 
Abstract: The segmentation of cranial nerves (CNs) tract provides a valuable quantitative tool for the analysis of the morphology and trajectory of individual CNs. Multimodal CNs tract segmentation networks, e.g., CNTSeg, which combine structural Magnetic Resonance Imaging (MRI) and diffusion MRI, have achieved promising segmentation performance. However, it is laborious or even infeasible to collect complete multimodal data in clinical practice due to limitations in equipment, user privacy, and working conditions. In this work, we propose a novel arbitrary-modal fusion network for volumetric CNs tract segmentation, called CNTSeg-v2, which trains one model to handle different combinations of available modalities. Instead of directly combining all the modalities, we select T1-weighted (T1w) images as the primary modality due to its simplicity in data acquisition and contribution most to the results, which supervises the information selection of other auxiliary modalities. Our model encompasses an Arbitrary-Modal Collaboration Module (ACM) designed to effectively extract informative features from other auxiliary modalities, guided by the supervision of T1w images. Meanwhile, we construct a Deep Distance-guided Multi-stage (DDM) decoder to correct small errors and discontinuities through signed distance maps to improve segmentation accuracy. We evaluate our CNTSeg-v2 on the Human Connectome Project (HCP) dataset and the clinical Multi-shell Diffusion MRI (MDM) dataset. Extensive experimental results show that our CNTSeg-v2 achieves state-of-the-art segmentation performance, outperforming all competing methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch</title>
<link>https://arxiv.org/abs/2505.02396</link>
<guid>https://arxiv.org/abs/2505.02396</guid>
<content:encoded><![CDATA[
arXiv:2505.02396v1 Announce Type: cross 
Abstract: Pneumonia Diagnosis, though it is crucial for an effective treatment, it can be hampered by uncertainty. This uncertainty starts to arise due to some factors like atypical presentations, limitations of diagnostic tools such as chest X-rays, and the presence of co-existing respiratory conditions. This research proposes one of the supervised learning methods, CNN. Using MobileNetV2 as the pre-trained one with ResNet101V2 architecture and using Keras API as the built from scratch model, for identifying lung diseases especially pneumonia. The datasets used in this research were obtained from the website through Kaggle. The result shows that by implementing CNN MobileNetV2 and CNN from scratch the result is promising. While validating data, MobileNetV2 performs with stability and minimal overfitting, while the training accuracy increased to 84.87% later it slightly decreased to 78.95%, with increasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is more stable. Although it takes more time to train each epoch. Meanwhile, after the 10th epoch, the Scratch model displayed more instability and overfitting despite having higher validation accuracy, training accuracy decreased significantly to 78.12% and the validation loss increased from 0.5698 to 1.1809. With these results, ResNet101V2 offers stability, and the Scratch model offers high accuracy.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Commonsense Scene Composition on Belief Scene Graphs</title>
<link>https://arxiv.org/abs/2505.02405</link>
<guid>https://arxiv.org/abs/2505.02405</guid>
<content:encoded><![CDATA[
arXiv:2505.02405v1 Announce Type: cross 
Abstract: This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation</title>
<link>https://arxiv.org/abs/2505.02476</link>
<guid>https://arxiv.org/abs/2505.02476</guid>
<content:encoded><![CDATA[
arXiv:2505.02476v1 Announce Type: cross 
Abstract: The validation of LiDAR-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. Virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. In contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. Existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. However, these methods do not consider validation and remain limited in controllability because they rely on empirical data. We solve these limitations by proposing Point Cloud Recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. Thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. We show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. By providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction</title>
<link>https://arxiv.org/abs/2505.02628</link>
<guid>https://arxiv.org/abs/2505.02628</guid>
<content:encoded><![CDATA[
arXiv:2505.02628v1 Announce Type: cross 
Abstract: Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in the medical field, while the high radiation exposure required for high-quality imaging raises significant concerns, particularly for vulnerable populations. Sparse-view reconstruction reduces radiation by using fewer X-ray projections while maintaining image quality, yet existing methods face challenges such as high computational demands and poor generalizability to different datasets. To overcome these limitations, we propose DeepSparse, the first foundation model for sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional Cross-Scale Embedding), a novel network that integrates multi-view 2D features and multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View Sampling Pretraining) framework, which pretrains the model on large datasets with both sparse-view and dense-view projections, and a two-step finetuning strategy to adapt and refine the model for new datasets. Extensive experiments and ablation studies demonstrate that our proposed DeepSparse achieves superior reconstruction quality compared to state-of-the-art methods, paving the way for safer and more efficient CBCT imaging.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter</title>
<link>https://arxiv.org/abs/2505.02664</link>
<guid>https://arxiv.org/abs/2505.02664</guid>
<content:encoded><![CDATA[
arXiv:2505.02664v1 Announce Type: cross 
Abstract: Grasp pose detection in cluttered, real-world environments remains a significant challenge due to noisy and incomplete sensory data combined with complex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0) method, a lightweight yet highly effective hypothesis-and-test robotics grasping framework which leverages an ensemble of Graph Neural Networks for efficient geometric reasoning from point cloud data. Building on the success of GtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp detection but was limited by assumptions of complete, noise-free point clouds and 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to efficiently produce 7-Dof grasp candidates. Candidates are assessed with an ensemble Graph Neural Network model which includes points within the gripper jaws (inside points) and surrounding contextual points (outside points). This improved representation boosts grasp detection performance over previous methods using the same generator. GtG 2.0 shows up to a 35% improvement in Average Precision on the GraspNet-1Billion benchmark compared to hypothesis-and-test and Graph Neural Network-based methods, ranking it among the top three frameworks. Experiments with a 3-Dof Delta Parallel robot and Kinect-v1 camera show a success rate of 91% and a clutter completion rate of 100%, demonstrating its flexibility and reliability.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-View Learning with Context-Guided Receptance for Image Denoising</title>
<link>https://arxiv.org/abs/2505.02705</link>
<guid>https://arxiv.org/abs/2505.02705</guid>
<content:encoded><![CDATA[
arXiv:2505.02705v1 Announce Type: cross 
Abstract: Image denoising is essential in low-level vision applications such as photography and automated driving. Existing methods struggle with distinguishing complex noise patterns in real-world scenes and consume significant computational resources due to reliance on Transformer-based models. In this work, the Context-guided Receptance Weighted Key-Value (\M) model is proposed, combining enhanced multi-view feature integration with efficient sequence modeling. Our approach introduces the Context-guided Token Shift (CTS) paradigm, which effectively captures local spatial dependencies and enhance the model's ability to model real-world noise distributions. Additionally, the Frequency Mix (FMix) module extracting frequency-domain features is designed to isolate noise in high-frequency spectra, and is integrated with spatial representations through a multi-view learning process. To improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is adopted, enabling full pixel-sequence interaction with linear complexity while overcoming the causal selection constraints. The model is validated on multiple real-world image denoising datasets, outperforming the existing state-of-the-art methods quantitatively and reducing inference time up to 40\%. Qualitative results further demonstrate the ability of our model to restore fine details in various scenes.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TWIST: Teleoperated Whole-Body Imitation System</title>
<link>https://arxiv.org/abs/2505.02833</link>
<guid>https://arxiv.org/abs/2505.02833</guid>
<content:encoded><![CDATA[
arXiv:2505.02833v1 Announce Type: cross 
Abstract: Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: https://humanoid-teleop.github.io
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition</title>
<link>https://arxiv.org/abs/2308.04369</link>
<guid>https://arxiv.org/abs/2308.04369</guid>
<content:encoded><![CDATA[
arXiv:2308.04369v3 Announce Type: replace 
Abstract: Event camera-based pattern recognition is a newly arising research topic in recent years. Current researchers usually transform the event streams into images, graphs, or voxels, and adopt deep neural networks for event-based classification. Although good performance can be achieved on simple event recognition datasets, however, their results may be still limited due to the following two issues. Firstly, they adopt spatial sparse event streams for recognition only, which may fail to capture the color and detailed texture information well. Secondly, they adopt either Spiking Neural Networks (SNN) for energy-efficient recognition with suboptimal results, or Artificial Neural Networks (ANN) for energy-intensive, high-performance recognition. However, seldom of them consider achieving a balance between these two aspects. In this paper, we formally propose to recognize patterns by fusing RGB frames and event streams simultaneously and propose a new RGB frame-event recognition framework to address the aforementioned issues. The proposed method contains four main modules, i.e., memory support Transformer network for RGB frame encoding, spiking neural network for raw event stream encoding, multi-modal bottleneck fusion module for RGB-Event feature aggregation, and prediction head. Due to the scarce of RGB-Event based classification dataset, we also propose a large-scale PokerEvent dataset which contains 114 classes, and 27102 frame-event pairs recorded using a DVS346 event camera. Extensive experiments on two RGB-Event based classification datasets fully validated the effectiveness of our proposed framework. We hope this work will boost the development of pattern recognition by fusing RGB frames and event streams. Both our dataset and source code of this work will be released at https://github.com/Event-AHU/SSTFormer
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Matching</title>
<link>https://arxiv.org/abs/2404.01249</link>
<guid>https://arxiv.org/abs/2404.01249</guid>
<content:encoded><![CDATA[
arXiv:2404.01249v3 Announce Type: replace 
Abstract: The paper proposes FireANTs, the first multi-scale Adaptive Riemannian Optimization algorithm for dense diffeomorphic image matching. One of the most critical and understudied aspects of diffeomorphic image matching algorithms are its highly ill-conditioned nature. We quantitatively capture the extent of ill-conditioning in a typical MRI matching task, motivating the need for an adaptive optimization algorithm for diffeomorphic matching. To this end, FireANTs generalizes the concept of momentum and adaptive estimates of the Hessian to mitigate this ill-conditioning in the non-Euclidean space of diffeomorphisms. Unlike common non-Euclidean manifolds, we also formalize considerations for multi-scale optimization of diffeomorphisms. Our rigorous mathematical results and operational contributions lead to a state-of-the-art dense matching algorithm that can be applied to generic image data with remarkable accuracy and robustness. We demonstrate consistent improvements in image matching performance across a spectrum of community-standard medical and biological correspondence matching challenges spanning a wide variety of image modalities, anatomies, resolutions, acquisition protocols, and preprocessing pipelines. This improvement is supplemented by 300x to 3200x speedup over existing CPU-based state-of-the-art algorithms. For the first time, we perform diffeomorphic matching of sub-micron mouse isocortex volumes at native resolution, and generate a 25{\mu}m mouse brain atlas in under 25 minutes. Our fast implementation also enables hyperparameter studies that were intractable with existing correspondence matching algorithms.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement</title>
<link>https://arxiv.org/abs/2404.02225</link>
<guid>https://arxiv.org/abs/2404.02225</guid>
<content:encoded><![CDATA[
arXiv:2404.02225v2 Announce Type: replace 
Abstract: We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework. It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. The key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOWA: Multiple-in-One Image Warping Model</title>
<link>https://arxiv.org/abs/2404.10716</link>
<guid>https://arxiv.org/abs/2404.10716</guid>
<content:encoded><![CDATA[
arXiv:2404.10716v3 Announce Type: replace 
Abstract: While recent image warping approaches achieved remarkable success on existing benchmarks, they still require training separate models for each specific task and cannot generalize well to different camera models or customized manipulations. To address diverse types of warping in practice, we propose a Multiple-in-One image WArping model (named MOWA) in this work. Specifically, we mitigate the difficulty of multi-task learning by disentangling the motion estimation at both the region level and pixel level. To further enable dynamic task-aware image warping, we introduce a lightweight point-based classifier that predicts the task type, serving as prompts to modulate the feature maps for more accurate estimation. To our knowledge, this is the first work that solves multiple practical warping tasks in one single model. Extensive experiments demonstrate that our MOWA, which is trained on six tasks for multiple-in-one image warping, outperforms state-of-the-art task-specific models across most tasks. Moreover, MOWA also exhibits promising potential to generalize into unseen scenes, as evidenced by cross-domain and zero-shot evaluations. The code and more visual results can be found on the project page: https://kangliao929.github.io/projects/mowa/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing person re-identification via Uncertainty Feature Fusion Method and Auto-weighted Measure Combination</title>
<link>https://arxiv.org/abs/2405.01101</link>
<guid>https://arxiv.org/abs/2405.01101</guid>
<content:encoded><![CDATA[
arXiv:2405.01101v5 Announce Type: replace 
Abstract: Person re-identification (Re-ID) is a challenging task that involves identifying the same person across different camera views in surveillance systems. Current methods usually rely on features from single-camera views, which can be limiting when dealing with multiple cameras and challenges such as changing viewpoints and occlusions. In this paper, a new approach is introduced that enhances the capability of ReID models through the Uncertain Feature Fusion Method (UFFM) and Auto-weighted Measure Combination (AMC). UFFM generates multi-view features using features extracted independently from multiple images to mitigate view bias. However, relying only on similarity based on multi-view features is limited because these features ignore the details represented in single-view features. Therefore, we propose the AMC method to generate a more robust similarity measure by combining various measures. Our method significantly improves Rank@1 accuracy and Mean Average Precision (mAP) when evaluated on person re-identification datasets. Combined with the BoT Baseline on challenging datasets, we achieve impressive results, with a 7.9% improvement in Rank@1 and a 12.1% improvement in mAP on the MSMT17 dataset. On the Occluded-DukeMTMC dataset, our method increases Rank@1 by 22.0% and mAP by 18.4%. Code is available: https://github.com/chequanghuy/Enhancing-Person-Re-Identification-via-UFFM-and-AMC
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh Generation</title>
<link>https://arxiv.org/abs/2405.13745</link>
<guid>https://arxiv.org/abs/2405.13745</guid>
<content:encoded><![CDATA[
arXiv:2405.13745v2 Announce Type: replace 
Abstract: Quadrilateral mesh generation plays a crucial role in numerical simulations within Computer-Aided Design and Engineering (CAD/E). Producing high-quality quadrangulation typically requires satisfying four key criteria. First, the quadrilateral mesh should closely align with principal curvature directions. Second, singular points should be strategically placed and effectively minimized. Third, the mesh should accurately conform to sharp feature edges. Lastly, quadrangulation results should exhibit robustness against noise and minor geometric variations. Existing methods generally involve first computing a regular cross field to represent quad element orientations across the surface, followed by extracting a quadrilateral mesh aligned closely with this cross field. A primary challenge with this approach is balancing the smoothness of the cross field with its alignment to pre-computed principal curvature directions, which are sensitive to small surface perturbations and often ill-defined in spherical or planar regions.
  To tackle this challenge, we propose NeurCross, a novel framework that simultaneously optimizes a cross field and a neural signed distance function (SDF), whose zero-level set serves as a proxy of the input shape. Our joint optimization is guided by three factors: faithful approximation of the optimized SDF surface to the input surface, alignment between the cross field and the principal curvature field derived from the SDF surface, and smoothness of the cross field. Acting as an intermediary, the neural SDF contributes in two essential ways. First, it provides an alternative, optimizable base surface exhibiting more regular principal curvature directions for guiding the cross field. Second, we leverage the Hessian matrix of the neural SDF to implicitly enforce cross field alignment with principal curvature directions...
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParallelEdits: Efficient Multi-object Image Editing</title>
<link>https://arxiv.org/abs/2406.00985</link>
<guid>https://arxiv.org/abs/2406.00985</guid>
<content:encoded><![CDATA[
arXiv:2406.00985v4 Announce Type: replace 
Abstract: Text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts. Despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges. A major challenge is making simultaneous edits across multiple objects or attributes. Applying these methods sequentially for multi-attribute edits increases computational demands and efficiency losses. In this paper, we address these challenges with significant contributions. Our main contribution is the development of ParallelEdits, a method that seamlessly manages simultaneous edits across multiple attributes. In contrast to previous approaches, ParallelEdits not only preserves the quality of single attribute edits but also significantly improves the performance of multitasking edits. This is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads. Additionally, we introduce the PIE-Bench++ dataset, an expansion of the original PIE-Bench dataset, to better support evaluating image-editing tasks involving multiple objects and attributes simultaneously. This dataset is a benchmark for evaluating text-driven image editing methods in multifaceted scenarios.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Implicit Optimization enables Robust Learnable Features for Deformable Image Registration</title>
<link>https://arxiv.org/abs/2406.07361</link>
<guid>https://arxiv.org/abs/2406.07361</guid>
<content:encoded><![CDATA[
arXiv:2406.07361v4 Announce Type: replace 
Abstract: Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time. However, existing DLIR methods forego many of the benefits and invariances of optimization methods. The lack of a task-specific inductive bias in DLIR methods leads to suboptimal performance, especially in the presence of domain shift. Our method aims to bridge this gap between statistical learning and optimization by explicitly incorporating optimization as a layer in a deep network. A deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver. This optimal warp is then used to minimize image and label alignment errors. By implicitly differentiating end-to-end through an iterative optimization solver, we explicitly exploit invariances of the correspondence matching problem induced by the optimization, while learning registration and label-aware features, and guaranteeing the warp functions to be a local minima of the registration objective in the feature space. Our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles. For the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining. End-to-end feature learning also facilitates interpretability of features and arbitrary test-time regularization, which is not possible with existing DLIR methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIGC++: Advanced Multi-Instance Generation Controller for Image Synthesis</title>
<link>https://arxiv.org/abs/2407.02329</link>
<guid>https://arxiv.org/abs/2407.02329</guid>
<content:encoded><![CDATA[
arXiv:2407.02329v3 Announce Type: replace 
Abstract: We introduce the Multi-Instance Generation (MIG) task, which focuses on generating multiple instances within a single image, each accurately placed at predefined positions with attributes such as category, color, and shape, strictly following user specifications. MIG faces three main challenges: avoiding attribute leakage between instances, supporting diverse instance descriptions, and maintaining consistency in iterative generation. To address attribute leakage, we propose the Multi-Instance Generation Controller (MIGC). MIGC generates multiple instances through a divide-and-conquer strategy, breaking down multi-instance shading into single-instance tasks with singular attributes, later integrated. To provide more types of instance descriptions, we developed MIGC++. MIGC++ allows attribute control through text \& images and position control through boxes \& masks. Lastly, we introduced the Consistent-MIG algorithm to enhance the iterative MIG ability of MIGC and MIGC++. This algorithm ensures consistency in unmodified regions during the addition, deletion, or modification of instances, and preserves the identity of instances when their attributes are changed. We introduce the COCO-MIG and Multimodal-MIG benchmarks to evaluate these methods. Extensive experiments on these benchmarks, along with the COCO-Position benchmark and DrawBench, demonstrate that our methods substantially outperform existing techniques, maintaining precise control over aspects including position, attribute, and quantity. Project page: https://github.com/limuloo/MIGC.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaRe-Ego: Contact-aware Relationship Modeling for Egocentric Interactive Hand-object Segmentation</title>
<link>https://arxiv.org/abs/2407.05576</link>
<guid>https://arxiv.org/abs/2407.05576</guid>
<content:encoded><![CDATA[
arXiv:2407.05576v3 Announce Type: replace 
Abstract: Egocentric Interactive hand-object segmentation (EgoIHOS) requires the segmentation of hands and interacting objects in egocentric images, which is crucial for understanding human behavior in assistive systems. Previous methods typically recognize hands and interacting objects as distinct semantic categories based solely on visual features, or simply use hand predictions as auxiliary cues for object segmentation. Despite the promising progress achieved by these methods, they fail to adequately model the interactive relationships between hands and objects while ignoring the coupled physical relationships among object categories, ultimately constraining their segmentation performance. To make up for the shortcomings of existing methods, we propose a novel method called CaRe-Ego that achieves state-of-the-art performance by emphasizing the contact between hands and objects from two aspects. First, we introduce a Hand-guided Object Feature Enhancer (HOFE) to establish the hand-object interactive relationships to extract more contact-relevant and discriminative object features. Second, we design the Contact-centric Object Decoupling Strategy (CODS) to explicitly model and disentangle coupling relationships among object categories, thereby emphasizing contact-aware feature learning. Experiments on various in-domain and out-of-domain test sets show that Care-Ego significantly outperforms existing methods with robust generalization capability. Codes are publicly available at https://github.com/yuggiehk/CaRe-Ego/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tailored Design of Audio-Visual Speech Recognition Models using Branchformers</title>
<link>https://arxiv.org/abs/2407.06606</link>
<guid>https://arxiv.org/abs/2407.06606</guid>
<content:encoded><![CDATA[
arXiv:2407.06606v3 Announce Type: replace 
Abstract: Recent advances in Audio-Visual Speech Recognition (AVSR) have led to unprecedented achievements in the field, improving the robustness of this type of system in adverse, noisy environments. In most cases, this task has been addressed through the design of models composed of two independent encoders, each dedicated to a specific modality. However, while recent works have explored unified audio-visual encoders, determining the optimal cross-modal architecture remains an ongoing challenge. Furthermore, such approaches often rely on models comprising vast amounts of parameters and high computational cost training processes. In this paper, we aim to bridge this research gap by introducing a novel audio-visual framework. Our proposed method constitutes, to the best of our knowledge, the first attempt to harness the flexibility and interpretability offered by encoder architectures, such as the Branchformer, in the design of parameter-efficient AVSR systems. To be more precise, the proposed framework consists of two steps: first, estimating audio- and video-only systems, and then designing a tailored audio-visual unified encoder based on the layer-level branch scores provided by the modality-specific models. Extensive experiments on English and Spanish AVSR benchmarks covering multiple data conditions and scenarios demonstrated the effectiveness of our proposed method. Even when trained on a moderate scale of data, our models achieve competitive word error rates (WER) of approximately 2.5\% for English and surpass existing approaches for Spanish, establishing a new benchmark with an average WER of around 9.1\%. These results reflect how our tailored AVSR system is able to reach state-of-the-art recognition rates while significantly reducing the model complexity w.r.t. the prevalent approach in the field. Code and pre-trained models are available at https://github.com/david-gimeno/tailored-avsr.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Does Audio Influence Visual Attention in Omnidirectional Videos? Database and Model</title>
<link>https://arxiv.org/abs/2408.05411</link>
<guid>https://arxiv.org/abs/2408.05411</guid>
<content:encoded><![CDATA[
arXiv:2408.05411v2 Announce Type: replace 
Abstract: Understanding and predicting viewer attention in omnidirectional videos (ODVs) is crucial for enhancing user engagement in virtual and augmented reality applications. Although both audio and visual modalities are essential for saliency prediction in ODVs, the joint exploitation of these two modalities has been limited, primarily due to the absence of large-scale audio-visual saliency databases and comprehensive analyses. This paper comprehensively investigates audio-visual attention in ODVs from both subjective and objective perspectives. Specifically, we first introduce a new audio-visual saliency database for omnidirectional videos, termed AVS-ODV database, containing 162 ODVs and corresponding eye movement data collected from 60 subjects under three audio modes including mute, mono, and ambisonics. Based on the constructed AVS-ODV database, we perform an in-depth analysis of how audio influences visual attention in ODVs. To advance the research on audio-visual saliency prediction for ODVs, we further establish a new benchmark based on the AVS-ODV database by testing numerous state-of-the-art saliency models, including visual-only models and audio-visual models. In addition, given the limitations of current models, we propose an innovative omnidirectional audio-visual saliency prediction network (OmniAVS), which is built based on the U-Net architecture, and hierarchically fuses audio and visual features from the multimodal aligned embedding space. Extensive experimental results demonstrate that the proposed OmniAVS model outperforms other state-of-the-art models on both ODV AVS prediction and traditional AVS predcition tasks. The AVS-ODV database and OmniAVS model will be released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation</title>
<link>https://arxiv.org/abs/2408.10453</link>
<guid>https://arxiv.org/abs/2408.10453</guid>
<content:encoded><![CDATA[
arXiv:2408.10453v2 Announce Type: replace 
Abstract: Text-to-video generation has been dominated by diffusion-based or autoregressive models. These novel models provide plausible versatility, but are criticized for improper physical motion, shading and illumination, camera motion, and temporal consistency. The film industry relies on manually-edited Computer-Generated Imagery (CGI) using 3D modeling software. Human-directed 3D synthetic videos address these shortcomings, but require tight collaboration between movie makers and 3D rendering experts. We introduce an automatic synthetic video generation pipeline based on Vision Large Language Model (VLM) agent collaborations. Given a language description of a video, multiple VLM agents direct various processes of the generation pipeline. They cooperate to create Blender scripts which render a video following the given description. Augmented with Blender-based movie making knowledge, the Director agent decomposes the text-based video description into sub-processes. For each sub-process, the Programmer agent produces Python-based Blender scripts based on function composing and API calling. The Reviewer agent, with knowledge of video reviewing, character motion coordinates, and intermediate screenshots, provides feedback to the Programmer agent. The Programmer agent iteratively improves scripts to yield the best video outcome. Our generated videos show better quality than commercial video generation models in five metrics on video quality and instruction-following performance. Our framework outperforms other approaches in a user study on quality, consistency, and rationality.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Face Clustering via Early Stopping and Edge Recall</title>
<link>https://arxiv.org/abs/2408.13431</link>
<guid>https://arxiv.org/abs/2408.13431</guid>
<content:encoded><![CDATA[
arXiv:2408.13431v2 Announce Type: replace 
Abstract: Large-scale face clustering has achieved significant progress, with many efforts dedicated to learning to cluster large-scale faces with supervised-learning. However, complex model design and tedious clustering processes are typical in existing methods. Such limitations result in infeasible clustering in real-world applications. Reasonable and efficient model design and training need to be taken into account. Besides, developing unsupervised face clustering algorithms is crucial, which are more realistic in real-world applications. In this paper, we propose a novel unsupervised face clustering algorithm FC-ES and a novel supervised face clustering algorithm FC-ESER to address these issues. An efficient and effective neighbor-based edge probability and a novel early stopping strategy are proposed in FC-ES, guaranteeing the accuracy and recall of large-scale face clustering simultaneously. Furthermore, to take advantage of supervised learning, a novel edge recall strategy is proposed in FC-ESER to further recall the edge connections that are not connected in FC-ES. Extensive experiments on multiple benchmarks for face, person, and vehicle clustering show that our proposed FC-ES and FC-ESER significantly outperform previous state-of-the-art methods. Our code will be available at https://github.com/jumptoliujj/FC-ESER.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CFCPalsy: Facial Image Synthesis with Cross-Fusion Cycle Diffusion Model for Facial Paralysis Individuals</title>
<link>https://arxiv.org/abs/2409.07271</link>
<guid>https://arxiv.org/abs/2409.07271</guid>
<content:encoded><![CDATA[
arXiv:2409.07271v4 Announce Type: replace 
Abstract: Currently, the diagnosis of facial paralysis remains a challenging task, often relying heavily on the subjective judgment and experience of clinicians, which can introduce variability and uncertainty in the assessment process. One promising application in real-life situations is the automatic estimation of facial paralysis. However, the scarcity of facial paralysis datasets limits the development of robust machine learning models for automated diagnosis and therapeutic interventions. To this end, this study aims to synthesize a high-quality facial paralysis dataset to address this gap, enabling more accurate and efficient algorithm training. Specifically, a novel Cross-Fusion Cycle Palsy Expression Generative Model (CFCPalsy) based on the diffusion model is proposed to combine different features of facial information and enhance the visual details of facial appearance and texture in facial regions, thus creating synthetic facial images that accurately represent various degrees and types of facial paralysis. We have qualitatively and quantitatively evaluated the proposed method on the commonly used public clinical datasets of facial paralysis to demonstrate its effectiveness. Experimental results indicate that the proposed method surpasses state-of-the-art methods, generating more realistic facial images and maintaining identity consistency.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face Forgery Detection</title>
<link>https://arxiv.org/abs/2409.09724</link>
<guid>https://arxiv.org/abs/2409.09724</guid>
<content:encoded><![CDATA[
arXiv:2409.09724v2 Announce Type: replace 
Abstract: The rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (FFD) techniques. Although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises and texts are not fully explored, which limits the generalization capability of the model. In addition, most FFD methods tend to identify facial images generated by GAN, but struggle to detect unseen diffusion-synthesized ones. To address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (CLIP), to achieve generalizable diffusion face forgery detection (DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP (MFCLIP) model, which mines comprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of DFFD. Specifically, we devise a fine-grained language encoder (FLE) that extracts fine global language features from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them to mine general visual forgery traces. Moreover, we build an innovative plug-and-play sample pair attention (SPA) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. Extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Vision-Language Gaussian Splatting</title>
<link>https://arxiv.org/abs/2410.07577</link>
<guid>https://arxiv.org/abs/2410.07577</guid>
<content:encoded><![CDATA[
arXiv:2410.07577v2 Announce Type: replace 
Abstract: Recent advancements in 3D reconstruction methods and vision-language models have propelled the development of multi-modal 3D scene understanding, which has vital applications in robotics, autonomous driving, and virtual/augmented reality. However, current multi-modal scene understanding approaches have naively embedded semantic representations into 3D reconstruction methods without striking a balance between visual and language modalities, which leads to unsatisfying semantic rasterization of translucent or reflective objects, as well as over-fitting on color modality. To alleviate these limitations, we propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3D vision-language Gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. We propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. We also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting. Extensive experiments demonstrate that our method achieves state-of-the-art performance in open-vocabulary semantic segmentation, surpassing existing methods by a significant margin.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models</title>
<link>https://arxiv.org/abs/2410.10821</link>
<guid>https://arxiv.org/abs/2410.10821</guid>
<content:encoded><![CDATA[
arXiv:2410.10821v3 Announce Type: replace 
Abstract: 3D meshes are widely used in computer vision and graphics for their efficiency in animation and minimal memory use, playing a crucial role in movies, games, AR, and VR. However, creating temporally consistent and realistic textures for mesh sequences remains labor-intensive for professional artists. On the other hand, while video diffusion models excel at text-driven video generation, they often lack 3D geometry awareness and struggle with achieving multi-view consistent texturing for 3D meshes. In this work, we present Tex4D, a zero-shot approach that integrates inherent 3D geometry knowledge from mesh sequences with the expressiveness of video diffusion models to produce multi-view and temporally consistent 4D textures. Given an untextured mesh sequence and a text prompt as inputs, our method enhances multi-view consistency by synchronizing the diffusion process across different views through latent aggregation in the UV space. To ensure temporal consistency, we leverage prior knowledge from a conditional video generation model for texture synthesis. However, straightforwardly combining the video diffusion model and the UV texture aggregation leads to blurry results. We analyze the underlying causes and propose a simple yet effective modification to the DDIM sampling process to address this issue. Additionally, we introduce a reference latent texture to strengthen the correlation between frames during the denoising process. To the best of our knowledge, Tex4D is the first method specifically designed for 4D scene texturing. Extensive experiments demonstrate its superiority in producing multi-view and multi-frame consistent videos based on untextured mesh sequences.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model Retrieval</title>
<link>https://arxiv.org/abs/2411.02979</link>
<guid>https://arxiv.org/abs/2411.02979</guid>
<content:encoded><![CDATA[
arXiv:2411.02979v2 Announce Type: replace 
Abstract: Reconstructing from multi-view images is a longstanding problem in 3D vision, where neural radiance fields (NeRFs) have shown great potential and get realistic rendered images of novel views. Currently, most NeRF methods either require accurate camera poses or a large number of input images, or even both. Reconstructing NeRF from few-view images without poses is challenging and highly ill-posed. To address this problem, we propose CAD-NeRF, a method reconstructed from less than 10 images without any known poses. Specifically, we build a mini library of several CAD models from ShapeNet and render them from many random views. Given sparse-view input images, we run a model and pose retrieval from the library, to get a model with similar shapes, serving as the density supervision and pose initializations. Here we propose a multi-view pose retrieval method to avoid pose conflicts among views, which is a new and unseen problem in uncalibrated NeRF methods. Then, the geometry of the object is trained by the CAD guidance. The deformation of the density field and camera poses are optimized jointly. Then texture and density are trained and fine-tuned as well. All training phases are in self-supervised manners. Comprehensive evaluations of synthetic and real images show that CAD-NeRF successfully learns accurate densities with a large deformation from retrieved CAD models, showing the generalization abilities.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Transfer Learning for Open-World Few-Shot Recognition</title>
<link>https://arxiv.org/abs/2411.09986</link>
<guid>https://arxiv.org/abs/2411.09986</guid>
<content:encoded><![CDATA[
arXiv:2411.09986v2 Announce Type: replace 
Abstract: Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. Although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. To unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. In the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. During the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. Additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. The proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniImageNet and tieredImageNet, with only a 1.5\% increase in training effort. Our work demonstrates the effectiveness of transfer learning in FSOSR.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Double-Ellipsoid Geometry of CLIP</title>
<link>https://arxiv.org/abs/2411.14517</link>
<guid>https://arxiv.org/abs/2411.14517</guid>
<content:encoded><![CDATA[
arXiv:2411.14517v2 Announce Type: replace 
Abstract: Contrastive Language-Image Pre-Training (CLIP) is highly instrumental in machine learning applications within a large variety of domains. We investigate the geometry of this embedding, which is still not well understood. We examine the raw unnormalized embedding and show that text and image reside on linearly separable ellipsoid shells, not centered at the origin. We explain the benefits of having this structure, allowing to better embed instances according to their uncertainty during contrastive training. Frequent concepts in the dataset yield more false negatives, inducing greater uncertainty. A new notion of conformity is introduced, which measures the average cosine similarity of an instance to any other instance within a representative data set. We show this measure can be accurately estimated by simply computing the cosine similarity to the modality mean vector. Furthermore, we find that CLIP's modality gap optimizes the matching of the conformity distributions of image and text.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model with Region-guided Referring and Grounding for CT Report Generation</title>
<link>https://arxiv.org/abs/2411.15539</link>
<guid>https://arxiv.org/abs/2411.15539</guid>
<content:encoded><![CDATA[
arXiv:2411.15539v2 Announce Type: replace 
Abstract: Computed tomography (CT) report generation is crucial to assist radiologists in interpreting CT volumes, which can be time-consuming and labor-intensive. Existing methods primarily only consider the global features of the entire volume, making it struggle to focus on specific regions and potentially missing abnormalities. To address this issue, we propose Reg2RG, the first region-guided referring and grounding framework for CT report generation, which enhances diagnostic performance by focusing on anatomical regions within the volume. Specifically, we utilize masks from a universal segmentation module to capture local features for each referring region. A local feature decoupling (LFD) strategy is proposed to preserve the local high-resolution details with little computational overhead. Then the local features are integrated with global features to capture inter-regional relationships within a cohesive context. Moreover, we propose a novel region-report alignment (RRA) training strategy. It leverages the recognition of referring regions to guide the generation of region-specific reports, enhancing the model's referring and grounding capabilities while also improving the report's interpretability. A large language model (LLM) is further employed as the language decoder to generate reports from integrated visual features, facilitating region-level comprehension. Extensive experiments on two large-scale chest CT-report datasets demonstrate the superiority of our method, which outperforms several state-of-the-art methods in terms of both natural language generation and clinical efficacy metrics while preserving promising interpretability. The code is available at https://github.com/zhi-xuan-chen/Reg2RG.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Aware Input Orchestration for Video Inpainting</title>
<link>https://arxiv.org/abs/2411.16926</link>
<guid>https://arxiv.org/abs/2411.16926</guid>
<content:encoded><![CDATA[
arXiv:2411.16926v2 Announce Type: replace 
Abstract: Traditional neural network-driven inpainting methods struggle to deliver high-quality results within the constraints of mobile device processing power and memory. Our research introduces an innovative approach to optimize memory usage by altering the composition of input data. Typically, video inpainting relies on a predetermined set of input frames, such as neighboring and reference frames, often limited to five-frame sets. Our focus is to examine how varying the proportion of these input frames impacts the quality of the inpainted video. By dynamically adjusting the input frame composition based on optical flow and changes of the mask, we have observed an improvement in various contents including rapid visual context changes.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Data Curation Effectively Distills Large-Scale Multimodal Models</title>
<link>https://arxiv.org/abs/2411.18674</link>
<guid>https://arxiv.org/abs/2411.18674</guid>
<content:encoded><![CDATA[
arXiv:2411.18674v2 Announce Type: replace 
Abstract: Knowledge distillation (KD) is the de facto standard for compressing large-scale models into smaller ones. Prior works have explored ever more complex KD strategies involving different objective functions, teacher-ensembles, and weight inheritance. In this work we explore an alternative, yet simple approach -- active data curation as effective distillation for contrastive multimodal pretraining. Our simple online batch selection method, ACID, outperforms strong KD baselines across various model-, data- and compute-configurations. Further, we find such an active data curation strategy to in fact be complementary to standard KD, and can be effectively combined to train highly performant inference-efficient models. Our simple and scalable pretraining framework, ACED, achieves state-of-the-art results across 27 zero-shot classification and retrieval tasks with upto 11% less inference FLOPs. We further demonstrate that our ACED models yield strong vision-encoders for training generative multimodal models in the LiT-Decoder setting, outperforming larger vision encoders for image-captioning and visual question-answering tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMO Sampler: Enhancing Text Rendering with Overshooting</title>
<link>https://arxiv.org/abs/2411.19415</link>
<guid>https://arxiv.org/abs/2411.19415</guid>
<content:encoded><![CDATA[
arXiv:2411.19415v2 Announce Type: replace 
Abstract: Achieving precise alignment between textual instructions and generated images in text-to-image generation is a significant challenge, particularly in rendering written text within images. Sate-of-the-art models like Stable Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. We introduce a training-free method with minimal computational overhead that significantly enhances text rendering quality. Specifically, we introduce an overshooting sampler for pretrained rectified flow (RF) models, by alternating between over-simulating the learned ordinary differential equation (ODE) and reintroducing noise. Compared to the Euler sampler, the overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from successive Euler steps and therefore improve the text rendering. However, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. To address this issue, we propose an Attention Modulated Overshooting sampler (AMO), which adaptively controls the strength of overshooting for each image patch according to their attention score with the text content. AMO demonstrates a 32.3% and 35.9% improvement in text rendering accuracy on SD3 and Flux without compromising overall image quality or increasing inference cost. Code available at: https://github.com/hxixixh/amo-release.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes</title>
<link>https://arxiv.org/abs/2412.00592</link>
<guid>https://arxiv.org/abs/2412.00592</guid>
<content:encoded><![CDATA[
arXiv:2412.00592v2 Announce Type: replace 
Abstract: We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data for autonomous driving. Our framework edits real-world LiDAR scans by introducing new object layouts while preserving the realism of the background environment. Compared to end-to-end frameworks that generate LiDAR point clouds from scratch, LiDAR-EDIT offers users full control over the object layout, including the number, type, and pose of objects, while keeping most of the original real-world background. Our method also provides object labels for the generated data. Compared to novel view synthesis techniques, our framework allows for the creation of counterfactual scenarios with object layouts significantly different from the original real-world scene. LiDAR-EDIT uses spherical voxelization to enforce correct LiDAR projective geometry in the generated point clouds by construction. During object removal and insertion, generative models are employed to fill the unseen background and object parts that were occluded in the original real LiDAR scans. Experimental results demonstrate that our framework produces realistic LiDAR scans with practical value for downstream tasks.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrushEdit: All-In-One Image Inpainting and Editing</title>
<link>https://arxiv.org/abs/2412.10316</link>
<guid>https://arxiv.org/abs/2412.10316</guid>
<content:encoded><![CDATA[
arXiv:2412.10316v3 Announce Type: replace 
Abstract: Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColorFlow: Retrieval-Augmented Image Sequence Colorization</title>
<link>https://arxiv.org/abs/2412.11815</link>
<guid>https://arxiv.org/abs/2412.11815</guid>
<content:encoded><![CDATA[
arXiv:2412.11815v2 Announce Type: replace 
Abstract: Automatic black-and-white image sequence colorization while preserving character and object identity (ID) is a complex task with significant market demand, such as in cartoon or comic series colorization. Despite advancements in visual colorization using large-scale generative models like diffusion models, challenges with controllability and identity consistency persist, making current solutions unsuitable for industrial application.To address this, we propose ColorFlow, a three-stage diffusion-based framework tailored for image sequence colorization in industrial applications. Unlike existing methods that require per-ID finetuning or explicit ID embedding extraction, we propose a novel robust and generalizable Retrieval Augmented Colorization pipeline for colorizing images with relevant color references. Our pipeline also features a dual-branch design: one branch for color identity extraction and the other for colorization, leveraging the strengths of diffusion models. We utilize the self-attention mechanism in diffusion models for strong in-context learning and color identity matching. To evaluate our model, we introduce ColorFlow-Bench, a comprehensive benchmark for reference-based colorization. Results show that ColorFlow outperforms existing models across multiple metrics, setting a new standard in sequential image colorization and potentially benefiting the art industry. We release our codes and models on our project page: https://zhuang2002.github.io/ColorFlow/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations</title>
<link>https://arxiv.org/abs/2412.14803</link>
<guid>https://arxiv.org/abs/2412.14803</guid>
<content:encoded><![CDATA[
arXiv:2412.14803v2 Announce Type: replace 
Abstract: Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data. In experiments, VPP achieves a 18.6\% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\% increase in success rates for complex real-world dexterous manipulation tasks. Project page at https://video-prediction-policy.github.io
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D Images</title>
<link>https://arxiv.org/abs/2501.10098</link>
<guid>https://arxiv.org/abs/2501.10098</guid>
<content:encoded><![CDATA[
arXiv:2501.10098v2 Announce Type: replace 
Abstract: Anatomical landmark localization in 2D/3D images is a critical task in medical imaging. Although many general-purpose tools exist for landmark localization in classical computer vision tasks, such as pose estimation, they lack the specialized features and modularity necessary for anatomical landmark localization applications in the medical domain. Therefore, we introduce landmarker, a Python package built on PyTorch. The package provides a comprehensive, flexible toolkit for developing and evaluating landmark localization algorithms, supporting a range of methodologies, including static and adaptive heatmap regression. landmarker enhances the accuracy of landmark identification, streamlines research and development processes, and supports various image formats and preprocessing pipelines. Its modular design allows users to customize and extend the toolkit for specific datasets and applications, accelerating innovation in medical imaging. landmarker addresses a critical need for precision and customization in landmark localization tasks not adequately met by existing general-purpose pose estimation tools.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Scene Understanding from Vision-Language Representations</title>
<link>https://arxiv.org/abs/2501.11653</link>
<guid>https://arxiv.org/abs/2501.11653</guid>
<content:encoded><![CDATA[
arXiv:2501.11653v3 Announce Type: replace 
Abstract: Images depicting complex, dynamic scenes are challenging to parse automatically, requiring both high-level comprehension of the overall situation and fine-grained identification of participating entities and their interactions. Current approaches use distinct methods tailored to sub-tasks such as Situation Recognition and detection of Human-Human and Human-Object Interactions. However, recent advances in image understanding have often leveraged web-scale vision-language (V&amp;L) representations to obviate task-specific engineering. In this work, we propose a framework for dynamic scene understanding tasks by leveraging knowledge from modern, frozen V&amp;L representations. By framing these tasks in a generic manner - as predicting and parsing structured text, or by directly concatenating representations to the input of existing models - we achieve state-of-the-art results while using a minimal number of trainable parameters relative to existing approaches. Moreover, our analysis of dynamic knowledge of these representations shows that recent, more powerful representations effectively encode dynamic scene semantics, making this approach newly possible.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cognitive Paradigm Approach to Probe the Perception-Reasoning Interface in VLMs</title>
<link>https://arxiv.org/abs/2501.13620</link>
<guid>https://arxiv.org/abs/2501.13620</guid>
<content:encoded><![CDATA[
arXiv:2501.13620v4 Announce Type: replace 
Abstract: A fundamental challenge in artificial intelligence involves understanding the cognitive mechanisms underlying visual reasoning in sophisticated models like Vision-Language Models (VLMs). How do these models integrate visual perception with abstract thought, especially when reasoning across multiple images or requiring fine-grained compositional understanding? Drawing inspiration from cognitive science, this paper introduces a structured evaluation framework using diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to dissect the perception-reasoning interface in VLMs. We propose three distinct evaluation paradigms, mirroring human problem-solving strategies: Direct Visual Rule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule extraction and application), and Componential Analysis (CA; analytical decomposition via task-agnostic textual descriptions). These paradigms systematically vary cognitive load and probe processing stages. Notably, CA enables multi-image reasoning evaluation even for single-image architectures and isolates reasoning from perception by operating on textual descriptions. Applying this framework, we demonstrate that CA, leveraging powerful language models for reasoning over rich, independently generated descriptions, achieves new state-of-the-art (SOTA) performance on challenging benchmarks including Bongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm reasoning improves significantly when perceptual challenges are mitigated, revealing a critical perception bottleneck. Our framework provides a valuable diagnostic tool and suggests that decoupling perception (via rich, task-agnostic description) from reasoning is a promising direction for robust and general visual intelligence.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDe: Blockwise Control for Denoising Diffusion Models</title>
<link>https://arxiv.org/abs/2502.00968</link>
<guid>https://arxiv.org/abs/2502.00968</guid>
<content:encoded><![CDATA[
arXiv:2502.00968v2 Announce Type: replace 
Abstract: Aligning diffusion models to downstream tasks often requires finetuning new models or gradient-based guidance at inference time to enable sampling from the reward-tilted posterior. In this work, we explore a simple inference-time gradient-free guidance approach, called controlled denoising (CoDe), that circumvents the need for differentiable guidance functions and model finetuning. CoDe is a blockwise sampling method applied during intermediate denoising steps, allowing for alignment with downstream rewards. Our experiments demonstrate that, despite its simplicity, CoDe offers a favorable trade-off between reward alignment, prompt instruction following, and inference cost, achieving a competitive performance against the state-of-the-art baselines. Our code is available at: https://github.com/anujinho/code.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAGNet: A Dual-View Attention-Guided Network for Efficient X-ray Security Inspection</title>
<link>https://arxiv.org/abs/2502.01710</link>
<guid>https://arxiv.org/abs/2502.01710</guid>
<content:encoded><![CDATA[
arXiv:2502.01710v4 Announce Type: replace 
Abstract: With the rapid development of modern transportation systems and the exponential growth of logistics volumes, intelligent X-ray-based security inspection systems play a crucial role in public safety. Although single-view X-ray baggage scanner is widely deployed, they struggles to accurately identify contraband in complex stacking scenarios due to strong viewpoint dependency and inadequate feature representation. To address this, we propose a Dual-View Attention-Guided Network for Efficient X-ray Security Inspection (DAGNet). This study builds on a shared-weight backbone network as the foundation and constructs three key modules that work together: (1) Frequency Domain Interaction Module (FDIM) dynamically enhances features by adjusting frequency components based on inter-view relationships; (2) Dual-View Hierarchical Enhancement Module (DVHEM) employs cross-attention to align features between views and capture hierarchical associations; (3) Convolutional Guided Fusion Module (CGFM) fuses features to suppress redundancy while retaining critical discriminative information. Collectively, these modules substantially improve the performance of dual-view X-ray security inspection. Experimental results demonstrate that DAGNet outperforms existing state-of-the-art approaches across multiple backbone architectures. The code is available at:https://github.com/ShilongHong/DAGNet.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training</title>
<link>https://arxiv.org/abs/2502.15251</link>
<guid>https://arxiv.org/abs/2502.15251</guid>
<content:encoded><![CDATA[
arXiv:2502.15251v3 Announce Type: replace 
Abstract: We present a framework for pre-training of 3D hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed SimHand. Pre-training with large-scale images achieves promising results in various tasks, but prior methods for 3D hand pose pre-training have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our pre-training method with contrastive learning. Specifically, we collect over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands: pairs of non-identical samples with similar hand poses. We then propose a novel contrastive learning method that embeds similar hand pairs closer in the feature space. Our method not only learns from similar samples but also adaptively weights the contrastive learning loss based on inter-sample distance, leading to additional performance gains. Our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method (PeCLR) in various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on AssemblyHands.
  Our code is available at https://github.com/ut-vision/SiMHand.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoNormia: Benchmarking Physical Social Norm Understanding</title>
<link>https://arxiv.org/abs/2502.20490</link>
<guid>https://arxiv.org/abs/2502.20490</guid>
<content:encoded><![CDATA[
arXiv:2502.20490v3 Announce Type: replace 
Abstract: Human activity is moderated by norms. However, machines are often trained without explicit supervision on norm understanding and reasoning, particularly when norms are physically- or socially-grounded. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present \dataset{} $\|\epsilon\|$, consisting of 1,853 challenging, multi-stage MCQ questions based on ego-centric videos of human interactions, evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 54\% on \dataset{} (versus a human bench of 92\%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation (RAG) method, it is possible to use \dataset{} to enhance normative reasoning in VLMs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-based 3D Semantic Scene Completion via Capture Dynamic Representations</title>
<link>https://arxiv.org/abs/2503.06222</link>
<guid>https://arxiv.org/abs/2503.06222</guid>
<content:encoded><![CDATA[
arXiv:2503.06222v2 Announce Type: replace 
Abstract: The vision-based semantic scene completion task aims to predict dense geometric and semantic 3D scene representations from 2D images. However, the presence of dynamic objects in the scene seriously affects the accuracy of the model inferring 3D structures from 2D images. Existing methods simply stack multiple frames of image input to increase dense scene semantic information, but ignore the fact that dynamic objects and non-texture areas violate multi-view consistency and matching reliability. To address these issues, we propose a novel method, CDScene: Vision-based Robust Semantic Scene Completion via Capturing Dynamic Representations. First, we leverage a multimodal large-scale model to extract 2D explicit semantics and align them into 3D space. Second, we exploit the characteristics of monocular and stereo depth to decouple scene information into dynamic and static features. The dynamic features contain structural relationships around dynamic objects, and the static features contain dense contextual spatial information. Finally, we design a dynamic-static adaptive fusion module to effectively extract and aggregate complementary features, achieving robust and accurate semantic scene completion in autonomous driving scenarios. Extensive experimental results on the SemanticKITTI, SSCBench-KITTI360, and SemanticKITTI-C datasets demonstrate the superiority and robustness of CDScene over existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning</title>
<link>https://arxiv.org/abs/2503.06457</link>
<guid>https://arxiv.org/abs/2503.06457</guid>
<content:encoded><![CDATA[
arXiv:2503.06457v2 Announce Type: replace 
Abstract: Data heterogeneity in federated learning, characterized by a significant misalignment between local and global distributions, leads to divergent local optimization directions and hinders global model training. Existing studies mainly focus on optimizing local updates or global aggregation, but these indirect approaches demonstrate instability when handling highly heterogeneous data distributions, especially in scenarios where label skew and domain skew coexist. To address this, we propose a geometry-guided data generation method that centers on simulating the global embedding distribution locally. We first introduce the concept of the geometric shape of an embedding distribution and then address the challenge of obtaining global geometric shapes under privacy constraints. Subsequently, we propose GGEUR, which leverages global geometric shapes to guide the generation of new samples, enabling a closer approximation to the ideal global distribution. In single-domain scenarios, we augment samples based on global geometric shapes to enhance model generalization; in multi-domain scenarios, we further employ class prototypes to simulate the global distribution across domains. Extensive experimental results demonstrate that our method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence. Code published at: https://github.com/WeiDai-David/2025CVPR_GGEUR
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting</title>
<link>https://arxiv.org/abs/2504.00159</link>
<guid>https://arxiv.org/abs/2504.00159</guid>
<content:encoded><![CDATA[
arXiv:2504.00159v2 Announce Type: replace 
Abstract: In this paper, we present SonarSplat, a novel Gaussian splatting framework for imaging sonar that demonstrates realistic novel view synthesis and models acoustic streaking phenomena. Our method represents the scene as a set of 3D Gaussians with acoustic reflectance and saturation properties. We develop a novel method to efficiently rasterize Gaussians to produce a range/azimuth image that is faithful to the acoustic image formation model of imaging sonar. In particular, we develop a novel approach to model azimuth streaking in a Gaussian splatting framework. We evaluate SonarSplat using real-world datasets of sonar images collected from an underwater robotic platform in a controlled test tank and in a real-world river environment. Compared to the state-of-the-art, SonarSplat offers improved image synthesis capabilities (+3.2 dB PSNR) and more accurate 3D reconstruction (52% lower Chamfer Distance). We also demonstrate that SonarSplat can be leveraged for azimuth streak removal.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis</title>
<link>https://arxiv.org/abs/2504.03471</link>
<guid>https://arxiv.org/abs/2504.03471</guid>
<content:encoded><![CDATA[
arXiv:2504.03471v2 Announce Type: replace 
Abstract: Traditional diffusion models typically employ a U-Net architecture. Previous studies have unveiled the roles of attention blocks in the U-Net. However, they overlook the dynamic evolution of their importance during the inference process, which hinders their further exploitation to improve image applications. In this study, we first theoretically proved that, re-weighting the outputs of the Transformer blocks within the U-Net is a "free lunch" for improving the signal-to-noise ratio during the sampling process. Next, we proposed Importance Probe to uncover and quantify the dynamic shifts in importance of the Transformer blocks throughout the denoising process. Finally, we design an adaptive importance-based re-weighting schedule tailored to specific image generation and editing tasks. Experimental results demonstrate that, our approach significantly improves the efficiency of the inference process, and enhances the aesthetic quality of the samples with identity consistency. Our method can be seamlessly integrated into any U-Net-based architecture. Code: https://github.com/Hytidel/UNetReweighting
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation</title>
<link>https://arxiv.org/abs/2504.04519</link>
<guid>https://arxiv.org/abs/2504.04519</guid>
<content:encoded><![CDATA[
arXiv:2504.04519v3 Announce Type: replace 
Abstract: Segment Anything 2 (SAM2) enables robust single-object tracking using segmentation. To extend this to multi-object tracking (MOT), we propose SAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking by Detection or Tracking by Query, SAM2MOT directly generates tracking boxes from segmentation masks, reducing reliance on detection accuracy. SAM2MOT has two key advantages: zero-shot generalization, allowing it to work across datasets without fine-tuning, and strong object association, inherited from SAM2. To further improve performance, we integrate a trajectory manager system for precise object addition and removal, and a cross-object interaction module to handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show state-of-the-art results. Notably, SAM2MOT outperforms existing methods on DanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT. Code is available at https://github.com/TripleJoy/SAM2MOT.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation</title>
<link>https://arxiv.org/abs/2504.05184</link>
<guid>https://arxiv.org/abs/2504.05184</guid>
<content:encoded><![CDATA[
arXiv:2504.05184v2 Announce Type: replace 
Abstract: The accurate segmentation of coronary Digital Subtraction Angiography (DSA) images is essential for diagnosing and treating coronary artery diseases. Despite advances in deep learning-based segmentation, challenges such as low contrast, noise, overlapping structures, high intra-class variance, and class imbalance limit precise vessel delineation. To overcome these limitations, we propose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture for coronary DSA image segmentation. The framework combined Multi-Scale Dilated Bottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM), which not only enhances multi-scale feature extraction but also preserve fine-grained details, and improve contextual understanding. Furthermore, we propose a new Supervised Prototypical Contrastive Loss (SPCL), which combines supervised and prototypical contrastive learning to minimize class imbalance and high intra-class variance by focusing on hard-to-classified background samples. Experiments carried out on a private coronary DSA dataset demonstrate that MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice coefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average Surface Distance (ASD) and Average Contour Distance (ACD). The developed framework provides clinicians with precise vessel segmentation, enabling accurate identification of coronary stenosis and supporting informed diagnostic and therapeutic decisions. The code will be released at the following GitHub profile link https://github.com/rayanmerghani/MSA-UNet3plus.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ID-Booth: Identity-consistent Face Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2504.07392</link>
<guid>https://arxiv.org/abs/2504.07392</guid>
<content:encoded><![CDATA[
arXiv:2504.07392v4 Announce Type: replace 
Abstract: Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at https://github.com/dariant/ID-Booth.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</title>
<link>https://arxiv.org/abs/2504.08685</link>
<guid>https://arxiv.org/abs/2504.08685</guid>
<content:encoded><![CDATA[
arXiv:2504.08685v2 Announce Type: replace 
Abstract: This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot Continual Learning</title>
<link>https://arxiv.org/abs/2504.08982</link>
<guid>https://arxiv.org/abs/2504.08982</guid>
<content:encoded><![CDATA[
arXiv:2504.08982v2 Announce Type: replace 
Abstract: Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. However, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. In this work, we propose a simple yet effective novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone augmented with parameter-efficient additive updates. Our approach freezes the pre-trained ViT parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. This design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. By fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen ViT while reducing the risk of overfitting. Furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. Extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline FSCIL methods.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cobra: Efficient Line Art COlorization with BRoAder References</title>
<link>https://arxiv.org/abs/2504.12240</link>
<guid>https://arxiv.org/abs/2504.12240</guid>
<content:encoded><![CDATA[
arXiv:2504.12240v2 Announce Type: replace 
Abstract: The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark</title>
<link>https://arxiv.org/abs/2504.14693</link>
<guid>https://arxiv.org/abs/2504.14693</guid>
<content:encoded><![CDATA[
arXiv:2504.14693v2 Announce Type: replace 
Abstract: Recent advancements in language multimodal models (LMMs) for video have demonstrated their potential for understanding video content, yet the task of comprehending multi-discipline lectures remains largely unexplored. We introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90 open-source and proprietary models, ranging from 0.5B to 40B parameters. Our results highlight the limitations of current models in addressing the cognitive challenges presented by these lectures, especially in tasks requiring both perception and reasoning. Additionally, we explore how the number of visual tokens and the large language models influence performance, offering insights into the interplay between multimodal perception and reasoning in lecture comprehension.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Attack to Protection: Leveraging Watermarking Attack Network for Advanced Add-on Watermarking</title>
<link>https://arxiv.org/abs/2008.06255</link>
<guid>https://arxiv.org/abs/2008.06255</guid>
<content:encoded><![CDATA[
arXiv:2008.06255v4 Announce Type: replace-cross 
Abstract: Multi-bit watermarking (MW) has been designed to enhance resistance against watermarking attacks, such as signal processing operations and geometric distortions. Various benchmark tools exist to assess this robustness through simulated attacks on watermarked images. However, these tools often fail to capitalize on the unique attributes of the targeted MW and typically neglect the aspect of visual quality, a critical factor in practical applications. To overcome these shortcomings, we introduce a watermarking attack network (WAN), a fully trainable watermarking benchmark tool designed to exploit vulnerabilities within MW systems and induce watermark bit inversions, significantly diminishing watermark extractability. The proposed WAN employs an architecture based on residual dense blocks, which is adept at both local and global feature learning, thereby maintaining high visual quality while obstructing the extraction of embedded information. Our empirical results demonstrate that the WAN effectively undermines various block-based MW systems while minimizing visual degradation caused by attacks. This is facilitated by our novel watermarking attack loss, which is specifically crafted to compromise these systems. The WAN functions not only as a benchmarking tool but also as an add-on watermarking (AoW) mechanism, augmenting established universal watermarking schemes by enhancing robustness or imperceptibility without requiring detailed method context and adapting to dynamic watermarking requirements. Extensive experimental results show that AoW complements the performance of the targeted MW system by independently enhancing both imperceptibility and robustness.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Loop Game: Quality Assessment and Optimization for Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2202.09738</link>
<guid>https://arxiv.org/abs/2202.09738</guid>
<content:encoded><![CDATA[
arXiv:2202.09738v2 Announce Type: replace-cross 
Abstract: There is an increasing consensus that the design and optimization of low light image enhancement methods need to be fully driven by perceptual quality. With numerous approaches proposed to enhance low-light images, much less work has been dedicated to quality assessment and quality optimization of low-light enhancement. In this paper, to close the gap between enhancement and assessment, we propose a loop enhancement framework that produces a clear picture of how the enhancement of low-light images could be optimized towards better visual quality. In particular, we create a large-scale database for QUality assessment Of The Enhanced LOw-Light Image (QUOTE-LOL), which serves as the foundation in studying and developing objective quality assessment measures. The objective quality assessment measure plays a critical bridging role between visual quality and enhancement and is further incorporated in the optimization in learning the enhancement model towards perceptual optimally. Finally, we iteratively perform the enhancement and optimization tasks, enhancing the low-light images continuously. The superiority of the proposed scheme is validated based on various low-light scenes.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Impact of Noisy Supervision in Foundation Model Learning</title>
<link>https://arxiv.org/abs/2403.06869</link>
<guid>https://arxiv.org/abs/2403.06869</guid>
<content:encoded><![CDATA[
arXiv:2403.06869v3 Announce Type: replace-cross 
Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Challenges in Deep Learning of Single-Station Ground Motion Records</title>
<link>https://arxiv.org/abs/2403.07569</link>
<guid>https://arxiv.org/abs/2403.07569</guid>
<content:encoded><![CDATA[
arXiv:2403.07569v2 Announce Type: replace-cross 
Abstract: Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models truly extract meaningful patterns from these complex time-series signals remains underexplored. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. Our experimental results reveal a strong dependence on the highly correlated Primary (P) and Secondary (S) phase arrival times. These findings expose a critical gap in the current research landscape, highlighting the lack of robust methodologies for deep learning from single-station ground motion recordings that do not rely on auxiliary inputs.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Covariant spatio-temporal receptive fields for spiking neural networks</title>
<link>https://arxiv.org/abs/2405.00318</link>
<guid>https://arxiv.org/abs/2405.00318</guid>
<content:encoded><![CDATA[
arXiv:2405.00318v3 Announce Type: replace-cross 
Abstract: Biological nervous systems constitute important sources of inspiration towards computers that are faster, cheaper, and more energy efficient. Neuromorphic disciplines view the brain as a coevolved system, simultaneously optimizing the hardware and the algorithms running on it. There are clear efficiency gains when bringing the computations into a physical substrate, but we presently lack theories to guide efficient implementations. Here, we present a principled computational model for neuromorphic systems in terms of spatio-temporal receptive fields, based on affine Gaussian kernels over space and leaky-integrator and leaky integrate-and-fire models over time. Our theory is provably covariant to spatial affine and temporal scaling transformations, and with close similarities to the visual processing in mammalian brains. We use these spatio-temporal receptive fields as a prior in an event-based vision task, and show that this improves the training of spiking networks, which otherwise is known as problematic for event-based vision. This work combines efforts within scale-space theory and computational neuroscience to identify theoretically well-founded ways to process spatio-temporal signals in neuromorphic systems. Our contributions are immediately relevant for signal processing and event-based vision, and can be extended to other processing tasks over space and time, such as memory and control.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models</title>
<link>https://arxiv.org/abs/2407.12772</link>
<guid>https://arxiv.org/abs/2407.12772</guid>
<content:encoded><![CDATA[
arXiv:2407.12772v2 Announce Type: replace-cross 
Abstract: The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMS-EVAL offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that emphasizes both coverage and efficiency. Additionally, we present Multimodal LIVEBENCH that utilizes continuously updating news and online forums to assess models' generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs. We opensource our codebase and maintain leaderboard of LIVEBENCH at https://github.com/EvolvingLMMs-Lab/lmms-eval and https://huggingface.co/spaces/lmms-lab/LiveBench.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition</title>
<link>https://arxiv.org/abs/2408.17090</link>
<guid>https://arxiv.org/abs/2408.17090</guid>
<content:encoded><![CDATA[
arXiv:2408.17090v2 Announce Type: replace-cross 
Abstract: Federated learning is a machine learning paradigm that enables decentralized clients to collaboratively learn a shared model while keeping all the training data local. While considerable research has focused on federated image generation, particularly Generative Adversarial Networks, Variational Autoencoders have received less attention. In this paper, we address the challenges of non-IID (independently and identically distributed) data environments featuring multiple groups of images of different types. Non-IID data distributions can lead to difficulties in maintaining a consistent latent space and can also result in local generators with disparate texture features being blended during aggregation. We thereby introduce FissionVAE that decouples the latent space and constructs decoder branches tailored to individual client groups. This method allows for customized learning that aligns with the unique data distributions of each group. Additionally, we incorporate hierarchical VAEs and demonstrate the use of heterogeneous decoder architectures within FissionVAE. We also explore strategies for setting the latent prior distributions to enhance the decoupling process. To evaluate our approach, we assemble two composite datasets: the first combines MNIST and FashionMNIST; the second comprises RGB datasets of cartoon and human faces, wild animals, marine vessels, and remote sensing images. Our experiments demonstrate that FissionVAE greatly improves generation quality on these datasets compared to baseline federated VAE models.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FolAI: Synchronized Foley Sound Generation with Semantic and Temporal Alignment</title>
<link>https://arxiv.org/abs/2412.15023</link>
<guid>https://arxiv.org/abs/2412.15023</guid>
<content:encoded><![CDATA[
arXiv:2412.15023v3 Announce Type: replace-cross 
Abstract: Traditional sound design workflows rely on manual alignment of audio events to visual cues, as in Foley sound design, where everyday actions like footsteps or object interactions are recreated to match the on-screen motion. This process is time-consuming, difficult to scale, and lacks automation tools that preserve creative intent. Despite recent advances in vision-to-audio generation, producing temporally coherent and semantically controllable sound effects from video remains a major challenge. To address these limitations, we introduce FolAI, a two-stage generative framework that decouples the when and the what of sound synthesis, i.e., the temporal structure extraction and the semantically guided generation, respectively. In the first stage, we estimate a smooth control signal from the video that captures the motion intensity and rhythmic structure over time, serving as a temporal scaffold for the audio. In the second stage, a diffusion-based generative model produces sound effects conditioned both on this temporal envelope and on high-level semantic embeddings, provided by the user, that define the desired auditory content (e.g., material or action type). This modular design enables precise control over both timing and timbre, streamlining repetitive tasks while preserving creative flexibility in professional Foley workflows. Results on diverse visual contexts, such as footstep generation and action-specific sonorization, demonstrate that our model reliably produces audio that is temporally aligned with visual motion, semantically consistent with user intent, and perceptually realistic. These findings highlight the potential of FolAI as a controllable and modular solution for scalable, high-quality Foley sound synthesis in professional and interactive settings. Supplementary materials are accessible on our dedicated demo page at https://ispamm.github.io/FolAI.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced Explainability in Breast Cancer Histopathology</title>
<link>https://arxiv.org/abs/2501.04206</link>
<guid>https://arxiv.org/abs/2501.04206</guid>
<content:encoded><![CDATA[
arXiv:2501.04206v2 Announce Type: replace-cross 
Abstract: Explainable AI (XAI) in medical histopathology is essential for enhancing the interpretability and clinical trustworthiness of deep learning models in cancer diagnosis. However, the black-box nature of these models often limits their clinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue Examination), a post-hoc explainable framework designed for breast cancer tissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach, extracting patches at various magnification levels, constructing an hierarchical graph, and utilising graph attention networks (GAT) with scalewise attention (SAN) to capture scale-dependent features. We trained the model on 140 tumour TMA cores and four benign whole slide images from which 140 benign samples were created, and tested it on 53 pathologist-annotated TMA samples. GRAPHITE outperformed traditional XAI methods, achieving a mean average precision (mAP) of 0.56, an area under the receiver operating characteristic curve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating that the model maintains high performance across a wide range of thresholds. In clinical utility, GRAPHITE achieved the highest area under the decision curve (AUDC) of 4.17e+5, indicating reliable decision support across thresholds. These results highlight GRAPHITE's potential as a clinically valuable tool in computational pathology, providing interpretable visualisations that align with the pathologists' diagnostic reasoning and support precision medicine.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTe-VR: Student Monitoring and Adaptive Response Technology for e-Learning in Virtual Reality</title>
<link>https://arxiv.org/abs/2501.10977</link>
<guid>https://arxiv.org/abs/2501.10977</guid>
<content:encoded><![CDATA[
arXiv:2501.10977v2 Announce Type: replace-cross 
Abstract: This work introduces SMARTe-VR, a platform for student monitoring in an immersive virtual reality environment designed for online education. SMARTe-VR aims to collect data for adaptive learning, focusing on facial biometrics and learning metadata. The platform allows instructors to create customized learning sessions with video lectures, featuring an interface with an AutoQA system to evaluate understanding, interaction tools (e.g., textbook highlighting and lecture tagging), and real-time feedback. Furthermore, we released a dataset that contains 5 research challenges with data from 10 users in VR-based TOEIC sessions. This data set, which spans more than 25 hours, includes facial features, learning metadata, 450 responses, difficulty levels of the questions, concept tags, and understanding labels. Alongside the database, we present preliminary experiments using Item Response Theory models, adapted for understanding detection using facial features. Two architectures were explored: a Temporal Convolutional Network for local features and a Multilayer Perceptron for global features.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Bias of Foundation Model under Long-tailed Distribution</title>
<link>https://arxiv.org/abs/2501.15955</link>
<guid>https://arxiv.org/abs/2501.15955</guid>
<content:encoded><![CDATA[
arXiv:2501.15955v2 Announce Type: replace-cross 
Abstract: Long-tailed learning has garnered increasing attention due to its practical significance. Among the various approaches, the fine-tuning paradigm has gained considerable interest with the advent of foundation models. However, most existing methods primarily focus on leveraging knowledge from these models, overlooking the inherent biases introduced by the imbalanced training data they rely on. In this paper, we examine how such imbalances from pre-training affect long-tailed downstream tasks. Specifically, we find the imbalance biases inherited in foundation models on downstream task as parameter imbalance and data imbalance. During fine-tuning, we observe that parameter imbalance plays a more critical role, while data imbalance can be mitigated using existing re-balancing strategies. Moreover, we find that parameter imbalance cannot be effectively addressed by current re-balancing techniques, such as adjusting the logits, during training, unlike data imbalance. To tackle both imbalances simultaneously, we build our method on causal learning and view the incomplete semantic factor as the confounder, which brings spurious correlations between input samples and labels. To resolve the negative effects of this, we propose a novel backdoor adjustment method that learns the true causal effect between input samples and labels, rather than merely fitting the correlations in the data. Notably, we achieve an average performance increase of about $1.67\%$ on each dataset.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning</title>
<link>https://arxiv.org/abs/2502.03270</link>
<guid>https://arxiv.org/abs/2502.03270</guid>
<content:encoded><![CDATA[
arXiv:2502.03270v2 Announce Type: replace-cross 
Abstract: The integration of pre-trained visual representations (PVRs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. However, PVRs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. These limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. This work identifies these shortcomings and proposes solutions to address them. First, we augment PVR features with temporal perception and a sense of task completion, effectively disentangling them in time. Second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. Our experiments demonstrate significant performance improvements, particularly in PVRs trained with masking objectives, and validate the effectiveness of our enhancements in addressing PVR-specific limitations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel approach to navigate the taxonomic hierarchy to address the Open-World Scenarios in Medicinal Plant Classification</title>
<link>https://arxiv.org/abs/2502.17289</link>
<guid>https://arxiv.org/abs/2502.17289</guid>
<content:encoded><![CDATA[
arXiv:2502.17289v2 Announce Type: replace-cross 
Abstract: In this article, we propose a novel approach for plant hierarchical taxonomy classification by posing the problem as an open class problem. It is observed that existing methods for medicinal plant classification often fail to perform hierarchical classification and accurately identifying unknown species, limiting their effectiveness in comprehensive plant taxonomy classification. Thus we address the problem of unknown species classification by assigning it best hierarchical labels. We propose a novel method, which integrates DenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for hierarchical classification. The approach systematically categorizes medicinal plants at multiple taxonomic levels, from phylum to species, ensuring detailed and precise classification. Using multi scale space attention, the model captures both local and global contextual information from the images, improving the distinction between similar species and the identification of new ones. It uses attention scores to focus on important features across multiple scales. The proposed method provides a solution for hierarchical classification, showcasing superior performance in identifying both known and unknown species. The model was tested on two state-of-art datasets with and without background artifacts and so that it can be deployed to tackle real word application. We used unknown species for testing our model. For unknown species the model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for predicting correct phylum, class, order and family respectively. Our proposed model size is almost four times less than the existing state of the art methods making it easily deploy able in real world application.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForesightNav: Learning Scene Imagination for Efficient Exploration</title>
<link>https://arxiv.org/abs/2504.16062</link>
<guid>https://arxiv.org/abs/2504.16062</guid>
<content:encoded><![CDATA[
arXiv:2504.16062v3 Announce Type: replace-cross 
Abstract: Understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. In this work, we propose ForesightNav, a novel exploration strategy inspired by human imagination and reasoning. Our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. These predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. We validate our imagination-based approach using the Structured3D dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. Our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav on the Structured3D Validation split. These contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon</title>
<link>https://arxiv.org/abs/2504.16276</link>
<guid>https://arxiv.org/abs/2504.16276</guid>
<content:encoded><![CDATA[
<div> Keywords: bird call classification, rare species, embedding space, conservation, endangered birds

Summary:
This paper introduces an automated bird call classification pipeline specifically designed for rare species that are not covered by existing large classifiers such as BirdNET and Perch. These traditional models lack the capability to detect species with limited training data, which poses a challenge for conservationists monitoring endangered birds with only a few known recordings. The approach leverages the embedding space of established bird classification networks and utilizes cosine similarity, along with filtering and denoising preprocessing methods, to enhance detection accuracy with minimal training data. The evaluation includes testing with Xeno-Canto recordings and a real-world application on the critically endangered tooth-billed pigeon, achieving perfect recall and high accuracy. This open-source system provides a practical solution for conservationists to identify and track rare species on the brink of extinction. 

<br /><br />Summary: <div>
arXiv:2504.16276v2 Announce Type: replace-cross 
Abstract: This paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like BirdNET and Perch. While these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. To address this, we leverage the embedding space of large bird classification networks and develop a classifier using cosine similarity, combined with filtering and denoising preprocessing techniques, to optimize detection with minimal training data. We evaluate various embedding spaces using clustering metrics and validate our approach in both a simulated scenario with Xeno-Canto recordings and a real-world test on the critically endangered tooth-billed pigeon (Didunculus strigirostris), which has no existing classifiers and only three confirmed recordings. The final model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon calls, making it practical for use in the field. This open-source system provides a practical tool for conservationists seeking to detect and monitor rare species on the brink of extinction.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes</title>
<link>https://arxiv.org/abs/2505.00734</link>
<guid>https://arxiv.org/abs/2505.00734</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, 3D reconstruction, novel view synthesis, calibrated cameras, disaster relief 

Summary: 
The article introduces a new public benchmark dataset for 3D reconstruction and novel view synthesis, designed to address challenges faced by first responders and law enforcement in creating photorealistic, navigable 3D site models. The dataset includes images from ground-level, security-level, and airborne cameras, posing real-world challenges such as limited numbers of images, heterogeneous unposed cameras, inconsistent lighting, and extreme viewpoint differences. The evaluation focuses on calibration of cameras and the quality of rendered views, with baseline performance demonstrated using up-to-date methods. The dataset aims to promote further research in this area by highlighting key challenges and opportunities for improvement. <div>
arXiv:2505.00734v1 Announce Type: new 
Abstract: Production of photorealistic, navigable 3D site models requires a large volume of carefully collected images that are often unavailable to first responders for disaster relief or law enforcement. Real-world challenges include limited numbers of images, heterogeneous unposed cameras, inconsistent lighting, and extreme viewpoint differences for images collected from varying altitudes. To promote research aimed at addressing these challenges, we have developed the first public benchmark dataset for 3D reconstruction and novel view synthesis based on multiple calibrated ground-level, security-level, and airborne cameras. We present datasets that pose real-world challenges, independently evaluate calibration of unposed cameras and quality of novel rendered views, demonstrate baseline performance using recent state-of-practice methods, and identify challenges for further research.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection</title>
<link>https://arxiv.org/abs/2505.00739</link>
<guid>https://arxiv.org/abs/2505.00739</guid>
<content:encoded><![CDATA[
<div> Motion-Guided Prompting, Sparse motion, Dense motion, Spatial-Temporal Memory Selection, Video object segmentation 

Summary:
MoSAM is introduced to enhance interactive object segmentation in videos by addressing challenges faced by SAM2. By incorporating object motion cues through Motion-Guided Prompting (MGP), the model adjusts focus based on motion direction, improving object tracking. Additionally, a Spatial-Temporal Memory Selection (ST-MS) mechanism dynamically identifies accurate segmentation frames, enhancing memory reliability. The combination of MGP and ST-MS in MoSAM leads to state-of-the-art results in video object segmentation and instance segmentation benchmarks. <div>
arXiv:2505.00739v1 Announce Type: new 
Abstract: The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional capabilities in interactive object segmentation for both images and videos. However, as a foundational model on interactive segmentation, SAM2 performs segmentation directly based on mask memory from the past six frames, leading to two significant challenges. Firstly, during inference in videos, objects may disappear since SAM2 relies solely on memory without accounting for object motion information, which limits its long-range object tracking capabilities. Secondly, its memory is constructed from fixed past frames, making it susceptible to challenges associated with object disappearance or occlusion, due to potentially inaccurate segmentation results in memory. To address these problems, we present MoSAM, incorporating two key strategies to integrate object motion cues into the model and establish more reliable feature memory. Firstly, we propose Motion-Guided Prompting (MGP), which represents the object motion in both sparse and dense manners, then injects them into SAM2 through a set of motion-guided prompts. MGP enables the model to adjust its focus towards the direction of motion, thereby enhancing the object tracking capabilities. Furthermore, acknowledging that past segmentation results may be inaccurate, we devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically identifies frames likely to contain accurate segmentation in both pixel- and frame-level. By eliminating potentially inaccurate mask predictions from memory, we can leverage more reliable memory features to exploit similar regions for improving segmentation results. Extensive experiments on various benchmarks of video object segmentation and video instance segmentation demonstrate that our MoSAM achieves state-of-the-art results compared to other competitors.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast2comm:Collaborative perception combined with prior knowledge</title>
<link>https://arxiv.org/abs/2505.00740</link>
<guid>https://arxiv.org/abs/2505.00740</guid>
<content:encoded><![CDATA[
<div> Keywords: Collaborative perception, Prior knowledge, Confidence features, Spatial prior, Bandwidth efficiency <br />
<br />
Summary: <br />
Collaborative perception can improve perceptual accuracy by sharing information among agents, but faces challenges such as balancing performance and bandwidth. The Fast2comm framework addresses these challenges by generating prior-supervised confidence features to distinguish foreground from background effectively. It utilizes GT Bounding Box-based spatial prior feature selection to optimize bandwidth efficiency while minimizing noise and adapting to localization errors. Feature fusion strategies are decoupled for dynamic bandwidth adaptation. Experimental validation on real-world and simulated datasets demonstrates the effectiveness of the proposed methods. The code is available on GitHub for further exploration. <div>
arXiv:2505.00740v1 Announce Type: new 
Abstract: Collaborative perception has the potential to significantly enhance perceptual accuracy through the sharing of complementary information among agents. However, real-world collaborative perception faces persistent challenges, particularly in balancing perception performance and bandwidth limitations, as well as coping with localization errors. To address these challenges, we propose Fast2comm, a prior knowledge-based collaborative perception framework. Specifically, (1)we propose a prior-supervised confidence feature generation method, that effectively distinguishes foreground from background by producing highly discriminative confidence features; (2)we propose GT Bounding Box-based spatial prior feature selection strategy to ensure that only the most informative prior-knowledge features are selected and shared, thereby minimizing background noise and optimizing bandwidth efficiency while enhancing adaptability to localization inaccuracies; (3)we decouple the feature fusion strategies between model training and testing phases, enabling dynamic bandwidth adaptation. To comprehensively validate our framework, we conduct extensive experiments on both real-world and simulated datasets. The results demonstrate the superior performance of our model and highlight the necessity of the proposed methods. Our code is available at https://github.com/Zhangzhengbin-TJ/Fast2comm.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models</title>
<link>https://arxiv.org/abs/2505.00741</link>
<guid>https://arxiv.org/abs/2505.00741</guid>
<content:encoded><![CDATA[
<div> Keywords: Plant diseases, Convolutional Neural Networks, Long Short-Term Memory, Classification, Agriculture<br />
<br />
Summary: <br />
Plant diseases are a significant challenge in agriculture, impacting crop yield and quality. Early detection and classification are crucial for effective management. This study utilized Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) models to classify plant leaf diseases. The CNN model achieved high accuracies of 99.1% in training and 96.4% in validation, while the LSTM model reached 93.43% validation accuracy. Performance metrics such as precision, recall, F1-score, and confusion matrix confirmed the reliability of the CNN approach. The results demonstrate that deep learning models, especially CNN, offer a reliable and scalable solution for accurate plant disease classification. This technology can be applied practically for agricultural monitoring and management. <br /> <div>
arXiv:2505.00741v1 Announce Type: new 
Abstract: Plant diseases pose a serious challenge to agriculture by reducing crop yield and affecting food quality. Early detection and classification of these diseases are essential for minimising losses and improving crop management practices. This study applies Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) models to classify plant leaf diseases using a dataset containing 70,295 training images and 17,572 validation images across 38 disease classes. The CNN model was trained using the Adam optimiser with a learning rate of 0.0001 and categorical cross-entropy as the loss function. After 10 training epochs, the model achieved a training accuracy of 99.1% and a validation accuracy of 96.4%. The LSTM model reached a validation accuracy of 93.43%. Performance was evaluated using precision, recall, F1-score, and confusion matrix, confirming the reliability of the CNN-based approach. The results suggest that deep learning models, particularly CNN, enable an effective solution for accurate and scalable plant disease classification, supporting practical applications in agricultural monitoring.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zoomer: Adaptive Image Focus Optimization for Black-box MLLM</title>
<link>https://arxiv.org/abs/2505.00742</link>
<guid>https://arxiv.org/abs/2505.00742</guid>
<content:encoded><![CDATA[
<div> Keywords: MLLMs, visual prompting, object recognition, token limits, performance improvement
Summary: 
\SysName is a novel visual prompting mechanism designed to enhance the performance of multimodal large language models (MLLMs) in vision-language tasks. It aims to address the challenges faced by these models in accurately processing visual data, especially in tasks requiring precise object recognition and fine visual details. \SysName introduces a prompt-aware strategy that highlights relevant image regions, a spatial-preserving orchestration schema to maintain object integrity, and a budget-aware prompting method to balance global context with essential visual details. Evaluations across multiple datasets show that \SysName consistently outperforms baseline methods, achieving up to a 26.9% improvement in accuracy while significantly reducing token consumption.<br /><br />Summary: <br />SysName enhances MLLM performance by highlighting relevant image regions dynamically, maintaining object integrity through a spatial-preserving orchestration schema, and balancing global context with visual details using a budget-aware prompting method. Comprehensive evaluations demonstrate its superiority over baseline methods, achieving significant accuracy improvement and reduced token consumption. <div>
arXiv:2505.00742v1 Announce Type: new 
Abstract: Recent advancements in multimodal large language models (MLLMs) have broadened the scope of vision-language tasks, excelling in applications like image captioning and interactive question-answering. However, these models struggle with accurately processing visual data, particularly in tasks requiring precise object recognition and fine visual details. Stringent token limits often result in the omission of critical information, hampering performance. To address these limitations, we introduce \SysName, a novel visual prompting mechanism designed to enhance MLLM performance while preserving essential visual details within token limits. \SysName features three key innovations: a prompt-aware strategy that dynamically highlights relevant image regions, a spatial-preserving orchestration schema that maintains object integrity, and a budget-aware prompting method that balances global context with crucial visual details. Comprehensive evaluations across multiple datasets demonstrate that \SysName consistently outperforms baseline methods, achieving up to a $26.9\%$ improvement in accuracy while significantly reducing token consumption.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2505.00743</link>
<guid>https://arxiv.org/abs/2505.00743</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-and-Language Navigation, Object Perception, Text Semantic Extraction, Multi-layer Transformer networks, Navigation decisions

Summary: 
The paper discusses the limitations of current Vision-and-Language Navigation (VLN) models, which include incomplete utilization of detailed language instructions and a lack of modeling of object relationships across modalities. To address these issues, the authors propose a Dual Object Perception-Enhancement Network (DOPE). The DOPE model consists of two modules: Text Semantic Extraction (TSE) and Text Object Perception-Augmentation (TOPA) to extract essential phrases from text instructions and leverage detailed object information. Additionally, an Image Object Perception-Augmentation (IOPA) module is introduced to enhance the modeling of object relationships across different modalities in images and text. Experimental results on the R2R and REVERIE datasets demonstrate the effectiveness of the proposed approach in improving navigation performance by enhancing decision-making accuracy and utilizing latent clues between objects.<br /><br />Summary: <div>
arXiv:2505.00743v1 Announce Type: new 
Abstract: Vision-and-Language Navigation (VLN) is a challenging task where an agent must understand language instructions and navigate unfamiliar environments using visual cues. The agent must accurately locate the target based on visual information from the environment and complete tasks through interaction with the surroundings. Despite significant advancements in this field, two major limitations persist: (1) Many existing methods input complete language instructions directly into multi-layer Transformer networks without fully exploiting the detailed information within the instructions, thereby limiting the agent's language understanding capabilities during task execution; (2) Current approaches often overlook the modeling of object relationships across different modalities, failing to effectively utilize latent clues between objects, which affects the accuracy and robustness of navigation decisions. We propose a Dual Object Perception-Enhancement Network (DOPE) to address these issues to improve navigation performance. First, we design a Text Semantic Extraction (TSE) to extract relatively essential phrases from the text and input them into the Text Object Perception-Augmentation (TOPA) to fully leverage details such as objects and actions within the instructions. Second, we introduce an Image Object Perception-Augmentation (IOPA), which performs additional modeling of object information across different modalities, enabling the model to more effectively utilize latent clues between objects in images and text, enhancing decision-making accuracy. Extensive experiments on the R2R and REVERIE datasets validate the efficacy of the proposed approach.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.00744</link>
<guid>https://arxiv.org/abs/2505.00744</guid>
<content:encoded><![CDATA[
<div> Localization, hallucination, medical, LMMs, HEAL-MedVQA <br />
Summary: <br />
The study discusses the limitations of current Medical Large Multi-modal Models (LMMs) in interpreting medical data, pointing out the issue of generating hallucinations that contradict source evidence due to inadequate localization reasoning. To address this, the authors introduce the HEAL-MedVQA benchmark, which evaluates LMMs' localization abilities and hallucination robustness through innovative evaluation protocols and a dataset of 67k VQA pairs. They propose the LobA framework to train LMMs to localize target regions of interest and emphasize segmented pathological areas for more grounded answers. Experimental results show that this approach outperforms existing biomedical LMMs on the challenging HEAL-MedVQA benchmark, enhancing robustness in medical Visual Question Answering (VQA). <div>
arXiv:2505.00744v1 Announce Type: new 
Abstract: Medical Large Multi-modal Models (LMMs) have demonstrated remarkable capabilities in medical data interpretation. However, these models frequently generate hallucinations contradicting source evidence, particularly due to inadequate localization reasoning. This work reveals a critical limitation in current medical LMMs: instead of analyzing relevant pathological regions, they often rely on linguistic patterns or attend to irrelevant image areas when responding to disease-related queries. To address this, we introduce HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive benchmark designed to evaluate LMMs' localization abilities and hallucination robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA pairs, with doctor-annotated anatomical segmentation masks for pathological regions. To improve visual reasoning, we propose the Localize-before-Answer (LobA) framework, which trains LMMs to localize target regions of interest and self-prompt to emphasize segmented pathological areas, generating grounded and reliable answers. Experimental results demonstrate that our approach significantly outperforms state-of-the-art biomedical LMMs on the challenging HEAL-MedVQA benchmark, advancing robustness in medical VQA.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations</title>
<link>https://arxiv.org/abs/2505.00745</link>
<guid>https://arxiv.org/abs/2505.00745</guid>
<content:encoded><![CDATA[
<div> optimizing responsiveness, continuous model adaptation, hierarchical collaborations, mobile video analysis, expert DNN models <br />
Summary:
MOCHA is a novel framework designed to enhance the responsiveness of continuous model adaptation in mobile video analysis systems. It utilizes hierarchical collaborations between mobile and cloud resources to reduce adaptation response delays by performing on-device model reuse and fast fine-tuning. The framework accelerates the retrieval of historical expert models by organizing them into a structured taxonomy based on domain semantics analyzed by a cloud foundation model. Efficient local model reuse is enabled through the maintenance of onboard expert model caches for frequent scenes, with proactive prefetching of model weights from the cloud model database. Extensive evaluations with real-world videos on three DNN tasks demonstrate that MOCHA improves model accuracy during adaptation by up to 6.8% while saving response delay and retraining time by up to 35.5x and 3.0x, respectively. <br /><br /> <div>
arXiv:2505.00745v1 Announce Type: new 
Abstract: Mobile video analysis systems often encounter various deploying environments, where environment shifts present greater demands for responsiveness in adaptations of deployed "expert DNN models". Existing model adaptation frameworks primarily operate in a cloud-centric way, exhibiting degraded performance during adaptation and delayed reactions to environment shifts. Instead, this paper proposes MOCHA, a novel framework optimizing the responsiveness of continuous model adaptation through hierarchical collaborations between mobile and cloud resources. Specifically, MOCHA (1) reduces adaptation response delays by performing on-device model reuse and fast fine-tuning before requesting cloud model retrieval and end-to-end retraining; (2) accelerates history expert model retrieval by organizing them into a structured taxonomy utilizing domain semantics analyzed by a cloud foundation model as indices; (3) enables efficient local model reuse by maintaining onboard expert model caches for frequent scenes, which proactively prefetch model weights from the cloud model database. Extensive evaluations with real-world videos on three DNN tasks show MOCHA improves the model accuracy during adaptation by up to 6.8% while saving the response delay and retraining time by up to 35.5x and 3.0x respectively.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis</title>
<link>https://arxiv.org/abs/2505.00746</link>
<guid>https://arxiv.org/abs/2505.00746</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, OCR errors, entropy, uncertainty landscape, GPT-4o <br />
Summary:
Vision-language models like OpenAI GPT-4o can transcribe mathematical documents from images. This study introduces a method that uses token-level confidence signals to identify local recognition errors. By converting Shannon entropy into an uncertainty landscape, a sliding window technique highlights areas likely to contain OCR errors. The analysis of curated research pages shows that true errors are concentrated in high-entropy regions. This approach proves to be a practical and lightweight tool for post-editing GPT-based OCR. The study provides code, sample data, and annotation guidelines for replication and further research. <br /> <div>
arXiv:2505.00746v1 Announce Type: new 
Abstract: Vision-language models such as OpenAI GPT-4o can transcribe mathematical documents directly from images, yet their token-level confidence signals are seldom used to pinpoint local recognition mistakes. We present an entropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into a visual ''uncertainty landscape''. By scanning the entropy sequence with a fixed-length sliding window, we obtain hotspots that are likely to contain OCR errors such as missing symbols, mismatched braces, or garbled prose. Using a small, curated set of scanned research pages rendered at several resolutions, we compare the highlighted hotspots with the actual transcription errors produced by GPT-4o. Our analysis shows that the vast majority of true errors are indeed concentrated inside the high-entropy regions. This study demonstrates--in a minimally engineered setting--that sliding-window entropy can serve as a practical, lightweight aid for post-editing GPT-based OCR. All code, sample data, and annotation guidelines are released to encourage replication and further research.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstructAttribute: Fine-grained Object Attributes editing with Instruction</title>
<link>https://arxiv.org/abs/2505.00751</link>
<guid>https://arxiv.org/abs/2505.00751</guid>
<content:encoded><![CDATA[
<div> diffusion models, T2I, image editing applications, precise control, attribute amplification<br />
Summary:<br />
The article introduces the Structure-Preserving and Attribute Amplification (SPAA) method for achieving precise control over color and material transformations in text-to-image diffusion models. The SPAA method allows for editing self-attention maps and cross-attention values to modify fine-grained attributes of objects effectively. The authors also created the Attribute Dataset using multimodal large language models (MLLM) to facilitate data filtering and instruction labeling for training. Their InstructAttribute model is instruction-based and enables fine-grained editing of object attributes, surpassing existing approaches in object-level color and material editing. The method's superior performance in achieving precise control over attributes in image editing applications is demonstrated through extensive experiments. <br /><br />Summary: <div>
arXiv:2505.00751v1 Announce Type: new 
Abstract: Text-to-image (T2I) diffusion models, renowned for their advanced generative abilities, are extensively utilized in image editing applications, demonstrating remarkable effectiveness. However, achieving precise control over fine-grained attributes still presents considerable challenges. Existing image editing techniques either fail to modify the attributes of an object or struggle to preserve its structure and maintain consistency in other areas of the image. To address these challenges, we propose the Structure-Preserving and Attribute Amplification (SPAA), a training-free method which enables precise control over the color and material transformations of objects by editing the self-attention maps and cross-attention values. Furthermore, we constructed the Attribute Dataset, which encompasses nearly all colors and materials associated with various objects, by integrating multimodal large language models (MLLM) to develop an automated pipeline for data filtering and instruction labeling. Training on this dataset, we present our InstructAttribute, an instruction-based model designed to facilitate fine-grained editing of color and material attributes. Extensive experiments demonstrate that our method achieves superior performance in object-level color and material editing, outperforming existing instruction-based image editing approaches.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking</title>
<link>https://arxiv.org/abs/2505.00752</link>
<guid>https://arxiv.org/abs/2505.00752</guid>
<content:encoded><![CDATA[
<div> Keywords: Nighttime UAV tracking, Dynamic Feature Blender, Dynamic Feature Activator, Vision Transformer, Efficiency

Summary: 
Nighttime UAV tracking is a challenging task due to illumination variations and perspective changes. Existing approaches either have high computational costs or introduce redundant mechanisms. In response, DARTer (Dynamic Adaptive Representation Tracker) is proposed as an end-to-end tracking framework for nighttime UAV scenarios. DARTer utilizes a Dynamic Feature Blender (DFB) to fuse multi-perspective nighttime features for robust representation and a Dynamic Feature Activator (DFA) to adaptively activate Vision Transformer layers, improving efficiency by reducing redundant computations. The model eliminates the need for complex multi-task loss functions, streamlining the training process. Extensive experiments on nighttime UAV tracking benchmarks demonstrate the superior performance of DARTer compared to state-of-the-art trackers, showcasing its balance of tracking accuracy and efficiency for real-world applications.<br /><br />Summary: <div>
arXiv:2505.00752v1 Announce Type: new 
Abstract: Nighttime UAV tracking presents significant challenges due to extreme illumination variations and viewpoint changes, which severely degrade tracking performance. Existing approaches either rely on light enhancers with high computational costs or introduce redundant domain adaptation mechanisms, failing to fully utilize the dynamic features in varying perspectives. To address these issues, we propose \textbf{DARTer} (\textbf{D}ynamic \textbf{A}daptive \textbf{R}epresentation \textbf{T}racker), an end-to-end tracking framework designed for nighttime UAV scenarios. DARTer leverages a Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime features from static and dynamic templates, enhancing representation robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates Vision Transformer layers based on extracted features, significantly improving efficiency by reducing redundant computations. Our model eliminates the need for complex multi-task loss functions, enabling a streamlined training process. Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate the superiority of DARTer over state-of-the-art trackers. These results confirm that DARTer effectively balances tracking accuracy and efficiency, making it a promising solution for real-world nighttime UAV tracking applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors</title>
<link>https://arxiv.org/abs/2505.00755</link>
<guid>https://arxiv.org/abs/2505.00755</guid>
<content:encoded><![CDATA[
<div> approach, P2P-Insole, 3D human skeletal data, insole-type sensors, IMUs

Summary:
P2P-Insole introduces a cost-effective method for estimating and visualizing 3D human skeletal data using insole sensors integrated with IMUs. The insoles, created using e-textile garment techniques, are inexpensive and ideal for large-scale production, costing less than $1 each. By utilizing foot pressure distribution, acceleration, and rotation data, the system overcomes limitations and provides a lightweight, minimally intrusive, and privacy-aware solution. A Transformer model is employed for efficient temporal feature extraction, enhanced with first and second derivatives. The inclusion of multimodal information like accelerometers and rotational measurements improves the accuracy of recognizing complex motion patterns. Experimental results demonstrate the robustness of the approach in various posture estimation tasks. This work lays the groundwork for a practical low-cost application in rehabilitation, injury prevention, and health monitoring, with potential for further development through sensor optimization and expanded datasets. 

<br /><br />Summary: <div>
arXiv:2505.00755v1 Announce Type: new 
Abstract: This work presents P2P-Insole, a low-cost approach for estimating and visualizing 3D human skeletal data using insole-type sensors integrated with IMUs. Each insole, fabricated with e-textile garment techniques, costs under USD 1, making it significantly cheaper than commercial alternatives and ideal for large-scale production. Our approach uses foot pressure distribution, acceleration, and rotation data to overcome limitations, providing a lightweight, minimally intrusive, and privacy-aware solution. The system employs a Transformer model for efficient temporal feature extraction, enriched by first and second derivatives in the input stream. Including multimodal information, such as accelerometers and rotational measurements, improves the accuracy of complex motion pattern recognition. These facts are demonstrated experimentally, while error metrics show the robustness of the approach in various posture estimation tasks. This work could be the foundation for a low-cost, practical application in rehabilitation, injury prevention, and health monitoring while enabling further development through sensor optimization and expanded datasets.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L</title>
<link>https://arxiv.org/abs/2505.00757</link>
<guid>https://arxiv.org/abs/2505.00757</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D radar, 3D object detection, Hailo-8L AI accelerator, tensor transformation, autonomous driving

Summary: 
4D radar is being explored for its application in autonomous driving, particularly for robust 3D object detection in adverse weather conditions. This study presents the implementation of a 4D radar-based 3D object detection model on the low-power Hailo-8L AI accelerator. The challenge of adapting conventional 5D input requirements to the Hailo-8L's 4D tensor support was addressed through a tensor transformation method during compilation. The system achieved a 46.47% AP_3D and 52.75% AP_BEV accuracy, comparable to GPU-based models, with an inference speed of 13.76 Hz. These results showcase the feasibility of deploying 4D radar technology for perception tasks in autonomous driving systems.<br /><br />Summary: <div>
arXiv:2505.00757v1 Announce Type: new 
Abstract: 4D radar has attracted attention in autonomous driving due to its ability to enable robust 3D object detection even under adverse weather conditions. To practically deploy such technologies, it is essential to achieve real-time processing within low-power embedded environments. Addressing this, we present the first on-chip implementation of a 4D radar-based 3D object detection model on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D tensors, posing a significant challenge. To overcome this limitation, we introduce a tensor transformation method that reshapes 5D inputs into 4D formats during the compilation process, enabling direct deployment without altering the model structure. The proposed system achieves 46.47% AP_3D and 52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while achieving an inference speed of 13.76 Hz. These results demonstrate the applicability of 4D radar-based perception technologies to autonomous driving systems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Language Models as Text-to-Image Model Evaluators</title>
<link>https://arxiv.org/abs/2505.00759</link>
<guid>https://arxiv.org/abs/2505.00759</guid>
<content:encoded><![CDATA[
<div> Evaluator agents, multi-modal large language models, prompt-generation consistency, image aesthetics, T2I model performance<br />
Summary:<br /> 
The article proposes a new evaluation framework, MT2IE, utilizing multi-modal large language models (MLLMs) to assess prompt-generation consistency and image aesthetics in text-to-image (T2I) generative models. By generating prompts iteratively, MT2IE efficiently evaluates T2I models and matches existing benchmark evaluations using only a fraction of prompts. The prompt-generation consistency scores from MT2IE show higher correlation with human judgment compared to previous scores. The framework successfully ranks T2I models relative to existing benchmarks while significantly reducing the number of prompts needed for evaluation, highlighting its effectiveness in assessing T2I model progress. <div>
arXiv:2505.00759v1 Announce Type: new 
Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Person detection and re-identification in open-world settings of retail stores and public spaces</title>
<link>https://arxiv.org/abs/2505.00772</link>
<guid>https://arxiv.org/abs/2505.00772</guid>
<content:encoded><![CDATA[
<div> Keywords: computer vision, person re-identification, smart cities, open-world environments, tracking solutions<br />
Summary:<br />
The article discusses the challenges of implementing computer vision systems for person re-identification in open-world environments, such as smart cities. In these scenarios, system integration and operation become more complex, requiring sophisticated tracking solutions and re-identification models. The main goal is to detect and localize individuals in video frames across multiple camera feeds and varying illumination conditions. The focus is on practical applications in retail and public spaces for improved marketing analytics. The article presents a real-time solution for person re-identification performance evaluation and highlights the importance of further research in system design and improvement. Overall, the study explores various computer vision techniques to address the challenges of person re-identification in dynamic and diverse environments.<br /> 
Summary: <div>
arXiv:2505.00772v1 Announce Type: new 
Abstract: Practical applications of computer vision in smart cities usually assume system integration and operation in challenging open-world environments. In the case of person re-identification task the main goal is to retrieve information whether the specific person has appeared in another place at a different time instance of the same video, or over multiple camera feeds. This typically assumes collecting raw data from video surveillance cameras in different places and under varying illumination conditions. In the considered open-world setting it also requires detection and localization of the person inside the analyzed video frame before the main re-identification step. With multi-person and multi-camera setups the system complexity becomes higher, requiring sophisticated tracking solutions and re-identification models. In this work we will discuss existing challenges in system design architectures, consider possible solutions based on different computer vision techniques, and describe applications of such systems in retail stores and public spaces for improved marketing analytics. In order to analyse sensitivity of person re-identification task under different open-world environments, a performance of one close to real-time solution will be demonstrated over several video captures and live camera feeds. Finally, based on conducted experiments we will indicate further research directions and possible system improvements.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring</title>
<link>https://arxiv.org/abs/2505.00786</link>
<guid>https://arxiv.org/abs/2505.00786</guid>
<content:encoded><![CDATA[
<div> deep learning, radar echogram, ice sheet dynamics, snow accumulation, climate warming<br />
Summary:<br />
- Deep learning algorithms are crucial for tracking internal layers in radar echograms to understand ice sheet dynamics impacted by climate warming.
- Lack of standardized dataset hampers algorithm testing and comparison for radar echogram layer tracking problem.
- Introduction of a comprehensive radar echogram dataset derived from Snow Radar airborne data collected during NASA Operation Ice Bridge mission.
- Dataset includes labeled and weakly-labeled echograms covering diverse snow zones with varying resolutions.
- Evaluation of five deep learning models on the dataset highlights the need for advanced end-to-end models to directly extract snow depth and accumulation from echograms, reducing post-processing requirements. This dataset and benchmarking framework serve as a valuable resource for advancing radar echogram layer tracking and snow accumulation estimation to enhance understanding of polar ice sheet response to climate warming. <br /> <div>
arXiv:2505.00786v1 Announce Type: new 
Abstract: Tracking internal layers in radar echograms with high accuracy is essential for understanding ice sheet dynamics and quantifying the impact of accelerated ice discharge in Greenland and other polar regions due to contemporary global climate warming. Deep learning algorithms have become the leading approach for automating this task, but the absence of a standardized and well-annotated echogram dataset has hindered the ability to test and compare algorithms reliably, limiting the advancement of state-of-the-art methods for the radar echogram layer tracking problem. This study introduces the first comprehensive ``deep learning ready'' radar echogram dataset derived from Snow Radar airborne data collected during the National Aeronautics and Space Administration Operation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled and 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation, wet) with varying along-track resolutions. To demonstrate its utility, we evaluated the performance of five deep learning models on the dataset. Our results show that while current computer vision segmentation algorithms can identify and track snow layer pixels in echogram images, advanced end-to-end models are needed to directly extract snow depth and annual accumulation from echograms, reducing or eliminating post-processing. The dataset and accompanying benchmarking framework provide a valuable resource for advancing radar echogram layer tracking and snow accumulation estimation, advancing our understanding of polar ice sheets response to climate warming.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models</title>
<link>https://arxiv.org/abs/2505.00788</link>
<guid>https://arxiv.org/abs/2505.00788</guid>
<content:encoded><![CDATA[
<div> SpatialLLM, 3D spatial reasoning, large multimodal models, 3D training data, 3D-informed data <br />
Summary: <br />
The paper discusses the limitations of current large multimodal models (LMMs) in understanding 3D spatial relationships and proposes SpatialLLM, a model with advanced 3D reasoning abilities. By addressing the scarcity of 3D training data, the authors introduce two types of 3D-informed datasets focused on object location and orientation as well as complex spatial relationships. These datasets are integrated with architectural and training designs of LMMs to enhance 3D reasoning capabilities. The study finds that SpatialLLM outperforms GPT-4o by 8.7% in terms of 3D reasoning. The systematic approach and empirical findings provide valuable insights for future research in improving machine understanding of 3D spatial relationships. <br /> <div>
arXiv:2505.00788v1 Announce Type: new 
Abstract: Humans naturally understand 3D spatial relationships, enabling complex reasoning like predicting collisions of vehicles from different directions. Current large multimodal models (LMMs), however, lack of this capability of 3D spatial reasoning. This limitation stems from the scarcity of 3D training data and the bias in current model designs toward 2D data. In this paper, we systematically study the impact of 3D-informed data, architecture, and training setups, introducing SpatialLLM, a large multi-modal model with advanced 3D spatial reasoning abilities. To address data limitations, we develop two types of 3D-informed training datasets: (1) 3D-informed probing data focused on object's 3D location and orientation, and (2) 3D-informed conversation data for complex spatial relationships. Notably, we are the first to curate VQA data that incorporate 3D orientation relationships on real images. Furthermore, we systematically integrate these two types of training data with the architectural and training designs of LMMs, providing a roadmap for optimal design aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM advances machines toward highly capable 3D-informed reasoning, surpassing GPT-4o performance by 8.7%. Our systematic empirical design and the resulting findings offer valuable insights for future research in this direction.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging</title>
<link>https://arxiv.org/abs/2505.00805</link>
<guid>https://arxiv.org/abs/2505.00805</guid>
<content:encoded><![CDATA[
<div> Keywords: wheat, hyperspectral imaging, deep learning, crop analysis, remote sensing

Summary:<br /><br />Wheat is a crucial crop for global food security, but faces challenges from pests, diseases, climate change, and water scarcity. Traditional monitoring methods are labor-intensive, prompting the use of hyperspectral imaging (HSI) for remote crop health assessment. The high dimensionality of HSI data and limited labeled samples pose challenges, leading to the adoption of deep learning for complex structure analysis. This review discusses benchmark datasets, advancements in deep learning methods, and applications such as variety classification, disease detection, and yield estimation in wheat crops. The strengths, limitations, and future opportunities of deep learning in HSI-based wheat crop analysis are highlighted. The review also includes a list of current state-of-the-art papers and a link to updates on advancements in the field. <div>
arXiv:2505.00805v1 Announce Type: new 
Abstract: As one of the most widely cultivated and consumed crops, wheat is essential to global food security. However, wheat production is increasingly challenged by pests, diseases, climate change, and water scarcity, threatening yields. Traditional crop monitoring methods are labor-intensive and often ineffective for early issue detection. Hyperspectral imaging (HSI) has emerged as a non-destructive and efficient technology for remote crop health assessment. However, the high dimensionality of HSI data and limited availability of labeled samples present notable challenges. In recent years, deep learning has shown great promise in addressing these challenges due to its ability to extract and analysis complex structures. Despite advancements in applying deep learning methods to HSI data for wheat crop analysis, no comprehensive survey currently exists in this field. This review addresses this gap by summarizing benchmark datasets, tracking advancements in deep learning methods, and analyzing key applications such as variety classification, disease detection, and yield estimation. It also highlights the strengths, limitations, and future opportunities in leveraging deep learning methods for HSI-based wheat crop analysis. We have listed the current state-of-the-art papers and will continue tracking updating them in the following https://github.com/fadi-07/Awesome-Wheat-HSI-DeepLearning.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Comparability of Model Fusion to Measured Data in Confuser Rejection</title>
<link>https://arxiv.org/abs/2505.00836</link>
<guid>https://arxiv.org/abs/2505.00836</guid>
<content:encoded><![CDATA[
<div> Keywords: Data collection, Synthetic Aperture Radar (SAR), Simulators, Ensembling techniques, Confuser rejection

Summary:
Data collection for training large deep learning networks, particularly for Synthetic Aperture Radar (SAR), is challenging due to the limited availability of real data. To address this issue, simulators using synthetic data generation have been developed. However, training models solely on synthetic data may not yield accurate results. To overcome this limitation, the authors propose ensembling multiple models trained on synthetic data. Additionally, synthetic data may not capture all possible targets present in real environments, requiring models to be able to reject unknown targets using confuser rejection techniques. By leveraging computational power and ensembling techniques, this approach aims to enhance the performance of deep learning models trained on synthetic data for SAR applications. 

<br /><br />Summary: Data collection challenges for deep learning networks, especially in SAR, are tackled using simulators and synthetic data generation. Ensembling multiple models trained on synthetic data improves model performance, while confuser rejection techniques help in handling unknown targets. <div>
arXiv:2505.00836v1 Announce Type: new 
Abstract: Data collection has always been a major issue in the modeling and training of large deep learning networks, as no dataset can account for every slight deviation we might see in live usage. Collecting samples can be especially costly for Synthetic Aperture Radar (SAR), limiting the amount of unique targets and operating conditions we are able to observe from. To counter this lack of data, simulators have been developed utilizing the shooting and bouncing ray method to allow for the generation of synthetic SAR data on 3D models. While effective, the synthetically generated data does not perfectly correlate to the measured data leading to issues when training models solely on synthetic data. We aim to use computational power as a substitution for this lack of quality measured data, by ensembling many models trained on synthetic data. Synthetic data is also not complete, as we do not know what targets might be present in a live environment. Therefore we need to have our ensembling techniques account for these unknown targets by applying confuser rejection in which our models will reject unknown targets it is presented with, and only classify those it has been trained on.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?</title>
<link>https://arxiv.org/abs/2505.00866</link>
<guid>https://arxiv.org/abs/2505.00866</guid>
<content:encoded><![CDATA[
<div> radial distortion, relative pose estimation, pinhole solver, RANSAC, neural network
Summary:
This paper explores the estimation of relative pose between cameras in the presence of radial distortion, a common occurrence in camera setups. Traditional approaches use complex minimal radial distortion solvers within a RANSAC loop to achieve accurate results. However, the study reveals that simpler alternatives can be just as effective. Two approaches are compared: one that combines a pinhole solver with sampled radial undistortion parameters, and another that employs a neural network to estimate distortion parameters. Results from extensive experiments on various datasets and camera configurations demonstrate that complex minimal radial distortion solvers may not be necessary for practical applications. The findings shed light on the efficacy of simple sampling of radial undistortion parameters over calibrating cameras using a learning-based prior approach. The code and benchmark created for relative pose estimation under radial distortion are publicly available for further research. <br /><br />Summary: <div>
arXiv:2505.00866v1 Announce Type: new 
Abstract: Estimating the relative pose between two cameras is a fundamental step in many applications such as Structure-from-Motion. The common approach to relative pose estimation is to apply a minimal solver inside a RANSAC loop. Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras exhibit radial distortion. Not modeling radial distortion leads to (significantly) worse results. However, minimal radial distortion solvers are significantly more complex than pinhole solvers, both in terms of run-time and implementation efforts. This paper compares radial distortion solvers with two simple-to-implement approaches that do not use minimal radial distortion solvers: The first approach combines an efficient pinhole solver with sampled radial undistortion parameters, where the sampled parameters are used for undistortion prior to applying the pinhole solver. The second approach uses a state-of-the-art neural network to estimate the distortion parameters rather than sampling them from a set of potential values. Extensive experiments on multiple datasets, and different camera setups, show that complex minimal radial distortion solvers are not necessary in practice. We discuss under which conditions a simple sampling of radial undistortion parameters is preferable over calibrating cameras using a learning-based prior approach. Code and newly created benchmark for relative pose estimation under radial distortion are available at https://github.com/kocurvik/rdnet.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion</title>
<link>https://arxiv.org/abs/2505.00938</link>
<guid>https://arxiv.org/abs/2505.00938</guid>
<content:encoded><![CDATA[
<div> Object Detection, Few-shot Learning, Cross-domain, Feature Confusion, Transformer <br />
<br />
Summary: 
CDFormer is introduced as a solution to cross-domain few-shot object detection, aiming to address feature confusion challenges. It includes two key modules: Object-background Distinguishing (OBD) and Object-object Distinguishing (OOD), leveraging a learnable background token to differentiate objects from the background and enhancing the distinction between objects of different classes. Experimental results show CDFormer outperforms existing methods, achieving significant improvements under different shot settings when fine-tuned. This approach provides a promising solution for detecting novel objects across various domains with limited class instances, making it a valuable contribution to the field of few-shot object detection. <div>
arXiv:2505.00938v1 Announce Type: new 
Abstract: Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects across different domains with limited class instances. Feature confusion, including object-background confusion and object-object confusion, presents significant challenges in both cross-domain and few-shot settings. In this work, we introduce CDFormer, a cross-domain few-shot object detection transformer against feature confusion, to address these challenges. The method specifically tackles feature confusion through two key modules: object-background distinguishing (OBD) and object-object distinguishing (OOD). The OBD module leverages a learnable background token to differentiate between objects and background, while the OOD module enhances the distinction between objects of different classes. Experimental results demonstrate that CDFormer outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0% mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively, when fine-tuned.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Animated Layouts as Structured Text Representations</title>
<link>https://arxiv.org/abs/2505.00975</link>
<guid>https://arxiv.org/abs/2505.00975</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video models, animated layout generation, video advertisements, Structured Text Representation, VAKER

Summary:
Animated Layout Generation is introduced to enhance control over text elements and animated graphics in video advertisements. A structured text representation enables fine-grained video control through hierarchical visual elements. VAKER (Video Ad maKER) is presented as a text-to-video advertisement generation pipeline that utilizes a three-stage generation process with Unstructured Text Reasoning for integration with Large Language Models (LLMs). VAKER automates video advertisement generation by incorporating dynamic layout trajectories for objects and graphics in specific frames. Extensive evaluations show that VAKER outperforms existing methods in generating video advertisements. The project page provides more information on the approach and results.<br /><br />Summary: <div>
arXiv:2505.00975v1 Announce Type: new 
Abstract: Despite the remarkable progress in text-to-video models, achieving precise control over text elements and animated graphics remains a significant challenge, especially in applications such as video advertisements. To address this limitation, we introduce Animated Layout Generation, a novel approach to extend static graphic layouts with temporal dynamics. We propose a Structured Text Representation for fine-grained video control through hierarchical visual elements. To demonstrate the effectiveness of our approach, we present VAKER (Video Ad maKER), a text-to-video advertisement generation pipeline that combines a three-stage generation process with Unstructured Text Reasoning for seamless integration with LLMs. VAKER fully automates video advertisement generation by incorporating dynamic layout trajectories for objects and graphics across specific video frames. Through extensive evaluations, we demonstrate that VAKER significantly outperforms existing methods in generating video advertisements. Project Page: https://yeonsangshin.github.io/projects/Vaker
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment</title>
<link>https://arxiv.org/abs/2505.00980</link>
<guid>https://arxiv.org/abs/2505.00980</guid>
<content:encoded><![CDATA[
<div> Mamba-based depth estimation, Lightweight network, Improved performance, Lower computational complexity, Real-world deployment <br />
Summary: <br />
The article introduces LMDepth, a lightweight Mamba-based monocular depth estimation network that aims to balance performance and computational efficiency. The network incorporates a modified pyramid spatial pooling module for multi-scale feature aggregation and context extraction to ensure accurate depth estimation. It also integrates multiple depth Mamba blocks in the decoder for efficient decoding of depth information from global features. LMDepth outperforms previous lightweight depth estimation methods with fewer parameters and lower computational complexity, as demonstrated on the NYUDv2 and KITTI datasets. The network is further validated for real-world edge applications through deployment on an embedded platform with INT8 quantization. This research shows the effectiveness of LMDepth in providing high-precision depth information while maintaining low computational overhead, making it a practical solution for various applications. <br /> <div>
arXiv:2505.00980v1 Announce Type: new 
Abstract: Monocular depth estimation provides an additional depth dimension to RGB images, making it widely applicable in various fields such as virtual reality, autonomous driving and robotic navigation. However, existing depth estimation algorithms often struggle to effectively balance performance and computational efficiency, which poses challenges for deployment on resource-constrained devices. To address this, we propose LMDepth, a lightweight Mamba-based monocular depth estimation network, designed to reconstruct high-precision depth information while maintaining low computational overhead. Specifically, we propose a modified pyramid spatial pooling module that serves as a multi-scale feature aggregator and context extractor, ensuring global spatial information for accurate depth estimation. Moreover, we integrate multiple depth Mamba blocks into the decoder. Designed with linear computations, the Mamba Blocks enable LMDepth to efficiently decode depth information from global features, providing a lightweight alternative to Transformer-based architectures that depend on complex attention mechanisms. Extensive experiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of our proposed LMDepth. Compared to previous lightweight depth estimation methods, LMDepth achieves higher performance with fewer parameters and lower computational complexity (measured by GFLOPs). We further deploy LMDepth on an embedded platform with INT8 quantization, validating its practicality for real-world edge applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</title>
<link>https://arxiv.org/abs/2505.00998</link>
<guid>https://arxiv.org/abs/2505.00998</guid>
<content:encoded><![CDATA[
<div> Motion Synthesis, Human Motion, Generative Models, Latent Feature Mapping, Diversity

Summary:
The paper introduces the Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for human motion synthesis. It consists of two stages: human motion reconstruction and diverse motion generation. The first stage learns the latent space distribution of human motions, while the second stage enhances diversity and accuracy through a deterministic feature mapping procedure with DerODE and a stochastic diverse output generation procedure with DivSDE. DSDFM simplifies the training process compared to previous methods and improves diversity without adding extra training parameters. Through qualitative and quantitative experiments, DSDFM outperforms existing methods, establishing its state-of-the-art position in human motion synthesis. <div>
arXiv:2505.00998v1 Announce Type: new 
Abstract: Human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. Recent score-based generative models (SGMs) have demonstrated impressive results on this task. However, their training process involves complex curvature trajectories, leading to unstable training process. In this paper, we propose a Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for human motion synthesis. DSDFM consists of two stages. The first human motion reconstruction stage aims to learn the latent space distribution of human motions. The second diverse motion generation stage aims to build connections between the Gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. This stage is achieved by the designed deterministic feature mapping procedure with DerODE and stochastic diverse output generation procedure with DivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can enhance diversity without introducing additional training parameters.Through qualitative and quantitative experiments, DSDFM achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer</title>
<link>https://arxiv.org/abs/2505.01003</link>
<guid>https://arxiv.org/abs/2505.01003</guid>
<content:encoded><![CDATA[
<div> Graph Convolutional Networks (GCNs), Transformers, 3D human pose estimation, skeleton representation, Graph Order Attention module <br />
<br />
Summary: <br />
The article introduces a new method for 3D human pose estimation that combines the strengths of GCNs and Transformers. Current techniques often overlook spatial and temporal relationships in skeleton representations. The proposed method utilizes GCN to represent skeletons with multiple graphs of different orders, incorporating a Graph Order Attention module to highlight the most representative orders for each joint. A temporal Body Aware Transformer is then employed to model global body feature dependencies while considering local inter-skeleton feature dependencies. The self-attention mechanism is enhanced to focus on central poses while gradually reducing attention towards the first and last poses. Experimental results on multiple datasets show the effectiveness of the proposed method, providing improved accuracy in 3D pose estimation tasks. The code and models are publicly available on Github for further research and development. <div>
arXiv:2505.01003v1 Announce Type: new 
Abstract: Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the prevailing techniques for 3D human pose estimation. However, Transformer-based methods either ignore the spatial neighborhood relationships between the joints when used for skeleton representations or disregard the local temporal patterns of the local joint movements in skeleton sequence modeling, while GCN-based methods often neglect the need for pose-specific representations. To address these problems, we propose a new method that exploits the graph modeling capability of GCN to represent each skeleton with multiple graphs of different orders, incorporated with a newly introduced Graph Order Attention module that dynamically emphasizes the most representative orders for each joint. The resulting spatial features of the sequence are further processed using a proposed temporal Body Aware Transformer that models the global body feature dependencies in the sequence with awareness of the local inter-skeleton feature dependencies of joints. Given that our 3D pose output aligns with the central 2D pose in the sequence, we improve the self-attention mechanism to be aware of the central pose while diminishing its focus gradually towards the first and the last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I datasets demonstrate the effectiveness of the proposed method. Code and models are made available on Github.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance</title>
<link>https://arxiv.org/abs/2505.01016</link>
<guid>https://arxiv.org/abs/2505.01016</guid>
<content:encoded><![CDATA[
<div> object detectors, fine-grained specialization, fine-tuning depth, backbone layers, performance evaluation <br />
Summary: 
The study evaluates the impact of fine-tuning depth on adapting large pre-trained object detectors for fine-grained fruit detection. By unfreezing backbone layers at different points and training on a custom fruit dataset, the researchers found that deeper fine-tuning (unfreezing down to layer 10) led to significant performance gains on the specialized task without sacrificing performance on the original COCO benchmark. The results showed a performance increase of up to 10% absolute mAP50 on the fruit detection task when adapting mid-to-late backbone features. Importantly, there was negligible performance degradation on the COCO dataset across all tested fine-tuning depths, indicating the effectiveness of deeper fine-tuning strategies for complex domains. This suggests that adapting deeper backbone features can be a successful approach for achieving fine-grained specialization without catastrophic forgetting of the model's general capabilities. <br /><br /> <div>
arXiv:2505.01016v1 Announce Type: new 
Abstract: The success of large pre-trained object detectors hinges on their adaptability to diverse downstream tasks. While fine-tuning is the standard adaptation method, specializing these models for challenging fine-grained domains necessitates careful consideration of feature granularity. The critical question remains: how deeply should the pre-trained backbone be fine-tuned to optimize for the specialized task without incurring catastrophic forgetting of the original general capabilities? Addressing this, we present a systematic empirical study evaluating the impact of fine-tuning depth. We adapt a standard YOLOv8n model to a custom, fine-grained fruit detection dataset by progressively unfreezing backbone layers (freeze points at layers 22, 15, and 10) and training. Performance was rigorously evaluated on both the target fruit dataset and, using a dual-head evaluation architecture, on the original COCO validation set. Our results demonstrate unequivocally that deeper fine-tuning (unfreezing down to layer 10) yields substantial performance gains (e.g., +10\% absolute mAP50) on the fine-grained fruit task compared to only training the head. Strikingly, this significant adaptation and specialization resulted in negligible performance degradation (<0.1\% absolute mAP difference) on the COCO benchmark across all tested freeze levels. We conclude that adapting mid-to-late backbone features is highly effective for fine-grained specialization. Critically, our results demonstrate this adaptation can be achieved without the commonly expected penalty of catastrophic forgetting, presenting a compelling case for exploring deeper fine-tuning strategies, particularly when targeting complex domains or when maximizing specialized performance is paramount.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing</title>
<link>https://arxiv.org/abs/2505.01032</link>
<guid>https://arxiv.org/abs/2505.01032</guid>
<content:encoded><![CDATA[
<div> Keywords: edge detection, image processing, statistical testing, noise suppression, adaptive window strategy

Summary: 
The article introduces a novel approach, EDD-MAIT, for edge detection and denoising in image processing. Traditional edge detection methods often produce overly detailed edge maps, leading to a lack of clarity. EDD-MAIT addresses this issue by integrating a channel attention mechanism with independence testing to improve robustness, accuracy, and efficiency. A gradient-driven adaptive window strategy dynamically adjusts window sizes, enhancing detail preservation and noise suppression. The method outperforms traditional and learning-based techniques on BSDS500 and BIPED datasets, showcasing improvements in F-score, MSE, PSNR, and reduced runtime. EDD-MAIT also exhibits robustness against Gaussian noise, generating accurate and clean edge maps in noisy environments. <div>
arXiv:2505.01032v1 Announce Type: new 
Abstract: Edge detection is crucial in image processing, but existing methods often produce overly detailed edge maps, affecting clarity. Fixed-window statistical testing faces issues like scale mismatch and computational redundancy. To address these, we propose a novel Multi-scale Adaptive Independence Testing-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive Statistical Testing-based edge detection and denoising method that integrates a channel attention mechanism with independence testing. A gradient-driven adaptive window strategy adjusts window sizes dynamically, improving detail preservation and noise suppression. EDD-MAIT achieves better robustness, accuracy, and efficiency, outperforming traditional and learning-based methods on BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and reduced runtime. It also shows robustness against Gaussian noise, generating accurate and clean edge maps in noisy environments.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge Detection based on Channel Attention and Inter-region Independence Test</title>
<link>https://arxiv.org/abs/2505.01040</link>
<guid>https://arxiv.org/abs/2505.01040</guid>
<content:encoded><![CDATA[
<div> Keywords: edge detection, CAM-EDIT, Channel Attention Mechanism, EDIT, noise robustness<br />
Summary:<br />
Existing edge detection methods often struggle with noise amplification and retention of non-salient details in industrial scenarios. To address this, CAM-EDIT integrates CAM and EDIT techniques. The CAM enhances edge features through multi-channel fusion, while the EDIT module uses statistical independence testing to suppress noise. Experimental results on BSDS500 and NYUDv2 datasets show state-of-the-art performance, with F-measure scores improving by 19.2% to 26.5% over traditional methods. CAM-EDIT outperforms the latest learning-based methods and exhibits better noise robustness under Gaussian noise, with a 2.2% PSNR improvement. The qualitative results demonstrate cleaner edge maps with reduced artifacts, showcasing the potential of CAM-EDIT for high-precision industrial applications.<br />  
Summary: <div>
arXiv:2505.01040v1 Announce Type: new 
Abstract: Existing edge detection methods often suffer from noise amplification and excessive retention of non-salient details, limiting their applicability in high-precision industrial scenarios. To address these challenges, we propose CAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM) and Edge Detection via Independence Testing (EDIT). The CAM module adaptively enhances discriminative edge features through multi-channel fusion, while the EDIT module employs region-wise statistical independence analysis (using Fisher's exact test and chi-square test) to suppress uncorrelated noise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate state-of-the-art performance. Among the nine comparison algorithms, the F-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of 19.2\% to 26.5\% over traditional methods (Canny, CannySR), and better than the latest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations further reveal a 2.2\% PSNR improvement under Gaussian noise compared to baseline methods. Qualitative results exhibit cleaner edge maps with reduced artifacts, demonstrating its potential for high-precision industrial applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferable Adversarial Attacks on Black-Box Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.01050</link>
<guid>https://arxiv.org/abs/2505.01050</guid>
<content:encoded><![CDATA[
<div> Adversarial examples, Vision Large Language Models, Transferability, Vulnerabilities, Universal perturbations<br />
Summary:<br />
The study explores the transferability of adversarial attacks to proprietary Vision Large Language Models (VLLMs) like GPT-4o, Claude, and Gemini. It demonstrates that targeted adversarial examples can induce specific misinterpretations of visual information, such as overlooking sensitive or hazardous content. The research also reveals the efficacy of universal perturbations in consistently triggering misinterpretations across multiple VLLMs. Results across object recognition, visual question answering, and image captioning tasks highlight the widespread vulnerability of current state-of-the-art VLLMs. The findings underscore the need for robust mitigations to ensure the safe and secure deployment of VLLMs.<br /><br />Summary: <div>
arXiv:2505.01050v1 Announce Type: new 
Abstract: Vision Large Language Models (VLLMs) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. While prior research has shown that adversarial attacks can transfer from open-source to proprietary black-box models in text-only and vision-only contexts, the extent and effectiveness of such vulnerabilities remain underexplored for VLLMs. We present a comprehensive analysis demonstrating that targeted adversarial examples are highly transferable to widely-used proprietary VLLMs such as GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to induce specific attacker-chosen interpretations of visual information, such as misinterpreting hazardous content as safe, overlooking sensitive or restricted material, or generating detailed incorrect responses aligned with the attacker's intent. Furthermore, we discover that universal perturbations -- modifications applicable to a wide set of images -- can consistently induce these misinterpretations across multiple proprietary VLLMs. Our experimental results on object recognition, visual question answering, and image captioning show that this vulnerability is common across current state-of-the-art models, and underscore an urgent need for robust mitigations to ensure the safe and secure deployment of VLLMs.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation</title>
<link>https://arxiv.org/abs/2505.01057</link>
<guid>https://arxiv.org/abs/2505.01057</guid>
<content:encoded><![CDATA[
<div> CNN-based attention smoothing framework, semantic segmentation, boundary instability, geometric smoothing, Chebyshev distance, multispatial transformations

Summary:
GeloVec is a novel CNN-based attention smoothing framework for semantic segmentation, addressing limitations seen in existing methods. By implementing higher-dimensional geometric smoothing and modified Chebyshev distance metrics, GeloVec establishes robust manifold relationships between visually coherent regions, improving segmentation accuracy. The adaptive sampling weights system calculates geometric distances in n-dimensional feature space, enhancing edge preservation and intra-class homogeneity. Multispatial transformation matrix incorporates tensorial projections with orthogonal basis vectors, creating discriminative feature representations efficiently. Experimental validation on benchmark datasets demonstrates significant mIoU gains compared to state-of-the-art methods. GeloVec's foundation in Riemannian geometry provides theoretical guarantees on segmentation stability, with parallelized geodesic transformations ensuring computational efficiency and strong generalization capabilities. <div>
arXiv:2505.01057v1 Announce Type: new 
Abstract: This paper introduces GeloVec, a new CNN-based attention smoothing framework for semantic segmentation that addresses critical limitations in conventional approaches. While existing attention-backed segmentation methods suffer from boundary instability and contextual discontinuities during feature mapping, our framework implements a higher-dimensional geometric smoothing method to establish a robust manifold relationships between visually coherent regions. GeloVec combines modified Chebyshev distance metrics with multispatial transformations to enhance segmentation accuracy through stabilized feature extraction. The core innovation lies in the adaptive sampling weights system that calculates geometric distances in n-dimensional feature space, achieving superior edge preservation while maintaining intra-class homogeneity. The multispatial transformation matrix incorporates tensorial projections with orthogonal basis vectors, creating more discriminative feature representations without sacrificing computational efficiency. Experimental validation across multiple benchmark datasets demonstrates significant improvements in segmentation performance, with mean Intersection over Union (mIoU) gains of 2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets respectively compared to state-of-the-art methods. GeloVec's mathematical foundation in Riemannian geometry provides theoretical guarantees on segmentation stability. Importantly, our framework maintains computational efficiency through parallelized implementation of geodesic transformations and exhibits strong generalization capabilities across disciplines due to the absence of information loss during transformations.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.01064</link>
<guid>https://arxiv.org/abs/2505.01064</guid>
<content:encoded><![CDATA[
<div> Keywords: Fine-grained Visual Recognition, Vocabulary-Free FGVR, Nearest-Neighbor Label Refinement, Multimodal Large Language Models, Weakly supervised dataset

Summary: 
Nearest-Neighbor Label Refinement (NeaR) is introduced to address the challenges of Vocabulary-Free Fine-grained Visual Recognition (FGVR) in scenarios with limited labeled data. This approach fine-tunes a downstream CLIP model using labels generated by Multimodal Large Language Models (MLLMs), creating a weakly supervised dataset from a small, unlabeled training set. NeaR is designed to handle the noise, stochasticity, and open-endedness of MLLM-generated labels, offering a new benchmark for efficient VF-FGVR. By leveraging the capabilities of MLLMs for label generation, NeaR provides a practical and effective solution for FGVR tasks in domains like medical imaging where curated datasets are scarce. This approach enables models to make predictions in an unconstrained output space without relying on predefined training labels, opening up new possibilities for fine-grained visual recognition in challenging scenarios. 

<br /><br />Summary: <div>
arXiv:2505.01064v1 Announce Type: new 
Abstract: Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce \textbf{Nea}rest-Neighbor Label \textbf{R}efinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Editability in Image Generation with Layer-wise Memory</title>
<link>https://arxiv.org/abs/2505.01079</link>
<guid>https://arxiv.org/abs/2505.01079</guid>
<content:encoded><![CDATA[
<div> mask inputs, sequential editing, layer-wise memory, Background Consistency Guidance, Multi-Query Disentanglement

Summary:
The article discusses the challenges in sequential image editing tasks and proposes a framework that addresses these issues. It introduces the concept of rough mask inputs for preserving existing content and integrating new elements naturally. The framework utilizes layer-wise memory to store previous edits' latent representations and prompt embeddings, enabling consistent editing across multiple modifications. Background Consistency Guidance ensures scene coherence by leveraging memorized latents, while Multi-Query Disentanglement in cross-attention aids in natural adaptation to existing content. The method is evaluated on a new benchmark dataset that includes semantic alignment metrics and interactive editing scenarios, showing superior performance in iterative image editing tasks with minimal user effort. The proposed framework allows for high-quality results throughout multiple editing steps, demonstrating its efficacy in complex editing scenarios. 

<br /><br />Summary: <div>
arXiv:2505.01079v1 Announce Type: new 
Abstract: Most real-world image editing tasks require multiple sequential edits to achieve desired results. Current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. These limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. We address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. Our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. We propose Background Consistency Guidance that leverages memorized latents to maintain scene coherence and Multi-Query Disentanglement in cross-attention that ensures natural adaptation to existing content. To evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. Through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation</title>
<link>https://arxiv.org/abs/2505.01091</link>
<guid>https://arxiv.org/abs/2505.01091</guid>
<content:encoded><![CDATA[
<div> medical data generation, multimodal, chest X-rays, clinical report, generative models <br />
Summary: <br />
- Introduction of a framework specifically designed for generating multimodal medical data, including multi-view chest X-rays and clinical reports. <br />
- Utilizes the MIMIC-CXR dataset to generate high-fidelity images and coherent reports, showing superior performance in FID and BLEU scores. <br />
- Framework demonstrated comparable or superior performance to real data in disease classification tasks, showcasing potential for medical research and diagnostics. <br />
- Emphasizes the importance of domain-specific adaptations for enhancing the relevance and utility of generative models in clinical applications. <br />
- Paves the way for advancements in synthetic multimodal medical data generation. <br /> <div>
arXiv:2505.01091v1 Announce Type: new 
Abstract: Generative models have revolutionized Artificial Intelligence (AI), particularly in multimodal applications. However, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent need for clinical accuracy. In this work, we introduce a framework specifically designed for multimodal medical data generation. By enabling the generation of multi-view chest X-rays and their associated clinical report, it bridges the gap between general-purpose vision-language models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR dataset, the proposed framework shows superior performance in generating high-fidelity images and semantically coherent reports. Our quantitative evaluation reveals significant results in terms of FID and BLEU scores, showcasing the quality of the generated data. Notably, our framework achieves comparable or even superior performance compared to real data on downstream disease classification tasks, underlining its potential as a tool for medical research and diagnostics. This study highlights the importance of domain-specific adaptations in enhancing the relevance and utility of generative models for clinical applications, paving the way for future advancements in synthetic multimodal medical data generation.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2505.01096</link>
<guid>https://arxiv.org/abs/2505.01096</guid>
<content:encoded><![CDATA[
<div> radiology, artificial intelligence, healthcare, low-resource languages, Vision-Language Models (VLMs)
Summary:<br />
- The study evaluates the performance of Vision-Language Models (VLMs) in generating radiology reports in low-resource languages (Italian, German, Spanish).<br />
- Language-specific models outperformed general and domain-specific models, emphasizing the importance of linguistic adaptation.<br />
- Models fine-tuned with medical terminology showed improved performance, highlighting the significance of domain-specific training.<br />
- The influence of the temperature parameter on report coherence was explored, providing insights for optimal model settings.<br />
- Tailored language and domain-specific training are crucial for enhancing the quality and accuracy of radiological reports in multilingual settings.<br /> <div>
arXiv:2505.01096v1 Announce Type: new 
Abstract: The integration of artificial intelligence in healthcare has opened new horizons for improving medical diagnostics and patient care. However, challenges persist in developing systems capable of generating accurate and contextually relevant radiology reports, particularly in low-resource languages. In this study, we present a comprehensive benchmark to evaluate the performance of instruction-tuned Vision-Language Models (VLMs) in the specialized task of radiology report generation across three low-resource languages: Italian, German, and Spanish. Employing the LLaVA architectural framework, we conducted a systematic evaluation of pre-trained models utilizing general datasets, domain-specific datasets, and low-resource language-specific datasets. In light of the unavailability of models that possess prior knowledge of both the medical domain and low-resource languages, we analyzed various adaptations to determine the most effective approach for these contexts. The results revealed that language-specific models substantially outperformed both general and domain-specific models in generating radiology reports, emphasizing the critical role of linguistic adaptation. Additionally, models fine-tuned with medical terminology exhibited enhanced performance across all languages compared to models with generic knowledge, highlighting the importance of domain-specific training. We also explored the influence of the temperature parameter on the coherence of report generation, providing insights for optimal model settings. Our findings highlight the importance of tailored language and domain-specific training for improving the quality and accuracy of radiological reports in multilingual settings. This research not only advances our understanding of VLMs adaptability in healthcare but also points to significant avenues for future investigations into model tuning and language-specific adaptations.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VSC: Visual Search Compositional Text-to-Image Diffusion Model</title>
<link>https://arxiv.org/abs/2505.01104</link>
<guid>https://arxiv.org/abs/2505.01104</guid>
<content:encoded><![CDATA[
<div> Pairwise image embeddings, attribute-object binding, compositional generation, segmentation-based localization training, T2I CompBench <br />
<br />
Summary: 
This paper introduces a new method for improving attribute-object binding in text-to-image diffusion models. By leveraging pairwise image embeddings, the approach decomposes complex prompts into sub-prompts, generates corresponding images, and computes visual prototypes that enhance representation when fused with text embeddings. Through segmentation-based localization training, cross-attention misalignment is addressed, leading to enhanced accuracy in binding multiple attributes to objects. The proposed approach outperforms existing models on the T2I CompBench benchmark in terms of image quality, as evaluated by humans, and demonstrates increased robustness with a growing number of binding pairs in the prompt. <div>
arXiv:2505.01104v1 Announce Type: new 
Abstract: Text-to-image diffusion models have shown impressive capabilities in generating realistic visuals from natural-language prompts, yet they often struggle with accurately binding attributes to corresponding objects, especially in prompts containing multiple attribute-object pairs. This challenge primarily arises from the limitations of commonly used text encoders, such as CLIP, which can fail to encode complex linguistic relationships and modifiers effectively. Existing approaches have attempted to mitigate these issues through attention map control during inference and the use of layout information or fine-tuning during training, yet they face performance drops with increased prompt complexity. In this work, we introduce a novel compositional generation method that leverages pairwise image embeddings to improve attribute-object binding. Our approach decomposes complex prompts into sub-prompts, generates corresponding images, and computes visual prototypes that fuse with text embeddings to enhance representation. By applying segmentation-based localization training, we address cross-attention misalignment, achieving improved accuracy in binding multiple attributes to objects. Our approaches outperform existing compositional text-to-image diffusion models on the benchmark T2I CompBench, achieving better image quality, evaluated by humans, and emerging robustness under scaling number of binding pairs in the prompt.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study</title>
<link>https://arxiv.org/abs/2505.01109</link>
<guid>https://arxiv.org/abs/2505.01109</guid>
<content:encoded><![CDATA[
<div> Instance-based, Embedding-based, Multiple Instance Learning, Self-supervised learning, Whole Slide Image classification

Summary:
In this study, the researchers explore the effectiveness of instance-based and embedding-based Multiple Instance Learning (MIL) for Whole Slide Image (WSI) classification. They conduct a comprehensive analysis involving 710 experiments across 4 datasets and compare 10 MIL strategies along with 6 self-supervised methods and 4 backbones. The results show that with high-quality features extracted using self-supervised learning (SSL), simple instance-based MIL methods outperform complex embedding-based MIL methods on the BRACS and Camelyon16 datasets. This suggests that more emphasis should be placed on developing well-adapted SSL methods for WSI classification rather than relying on complex embedding-based MIL techniques. The simplicity and interpretability of instance-based MIL methods make them more accessible and explainable to clinicians, leading to improved performance and practical applications in the pathology domain. 

<br /><br />Summary: <div>
arXiv:2505.01109v1 Announce Type: new 
Abstract: Multiple Instance Learning (MIL) has emerged as the best solution for Whole Slide Image (WSI) classification. It consists of dividing each slide into patches, which are treated as a bag of instances labeled with a global label. MIL includes two main approaches: instance-based and embedding-based. In the former, each patch is classified independently, and then the patch scores are aggregated to predict the bag label. In the latter, bag classification is performed after aggregating patch embeddings. Even if instance-based methods are naturally more interpretable, embedding-based MILs have usually been preferred in the past due to their robustness to poor feature extractors. However, recently, the quality of feature embeddings has drastically increased using self-supervised learning (SSL). Nevertheless, many authors continue to endorse the superiority of embedding-based MIL. To investigate this further, we conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6 self-supervised methods with 4 backbones, 4 foundation models, and various pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL methods never used before in the pathology domain. Through these extensive experiments, we show that with a good SSL feature extractor, simple instance-based MILs, with very few parameters, obtain similar or better performance than complex, state-of-the-art (SOTA) embedding-based MIL methods, setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple instance-based MIL methods are naturally more interpretable and explainable to clinicians, our results suggest that more effort should be put into well-adapted SSL methods for WSI rather than into complex embedding-based MIL methods.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis</title>
<link>https://arxiv.org/abs/2505.01172</link>
<guid>https://arxiv.org/abs/2505.01172</guid>
<content:encoded><![CDATA[
<div> Keywords: video generation, PCA, consistency, quality, training-free

Summary:
FreePCA introduces a training-free method for generating long videos by decoupling global and local information using Principal Component Analysis (PCA). By separating consistent appearance and motion intensity features, FreePCA achieves high quality and consistency in generated videos. The method ensures smooth transitions and preserves original quality by integrating these features progressively. The reuse of mean statistics from the initial noise further enhances consistency. Experiments demonstrate the effectiveness of FreePCA across various video diffusion models without the need for additional training. The proposed approach provides a solution to the challenges associated with long video generation, such as motion inconsistency and visual quality. <div>
arXiv:2505.01172v1 Announce Type: new 
Abstract: Long video generation involves generating extended videos using models trained on short videos, suffering from distribution shifts due to varying frame counts. It necessitates the use of local information from the original short frames to enhance visual and motion quality, and global information from the entire long frames to ensure appearance consistency. Existing training-free methods struggle to effectively integrate the benefits of both, as appearance and motion in videos are closely coupled, leading to motion inconsistency and visual quality. In this paper, we reveal that global and local information can be precisely decoupled into consistent appearance and motion intensity information by applying Principal Component Analysis (PCA), allowing for refined complementary integration of global consistency and local quality. With this insight, we propose FreePCA, a training-free long video generation paradigm based on PCA that simultaneously achieves high consistency and quality. Concretely, we decouple consistent appearance and motion intensity features by measuring cosine similarity in the principal component space. Critically, we progressively integrate these features to preserve original quality and ensure smooth transitions, while further enhancing consistency by reusing the mean statistics of the initial noise. Experiments demonstrate that FreePCA can be applied to various video diffusion models without requiring training, leading to substantial improvements. Code is available at https://github.com/JosephTiTan/FreePCA.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSTMotion: Training-free Scene-awarenText-to-motion Generation</title>
<link>https://arxiv.org/abs/2505.01182</link>
<guid>https://arxiv.org/abs/2505.01182</guid>
<content:encoded><![CDATA[
<div> framework, text-to-motion, scene-aware, pre-trained, motion generation <br />
Summary: 
The article introduces a novel Training-free Scene-aware Text-to-Motion framework, called TSTMotion, which enhances pre-trained blank-background motion generators with scene-aware capabilities. By leveraging foundation models, the framework can generate scene-aware text-driven motion sequences based on 3D scenes and text descriptions. The framework eliminates the need for large-scale ground-truth motion sequences in diverse 3D scenes, making it more cost-effective. Experimental results show the effectiveness and versatility of the proposed framework. Code for the framework is available on the project page. <div>
arXiv:2505.01182v1 Announce Type: new 
Abstract: Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a \textbf{T}raining-free \textbf{S}cene-aware \textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in \href{https://tstmotion.github.io/}{Project Page}.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Vision-based Vehicle Speed Estimation</title>
<link>https://arxiv.org/abs/2505.01203</link>
<guid>https://arxiv.org/abs/2505.01203</guid>
<content:encoded><![CDATA[
<div> improvements, real-time performance, speed estimation, vehicle detection, computational efficiency  
Summary:  
The paper introduces a computationally efficient method for estimating vehicle speed from traffic camera footage, building on previous work involving 3D bounding boxes and vanishing point geometry. Several enhancements are made to improve real-time performance, with evaluations conducted on the BrnoCompSpeed dataset showing improved speed estimation accuracy and detection precision. The study explores the trade-off between accuracy and computational cost, highlighting the effectiveness of smaller models with post-training quantization for real-world implementation. The best performing model outperforms previous benchmarks in median speed estimation error, detection precision, and recall, while achieving a 5.5 times increase in speed. <div>
arXiv:2505.01203v1 Announce Type: new 
Abstract: This paper presents a computationally efficient method for vehicle speed estimation from traffic camera footage. Building upon previous work that utilizes 3D bounding boxes derived from 2D detections and vanishing point geometry, we introduce several improvements to enhance real-time performance. We evaluate our method in several variants on the BrnoCompSpeed dataset in terms of vehicle detection and speed estimation accuracy. Our extensive evaluation across various hardware platforms, including edge devices, demonstrates significant gains in frames per second (FPS) compared to the prior state-of-the-art, while maintaining comparable or improved speed estimation accuracy. We analyze the trade-off between accuracy and computational cost, showing that smaller models utilizing post-training quantization offer the best balance for real-world deployment. Our best performing model beats previous state-of-the-art in terms of median vehicle speed estimation error (0.58 km/h vs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs. 83.32%) while also being 5.5 times faster.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph</title>
<link>https://arxiv.org/abs/2505.01207</link>
<guid>https://arxiv.org/abs/2505.01207</guid>
<content:encoded><![CDATA[
<div> Sparse-view camera pose estimation, T-Graph, translation information, Multilayer Perceptron, relative-t, pair-t 

Summary: 
The article introduces T-Graph, a lightweight module designed to enhance camera pose estimation in sparse-view scenarios by considering translation information between viewpoints. T-Graph utilizes a Multilayer Perceptron to construct a fully connected translation graph, improving the accuracy of camera pose estimation. Two representations, relative-t and pair-t, capture spatial relationships and rotation-disentangled information, enhancing adaptability across diverse applications. Experiments conducted on two state-of-the-art methods, RelPose++ and Forge, using public datasets (C03D and IMC PhotoTourism) demonstrate the effectiveness and generalizability of T-Graph. Consistent improvements in camera center accuracy, ranging from 1% to 6% with 2 to 8 viewpoints, validate the module's performance in sparse-view settings. <div>
arXiv:2505.01207v1 Announce Type: new 
Abstract: Sparse-view camera pose estimation, which aims to estimate the 6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from different viewpoints, is a fundamental yet challenging problem in remote sensing applications. Existing methods often overlook the translation information between each pair of viewpoints, leading to suboptimal performance in sparse-view scenarios. To address this limitation, we introduce T-Graph, a lightweight, plug-and-play module to enhance camera pose estimation in sparse-view settings. T-graph takes paired image features as input and maps them through a Multilayer Perceptron (MLP). It then constructs a fully connected translation graph, where nodes represent cameras and edges encode their translation relationships. It can be seamlessly integrated into existing models as an additional branch in parallel with the original prediction, maintaining efficiency and ease of use. Furthermore, we introduce two pairwise translation representations, relative-t and pair-t, formulated under different local coordinate systems. While relative-t captures intuitive spatial relationships, pair-t offers a rotation-disentangled alternative. The two representations contribute to enhanced adaptability across diverse application scenarios, further improving our module's robustness. Extensive experiments on two state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D and IMC PhotoTourism) validate both the effectiveness and generalizability of T-Graph. The results demonstrate consistent improvements across various metrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8 viewpoints.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Dynamic Range Novel View Synthesis with Single Exposure</title>
<link>https://arxiv.org/abs/2505.01212</link>
<guid>https://arxiv.org/abs/2505.01212</guid>
<content:encoded><![CDATA[
<div> Keywords: High Dynamic Range, Novel View Synthesis, Single-exposure, LDR images, Mono-HDR-3D<br />
Summary:<br />
High Dynamic Range Novel View Synthesis (HDR-NVS) aims to create a 3D scene HDR model from Low Dynamic Range (LDR) imagery. Current multiple-exposure HDR-NVS methods face drawbacks like motion artifacts and high costs. This study introduces the single-exposure HDR-NVS problem and presents the Mono-HDR-3D approach, comprising two modules for converting LDR colors to HDR and vice versa. This design enables unsupervised learning through a closed loop system. Mono-HDR-3D can seamlessly integrate with existing NVS models. Extensive experiments demonstrate its superior performance compared to previous methods. Source code will be made available for further research and development.<br /><br />Summary: High Dynamic Range Novel View Synthesis (HDR-NVS) addresses the challenge of creating a 3D scene HDR model using single-exposure LDR images. The Mono-HDR-3D approach introduces two modules dedicated to LDR to HDR conversion and enables unsupervised learning in a closed loop system. This approach surpasses previous methods in performance and can be easily integrated with existing NVS models. <div>
arXiv:2505.01212v1 Announce Type: new 
Abstract: High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D scene HDR model from Low Dynamic Range (LDR) imagery. Typically, multiple-exposure LDR images are employed to capture a wider range of brightness levels in a scene, as a single LDR image cannot represent both the brightest and darkest regions simultaneously. While effective, this multiple-exposure HDR-NVS approach has significant limitations, including susceptibility to motion artifacts (e.g., ghosting and blurring), high capture and storage costs. To overcome these challenges, we introduce, for the first time, the single-exposure HDR-NVS problem, where only single exposure LDR images are available during training. We further introduce a novel approach, Mono-HDR-3D, featuring two dedicated modules formulated by the LDR image formation principles, one for converting LDR colors to HDR counterparts, and the other for transforming HDR images to LDR format so that unsupervised learning is enabled in a closed loop. Designed as a meta-algorithm, our approach can be seamlessly integrated with existing NVS models. Extensive experiments show that Mono-HDR-3D significantly outperforms previous methods. Source code will be released.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement</title>
<link>https://arxiv.org/abs/2505.01224</link>
<guid>https://arxiv.org/abs/2505.01224</guid>
<content:encoded><![CDATA[
<div> Keywords: Underwater image enhancement, state space models, spatial correlation, dynamic convolution, multi-scale representations

Summary: 
The article introduces a novel framework called RD-UIE for underwater image enhancement. It addresses the limitations of existing state space models by incorporating a sorting-based scanning mechanism that dynamically reorders scanning sequences based on spatial correlation. This mechanism prioritizes informative components and enhances feature extraction. The Visually Self-adaptive State Block (VSSB) harmonizes dynamic sorting with input-dependent dynamic convolution to integrate global context and local relational cues effectively. Additionally, the cross-feature bridge (CFB) is designed to adaptively fuse multi-scale representations for robust feature extraction and refinement. Experimental results on underwater enhancement benchmarks demonstrate that RD-UIE outperforms the current state-of-the-art approach, WMamba, in terms of quantitative metrics and visual fidelity, achieving a 0.55 dB performance gain on average. The code for RD-UIE is publicly available on GitHub. 

<br /><br />Summary: <div>
arXiv:2505.01224v1 Announce Type: new 
Abstract: Underwater image enhancement (UIE) is a critical preprocessing step for marine vision applications, where wavelength-dependent attenuation causes severe content degradation and color distortion. While recent state space models like Mamba show potential for long-range dependency modeling, their unfolding operations and fixed scan paths on 1D sequences fail to adapt to local object semantics and global relation modeling, limiting their efficacy in complex underwater environments. To address this, we enhance conventional Mamba with the sorting-based scanning mechanism that dynamically reorders scanning sequences based on statistical distribution of spatial correlation of all pixels. In this way, it encourages the network to prioritize the most informative components--structural and semantic features. Upon building this mechanism, we devise a Visually Self-adaptive State Block (VSSB) that harmonizes dynamic sorting of Mamba with input-dependent dynamic convolution, enabling coherent integration of global context and local relational cues. This exquisite design helps eliminate global focus bias, especially for widely distributed contents, which greatly weakens the statistical frequency. For robust feature extraction and refinement, we design a cross-feature bridge (CFB) to adaptively fuse multi-scale representations. These efforts compose the novel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive experiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms the state-of-the-art approach WMamba in both quantitative metrics and visual fidelity, averagely achieving 0.55 dB performance gain on the three benchmarks. Our code is available at https://github.com/kkoucy/RD-UIE/tree/main
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core-Set Selection for Data-efficient Land Cover Segmentation</title>
<link>https://arxiv.org/abs/2505.01225</link>
<guid>https://arxiv.org/abs/2505.01225</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, deep learning, core-set selection, land cover classification, data-centric learning

Summary: 
This paper introduces novel core-set selection methods for selecting important subsets of samples from remote sensing image segmentation datasets. The proposed methods consider both the quantity and quality of data by relying on imagery only, labels only, and a combination of both. The approaches are benchmarked against a random-selection baseline on three popular land cover classification datasets: DFC2022, Vaihingen, and Potsdam. Results show that training on a subset of samples outperforms the random baseline in each dataset, with some methods even surpassing training on all available data. This highlights the significance of data-centric learning in the realm of remote sensing, emphasizing the importance of thoughtful data selection to enhance model performance. The code for these methods is publicly available for further exploration and application. 

Summary: <div>
arXiv:2505.01225v1 Announce Type: new 
Abstract: The increasing accessibility of remotely sensed data and the potential of such data to inform large-scale decision-making has driven the development of deep learning models for many Earth Observation tasks. Traditionally, such models must be trained on large datasets. However, the common assumption that broadly larger datasets lead to better outcomes tends to overlook the complexities of the data distribution, the potential for introducing biases and noise, and the computational resources required for processing and storing vast datasets. Therefore, effective solutions should consider both the quantity and quality of data. In this paper, we propose six novel core-set selection methods for selecting important subsets of samples from remote sensing image segmentation datasets that rely on imagery only, labels only, and a combination of each. We benchmark these approaches against a random-selection baseline on three commonly used land cover classification datasets: DFC2022, Vaihingen, and Potsdam. In each of the datasets, we demonstrate that training on a subset of samples outperforms the random baseline, and some approaches outperform training on all available data. This result shows the importance and potential of data-centric learning for the remote sensing domain. The code is available at https://github.com/keillernogueira/data-centric-rs-classification/.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.01235</link>
<guid>https://arxiv.org/abs/2505.01235</guid>
<content:encoded><![CDATA[
<div> Keywords: online reconstruction, dynamic scenes, temporal consistency, rendering quality, real-world recordings

Summary:
Online reconstruction of dynamic scenes from live-streaming video inputs is essential for learning scenes in real time. Existing methods have focused on efficiency and rendering quality, but have overlooked temporal consistency, leading to noticeable artifacts in static regions. This study addresses the issue of temporal inconsistency in online reconstruction caused by errors in real-world recordings. A method is proposed to enhance temporal consistency by subtracting learned errors from observations with temporal inconsistency inherent in cameras. The approach improves both temporal consistency and rendering quality compared to various baselines across datasets. The method's effectiveness is demonstrated through code, video results, and checkpoints available at the provided link. <div>
arXiv:2505.01235v1 Announce Type: new 
Abstract: Online reconstruction of dynamic scenes is significant as it enables learning scenes from live-streaming video inputs, while existing offline dynamic reconstruction methods rely on recorded video inputs. However, previous online reconstruction approaches have primarily focused on efficiency and rendering quality, overlooking the temporal consistency of their results, which often contain noticeable artifacts in static regions. This paper identifies that errors such as noise in real-world recordings affect temporal inconsistency in online reconstruction. We propose a method that enhances temporal consistency in online reconstruction from observations with temporal inconsistency which is inevitable in cameras. We show that our method restores the ideal observation by subtracting the learned error. We demonstrate that applying our method to various baselines significantly enhances both temporal consistency and rendering quality across datasets. Code, video results, and checkpoints are available at https://bbangsik13.github.io/OR2.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design</title>
<link>https://arxiv.org/abs/2505.01249</link>
<guid>https://arxiv.org/abs/2505.01249</guid>
<content:encoded><![CDATA[
<div> fusing, fixations, retinal transformation, factor analysis, Bayesian experimental design <br />
Summary:
Humans and vertebrates must fuse multiple fixations of a scene using high-resolution fovea and decreasing peripheral resolution. This paper models retinal transformations as linear downsampling of high-resolution latent scene images, enabling exact inference for latent variables in factor analysis and mixtures of factor analysis models. Utilizing the linear transformation, the paper formulates and solves the "where to look next" problem as a Bayesian experimental design using the Expected Information Gain criterion. Experimental results on Frey faces and MNIST datasets confirm the effectiveness of the proposed models. <div>
arXiv:2505.01249v1 Announce Type: new 
Abstract: Humans (and many vertebrates) face the problem of fusing together multiple fixations of a scene in order to obtain a representation of the whole, where each fixation uses a high-resolution fovea and decreasing resolution in the periphery. In this paper we explicitly represent the retinal transformation of a fixation as a linear downsampling of a high-resolution latent image of the scene, exploiting the known geometry. This linear transformation allows us to carry out exact inference for the latent variables in factor analysis (FA) and mixtures of FA models of the scene. Further, this allows us to formulate and solve the choice of "where to look next" as a Bayesian experimental design problem using the Expected Information Gain criterion. Experiments on the Frey faces and MNIST datasets demonstrate the effectiveness of our models.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2505.01257</link>
<guid>https://arxiv.org/abs/2505.01257</guid>
<content:encoded><![CDATA[
<div> Transformer-based modules, association module, multi-object tracking, tracking-by-detection, CAMEL

Summary:<br /><br />This paper introduces CAMEL, a novel association module for online multi-object tracking that learns association strategies directly from data. CAMEL breaks free from hand-crafted rules by utilizing transformer-based modules to model complex interactions between tracked targets and association cues. It maintains modularity like tracking-by-detection methods but learns resilient associations. CAMELTrack, the proposed tracking pipeline, achieves state-of-the-art performance on multiple benchmarks. The method remains lightweight and fast to train, leveraging external models while improving tracking accuracy. The code for CAMELTrack is available. <div>
arXiv:2505.01257v1 Announce Type: new 
Abstract: Online multi-object tracking has been recently dominated by tracking-by-detection (TbD) methods, where recent advances rely on increasingly sophisticated heuristics for tracklet representation, feature fusion, and multi-stage matching. The key strength of TbD lies in its modular design, enabling the integration of specialized off-the-shelf models like motion predictors and re-identification. However, the extensive usage of human-crafted rules for temporal associations makes these methods inherently limited in their ability to capture the complex interplay between various tracking cues. In this work, we introduce CAMEL, a novel association module for Context-Aware Multi-Cue ExpLoitation, that learns resilient association strategies directly from data, breaking free from hand-crafted heuristics while maintaining TbD's valuable modularity. At its core, CAMEL employs two transformer-based modules and relies on a novel association-centric training scheme to effectively model the complex interactions between tracked targets and their various association cues. Unlike end-to-end detection-by-tracking approaches, our method remains lightweight and fast to train while being able to leverage external off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack, achieves state-of-the-art performance on multiple tracking benchmarks. Our code is available at https://github.com/TrackingLaboratory/CAMELTrack.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</title>
<link>https://arxiv.org/abs/2505.01267</link>
<guid>https://arxiv.org/abs/2505.01267</guid>
<content:encoded><![CDATA[
<div> frequency domain, adversarial purification, image restoration, defense methods, content preservation

Summary:
The diffusion-based adversarial purification methods aim to mitigate adversarial perturbations by drowning them in isotropic noise and then recovering clean images. However, due to a lack of distribution information in the pixel domain, normal semantics may be damaged. By analyzing images in the frequency domain, it was observed that damage from adversarial perturbations increases with frequency. This insight led to the proposal of a new purification method that focuses on preserving low-frequency components while eliminating perturbations. Experimental results showed that this method outperformed existing defense techniques, demonstrating its effectiveness in content and structure preservation. <div>
arXiv:2505.01267v1 Announce Type: new 
Abstract: The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image. For the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. Empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors</title>
<link>https://arxiv.org/abs/2505.01322</link>
<guid>https://arxiv.org/abs/2505.01322</guid>
<content:encoded><![CDATA[
<div> Framework, Object insertion, 3D scenes, Natural language, Spatial reasoning
<br />
Summary: 
FreeInsert is a novel framework for text-driven object insertion in 3D scenes that does not rely on spatial priors. It leverages foundation models to disentangle object generation from spatial placement, allowing for unsupervised and flexible object insertion. The framework starts with an MLLM-based parser to extract structured semantics from user instructions, guiding object reconstruction and learning of degrees of freedom. MLLMs are used to initialize object pose and scale, while a refinement stage integrates spatial semantics and MLLM-inferred priors for precise placement. The object appearance is enhanced using the inserted-object image for visual fidelity. Experimental results show that FreeInsert achieves coherent, precise, and realistic 3D insertions, providing users with a user-friendly and flexible editing experience. 
<br /> <div>
arXiv:2505.01322v1 Announce Type: new 
Abstract: Text-driven object insertion in 3D scenes is an emerging task that enables intuitive scene editing through natural language. However, existing 2D editing-based methods often rely on spatial priors such as 2D masks or 3D bounding boxes, and they struggle to ensure consistency of the inserted object. These limitations hinder flexibility and scalability in real-world applications. In this paper, we propose FreeInsert, a novel framework that leverages foundation models including MLLMs, LGMs, and diffusion models to disentangle object generation from spatial placement. This enables unsupervised and flexible object insertion in 3D scenes without spatial priors. FreeInsert starts with an MLLM-based parser that extracts structured semantics, including object types, spatial relationships, and attachment regions, from user instructions. These semantics guide both the reconstruction of the inserted object for 3D consistency and the learning of its degrees of freedom. We leverage the spatial reasoning capabilities of MLLMs to initialize object pose and scale. A hierarchical, spatially aware refinement stage further integrates spatial semantics and MLLM-inferred priors to enhance placement. Finally, the appearance of the object is improved using the inserted-object image to enhance visual fidelity. Experimental results demonstrate that FreeInsert achieves semantically coherent, spatially precise, and visually realistic 3D insertions without relying on spatial priors, offering a user-friendly and flexible editing experience.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monitoring morphometric drift in lifelong learning segmentation of the spinal cord</title>
<link>https://arxiv.org/abs/2505.01364</link>
<guid>https://arxiv.org/abs/2505.01364</guid>
<content:encoded><![CDATA[
<div> Keywords: spinal cord segmentation, morphometric measures, lifelong learning, normative database, MRI contrasts

Summary:<br />
- The study introduces a spinal cord segmentation model trained on a dataset with various MRI contrasts and spinal pathologies, demonstrating superior performance on challenging lumbar spinal cord cases with an average Dice score of 0.95.
- A lifelong learning framework is proposed to monitor morphometric drift as the model is updated using additional datasets, providing a quick feedback loop for model development.
- The automatic workflow for monitoring morphometric drift allows for efficient updating of a normative database of healthy participants, with minimal drift observed between current and previous model versions.
- The model is freely available in Spinal Cord Toolbox v7.0 for wider use and accessibility. 

<br /><br />Summary: <div>
arXiv:2505.01364v1 Announce Type: new 
Abstract: Morphometric measures derived from spinal cord segmentations can serve as diagnostic and prognostic biomarkers in neurological diseases and injuries affecting the spinal cord. While robust, automatic segmentation methods to a wide variety of contrasts and pathologies have been developed over the past few years, whether their predictions are stable as the model is updated using new datasets has not been assessed. This is particularly important for deriving normative values from healthy participants. In this study, we present a spinal cord segmentation model trained on a multisite $(n=75)$ dataset, including 9 different MRI contrasts and several spinal cord pathologies. We also introduce a lifelong learning framework to automatically monitor the morphometric drift as the model is updated using additional datasets. The framework is triggered by an automatic GitHub Actions workflow every time a new model is created, recording the morphometric values derived from the model's predictions over time. As a real-world application of the proposed framework, we employed the spinal cord segmentation model to update a recently-introduced normative database of healthy participants containing commonly used measures of spinal cord morphometry. Results showed that: (i) our model outperforms previous versions and pathology-specific models on challenging lumbar spinal cord cases, achieving an average Dice score of $0.95 \pm 0.03$; (ii) the automatic workflow for monitoring morphometric drift provides a quick feedback loop for developing future segmentation models; and (iii) the scaling factor required to update the database of morphometric measures is nearly constant among slices across the given vertebral levels, showing minimum drift between the current and previous versions of the model monitored by the framework. The model is freely available in Spinal Cord Toolbox v7.0.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing</title>
<link>https://arxiv.org/abs/2505.01385</link>
<guid>https://arxiv.org/abs/2505.01385</guid>
<content:encoded><![CDATA[
<div> Algorithm, remote sensing, instance segmentation, polylines, collinearity-aware<br />
<br />
Summary: <br />
This paper introduces a novel algorithm, the Global Collinearity-aware Polygonizer (GCP), for mapping polygonal buildings from remote sensing images. GCP utilizes an instance segmentation framework to process binary masks and refine polylines sampled along building contours. A collinearity-aware polygon simplification module optimizes a balance between simplicity and fidelity to generate accurate polygon representations. The optimized objective function is integrated into network training for seamless performance. Validation on public benchmarks demonstrates GCP's effectiveness, and experiments show improved accuracy over traditional methods like the Douglas-Peucker algorithm. The broad applicability of GCP is highlighted, making it a versatile tool for various mapping tasks. The code for the algorithm will be available on the GitHub repository of 'zhu-xlab'. <br /> <div>
arXiv:2505.01385v1 Announce Type: new 
Abstract: This paper addresses the challenge of mapping polygonal buildings from remote sensing images and introduces a novel algorithm, the Global Collinearity-aware Polygonizer (GCP). GCP, built upon an instance segmentation framework, processes binary masks produced by any instance segmentation model. The algorithm begins by collecting polylines sampled along the contours of the binary masks. These polylines undergo a refinement process using a transformer-based regression module to ensure they accurately fit the contours of the targeted building instances. Subsequently, a collinearity-aware polygon simplification module simplifies these refined polylines and generate the final polygon representation. This module employs dynamic programming technique to optimize an objective function that balances the simplicity and fidelity of the polygons, achieving globally optimal solutions. Furthermore, the optimized collinearity-aware objective is seamlessly integrated into network training, enhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has been validated on two public benchmarks for polygonal building mapping. Further experiments reveal that applying the collinearity-aware polygon simplification module to arbitrary polylines, without prior knowledge, enhances accuracy over traditional methods such as the Douglas-Peucker algorithm. This finding underscores the broad applicability of GCP. The code for the proposed method will be made available at https://github.com/zhu-xlab.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer</title>
<link>https://arxiv.org/abs/2505.01390</link>
<guid>https://arxiv.org/abs/2505.01390</guid>
<content:encoded><![CDATA[
arXiv:2505.01390v1 Announce Type: new 
Abstract: This study proposes a novel approach combining Multimodal Deep Learning with intrinsic eXplainable Artificial Intelligence techniques to predict pathological response in non-small cell lung cancer patients undergoing neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal deep learning approaches, we introduce an intermediate fusion strategy that integrates imaging and clinical data, enabling efficient interaction between data modalities. The proposed Multimodal Doctor-in-the-Loop method further enhances clinical relevance by embedding clinicians' domain knowledge directly into the training process, guiding the model's focus gradually from broader lung regions to specific lesions. Results demonstrate improved predictive accuracy and explainability, providing insights into optimal data integration strategies for clinical applications.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</title>
<link>https://arxiv.org/abs/2505.01406</link>
<guid>https://arxiv.org/abs/2505.01406</guid>
<content:encoded><![CDATA[
arXiv:2505.01406v1 Announce Type: new 
Abstract: The rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. Existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. In this work, we introduce VIDSTAMP, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. By fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal perceptual impact. Leveraging architectural components such as 3D convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. VIDSTAMP embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log P-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. Code: Code: \url{https://github.com/SPIN-UMass/VidStamp}
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Depth and Attention Mechanisms for Improved RGB Image Inpainting</title>
<link>https://arxiv.org/abs/2505.00735</link>
<guid>https://arxiv.org/abs/2505.00735</guid>
<content:encoded><![CDATA[
arXiv:2505.00735v1 Announce Type: cross 
Abstract: Existing deep learning-based image inpainting methods typically rely on convolutional networks with RGB images to reconstruct images. However, relying exclusively on RGB images may neglect important depth information, which plays a critical role in understanding the spatial and structural context of a scene. Just as human vision leverages stereo cues to perceive depth, incorporating depth maps into the inpainting process can enhance the model's ability to reconstruct images with greater accuracy and contextual awareness. In this paper, we propose a novel approach that incorporates both RGB and depth images for enhanced image inpainting. Our models employ a dual encoder architecture, where one encoder processes the RGB image and the other handles the depth image. The encoded features from both encoders are then fused in the decoder using an attention mechanism, effectively integrating the RGB and depth representations. We use two different masking strategies, line and square, to test the robustness of the model under different types of occlusions. To further analyze the effectiveness of our approach, we use Gradient-weighted Class Activation Mapping (Grad-CAM) visualizations to examine the regions of interest the model focuses on during inpainting. We show that incorporating depth information alongside the RGB image significantly improves the reconstruction quality. Through both qualitative and quantitative comparisons, we demonstrate that the depth-integrated model outperforms the baseline, with attention mechanisms further enhancing inpainting performance, as evidenced by multiple evaluation metrics and visualization.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond</title>
<link>https://arxiv.org/abs/2505.00737</link>
<guid>https://arxiv.org/abs/2505.00737</guid>
<content:encoded><![CDATA[
arXiv:2505.00737v1 Announce Type: cross 
Abstract: Plant phenotyping plays a pivotal role in understanding plant traits and their interactions with the environment, making it crucial for advancing precision agriculture and crop improvement. 3D reconstruction technologies have emerged as powerful tools for capturing detailed plant morphology and structure, offering significant potential for accurate and automated phenotyping. This paper provides a comprehensive review of the 3D reconstruction techniques for plant phenotyping, covering classical reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel 3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on high-resolution sensors, are widely adopted due to their simplicity and flexibility in representing plant structures. However, they face challenges such as data density, noise, and scalability. NeRF, a recent advancement, enables high-quality, photorealistic 3D reconstructions from sparse viewpoints, but its computational cost and applicability in outdoor environments remain areas of active research. The emerging 3DGS technique introduces a new paradigm in reconstructing plant structures by representing geometry through Gaussian primitives, offering potential benefits in both efficiency and scalability. We review the methodologies, applications, and performance of these approaches in plant phenotyping and discuss their respective strengths, limitations, and future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants). Through this review, we aim to provide insights into how these diverse 3D reconstruction techniques can be effectively leveraged for automated and high-throughput plant phenotyping, contributing to the next generation of agricultural technology.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey</title>
<link>https://arxiv.org/abs/2505.00747</link>
<guid>https://arxiv.org/abs/2505.00747</guid>
<content:encoded><![CDATA[
arXiv:2505.00747v1 Announce Type: cross 
Abstract: Cooperative perception extends the perception capabilities of autonomous vehicles by enabling multi-agent information sharing via Vehicle-to-Everything (V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic "information sensor" characterized by limited communication, heterogeneity, mobility, and scalability. This survey provides a comprehensive review of recent advancements from the perspective of information-centric cooperative perception, focusing on three key dimensions: information representation, information fusion, and large-scale deployment. We categorize information representation into data-level, feature-level, and object-level schemes, and highlight emerging methods for reducing data volume and compressing messages under communication constraints. In information fusion, we explore techniques under both ideal and non-ideal conditions, including those addressing heterogeneity, localization errors, latency, and packet loss. Finally, we summarize system-level approaches to support scalability in dense traffic scenarios. Compared with existing surveys, this paper introduces a new perspective by treating V2X communication as an information sensor and emphasizing the challenges of deploying cooperative perception in real-world intelligent transportation systems.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning</title>
<link>https://arxiv.org/abs/2505.00935</link>
<guid>https://arxiv.org/abs/2505.00935</guid>
<content:encoded><![CDATA[
arXiv:2505.00935v1 Announce Type: cross 
Abstract: The increase in available computing power and the Deep Learning revolution have allowed the exploration of new topics and frontiers in Artificial Intelligence research. A new field called Embodied Artificial Intelligence, which places at the intersection of Computer Vision, Robotics, and Decision Making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. The recent availability of large collections of 3D models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. These intelligent agents are intended to perform a certain task in a possibly unknown environment. To this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. This dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. We aim to contribute to research in Embodied AI and autonomous agents, in order to foster future work in this field. We present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-demand Test-time Adaptation for Edge Devices</title>
<link>https://arxiv.org/abs/2505.00986</link>
<guid>https://arxiv.org/abs/2505.00986</guid>
<content:encoded><![CDATA[
arXiv:2505.00986v1 Announce Type: cross 
Abstract: Continual Test-time adaptation (CTTA) continuously adapts the deployed model on every incoming batch of data. While achieving optimal accuracy, existing CTTA approaches present poor real-world applicability on resource-constrained edge devices, due to the substantial memory overhead and energy consumption. In this work, we first introduce a novel paradigm -- on-demand TTA -- which triggers adaptation only when a significant domain shift is detected. Then, we present OD-TTA, an on-demand TTA framework for accurate and efficient adaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a lightweight domain shift detection mechanism to activate TTA only when it is needed, drastically reducing the overall computation overhead, 2) a source domain selection module that chooses an appropriate source model for adaptation, ensuring high and robust accuracy, 3) a decoupled Batch Normalization (BN) update scheme to enable memory-efficient adaptation with small batch sizes. Extensive experiments show that OD-TTA achieves comparable and even better performance while reducing the energy and computation overhead remarkably, making TTA a practical reality.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse</title>
<link>https://arxiv.org/abs/2505.00995</link>
<guid>https://arxiv.org/abs/2505.00995</guid>
<content:encoded><![CDATA[
arXiv:2505.00995v1 Announce Type: cross 
Abstract: As the agricultural workforce declines and labor costs rise, robotic yield estimation has become increasingly important. While unmanned ground vehicles (UGVs) are commonly used for indoor farm monitoring, their deployment in greenhouses is often constrained by infrastructure limitations, sensor placement challenges, and operational inefficiencies. To address these issues, we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial odometry algorithm for precise navigation in GNSS-denied environments and utilizes a 3D multi-object tracking algorithm to estimate the count and weight of cherry tomatoes. We evaluate the system using two dataset: one from a harvesting row and another from a growing row. In the harvesting-row dataset, the proposed system achieves 94.4\% counting accuracy and 87.5\% weight estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For the growing-row dataset, which consists of occluded unripened fruits, we qualitatively analyze tracking performance and highlight future research directions for improving perception in greenhouse with strong occlusions. Our findings demonstrate the potential of UAVs for efficient robotic yield estimation in commercial greenhouses.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Resistance of Neural Network Watermarking to Fine-tuning</title>
<link>https://arxiv.org/abs/2505.01007</link>
<guid>https://arxiv.org/abs/2505.01007</guid>
<content:encoded><![CDATA[
arXiv:2505.01007v1 Announce Type: cross 
Abstract: This paper proves a new watermarking method to embed the ownership information into a deep neural network (DNN), which is robust to fine-tuning. Specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised Fourier transform to extract frequency components from the convolutional filter. Additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. In this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. Preliminary experiments demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization</title>
<link>https://arxiv.org/abs/2505.01113</link>
<guid>https://arxiv.org/abs/2505.01113</guid>
<content:encoded><![CDATA[
arXiv:2505.01113v1 Announce Type: cross 
Abstract: Recently, camera localization has been widely adopted in autonomous robotic navigation due to its efficiency and convenience. However, autonomous navigation in unknown environments often suffers from scene ambiguity, environmental disturbances, and dynamic object transformation in camera localization. To address this problem, inspired by the biological brain navigation mechanism (such as grid cells, place cells, and head direction cells), we propose a novel neurobiological camera location method, namely NeuroLoc. Firstly, we designed a Hebbian learning module driven by place cells to save and replay historical information, aiming to restore the details of historical representations and solve the issue of scene fuzziness. Secondly, we utilized the head direction cell-inspired internal direction learning as multi-head attention embedding to help restore the true orientation in similar scenes. Finally, we added a 3D grid center prediction in the pose regression module to reduce the final wrong prediction. We evaluate the proposed NeuroLoc on commonly used benchmark indoor and outdoor datasets. The experimental results show that our NeuroLoc can enhance the robustness in complex environments and improve the performance of pose regression by using only a single image.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment</title>
<link>https://arxiv.org/abs/2505.01237</link>
<guid>https://arxiv.org/abs/2505.01237</guid>
<content:encoded><![CDATA[
arXiv:2505.01237v1 Announce Type: cross 
Abstract: Recent advances in audio-visual learning have shown promising results in learning representations across modalities. However, most approaches rely on global audio representations that fail to capture fine-grained temporal correspondences with visual frames. Additionally, existing methods often struggle with conflicting optimization objectives when trying to jointly learn reconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync as a simple yet effective extension of the original CAV-MAE framework for self-supervised audio-visual learning. We address three key challenges: First, we tackle the granularity mismatch between modalities by treating audio as a temporal sequence aligned with video frames, rather than using global representations. Second, we resolve conflicting optimization goals by separating contrastive and reconstruction objectives through dedicated global tokens. Third, we improve spatial localization by introducing learnable register tokens that reduce semantic load on patch tokens. We evaluate the proposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on zero-shot retrieval, classification and localization tasks demonstrating state-of-the-art performance and outperforming more complex architectures.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging</title>
<link>https://arxiv.org/abs/2505.01239</link>
<guid>https://arxiv.org/abs/2505.01239</guid>
<content:encoded><![CDATA[
arXiv:2505.01239v1 Announce Type: cross 
Abstract: Accurate lung tumor segmentation is crucial for improving diagnosis, treatment planning, and patient outcomes in oncology. However, the complexity of tumor morphology, size, and location poses significant challenges for automated segmentation. This study presents a comprehensive benchmarking analysis of deep learning-based segmentation models, comparing traditional architectures such as U-Net and DeepLabV3, self-configuring models like nnUNet, and foundation models like MedSAM, and MedSAM~2. Evaluating performance across two lung tumor segmentation datasets, we assess segmentation accuracy and computational efficiency under various learning paradigms, including few-shot learning and fine-tuning. The results reveal that while traditional models struggle with tumor delineation, foundation models, particularly MedSAM~2, outperform them in both accuracy and computational efficiency. These findings underscore the potential of foundation models for lung tumor segmentation, highlighting their applicability in improving clinical workflows and patient outcomes.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing</title>
<link>https://arxiv.org/abs/2505.01263</link>
<guid>https://arxiv.org/abs/2505.01263</guid>
<content:encoded><![CDATA[
arXiv:2505.01263v1 Announce Type: cross 
Abstract: Movie Dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of a given brief reference audio. Existing methods focus primarily on reducing the word error rate while ignoring the importance of lip-sync and acoustic quality. To address these issues, we propose a large language model (LLM) based flow matching architecture for dubbing, named FlowDubber, which achieves high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via the proposed voice-enhanced flow matching than previous works. First, we introduce Qwen2.5 as the backbone of LLM to learn the in-context sequence from movie scripts and reference audio. Then, the proposed semantic-aware learning focuses on capturing LLM semantic knowledge at the phoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment with lip movement, reducing ambiguities where similar phonemes might be confused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves acoustic quality in two aspects, which introduces an LLM-based acoustics flow matching guidance to strengthen clarity and uses affine style prior to enhance identity when recovering noise into mel-spectrograms via gradient vector field prediction. Extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks. The demos are available at {\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture</title>
<link>https://arxiv.org/abs/2505.01313</link>
<guid>https://arxiv.org/abs/2505.01313</guid>
<content:encoded><![CDATA[
arXiv:2505.01313v1 Announce Type: cross 
Abstract: This paper proposes a neural architecture search space using ResNet as a framework, with search objectives including parameters for convolution, pooling, fully connected layers, and connectivity of the residual network. In addition to recognition accuracy, this paper uses the loss value on the validation set as a secondary objective for optimization. The experimental results demonstrate that the search space of this paper together with the optimisation approach can find competitive network architectures on the MNIST, Fashion-MNIST and CIFAR100 datasets.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GENMO: A GENeralist Model for Human MOtion</title>
<link>https://arxiv.org/abs/2505.01425</link>
<guid>https://arxiv.org/abs/2505.01425</guid>
<content:encoded><![CDATA[
arXiv:2505.01425v1 Announce Type: cross 
Abstract: Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VitalVideos-Europe: A dataset of face videos with PPG and blood pressure ground truths</title>
<link>https://arxiv.org/abs/2306.11891</link>
<guid>https://arxiv.org/abs/2306.11891</guid>
<content:encoded><![CDATA[
arXiv:2306.11891v3 Announce Type: replace 
Abstract: We collected a large dataset consisting of 850 unique participants. For every participant we recorded two 30 second uncompressed videos, synchronized PPG waveforms and a single blood pressure measurement. Gender, age and skin color were also registered for every participant. The dataset includes roughly equal numbers of males and females, as well as participants of all ages. While the skin color distribution could have been more balanced, the dataset contains individuals from every skin color. The data was collected in a diverse set of locations to ensure a wide variety of backgrounds and lighting conditions. In an effort to assist in the research and development of remote vital sign measurement we are now opening up access to this dataset.
  vitalvideos.org for all datasets.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Concept-driven Image Generation with Text-to-Image Diffusion Model</title>
<link>https://arxiv.org/abs/2402.11487</link>
<guid>https://arxiv.org/abs/2402.11487</guid>
<content:encoded><![CDATA[
arXiv:2402.11487v3 Announce Type: replace 
Abstract: Text-to-image (TTI) diffusion models have demonstrated impressive results in generating high-resolution images of complex and imaginative scenes. Recent approaches have further extended these methods with personalization techniques that allow them to integrate user-illustrated concepts (e.g., the user him/herself) using a few sample image illustrations. However, the ability to generate images with multiple interacting concepts, such as human subjects, as well as concepts that may be entangled in one, or across multiple, image illustrations remains illusive. In this work, we propose a concept-driven TTI personalization framework that addresses these core challenges. We build on existing works that learn custom tokens for user-illustrated concepts, allowing those to interact with existing text tokens in the TTI model. However, importantly, to disentangle and better learn the concepts in question, we jointly learn (latent) segmentation masks that disentangle these concepts in user-provided image illustrations. We do so by introducing an Expectation Maximization (EM)-like optimization procedure where we alternate between learning the custom tokens and estimating (latent) masks encompassing corresponding concepts in user-supplied images. We obtain these masks based on cross-attention, from within the U-Net parameterized latent diffusion model and subsequent DenseCRF optimization. We illustrate that such joint alternating refinement leads to the learning of better tokens for concepts and, as a by-product, latent masks. We illustrate the benefits of the proposed approach qualitatively and quantitatively with several examples and use cases that can combine three or more entangled concepts.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-Hologen: An End-to-End Generative Framework for Phase-Only Holograms</title>
<link>https://arxiv.org/abs/2404.01330</link>
<guid>https://arxiv.org/abs/2404.01330</guid>
<content:encoded><![CDATA[
arXiv:2404.01330v2 Announce Type: replace 
Abstract: Holography stands at the forefront of visual technology, offering immersive, three-dimensional visualizations through the manipulation of light wave amplitude and phase. Although generative models have been extensively explored in the image domain, their application to holograms remains relatively underexplored due to the inherent complexity of phase learning. Exploiting generative models for holograms offers exciting opportunities for advancing innovation and creativity, such as semantic-aware hologram generation and editing. Currently, the most viable approach for utilizing generative models in the hologram domain involves integrating an image-based generative model with an image-to-hologram conversion model, which comes at the cost of increased computational complexity and inefficiency. To tackle this problem, we introduce P-Hologen, the first end-to-end generative framework designed for phase-only holograms (POHs). P-Hologen employs vector quantized variational autoencoders to capture the complex distributions of POHs. It also integrates the angular spectrum method into the training process, constructing latent spaces for complex phase data using strategies from the image processing domain. Extensive experiments demonstrate that P-Hologen achieves superior quality and computational efficiency compared to the existing methods. Furthermore, our model generates high-quality unseen, diverse holographic content from its learned latent space without requiring pre-existing images. Our work paves the way for new applications and methodologies in holographic content creation, opening a new era in the exploration of generative holographic content. The code for our paper is publicly available on https://github.com/james0223/P-Hologen.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Part-aware Shape Generation with Latent 3D Diffusion of Neural Voxel Fields</title>
<link>https://arxiv.org/abs/2405.00998</link>
<guid>https://arxiv.org/abs/2405.00998</guid>
<content:encoded><![CDATA[
arXiv:2405.00998v5 Announce Type: replace 
Abstract: This paper presents a novel latent 3D diffusion model for the generation of neural voxel fields, aiming to achieve accurate part-aware structures. Compared to existing methods, there are two key designs to ensure high-quality and accurate part-aware generation. On one hand, we introduce a latent 3D diffusion process for neural voxel fields, enabling generation at significantly higher resolutions that can accurately capture rich textural and geometric details. On the other hand, a part-aware shape decoder is introduced to integrate the part codes into the neural voxel fields, guiding the accurate part decomposition and producing high-quality rendering results. Through extensive experimentation and comparisons with state-of-the-art methods, we evaluate our approach across four different classes of data. The results demonstrate the superior generative capabilities of our proposed method in part-aware shape generation, outperforming existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Classification by Coupling Data Mollification with Label Smoothing</title>
<link>https://arxiv.org/abs/2406.01494</link>
<guid>https://arxiv.org/abs/2406.01494</guid>
<content:encoded><![CDATA[
arXiv:2406.01494v3 Announce Type: replace 
Abstract: Introducing training-time augmentations is a key technique to enhance generalization and prepare deep neural networks against test-time corruptions. Inspired by the success of generative diffusion models, we propose a novel approach of coupling data mollification, in the form of image noising and blurring, with label smoothing to align predicted label confidences with image degradation. The method is simple to implement, introduces negligible overheads, and can be combined with existing augmentations. We demonstrate improved robustness and uncertainty quantification on the corrupted image benchmarks of CIFAR, TinyImageNet and ImageNet datasets.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual-Friendly Concept Protection via Selective Adversarial Perturbations</title>
<link>https://arxiv.org/abs/2408.08518</link>
<guid>https://arxiv.org/abs/2408.08518</guid>
<content:encoded><![CDATA[
arXiv:2408.08518v2 Announce Type: replace 
Abstract: Personalized concept generation by tuning diffusion models with a few images raises potential legal and ethical concerns regarding privacy and intellectual property rights. Researchers attempt to prevent malicious personalization using adversarial perturbations. However, previous efforts have mainly focused on the effectiveness of protection while neglecting the visibility of perturbations. They utilize global adversarial perturbations, which introduce noticeable alterations to original images and significantly degrade visual quality. In this work, we propose the Visual-Friendly Concept Protection (VCPro) framework, which prioritizes the protection of key concepts chosen by the image owner through adversarial perturbations with lower perceptibility. To ensure these perturbations are as inconspicuous as possible, we introduce a relaxed optimization objective to identify the least perceptible yet effective adversarial perturbations, solved using the Lagrangian multiplier method. Qualitative and quantitative experiments validate that VCPro achieves a better trade-off between the visibility of perturbations and protection effectiveness, effectively prioritizing the protection of target concepts in images with less perceptible perturbations.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM</title>
<link>https://arxiv.org/abs/2409.00362</link>
<guid>https://arxiv.org/abs/2409.00362</guid>
<content:encoded><![CDATA[
arXiv:2409.00362v2 Announce Type: replace 
Abstract: Recent advancements in monocular neural depth estimation, particularly those achieved by the UniDepth network, have prompted the investigation of integrating UniDepth within a Gaussian splatting framework for monocular SLAM. This study presents UDGS-SLAM, a novel approach that eliminates the necessity of RGB-D sensors for depth estimation within Gaussian splatting framework. UDGS-SLAM employs statistical filtering to ensure local consistency of the estimated depth and jointly optimizes camera trajectory and Gaussian scene representation parameters. The proposed method achieves high-fidelity rendered images and low ATERMSE of the camera trajectory. The performance of UDGS-SLAM is rigorously evaluated using the TUM RGB-D dataset and benchmarked against several baseline methods, demonstrating superior performance across various scenarios. Additionally, an ablation study is conducted to validate design choices and investigate the impact of different network backbone encoders on system performance.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration</title>
<link>https://arxiv.org/abs/2410.01723</link>
<guid>https://arxiv.org/abs/2410.01723</guid>
<content:encoded><![CDATA[
arXiv:2410.01723v4 Announce Type: replace 
Abstract: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$X^2$-DFD: A framework for eXplainable and eXtendable Deepfake Detection</title>
<link>https://arxiv.org/abs/2410.06126</link>
<guid>https://arxiv.org/abs/2410.06126</guid>
<content:encoded><![CDATA[
arXiv:2410.06126v3 Announce Type: replace 
Abstract: Detecting deepfakes has become an important task. Most existing detection methods provide only real/fake predictions without offering human-comprehensible explanations. Recent studies leveraging MLLMs for deepfake detection have shown improvements in explainability. However, the performance of pre-trained MLLMs (e.g., LLaVA) remains limited due to a lack of understanding of their capabilities for this task and strategies to enhance them. In this work, we empirically assess the strengths and weaknesses of MLLMs specifically in deepfake detection via forgery features analysis. Building on these assessments, we propose a novel framework called ${X}^2$-DFD, consisting of three core modules. The first module, Model Feature Assessment (MFA), measures the detection capabilities of forgery features intrinsic to MLLMs, and gives a descending ranking of these features. The second module, Strong Feature Strengthening (SFS), enhances the detection and explanation capabilities by fine-tuning the MLLM on a dataset constructed based on the top-ranked features. The third module, Weak Feature Supplementing (WFS), improves the fine-tuned MLLM's capabilities on lower-ranked features by integrating external dedicated deepfake detectors. To verify the effectiveness of this framework, we further present a practical implementation, where an automated forgery features generation, evaluation, and ranking procedure is designed for MFA module; an automated generation procedure of the fine-tuning dataset containing real and fake images with explanations based on top-ranked features is developed for SFS model; an external conventional deepfake detector focusing on blending artifact, which corresponds to a low detection capability in the pre-trained MLLM, is integrated for WFS module. Experiments show that our approach enhances both detection and explanation performance.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoGene: Audio-Driven Emotional 3D Talking-Head Generation</title>
<link>https://arxiv.org/abs/2410.17262</link>
<guid>https://arxiv.org/abs/2410.17262</guid>
<content:encoded><![CDATA[
arXiv:2410.17262v2 Announce Type: replace 
Abstract: Audio-driven talking-head generation is a crucial and useful technology for virtual human interaction and film-making. While recent advances have focused on improving image fidelity and lip synchronization, generating accurate emotional expressions remains underexplored. In this paper, we introduce EmoGene, a novel framework for synthesizing high-fidelity, audio-driven video portraits with accurate emotional expressions. Our approach employs a variational autoencoder (VAE)-based audio-to-motion module to generate facial landmarks, which are concatenated with emotional embedding in a motion-to-emotion module to produce emotional landmarks. These landmarks drive a Neural Radiance Fields (NeRF)-based emotion-to-video module to render realistic emotional talking-head videos. Additionally, we propose a pose sampling method to generate natural idle-state (non-speaking) videos for silent audio inputs. Extensive experiments demonstrate that EmoGene outperforms previous methods in generating high-fidelity emotional talking-head videos.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2411.14432</link>
<guid>https://arxiv.org/abs/2411.14432</guid>
<content:encoded><![CDATA[
arXiv:2411.14432v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks</title>
<link>https://arxiv.org/abs/2411.16721</link>
<guid>https://arxiv.org/abs/2411.16721</guid>
<content:encoded><![CDATA[
arXiv:2411.16721v3 Announce Type: replace 
Abstract: Vision Language Models (VLMs) can produce unintended and harmful content when exposed to adversarial attacks, particularly because their vision capabilities create new vulnerabilities. Existing defenses, such as input preprocessing, adversarial training, and response evaluation-based methods, are often impractical for real-world deployment due to their high costs. To address this challenge, we propose ASTRA, an efficient and effective defense by adaptively steering models away from adversarial feature directions to resist VLM attacks. Our key procedures involve finding transferable steering vectors representing the direction of harmful response and applying adaptive activation steering to remove these directions at inference time. To create effective steering vectors, we randomly ablate the visual tokens from the adversarial images and identify those most strongly associated with jailbreaks. These tokens are then used to construct steering vectors. During inference, we perform the adaptive steering method that involves the projection between the steering vectors and calibrated activation, resulting in little performance drops on benign inputs while strongly avoiding harmful outputs under adversarial inputs. Extensive experiments across multiple models and baselines demonstrate our state-of-the-art performance and high efficiency in mitigating jailbreak risks. Additionally, ASTRA exhibits good transferability, defending against unseen attacks (i.e., structured-based attack, perturbation-based attack with project gradient descent variants, and text-only attack). Our code is available at \url{https://github.com/ASTRAL-Group/ASTRA}.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You KAN Do It in a Single Shot: Plug-and-Play Methods with Single-Instance Priors</title>
<link>https://arxiv.org/abs/2412.06204</link>
<guid>https://arxiv.org/abs/2412.06204</guid>
<content:encoded><![CDATA[
arXiv:2412.06204v2 Announce Type: replace 
Abstract: The use of Plug-and-Play (PnP) methods has become a central approach for solving inverse problems, with denoisers serving as regularising priors that guide optimisation towards a clean solution. In this work, we introduce KAN-PnP, an optimisation framework that incorporates Kolmogorov-Arnold Networks (KANs) as denoisers within the Plug-and-Play (PnP) paradigm. KAN-PnP is specifically designed to solve inverse problems with single-instance priors, where only a single noisy observation is available, eliminating the need for large datasets typically required by traditional denoising methods. We show that KANs, based on the Kolmogorov-Arnold representation theorem, serve effectively as priors in such settings, providing a robust approach to denoising. We prove that the KAN denoiser is Lipschitz continuous, ensuring stability and convergence in optimisation algorithms like PnP-ADMM, even in the context of single-shot learning. Additionally, we provide theoretical guarantees for KAN-PnP, demonstrating its convergence under key conditions: the convexity of the data fidelity term, Lipschitz continuity of the denoiser, and boundedness of the regularisation functional. These conditions are crucial for stable and reliable optimisation. Our experimental results show, on super-resolution and joint optimisation, that KAN-PnP outperforms exiting methods, delivering superior performance in single-shot learning with minimal data. The method exhibits strong convergence properties, achieving high accuracy with fewer iterations.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph Convolution Networks</title>
<link>https://arxiv.org/abs/2501.18851</link>
<guid>https://arxiv.org/abs/2501.18851</guid>
<content:encoded><![CDATA[
arXiv:2501.18851v3 Announce Type: replace 
Abstract: Most existing RGB-D semantic segmentation methods focus on the feature level fusion, including complex cross-modality and cross-scale fusion modules. However, these methods may cause misalignment problem in the feature fusion process and counter-intuitive patches in the segmentation results. Inspired by the popular pixel-node-pixel pipeline, we propose to 1) fuse features from two modalities in a late fusion style, during which the geometric feature injection is guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on the fused feature to alleviate the emergence of irregular patches by inferring patch relationship. At the 3D feature extraction stage, we argue that traditional CNNs are not efficient enough for depth maps. So, we encode depth map into normal map, after which CNNs can easily extract object surface tendencies.At projection matrix generation stage, we find the existence of Biased-Assignment and Ambiguous-Locality issues in the original pipeline. Therefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no missing important pixel features, which can be viewed as hard pixel mining process; 2) connect regions that are close to each other in the Euclidean space as well as in the semantic space with larger edge weights so that location informations can been considered. Extensive experiments on two public datasets, NYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost the performance of RGB-D semantic segmentation task.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention</title>
<link>https://arxiv.org/abs/2503.01284</link>
<guid>https://arxiv.org/abs/2503.01284</guid>
<content:encoded><![CDATA[
arXiv:2503.01284v3 Announce Type: replace 
Abstract: Soybean leaf disease detection is critical for agricultural productivity but faces challenges due to visually similar symptoms and limited interpretability in conventional methods. While Convolutional Neural Networks (CNNs) excel in spatial feature extraction, they often neglect inter-image relational dependencies, leading to misclassifications. This paper proposes an interpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that synergizes MobileNetV2 for localized feature extraction and GraphSAGE for relational modeling. The framework constructs a graph where nodes represent leaf images, with edges defined by cosine similarity-based adjacency matrices and adaptive neighborhood sampling. This design captures fine-grained lesion features and global symptom patterns, addressing inter-class similarity challenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM visualizations, generating heatmaps to highlight disease-influential regions. Evaluated on a dataset of ten soybean leaf diseases, the model achieves $97.16\%$ accuracy, surpassing standalone CNNs ($\le95.04\%$) and traditional machine learning models ($\le77.05\%$). Ablation studies validate the sequential architecture's superiority over parallel or single-model configurations. With only 2.3 million parameters, the lightweight MobileNetV2-GraphSAGE combination ensures computational efficiency, enabling real-time deployment in resource-constrained environments. The proposed approach bridges the gap between accurate classification and practical applicability, offering a robust, interpretable tool for agricultural diagnostics while advancing CNN-GNN integration in plant pathology research.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Volumetric medical image segmentation through dual self-distillation in U-shaped networks</title>
<link>https://arxiv.org/abs/2306.03271</link>
<guid>https://arxiv.org/abs/2306.03271</guid>
<content:encoded><![CDATA[
arXiv:2306.03271v3 Announce Type: replace-cross 
Abstract: U-shaped networks and its variants have demonstrated exceptional results for medical image segmentation. In this paper, we propose a novel dual self-distillation (DSD) framework in U-shaped networks for volumetric medical image segmentation. DSD distills knowledge from the ground-truth segmentation labels to the decoder layers. Additionally, DSD also distills knowledge from the deepest decoder and encoder layer to the shallower decoder and encoder layers respectively of a single U-shaped network. DSD is a general training strategy that could be attached to the backbone architecture of any U-shaped network to further improve its segmentation performance. We attached DSD on several state-of-the-art U-shaped backbones, and extensive experiments on various public 3D medical image segmentation datasets (cardiac substructure, brain tumor and Hippocampus) demonstrated significant improvement over the same backbones without DSD. On average, after attaching DSD to the U-shaped backbones, we observed an increase of 2.82\%, 4.53\% and 1.3\% in Dice similarity score, a decrease of 7.15 mm, 6.48 mm and 0.76 mm in the Hausdorff distance, for cardiac substructure, brain tumor and Hippocampus segmentation, respectively. These improvements were achieved with negligible increase in the number of trainable parameters and training time. Our proposed DSD framework also led to significant qualitative improvements for cardiac substructure, brain tumor and Hippocampus segmentation over the U-shaped backbones. The source code is publicly available at https://github.com/soumbane/DualSelfDistillation.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Vision &amp; Language Decoders use Images and Text equally? How Self-consistent are their Explanations?</title>
<link>https://arxiv.org/abs/2404.18624</link>
<guid>https://arxiv.org/abs/2404.18624</guid>
<content:encoded><![CDATA[
arXiv:2404.18624v4 Announce Type: replace-cross 
Abstract: Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. In this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that most tested VLMs are less self-consistent than LLMs. Text contributions in all tested VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to VL encoders. We find that the tested VL decoders still struggle with most phenomena tested by VALSE.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training</title>
<link>https://arxiv.org/abs/2411.17662</link>
<guid>https://arxiv.org/abs/2411.17662</guid>
<content:encoded><![CDATA[
arXiv:2411.17662v2 Announce Type: replace-cross 
Abstract: Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. While images of robots inherently contain rich information about the robot's physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. To address this, we introduce RoboPEPP, a method that fuses information about the robot's physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. Specifically, we mask the robot's joints and pre-train an encoder-predictor model to infer the joints' embeddings from surrounding unmasked regions, enhancing the encoder's understanding of the robot's physical model. The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-Inspired Deep Learning Framework with Polar Coordinate Attention for Ptychographic Imaging</title>
<link>https://arxiv.org/abs/2412.06806</link>
<guid>https://arxiv.org/abs/2412.06806</guid>
<content:encoded><![CDATA[
arXiv:2412.06806v2 Announce Type: replace-cross 
Abstract: Ptychographic imaging confronts inherent challenges in applying deep learning for phase retrieval from diffraction patterns. Conventional neural architectures, both convolutional neural networks and Transformer-based methods, are optimized for natural images with Euclidean spatial neighborhood-based inductive biases that exhibit geometric mismatch with the concentric coherent patterns characteristic of diffraction data in reciprocal space. In this paper, we present PPN, a physics-inspired deep learning network with Polar Coordinate Attention (PoCA) for ptychographic imaging, that aligns neural inductive biases with diffraction physics through a dual-branch architecture separating local feature extraction from non-local coherence modeling. It consists of a PoCA mechanism that replaces Euclidean spatial priors with physically consistent radial-angular correlations. PPN outperforms existing end-to-end models, with spectral and spatial analysis confirming its greater preservation of high-frequency details. Notably, PPN maintains robust performance compared to iterative methods even at low overlap ratios, making it well suited for high-throughput imaging in real-world acquisition scenarios for samples with consistent structural characteristics.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DriveGPT: Scaling Autoregressive Behavior Models for Driving</title>
<link>https://arxiv.org/abs/2412.14415</link>
<guid>https://arxiv.org/abs/2412.14415</guid>
<content:encoded><![CDATA[
arXiv:2412.14415v3 Announce Type: replace-cross 
Abstract: We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision-making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples, including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms state-of-the-art baselines and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network</title>
<link>https://arxiv.org/abs/2503.12623</link>
<guid>https://arxiv.org/abs/2503.12623</guid>
<content:encoded><![CDATA[
arXiv:2503.12623v2 Announce Type: replace-cross 
Abstract: Dynamic emotion recognition in the wild remains challenging due to the transient nature of emotional expressions and temporal misalignment of multi-modal cues. Traditional approaches predict valence and arousal and often overlook the inherent correlation between these two dimensions. The proposed Multi-modal Attention for Valence-Arousal Emotion Network (MAVEN) integrates visual, audio, and textual modalities through a bi-directional cross-modal attention mechanism. MAVEN uses modality-specific encoders to extract features from synchronized video frames, audio segments, and transcripts, predicting emotions in polar coordinates following Russell's circumplex model. The evaluation of the Aff-Wild2 dataset using MAVEN achieved a concordance correlation coefficient (CCC) of 0.3061, surpassing the ResNet-50 baseline model with a CCC of 0.22. The multistage architecture captures the subtle and transient nature of emotional expressions in conversational videos and improves emotion recognition in real-world situations. The code is available at: https://github.com/Vrushank-Ahire/MAVEN_8th_ABAW
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADAPT: An Autonomous Forklift for Construction Site Operation</title>
<link>https://arxiv.org/abs/2503.14331</link>
<guid>https://arxiv.org/abs/2503.14331</guid>
<content:encoded><![CDATA[
arXiv:2503.14331v3 Announce Type: replace-cross 
Abstract: Efficient material logistics play a critical role in controlling costs and schedules in the construction industry. However, manual material handling remains prone to inefficiencies, delays, and safety risks. Autonomous forklifts offer a promising solution to streamline on-site logistics, reducing reliance on human operators and mitigating labor shortages. This paper presents the development and evaluation of ADAPT (Autonomous Dynamic All-terrain Pallet Transporter), a fully autonomous off-road forklift designed for construction environments. Unlike structured warehouse settings, construction sites pose significant challenges, including dynamic obstacles, unstructured terrain, and varying weather conditions. To address these challenges, our system integrates AI-driven perception techniques with traditional approaches for decision making, planning, and control, enabling reliable operation in complex environments. We validate the system through extensive real-world testing, comparing its continuous performance against an experienced human operator across various weather conditions. Our findings demonstrate that autonomous outdoor forklifts can operate near human-level performance, offering a viable path toward safer and more efficient construction logistics.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deciphering scrolls with tomography: A training experiment</title>
<link>https://arxiv.org/abs/2504.11485</link>
<guid>https://arxiv.org/abs/2504.11485</guid>
<content:encoded><![CDATA[
arXiv:2504.11485v2 Announce Type: replace-cross 
Abstract: The recovery of severely damaged ancient written documents has proven to be a major challenge for many scientists, mainly due to the impracticality of physical unwrapping them. Non-destructive techniques, such as X-ray computed tomography (CT), combined with computer vision algorithms, have emerged as a means of facilitating the virtual reading of the hidden contents of the damaged documents. This paper proposes an educational laboratory aimed at simulating the entire process of acquisition and virtual recovery of the ancient works. We have developed an experimental setup that uses visible light to replace the detrimental X-rays, and a didactic software pipeline that allows students to virtually reconstruct a transparent rolled sheet with printed text on it, the wrapped scroll.
]]></content:encoded>
<pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Borrow Features for Improved Detection of Small Objects in Single-Shot Detectors</title>
<link>https://arxiv.org/abs/2505.00044</link>
<guid>https://arxiv.org/abs/2505.00044</guid>
<content:encoded><![CDATA[
<div> Feature Matching Block, Feature Representing Block, Feature Fusion Block, small object detection, convolutional feature maps

Summary:
The article introduces a novel framework for improving small object detection in single-shot object detectors. The proposed framework includes three key components: the Feature Matching Block (FMB) to identify similar descriptors, the Feature Representing Block (FRB) to enhance shallow features through aggregation, and the Feature Fusion Block (FFB) to refine feature maps by integrating different information. The architecture allows small object representations to borrow discriminative features from larger instances in the same class. By enhancing the descriptive capacity of shallow layers while maintaining real-time detection performance, the method significantly boosts small object detection accuracy compared to baseline methods. The experimental results demonstrate the effectiveness of the approach in achieving robust object detection in complex visual environments. This work shows promise in addressing the trade-off between spatial resolution and semantic richness in convolutional feature maps. 

<br /><br />Summary: <div>
arXiv:2505.00044v1 Announce Type: new 
Abstract: Detecting small objects remains a significant challenge in single-shot object detectors due to the inherent trade-off between spatial resolution and semantic richness in convolutional feature maps. To address this issue, we propose a novel framework that enables small object representations to "borrow" discriminative features from larger, semantically richer instances within the same class. Our architecture introduces three key components: the Feature Matching Block (FMB) to identify semantically similar descriptors across layers, the Feature Representing Block (FRB) to generate enhanced shallow features through weighted aggregation, and the Feature Fusion Block (FFB) to refine feature maps by integrating original, borrowed, and context information. Built upon the SSD framework, our method improves the descriptive capacity of shallow layers while maintaining real-time detection performance. Experimental results demonstrate that our approach significantly boosts small object detection accuracy over baseline methods, offering a promising direction for robust object detection in complex visual environments.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design</title>
<link>https://arxiv.org/abs/2505.00134</link>
<guid>https://arxiv.org/abs/2505.00134</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, histopathology, prompt engineering, diagnostic accuracy, anatomical precision

Summary: 
In this study, the performance of three state-of-the-art Vision-Language Models (VLMs) - Quilt-Net, Quilt-LLAVA, and CONCH - in histopathology analysis was systematically explored using an in-house dataset of giga-pixel whole slide images (WSIs) of digestive pathology. The research focused on the impact of prompt engineering on model accuracy, with a specific emphasis on domain specificity, anatomical precision, instructional framing, and output constraints. The findings revealed that CONCH exhibited the highest accuracy when provided with precise anatomical references, highlighting the importance of anatomical context in image analysis. Additionally, the study emphasized the significance of domain alignment and domain-specific training over model complexity alone in achieving superior performance. These results establish guidelines for prompt engineering in computational pathology and underscore the potential of VLMs to enhance diagnostic accuracy through domain-appropriate prompts.<br /><br />Summary: <div>
arXiv:2505.00134v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have gained significant attention in computational pathology due to their multimodal learning capabilities that enhance big-data analytics of giga-pixel whole slide image (WSI). However, their sensitivity to large-scale clinical data, task formulations, and prompt design remains an open question, particularly in terms of diagnostic accuracy. In this paper, we present a systematic investigation and analysis of three state of the art VLMs for histopathology, namely Quilt-Net, Quilt-LLAVA, and CONCH, on an in-house digestive pathology dataset comprising 3,507 WSIs, each in giga-pixel form, across distinct tissue types. Through a structured ablative study on cancer invasiveness and dysplasia status, we develop a comprehensive prompt engineering framework that systematically varies domain specificity, anatomical precision, instructional framing, and output constraints. Our findings demonstrate that prompt engineering significantly impacts model performance, with the CONCH model achieving the highest accuracy when provided with precise anatomical references. Additionally, we identify the critical importance of anatomical context in histopathological image analysis, as performance consistently degraded when reducing anatomical precision. We also show that model complexity alone does not guarantee superior performance, as effective domain alignment and domain-specific training are critical. These results establish foundational guidelines for prompt engineering in computational pathology and highlight the potential of VLMs to enhance diagnostic accuracy when properly instructed with domain-appropriate prompts.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis</title>
<link>https://arxiv.org/abs/2505.00135</link>
<guid>https://arxiv.org/abs/2505.00135</guid>
<content:encoded><![CDATA[
<div> Approach, Text-to-video generator, Stereo generation, 3D effect, Video synthesis

Summary: 
The article introduces a new approach for generating stereoscopic 3D videos from existing 2D videos without intermediate disparity estimation or depth calculation. The proposed framework directly synthesizes a shifted viewpoint by leveraging pre-trained video models' priors on geometry, object materials, optics, and semantics. This method overcomes limitations encountered with specular surfaces or transparent objects in traditional approaches, which often result in artifacts and incorrect pixel shifts during warping. By skipping these intermediate steps and focusing on generating the new viewpoint, the framework successfully creates compelling 3D effects in videos. The effectiveness of this approach is demonstrated in complex, real-world scenarios with diverse object materials and compositions. Videos showcasing the results can be viewed on the provided website. 

<br /><br />Summary: <div>
arXiv:2505.00135v1 Announce Type: new 
Abstract: The rising popularity of immersive visual experiences has increased interest in stereoscopic 3D video generation. Despite significant advances in video synthesis, creating 3D videos remains challenging due to the relative scarcity of 3D video data. We propose a simple approach for transforming a text-to-video generator into a video-to-stereo generator. Given an input video, our framework automatically produces the video frames from a shifted viewpoint, enabling a compelling 3D effect. Prior and concurrent approaches for this task typically operate in multiple phases, first estimating video disparity or depth, then warping the video accordingly to produce a second view, and finally inpainting the disoccluded regions. This approach inherently fails when the scene involves specular surfaces or transparent objects. In such cases, single-layer disparity estimation is insufficient, resulting in artifacts and incorrect pixel shifts during warping. Our work bypasses these restrictions by directly synthesizing the new viewpoint, avoiding any intermediate steps. This is achieved by leveraging a pre-trained video model's priors on geometry, object materials, optics, and semantics, without relying on external geometry models or manually disentangling geometry from the synthesis process. We demonstrate the advantages of our approach in complex, real-world scenarios featuring diverse object materials and compositions. See videos on https://video-eye2eye.github.io
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.00150</link>
<guid>https://arxiv.org/abs/2505.00150</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, hateful memes, Vision-Language Models, hate speech detection, online communication

Summary: 
The paper discusses the use of Vision-Language Models (VLMs) to detect and mitigate hateful content in multimodal memes on social media platforms. It introduces a definition-guided prompting technique for detecting hateful memes and presents a framework called UnHateMeme to transform hateful content into non-hateful forms. By leveraging state-of-the-art pretrained VLMs such as LLaVA, Gemini, and GPT-4o, the study demonstrates impressive performance in detecting and mitigating hateful memes. Through empirical experiments, the effectiveness and limitations of these VLMs in addressing hate speech are analyzed. The research aims to highlight the importance of using advanced AI models to create safe and respectful online environments.<br /><br />Summary: <div>
arXiv:2505.00150v1 Announce Type: new 
Abstract: The rapid evolution of social media has provided enhanced communication channels for individuals to create online content, enabling them to express their thoughts and opinions. Multimodal memes, often utilized for playful or humorous expressions with visual and textual elements, are sometimes misused to disseminate hate speech against individuals or groups. While the detection of hateful memes is well-researched, developing effective methods to transform hateful content in memes remains a significant challenge. Leveraging the powerful generation and reasoning capabilities of Vision-Language Models (VLMs), we address the tasks of detecting and mitigating hateful content. This paper presents two key contributions: first, a definition-guided prompting technique for detecting hateful memes, and second, a unified framework for mitigating hateful content in memes, named UnHateMeme, which works by replacing hateful textual and/or visual components. With our definition-guided prompts, VLMs achieve impressive performance on hateful memes detection task. Furthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a strong capability to convert hateful memes into non-hateful forms that meet human-level criteria for hate speech and maintain multimodal coherence between image and text. Through empirical experiments, we show the effectiveness of state-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the proposed tasks, providing a comprehensive analysis of their respective strengths and limitations for these tasks. This paper aims to shed light on important applications of VLMs for ensuring safe and respectful online environments.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.00156</link>
<guid>https://arxiv.org/abs/2505.00156</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision Language Models, 3D scene understanding, V3LMA, autonomous driving, object detection

Summary: 
V3LMA is a new approach that combines Large Language Models with Large Vision Language Models to enhance 3D scene understanding in the context of autonomous driving. By leveraging textual descriptions from object detections and video inputs, V3LMA improves situational awareness and decision-making in complex traffic scenarios without the need for fine-tuning. The method achieves a benchmark score of 0.56 on the LingoQA benchmark, demonstrating its effectiveness in interpreting traffic scenes for safer autonomous driving systems. Different fusion strategies and token combinations are explored to further advance the understanding of 3D environments, ultimately aiming to create a complete and safe understanding of dynamic surroundings for autonomous vehicles.

<br /><br />Summary: <div>
arXiv:2505.00156v1 Announce Type: new 
Abstract: Large Vision Language Models (LVLMs) have shown strong capabilities in understanding and analyzing visual scenes across various domains. However, in the context of autonomous driving, their limited comprehension of 3D environments restricts their effectiveness in achieving a complete and safe understanding of dynamic surroundings. To address this, we introduce V3LMA, a novel approach that enhances 3D scene understanding by integrating Large Language Models (LLMs) with LVLMs. V3LMA leverages textual descriptions generated from object detections and video inputs, significantly boosting performance without requiring fine-tuning. Through a dedicated preprocessing pipeline that extracts 3D object data, our method improves situational awareness and decision-making in complex traffic scenarios, achieving a score of 0.56 on the LingoQA benchmark. We further explore different fusion strategies and token combinations with the goal of advancing the interpretation of traffic scenes, ultimately enabling safer autonomous driving systems.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Motion Models for Assessing Generated Videos</title>
<link>https://arxiv.org/abs/2505.00209</link>
<guid>https://arxiv.org/abs/2505.00209</guid>
<content:encoded><![CDATA[
<div> motion, video generative models, point tracks, evaluation metric, object interactions

Summary:<br />
The article introduces a novel evaluation metric for video generative models that focuses on capturing plausible object interactions and motion. By auto-encoding point tracks, this approach generates motion features that can compare distributions of videos and evaluate the motion of single videos effectively. The metric based on point tracks proves to be more sensitive to temporal distortions in synthetic data and can predict human evaluations of temporal consistency and realism in generated videos more accurately than existing methods. Using point tracks also enables the spatiotemporal localization of generative video errors, providing additional interpretability compared to previous methods. The results demonstrate the effectiveness of this approach in evaluating motion quality in generated videos and offer a valuable tool for improving the temporal coherence and realism of video generative models. <div>
arXiv:2505.00209v1 Announce Type: new 
Abstract: A current limitation of video generative video models is that they generate plausible looking frames, but poor motion -- an issue that is not well captured by FVD and other popular methods for evaluating generated videos. Here we go beyond FVD by developing a metric which better measures plausible object interactions and motion. Our novel approach is based on auto-encoding point tracks and yields motion features that can be used to not only compare distributions of videos (as few as one generated and one ground truth, or as many as two datasets), but also for evaluating motion of single videos. We show that using point tracks instead of pixel reconstruction or action recognition features results in a metric which is markedly more sensitive to temporal distortions in synthetic data, and can predict human evaluations of temporal consistency and realism in generated videos obtained from open-source models better than a wide range of alternatives. We also show that by using a point track representation, we can spatiotemporally localize generative video inconsistencies, providing extra interpretability of generated video errors relative to prior work. An overview of the results and link to the code can be found on the project page: http://trajan-paper.github.io.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust and Generalizable Gerchberg Saxton based Physics Inspired Neural Networks for Computer Generated Holography: A Sensitivity Analysis Framework</title>
<link>https://arxiv.org/abs/2505.00220</link>
<guid>https://arxiv.org/abs/2505.00220</guid>
<content:encoded><![CDATA[
<div> Keywords: Computer-generated holography, Physics-inspired neural networks, Forward models, Sensitivity analysis, Generalization<br />
Summary:<br />
Computer-generated holography (CGH) enables various applications, but the fundamental challenge lies in solving the inverse problem of phase retrieval. Physics-inspired neural networks (PINNs), particularly Gerchberg-Saxton-based PINNs (GS-PINNs), have shown promising phase retrieval capabilities. However, their performance heavily relies on forward models (FMs) and their hyperparameters (FMHs), which can limit generalization and make benchmarking complex. A systematic sensitivity analysis framework based on Saltelli's method was introduced to quantify the impact of FMHs on GS-PINN performance. Pixel-resolution of spatial light modulators (SLMs) was identified as the primary factor affecting neural network sensitivity. Free space propagation forward models outperformed Fourier holography, leading to better parameterization and generalization. A composite evaluation metric was proposed to evaluate performance consistency, generalization capability, and hyperparameter perturbation resilience, establishing a unified benchmarking standard for CGH configurations. This research provides guidelines for forward model selection, neural network architecture, and performance evaluation in CGH applications. <div>
arXiv:2505.00220v1 Announce Type: new 
Abstract: Computer-generated holography (CGH) enables applications in holographic augmented reality (AR), 3D displays, systems neuroscience, and optical trapping. The fundamental challenge in CGH is solving the inverse problem of phase retrieval from intensity measurements. Physics-inspired neural networks (PINNs), especially Gerchberg-Saxton-based PINNs (GS-PINNs), have advanced phase retrieval capabilities. However, their performance strongly depends on forward models (FMs) and their hyperparameters (FMHs), limiting generalization, complicating benchmarking, and hindering hardware optimization. We present a systematic sensitivity analysis framework based on Saltelli's extension of Sobol's method to quantify FMH impacts on GS-PINN performance. Our analysis demonstrates that SLM pixel-resolution is the primary factor affecting neural network sensitivity, followed by pixel-pitch, propagation distance, and wavelength. Free space propagation forward models demonstrate superior neural network performance compared to Fourier holography, providing enhanced parameterization and generalization. We introduce a composite evaluation metric combining performance consistency, generalization capability, and hyperparameter perturbation resilience, establishing a unified benchmarking standard across CGH configurations. Our research connects physics-inspired deep learning theory with practical CGH implementations through concrete guidelines for forward model selection, neural network architecture, and performance evaluation. Our contributions advance the development of robust, interpretable, and generalizable neural networks for diverse holographic applications, supporting evidence-based decisions in CGH research and implementation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports</title>
<link>https://arxiv.org/abs/2505.00228</link>
<guid>https://arxiv.org/abs/2505.00228</guid>
<content:encoded><![CDATA[
<div> largest, chest X-ray, dataset, AI systems, medical imaging <br />
Summary: <br />
ReXGradient-160K is a new dataset that consists of 160,000 chest X-ray studies with radiological reports from over 100,000 patients. It is the largest publicly available chest X-ray dataset to date, providing valuable resources for the development and evaluation of AI systems in medical imaging and automated report generation models. The dataset includes multiple images per study and detailed radiology reports, divided into training, validation, and test sets. An additional private test set is reserved for model evaluation on the ReXrank benchmark. By releasing this comprehensive dataset, the creators aim to accelerate research in medical imaging AI and improve automated radiological analysis. The dataset will be open-sourced for public use. <div>
arXiv:2505.00228v1 Announce Type: new 
Abstract: We present ReXGradient-160K, representing the largest publicly available chest X-ray dataset to date in terms of the number of patients. This dataset contains 160,000 chest X-ray studies with paired radiological reports from 109,487 unique patients across 3 U.S. health systems (79 medical sites). This comprehensive dataset includes multiple images per study and detailed radiology reports, making it particularly valuable for the development and evaluation of AI systems for medical imaging and automated report generation models. The dataset is divided into training (140,000 studies), validation (10,000 studies), and public test (10,000 studies) sets, with an additional private test set (10,000 studies) reserved for model evaluation on the ReXrank benchmark. By providing this extensive dataset, we aim to accelerate research in medical imaging AI and advance the state-of-the-art in automated radiological analysis. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Agentic Video Analytics Systems with Video Language Models</title>
<link>https://arxiv.org/abs/2505.00254</link>
<guid>https://arxiv.org/abs/2505.00254</guid>
<content:encoded><![CDATA[
<div> Video-Language Models, VLMs, AVA, Event Knowledge Graphs, EKGs

Summary:
AVA is a new system utilizing Video-Language Models (VLMs) for advanced video analytics. It addresses the challenge of processing ultra-long video content by constructing Event Knowledge Graphs (EKGs) in near real-time for efficient indexing and utilizing an agentic retrieval-generation mechanism. AVA outperforms existing VLM and Retrieval-Augmented Generation (RAG) systems on benchmarks, achieving 62.3% and 64.1% accuracy. A new benchmark, AVA-100, with videos over 10 hours and complex question-answer pairs, shows AVA achieving top-tier performance with 75.8% accuracy. This system has the potential to revolutionize open-ended video understanding, reasoning, and analytics in a variety of domains. <br /><br />Summary: <div>
arXiv:2505.00254v1 Announce Type: new 
Abstract: AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Video-Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVA, a VLM-powered system designed for open-ended, advanced video analytics. AVA incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively, significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVA-100, AVA achieves top-tier performance with an accuracy of 75.8%.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pack-PTQ: Advancing Post-training Quantization of Neural Networks by Pack-wise Reconstruction</title>
<link>https://arxiv.org/abs/2505.00259</link>
<guid>https://arxiv.org/abs/2505.00259</guid>
<content:encoded><![CDATA[
<div> Hessian-guided adaptive packing, mixed-precision quantization, image classification, point cloud classification, model compression
<br />
Post-training quantization (PTQ) is a popular method for compressing complex models, but existing PTQ methods often result in accuracy drops in low-bit cases due to block-wise reconstruction. This paper introduces Pack-PTQ, a novel approach that addresses these limitations. Pack-PTQ employs a Hessian-guided adaptive packing mechanism to partition blocks into non-overlapping packs, preserving cross-block dependency and enabling accurate quantization parameter estimation. Additionally, a mixed-precision quantization approach assigns varied bit-widths to packs based on sensitivity, further improving performance. Experimental results on image and point cloud classification tasks show that Pack-PTQ outperforms state-of-the-art PTQ methods across various network architectures.
<br /><br />Summary: <div>
arXiv:2505.00259v1 Announce Type: new 
Abstract: Post-training quantization (PTQ) has evolved as a prominent solution for compressing complex models, which advocates a small calibration dataset and avoids end-to-end retraining. However, most existing PTQ methods employ block-wise reconstruction, which neglects cross-block dependency and exhibits a notable accuracy drop in low-bit cases. To address these limitations, this paper presents a novel PTQ method, dubbed Pack-PTQ. First, we design a Hessian-guided adaptive packing mechanism to partition blocks into non-overlapping packs, which serve as the base unit for reconstruction, thereby preserving the cross-block dependency and enabling accurate quantization parameters estimation. Second, based on the pack configuration, we propose a mixed-precision quantization approach to assign varied bit-widths to packs according to their distinct sensitivities, thereby further enhancing performance. Extensive experiments on 2D image and 3D point cloud classification tasks, using various network architectures, demonstrate the superiority of our method over the state-of-the-art PTQ methods.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care</title>
<link>https://arxiv.org/abs/2505.00275</link>
<guid>https://arxiv.org/abs/2505.00275</guid>
<content:encoded><![CDATA[
<div> Video-LLaVA, medication adherence, LVLM, VQA, tuberculosis<br />
<br />
Summary:<br />
The article introduces AdCare-VLM, a specialized LVLM for medication adherence through patient videos. Utilizing a dataset of tuberculosis medication monitoring videos, the model is fine-tuned for adherence pattern detection. The LLM-TB-VQA dataset is created to encompass various adherence cases. The method identifies correlations between visual features and medical concepts, improving multimodal interactions. Experimental results show improvement over existing VLM models. Ablation studies and attention map visualizations validate the approach, enhancing interpretability. <div>
arXiv:2505.00275v1 Announce Type: new 
Abstract: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS, epilepsy, and tuberculosis, necessitate rigorous adherence to medication to avert disease progression, manage symptoms, and decrease mortality rates. Adherence is frequently undermined by factors including patient behavior, caregiver support, elevated medical costs, and insufficient healthcare infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based multimodal large vision language model (LVLM) aimed at visual question answering (VQA) concerning medication adherence through patient videos. We employ a private dataset comprising 806 custom-annotated tuberculosis (TB) medication monitoring videos, which have been labeled by clinical experts, to fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a detailed medical adherence VQA dataset that encompasses positive, negative, and ambiguous adherence cases. Our method identifies correlations between visual features, such as the clear visibility of the patient's face, medication, water intake, and the act of ingestion, and their associated medical concepts in captions. This facilitates the integration of aligned visual-linguistic representations and improves multimodal interactions. Experimental results indicate that our method surpasses parameter-efficient fine-tuning (PEFT) enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute improvements ranging from 3.1% to 3.54% across pre-trained, regular, and low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and attention map visualizations substantiate our approach, enhancing interpretability.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-grained spatial-temporal perception for gas leak segmentation</title>
<link>https://arxiv.org/abs/2505.00295</link>
<guid>https://arxiv.org/abs/2505.00295</guid>
<content:encoded><![CDATA[
<div> algorithm, gas leak segmentation, spatial-temporal perception, fine-grained, object features<br />
Summary:<br />
The paper introduces a Fine-grained Spatial-Temporal Perception (FGSTP) algorithm for gas leak segmentation, addressing the challenges of detecting and segmenting random-shaped and concealed gas leaks. The algorithm captures motion clues across frames and refines object features in an end-to-end network. It constructs a correlation volume to capture motion information and progressively refines object-level features using previous outputs. A decoder optimizes boundary segmentation. The researchers manually label the GasVid gas leak video dataset due to the lack of highly precise labeled data for gas leak segmentation. Experimental results show that the FGSTP model excels in segmenting non-rigid objects like gas leaks, outperforming other state-of-the-art models in generating accurate masks. <br /> <div>
arXiv:2505.00295v1 Announce Type: new 
Abstract: Gas leaks pose significant risks to human health and the environment. Despite long-standing concerns, there are limited methods that can efficiently and accurately detect and segment leaks due to their concealed appearance and random shapes. In this paper, we propose a Fine-grained Spatial-Temporal Perception (FGSTP) algorithm for gas leak segmentation. FGSTP captures critical motion clues across frames and integrates them with refined object features in an end-to-end network. Specifically, we first construct a correlation volume to capture motion information between consecutive frames. Then, the fine-grained perception progressively refines the object-level features using previous outputs. Finally, a decoder is employed to optimize boundary segmentation. Because there is no highly precise labeled dataset for gas leak segmentation, we manually label a gas leak video dataset, GasVid. Experimental results on GasVid demonstrate that our model excels in segmenting non-rigid objects such as gas leaks, generating the most accurate mask compared to other state-of-the-art (SOTA) models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality</title>
<link>https://arxiv.org/abs/2505.00308</link>
<guid>https://arxiv.org/abs/2505.00308</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Quality Assessment, Auto-generated contours, Online Adaptive Radiotherapy, Bayesian Ordinal Classification<br />
<br />
Summary:<br />
This study introduces a Deep Learning-based quality assessment approach for evaluating auto-generated contours in radiotherapy, particularly in Online Adaptive Radiotherapy (OART). By utilizing Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method allows for confident QA predictions without the need for ground truth contours or extensive manual labeling. The BOC model demonstrated robust performance across various data scenarios, achieving over 90% accuracy on test data with minimal manual labels. Through calibration and uncertainty quantification, the model accurately predicted over 93% of auto-contours' qualities in over 98% of cases, reducing the need for unnecessary manual reviews and highlighting cases requiring correction. This approach enhances contouring efficiency in OART, reducing manual workload and facilitating rapid, informed clinical decisions for safer and more reliable radiotherapy workflows. <div>
arXiv:2505.00308v1 Announce Type: new 
Abstract: Purpose: This study presents a Deep Learning (DL)-based quality assessment (QA) approach for evaluating auto-generated contours (auto-contours) in radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. Methods: We developed a BOC model to classify auto-contour quality and quantify prediction uncertainty. A calibration step was used to optimize uncertainty thresholds that meet clinical accuracy needs. The method was validated under three data scenarios: no manual labels, limited labels, and extensive labels. For rectum contours in prostate cancer, we applied geometric surrogate labels when manual labels were absent, transfer learning when limited, and direct supervision when ample labels were available. Results: The BOC model delivered robust performance across all scenarios. Fine-tuning with just 30 manual labels and calibrating with 34 subjects yielded over 90% accuracy on test data. Using the calibrated threshold, over 93% of the auto-contours' qualities were accurately predicted in over 98% of cases, reducing unnecessary manual reviews and highlighting cases needing correction. Conclusion: The proposed QA model enhances contouring efficiency in OART by reducing manual workload and enabling fast, informed clinical decisions. Through uncertainty quantification, it ensures safer, more reliable radiotherapy workflows.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.00312</link>
<guid>https://arxiv.org/abs/2505.00312</guid>
<content:encoded><![CDATA[
<div> Keywords: Deepfake detection, Ensemble framework, Deep learning, State-of-the-art architectures, Generalization

Summary:
- The rise of synthetic media has increased the importance of deepfake detection to safeguard digital identity and trust.
- A novel two-tier ensemble framework for deepfake detection is proposed, combining multiple instances of Xception, Res2Net101, and EfficientNet-B7 architectures.
- The framework incorporates model diversity by instantiating each architecture three times with different initializations and dynamically combining predictions through a learnable weighting mechanism.
- In experiments, the framework achieved state-of-the-art performance on FF++ and CelebDF-v2 datasets with high AUC and F1 scores.
- The framework demonstrated robust cross-dataset generalization, showcasing its effectiveness in detecting deepfakes across diverse datasets.

<br /><br />Summary: 
The article introduces a two-tier ensemble framework for deepfake detection that combines state-of-the-art architectures and enhances model diversity through unique instantiation and dynamic prediction weighting. Experimental results show the framework's superior performance in intra-dataset detection with high AUC and F1 scores. Additionally, the framework exhibits strong generalization capabilities across diverse datasets, showcasing its effectiveness in detecting deepfakes in various scenarios. <div>
arXiv:2505.00312v1 Announce Type: new 
Abstract: Deepfake detection has become increasingly important due to the rise of synthetic media, which poses significant risks to digital identity and cyber presence for security and trust. While multiple approaches have improved detection accuracy, challenges remain in achieving consistent performance across diverse datasets and manipulation types. In response, we propose a novel two-tier ensemble framework for deepfake detection based on deep learning that hierarchically combines multiple instances of three state-of-the-art architectures: Xception, Res2Net101, and EfficientNet-B7. Our framework employs a unique approach where each architecture is instantiated three times with different initializations to enhance model diversity, followed by a learnable weighting mechanism that dynamically combines their predictions. Unlike traditional fixed-weight ensembles, our first-tier averages predictions within each architecture family to reduce model variance, while the second tier learns optimal contribution weights through backpropagation, automatically adjusting each architecture's influence based on their detection reliability. Our experiments achieved state-of-the-art intra-dataset performance with AUC scores of 99.22% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.06% (FF++) and 99.94% (CelebDF-v2) without augmentation. With augmentation, we achieve AUC scores of 99.47% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.43% (FF++) and 99.95% (CelebDF-v2). The framework demonstrates robust cross-dataset generalization, achieving AUC scores of 88.20% and 72.52%, and F1 scores of 93.16% and 80.62% in cross-dataset evaluations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.00334</link>
<guid>https://arxiv.org/abs/2505.00334</guid>
<content:encoded><![CDATA[
<div> wavelet, diffusion model, image super-resolution, deep learning, high-resolution reconstruction <br />
<br />
Summary: <br />
Image Super-Resolution is a crucial task in computer vision, but existing methods struggle to balance perceptual quality and structural fidelity. The ResQu framework introduced in this work integrates quaternion wavelet preprocessing with latent diffusion models, using a unique encoder. Unlike previous methods, ResQu enhances the conditioning process by incorporating quaternion wavelet embeddings dynamically during denoising. The framework also leverages generative priors from foundation models like Stable Diffusion. Extensive experiments show that ResQu achieves outstanding SR results, surpassing existing approaches in perceptual quality and standard evaluation metrics. The code for ResQu will be made available after the revision process. <div>
arXiv:2505.00334v1 Announce Type: new 
Abstract: Image Super-Resolution is a fundamental problem in computer vision with broad applications spacing from medical imaging to satellite analysis. The ability to reconstruct high-resolution images from low-resolution inputs is crucial for enhancing downstream tasks such as object detection and segmentation. While deep learning has significantly advanced SR, achieving high-quality reconstructions with fine-grained details and realistic textures remains challenging, particularly at high upscaling factors. Recent approaches leveraging diffusion models have demonstrated promising results, yet they often struggle to balance perceptual quality with structural fidelity. In this work, we introduce ResQu a novel SR framework that integrates a quaternion wavelet preprocessing framework with latent diffusion models, incorporating a new quaternion wavelet- and time-aware encoder. Unlike prior methods that simply apply wavelet transforms within diffusion models, our approach enhances the conditioning process by exploiting quaternion wavelet embeddings, which are dynamically integrated at different stages of denoising. Furthermore, we also leverage the generative priors of foundation models such as Stable Diffusion. Extensive experiments on domain-specific datasets demonstrate that our method achieves outstanding SR results, outperforming in many cases existing approaches in perceptual quality and standard evaluation metrics. The code will be available after the revision process.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Neural Video Representation with Temporally Coherent Modulation</title>
<link>https://arxiv.org/abs/2505.00335</link>
<guid>https://arxiv.org/abs/2505.00335</guid>
<content:encoded><![CDATA[
<div> Keywords: Implicit neural representations, video applications, parameter efficiency, Neural Video representation, video compression

Summary:<br /><br />Implicit neural representations (INR) have shown success in various domains, but the training process needs to be accelerated for real-world applications. A new framework called Neural Video representation with Temporally coherent Modulation (NVTM) is proposed to efficiently capture the dynamic characteristics of video content by decomposing data into 2D grids with flow information. NVTM outperforms grid-type methods in parameter efficiency, encoding speed, and video quality, showing improvements in PSNR/LPIPS on dynamic video datasets. It achieves faster encoding speeds compared to NeRV-style methods, with a 3x speed increase. When applied to compression tasks, NVTM shows comparable performance to established video compression standards like H.264 and HEVC. Extensive experiments across super resolution, frame interpolation, and video inpainting tasks demonstrate the superior performance of NVTM in diverse applications. The research presents a promising advancement in utilizing INR for efficient and effective video representation and compression. <br /><br />Summary: <div>
arXiv:2505.00335v1 Announce Type: new 
Abstract: Implicit neural representations (INR) has found successful applications across diverse domains. To employ INR in real-life, it is important to speed up training. In the field of INR for video applications, the state-of-the-art approach employs grid-type parametric encoding and successfully achieves a faster encoding speed in comparison to its predecessors. However, the grid usage, which does not consider the video's dynamic nature, leads to redundant use of trainable parameters. As a result, it has significantly lower parameter efficiency and higher bitrate compared to NeRV-style methods that do not use a parametric encoding. To address the problem, we propose Neural Video representation with Temporally coherent Modulation (NVTM), a novel framework that can capture dynamic characteristics of video. By decomposing the spatio-temporal 3D video data into a set of 2D grids with flow information, NVTM enables learning video representation rapidly and uses parameter efficiently. Our framework enables to process temporally corresponding pixels at once, resulting in the fastest encoding speed for a reasonable video quality, especially when compared to the NeRV-style method, with a speed increase of over 3 times. Also, it remarks an average of 1.54dB/0.019 improvements in PSNR/LPIPS on UVG (Dynamic) (even with 10% fewer parameters) and an average of 1.84dB/0.013 improvements in PSNR/LPIPS on MCL-JCV (Dynamic), compared to previous grid-type works. By expanding this to compression tasks, we demonstrate comparable performance to video compression standards (H.264, HEVC) and recent INR approaches for video compression. Additionally, we perform extensive experiments demonstrating the superior performance of our algorithm across diverse tasks, encompassing super resolution, frame interpolation and video inpainting. Project page is https://sujiikim.github.io/NVTM/.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated segmenta-on of pediatric neuroblastoma on multi-modal MRI: Results of the SPPIN challenge at MICCAI 2023</title>
<link>https://arxiv.org/abs/2505.00369</link>
<guid>https://arxiv.org/abs/2505.00369</guid>
<content:encoded><![CDATA[
<div> Challenge, Surgery, Neuroblastoma, MRI, Segmentation  
Summary:<br /><br />The Surgical Planning in Pediatric Neuroblastoma (SPPIN) challenge focused on automatic segmentation of neuroblastoma on multi-model MRI scans to aid in surgical planning for pediatric cancer. The challenge involved training and testing phases, with teams ranked based on segmentation accuracy metrics. The highest-ranking team utilized a large pre-trained network, STU-Net, achieving high segmentation scores. However, there was a significant difference in segmentation results between diagnostic and post-chemotherapy MRI scans. The challenge highlighted the need for more reliable segmentation methods, especially for small, pre-treated tumors, to improve surgical planning for pediatric neuroblastoma. This challenge marks a significant advancement in medical segmentation in extracranial pediatric oncology, emphasizing the potential benefits of pretraining in small, heterogeneous datasets. More research and development are needed to enhance the clinical applicability of segmentation models for pediatric neuroblastoma surgery.  
Summary: <div>
arXiv:2505.00369v1 Announce Type: new 
Abstract: Surgery plays an important role within the treatment for neuroblastoma, a common pediatric cancer. This requires careful planning, often via magnetic resonance imaging (MRI)-based anatomical 3D models. However, creating these models is often time-consuming and user dependent. We organized the Surgical Planning in Pediatric Neuroblastoma (SPPIN) challenge, to stimulate developments on this topic, and set a benchmark for fully automatic segmentation of neuroblastoma on multi-model MRI. The challenge started with a training phase, where teams received 78 sets of MRI scans from 34 patients, consisting of both diagnostic and post-chemotherapy MRI scans. The final test phase, consisting of 18 MRI sets from 9 patients, determined the ranking of the teams. Ranking was based on the Dice similarity coefficient (Dice score), the 95th percentile of the Hausdorff distance (HD95) and the volumetric similarity (VS). The SPPIN challenge was hosted at MICCAI 2023. The final leaderboard consisted of 9 teams. The highest-ranking team achieved a median Dice score 0.82, a median HD95 of 7.69 mm and a VS of 0.91, utilizing a large, pretrained network called STU-Net. A significant difference for the segmentation results between diagnostic and post-chemotherapy MRI scans was observed (Dice = 0.89 vs Dice = 0.59, P = 0.01) for the highest-ranking team. SPPIN is the first medical segmentation challenge in extracranial pediatric oncology. The highest-ranking team used a large pre-trained network, suggesting that pretraining can be of use in small, heterogenous datasets. Although the results of the highest-ranking team were high for most patients, segmentation especially in small, pre-treated tumors were insufficient. Therefore, more reliable segmentation methods are needed to create clinically applicable models to aid surgical planning in pediatric neuroblastoma.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation</title>
<link>https://arxiv.org/abs/2505.00378</link>
<guid>https://arxiv.org/abs/2505.00378</guid>
<content:encoded><![CDATA[
<div> Keywords: Open-vocabulary 3D panoptic segmentation, Neural Radiance Field, instance disambiguation, 3D point clouds, global consistency<br />
Summary:<br />
The paper introduces Cues3D, a novel approach for 3D panoptic segmentation using Neural Radiance Field (NeRF). Cues3D leverages NeRF's globally consistent geometry to achieve effective object distinction without explicit cross-view supervision. The training framework consists of three phases  initialization, disambiguation, and refinement  to ensure unique 3D instance IDs across views. An instance disambiguation method is proposed to match NeRF-rendered 3D masks and maintain globally unique 3D instance identities. Experimental results on various datasets demonstrate that Cues3D outperforms 2D image-based methods and is competitive with 2D-3D merging based methods, especially when additional 3D point clouds are used. The code for Cues3D is available on GitHub for further exploration and research purposes.<br /><br />Summary: <div>
arXiv:2505.00378v1 Announce Type: new 
Abstract: Open-vocabulary 3D panoptic segmentation has recently emerged as a significant trend. Top-performing methods currently integrate 2D segmentation with geometry-aware 3D primitives. However, the advantage would be lost without high-fidelity 3D point clouds, such as methods based on Neural Radiance Field (NeRF). These methods are limited by the insufficient capacity to maintain consistency across partial observations. To address this, recent works have utilized contrastive loss or cross-view association pre-processing for view consensus. In contrast to them, we present Cues3D, a compact approach that relies solely on NeRF instead of pre-associations. The core idea is that NeRF's implicit 3D field inherently establishes a globally consistent geometry, enabling effective object distinction without explicit cross-view supervision. We propose a three-phase training framework for NeRF, initialization-disambiguation-refinement, whereby the instance IDs are corrected using the initially-learned knowledge. Additionally, an instance disambiguation method is proposed to match NeRF-rendered 3D masks and ensure globally unique 3D instance identities. With the aid of Cues3D, we obtain highly consistent and unique 3D instance ID for each object across views with a balanced version of NeRF. Our experiments are conducted on ScanNet v2, ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and semantic segmentation tasks. Cues3D outperforms other 2D image-based methods and competes with the latest 2D-3D merging based methods, while even surpassing them when using additional 3D point clouds. The code link could be found in the appendix and will be released on \href{https://github.com/mRobotit/Cues3D}{github}
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks</title>
<link>https://arxiv.org/abs/2505.00380</link>
<guid>https://arxiv.org/abs/2505.00380</guid>
<content:encoded><![CDATA[
<div> Cross-spectral face recognition, NIR-VIS, presentation attacks, robustness, vulnerability<br />
Summary:<br />
Cross-spectral face recognition systems aim to improve facial recognition by matching near-infrared (NIR) images with visible-spectrum (VIS) images, offering benefits such as illumination robustness and resistance to presentation attacks. However, the vulnerability of NIR-VIS systems to specific attacks has not been thoroughly investigated. A comprehensive evaluation revealed that while these systems demonstrate reliability, they are still susceptible to certain attacks, highlighting the necessity for further research in this area. <div>
arXiv:2505.00380v1 Announce Type: new 
Abstract: Cross-spectral face recognition systems are designed to enhance the performance of facial recognition systems by enabling cross-modal matching under challenging operational conditions. A particularly relevant application is the matching of near-infrared (NIR) images to visible-spectrum (VIS) images, enabling the verification of individuals by comparing NIR facial captures acquired with VIS reference images. The use of NIR imaging offers several advantages, including greater robustness to illumination variations, better visibility through glasses and glare, and greater resistance to presentation attacks. Despite these claimed benefits, the robustness of NIR-based systems against presentation attacks has not been systematically studied in the literature. In this work, we conduct a comprehensive evaluation into the vulnerability of NIR-VIS cross-spectral face recognition systems to presentation attacks. Our empirical findings indicate that, although these systems exhibit a certain degree of reliability, they remain vulnerable to specific attacks, emphasizing the need for further research in this area.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOTA: Spike-Navigated Optimal TrAnsport Saliency Region Detection in Composite-bias Videos</title>
<link>https://arxiv.org/abs/2505.00394</link>
<guid>https://arxiv.org/abs/2505.00394</guid>
<content:encoded><![CDATA[
<div> spike camera, saliency detection, bias mitigation, SOTA framework, debiasing techniques  
Summary:  
Spike-navigated Optimal TrAnsport Saliency Region Detection (SOTA) is proposed to improve saliency detection in real-world scenarios using spike cameras. The framework leverages Spike-based Micro-debias (SM) to capture subtle frame-to-frame variations and Spike-based Global-debias (SG) to reduce inconsistencies across diverse conditions. By mitigating biases in both spatial and temporal dimensions, SOTA outperforms existing methods by eliminating composite noise bias. Extensive experiments on real and synthetic datasets demonstrate the effectiveness of the proposed framework. The code and dataset for SOTA will be available at the provided GitHub repository. <br /><br />Summary: <div>
arXiv:2505.00394v1 Announce Type: new 
Abstract: Existing saliency detection methods struggle in real-world scenarios due to motion blur and occlusions. In contrast, spike cameras, with their high temporal resolution, significantly enhance visual saliency maps. However, the composite noise inherent to spike camera imaging introduces discontinuities in saliency detection. Low-quality samples further distort model predictions, leading to saliency bias. To address these challenges, we propose Spike-navigated Optimal TrAnsport Saliency Region Detection (SOTA), a framework that leverages the strengths of spike cameras while mitigating biases in both spatial and temporal dimensions. Our method introduces Spike-based Micro-debias (SM) to capture subtle frame-to-frame variations and preserve critical details, even under minimal scene or lighting changes. Additionally, Spike-based Global-debias (SG) refines predictions by reducing inconsistencies across diverse conditions. Extensive experiments on real and synthetic datasets demonstrate that SOTA outperforms existing methods by eliminating composite noise bias. Our code and dataset will be released at https://github.com/lwxfight/sota.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos</title>
<link>https://arxiv.org/abs/2505.00421</link>
<guid>https://arxiv.org/abs/2505.00421</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D human avatar reconstruction, monocular videos, animatable, 2D Gaussian Splatting, Rotation Compensation Network

Summary: 
Our new framework for 3D human avatar reconstruction from monocular videos, based on 2D Gaussian Splatting (2DGS), addresses challenges in capturing fine geometric details and maintaining animation stability. By incorporating global SMPL pose parameters, we align discrepancies and enable realistic pose-driven animation. The Rotation Compensation Network (RCN) learns rotation residuals, improving non-rigid deformation handling and ensuring smooth pose transitions. Experimental results demonstrate the success of our method in reconstructing highly animatable avatars with fine details and stable, natural pose variations. Outperforming current methods in reconstruction quality and animation robustness, our approach is a significant advancement for applications in game development, augmented reality, and social media. <br /><br />Summary: <div>
arXiv:2505.00421v1 Announce Type: new 
Abstract: High-quality, animatable 3D human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. However, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. To address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. Furthermore, we introduce a Rotation Compensation Network (RCN) that learns rotation residuals by integrating local geometric features with global pose parameters. This network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. Experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. Our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Pretrained Diffusion Models for Zero-Shot Part Assembly</title>
<link>https://arxiv.org/abs/2505.00426</link>
<guid>https://arxiv.org/abs/2505.00426</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D part assembly, zero-shot learning, point cloud diffusion models, Iterative Closest Point (ICP), pushing-away strategy

Summary:
In the paper, a new approach for 3D part assembly using zero-shot learning is proposed. By utilizing pre-trained point cloud diffusion models as discriminators, the method guides the manipulation of parts to form realistic shapes without the need for extensive manual labeling. Theoretical analysis shows that the zero-shot assembly process can be transformed into an Iterative Closest Point (ICP) process. A novel pushing-away strategy is introduced to address overlap parts, improving the method's robustness. Extensive experiments and comparisons with baseline methods demonstrate the effectiveness of the proposed approach, surpassing even supervised learning methods. The code for the method has been made available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2505.00426v1 Announce Type: new 
Abstract: 3D part assembly aims to understand part relationships and predict their 6-DoF poses to construct realistic 3D shapes, addressing the growing demand for autonomous assembly, which is crucial for robots. Existing methods mainly estimate the transformation of each part by training neural networks under supervision, which requires a substantial quantity of manually labeled data. However, the high cost of data collection and the immense variability of real-world shapes and parts make traditional methods impractical for large-scale applications. In this paper, we propose first a zero-shot part assembly method that utilizes pre-trained point cloud diffusion models as discriminators in the assembly process, guiding the manipulation of parts to form realistic shapes. Specifically, we theoretically demonstrate that utilizing a diffusion model for zero-shot part assembly can be transformed into an Iterative Closest Point (ICP) process. Then, we propose a novel pushing-away strategy to address the overlap parts, thereby further enhancing the robustness of the method. To verify our work, we conduct extensive experiments and quantitative comparisons to several strong baseline methods, demonstrating the effectiveness of the proposed approach, which even surpasses the supervised learning method. The code has been released on https://github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClearLines - Camera Calibration from Straight Lines</title>
<link>https://arxiv.org/abs/2505.00452</link>
<guid>https://arxiv.org/abs/2505.00452</guid>
<content:encoded><![CDATA[
<div> Dataset creation, Straight 3D line detection, Geometric computer vision, Calibration, Outdoor scenarios
Summary:
The study introduces the "ClearLines" dataset for straight line calibration in geometric computer vision, addressing challenges in real-world outdoor environments. The dataset aims to facilitate the development and refinement of algorithms for detecting straight 3D lines. The practical insights shared in the study offer a guide for researchers working in this field. The problem of calibration from straight lines, though well-established theoretically, faces limitations in real-world applications due to cluttered scenes, interrupted reprojections, and varying lighting conditions. The ClearLines dataset fills a void in the field by providing a dedicated dataset for algorithm development, aiming to enhance the practical applicability of straight line calibration techniques. <div>
arXiv:2505.00452v1 Announce Type: new 
Abstract: The problem of calibration from straight lines is fundamental in geometric computer vision, with well-established theoretical foundations. However, its practical applicability remains limited, particularly in real-world outdoor scenarios. These environments pose significant challenges due to diverse and cluttered scenes, interrupted reprojections of straight 3D lines, and varying lighting conditions, making the task notoriously difficult. Furthermore, the field lacks a dedicated dataset encouraging the development of respective detection algorithms. In this study, we present a small dataset named "ClearLines", and by detailing its creation process, provide practical insights that can serve as a guide for developing and refining straight 3D line detection algorithms.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.00482</link>
<guid>https://arxiv.org/abs/2505.00482</guid>
<content:encoded><![CDATA[
<div> RGB, depth, JointDiT, diffusion transformer, joint distribution modeling

Summary:
JointDiT is a diffusion transformer model that effectively models the joint distribution of RGB and depth information. It employs adaptive scheduling weights and an unbalanced timestep sampling strategy to train across all noise levels for each modality, enabling it to handle various combinatorial generation tasks. The model generates high-fidelity images and accurate depth maps, demonstrating exceptional joint generation performance. Additionally, JointDiT achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can be a viable alternative to conditional generation methods. The proposed techniques in JointDiT make it versatile and robust in handling different tasks related to RGB and depth information. <div>
arXiv:2505.00482v1 Announce Type: new 
Abstract: We present JointDiT, a diffusion transformer that models the joint distribution of RGB and depth. By leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, JointDiT not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. This solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. With these techniques, we train our model across all noise levels for each modality, enabling JointDiT to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. JointDiT demonstrates outstanding joint generation performance. Furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. The project page is available at https://byungki-k.github.io/JointDiT/.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution</title>
<link>https://arxiv.org/abs/2505.00497</link>
<guid>https://arxiv.org/abs/2505.00497</guid>
<content:encoded><![CDATA[
<div> Keywords: lip synchronization, KeySync, facial animation, expression leakage, facial occlusions

Summary:
KeySync is a novel two-stage framework designed to address the challenges in lip synchronization, including temporal consistency, expression leakage, and facial occlusions. It significantly improves lip reconstruction and cross-synchronization, surpassing existing methods. KeySync incorporates a carefully crafted masking strategy to effectively handle expression leakage from the input video and facial occlusions, which are often overlooked. The framework's effectiveness is demonstrated through state-of-the-art results and a novel leakage metric, LipLeak. The code and model weights for KeySync are available online for further exploration and validation. Overall, KeySync presents a robust solution for enhancing lip synchronization quality and addressing crucial issues in automated dubbing and other real-world applications. 

<br /><br />Summary: <div>
arXiv:2505.00497v1 Announce Type: new 
Abstract: Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable Human-aligned Benchmark for Text-guided Image Editing</title>
<link>https://arxiv.org/abs/2505.00502</link>
<guid>https://arxiv.org/abs/2505.00502</guid>
<content:encoded><![CDATA[
<div> benchmark, text-guided, image editing, evaluation method, human-aligned

Summary: 
The article introduces a Human-Aligned benchmark for Text-guided Image Editing (HATIE) to address the lack of a widely-accepted standard evaluation method for text-guided image editing models. The benchmark set covers a wide range of editing tasks, allowing for reliable evaluation across various cases. HATIE offers a fully-automated and omnidirectional evaluation pipeline, combining multiple scores to align with human perception. Empirical verification demonstrates that HATIE's evaluation is indeed human-aligned in different aspects. Benchmark results on state-of-the-art models provide insights into their performance. <div>
arXiv:2505.00502v1 Announce Type: new 
Abstract: A variety of text-guided image editing models have been proposed recently. However, there is no widely-accepted standard evaluation method mainly due to the subjective nature of the task, letting researchers rely on manual user study. To address this, we introduce a novel Human-Aligned benchmark for Text-guided Image Editing (HATIE). Providing a large-scale benchmark set covering a wide range of editing tasks, it allows reliable evaluation, not limited to specific easy-to-evaluate cases. Also, HATIE provides a fully-automated and omnidirectional evaluation pipeline. Particularly, we combine multiple scores measuring various aspects of editing so as to align with human perception. We empirically verify that the evaluation of HATIE is indeed human-aligned in various aspects, and provide benchmark results on several state-of-the-art models to provide deeper insights on their performance.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection</title>
<link>https://arxiv.org/abs/2505.00507</link>
<guid>https://arxiv.org/abs/2505.00507</guid>
<content:encoded><![CDATA[
<div> Active Learning, 3D object detection, Autonomous Driving, HeAL, Heuristical features<br />
Summary:<br />
Active Learning is a valuable approach for training models in Autonomous Driving, especially for 3D object detection. Existing methods struggle with sample selection in uncontrolled scenarios and often focus only on theoretical aspects. HeAL (Heuristical-enhanced Active Learning) introduces heuristical features like object distance and point-quantity to improve sample selection. By integrating these features with Localization and Classification, HeAL delivers high-quality samples for training. Evaluation on KITTI dataset shows that HeAL achieves competitive mean Average Precision (mAP) compared to State-of-the-Art approaches. It also reaches the same mAP as full-supervised baselines using only 24% of the samples. <div>
arXiv:2505.00507v1 Announce Type: new 
Abstract: Active Learning has proved to be a relevant approach to perform sample selection for training models for Autonomous Driving. Particularly, previous works on active learning for 3D object detection have shown that selection of samples in uncontrolled scenarios is challenging. Furthermore, current approaches focus exclusively on the theoretical aspects of the sample selection problem but neglect the practical insights that can be obtained from the extensive literature and application of 3D detection models. In this paper, we introduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection) which integrates those heuristical features together with Localization and Classification to deliver the most contributing samples to the model's training. In contrast to previous works, our approach integrates heuristical features such as object distance and point-quantity to estimate the uncertainty, which enhance the usefulness of selected samples to train detection models. Our quantitative evaluation on KITTI shows that HeAL presents competitive mAP with respect to the State-of-the-Art, and achieves the same mAP as the full-supervised baseline with only 24% of the samples.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inconsistency-based Active Learning for LiDAR Object Detection</title>
<link>https://arxiv.org/abs/2505.00511</link>
<guid>https://arxiv.org/abs/2505.00511</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, object detection, autonomous driving, LiDAR, active learning

Summary: 
Deep learning models for object detection in autonomous driving have shown significant performance improvements but require large labeled datasets for training. This process is costly and time-consuming, prompting the need for new strategies. Active learning, a method extensively studied in the image domain, is applied to the LiDAR domain in this work. Multiple inconsistency-based sample selection strategies are developed and evaluated for efficacy. By employing a naive inconsistency approach based on the number of detected boxes, the same mean Average Precision (mAP) is achieved as the random sampling strategy with only 50% of the labeled data. This demonstrates the potential for optimizing the data labeling process in autonomous driving applications through active learning strategies tailored to LiDAR data. 

Summary: <div>
arXiv:2505.00511v1 Announce Type: new 
Abstract: Deep learning models for object detection in autonomous driving have recently achieved impressive performance gains and are already being deployed in vehicles worldwide. However, current models require increasingly large datasets for training. Acquiring and labeling such data is costly, necessitating the development of new strategies to optimize this process. Active learning is a promising approach that has been extensively researched in the image domain. In our work, we extend this concept to the LiDAR domain by developing several inconsistency-based sample selection strategies and evaluate their effectiveness in various settings. Our results show that using a naive inconsistency approach based on the number of detected boxes, we achieve the same mAP as the random sampling strategy with 50% of the labeled data.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method</title>
<link>https://arxiv.org/abs/2505.00512</link>
<guid>https://arxiv.org/abs/2505.00512</guid>
<content:encoded><![CDATA[
<div> intersection detection, LiDAR-based method, semantic road segmentation, vehicle localization, benchmarking pipeline

Summary:
A new LiDAR-based method for intersection detection is presented in this paper. The method fuses semantic road segmentation with vehicle localization to detect intersection candidates in a bird's eye view and refines them by analyzing branch topology using a least squares formulation. An automated benchmarking pipeline is introduced to evaluate the method by pairing detections with OpenStreetMap intersection nodes using precise GNSS/INS ground-truth poses. Tested on eight SemanticKITTI sequences, the approach achieves a mean localization error of 1.9 m, 89% precision, and 77% recall at a 5 m tolerance, outperforming the latest learning-based baseline and proving its robustness to segmentation errors. This method provides a reliable solution for intersection detection in road networks, leveraging existing semantic information to improve accuracy and efficiency. 

<br /><br />Summary: <div>
arXiv:2505.00512v1 Announce Type: new 
Abstract: Intersections are geometric and functional key points in every road network. They offer strong landmarks to correct GNSS dropouts and anchor new sensor data in up-to-date maps. Despite that importance, intersection detectors either ignore the rich semantic information already computed onboard or depend on scarce, hand-labeled intersection datasets. To close that gap, this paper presents a LiDAR-based method for intersection detection that (i) fuses semantic road segmentation with vehicle localization to detect intersection candidates in a bird's eye view (BEV) representation and (ii) refines those candidates by analyzing branch topology with a least squares formulation. To evaluate our method, we introduce an automated benchmarking pipeline that pairs detections with OpenStreetMap (OSM) intersection nodes using precise GNSS/INS ground-truth poses. Tested on eight SemanticKITTI sequences, the approach achieves a mean localization error of 1.9 m, 89% precision, and 77% recall at a 5 m tolerance, outperforming the latest learning-based baseline. Moreover, the method is robust to segmentation errors higher than those of the benchmark model, demonstrating its applicability in the real world.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic</title>
<link>https://arxiv.org/abs/2505.00534</link>
<guid>https://arxiv.org/abs/2505.00534</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision sensors, Intelligent Transportation Systems, Multi-Object Multi-Camera Tracking, Deep learning, Object detection

Summary:
The article introduces a deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT) in urban traffic scenarios. The framework uses Mask R-CNN for object detection and Non-Maximum Suppression for target selection. Transfer learning is implemented for re-identification to associate vehicle tracklets across multiple cameras. Special loss functions and distance measures are utilized to handle challenges like occlusion and illumination. Feature extraction is done using ResNet-152 and Deep SORT for vehicle tracking. The framework is tested on the 5th AI City Challenge dataset and achieves competitive performance with an IDF1 score of 0.8289, precision of 0.9026, and recall of 0.8527. This demonstrates its effectiveness in robust and accurate vehicle tracking.<br /><br />Summary: <div>
arXiv:2505.00534v1 Announce Type: new 
Abstract: Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-ray illicit object detection using hybrid CNN-transformer neural network architectures</title>
<link>https://arxiv.org/abs/2505.00564</link>
<guid>https://arxiv.org/abs/2505.00564</guid>
<content:encoded><![CDATA[
<div> CNN, transformer, X-ray security, object detection, deep learning
Summary:
This paper explores the integration of CNN and transformer architectures in X-ray security applications for object detection. The study evaluates hybrid CNN-transformer architectures against a CNN object detection baseline (YOLOv8) on three public X-ray inspection datasets. The results show that while YOLOv8 with a default backbone performs well on certain datasets, hybrid CNN-transformer architectures demonstrate increased robustness when faced with domain distribution shifts in X-ray images. Object-level detection performance and object-size error analysis highlight the strengths and weaknesses of each architectural combination, providing insights for future research in this field. The source code and network weights of the models used in the study are available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2505.00564v1 Announce Type: new 
Abstract: In the field of X-ray security applications, even the smallest details can significantly impact outcomes. Objects that are heavily occluded or intentionally concealed pose a great challenge for detection, whether by human observation or through advanced technological applications. While certain Deep Learning (DL) architectures demonstrate strong performance in processing local information, such as Convolutional Neural Networks (CNNs), others excel in handling distant information, e.g., transformers. In X-ray security imaging the literature has been dominated by the use of CNN-based methods, while the integration of the two aforementioned leading architectures has not been sufficiently explored. In this paper, various hybrid CNN-transformer architectures are evaluated against a common CNN object detection baseline, namely YOLOv8. In particular, a CNN (HGNetV2) and a hybrid CNN-transformer (Next-ViT-S) backbone are combined with different CNN/transformer detection heads (YOLOv8 and RT-DETR). The resulting architectures are comparatively evaluated on three challenging public X-ray inspection datasets, namely EDS, HiXray, and PIDray. Interestingly, while the YOLOv8 detector with its default backbone (CSP-DarkNet53) is generally shown to be advantageous on the HiXray and PIDray datasets, when a domain distribution shift is incorporated in the X-ray images (as happens in the EDS datasets), hybrid CNN-transformer architectures exhibit increased robustness. Detailed comparative evaluation results, including object-level detection performance and object-size error analysis, demonstrate the strengths and weaknesses of each architectural combination and suggest guidelines for future research. The source code and network weights of the models employed in this study are available at https://github.com/jgenc/xray-comparative-evaluation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities</title>
<link>https://arxiv.org/abs/2505.00568</link>
<guid>https://arxiv.org/abs/2505.00568</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal MRI, pre-training, BM-MAE, image modeling, transferable representations

Summary: 
Multimodal MRI is essential for brain tumor care, but missing modalities complicate model training. The proposed BM-MAE pre-training approach addresses this challenge, allowing models to adapt to any combination of available modalities without retraining. The method extracts comprehensive representations capturing both intra- and inter-modal information. Experimental results demonstrate superior performance compared to baselines requiring separate pre-training for each modality subset and significant improvement over training from scratch on various tasks. Moreover, BM-MAE efficiently reconstructs missing modalities, showcasing its practical utility in clinical settings. The code and trained models are publicly available for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2505.00568v1 Announce Type: new 
Abstract: Multimodal magnetic resonance imaging (MRI) constitutes the first line of investigation for clinicians in the care of brain tumors, providing crucial insights for surgery planning, treatment monitoring, and biomarker identification. Pre-training on large datasets have been shown to help models learn transferable representations and adapt with minimal labeled data. This behavior is especially valuable in medical imaging, where annotations are often scarce. However, applying this paradigm to multimodal medical data introduces a challenge: most existing approaches assume that all imaging modalities are available during both pre-training and fine-tuning. In practice, missing modalities often occur due to acquisition issues, specialist unavailability, or specific experimental designs on small in-house datasets. Consequently, a common approach involves training a separate model for each desired modality combination, making the process both resource-intensive and impractical for clinical use. Therefore, we introduce BM-MAE, a masked image modeling pre-training strategy tailored for multimodal MRI data. The same pre-trained model seamlessly adapts to any combination of available modalities, extracting rich representations that capture both intra- and inter-modal information. This allows fine-tuning on any subset of modalities without requiring architectural changes, while still benefiting from a model pre-trained on the full set of modalities. Extensive experiments show that the proposed pre-training strategy outperforms or remains competitive with baselines that require separate pre-training for each modality subset, while substantially surpassing training from scratch on several downstream tasks. Additionally, it can quickly and efficiently reconstruct missing modalities, highlighting its practical value. Code and trained models are available at: https://github.com/Lucas-rbnt/bmmae
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnimalMotionCLIP: Embedding motion in CLIP for Animal Behavior Analysis</title>
<link>https://arxiv.org/abs/2505.00569</link>
<guid>https://arxiv.org/abs/2505.00569</guid>
<content:encoded><![CDATA[
<div> Keyword: deep learning, animal behavior recognition, CLIP, motion information, temporal modeling

Summary:
AnimalMotionCLIP addresses challenges in adapting deep learning techniques to animal behavior recognition by incorporating motion information into the CLIP framework. The proposed model interleaves video frames and optical flow information, allowing for the recognition of fine temporal actions crucial in animal behavior analysis. Different temporal modeling schemes are explored and compared, including dense, semi-dense, and sparse aggregation of classifiers. Experiments on the Animal Kingdom dataset show that AnimalMotionCLIP outperforms current state-of-the-art approaches in recognizing animal behaviors. This innovative approach demonstrates the effectiveness of integrating motion information and devising effective temporal modeling schemes in deep learning models for animal behavior recognition.<br /><br />Summary: <div>
arXiv:2505.00569v1 Announce Type: new 
Abstract: Recently, there has been a surge of interest in applying deep learning techniques to animal behavior recognition, particularly leveraging pre-trained visual language models, such as CLIP, due to their remarkable generalization capacity across various downstream tasks. However, adapting these models to the specific domain of animal behavior recognition presents two significant challenges: integrating motion information and devising an effective temporal modeling scheme. In this paper, we propose AnimalMotionCLIP to address these challenges by interleaving video frames and optical flow information in the CLIP framework. Additionally, several temporal modeling schemes using an aggregation of classifiers are proposed and compared: dense, semi dense, and sparse. As a result, fine temporal actions can be correctly recognized, which is of vital importance in animal behavior analysis. Experiments on the Animal Kingdom dataset demonstrate that AnimalMotionCLIP achieves superior performance compared to state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets</title>
<link>https://arxiv.org/abs/2505.00584</link>
<guid>https://arxiv.org/abs/2505.00584</guid>
<content:encoded><![CDATA[
<div> Keywords: object detection, tracking, autonomous navigation, synthetic data augmentation, sensor failures <br />
Summary: <br />
- Object detection and tracking are essential for autonomous navigation, with neural networks showing promise in this area. 
- The focus of this paper is on improving the robustness of detection and tracking pipelines to sensor failures by creating a realistic synthetic data augmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. 
- The goal is to accurately simulate sensor failures and data deterioration due to real-world interferences.
- The authors present the results of a baseline lightweight Noise Recognition neural network trained and tested on the augmented dataset, achieving an overall recognition accuracy of 54.4% on 11 categories across 10086 images and 2145 radar point-clouds. <br /> <div>
arXiv:2505.00584v1 Announce Type: new 
Abstract: Detecting and tracking objects is a crucial component of any autonomous navigation method. For the past decades, object detection has yielded promising results using neural networks on various datasets. While many methods focus on performance metrics, few projects focus on improving the robustness of these detection and tracking pipelines, notably to sensor failures. In this paper we attempt to address this issue by creating a realistic synthetic data augmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Our goal is to accurately simulate sensor failures and data deterioration due to real-world interferences. We also present our results of a baseline lightweight Noise Recognition neural network trained and tested on our augmented dataset, reaching an overall recognition accuracy of 54.4\% on 11 categories across 10086 images and 2145 radar point-clouds.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading</title>
<link>https://arxiv.org/abs/2505.00592</link>
<guid>https://arxiv.org/abs/2505.00592</guid>
<content:encoded><![CDATA[
<div> transfer learning, disease image grading, uncertainty-aware, multi-experts, knowledge distillation

Summary:
The article introduces a novel Uncertainty-aware Multi-experts Knowledge Distillation (UMKD) framework for automatic disease image grading. The framework addresses the challenges posed by domain shifts and data imbalance, which introduce bias into the model and hinder deployment in clinical settings. UMKD leverages multiple expert models to transfer knowledge to a single student model, decoupling task-agnostic and task-specific features for more accurate grading. An uncertainty-aware decoupled distillation mechanism dynamically adjusts knowledge transfer weights based on expert model uncertainties, ensuring robust distillation. UMKD also handles model architecture heterogeneity and distribution discrepancies between source and target domains. Experimental results on histology prostate grading and fundus image grading datasets show that UMKD achieves a new state-of-the-art performance in both source-imbalanced and target-imbalanced scenarios, offering a reliable and practical solution for disease image grading. 

<br /><br />Summary: <div>
arXiv:2505.00592v1 Announce Type: new 
Abstract: Automatic disease image grading is a significant application of artificial intelligence for healthcare, enabling faster and more accurate patient assessments. However, domain shifts, which are exacerbated by data imbalance, introduce bias into the model, posing deployment difficulties in clinical applications. To address the problem, we propose a novel \textbf{U}ncertainty-aware \textbf{M}ulti-experts \textbf{K}nowledge \textbf{D}istillation (UMKD) framework to transfer knowledge from multiple expert models to a single student model. Specifically, to extract discriminative features, UMKD decouples task-agnostic and task-specific features with shallow and compact feature alignment in the feature space. At the output space, an uncertainty-aware decoupled distillation (UDD) mechanism dynamically adjusts knowledge transfer weights based on expert model uncertainties, ensuring robust and reliable distillation. Additionally, UMKD also tackles the problems of model architecture heterogeneity and distribution discrepancies between source and target domains, which are inadequately tackled by previous KD approaches. Extensive experiments on histology prostate grading (\textit{SICAPv2}) and fundus image grading (\textit{APTOS}) demonstrate that UMKD achieves a new state-of-the-art in both source-imbalanced and target-imbalanced scenarios, offering a robust and practical solution for real-world disease image grading.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Trajectory Prediction of Vessels for Inland Navigation</title>
<link>https://arxiv.org/abs/2505.00599</link>
<guid>https://arxiv.org/abs/2505.00599</guid>
<content:encoded><![CDATA[
<div> object detection, vessel tracking, trajectory prediction, Kalman filter, inland navigation

Summary:
The study focuses on improving vessel trajectory prediction for autonomous systems in inland navigation. By integrating advanced object detection methods, Kalman filters, and spline-based interpolation, accurate vessel tracking is achieved despite challenging surroundings that often lead to misclassifications. Comparative evaluations of tracking algorithms such as BoT-SORT, Deep OC-SORT, and ByeTrack highlight the effectiveness of the Kalman filter in smoothing trajectories. Experimental results from various scenarios demonstrate enhanced accuracy in predicting vessel movements, crucial for collision avoidance and situational awareness. The necessity of customized datasets and models specific to inland navigation is emphasized. Future research will involve expanding datasets, incorporating vessel classification, and further refining predictions to support both autonomous systems and human operators in complex environments. 

<br /><br />Summary: <div>
arXiv:2505.00599v1 Announce Type: new 
Abstract: The future of inland navigation increasingly relies on autonomous systems and remote operations, emphasizing the need for accurate vessel trajectory prediction. This study addresses the challenges of video-based vessel tracking and prediction by integrating advanced object detection methods, Kalman filters, and spline-based interpolation. However, existing detection systems often misclassify objects in inland waterways due to complex surroundings. A comparative evaluation of tracking algorithms, including BoT-SORT, Deep OC-SORT, and ByeTrack, highlights the robustness of the Kalman filter in providing smoothed trajectories. Experimental results from diverse scenarios demonstrate improved accuracy in predicting vessel movements, which is essential for collision avoidance and situational awareness. The findings underline the necessity of customized datasets and models for inland navigation. Future work will expand the datasets and incorporate vessel classification to refine predictions, supporting both autonomous systems and human operators in complex environments.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dietary Intake Estimation via Continuous 3D Reconstruction of Food</title>
<link>https://arxiv.org/abs/2505.00606</link>
<guid>https://arxiv.org/abs/2505.00606</guid>
<content:encoded><![CDATA[
<div> Keywords: dietary habits, monitoring, 3D food models, pose estimation algorithms, automated state recognition 

Summary: 
This study presents a novel approach for accurately monitoring dietary habits by using 3D food models generated from monocular 2D video. By leveraging COLMAP and pose estimation algorithms, detailed 3D representations of food can be created to observe changes in food volume during consumption. Experiments with both toy models and real food items have shown promising results, demonstrating the potential of this approach. Additionally, a new methodology for automated state recognition challenges has been proposed to accurately detect state changes and maintain model fidelity. The use of 3D reconstruction in dietary monitoring shows great promise in providing comprehensive insights into eating behaviors, which can contribute to the development of more automated and accurate dietary monitoring tools. 

<br /><br /> <div>
arXiv:2505.00606v1 Announce Type: new 
Abstract: Monitoring dietary habits is crucial for preventing health risks associated with overeating and undereating, including obesity, diabetes, and cardiovascular diseases. Traditional methods for tracking food intake rely on self-reported data before or after the eating, which are prone to inaccuracies. This study proposes an approach to accurately monitor ingest behaviours by leveraging 3D food models constructed from monocular 2D video. Using COLMAP and pose estimation algorithms, we generate detailed 3D representations of food, allowing us to observe changes in food volume as it is consumed. Experiments with toy models and real food items demonstrate the approach's potential. Meanwhile, we have proposed a new methodology for automated state recognition challenges to accurately detect state changes and maintain model fidelity. The 3D reconstruction approach shows promise in capturing comprehensive dietary behaviour insights, ultimately contributing to the development of automated and accurate dietary monitoring tools.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction</title>
<link>https://arxiv.org/abs/2505.00615</link>
<guid>https://arxiv.org/abs/2505.00615</guid>
<content:encoded><![CDATA[
<div> Transformers, 3D reconstruction, facial geometry, FLAME mesh topology, benchmark

Summary:
Pixel3DMM is proposed to reconstruct 3D human faces from a single RGB image using vision transformers. It leverages the DINO foundation model's latent features and introduces tailored prediction heads for surface normal and uv-coordinates. The model is trained on three high-quality 3D face datasets, leading to over 1,000 identities and 976K images registered against the FLAME mesh topology. A FLAME fitting optimization is used for 3D face reconstruction from uv-coordinate and normal estimates. A new benchmark evaluates single-image face reconstruction, encompassing diverse expressions, viewing angles, and ethnicities, including posed and neutral facial geometry. Compared to competitive baselines, Pixel3DMM achieves over 15% higher geometric accuracy for posed facial expressions.

<br /><br />Summary: <div>
arXiv:2505.00615v1 Announce Type: new 
Abstract: We address the 3D reconstruction of human faces from a single RGB image. To this end, we propose Pixel3DMM, a set of highly-generalized vision transformers which predict per-pixel geometric cues in order to constrain the optimization of a 3D morphable face model (3DMM). We exploit the latent features of the DINO foundation model, and introduce a tailored surface normal and uv-coordinate prediction head. We train our model by registering three high-quality 3D face datasets against the FLAME mesh topology, which results in a total of over 1,000 identities and 976K images. For 3D face reconstruction, we propose a FLAME fitting opitmization that solves for the 3DMM parameters from the uv-coordinate and normal estimates. To evaluate our method, we introduce a new benchmark for single-image face reconstruction, which features high diversity facial expressions, viewing angles, and ethnicities. Crucially, our benchmark is the first to evaluate both posed and neutral facial geometry. Ultimately, our method outperforms the most competitive baselines by over 15% in terms of geometric accuracy for posed facial expressions.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse Semantics-Guided Feature Alignment and Decoupling for Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2505.00619</link>
<guid>https://arxiv.org/abs/2505.00619</guid>
<content:encoded><![CDATA[
<div> Keywords: VI-ReID, feature alignment, feature decoupling, pedestrian description, style noise

Summary:
The research addresses challenges in Visible-Infrared Person Re-Identification (VI-ReID) by proposing a novel Diverse Semantics-guided Feature Alignment and Decoupling (DSFAD) network. This network aligns identity-relevant features from different modalities into a textual embedding space and disentangles identity-irrelevant features within each modality. The approach includes a Diverse Semantics-guided Feature Alignment (DSFA) module that generates pedestrian descriptions with diverse sentence structures to guide feature alignment. A Semantic Margin-guided Feature Decoupling (SMFD) module filters out style information by decomposing visual features and constraining similarity between different components. A Semantic Consistency-guided Feature Restitution (SCFR) module ensures the preservation of pedestrian semantics during feature decoupling. Experimental results on three VI-ReID datasets demonstrate the effectiveness of the proposed DSFAD approach. <br /><br />Summary: <div>
arXiv:2505.00619v1 Announce Type: new 
Abstract: Visible-Infrared Person Re-Identification (VI-ReID) is a challenging task due to the large modality discrepancy between visible and infrared images, which complicates the alignment of their features into a suitable common space. Moreover, style noise, such as illumination and color contrast, reduces the identity discriminability and modality invariance of features. To address these challenges, we propose a novel Diverse Semantics-guided Feature Alignment and Decoupling (DSFAD) network to align identity-relevant features from different modalities into a textual embedding space and disentangle identity-irrelevant features within each modality. Specifically, we develop a Diverse Semantics-guided Feature Alignment (DSFA) module, which generates pedestrian descriptions with diverse sentence structures to guide the cross-modality alignment of visual features. Furthermore, to filter out style information, we propose a Semantic Margin-guided Feature Decoupling (SMFD) module, which decomposes visual features into pedestrian-related and style-related components, and then constrains the similarity between the former and the textual embeddings to be at least a margin higher than that between the latter and the textual embeddings. Additionally, to prevent the loss of pedestrian semantics during feature decoupling, we design a Semantic Consistency-guided Feature Restitution (SCFR) module, which further excavates useful information for identification from the style-related features and restores it back into the pedestrian-related features, and then constrains the similarity between the features after restitution and the textual embeddings to be consistent with that between the features before decoupling and the textual embeddings. Extensive experiments on three VI-ReID datasets demonstrate the superiority of our DSFAD.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain Foundation Models with Hypergraph Dynamic Adapter for Brain Disease Analysis</title>
<link>https://arxiv.org/abs/2505.00627</link>
<guid>https://arxiv.org/abs/2505.00627</guid>
<content:encoded><![CDATA[
<div> brain diseases, Alzheimer's disease, brain tumors, brain foundation models, SAM-Brain3D<br />
<br />Summary: 
The article introduces SAM-Brain3D, a brain-specific foundation model trained on a large dataset of brain image-label pairs across various MRI sub-modalities. It also introduces HyDA, a lightweight adapter for efficient downstream adaptation. SAM-Brain3D aims to capture detailed brain-specific anatomical and modality priors for segmenting diverse brain targets and broader tasks. HyDA utilizes hypergraphs to fuse multi-modal data and generate personalized convolutional kernels for feature fusion and patient-wise adaptation. The framework excels in brain disease segmentation and classification tasks, outperforming existing approaches. The study highlights the potential of multi-modal, multi-scale, and dynamic foundation modeling in advancing brain disease analysis. <div>
arXiv:2505.00627v1 Announce Type: new 
Abstract: Brain diseases, such as Alzheimer's disease and brain tumors, present profound challenges due to their complexity and societal impact. Recent advancements in brain foundation models have shown significant promise in addressing a range of brain-related tasks. However, current brain foundation models are limited by task and data homogeneity, restricted generalization beyond segmentation or classification, and inefficient adaptation to diverse clinical tasks. In this work, we propose SAM-Brain3D, a brain-specific foundation model trained on over 66,000 brain image-label pairs across 14 MRI sub-modalities, and Hypergraph Dynamic Adapter (HyDA), a lightweight adapter for efficient and effective downstream adaptation. SAM-Brain3D captures detailed brain-specific anatomical and modality priors for segmenting diverse brain targets and broader downstream tasks. HyDA leverages hypergraphs to fuse complementary multi-modal data and dynamically generate patient-specific convolutional kernels for multi-scale feature fusion and personalized patient-wise adaptation. Together, our framework excels across a broad spectrum of brain disease segmentation and classification tasks. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art approaches, offering a new paradigm for brain disease analysis through multi-modal, multi-scale, and dynamic foundation modeling.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook</title>
<link>https://arxiv.org/abs/2505.00630</link>
<guid>https://arxiv.org/abs/2505.00630</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, Remote sensing, State Space Models, Mamba architecture, Survey

Summary:
State Space Models (SSMs), specifically the Mamba architecture, offer a promising solution for addressing limitations in current deep learning approaches for remote sensing. This survey reviews over 120 studies to establish a comprehensive taxonomy of Mamba-based methodologies. It covers foundational principles of vision Mamba architectures, micro-architectural advancements like adaptive scan strategies, and macro-architectural integrations such as CNN-Transformer-Mamba hybrids. The survey also includes rigorous benchmarking of Mamba methods against state-of-the-art techniques across various remote sensing tasks. The paper also highlights unresolved challenges and provides actionable future directions. By bridging the gap between SSM theory and remote sensing applications, the survey positions Mamba as a transformative framework for remote sensing analysis. An open-source repository is curated to encourage community-driven advancements in this field.

<br /><br />Summary: <div>
arXiv:2505.00630v1 Announce Type: new 
Abstract: Deep learning has profoundly transformed remote sensing, yet prevailing architectures like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) remain constrained by critical trade-offs: CNNs suffer from limited receptive fields, while ViTs grapple with quadratic computational complexity, hindering their scalability for high-resolution remote sensing data. State Space Models (SSMs), particularly the recently proposed Mamba architecture, have emerged as a paradigm-shifting solution, combining linear computational scaling with global context modeling. This survey presents a comprehensive review of Mamba-based methodologies in remote sensing, systematically analyzing about 120 studies to construct a holistic taxonomy of innovations and applications. Our contributions are structured across five dimensions: (i) foundational principles of vision Mamba architectures, (ii) micro-architectural advancements such as adaptive scan strategies and hybrid SSM formulations, (iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids and frequency-domain adaptations, (iv) rigorous benchmarking against state-of-the-art methods in multiple application tasks, such as object detection, semantic segmentation, change detection, etc. and (v) critical analysis of unresolved challenges with actionable future directions. By bridging the gap between SSM theory and remote sensing practice, this survey establishes Mamba as a transformative framework for remote sensing analysis. To our knowledge, this paper is the first systematic review of Mamba architectures in remote sensing. Our work provides a structured foundation for advancing research in remote sensing systems through SSM-based methods. We curate an open-source repository (https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster community-driven advancements.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments</title>
<link>https://arxiv.org/abs/2505.00668</link>
<guid>https://arxiv.org/abs/2505.00668</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, air purification, air quality index, Delhi, spatial optimization
<br />
Summary: 
This study introduces a novel deep reinforcement learning (DRL) framework to optimize the placement of air purification booths in Delhi, a highly polluted city. By utilizing Proximal Policy Optimization (PPO), the framework identifies key locations for air purification infrastructure based on various factors such as population density, traffic patterns, and industrial influence. Compared to traditional placement strategies, the DRL approach demonstrates superior performance in improving the air quality index (AQI) while ensuring wide spatial coverage and equitable environmental benefits. The results highlight the potential of AI-driven spatial optimization in enhancing urban air quality management and advancing smart city initiatives. <div>
arXiv:2505.00668v1 Announce Type: new 
Abstract: Urban air pollution remains a pressing global concern, particularly in densely populated and traffic-intensive metropolitan areas like Delhi, where exposure to harmful pollutants severely impacts public health. Delhi, being one of the most polluted cities globally, experiences chronic air quality issues due to vehicular emissions, industrial activities, and construction dust, which exacerbate its already fragile atmospheric conditions. Traditional pollution mitigation strategies, such as static air purifying installations, often fail to maximize their impact due to suboptimal placement and limited adaptability to dynamic urban environments. This study presents a novel deep reinforcement learning (DRL) framework to optimize the placement of air purification booths to improve the air quality index (AQI) in the city of Delhi. We employ Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm, to iteratively learn and identify high-impact locations based on multiple spatial and environmental factors, including population density, traffic patterns, industrial influence, and green space constraints. Our approach is benchmarked against conventional placement strategies, including random and greedy AQI-based methods, using multi-dimensional performance evaluation metrics such as AQI improvement, spatial coverage, population and traffic impact, and spatial entropy. Experimental results demonstrate that the RL-based approach outperforms baseline methods by achieving a balanced and effective distribution of air purification infrastructure. Notably, the DRL framework achieves an optimal trade-off between AQI reduction and high-coverage deployment, ensuring equitable environmental benefits across urban regions. The findings underscore the potential of AI-driven spatial optimization in advancing smart city initiatives and data-driven urban air quality management.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Test-time Scaling for GUI Agent Grounding</title>
<link>https://arxiv.org/abs/2505.00684</link>
<guid>https://arxiv.org/abs/2505.00684</guid>
<content:encoded><![CDATA[
<div> Keywords: RegionFocus, visual test-time scaling, vision language model agents, grounding accuracy, interactive settings

Summary:
RegionFocus is a new visual test-time scaling approach for Vision Language Model Agents, aiming to improve grounding accuracy in understanding webpages by dynamically zooming in on relevant regions. The approach includes an image-as-map mechanism to visualize key landmarks, aiding in action selection. Significant performance gains of 28% on Screenspot-pro and 24% on WebVoyager benchmarks were observed when applying RegionFocus to state-of-the-art vision language model agents UI-TARS and Qwen2.5-VL. By utilizing RegionFocus on a Qwen2.5-VL-72B model, a new state-of-the-art grounding performance of 61.6% was achieved on the ScreenSpot-Pro benchmark. The code for RegionFocus will be publicly released on GitHub at https://github.com/tiangeluo/RegionFocus.

<br /><br />Summary: <div>
arXiv:2505.00684v1 Announce Type: new 
Abstract: We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+\% on Screenspot-pro and 24+\% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6\% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous Micromobility through Scalable Urban Simulation</title>
<link>https://arxiv.org/abs/2505.00690</link>
<guid>https://arxiv.org/abs/2505.00690</guid>
<content:encoded><![CDATA[
<div> autonomous micromobility, urban simulation, AI agents, robot learning, urban scenes <br />
<br />Summary: 
The study focuses on advancing autonomous micromobility through the use of AI agents in urban environments. A scalable urban simulation platform called URBAN-SIM is introduced to train embodied agents in realistic urban settings. This platform includes modules for urban generation, dynamics generation, and scene sampling to enhance robot learning. Additionally, a benchmarking suite called URBAN-BENCH is proposed to assess the agents' abilities in urban locomotion, navigation, and traversal. The study evaluates four robots with different embodiments across various tasks to identify their strengths and limitations in different terrains and urban structures. The research demonstrates the potential of AI-assisted micromobility to improve safety and efficiency in navigating urban spaces. <div>
arXiv:2505.00690v1 Announce Type: new 
Abstract: Micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot's strengths and limitations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RayZer: A Self-supervised Large View Synthesis Model</title>
<link>https://arxiv.org/abs/2505.00702</link>
<guid>https://arxiv.org/abs/2505.00702</guid>
<content:encoded><![CDATA[
<div> self-supervised, multi-view, 3D Vision, RayZer, novel view synthesis  
Summary:<br />RayZer is a self-supervised multi-view 3D Vision model that trains without using any 3D supervision, such as camera poses and scene geometry. It takes uncalibrated images as input, predicts camera parameters, reconstructs scenes, and synthesizes new views using self-predicted camera poses. Its 3D awareness emerges from a disentangling framework for camera and scene representations and a transformer-based model built on ray structures. RayZer outperforms methods relying on pose annotations in novel view synthesis, showcasing its effectiveness in training with 2D image supervision. The project website for RayZer provides additional information and resources for further exploration.<br /><br />Summary: <div>
arXiv:2505.00702v1 Announce Type: new 
Abstract: We present RayZer, a self-supervised multi-view 3D Vision model trained without any 3D supervision, i.e., camera poses and scene geometry, while exhibiting emerging 3D awareness. Concretely, RayZer takes unposed and uncalibrated images as input, recovers camera parameters, reconstructs a scene representation, and synthesizes novel views. During training, RayZer relies solely on its self-predicted camera poses to render target views, eliminating the need for any ground-truth camera annotations and allowing RayZer to be trained with 2D image supervision. The emerging 3D awareness of RayZer is attributed to two key factors. First, we design a self-supervised framework, which achieves 3D-aware auto-encoding of input images by disentangling camera and scene representations. Second, we design a transformer-based model in which the only 3D prior is the ray structure, connecting camera, pixel, and scene simultaneously. RayZer demonstrates comparable or even superior novel view synthesis performance than ``oracle'' methods that rely on pose annotations in both training and testing. Project: https://hwjiang1510.github.io/RayZer/
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT</title>
<link>https://arxiv.org/abs/2505.00703</link>
<guid>https://arxiv.org/abs/2505.00703</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, text-to-image generation, chain-of-thought reasoning, reinforcement learning, BiCoT-GRPO.

Summary:<br />
- The paper introduces T2I-R1, a novel text-to-image generation model that incorporates chain-of-thought reasoning and reinforcement learning to improve performance.
- Two levels of chain-of-thought reasoning are identified: semantic-level CoT for high-level planning and token-level CoT for low-level pixel processing.
- The model, BiCoT-GRPO, optimizes both levels of CoT with an ensemble of generation rewards during training.
- By applying these reasoning strategies to the baseline model, Janus-Pro, T2I-R1 outperforms it with a 13% improvement on T2I-CompBench and a 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1.
- The code for T2I-R1 is publicly available on GitHub for further research and implementation. 

Summary: <br />
Recent advancements in large language models have led to the development of T2I-R1, a text-to-image generation model that utilizes chain-of-thought reasoning and reinforcement learning. By incorporating two levels of reasoning, semantic-level CoT for high-level planning and token-level CoT for pixel processing, along with the BiCoT-GRPO optimization method, T2I-R1 surpasses the state-of-the-art model FLUX.1. The model demonstrates a 13% improvement on T2I-CompBench and a 19% improvement on the WISE benchmark when compared to the baseline model Janus-Pro. The availability of the code on GitHub promotes further exploration and application of these innovative reasoning strategies in the visual generation domain. <div>
arXiv:2505.00703v1 Announce Type: new 
Abstract: Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning</title>
<link>https://arxiv.org/abs/2504.21707</link>
<guid>https://arxiv.org/abs/2504.21707</guid>
<content:encoded><![CDATA[
<div> Keywords: representation learning, divergence alignment, recursive optimization, contrastive clustering, dimensionality reduction

Summary:
RKDO proposes a new framework for representation learning that reframes modern objectives as recursive divergence alignment processes. By emphasizing the dynamic evolution of KL divergences across localized conditional distributions, RKDO offers a more efficient optimization landscape for learning representations. Experimental results show that RKDO outperforms static approaches, achieving approximately 30% lower loss values on three datasets and reducing computational resources needed by 60 to 80%. This dynamic formalism captures contrastive clustering and dimensionality reduction methods as static cases while also providing stability and local adaptation in the learning process. The recursive updating mechanism of RKDO introduces significant implications for resource-constrained applications, making it a promising approach for various representation learning tasks. 

<br /><br />Summary: <div>
arXiv:2504.21707v1 Announce Type: cross 
Abstract: We propose a generalization of modern representation learning objectives by reframing them as recursive divergence alignment processes over localized conditional distributions While recent frameworks like Information Contrastive Learning I-Con unify multiple learning paradigms through KL divergence between fixed neighborhood conditionals we argue this view underplays a crucial recursive structure inherent in the learning process. We introduce Recursive KL Divergence Optimization RKDO a dynamic formalism where representation learning is framed as the evolution of KL divergences across data neighborhoods. This formulation captures contrastive clustering and dimensionality reduction methods as static slices while offering a new path to model stability and local adaptation. Our experiments demonstrate that RKDO offers dual efficiency advantages approximately 30 percent lower loss values compared to static approaches across three different datasets and 60 to 80 percent reduction in computational resources needed to achieve comparable results. This suggests that RKDOs recursive updating mechanism provides a fundamentally more efficient optimization landscape for representation learning with significant implications for resource constrained applications.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SR-NeRV: Improving Embedding Efficiency of Neural Video Representation via Super-Resolution</title>
<link>https://arxiv.org/abs/2505.00046</link>
<guid>https://arxiv.org/abs/2505.00046</guid>
<content:encoded><![CDATA[
<div> Implicit Neural Representations (INRs), neural video compression, super-resolution network, reconstruction quality, model size<br />
Summary: <br />
Implicit Neural Representations (INRs) are gaining attention for modeling complex signals like in neural video compression. A novel approach integrating a super-resolution (SR) network with INRs is proposed to address the challenge of reconstructing high-frequency details in compressed videos efficiently. This method leverages the low temporal redundancy of high-frequency components across frames to delegate fine detail reconstruction to the SR network. Experimental results show that the proposed approach surpasses traditional INR-based methods in reconstruction quality while maintaining similar model sizes. This integration of SR network with INRs enhances the reconstruction capability of compressed videos, showcasing its potential for practical applications in video compression. <div>
arXiv:2505.00046v1 Announce Type: cross 
Abstract: Implicit Neural Representations (INRs) have garnered significant attention for their ability to model complex signals across a variety of domains. Recently, INR-based approaches have emerged as promising frameworks for neural video compression. While conventional methods primarily focus on embedding video content into compact neural networks for efficient representation, they often struggle to reconstruct high-frequency details under stringent model size constraints, which are critical in practical compression scenarios. To address this limitation, we propose an INR-based video representation method that integrates a general-purpose super-resolution (SR) network. Motivated by the observation that high-frequency components exhibit low temporal redundancy across frames, our method entrusts the reconstruction of fine details to the SR network. Experimental results demonstrate that the proposed method outperforms conventional INR-based baselines in terms of reconstruction quality, while maintaining comparable model sizes.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling</title>
<link>https://arxiv.org/abs/2505.00063</link>
<guid>https://arxiv.org/abs/2505.00063</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, General Document Intelligence Benchmark, document-specific tasks, visual capabilities, intelligence-preserving training strategy

Summary: 
The article introduces the need for a comprehensive benchmark, the General Document Intelligence Benchmark (GDI-Bench), to evaluate the capabilities of multimodal large language models (MLLMs) in various document-specific tasks. The GDI-Bench includes 1.9k images across 9 key scenarios and 19 tasks, allowing performance assessment by difficulty. The evaluation on open-source and closed-source models reveals strengths and weaknesses, such as GPT-4o excelling in reasoning but lacking in visual capabilities. To address diverse tasks, the GDI Model is proposed to prevent catastrophic forgetting during supervised fine-tuning, achieving state-of-the-art performance. Both the benchmark and model will be open source, aiding in model optimization and weakness identification. <br /><br />Summary: <div>
arXiv:2505.00063v1 Announce Type: cross 
Abstract: The rapid advancement of multimodal large language models (MLLMs) has profoundly impacted the document domain, creating a wide array of application scenarios. This progress highlights the need for a comprehensive benchmark to evaluate these models' capabilities across various document-specific tasks. However, existing benchmarks often fail to locate specific model weaknesses or guide systematic improvements. To bridge this gap, we introduce a General Document Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key scenarios and 19 document-specific tasks. By decoupling visual complexity and reasoning complexity, the GDI-Bench structures graded tasks that allow performance assessment by difficulty, aiding in model weakness identification and optimization guidance. We evaluate the GDI-Bench on various open-source and closed-source models, conducting decoupled analyses in the visual and reasoning domains. For instance, the GPT-4o model excels in reasoning tasks but exhibits limitations in visual capabilities. To address the diverse tasks and domains in the GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic forgetting during the supervised fine-tuning (SFT) process through a intelligence-preserving training strategy. Our model achieves state-of-the-art performance on previous benchmarks and the GDI-Bench. Both our benchmark and model will be open source.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rootlets-based registration to the spinal cord PAM50 template</title>
<link>https://arxiv.org/abs/2505.00115</link>
<guid>https://arxiv.org/abs/2505.00115</guid>
<content:encoded><![CDATA[
<div> Keywords: spinal cord functional MRI, registration, spinal nerve rootlets, inter-subject alignment, group analysis <br />
<br />
Summary: 
This study introduces a new method for registering spinal cord functional MRI studies by utilizing spinal nerve rootlets for alignment, improving accuracy and reproducibility across individuals. The proposed registration approach was validated on a large multi-subject, multi-site dataset and a dataset with varied neck positions, demonstrating superior alignment compared to traditional disc-based methods. Task-based functional MRI analysis using rootlet-based registration showed increased Z scores and activation cluster size compared to disc-based registration, indicating improved group-level analysis outcomes. The method proved to be more stable across different neck positions, enhancing both inter- and intra-subject anatomical alignment. Overall, rootlet-based registration offers a promising solution for enhancing the precision and reliability of spinal cord neuroimaging group analysis. <br /> <div>
arXiv:2505.00115v1 Announce Type: cross 
Abstract: Spinal cord functional MRI studies require precise localization of spinal levels for reliable voxelwise group analyses. Traditional template-based registration of the spinal cord uses intervertebral discs for alignment. However, substantial anatomical variability across individuals exists between vertebral and spinal levels. This study proposes a novel registration approach that leverages spinal nerve rootlets to improve alignment accuracy and reproducibility across individuals. We developed a registration method leveraging dorsal cervical rootlets segmentation and aligning them non-linearly with the PAM50 spinal cord template. Validation was performed on a multi-subject, multi-site dataset (n=267, 44 sites) and a multi-subject dataset with various neck positions (n=10, 3 sessions). We further validated the method on task-based functional MRI (n=23) to compare group-level activation maps using rootlet-based registration to traditional disc-based methods. Rootlet-based registration showed superior alignment across individuals compared to the traditional disc-based method. Notably, rootlet positions were more stable across neck positions. Group-level analysis of task-based functional MRI using rootlet-based increased Z scores and activation cluster size compared to disc-based registration (number of active voxels from 3292 to 7978). Rootlet-based registration enhances both inter- and intra-subject anatomical alignment and yields better spatial normalization for group-level fMRI analyses. Our findings highlight the potential of rootlet-based registration to improve the precision and reliability of spinal cord neuroimaging group analysis.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and robust 3D blind harmonization for large domain gaps</title>
<link>https://arxiv.org/abs/2505.00133</link>
<guid>https://arxiv.org/abs/2505.00133</guid>
<content:encoded><![CDATA[
arXiv:2505.00133v1 Announce Type: cross 
Abstract: Blind harmonization has emerged as a promising technique for MR image harmonization to achieve scale-invariant representations, requiring only target domain data (i.e., no source domain data necessary). However, existing methods face limitations such as inter-slice heterogeneity in 3D, moderate image quality, and limited performance for a large domain gap. To address these challenges, we introduce BlindHarmonyDiff, a novel blind 3D harmonization framework that leverages an edge-to-image model tailored specifically to harmonization. Our framework employs a 3D rectified flow trained on target domain images to reconstruct the original image from an edge map, then yielding a harmonized image from the edge of a source domain image. We propose multi-stride patch training for efficient 3D training and a refinement module for robust inference by suppressing hallucination. Extensive experiments demonstrate that BlindHarmonyDiff outperforms prior arts by harmonizing diverse source domain images to the target domain, achieving higher correspondence to the target domain characteristics. Downstream task-based quality assessments such as tissue segmentation and age prediction on diverse MR scanners further confirm the effectiveness of our approach and demonstrate the capability of our robust and generalizable blind harmonization.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuroevolution of Self-Attention Over Proto-Objects</title>
<link>https://arxiv.org/abs/2505.00186</link>
<guid>https://arxiv.org/abs/2505.00186</guid>
<content:encoded><![CDATA[
arXiv:2505.00186v1 Announce Type: cross 
Abstract: Proto-objects - image regions that share common visual properties - offer a promising alternative to traditional attention mechanisms based on rectangular-shaped image patches in neural networks. Although previous work demonstrated that evolving a patch-based hard-attention module alongside a controller network could achieve state-of-the-art performance in visual reinforcement learning tasks, our approach leverages image segmentation to work with higher-level features. By operating on proto-objects rather than fixed patches, we significantly reduce the representational complexity: each image decomposes into fewer proto-objects than regular patches, and each proto-object can be efficiently encoded as a compact feature vector. This enables a substantially smaller self-attention module that processes richer semantic information. Our experiments demonstrate that this proto-object-based approach matches or exceeds the state-of-the-art performance of patch-based implementations with 62% less parameters and 2.6 times less training time.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2505.00337</link>
<guid>https://arxiv.org/abs/2505.00337</guid>
<content:encoded><![CDATA[
arXiv:2505.00337v1 Announce Type: cross 
Abstract: Text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. Yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. Existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. To fill this gap, we introduce \textbf{T2VPhysBench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including Newtonian mechanics, conservation principles, and phenomenological effects. Our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. The results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Lightweight Hyperspectral Image Super-Resolution with Depthwise Separable Dilated Convolutional Network</title>
<link>https://arxiv.org/abs/2505.00374</link>
<guid>https://arxiv.org/abs/2505.00374</guid>
<content:encoded><![CDATA[
arXiv:2505.00374v1 Announce Type: cross 
Abstract: Deep neural networks have demonstrated highly competitive performance in super-resolution (SR) for natural images by learning mappings from low-resolution (LR) to high-resolution (HR) images. However, hyperspectral super-resolution remains an ill-posed problem due to the high spectral dimensionality of the data and the scarcity of available training samples. Moreover, existing methods often rely on large models with a high number of parameters or require the fusion with panchromatic or RGB images, both of which are often impractical in real-world scenarios. Inspired by the MobileNet architecture, we introduce a lightweight depthwise separable dilated convolutional network (DSDCN) to address the aforementioned challenges. Specifically, our model leverages multiple depthwise separable convolutions, similar to the MobileNet architecture, and further incorporates a dilated convolution fusion block to make the model more flexible for the extraction of both spatial and spectral features. In addition, we propose a custom loss function that combines mean squared error (MSE), an L2 norm regularization-based constraint, and a spectral angle-based loss, ensuring the preservation of both spectral and spatial details. The proposed model achieves very competitive performance on two publicly available hyperspectral datasets, making it well-suited for hyperspectral image super-resolution tasks. The source codes are publicly available at: \href{https://github.com/Usman1021/lightweight}{https://github.com/Usman1021/lightweight}.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORSTITCH - A free, open source software for stitching and georeferencing underwater coral reef videos</title>
<link>https://arxiv.org/abs/2505.00462</link>
<guid>https://arxiv.org/abs/2505.00462</guid>
<content:encoded><![CDATA[
arXiv:2505.00462v1 Announce Type: cross 
Abstract: CorStitch is an open-source software developed to automate the creation of accurate georeferenced reef mosaics from video transects obtained through Automated Rapid Reef Assessment System surveys. We utilized a Fourier-based image correlation algorithm to stitch sequential video frames, aligning them with synchronized GNSS timestamps. The resulting compressed Keyhole Markup Language files, compatible with geographic information systems such as Google Earth, enable detailed spatial analysis. Validation through comparative analysis of mosaics from two temporally distinct surveys of the same reef demonstrated the software's consistent and reliable performance.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Methodological and Structural Review of Parkinsons Disease Detection Across Diverse Data Modalities</title>
<link>https://arxiv.org/abs/2505.00525</link>
<guid>https://arxiv.org/abs/2505.00525</guid>
<content:encoded><![CDATA[
arXiv:2505.00525v1 Announce Type: cross 
Abstract: Parkinsons Disease (PD) is a progressive neurological disorder that primarily affects motor functions and can lead to mild cognitive impairment (MCI) and dementia in its advanced stages. With approximately 10 million people diagnosed globally 1 to 1.8 per 1,000 individuals, according to reports by the Japan Times and the Parkinson Foundation early and accurate diagnosis of PD is crucial for improving patient outcomes. While numerous studies have utilized machine learning (ML) and deep learning (DL) techniques for PD recognition, existing surveys are limited in scope, often focusing on single data modalities and failing to capture the potential of multimodal approaches. To address these gaps, this study presents a comprehensive review of PD recognition systems across diverse data modalities, including Magnetic Resonance Imaging (MRI), gait-based pose analysis, gait sensory data, handwriting analysis, speech test data, Electroencephalography (EEG), and multimodal fusion techniques. Based on over 347 articles from leading scientific databases, this review examines key aspects such as data collection methods, settings, feature representations, and system performance, with a focus on recognition accuracy and robustness. This survey aims to serve as a comprehensive resource for researchers, providing actionable guidance for the development of next generation PD recognition systems. By leveraging diverse data modalities and cutting-edge machine learning paradigms, this work contributes to advancing the state of PD diagnostics and improving patient care through innovative, multimodal approaches.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Assisted Outer Volume Removal for Highly-Accelerated Real-Time Dynamic MRI</title>
<link>https://arxiv.org/abs/2505.00643</link>
<guid>https://arxiv.org/abs/2505.00643</guid>
<content:encoded><![CDATA[
arXiv:2505.00643v1 Announce Type: cross 
Abstract: Real-time (RT) dynamic MRI plays a vital role in capturing rapid physiological processes, offering unique insights into organ motion and function. Among these applications, RT cine MRI is particularly important for functional assessment of the heart with high temporal resolution. RT imaging enables free-breathing, ungated imaging of cardiac motion, making it a crucial alternative for patients who cannot tolerate conventional breath-hold, ECG-gated acquisitions. However, achieving high acceleration rates in RT cine MRI is challenging due to aliasing artifacts from extra-cardiac tissues, particularly at high undersampling factors. In this study, we propose a novel outer volume removal (OVR) method to address this challenge by eliminating aliasing contributions from non-cardiac regions in a post-processing framework. Our approach estimates the outer volume signal for each timeframe using composite temporal images from time-interleaved undersampling patterns, which inherently contain pseudo-periodic ghosting artifacts. A deep learning (DL) model is trained to identify and remove these artifacts, producing a clean outer volume estimate that is subsequently subtracted from the corresponding k-space data. The final reconstruction is performed with a physics-driven DL (PD-DL) method trained using an OVR-specific loss function to restore high spatio-temporal resolution images. Experimental results show that the proposed method at high accelerations achieves image quality that is visually comparable to clinical baseline images, while outperforming conventional reconstruction techniques, both qualitatively and quantitatively. The proposed approach provides a practical and effective solution for artifact reduction in RT cine MRI without requiring acquisition modifications, offering a pathway to higher acceleration rates while preserving diagnostic quality.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MINERVA: Evaluating Complex Video Reasoning</title>
<link>https://arxiv.org/abs/2505.00681</link>
<guid>https://arxiv.org/abs/2505.00681</guid>
<content:encoded><![CDATA[
arXiv:2505.00681v1 Announce Type: cross 
Abstract: Multimodal LLMs are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. This makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. To remedy this, we provide a new video reasoning dataset called MINERVA for modern multimodal models. Each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. Our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. Extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. We perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We use this to explore both human and LLM-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. The dataset, along with questions, answer candidates and reasoning traces will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuideSR: Rethinking Guidance for One-Step High-Fidelity Diffusion-Based Super-Resolution</title>
<link>https://arxiv.org/abs/2505.00687</link>
<guid>https://arxiv.org/abs/2505.00687</guid>
<content:encoded><![CDATA[
arXiv:2505.00687v1 Announce Type: cross 
Abstract: In this paper, we propose GuideSR, a novel single-step diffusion-based image super-resolution (SR) model specifically designed to enhance image fidelity. Existing diffusion-based SR approaches typically adapt pre-trained generative models to image restoration tasks by adding extra conditioning on a VAE-downsampled representation of the degraded input, which often compromises structural fidelity. GuideSR addresses this limitation by introducing a dual-branch architecture comprising: (1) a Guidance Branch that preserves high-fidelity structures from the original-resolution degraded input, and (2) a Diffusion Branch, which a pre-trained latent diffusion model to enhance perceptual quality. Unlike conventional conditioning mechanisms, our Guidance Branch features a tailored structure for image restoration tasks, combining Full Resolution Blocks (FRBs) with channel attention and an Image Guidance Network (IGN) with guided attention. By embedding detailed structural information directly into the restoration pipeline, GuideSR produces sharper and more visually consistent results. Extensive experiments on benchmark datasets demonstrate that GuideSR achieves state-of-the-art performance while maintaining the low computational cost of single-step approaches, with up to 1.39dB PSNR gain on challenging real-world datasets. Our approach consistently outperforms existing methods across various reference-based metrics including PSNR, SSIM, LPIPS, DISTS and FID, further representing a practical advancement for real-world image restoration.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robotic Visual Instruction</title>
<link>https://arxiv.org/abs/2505.00693</link>
<guid>https://arxiv.org/abs/2505.00693</guid>
<content:encoded><![CDATA[
arXiv:2505.00693v1 Announce Type: cross 
Abstract: Recently, natural language has been the primary medium for human-robot interaction. However, its inherent lack of spatial precision for robotic control introduces challenges such as ambiguity and verbosity. To address these limitations, we introduce the Robotic Visual Instruction (RoVI), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. RoVI effectively encodes spatial-temporal information into human-interpretable visual instructions through 2D sketches, utilizing arrows, circles, colors, and numbers to direct 3D robotic manipulation. To enable robots to understand RoVI better and generate precise actions based on RoVI, we present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for RoVI-conditioned policies. This approach leverages Vision-Language Models (VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from 2D pixel space via keypoint extraction, and then transform them into executable 3D action sequences. We additionally curate a specialized dataset of 15K instances to fine-tune small VLMs for edge deployment, enabling them to effectively learn RoVI capabilities. Our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. Notably, VIEW achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. Code and Datasets in this paper will be released soon.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Weather Synthesis and Removal with Video Diffusion Models</title>
<link>https://arxiv.org/abs/2505.00704</link>
<guid>https://arxiv.org/abs/2505.00704</guid>
<content:encoded><![CDATA[
arXiv:2505.00704v1 Announce Type: cross 
Abstract: Generating realistic and controllable weather effects in videos is valuable for many applications. Physics-based weather simulation requires precise reconstructions that are hard to scale to in-the-wild videos, while current video editing often lacks realism and control. In this work, we introduce WeatherWeaver, a video diffusion model that synthesizes diverse weather effects -- including rain, snow, fog, and clouds -- directly into any input video without the need for 3D modeling. Our model provides precise control over weather effect intensity and supports blending various weather types, ensuring both realism and adaptability. To overcome the scarcity of paired training data, we propose a novel data strategy combining synthetic videos, generative image editing, and auto-labeled real-world videos. Extensive evaluations show that our method outperforms state-of-the-art methods in weather simulation and removal, providing high-quality, physically plausible, and scene-identity-preserving results over various real-world videos.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretability-Aware Vision Transformer</title>
<link>https://arxiv.org/abs/2309.08035</link>
<guid>https://arxiv.org/abs/2309.08035</guid>
<content:encoded><![CDATA[
arXiv:2309.08035v2 Announce Type: replace 
Abstract: Vision Transformers (ViTs) have become prominent models for solving various vision tasks. However, the interpretability of ViTs has not kept pace with their promising performance. While there has been a surge of interest in developing {\it post hoc} solutions to explain ViTs' outputs, these methods do not generalize to different downstream tasks and various transformer architectures. Furthermore, if ViTs are not properly trained with the given data and do not prioritize the region of interest, the {\it post hoc} methods would be less effective. Instead of developing another {\it post hoc} approach, we introduce a novel training procedure that inherently enhances model interpretability. Our interpretability-aware ViT (IA-ViT) draws inspiration from a fresh insight: both the class patch and image patches consistently generate predicted distributions and attention maps. IA-ViT is composed of a feature extractor, a predictor, and an interpreter, which are trained jointly with an interpretability-aware training objective. Consequently, the interpreter simulates the behavior of the predictor and provides a faithful explanation through its single-head self-attention mechanism. Our comprehensive experimental results demonstrate the effectiveness of IA-ViT in several image classification tasks, with both qualitative and quantitative evaluations of model performance and interpretability. Source code is available from: https://github.com/qiangyao1988/IA-ViT.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangle Before Anonymize: A Two-stage Framework for Attribute-preserved and Occlusion-robust De-identification</title>
<link>https://arxiv.org/abs/2311.08786</link>
<guid>https://arxiv.org/abs/2311.08786</guid>
<content:encoded><![CDATA[
arXiv:2311.08786v2 Announce Type: replace 
Abstract: In an era where personal photos are easily leaked and collected, face de-identification is a crucial method for protecting identity privacy. However, current face de-identification techniques face challenges in preserving attribute details and often produce anonymized results with reduced authenticity. These shortcomings are particularly evident when handling occlusions,frequently resulting in noticeable editing artifacts. Our primary finding in this work is that simultaneous training of identity disentanglement and anonymization hinders their respective effectiveness.Therefore, we propose "Disentangle Before Anonymize",a novel two-stage Framework(DBAF)designed for attributepreserved and occlusion-robust de-identification. This framework includes a Contrastive Identity Disentanglement (CID) module and a Key-authorized Reversible Identity Anonymization (KRIA) module, achieving faithful attribute preservation and high-quality identity anonymization edits. Additionally, we introduce a Multiscale Attentional Attribute Retention (MAAR) module to address the issue of reduced anonymization quality under occlusions.Extensive experiments demonstrate that our method outperforms state-of-the-art de-identification approaches, delivering superior quality, enhanced detail fidelity, improved attribute preservation performance, and greater robustness to occlusions.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scene-Conditional 3D Object Stylization and Composition</title>
<link>https://arxiv.org/abs/2312.12419</link>
<guid>https://arxiv.org/abs/2312.12419</guid>
<content:encoded><![CDATA[
arXiv:2312.12419v2 Announce Type: replace 
Abstract: Recently, 3D generative models have made impressive progress, enabling the generation of almost arbitrary 3D assets from text or image inputs. However, these approaches generate objects in isolation without any consideration for the scene where they will eventually be placed. In this paper, we propose a framework that allows for the stylization of an existing 3D asset to fit into a given 2D scene, and additionally produce a photorealistic composition as if the asset was placed within the environment. This not only opens up a new level of control for object stylization, for example, the same assets can be stylized to reflect changes in the environment, such as summer to winter or fantasy versus futuristic settings-but also makes the object-scene composition more controllable. We achieve this by combining modeling and optimizing the object's texture and environmental lighting through differentiable ray tracing with image priors from pre-trained text-to-image diffusion models. We demonstrate that our method is applicable to a wide variety of indoor and outdoor scenes and arbitrary objects. Project page: https://jensenzhoujh.github.io/scene-cond-3d/.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latte: Latent Diffusion Transformer for Video Generation</title>
<link>https://arxiv.org/abs/2401.03048</link>
<guid>https://arxiv.org/abs/2401.03048</guid>
<content:encoded><![CDATA[
arXiv:2401.03048v3 Announce Type: replace 
Abstract: We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>F2M-Reg: Unsupervised RGB-D Point Cloud Registration with Frame-to-Model Optimization</title>
<link>https://arxiv.org/abs/2405.00507</link>
<guid>https://arxiv.org/abs/2405.00507</guid>
<content:encoded><![CDATA[
arXiv:2405.00507v3 Announce Type: replace 
Abstract: This work studies the problem of unsupervised RGB-D point cloud registration, which aims at training a robust registration model without ground-truth pose supervision. Existing methods usually leverages unposed RGB-D sequences and adopt a frame-to-frame framework based on differentiable rendering to train the registration model, which enforces the photometric and geometric consistency between the two frames for supervision. However, this frame-to-frame framework is vulnerable to inconsistent factors between different frames, e.g., lighting changes, geometry occlusion, and reflective materials, which leads to suboptimal convergence of the registration model. In this paper, we propose a novel frame-to-model optimization framework named F2M-Reg for unsupervised RGB-D point cloud registration. We leverage the neural implicit field as a global model of the scene and optimize the estimated poses of the frames by registering them to the global model, and the registration model is subsequently trained with the optimized poses. Thanks to the global encoding capability of neural implicit field, our frame-to-model framework is significantly more robust to inconsistent factors between different frames and thus can provide better supervision for the registration model. Besides, we demonstrate that F2M-Reg can be further enhanced by a simplistic synthetic warming-up strategy. To this end, we construct a photorealistic synthetic dataset named Sim-RGBD to initialize the registration model for the frame-to-model optimization on real-world RGB-D sequences. Extensive experiments on four challenging benchmarks have shown that our method surpasses the previous state-of-the-art counterparts by a large margin, especially under scenarios with severe lighting changes and low overlap. Our code and models are available at https://github.com/MrIsland/F2M_Reg.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S3Former: Self-supervised High-resolution Transformer for Solar PV Profiling</title>
<link>https://arxiv.org/abs/2405.04489</link>
<guid>https://arxiv.org/abs/2405.04489</guid>
<content:encoded><![CDATA[
arXiv:2405.04489v2 Announce Type: replace 
Abstract: As the impact of climate change escalates, the global necessity to transition to sustainable energy sources becomes increasingly evident. Renewable energies have emerged as a viable solution for users, with Photovoltaic energy being a favored choice for small installations due to its reliability and efficiency. Accurate mapping of PV installations is crucial for understanding the extension of its adoption and informing energy policy. To meet this need, we introduce S3Former, designed to segment solar panels from aerial imagery and provide size and location information critical for analyzing the impact of such installations on the grid. Solar panel identification is challenging due to factors such as varying weather conditions, roof characteristics, Ground Sampling Distance variations and lack of appropriate initialization weights for optimized training. To tackle these complexities, S3Former features a Masked Attention Mask Transformer incorporating a self-supervised learning pretrained backbone. Specifically, our model leverages low-level and high-level features extracted from the backbone and incorporates an instance query mechanism incorporated on the Transformer architecture to enhance the localization of solar PV installations. We introduce a self-supervised learning phase (pretext task) to improve the initialization weights on the backbone of S3Former. We evaluated S3Former using diverse datasets, demonstrate improvement state-of-the-art models.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State Estimation Drift Mitigation on LiDAR PointCloud</title>
<link>https://arxiv.org/abs/2405.11536</link>
<guid>https://arxiv.org/abs/2405.11536</guid>
<content:encoded><![CDATA[
arXiv:2405.11536v4 Announce Type: replace 
Abstract: This paper addresses limitations in 3D tracking-by-detection methods, particularly in identifying legitimate trajectories and reducing state estimation drift in Kalman filters. Existing methods often use threshold-based filtering for detection scores, which can fail for distant and occluded objects, leading to false positives. To tackle this, we propose a novel track validity mechanism and multi-stage observational gating process, significantly reducing ghost tracks and enhancing tracking performance. Our method achieves a $29.47\%$ improvement in Multi-Object Tracking Accuracy (MOTA) on the KITTI validation dataset with the Second detector. Additionally, a refined Kalman filter term reduces localization noise, improving higher-order tracking accuracy (HOTA) by $4.8\%$. The online framework, RobMOT, outperforms state-of-the-art methods across multiple detectors, with HOTA improvements of up to $3.92\%$ on the KITTI testing dataset and $8.7\%$ on the validation dataset, while achieving low identity switch scores. RobMOT excels in challenging scenarios, tracking distant objects and prolonged occlusions, with a $1.77\%$ MOTA improvement on the Waymo Open dataset, and operates at a remarkable 3221 FPS on a single CPU, proving its efficiency for real-time multi-object tracking.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D StreetUnveiler with Semantic-aware 2DGS -- a simple baseline</title>
<link>https://arxiv.org/abs/2405.18416</link>
<guid>https://arxiv.org/abs/2405.18416</guid>
<content:encoded><![CDATA[
arXiv:2405.18416v4 Announce Type: replace 
Abstract: Unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving. However, removing all temporarily static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge. Unlike object-centric 3D inpainting, which relies on thorough observation in a small scene, street scene cases involve long trajectories that differ from previous 3D inpainting tasks. The camera-centric moving environment of captured videos further complicates the task due to the limited degree and time duration of object observation. To address these obstacles, we introduce StreetUnveiler to reconstruct an empty street. StreetUnveiler learns a 3D representation of the empty street from crowded observations. Our representation is based on the hard-label semantic 2D Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians to be removed. We inpaint rendered image after removing unwanted Gaussians to provide pseudo-labels and subsequently re-optimize the 2DGS. Given its temporal continuous movement, we divide the empty street scene into observed, partial-observed, and unobserved regions, which we propose to locate through a rendered alpha map. This decomposition helps us to minimize the regions that need to be inpainted. To enhance the temporal consistency of the inpainting, we introduce a novel time-reversal framework to inpaint frames in reverse order and use later frames as references for earlier frames to fully utilize the long-trajectory observations. Our experiments conducted on the street scene dataset successfully reconstructed a 3D representation of the empty street. The mesh representation of the empty street can be extracted for further applications. The project page and more visualizations can be found at: https://streetunveiler.github.io
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion</title>
<link>https://arxiv.org/abs/2406.03184</link>
<guid>https://arxiv.org/abs/2406.03184</guid>
<content:encoded><![CDATA[
arXiv:2406.03184v2 Announce Type: replace 
Abstract: Existing single image-to-3D creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3D reconstruction. However, training these two stages separately leads to significant data bias in the inference phase, thus affecting the quality of reconstructed results. We introduce a unified 3D generation framework, named Ouroboros3D, which integrates diffusion-based multi-view image generation and 3D reconstruction into a recursive diffusion process. In our framework, these two modules are jointly trained through a self-conditioning mechanism, allowing them to adapt to each other's characteristics for robust inference. During the multi-view denoising process, the multi-view diffusion model uses the 3D-aware maps rendered by the reconstruction module at the previous timestep as additional conditions. The recursive diffusion framework with 3D-aware feedback unites the entire process and improves geometric consistency.Experiments show that our framework outperforms separation of these two stages and existing methods that combine them at the inference phase. Project page: https://costwen.github.io/Ouroboros3D/
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EZIGen: Enhancing zero-shot personalized image generation with precise subject encoding and decoupled guidance</title>
<link>https://arxiv.org/abs/2409.08091</link>
<guid>https://arxiv.org/abs/2409.08091</guid>
<content:encoded><![CDATA[
arXiv:2409.08091v4 Announce Type: replace 
Abstract: Zero-shot personalized image generation models aim to produce images that align with both a given text prompt and subject image, requiring the model to incorporate both sources of guidance. Existing methods often struggle to capture fine-grained subject details and frequently prioritize one form of guidance over the other, resulting in suboptimal subject encoding and imbalanced generation. In this study, we uncover key insights into overcoming such drawbacks, notably that 1) the choice of the subject image encoder critically influences subject identity preservation and training efficiency, and 2) the text and subject guidance should take effect at different denoising stages. Building on these insights, we introduce a new approach, EZIGen, that employs two main components: leveraging a fixed pre-trained Diffusion UNet itself as subject encoder, following a process that balances the two guidances by separating their dominance stage and revisiting certain time steps to bootstrap subject transfer quality. Through these two components, EZIGen, initially built upon SD2.1-base, achieved state-of-the-art performances on multiple personalized generation benchmarks with a unified model, while using 100 times less training data. Moreover, by further migrating our design to SDXL, EZIGen is proven to be a versatile model-agnostic solution for personalized generation. Demo Page: zichengduan.github.io/pages/EZIGen/index.html
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LT3SD: Latent Trees for 3D Scene Diffusion</title>
<link>https://arxiv.org/abs/2409.08215</link>
<guid>https://arxiv.org/abs/2409.08215</guid>
<content:encoded><![CDATA[
arXiv:2409.08215v2 Announce Type: replace 
Abstract: We present LT3SD, a novel latent diffusion model for large-scale 3D scene generation. Recent advances in diffusion models have shown impressive results in 3D object generation, but are limited in spatial extent and quality when extended to 3D scenes. To generate complex and diverse 3D scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level. To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes through shared diffusion generation across multiple scene patches. Through extensive experiments, we demonstrate the efficacy and benefits of LT3SD for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PPT: Pretraining with Pseudo-Labeled Trajectories for Motion Forecasting</title>
<link>https://arxiv.org/abs/2412.06491</link>
<guid>https://arxiv.org/abs/2412.06491</guid>
<content:encoded><![CDATA[
arXiv:2412.06491v2 Announce Type: replace 
Abstract: Accurately predicting how agents move in dynamic scenes is essential for safe autonomous driving. State-of-the-art motion forecasting models rely on large curated datasets with manually annotated or heavily post-processed trajectories. However, building these datasets is costly, generally manual, hard to scale, and lacks reproducibility. They also introduce domain gaps that limit generalization across environments. We introduce PPT (Pretraining with Pseudo-labeled Trajectories), a simple and scalable alternative that uses unprocessed and diverse trajectories automatically generated from off-the-shelf 3D detectors and tracking. Unlike traditional pipelines aiming for clean, single-label annotations, PPT embraces noise and diversity as useful signals for learning robust representations. With optional finetuning on a small amount of labeled data, models pretrained with PPT achieve strong performance across standard benchmarks particularly in low-data regimes, and in cross-domain, end-to-end and multi-class settings. PPT is easy to implement and improves generalization in motion forecasting. Code and data will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds</title>
<link>https://arxiv.org/abs/2501.01728</link>
<guid>https://arxiv.org/abs/2501.01728</guid>
<content:encoded><![CDATA[
arXiv:2501.01728v2 Announce Type: replace 
Abstract: Assessment of forest biodiversity is crucial for ecosystem management and conservation. While traditional field surveys provide high-quality assessments, they are labor-intensive and spatially limited. This study investigates whether deep learning-based fusion of close-range sensing data from 2D orthophotos and 3D airborne laser scanning (ALS) point clouds can reliable assess the biodiversity potential of forests. We introduce the BioVista dataset, comprising 44 378 paired samples of orthophotos and ALS point clouds from temperate forests in Denmark, designed to explore multimodal fusion approaches. Using deep neural networks (ResNet for orthophotos and PointVector for ALS point clouds), we investigate each data modality's ability to assess forest biodiversity potential, achieving overall accuracies of 76.7% and 75.8%, respectively. We explore various 2D and 3D fusion approaches: confidence-based ensembling, feature-level concatenation, and end-to-end training, achieving overall accuracies of 80.5%, 81.4% and 80.4% respectively. Our results demonstrate that spectral information from orthophotos and structural information from ALS point clouds effectively complement each other in forest biodiversity assessment.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2501.09503</link>
<guid>https://arxiv.org/abs/2501.09503</guid>
<content:encoded><![CDATA[
arXiv:2501.09503v2 Announce Type: replace 
Abstract: Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an "encode-then-route" manner. In the encoding step, AnyStory utilizes a universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion Transformer via Error-Optimized Cache</title>
<link>https://arxiv.org/abs/2501.19243</link>
<guid>https://arxiv.org/abs/2501.19243</guid>
<content:encoded><![CDATA[
arXiv:2501.19243v2 Announce Type: replace 
Abstract: Diffusion Transformer (DiT) is a crucial method for content generation. However, it needs a lot of time to sample. Many studies have attempted to use caching to reduce the time consumption of sampling. Existing caching methods accelerate generation by reusing DiT features from the previous time step and skipping calculations in the next, but they tend to locate and cache low-error modules without focusing on reducing caching-induced errors, resulting in a sharp decline in generated content quality when increasing caching intensity. To solve this problem, we propose the Error-Optimized Cache (EOC). This method introduces three key improvements: (1) Prior knowledge extraction: Extract and process the caching differences; (2) A judgment method for cache optimization: Determine whether certain caching steps need to be optimized; (3) Cache optimization: reduce caching errors. Experiments show that this algorithm significantly reduces the error accumulation caused by caching, especially excessive caching. On the ImageNet dataset, without substantially increasing the computational load, this method improves the FID of the generated images when the rule-based model FORA has a caching level of 75%, 50%, and 25%, and the training-based model Learning-to-cache has a caching level of 22%. Specifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857 to 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%) respectively.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling</title>
<link>https://arxiv.org/abs/2502.20824</link>
<guid>https://arxiv.org/abs/2502.20824</guid>
<content:encoded><![CDATA[
arXiv:2502.20824v2 Announce Type: replace 
Abstract: Smartphone cameras have become ubiquitous imaging tools, yet their small sensors and compact optics often limit spatial resolution and introduce distortions. Combining information from multiple low-resolution (LR) frames to produce a high-resolution (HR) image has been explored to overcome the inherent limitations of smartphone cameras. Despite the promise of multi-frame super-resolution (MFSR), current approaches are hindered by datasets that fail to capture the characteristic noise and motion patterns found in real-world handheld burst images. In this work, we address this gap by introducing a novel synthetic data engine that uses multi-exposure static images to synthesize LR-HR training pairs while preserving sensor-specific noise characteristics and image motion found during handheld burst photography. We also propose MFSR-GAN: a multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches, MFSR-GAN emphasizes a "base frame" throughout its architecture to mitigate artifacts. Experimental results on both synthetic and real data demonstrates that MFSR-GAN trained with our synthetic engine yields sharper, more realistic reconstructions than existing methods for real-world MFSR.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation</title>
<link>https://arxiv.org/abs/2504.02782</link>
<guid>https://arxiv.org/abs/2504.02782</guid>
<content:encoded><![CDATA[
arXiv:2504.02782v2 Announce Type: replace 
Abstract: The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation</title>
<link>https://arxiv.org/abs/2504.14467</link>
<guid>https://arxiv.org/abs/2504.14467</guid>
<content:encoded><![CDATA[
arXiv:2504.14467v2 Announce Type: replace 
Abstract: Zero-shot referring image segmentation aims to locate and segment the target region based on a referring expression, with the primary challenge of aligning and matching semantics across visual and textual modalities without training. Previous works address this challenge by utilizing Vision-Language Models and mask proposal networks for region-text matching. However, this paradigm may lead to incorrect target localization due to the inherent ambiguity and diversity of free-form referring expressions. To alleviate this issue, we present LGD (Leveraging Generative Descriptions), a framework that utilizes the advanced language generation capabilities of Multi-Modal Large Language Models to enhance region-text matching performance in Vision-Language Models. Specifically, we first design two kinds of prompts, the attribute prompt and the surrounding prompt, to guide the Multi-Modal Large Language Models in generating descriptions related to the crucial attributes of the referent object and the details of surrounding objects, referred to as attribute description and surrounding description, respectively. Secondly, three visual-text matching scores are introduced to evaluate the similarity between instance-level visual features and textual features, which determines the mask most associated with the referring expression. The proposed method achieves new state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU compared to previous methods.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class Uncertainty: A Measure to Mitigate Class Imbalance</title>
<link>https://arxiv.org/abs/2311.14090</link>
<guid>https://arxiv.org/abs/2311.14090</guid>
<content:encoded><![CDATA[
arXiv:2311.14090v2 Announce Type: replace-cross 
Abstract: Class-wise characteristics of training examples affect the performance of deep classifiers. A well-studied example is when the number of training examples of classes follows a long-tailed distribution, a situation that is likely to yield sub-optimal performance for under-represented classes. This class imbalance problem is conventionally addressed by approaches relying on the class-wise cardinality of training examples, such as data resampling. In this paper, we demonstrate that considering solely the cardinality of classes does not cover all issues causing class imbalance. To measure class imbalance, we propose "Class Uncertainty" as the average predictive uncertainty of the training examples, and we show that this novel measure captures the differences across classes better than cardinality. We also curate SVCI-20 as a novel dataset in which the classes have equal number of training examples but they differ in terms of their hardness; thereby causing a type of class imbalance which cannot be addressed by the approaches relying on cardinality. We incorporate our "Class Uncertainty" measure into a diverse set of ten class imbalance mitigation methods to demonstrate its effectiveness on long-tailed datasets as well as on our SVCI-20. Code and datasets will be made available.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression</title>
<link>https://arxiv.org/abs/2403.16677</link>
<guid>https://arxiv.org/abs/2403.16677</guid>
<content:encoded><![CDATA[
arXiv:2403.16677v3 Announce Type: replace-cross 
Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks. This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Lastly, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that FOOL permits downlinking over 100x the data volume without relying on prior information on the downstream tasks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Global Localization using Multi-Modal Object-Instance Re-Identification</title>
<link>https://arxiv.org/abs/2409.12002</link>
<guid>https://arxiv.org/abs/2409.12002</guid>
<content:encoded><![CDATA[
arXiv:2409.12002v2 Announce Type: replace-cross 
Abstract: Re-identification (ReID) is a critical challenge in computer vision, predominantly studied in the context of pedestrians and vehicles. However, robust object-instance ReID, which has significant implications for tasks such as autonomous exploration, long-term perception, and scene understanding, remains underexplored. In this work, we address this gap by proposing a novel dual-path object-instance re-identification transformer architecture that integrates multimodal RGB and depth information. By leveraging depth data, we demonstrate improvements in ReID across scenes that are cluttered or have varying illumination conditions. Additionally, we develop a ReID-based localization framework that enables accurate camera localization and pose identification across different viewpoints. We validate our methods using two custom-built RGB-D datasets, as well as multiple sequences from the open-source TUM RGB-D datasets. Our approach demonstrates significant improvements in both object instance ReID (mAP of 75.18) and localization accuracy (success rate of 83% on TUM-RGBD), highlighting the essential role of object ReID in advancing robotic perception. Our models, frameworks, and datasets have been made publicly available.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models</title>
<link>https://arxiv.org/abs/2409.16663</link>
<guid>https://arxiv.org/abs/2409.16663</guid>
<content:encoded><![CDATA[
arXiv:2409.16663v4 Announce Type: replace-cross 
Abstract: We propose the use of latent space generative world models to address the covariate shift problem in autonomous driving. A world model is a neural network capable of predicting an agent's next state given past states and actions. By leveraging a world model during training, the driving policy effectively mitigates covariate shift without requiring an excessive amount of training data. During end-to-end training, our policy learns how to recover from errors by aligning with states observed in human demonstrations, so that at runtime it can recover from perturbations outside the training distribution. Additionally, we introduce a novel transformer-based perception encoder that employs multi-view cross-attention and a learned scene query. We present qualitative and quantitative results, demonstrating significant improvements upon prior state of the art in closed-loop testing in the CARLA simulator, as well as showing the ability to handle perturbations in both CARLA and NVIDIA's DRIVE Sim.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping</title>
<link>https://arxiv.org/abs/2411.12286</link>
<guid>https://arxiv.org/abs/2411.12286</guid>
<content:encoded><![CDATA[
arXiv:2411.12286v2 Announce Type: replace-cross 
Abstract: Inferring affordable (i.e., graspable) parts of arbitrary objects based on human specifications is essential for robots advancing toward open-vocabulary manipulation. Current grasp planners, however, are hindered by limited vision-language comprehension and time-consuming 3D radiance modeling, restricting real-time, open-vocabulary interactions with objects. To address these limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary Affordance Reasoning framework, which fine-tunes the Large Language Models (LLMs) to predict the visual affordance of graspable object parts within RGB feature space. We compile a dataset of over 10,000 images from human-object interactions, annotated with unified visual and linguistic affordance labels, to enable multi-modal fine-tuning. GLOVER inherits world knowledge and common-sense reasoning from LLMs, facilitating more fine-grained object understanding and sophisticated tool-use reasoning. To enable effective real-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a non-parametric grasp planner that aligns the gripper pose with a superquadric surface derived from affordance data. In evaluations across 30 table-top real-world scenes, GLOVER achieves success rates of 86.0% in part identification and 76.3% in grasping, with speeds approximately 29 times faster in affordance reasoning and 40 times faster in grasping pose estimation than the previous state-of-the-art. We also validate the generalization across embodiments, showing effectiveness in humanoid robots with dexterous hands.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Data Poisoning Attacks on Quantum Machine Learning in the NISQ Era</title>
<link>https://arxiv.org/abs/2411.14412</link>
<guid>https://arxiv.org/abs/2411.14412</guid>
<content:encoded><![CDATA[
arXiv:2411.14412v3 Announce Type: replace-cross 
Abstract: With the growing interest in Quantum Machine Learning (QML) and the increasing availability of quantum computers through cloud providers, addressing the potential security risks associated with QML has become an urgent priority. One key concern in the QML domain is the threat of data poisoning attacks in the current quantum cloud setting. Adversarial access to training data could severely compromise the integrity and availability of QML models. Classical data poisoning techniques require significant knowledge and training to generate poisoned data, and lack noise resilience, making them ineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era. In this work, we first propose a simple yet effective technique to measure intra-class encoder state similarity (ESS) by analyzing the outputs of encoding circuits. Leveraging this approach, we introduce a \underline{Qu}antum \underline{I}ndiscriminate \underline{D}ata Poisoning attack, QUID. Through extensive experiments conducted in both noiseless and noisy environments (e.g., IBM\_Brisbane's noise), across various architectures and datasets, QUID achieves up to $92\%$ accuracy degradation in model performance compared to baseline models and up to $75\%$ accuracy degradation compared to random label-flipping. We also tested QUID against state-of-the-art classical defenses, with accuracy degradation still exceeding $50\%$, demonstrating its effectiveness. This work represents the first attempt to reevaluate data poisoning attacks in the context of QML.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seamless Optical Cloud Computing across Edge-Metro Network for Generative AI</title>
<link>https://arxiv.org/abs/2412.12126</link>
<guid>https://arxiv.org/abs/2412.12126</guid>
<content:encoded><![CDATA[
arXiv:2412.12126v2 Announce Type: replace-cross 
Abstract: The rapid advancement of generative artificial intelligence (AI) in recent years has profoundly reshaped modern lifestyles, necessitating a revolutionary architecture to support the growing demands for computational power. Cloud computing has become the driving force behind this transformation. However, it consumes significant power and faces computation security risks due to the reliance on extensive data centers and servers in the cloud. Reducing power consumption while enhancing computational scale remains persistent challenges in cloud computing. Here, we propose and experimentally demonstrate an optical cloud computing system that can be seamlessly deployed across edge-metro network. By modulating inputs and models into light, a wide range of edge nodes can directly access the optical computing center via the edge-metro network. The experimental validations show an energy efficiency of 118.6 mW/TOPs (tera operations per second), reducing energy consumption by two orders of magnitude compared to traditional electronic-based cloud computing solutions. Furthermore, it is experimentally validated that this architecture can perform various complex generative AI models through parallel computing to achieve image generation tasks.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment-and-Classify: ROI-Guided Generalizable Contrast Phase Classification in CT Using XGBoost</title>
<link>https://arxiv.org/abs/2501.14066</link>
<guid>https://arxiv.org/abs/2501.14066</guid>
<content:encoded><![CDATA[
arXiv:2501.14066v2 Announce Type: replace-cross 
Abstract: Purpose: To automate contrast phase classification in CT using organ-specific features extracted from a widely used segmentation tool with a lightweight decision tree classifier.
  Materials and Methods: This retrospective study utilized three public CT datasets from separate institutions. The phase prediction model was trained on the WAW-TACE (median age: 66 [60,73]; 185 males) dataset, and externally validated on the VinDr-Multiphase (146 males; 63 females; 56 unk) and C4KC-KiTS (median age: 61 [50.68; 123 males) datasets. Contrast phase classification was performed using organ-specific features extracted by TotalSegmentator, followed by prediction using a gradient-boosted decision tree classifier.
  Results: On the VinDr-Multiphase dataset, the phase prediction model achieved the highest or comparable AUCs across all phases (>0.937), with superior F1-scores in the non-contrast (0.994), arterial (0.937), and delayed (0.718) phases. Statistical testing indicated significant performance differences only in the arterial and delayed phases (p<0.05). On the C4KC-KiTS dataset, the phase prediction model achieved the highest AUCs across all phases (>0.991), with superior F1-scores in arterial/venous (0.968) and delayed (0.935) phases. Statistical testing confirmed significant improvements over all baseline models in these two phases (p<0.05). Performance in the non-contrast class, however, was comparable across all models, with no statistically significant differences observed (p>0.05).
  Conclusion: The lightweight model demonstrated strong performance relative to all baseline models, and exhibited robust generalizability across datasets from different institutions.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference</title>
<link>https://arxiv.org/abs/2502.18137</link>
<guid>https://arxiv.org/abs/2502.18137</guid>
<content:encoded><![CDATA[
arXiv:2502.18137v2 Announce Type: replace-cross 
Abstract: An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-aware Bayesian machine learning modelling of land cover classification</title>
<link>https://arxiv.org/abs/2503.21510</link>
<guid>https://arxiv.org/abs/2503.21510</guid>
<content:encoded><![CDATA[
arXiv:2503.21510v2 Announce Type: replace-cross 
Abstract: Land cover classification involves the production of land cover maps, which determine the type of land through remote sensing imagery. Over recent years, such classification is being performed by machine learning classification models, which can give highly accurate predictions on land cover per pixel using large quantities of input training data. However, such models do not currently take account of input measurement uncertainty, which is vital for traceability in metrology. In this work we propose a Bayesian classification framework using generative modelling to take account of input measurement uncertainty. We take the specific case of Bayesian quadratic discriminant analysis, and apply it to land cover datasets from Copernicus Sentinel-2 in 2020 and 2021. We benchmark the performance of the model against more popular classification models used in land cover maps such as random forests and neural networks. We find that such Bayesian models are more trustworthy, in the sense that they are more interpretable, explicitly model the input measurement uncertainty, and maintain predictive performance of class probability outputs across datasets of different years and sizes, whilst also being computationally efficient.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Mixture Flow Matching Models</title>
<link>https://arxiv.org/abs/2504.05304</link>
<guid>https://arxiv.org/abs/2504.05304</guid>
<content:encoded><![CDATA[
arXiv:2504.05304v2 Announce Type: replace-cross 
Abstract: Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\times$256.
]]></content:encoded>
<pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2504.15032</link>
<guid>https://arxiv.org/abs/2504.15032</guid>
<content:encoded><![CDATA[
<div> keywords: Compositional text-to-video generation, DyST-XL, training-free framework, entity-attribute graphs, frame-aware control <br />
Summary:
DyST-XL is a training-free framework designed to improve text-to-video generation models by addressing issues such as layout discontinuity and implausible interaction dynamics. It leverages large language models to parse input prompts into entity-attribute graphs and generates physics-aware keyframe layouts. The framework also enforces localized text-video alignment through frame-aware attention masking, allowing for precise control over individual entities. Additionally, DyST-XL utilizes an Entity-Consistency Constraint strategy to preserve object identity throughout frames. Experimental results show significant improvements in compositional text-to-video generation, especially on complex prompts. The code for DyST-XL is available on GitHub, making it accessible for further research and development. <br /><br />Summary: <div>
arXiv:2504.15032v2 Announce Type: replace 
Abstract: Compositional text-to-video generation, which requires synthesizing dynamic scenes with multiple interacting entities and precise spatial-temporal relationships, remains a critical challenge for diffusion-based models. Existing methods struggle with layout discontinuity, entity identity drift, and implausible interaction dynamics due to unconstrained cross-attention mechanisms and inadequate physics-aware reasoning. To address these limitations, we propose DyST-XL, a \textbf{training-free} framework that enhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through frame-aware control. DyST-XL integrates three key innovations: (1) A Dynamic Layout Planner that leverages large language models (LLMs) to parse input prompts into entity-attribute graphs and generates physics-aware keyframe layouts, with intermediate frames interpolated via trajectory optimization; (2) A Dual-Prompt Controlled Attention Mechanism that enforces localized text-video alignment through frame-aware attention masking, achieving precise control over individual entities; and (3) An Entity-Consistency Constraint strategy that propagates first-frame feature embeddings to subsequent frames during denoising, preserving object identity without manual annotation. Experiments demonstrate that DyST-XL excels in compositional text-to-video generation, significantly improving performance on complex prompts and bridging a crucial gap in training-free video synthesis. The code is released in https://github.com/XiaoBuL/DyST-XL.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels</title>
<link>https://arxiv.org/abs/2504.21040</link>
<guid>https://arxiv.org/abs/2504.21040</guid>
<content:encoded><![CDATA[
<div> Keywords: urban street environments, big data, street view images, multimodal large language models, walkability

Summary: 
This study explores the use of multimodal large language models (MLLMs) in evaluating the walkability of urban environments using street view images (SVIs). The integration of expert urban design knowledge into the MLLM prompts enhances its capability and reliability. Walkability metrics from existing literature are categorized using relevant ontologies, with a focus on pedestrian safety and attractiveness subthemes. The study analyzes the MLLM's ability to evaluate SVI walkability based on varying levels of specificity in the prompts. While MLLMs can provide assessments and interpretations based on general knowledge, they may provide optimistic scores and make mistakes in interpreting metrics. By integrating expert knowledge, the MLLM's evaluative performance improves in consistency and accuracy. This research highlights the potential of MLLMs in automating image-text evaluations in urban design analysis. 

<br /><br />Summary: <div>
arXiv:2504.21040v1 Announce Type: new 
Abstract: Urban street environments are vital to supporting human activity in public spaces. The emergence of big data, such as street view images (SVIs) combined with multimodal large language models (MLLMs), is transforming how researchers and practitioners investigate, measure, and evaluate semantic and visual elements of urban environments. Considering the low threshold for creating automated evaluative workflows using MLLMs, it is crucial to explore both the risks and opportunities associated with these probabilistic models. In particular, the extent to which the integration of expert knowledge can influence the performance of MLLMs in evaluating the quality of urban design has not been fully explored. This study sets out an initial exploration of how integrating more formal and structured representations of expert urban design knowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's capability and reliability in evaluating the walkability of built environments using SVIs. We collect walkability metrics from the existing literature and categorize them using relevant ontologies. We then select a subset of these metrics, focusing on the subthemes of pedestrian safety and attractiveness, and develop prompts for the MLLM accordingly. We analyze the MLLM's ability to evaluate SVI walkability subthemes through prompts with varying levels of clarity and specificity regarding evaluation criteria. Our experiments demonstrate that MLLMs are capable of providing assessments and interpretations based on general knowledge and can support the automation of multimodal image-text evaluations. However, they generally provide more optimistic scores and can make mistakes when interpreting the provided metrics, resulting in incorrect evaluations. By integrating expert knowledge, the MLLM's evaluative performance exhibits higher consistency and concentration.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Legilimens: Performant Video Analytics on the System-on-Chip Edge</title>
<link>https://arxiv.org/abs/2504.21136</link>
<guid>https://arxiv.org/abs/2504.21136</guid>
<content:encoded><![CDATA[
<div> System-on-Chip GPUs, Mobile Edge Devices, Edge Computing, Continuous Learning, Video Analytics
<br />
Legilimens is a continuous learning system designed for mobile edge devices' System-on-Chip GPUs. It leverages visually distinct scenes with overlapping model embeddings to enable lightweight specialization with minimal data samples. The system includes techniques to select high-utility data samples for retraining, update the base model efficiently, and maximize accuracy by time-sharing compute resources between retraining and live inference. Legilimens significantly reduces retraining costs by 2.8-10x compared to existing systems, leading to 18-45% higher accuracies across various workloads.
<br /><br />Summary: <div>
arXiv:2504.21136v1 Announce Type: new 
Abstract: Continually retraining models has emerged as a primary technique to enable high-accuracy video analytics on edge devices. Yet, existing systems employ such adaptation by relying on the spare compute resources that traditional (memory-constrained) edge servers afford. In contrast, mobile edge devices such as drones and dashcams offer a fundamentally different resource profile: weak(er) compute with abundant unified memory pools. We present Legilimens, a continuous learning system for the mobile edge's System-on-Chip GPUs. Our driving insight is that visually distinct scenes that require retraining exhibit substantial overlap in model embeddings; if captured into a base model on device memory, specializing to each new scene can become lightweight, requiring very few samples. To practically realize this approach, Legilimens presents new, compute-efficient techniques to (1) select high-utility data samples for retraining specialized models, (2) update the base model without complete retraining, and (3) time-share compute resources between retraining and live inference for maximal accuracy. Across diverse workloads, Legilimens lowers retraining costs by 2.8-10x compared to existing systems, resulting in 18-45% higher accuracies.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis</title>
<link>https://arxiv.org/abs/2504.21154</link>
<guid>https://arxiv.org/abs/2504.21154</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion recognition, contemporary dance, Laban Movement Analysis, 3D keypoints data, machine learning

Summary:
This paper introduces a new framework for emotion recognition in contemporary dance, enhancing existing Laban Movement Analysis (LMA) feature descriptors and introducing novel descriptors. The framework utilizes 3D keypoints data from professional dancers performing contemporary dance to capture both quantitative and qualitative movement characteristics. Multiple classifiers, such as Random Forests and Support Vector Machines, are trained to recognize emotions expressed through dance movements. The study also employs explainable machine learning methods to provide a detailed explanation of features and their impact on model predictions. The research aims to improve emotion recognition in contemporary dance, with potential applications in performance analysis, dance training, and human-computer interaction. The framework achieves a high accuracy of 96.85%, showcasing its effectiveness in recognizing emotions in dance performances.

<br /><br />Summary: <div>
arXiv:2504.21154v1 Announce Type: new 
Abstract: This paper presents a novel framework for emotion recognition in contemporary dance by improving existing Laban Movement Analysis (LMA) feature descriptors and introducing robust, novel descriptors that capture both quantitative and qualitative aspects of the movement. Our approach extracts expressive characteristics from 3D keypoints data of professional dancers performing contemporary dance under various emotional states, and trains multiple classifiers, including Random Forests and Support Vector Machines. Additionally, we provide in-depth explanation of features and their impact on model predictions using explainable machine learning methods. Overall, our study improves emotion recognition in contemporary dance and offers promising applications in performance analysis, dance training, and human--computer interaction, with a highest accuracy of 96.85\%.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dance Style Recognition Using Laban Movement Analysis</title>
<link>https://arxiv.org/abs/2504.21166</link>
<guid>https://arxiv.org/abs/2504.21166</guid>
<content:encoded><![CDATA[
<div> Keywords: automated movement analysis, dance style recognition, Laban Movement Analysis, 3D pose estimation, explainable AI methods

Summary:
This study introduces a novel pipeline for dance style recognition using features extracted through Laban Movement Analysis (LMA). Traditional methods focus on cross-frame movement analysis but lack temporal context. The proposed pipeline combines 3D pose estimation, 3D human mesh reconstruction, and floor aware body modeling to extract LMA features. A sliding window approach captures movement evolution across time, addressing the temporal limitation. Machine learning methods are then used for classification, with explainable AI techniques evaluating feature contributions. The method achieves a high classification accuracy of 99.18%, demonstrating that adding temporal context significantly enhances dance style recognition performance. <div>
arXiv:2504.21166v1 Announce Type: new 
Abstract: The growing interest in automated movement analysis has presented new challenges in recognition of complex human activities including dance. This study focuses on dance style recognition using features extracted using Laban Movement Analysis. Previous studies for dance style recognition often focus on cross-frame movement analysis, which limits the ability to capture temporal context and dynamic transitions between movements. This gap highlights the need for a method that can add temporal context to LMA features. For this, we introduce a novel pipeline which combines 3D pose estimation, 3D human mesh reconstruction, and floor aware body modeling to effectively extract LMA features. To address the temporal limitation, we propose a sliding window approach that captures movement evolution across time in features. These features are then used to train various machine learning methods for classification, and their explainability explainable AI methods to evaluate the contribution of each feature to classification performance. Our proposed method achieves a highest classification accuracy of 99.18\% which shows that the addition of temporal context significantly improves dance style recognition performance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping</title>
<link>https://arxiv.org/abs/2504.21194</link>
<guid>https://arxiv.org/abs/2504.21194</guid>
<content:encoded><![CDATA[
<div> Keywords: geolocation, International Space Station, image processing, machine learning, Earth observation

Summary: 
This paper introduces a novel approach to geolocating images taken from the International Space Station (ISS) using advanced machine learning techniques. Three image processing pipelines are utilized, including a Neural Network approach, a SIFT based method, and the GPT-4 model. These pipelines are specifically designed to analyze high-resolution ISS imagery and identify Earth locations. The Neural Network approach shows high accuracy in matching geographical features, the SIFT pipeline excels in processing zoomed-in images, and the GPT-4 model provides enriched descriptions and location predictions. Through evaluation on a dataset of over 140 ISS images, the methods demonstrate promise in automated geolocation with varying levels of success. This research contributes to remote sensing and Earth observation by improving the accuracy and efficiency of geolocating space-based imagery, benefiting environmental monitoring and global mapping efforts.<br /><br />Summary: <div>
arXiv:2504.21194v1 Announce Type: new 
Abstract: This paper presents a novel approach to geolocating images captured from the International Space Station (ISS) using advanced machine learning algorithms. Despite having precise ISS coordinates, the specific Earth locations depicted in astronaut-taken photographs often remain unidentified. Our research addresses this gap by employing three distinct image processing pipelines: a Neural Network based approach, a SIFT based method, and GPT-4 model. Each pipeline is tailored to process high-resolution ISS imagery, identifying both natural and man-made geographical features. Through extensive evaluation on a diverse dataset of over 140 ISS images, our methods demonstrate significant promise in automated geolocation with varied levels of success. The NN approach showed a high success rate in accurately matching geographical features, while the SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided enriched geographical descriptions alongside location predictions. This research contributes to the fields of remote sensing and Earth observation by enhancing the accuracy and efficiency of geolocating space-based imagery, thereby aiding environmental monitoring and global mapping efforts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemeBLIP2: A novel lightweight multimodal system to detect harmful memes</title>
<link>https://arxiv.org/abs/2504.21226</link>
<guid>https://arxiv.org/abs/2504.21226</guid>
<content:encoded><![CDATA[
<div> Keywords: memes, harmful content detection, multimodal system, image-text fusion, BLIP-2

Summary:
Memes are popular internet content that often combine visuals with text to convey humor or opinions. However, some memes can also contain harmful messages like hate speech. In response to this issue, the researchers have developed MemeBLIP2, a lightweight multimodal system that effectively detects harmful memes by integrating image and text features. This system incorporates modules that align image and text representations and fuse them for improved classification. By using BLIP-2 as the core vision-language model, MemeBLIP2 has been evaluated on the PrideMM datasets, demonstrating an ability to capture subtle cues in both modalities, even in cases with ironic or culturally specific content. This approach enhances the detection of harmful material in memes, contributing to efforts to mitigate the spread of harmful messages on social media platforms.<br /><br />Summary: <div>
arXiv:2504.21226v1 Announce Type: new 
Abstract: Memes often merge visuals with brief text to share humor or opinions, yet some memes contain harmful messages such as hate speech. In this paper, we introduces MemeBLIP2, a light weight multimodal system that detects harmful memes by combining image and text features effectively. We build on previous studies by adding modules that align image and text representations into a shared space and fuse them for better classification. Using BLIP-2 as the core vision-language model, our system is evaluated on the PrideMM datasets. The results show that MemeBLIP2 can capture subtle cues in both modalities, even in cases with ironic or culturally specific content, thereby improving the detection of harmful material.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection</title>
<link>https://arxiv.org/abs/2504.21231</link>
<guid>https://arxiv.org/abs/2504.21231</guid>
<content:encoded><![CDATA[
<div> Ultrasound, neck, deep learning, anatomical landmark detection, class imbalance <br />
<br />
Summary: 
The article introduces T2ID-CAS, a hybrid approach that combines a text-to-image latent diffusion model with class-aware sampling to address class imbalance in neck ultrasound datasets for anatomical landmark detection. The approach aims to generate high-quality synthetic samples for underrepresented classes like tracheal rings and vocal folds. Experimental results using YOLOv9 demonstrated a significant improvement in mean Average Precision, from 66 to 88.2, showcasing the potential of T2ID-CAS as a computationally efficient solution for mitigating class imbalance in AI-assisted ultrasound-guided interventions. This novel approach could enhance procedural efficiency in airway management by providing more accurate and reliable anatomical landmark detection in real-time neck ultrasound imaging. <div>
arXiv:2504.21231v1 Announce Type: new 
Abstract: Neck ultrasound (US) plays a vital role in airway management by providing non-invasive, real-time imaging that enables rapid and precise interventions. Deep learning-based anatomical landmark detection in neck US can further facilitate procedural efficiency. However, class imbalance within datasets, where key structures like tracheal rings and vocal folds are underrepresented, presents significant challenges for object detection models. To address this, we propose T2ID-CAS, a hybrid approach that combines a text-to-image latent diffusion model with class-aware sampling to generate high-quality synthetic samples for underrepresented classes. This approach, rarely explored in the ultrasound domain, improves the representation of minority classes. Experimental results using YOLOv9 for anatomical landmark detection in neck US demonstrated that T2ID-CAS achieved a mean Average Precision of 88.2, significantly surpassing the baseline of 66. This highlights its potential as a computationally efficient and scalable solution for mitigating class imbalance in AI-assisted ultrasound-guided interventions.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subject Information Extraction for Novelty Detection with Domain Shifts</title>
<link>https://arxiv.org/abs/2504.21247</link>
<guid>https://arxiv.org/abs/2504.21247</guid>
<content:encoded><![CDATA[
<div> novelty detection, unsupervised, domain shift, subject information, deep Gaussian mixture model  
Summary:  
This paper introduces a new method for unsupervised novelty detection (UND) that addresses the challenge of domain shift between training and testing data. The method separates subject information from background variation to improve detection performance under different domains. By minimizing the mutual information between subject and background representations and utilizing a deep Gaussian mixture model to model background variation, the proposed method is able to effectively detect novelties without being affected by domain discrepancies. Experimental results demonstrate that the model generalizes well to unseen domains and outperforms baseline methods, particularly in scenarios with significant domain shifts. <div>
arXiv:2504.21247v1 Announce Type: new 
Abstract: Unsupervised novelty detection (UND), aimed at identifying novel samples, is essential in fields like medical diagnosis, cybersecurity, and industrial quality control. Most existing UND methods assume that the training data and testing normal data originate from the same domain and only consider the distribution variation between training data and testing data. However, in real scenarios, it is common for normal testing and training data to originate from different domains, a challenge known as domain shift. The discrepancies between training and testing data often lead to incorrect classification of normal data as novel by existing methods. A typical situation is that testing normal data and training data describe the same subject, yet they differ in the background conditions. To address this problem, we introduce a novel method that separates subject information from background variation encapsulating the domain information to enhance detection performance under domain shifts. The proposed method minimizes the mutual information between the representations of the subject and background while modelling the background variation using a deep Gaussian mixture model, where the novelty detection is conducted on the subject representations solely and hence is not affected by the variation of domains. Extensive experiments demonstrate that our model generalizes effectively to unseen domains and significantly outperforms baseline methods, especially under substantial domain shifts between training and testing data.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild</title>
<link>https://arxiv.org/abs/2504.21248</link>
<guid>https://arxiv.org/abs/2504.21248</guid>
<content:encoded><![CDATA[
<div> Facial expression recognition, computer vision, multi-modal transfer learning, video-based dataset, ResNets, OpenPose, OmniVec networks <br />
<br />
Facial expression recognition (FER) is a challenging problem in computer vision with applications in diverse fields. This study explores the use of multi-modal transfer learning to enhance FER performance on the Dynamic Facial Expression in-the-Wild (DFEW) dataset. By combining pretrained ResNets, OpenPose, and OmniVec networks, the researchers investigate the impact of cross-temporal, multi-modal features on classification accuracy. The study shows that these finely-tuned multi-modal feature generators lead to a modest improvement in accuracy for the transformer-based classification model. This research highlights the potential of leveraging multi-modal approaches to improve FER performance, especially in complex datasets like DFEW. <br /><br />Summary: <div>
arXiv:2504.21248v1 Announce Type: new 
Abstract: Facial expression recognition (FER) is a subset of computer vision with important applications for human-computer-interaction, healthcare, and customer service. FER represents a challenging problem-space because accurate classification requires a model to differentiate between subtle changes in facial features. In this paper, we examine the use of multi-modal transfer learning to improve performance on a challenging video-based FER dataset, Dynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrained ResNets, OpenPose, and OmniVec networks, we explore the impact of cross-temporal, multi-modal features on classification accuracy. Ultimately, we find that these finely-tuned multi-modal feature generators modestly improve accuracy of our transformer-based classification model.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning</title>
<link>https://arxiv.org/abs/2504.21263</link>
<guid>https://arxiv.org/abs/2504.21263</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual-Context Learning, Prompt Selection, Context Integration, Condenser, Benchmark Tasks <br />
Summary: <br />
Visual In-Context Learning (VICL) aims to solve vision tasks by mimicking human-like task completion through analogy. One key aspect in VICL is prompt selection, where multiple suitable prompts might exist but individually fall short. In response, a new approach called prompt condensation is proposed, which involves candidate prompts collaborating to integrate informative contexts efficiently without losing resolution. Condenser, a lightweight plugin, compresses relevant context across multiple prompts and optimizes integration with the backbone. Experimental results show that Condenser outperforms existing methods in benchmark tasks, demonstrating superior context compression, scalability with more prompts, and improved computational efficiency compared to ensemble methods. The open-sourced code for Condenser is available at https://github.com/gimpong/CVPR25-Condenser. <br /> <div>
arXiv:2504.21263v1 Announce Type: new 
Abstract: Visual In-Context Learning (VICL) enables adaptively solving vision tasks by leveraging pixel demonstrations, mimicking human-like task completion through analogy. Prompt selection is critical in VICL, but current methods assume the existence of a single "ideal" prompt in a pool of candidates, which in practice may not hold true. Multiple suitable prompts may exist, but individually they often fall short, leading to difficulties in selection and the exclusion of useful context. To address this, we propose a new perspective: prompt condensation. Rather than relying on a single prompt, candidate prompts collaborate to efficiently integrate informative contexts without sacrificing resolution. We devise Condenser, a lightweight external plugin that compresses relevant fine-grained context across multiple prompts. Optimized end-to-end with the backbone, Condenser ensures accurate integration of contextual cues. Experiments demonstrate Condenser outperforms state-of-the-arts across benchmark tasks, showing superior context compression, scalability with more prompts, and enhanced computational efficiency compared to ensemble methods, positioning it as a highly competitive solution for VICL. Code is open-sourced at https://github.com/gimpong/CVPR25-Condenser.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion</title>
<link>https://arxiv.org/abs/2504.21266</link>
<guid>https://arxiv.org/abs/2504.21266</guid>
<content:encoded><![CDATA[
<div> Keyword: action recognition, feature diversity, CoCoDiff, latent diffusion model, semantic consistency

Summary: 
The article introduces a novel Coarse-fine text co-guidance Diffusion model (CoCoDiff) for action recognition tasks. This model aims to enhance feature diversity while maintaining semantic consistency in the latent space. By leveraging diffusion and multi-granularity textual guidance, CoCoDiff generates diverse action representations from spatio-temporal features extracted from skeleton sequences. The approach incorporates a coarse-fine text co-guided strategy utilizing textual information from large language models (LLMs) to ensure semantic consistency between generated features and original inputs. Importantly, CoCoDiff operates as an auxiliary module during training without incurring additional inference costs. Experimental results show that CoCoDiff achieves state-of-the-art performance on skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120, and Kinetics-Skeleton. 

<br /><br />Summary: <div>
arXiv:2504.21266v1 Announce Type: new 
Abstract: In action recognition tasks, feature diversity is essential for enhancing model generalization and performance. Existing methods typically promote feature diversity by expanding the training data in the sample space, which often leads to inefficiencies and semantic inconsistencies. To overcome these problems, we propose a novel Coarse-fine text co-guidance Diffusion model (CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in the latent space by leveraging diffusion and multi-granularity textual guidance. Specifically, our approach feeds spatio-temporal features extracted from skeleton sequences into a latent diffusion model to generate diverse action representations. Meanwhile, we introduce a coarse-fine text co-guided strategy that leverages textual information from large language models (LLMs) to ensure semantic consistency between the generated features and the original inputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module during training, incurring no additional inference cost. Extensive experiments demonstrate that CoCoDiff achieves SOTA performance on skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and Kinetics-Skeleton.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image</title>
<link>https://arxiv.org/abs/2504.21281</link>
<guid>https://arxiv.org/abs/2504.21281</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D medical image segmentation, multi-modal, Mamba model, feature extraction, adaptive feature fusion 

Summary:
Our paper introduces a novel approach for 3D tumor segmentation in multi-modal medical images by leveraging the Mamba model. We address the challenges of capturing global features and efficient fusion of modality-specific information. Our proposed method includes a specific modality Mamba encoder for feature extraction, a bi-level integration block for adaptive feature fusion, and a decoder for generating tumor segmentation maps. By dynamically merging multi-modal and multi-level features using modality and channel attention learning, our approach achieves competitive performance compared to existing CNN, Transformer, and Mamba-based methods. Experimental results on PET/CT and MRI datasets demonstrate the effectiveness of our approach in accurately identifying tumor regions across different modalities. Our method showcases improved scalability, global context modeling, and efficient feature fusion, highlighting its potential for enhancing 3D medical image segmentation tasks.<br /><br />Summary: <div>
arXiv:2504.21281v1 Announce Type: new 
Abstract: Multi-modal 3D medical image segmentation aims to accurately identify tumor regions across different modalities, facing challenges from variations in image intensity and tumor morphology. Traditional convolutional neural network (CNN)-based methods struggle with capturing global features, while Transformers-based methods, despite effectively capturing global context, encounter high computational costs in 3D medical image segmentation. The Mamba model combines linear scalability with long-distance modeling, making it a promising approach for visual representation learning. However, Mamba-based 3D multi-modal segmentation still struggles to leverage modality-specific features and fuse complementary information effectively. In this paper, we propose a Mamba based feature extraction and adaptive multilevel feature fusion for 3D tumor segmentation using multi-modal medical image. We first develop the specific modality Mamba encoder to efficiently extract long-range relevant features that represent anatomical and pathological structures present in each modality. Moreover, we design an bi-level synergistic integration block that dynamically merges multi-modal and multi-level complementary features by the modality attention and channel attention learning. Lastly, the decoder combines deep semantic information with fine-grained details to generate the tumor segmentation map. Experimental results on medical image datasets (PET/CT and MRI multi-sequence) show that our approach achieve competitive performance compared to the state-of-the-art CNN, Transformer, and Mamba-based approaches.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions</title>
<link>https://arxiv.org/abs/2504.21292</link>
<guid>https://arxiv.org/abs/2504.21292</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, U-Net, Diffusion Transformer, self-attention, convolutional inductive biases

Summary:
The study explores contemporary diffusion models based on U-Net and Diffusion Transformer architectures, known for their image generation capabilities using attention mechanisms. Contrary to common belief, their analysis reveals that self-attention in these models predominantly showcases localized attention patterns similar to convolutional inductive biases, challenging the necessity of global interactions in self-attention. To address this, they propose \(\Delta\)ConvFusion, replacing traditional self-attention modules with Pyramid Convolution Blocks (\(\Delta\)ConvBlocks). By distilling attention patterns into localized convolutional operations, \(\Delta\)ConvFusion maintains generative fidelity while significantly reducing computational costs by 6929$\times$, outperforming LinFusion by 5.42$\times in efficiency. This innovative approach opens new avenues for efficient image generation without compromising quality. 

<br /><br />Summary: <div>
arXiv:2504.21292v1 Announce Type: new 
Abstract: Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT) architectures have revolutionized image generation through transformer-based attention mechanisms. The prevailing paradigm has commonly employed self-attention with quadratic computational complexity to handle global spatial relationships in complex images, thereby synthesizing high-fidelity images with coherent visual semantics.Contrary to conventional wisdom, our systematic layer-wise analysis reveals an interesting discrepancy: self-attention in pre-trained diffusion models predominantly exhibits localized attention patterns, closely resembling convolutional inductive biases. This suggests that global interactions in self-attention may be less critical than commonly assumed.Driven by this, we propose \(\Delta\)ConvFusion to replace conventional self-attention modules with Pyramid Convolution Blocks (\(\Delta\)ConvBlocks).By distilling attention patterns into localized convolutional operations while keeping other components frozen, \(\Delta\)ConvFusion achieves performance comparable to transformer-based counterparts while reducing computational cost by 6929$\times$ and surpassing LinFusion by 5.42$\times$ in efficiency--all without compromising generative fidelity.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Multi-view Multi-class Anomaly Detection</title>
<link>https://arxiv.org/abs/2504.21294</link>
<guid>https://arxiv.org/abs/2504.21294</guid>
<content:encoded><![CDATA[
<div> encoder, anomaly detection, multi-view, feature modeling, anomaly amplification

Summary:
The paper introduces a Multi-View Multi-Class Anomaly Detection model (MVMCAD) that integrates information from multiple views to accurately identify anomalies. It proposes a semi-frozen encoder with a pre-encoder prior enhancement mechanism for stable cross-view feature modeling. An Anomaly Amplification Module (AAM) is introduced to enhance anomaly signals by modeling global token interactions and suppressing normal regions. Additionally, a Cross-Feature Loss aligns shallow encoder features with deep decoder features to increase sensitivity to anomalies at different semantic levels. Experimental results on the Real-IAD dataset for multi-view multi-class anomaly detection show state-of-the-art performance, achieving 91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and pixel-level anomaly detection, respectively. <div>
arXiv:2504.21294v1 Announce Type: new 
Abstract: The latest trend in anomaly detection is to train a unified model instead of training a separate model for each category. However, existing multi-class anomaly detection (MCAD) models perform poorly in multi-view scenarios because they often fail to effectively model the relationships and complementary information among different views. In this paper, we introduce a Multi-View Multi-Class Anomaly Detection model (MVMCAD), which integrates information from multiple views to accurately identify anomalies. Specifically, we propose a semi-frozen encoder, where a pre-encoder prior enhancement mechanism is added before the frozen encoder, enabling stable cross-view feature modeling and efficient adaptation for improved anomaly detection. Furthermore, we propose an Anomaly Amplification Module (AAM) that models global token interactions and suppresses normal regions to enhance anomaly signals, leading to improved detection performance in multi-view settings. Finally, we propose a Cross-Feature Loss that aligns shallow encoder features with deep decoder features and vice versa, enhancing the model's sensitivity to anomalies at different semantic levels under multi-view scenarios. Extensive experiments on the Real-IAD dataset for multi-view multi-class anomaly detection validate the effectiveness of our approach, achieving state-of-the-art performance of 91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level, respectively.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching</title>
<link>https://arxiv.org/abs/2504.21302</link>
<guid>https://arxiv.org/abs/2504.21302</guid>
<content:encoded><![CDATA[
<div> Keywords: stereo matching, unsupervised domain adaptation, soft argmin, smooth L1 loss, multi-modal distribution

Summary: 
In this paper, a novel approach called Constrain Multi-modal Distribution (CMD) is introduced to address the issue of multimodal disparity probability distributions in target domains, commonly observed in unsupervised domain adaptation scenarios in stereo matching. The CMD approach incorporates uncertainty-regularized minimization and anisotropic soft argmin techniques to encourage predominantly unimodal disparity distributions in the target domain, leading to improved prediction accuracy. Experimental results show that applying CMD to various stereo-matching networks enhances generalization in both top-performing and domain-adaptable models. The implementation code for CMD is available on GitHub at https://github.com/gallenszl/CMD. <br /><br />Summary: <div>
arXiv:2504.21302v1 Announce Type: new 
Abstract: Recently, learning-based stereo matching methods have achieved great improvement in public benchmarks, where soft argmin and smooth L1 loss play a core contribution to their success. However, in unsupervised domain adaptation scenarios, we observe that these two operations often yield multimodal disparity probability distributions in target domains, resulting in degraded generalization. In this paper, we propose a novel approach, Constrain Multi-modal Distribution (CMD), to address this issue. Specifically, we introduce \textit{uncertainty-regularized minimization} and \textit{anisotropic soft argmin} to encourage the network to produce predominantly unimodal disparity distributions in the target domain, thereby improving prediction accuracy. Experimentally, we apply the proposed method to multiple representative stereo-matching networks and conduct domain adaptation from synthetic data to unlabeled real-world scenes. Results consistently demonstrate improved generalization in both top-performing and domain-adaptable stereo-matching models. The code for CMD will be available at: \href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning</title>
<link>https://arxiv.org/abs/2504.21307</link>
<guid>https://arxiv.org/abs/2504.21307</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, harmful content, fine-tuning, jailbreaking attacks, interpretability

Summary:
Diffusion models have shown excellent generalization capabilities but can memorize and produce harmful content when given specific text prompts. Fine-tuning methods attempt to address this by unlearning harmful concepts, but they are vulnerable to jailbreaking attacks. These attacks reveal that harmful concepts are not fully erased from the model. A new attack method is proposed in this work, utilizing interpretable attack token embeddings to uncover implicit textual components that allow unlearned models to retain harmful concepts. These attack token embeddings are robust and transferable, enabling the development of a defense strategy that is effective against both the proposed attack and existing methods. Experimental results confirm the efficacy of the attack and defense strategies in mitigating the generation of harmful content by diffusion models. 

<br /><br />Summary: <div>
arXiv:2504.21307v1 Announce Type: new 
Abstract: Despite the remarkable generalization capabilities of diffusion models, recent studies have shown that these models can memorize and generate harmful content when prompted with specific text instructions. Although fine-tuning approaches have been developed to mitigate this issue by unlearning harmful concepts, these methods can be easily circumvented through jailbreaking attacks. This indicates that the harmful concept has not been fully erased from the model. However, existing attack methods, while effective, lack interpretability regarding why unlearned models still retain the concept, thereby hindering the development of defense strategies. In this work, we address these limitations by proposing an attack method that learns an orthogonal set of interpretable attack token embeddings. The attack token embeddings can be decomposed into human-interpretable textual elements, revealing that unlearned models still retain the target concept through implicit textual components. Furthermore, these attack token embeddings are robust and transferable across text prompts, initial noises, and unlearned models. Finally, leveraging this diverse set of embeddings, we design a defense method applicable to both our proposed attack and existing attack methods. Experimental results demonstrate the effectiveness of both our attack and defense strategies.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images</title>
<link>https://arxiv.org/abs/2504.21308</link>
<guid>https://arxiv.org/abs/2504.21308</guid>
<content:encoded><![CDATA[
<div> benchmark, quality assessment, text-to-image generation, human images, multidimensional evaluation

Summary:<br />
- The article introduces AGHI-QA, a benchmark specifically for assessing the quality of AI-generated human images, which addresses the lack of fine-grained evaluations in current image quality assessment methods.
- AGHI-QA dataset consists of 4,000 images generated from 400 text prompts using 10 T2I models, with multidimensional annotations collected through a subjective study.
- The study includes perceptual quality scores, text-image correspondence scores, and labeling of visible and distorted body parts in AGHIs.
- The proposed AGHI-Assessor metric combines a large multimodal model with domain-specific human features to predict quality and detect structural distortions in AGHIs.
- Experimental results show that AGHI-Assessor outperforms existing IQA methods and leading LMMs in assessing the quality of AI-generated human images. 

<br /><br />Summary: <div>
arXiv:2504.21308v1 Announce Type: new 
Abstract: The rapid development of text-to-image (T2I) generation approaches has attracted extensive interest in evaluating the quality of generated images, leading to the development of various quality assessment methods for general-purpose T2I outputs. However, existing image quality assessment (IQA) methods are limited to providing global quality scores, failing to deliver fine-grained perceptual evaluations for structurally complex subjects like humans, which is a critical challenge considering the frequent anatomical and textural distortions in AI-generated human images (AGHIs). To address this gap, we introduce AGHI-QA, the first large-scale benchmark specifically designed for quality assessment of AGHIs. The dataset comprises 4,000 images generated from 400 carefully crafted text prompts using 10 state of-the-art T2I models. We conduct a systematic subjective study to collect multidimensional annotations, including perceptual quality scores, text-image correspondence scores, visible and distorted body part labels. Based on AGHI-QA, we evaluate the strengths and weaknesses of current T2I methods in generating human images from multiple dimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that integrates the large multimodal model (LMM) with domain-specific human features for precise quality prediction and identification of visible and distorted body parts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor showcases state-of-the-art performance, significantly outperforming existing IQA methods in multidimensional quality assessment and surpassing leading LMMs in detecting structural distortions in AGHIs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Evaluation of a Visual Question Answering Strategy for Zero-shot Facial Expression Recognition in Still Images</title>
<link>https://arxiv.org/abs/2504.21309</link>
<guid>https://arxiv.org/abs/2504.21309</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial expression recognition, deep learning, zero-shot learning, Visual Language Models, generalization<br />
Summary:<br />
Facial expression recognition (FER) is a significant area in computer vision, but challenges remain in generalizing to new scenarios, especially in zero-shot learning. This study explores the use of Visual Language Models (VLMs) in FER, employing a Visual Question Answering strategy to address the task-specific knowledge gap. The evaluation compares the performance of FER models with and without VLM integration on popular benchmarks like AffectNet, FERPlus, and RAF-DB. The results reveal that some VLMs show promising performance in zero-shot FER scenarios, highlighting the potential for further exploration to enhance generalization in FER tasks. <div>
arXiv:2504.21309v1 Announce Type: new 
Abstract: Facial expression recognition (FER) is a key research area in computer vision and human-computer interaction. Despite recent advances in deep learning, challenges persist, especially in generalizing to new scenarios. In fact, zero-shot FER significantly reduces the performance of state-of-the-art FER models. To address this problem, the community has recently started to explore the integration of knowledge from Large Language Models for visual tasks. In this work, we evaluate a broad collection of locally executed Visual Language Models (VLMs), avoiding the lack of task-specific knowledge by adopting a Visual Question Answering strategy. We compare the proposed pipeline with state-of-the-art FER models, both integrating and excluding VLMs, evaluating well-known FER benchmarks: AffectNet, FERPlus, and RAF-DB. The results show excellent performance for some VLMs in zero-shot FER scenarios, indicating the need for further exploration to improve FER generalization.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation</title>
<link>https://arxiv.org/abs/2504.21325</link>
<guid>https://arxiv.org/abs/2504.21325</guid>
<content:encoded><![CDATA[
<div> diffusion-based AFG, Korean font images, phonetic representations, style encoder, perceptual loss<br />Summary:<br />Automatic Font Generation (AFG) is a challenging task, particularly for complex languages like Korean. Traditional AFG methods face stability issues and struggle with fine detail. This study introduces a diffusion-based AFG method that generates diverse Korean font images using just one reference image. The approach incrementally refines noisy images for stable training and visually appealing results. A key innovation is the text encoder, which processes phonetic representations for accurate character generation, even for unseen characters. Utilizing a pre-trained style encoder and perceptual loss enhances generation quality, focusing on global style. Experimental results showcase the model's ability to consistently produce detailed and authentic Korean font images, surpassing benchmark methods. This method provides a reliable tool for generating various Korean font styles efficiently. <br /><br />Summary: <div>
arXiv:2504.21325v1 Announce Type: new 
Abstract: Automatic font generation (AFG) is the process of creating a new font using only a few examples of the style images. Generating fonts for complex languages like Korean and Chinese, particularly in handwritten styles, presents significant challenges. Traditional AFGs, like Generative adversarial networks (GANs) and Variational Auto-Encoders (VAEs), are usually unstable during training and often face mode collapse problems. They also struggle to capture fine details within font images. To address these problems, we present a diffusion-based AFG method which generates high-quality, diverse Korean font images using only a single reference image, focusing on handwritten and printed styles. Our approach refines noisy images incrementally, ensuring stable training and visually appealing results. A key innovation is our text encoder, which processes phonetic representations to generate accurate and contextually correct characters, even for unseen characters. We used a pre-trained style encoder from DG FONT to effectively and accurately encode the style images. To further enhance the generation quality, we used perceptual loss that guides the model to focus on the global style of generated images. Experimental results on over 2000 Korean characters demonstrate that our model consistently generates accurate and detailed font images and outperforms benchmark methods, making it a reliable tool for generating authentic Korean fonts across different styles.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Visual Artifact Detection in Sora-Generated Videos</title>
<link>https://arxiv.org/abs/2504.21334</link>
<guid>https://arxiv.org/abs/2504.21334</guid>
<content:encoded><![CDATA[
<div> classification framework, visual artifacts, video generation, deep learning, ResNet-50
Summary: 
This study explores visual artifacts in videos generated by the OpenAI model Sora and develops a classification framework targeting common artifact types. By analyzing 300 annotated frames from 15 videos, the researchers trained multiple 2D CNN architectures and achieved an average accuracy of 94.14% with ResNet-50. The study supports VidLLM development by creating evaluation datasets, offering interpretable artifact analysis, and identifying visual risks related to safety and factuality. <br /><br />Summary: <div>
arXiv:2504.21334v1 Announce Type: new 
Abstract: The December 2024 release of OpenAI's Sora, a powerful video generation model driven by natural language prompts, highlights a growing convergence between large language models (LLMs) and video synthesis. As these multimodal systems evolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating, and interacting with visual content, understanding their limitations and ensuring their safe deployment becomes essential. This study investigates visual artifacts frequently found and reported in Sora-generated videos, which can compromise quality, mislead viewers, or propagate disinformation. We propose a multi-label classification framework targeting four common artifact label types: label 1: boundary / edge defects, label 2: texture / noise issues, label 3: movement / joint anomalies, and label 4: object mismatches / disappearances. Using a dataset of 300 manually annotated frames extracted from 15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50, EfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50 achieved an average multi-label classification accuracy of 94.14%. This work supports the broader development of VidLLMs by contributing to (1) the creation of datasets for video quality evaluation, (2) interpretable artifact-based analysis beyond language metrics, and (3) the identification of visual risks relevant to factuality and safety.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation</title>
<link>https://arxiv.org/abs/2504.21336</link>
<guid>https://arxiv.org/abs/2504.21336</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-modal interpretation, Biomedical images, Large Language Models, Segment Anything Model, Grounded interpretation <br />
Summary: <br />
The article introduces UniBiomed, a universal foundation model for grounded biomedical image interpretation that combines a Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM). UniBiomed can handle various biomedical tasks across different imaging modalities, thanks to a curated dataset of over 27 million image-text pairs. The model achieves state-of-the-art performance in segmentation, disease recognition, diagnosis, question answering, and report generation on 84 datasets. Unlike previous models, UniBiomed doesn't require manual crafting of prompts, enabling automated end-to-end interpretation. This revolutionary approach in biomedical AI enhances diagnostic efficiency and accuracy, making a significant impact on clinical workflows. In conclusion, UniBiomed unlocks powerful grounded interpretation capabilities for more effective and precise biomedical image analysis.<br /><br /> <div>
arXiv:2504.21336v1 Announce Type: new 
Abstract: Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability</title>
<link>https://arxiv.org/abs/2504.21340</link>
<guid>https://arxiv.org/abs/2504.21340</guid>
<content:encoded><![CDATA[
<div> transformer model, cervical cell image classification, machine learning, artificial neural network, Kernel SHAP analysis 

Summary: 
The article introduces a novel approach to classifying cervical cell images for cancer screening using the EVA-02 transformer model. The process involves fine-tuning EVA-02, extracting features, selecting important features with multiple machine learning models, and training a new neural network with optional loss weighting for better generalization. The best model achieved an F1-score of 0.85227, surpassing the baseline EVA-02 model. By utilizing Kernel SHAP analysis, key features related to cell morphology and staining characteristics were identified, offering interpretable insights into the model's decision-making process. The code for this work is available on GitHub for reference. <br /><br />Summary: <div>
arXiv:2504.21340v1 Announce Type: new 
Abstract: We propose a novel approach to cervical cell image classification for cervical cancer screening using the EVA-02 transformer model. We developed a four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important features through multiple machine learning models, and training a new artificial neural network with optional loss weighting for improved generalization. With this design, our best model achieved an F1-score of 0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized Kernel SHAP analysis and identified key features correlating with cell morphology and staining characteristics, providing interpretable insights into the decision-making process of the fine-tuned model. Our code is available at https://github.com/Khoa-NT/isbi2025_ps3c.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection</title>
<link>https://arxiv.org/abs/2504.21344</link>
<guid>https://arxiv.org/abs/2504.21344</guid>
<content:encoded><![CDATA[
<div> semantic features, deep features, machine learning models, lung nodule malignancy, lung cancer diagnosis
Summary:
- The research integrates semantic features from radiologists' assessments with a Contrastive Language-Image Pretraining model to predict lung cancer.
- Data from multiple datasets were used to train and evaluate the model, showing superior performance in predicting one-year lung cancer diagnosis compared to state-of-the-art models.
- The model achieved an AUROC of 0.90 and AUPRC of 0.78, demonstrating high accuracy in classifying lung nodules as benign or malignant.
- Utilizing CLIP, the model could provide predictions on semantic features like nodule margin, consistency, and pleural attachment, aiding in explaining model predictions to clinicians.
- This approach ensures the model learns clinically relevant, explainable features and generalizes well across different clinical settings. 
<br /><br />Summary: <div>
arXiv:2504.21344v1 Announce Type: new 
Abstract: Objective: A number of machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, allowing the model to learn clinically relevant, robust, and explainable features for predicting lung cancer. Methods: We obtained 938 low-dose CT scans from the National Lung Screening Trial with 1,246 nodules and semantic features. The Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We finetuned a pretrained Contrastive Language-Image Pretraining model with a parameter-efficient fine-tuning approach to align imaging and semantic features and predict the one-year lung cancer diagnosis. Results: We evaluated the performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and compared it to three state-of-the-art models. Our model demonstrated an AUROC of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on external datasets. Using CLIP, we also obtained predictions on semantic features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and pleural attachment (0.84), that can be used to explain model predictions. Conclusion: Our approach accurately classifies lung nodules as benign or malignant, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing</title>
<link>https://arxiv.org/abs/2504.21356</link>
<guid>https://arxiv.org/abs/2504.21356</guid>
<content:encoded><![CDATA[
<div> Unified multimodal large language models, Nexus-Gen, integration, language reasoning, image synthesis, diffusion models

Summary:

Nexus-Gen is a unified multimodal large language model that combines the language reasoning abilities of LLMs with the image synthesis power of diffusion models. The model uses a dual-phase alignment training process to align the embedding spaces of LLMs and diffusion models. During training, Nexus-Gen addresses a critical discrepancy in the autoregressive paradigm by introducing a prefilled autoregression strategy that improves generation quality. This approach allows the model to comprehensively handle image understanding, generation, and editing tasks. Nexus-Gen's capabilities have been demonstrated through its performance in various multimodal tasks. The model, datasets, and codes are openly available on GitHub to facilitate further research in the field.

Summary: <div>
arXiv:2504.21356v1 Announce Type: new 
Abstract: Unified multimodal large language models (MLLMs) aim to integrate multimodal understanding and generation abilities through a single framework. Despite their versatility, existing open-source unified models exhibit performance gaps against domain-specific architectures. To bridge this gap, we present Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models. To align the embedding space of the LLM and diffusion model, we conduct a dual-phase alignment training process. (1) The autoregressive LLM learns to predict image embeddings conditioned on multimodal inputs, while (2) the vision decoder is trained to reconstruct high-fidelity images from these embeddings. During training the LLM, we identified a critical discrepancy between the autoregressive paradigm's training and inference phases, where error accumulation in continuous embedding space severely degrades generation quality. To avoid this issue, we introduce a prefilled autoregression strategy that prefills input sequence with position-embedded special tokens instead of continuous embeddings. Through dual-phase training, Nexus-Gen has developed the integrated capability to comprehensively address the image understanding, generation and editing tasks. All models, datasets, and codes are published at https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements across the field.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality</title>
<link>https://arxiv.org/abs/2504.21368</link>
<guid>https://arxiv.org/abs/2504.21368</guid>
<content:encoded><![CDATA[
<div> Diffusion autoencoders, noise prediction model, linear-$\beta$ noise schedule, high noise levels, low-quality images <br />
Summary: <br />
Diffusion autoencoders (DAEs) are often trained with high noise levels, resulting in low-quality and blurry images. A new training method is proposed, dividing training into two phases. In the first phase, the DAE is trained as a vanilla autoencoder with high noise levels to populate the latent code with structural information. In the second phase, a noise schedule is used to focus on perfecting details. This approach balances the reconstruction of high-level structures and low-level details, enhancing image quality while preserving latent code properties. <div>
arXiv:2504.21368v1 Announce Type: new 
Abstract: Diffusion autoencoders (DAEs) are typically formulated as a noise prediction model and trained with a linear-$\beta$ noise schedule that spends much of its sampling steps at high noise levels. Because high noise levels are associated with recovering large-scale image structures and low noise levels with recovering details, this configuration can result in low-quality and blurry images. However, it should be possible to improve details while spending fewer steps recovering structures because the latent code should already contain structural information. Based on this insight, we propose a new DAE training method that improves the quality of reconstructed images. We divide training into two phases. In the first phase, the DAE is trained as a vanilla autoencoder by always setting the noise level to the highest, forcing the encoder and decoder to populate the latent code with structural information. In the second phase, we incorporate a noise schedule that spends more time in the low-noise region, allowing the DAE to learn how to perfect the details. Our method results in images that have accurate high-level structures and low-level details while still preserving useful properties of the latent codes.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided Diffusion for Real-world Image Dehazing</title>
<link>https://arxiv.org/abs/2504.21385</link>
<guid>https://arxiv.org/abs/2504.21385</guid>
<content:encoded><![CDATA[
<div> diffusion process, image dehazing, denoising Unet, atmospheric scattering model, domain gap<br />
Summary:<br />
The article introduces Image Dehazing Diffusion Models (IDDM), a novel approach for dehazing real-world images. IDDM leverages the atmospheric scattering model to guide a diffusion process that incorporates noise diffusion for dehazing. A specialized training strategy is designed around IDDM to bridge the domain gap between synthetic and real-world data. During the forward process, IDDM introduces haze and noise into clear images and then separates them during sampling. By training with physics-guided information, IDDM demonstrates domain generalization and effectively restores real-world hazy images despite being trained on synthetic datasets. Extensive experiments validate the effectiveness of IDDM through quantitative and qualitative comparisons with existing methods. <br /><br /> <div>
arXiv:2504.21385v1 Announce Type: new 
Abstract: Due to the domain gap between real-world and synthetic hazy images, current data-driven dehazing algorithms trained on synthetic datasets perform well on synthetic data but struggle to generalize to real-world scenarios. To address this challenge, we propose \textbf{I}mage \textbf{D}ehazing \textbf{D}iffusion \textbf{M}odels (IDDM), a novel diffusion process that incorporates the atmospheric scattering model into noise diffusion. IDDM aims to use the gradual haze formation process to help the denoising Unet robustly learn the distribution of clear images from the conditional input hazy images. We design a specialized training strategy centered around IDDM. Diffusion models are leveraged to bridge the domain gap from synthetic to real-world, while the atmospheric scattering model provides physical guidance for haze formation. During the forward process, IDDM simultaneously introduces haze and noise into clear images, and then robustly separates them during the sampling process. By training with physics-guided information, IDDM shows the ability of domain generalization, and effectively restores the real-world hazy images despite being trained on synthetic datasets. Extensive experiments demonstrate the effectiveness of our method through both quantitative and qualitative comparisons with state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Different Deep Neural Network Models in the Cultural Heritage Domain</title>
<link>https://arxiv.org/abs/2504.21387</link>
<guid>https://arxiv.org/abs/2504.21387</guid>
<content:encoded><![CDATA[
<div> Keywords: computer vision, deep learning, cultural heritage, convolutional neural networks, transformer architectures

Summary:
Testing the abilities of various deep learning architectures in transferring knowledge from generic datasets like ImageNet to cultural heritage tasks, this study compared the performance of VGG, ResNet, DenseNet, Visual Transformer, Swin Transformer, and PoolFormer. The findings revealed that DenseNet demonstrated the best efficiency-computability ratio among the architectures assessed. This research highlights the importance of integrating computer vision and deep learning in preserving and documenting cultural heritage, as well as enhancing visitor experiences. The study underscores the significance of leveraging advanced technologies like convolutional neural networks and transformer architectures in this domain and showcases the potential of deep learning techniques in the realm of cultural heritage preservation and improve user experiences in this context.
<br /><br />Summary: <div>
arXiv:2504.21387v1 Announce Type: new 
Abstract: The integration of computer vision and deep learning is an essential part of documenting and preserving cultural heritage, as well as improving visitor experiences. In recent years, two deep learning paradigms have been established in the field of computer vision: convolutional neural networks and transformer architectures. The present study aims to make a comparative analysis of some representatives of these two techniques of their ability to transfer knowledge from generic dataset, such as ImageNet, to cultural heritage specific tasks. The results of testing examples of the architectures VGG, ResNet, DenseNet, Visual Transformer, Swin Transformer, and PoolFormer, showed that DenseNet is the best in terms of efficiency-computability ratio.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering</title>
<link>https://arxiv.org/abs/2504.21403</link>
<guid>https://arxiv.org/abs/2504.21403</guid>
<content:encoded><![CDATA[
<div> Keywords: video question answering, token selection, adaptive framework, attention-based metric, performance improvement

Summary:
The paper introduces a new approach, EXPLORE-THEN-SELECT, for efficiently handling the large volume of tokens in video question answering tasks. By dynamically adjusting the allocation of static and dynamic information based on question requirements, this strategy optimizes token usage within limited budgets. The framework explores different token allocations and utilizes a query-aware attention-based metric to select the optimal combination without requiring model updates. This plug-and-play solution can be seamlessly integrated into various video-language models. Extensive experiments demonstrate that the proposed method outperforms existing approaches, achieving significant performance improvements of up to 5.8% across different video question answering benchmarks. <div>
arXiv:2504.21403v1 Announce Type: new 
Abstract: Video question answering benefits from the rich information available in videos, enabling a wide range of applications. However, the large volume of tokens generated from longer videos presents significant challenges to memory efficiency and model performance. To alleviate this issue, existing works propose to compress video inputs, but usually overlooking the varying importance of static and dynamic information across different queries, leading to inefficient token usage within limited budgets. To tackle this, we propose a novel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust static and dynamic information needed based on question requirements. Our framework first explores different token allocations between static frames, which preserve spatial details, and dynamic frames, which capture temporal changes. Next, it employs a query-aware attention-based metric to select the optimal token combination without model updates. Our proposed framework is plug-and-play that can be seamlessly integrated within diverse video-language models. Extensive experiments show that our method achieves significant performance improvements (up to 5.8%) among various video question answering benchmarks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining</title>
<link>https://arxiv.org/abs/2504.21414</link>
<guid>https://arxiv.org/abs/2504.21414</guid>
<content:encoded><![CDATA[
<div> Keywords: Cross-domain few-shot segmentation, adaptation, model structures, support samples, domain shift

Summary:
The paper introduces a novel method called Informative Structure Adaptation (ISA) for cross-domain few-shot segmentation (CD-FSS). Instead of retraining existing FSS models for new domains, ISA adapts informative model structures using domain characteristics learned from few-shot labeled support samples during inference. This approach eliminates the need for costly retraining and redesign of CD-FSS models. The method identifies domain-specific model structures using a structure Fisher score and progressively trains selected structures with hierarchically constructed training samples. ISA effectively addresses domain shifts and equips well-trained FSS models with flexible adaptation capabilities. Extensive experiments demonstrate the superior performance of ISA on multiple CD-FSS benchmarks. Overall, ISA provides a cost-effective and efficient solution for adapting FSS models to new domains without the need for full retraining. 

<br /><br />Summary: <div>
arXiv:2504.21414v1 Announce Type: new 
Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes in new domains, which is often challenging due to the diverse characteristics of target domains and the limited availability of support data. Most CD-FSS methods redesign and retrain in-domain FSS models using various domain-generalization techniques, which are effective but costly to train. To address these issues, we propose adapting informative model structures of the well-trained FSS model for target domains by learning domain characteristics from few-shot labeled support samples during inference, thereby eliminating the need for retraining. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing well-trained in-domain FSS models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision</title>
<link>https://arxiv.org/abs/2504.21423</link>
<guid>https://arxiv.org/abs/2504.21423</guid>
<content:encoded><![CDATA[
<div> Diff-Prompt, Prompt learning, Multimodal models, Fine-tuning, Diffusion model <br />
Summary: <br />
This paper introduces Diff-Prompt, a method for generating rich and fine-grained prompts using a diffusion model to enhance performance in complex downstream tasks. The approach involves training a Mask-VAE to compress masks into a latent space, utilizing an improved Diffusion Transformer to train a prompt generator, and aligning the denoising process of the prompt generator with a pre-trained model. Experimental results on a pixel-level task show that Diff-Prompt outperforms other parameter-efficient fine-tuning approaches, achieving significant improvements in R@1 and R@5 metrics. The study highlights the potential of generative models for prompt generation in enhancing multimodal model performance. The code for Diff-Prompt is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2504.21423v1 Announce Type: new 
Abstract: Prompt learning has demonstrated promising results in fine-tuning pre-trained multimodal models. However, the performance improvement is limited when applied to more complex and fine-grained tasks. The reason is that most existing methods directly optimize the parameters involved in the prompt generation process through loss backpropagation, which constrains the richness and specificity of the prompt representations. In this paper, we propose Diffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion model to generate rich and fine-grained prompt information for complex downstream tasks. Specifically, our approach consists of three stages. In the first stage, we train a Mask-VAE to compress the masks into latent space. In the second stage, we leverage an improved Diffusion Transformer (DiT) to train a prompt generator in the latent space, using the masks for supervision. In the third stage, we align the denoising process of the prompt generator with the pre-trained model in the semantic space, and use the generated prompts to fine-tune the model. We conduct experiments on a complex pixel-level downstream task, referring expression comprehension, and compare our method with various parameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum improvement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model and also outperforms other state-of-the-art methods across multiple metrics. The experimental results validate the effectiveness of our approach and highlight the potential of using generative models for prompt generation. Code is available at https://github.com/Kelvin-ywc/diff-prompt.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
<link>https://arxiv.org/abs/2504.21435</link>
<guid>https://arxiv.org/abs/2504.21435</guid>
<content:encoded><![CDATA[
<div> benchmark, series, narrative, MLLMs, PC-DCoT

Summary: 

The article introduces SeriesBench, a new benchmark designed to evaluate the video understanding capabilities of Multi-modal Large Language Models (MLLMs) in the context of narrative-driven series. SeriesBench consists of 105 curated series covering 28 specialized tasks that require deep narrative understanding. The benchmark addresses the limitations of existing benchmarks that focus on standalone videos by emphasizing the complex and continuous narratives often found in series. A novel long-span narrative annotation method and a narrative reasoning framework called PC-DCoT are proposed to enhance model capacity for analyzing plot structures and character relationships within series. Results show that current MLLMs struggle with understanding narrative-driven series, but incorporating PC-DCoT leads to performance improvements. The availability of SeriesBench and the use of PC-DCoT underscore the importance of advancing model capabilities in understanding narrative-driven series for the future development of MLLMs. <div>
arXiv:2504.21435v1 Announce Type: new 
Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on \textbf{standalone} videos and mainly assess ``visual elements'' like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a \textbf{series}. To address this challenge, we propose \textbf{SeriesBench}, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, \textbf{PC-DCoT}. Extensive results on \textbf{SeriesBench} indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while \textbf{PC-DCoT} enables these MLLMs to achieve performance improvements. Overall, our \textbf{SeriesBench} and \textbf{PC-DCoT} highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Visual Layer Selection in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.21447</link>
<guid>https://arxiv.org/abs/2504.21447</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, CLIP-ViT, layer-wise representation similarity, visual layer selection, OCR tasks<br />
Summary:<br />
- The study focuses on the impact of different CLIP-ViT layers on Multimodal large language models (MLLMs), categorizing them into shallow, middle, and deep layers based on behavior similarity.
- Deep layers are crucial for Optical Character Recognition (OCR) tasks, while shallow and middle layers perform better on reasoning tasks requiring counting, positioning, and object localization.
- A fusion of features across all layers outperforms specialized fusion and single-layer selections on 9 out of 10 datasets, demonstrating the importance of utilizing a variety of layers in MLLMs.
- The research provides a systematic analysis of visual layer selection in MLLMs, offering insights for future studies in visual representation learning for these models.<br /><br />Summary: <div>
arXiv:2504.21447v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have achieved impressive performance across a wide range of tasks, typically using CLIP-ViT as their visual encoder due to its strong text-image alignment capabilities. While prior studies suggest that different CLIP-ViT layers capture different types of information, with shallower layers focusing on fine visual details and deeper layers aligning more closely with textual semantics, most MLLMs still select visual features based on empirical heuristics rather than systematic analysis. In this work, we propose a Layer-wise Representation Similarity approach to group CLIP-ViT layers with similar behaviors into {shallow, middle, and deep} categories and assess their impact on MLLM performance. Building on this foundation, we revisit the visual layer selection problem in MLLMs at scale, training LLaVA-style models ranging from 1.4B to 7B parameters. Through extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep layers are essential for OCR tasks; (2) shallow and middle layers substantially outperform deep layers on reasoning tasks involving counting, positioning, and object localization; (3) a lightweight fusion of features across shallow, middle, and deep layers consistently outperforms specialized fusion baselines and single-layer selections, achieving gains on 9 out of 10 datasets. Our work offers the first principled study of visual layer selection in MLLMs, laying the groundwork for deeper investigations into visual representation learning for MLLMs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification</title>
<link>https://arxiv.org/abs/2504.21464</link>
<guid>https://arxiv.org/abs/2504.21464</guid>
<content:encoded><![CDATA[
<div> Keywords: Diabetic retinopathy, deep learning, VR-FuseNet, dataset balancing, XAI

Summary:
The paper introduces VR-FuseNet, a hybrid deep learning model designed for automated diabetic retinopathy detection. By combining VGG19 and ResNet50V2 architectures, the model achieves an accuracy of 91.824%, outperforming individual networks. The dataset used is a hybrid dataset created from five publicly available diabetic retinopathy datasets, addressing issues such as class imbalance and diversity. Preprocessing techniques like SMOTE and CLAHE are applied to enhance dataset robustness and generalizability. For interpretability, the model incorporates explainable artificial intelligence (XAI) techniques to generate visual explanations highlighting relevant retinal features affecting predictions, aiding clinicians in validation and interpretation. The proposed VR-FuseNet model offers an effective and efficient approach to diabetic retinopathy detection, crucial for early intervention and prevention of vision loss. 

<br /><br />Summary: <div>
arXiv:2504.21464v1 Announce Type: new 
Abstract: Diabetic retinopathy is a severe eye condition caused by diabetes where the retinal blood vessels get damaged and can lead to vision loss and blindness if not treated. Early and accurate detection is key to intervention and stopping the disease progressing. For addressing this disease properly, this paper presents a comprehensive approach for automated diabetic retinopathy detection by proposing a new hybrid deep learning model called VR-FuseNet. Diabetic retinopathy is a major eye disease and leading cause of blindness especially among diabetic patients so accurate and efficient automated detection methods are required. To address the limitations of existing methods including dataset imbalance, diversity and generalization issues this paper presents a hybrid dataset created from five publicly available diabetic retinopathy datasets. Essential preprocessing techniques such as SMOTE for class balancing and CLAHE for image enhancement are applied systematically to the dataset to improve the robustness and generalizability of the dataset. The proposed VR-FuseNet model combines the strengths of two state-of-the-art convolutional neural networks, VGG19 which captures fine-grained spatial features and ResNet50V2 which is known for its deep hierarchical feature extraction. This fusion improves the diagnostic performance and achieves an accuracy of 91.824%. The model outperforms individual architectures on all performance metrics demonstrating the effectiveness of hybrid feature extraction in Diabetic Retinopathy classification tasks. To make the proposed model more clinically useful and interpretable this paper incorporates multiple XAI techniques. These techniques generate visual explanations that clearly indicate the retinal features affecting the model's prediction such as microaneurysms, hemorrhages and exudates so that clinicians can interpret and validate.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space</title>
<link>https://arxiv.org/abs/2504.21467</link>
<guid>https://arxiv.org/abs/2504.21467</guid>
<content:encoded><![CDATA[
<div> Point cloud registration, 3D computer vision, multiview, generative approach, POLAR<br />
<br />
Summary:<br />
Point cloud rigid registration in multiview scenarios is addressed by the POLAR method. It efficiently handles a large number of views and high levels of degradations. By using a pretrained autoencoder, a specialized loss function, and a multistart optimization strategy, POLAR outperforms existing approaches on synthetic and real data. This method transposes the registration problem into a latent space, providing robustness to large transformations and initial angles. POLAR is available as a standalone package for easy installation and implementation. <div>
arXiv:2504.21467v1 Announce Type: new 
Abstract: Point cloud rigid registration is a fundamental problem in 3D computer vision. In the multiview case, we aim to find a set of 6D poses to align a set of objects. Methods based on pairwise registration rely on a subsequent synchronization algorithm, which makes them poorly scalable with the number of views. Generative approaches overcome this limitation, but are based on Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence, they are not well suited to handle large transformations. Moreover, most existing methods cannot handle high levels of degradations. In this paper, we introduce POLAR (POint cloud LAtent Registration), a multiview registration method able to efficiently deal with a large number of views, while being robust to a high level of degradations and large initial angles. To achieve this, we transpose the registration problem into the latent space of a pretrained autoencoder, design a loss taking degradations into account, and develop an efficient multistart optimization strategy. Our proposed method significantly outperforms state-of-the-art approaches on synthetic and real data. POLAR is available at github.com/pypolar/polar or as a standalone package which can be installed with pip install polaregistration.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion</title>
<link>https://arxiv.org/abs/2504.21468</link>
<guid>https://arxiv.org/abs/2504.21468</guid>
<content:encoded><![CDATA[
<div> Quaternion matrices, quaternion nuclear norm, Frobenius norm, nonconvex approximation, rank, quaternion singular value decomposition. 

Summary:
The paper introduces the quaternion nuclear norm over the Frobenius norm (QNOF) as a method for recovering hidden structures from incomplete or noisy multi-dimensional data represented by quaternion matrices. QNOF is parameter-free and scale-invariant, offering a novel approach to approximating the rank of quaternion matrices. By utilizing quaternion singular value decomposition, it simplifies the solution to the singular value L1/L2 problem. The QNOF is extended to robust quaternion matrix completion, employing the alternating direction multiplier method to ensure weak convergence under mild conditions. Extensive numerical experiments demonstrate the superiority of the proposed model, consistently outperforming existing quaternion methods. <div>
arXiv:2504.21468v1 Announce Type: new 
Abstract: Recovering hidden structures from incomplete or noisy data remains a pervasive challenge across many fields, particularly where multi-dimensional data representation is essential. Quaternion matrices, with their ability to naturally model multi-dimensional data, offer a promising framework for this problem. This paper introduces the quaternion nuclear norm over the Frobenius norm (QNOF) as a novel nonconvex approximation for the rank of quaternion matrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion singular value decomposition, we prove that solving the QNOF can be simplified to solving the singular value $L_1/L_2$ problem. Additionally, we extend the QNOF to robust quaternion matrix completion, employing the alternating direction multiplier method to derive solutions that guarantee weak convergence under mild conditions. Extensive numerical experiments validate the proposed model's superiority, consistently outperforming state-of-the-art quaternion methods.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Orthogonal NMF with Label Propagation for Image Clustering</title>
<link>https://arxiv.org/abs/2504.21472</link>
<guid>https://arxiv.org/abs/2504.21472</guid>
<content:encoded><![CDATA[
<div> Keywords: Non-negative matrix factorization, supervised learning, label propagation, orthogonal constraints, robustness

Summary:
The article introduces a new approach called robust orthogonal nonnegative matrix factorization (RONMF) to address the limitations of existing NMF methods in image clustering. RONMF incorporates label propagation and graph Laplacian regularization to leverage limited supervised information. It also introduces a non-convex structure to measure reconstruction error and enforces orthogonal constraints on the basis matrix for reduced noise corruption and increased robustness. An efficient ADMM-based optimization algorithm with closed-form solutions for subproblems is proposed to solve RONMF. Experimental evaluations on various image datasets demonstrate superior performance of RONMF compared to state-of-the-art NMF methods in terms of standard metrics and robustness. Code for RONMF implementation will be available on GitHub at https://github.com/slinda-liu.

<br /><br />Summary: 
- Introduction of robust orthogonal nonnegative matrix factorization (RONMF) for image clustering 
- Incorporation of label propagation and graph Laplacian regularization for supervised learning 
- Introduction of non-convex structure and orthogonal constraints for increased robustness 
- Development of an efficient ADMM-based optimization algorithm with closed-form solutions 
- Experimental evaluations demonstrating superior performance and robustness of RONMF compared to existing NMF methods. <div>
arXiv:2504.21472v1 Announce Type: new 
Abstract: Non-negative matrix factorization (NMF) is a popular unsupervised learning approach widely used in image clustering. However, in real-world clustering scenarios, most existing NMF methods are highly sensitive to noise corruption and are unable to effectively leverage limited supervised information. To overcome these drawbacks, we propose a unified non-convex framework with label propagation called robust orthogonal nonnegative matrix factorization (RONMF). This method not only considers the graph Laplacian and label propagation as regularization terms but also introduces a more effective non-convex structure to measure the reconstruction error and imposes orthogonal constraints on the basis matrix to reduce the noise corruption, thereby achieving higher robustness. To solve RONMF, we develop an alternating direction method of multipliers (ADMM)-based optimization algorithm. In particular, all subproblems have closed-form solutions, which ensures its efficiency. Experimental evaluations on eight public image datasets demonstrate that the proposed RONMF outperforms state-of-the-art NMF methods across various standard metrics and shows excellent robustness. The code will be available at https://github.com/slinda-liu.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.21476</link>
<guid>https://arxiv.org/abs/2504.21476</guid>
<content:encoded><![CDATA[
<div> Keywords: GarmentDiffusion, generative model, sewing patterns, multimodal inputs, diffusion transformer

Summary:<br />
- GarmentDiffusion is a new generative model that can create precise 3D sewing patterns from multiple types of inputs, such as text, images, and incomplete sewing patterns.
- The model efficiently encodes sewing pattern parameters into compact edge token representations, resulting in significantly shorter sequences compared to existing models like SewingGPT in DressCode.
- By utilizing a diffusion transformer, GarmentDiffusion can denoise edge tokens along the temporal axis while maintaining a constant number of denoising steps, regardless of dataset-specific statistics.
- The model's design combinations have achieved a sewing pattern generation speed that is 100 times faster than SewingGPT.
- GarmentDiffusion has set new state-of-the-art results on DressCodeData and the largest sewing pattern dataset, GarmentCodeData. The project website provides further information and resources for this innovative approach to generative modeling. 

Summary: <br />GarmentDiffusion is a cutting-edge generative model that revolutionizes the creation of sewing patterns by efficiently processing various types of input and significantly increasing speed and accuracy in pattern generation compared to existing methods. <div>
arXiv:2504.21476v1 Announce Type: new 
Abstract: Garment sewing patterns are fundamental design elements that bridge the gap between design concepts and practical manufacturing. The generative modeling of sewing patterns is crucial for creating diversified garments. However, existing approaches are limited either by reliance on a single input modality or by suboptimal generation efficiency. In this work, we present \textbf{\textit{GarmentDiffusion}}, a new generative model capable of producing centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text, image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing pattern parameters into compact edge token representations, achieving a sequence length that is $\textbf{10}\times$ shorter than that of the autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we simultaneously denoise all edge tokens along the temporal axis, while maintaining a constant number of denoising steps regardless of dataset-specific edge and panel statistics. With all combination of designs of our model, the sewing pattern generation speed is accelerated by $\textbf{100}\times$ compared to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well as on the largest sewing pattern dataset, namely GarmentCodeData. The project website is available at https://shenfu-research.github.io/Garment-Diffusion/.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation</title>
<link>https://arxiv.org/abs/2504.21478</link>
<guid>https://arxiv.org/abs/2504.21478</guid>
<content:encoded><![CDATA[
<div> Keywords: Data-Free Knowledge Distillation, Embedding, Transferability, Efficiency, Image Recognition

Summary:
CAE-DFKD introduces Category-Aware Embedding to Data-Free Knowledge Distillation, enhancing transferability of learned representations beyond image recognition tasks. Key points include:
- CAE-DFKD improves efficiency by modifying the generator training paradigm.
- The method achieves competitive performance with existing DFKD techniques in image recognition.
- CAE-DFKD demonstrates remarkable transferability of data-free learned representations in downstream tasks.
<br /><br />Summary: <div>
arXiv:2504.21478v1 Announce Type: new 
Abstract: Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from the given pre-trained teacher network to the target student model without access to the real training data. Existing DFKD methods focus primarily on improving image recognition performance on associated datasets, often neglecting the crucial aspect of the transferability of learned representations. In this paper, we propose Category-Aware Embedding Data-Free Knowledge Distillation (CAE-DFKD), which addresses at the embedding level the limitations of previous rely on image-level methods to improve model generalization but fail when directly applied to DFKD. The superiority and flexibility of CAE-DFKD are extensively evaluated, including: \textit{\textbf{i.)}} Significant efficiency advantages resulting from altering the generator training paradigm; \textit{\textbf{ii.)}} Competitive performance with existing DFKD state-of-the-art methods on image recognition tasks; \textit{\textbf{iii.)}} Remarkable transferability of data-free learned representations demonstrated in downstream tasks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration</title>
<link>https://arxiv.org/abs/2504.21487</link>
<guid>https://arxiv.org/abs/2504.21487</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, image restoration, universal posterior sampling, high-order solvers, noise estimation

Summary:
Diffusion models have shown significant progress in universal image restoration, but existing methods often suffer from cumulative errors due to substantial step intervals in inference. In this study, a diffusion generalist solver named DGSolver is introduced to address these challenges. By deriving exact ordinary differential equations and implementing high-order solvers with an accelerated sampling strategy, DGSolver enhances both accuracy and efficiency. Universal posterior sampling is integrated to improve noise estimation, leading to more accurate restoration results and error corrections in inverse inference. Extensive experiments demonstrate that DGSolver surpasses current state-of-the-art methods in restoration accuracy, stability, and scalability, as evidenced through qualitative and quantitative evaluations. The code and models for DGSolver will be available on the GitHub repository for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2504.21487v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable progress in universal image restoration. While existing methods speed up inference by reducing sampling steps, substantial step intervals often introduce cumulative errors. Moreover, they struggle to balance the commonality of degradation representations and restoration quality. To address these challenges, we introduce \textbf{DGSolver}, a diffusion generalist solver with universal posterior sampling. We first derive the exact ordinary differential equations for generalist diffusion models and tailor high-order solvers with a queue-based accelerated sampling strategy to improve both accuracy and efficiency. We then integrate universal posterior sampling to better approximate manifold-constrained gradients, yielding a more accurate noise estimation and correcting errors in inverse inference. Extensive experiments show that DGSolver outperforms state-of-the-art methods in restoration accuracy, stability, and scalability, both qualitatively and quantitatively. Code and models will be available at https://github.com/MiliLab/DGSolver.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2504.21491</link>
<guid>https://arxiv.org/abs/2504.21491</guid>
<content:encoded><![CDATA[
<div> fusion architecture, ClassWise-CRF, remote sensing, semantic segmentation, expert networks 

Summary:
The article introduces a result-level category-specific fusion architecture called ClassWise-CRF for semantic segmentation of remote sensing images. This architecture selects expert networks for specific categories and integrates their segmentation predictions using a weighted fusion approach based on segmentation performance. Inspired by Conditional Random Field (CRF), it treats segmentation predictions as confidence vector fields and uses segmentation metrics as priors for fusion. The fusion method dynamically adjusts weights for different categories, optimizing category-specific results. Unary and pairwise potentials in CRF are employed to enhance spatial consistency and boundary accuracy. Experimental results on two datasets show significant improvement in segmentation performance, with mean Intersection over Union (mIoU) metrics increasing on both validation and test sets. The ClassWise-CRF architecture demonstrates effectiveness and generality in semantic segmentation. The code is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2504.21491v1 Announce Type: new 
Abstract: We propose a result-level category-specific fusion architecture called ClassWise-CRF. This architecture employs a two-stage process: first, it selects expert networks that perform well in specific categories from a pool of candidate networks using a greedy algorithm; second, it integrates the segmentation predictions of these selected networks by adaptively weighting their contributions based on their segmentation performance in each category. Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture treats the segmentation predictions from multiple networks as confidence vector fields. It leverages segmentation metrics (such as Intersection over Union) from the validation set as priors and employs an exponential weighting strategy to fuse the category-specific confidence scores predicted by each network. This fusion method dynamically adjusts the weights of each network for different categories, achieving category-specific optimization. Building on this, the architecture further optimizes the fused results using unary and pairwise potentials in CRF to ensure spatial consistency and boundary accuracy. To validate the effectiveness of ClassWise-CRF, we conducted experiments on two remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced semantic segmentation networks. The results show that the ClassWise-CRF architecture significantly improves segmentation performance: on the LoveDA dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on the validation set and by 0.68% on the test set; on the Vaihingen dataset, the mIoU improved by 0.87% on the validation set and by 0.91% on the test set. These results fully demonstrate the effectiveness and generality of the ClassWise-CRF architecture in semantic segmentation of remote sensing images. The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consistency-aware Fake Videos Detection on Short Video Platforms</title>
<link>https://arxiv.org/abs/2504.21495</link>
<guid>https://arxiv.org/abs/2504.21495</guid>
<content:encoded><![CDATA[
<div> Keywords: fake news detection, short video platforms, cross-modal inconsistencies, multimodal feature fusion, probability scores fusion

Summary:
This paper introduces a novel approach for detecting fake news on short video platforms. Existing detection methods struggle with rapidly evolving content manipulation technologies. The proposed method focuses on identifying and leveraging cross-modal inconsistencies as discriminative cues. The approach includes two core modules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative Diagnosis (MMCD). CMCL involves generating pseudo-labels with a Large Language Model and diagnosing cross-modal consistency. MMCD integrates multimodal features through Feature Fusion and Probability Scores Fusion. Experimental results on FakeSV and FakeTT benchmarks demonstrate the model's exceptional performance in fake video detection. The approach improves detection accuracy by utilizing cross-modal inconsistencies as discriminative features. 

<br /><br />Summary: <div>
arXiv:2504.21495v1 Announce Type: new 
Abstract: This paper focuses to detect the fake news on the short video platforms. While significant research efforts have been devoted to this task with notable progress in recent years, current detection accuracy remains suboptimal due to the rapid evolution of content manipulation and generation technologies. Existing approaches typically employ a cross-modal fusion strategy that directly combines raw video data with metadata inputs before applying a classification layer. However, our empirical observations reveal a critical oversight: manipulated content frequently exhibits inter-modal inconsistencies that could serve as valuable discriminative features, yet remain underutilized in contemporary detection frameworks. Motivated by this insight, we propose a novel detection paradigm that explicitly identifies and leverages cross-modal contradictions as discriminative cues. Our approach consists of two core modules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative Diagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal Consistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used to generate pseudo-labels for evaluating cross-modal semantic consistency. Then, CMCD extracts [CLS] tokens and computes cosine loss to quantify cross-modal inconsistencies. MMCD further integrates multimodal features through Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF). MFF employs a co-attention mechanism to enhance semantic interactions across different modalities, while a Transformer is utilized for comprehensive feature fusion. Meanwhile, PSF further integrates the fake news probability scores obtained in the previous step. Extensive experiments on established benchmarks (FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in Fake videos detection.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance</title>
<link>https://arxiv.org/abs/2504.21497</link>
<guid>https://arxiv.org/abs/2504.21497</guid>
<content:encoded><![CDATA[
<div> FLAME model, video face reenactment, latent diffusion framework, 3D face parametric model, motion control<br />
Summary: 
This paper introduces a method for video face reenactment that incorporates a 3D face parametric model into a latent diffusion framework. By utilizing the FLAME model, detailed face geometry and motion features can be accurately extracted from driving videos. The method enhances the latent diffusion model with 3D expression and pose information derived from FLAME sequences. A face movements fusion module with self-attention mechanisms combines identity and motion features spatially. The method allows for parametric alignment of face identity between a reference image and motion captured from a driving video. Experimental results demonstrate high-quality face animations with precise expression and head pose variation modeling, as well as strong generalization performance on out-of-domain images. The code for this method is publicly available on GitHub at https://github.com/weimengting/MagicPortrait. <div>
arXiv:2504.21497v1 Announce Type: new 
Abstract: In this paper, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This enables precise extraction of detailed face geometry and motion features from driving videos. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. A multi-layer face movements fusion module with integrated self-attention mechanisms is used to combine identity and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at https://github.com/weimengting/MagicPortrait.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks</title>
<link>https://arxiv.org/abs/2504.21544</link>
<guid>https://arxiv.org/abs/2504.21544</guid>
<content:encoded><![CDATA[
<div> Adapter, Fine-tuning, 3D segmentation, Electron microscopy, Neural structures

Summary:
The article introduces SAM4EM, a new method for 3D segmentation of complex neural structures in electron microscopy data. It utilizes the Segment Anything Model (SAM) and incorporates advanced fine-tuning techniques. Key contributions include a prompt-free adapter for SAM, a dual-stage fine-tuning approach using Low-Rank Adaptation (LoRA), and a 3D memory attention mechanism for consistency in segmentation. A benchmark dataset for astrocytic processes and synapses is also released. The method demonstrates improved accuracy in segmenting mitochondria, glia, and synapses compared to state-of-the-art methods, including SAM-based adapters from the medical field. Results indicate better performance in segmenting challenging structures like glia and post-synaptic densities. Code and models are available on GitHub for further research. 

<br /><b>Summary:</b> <div>
arXiv:2504.21544v1 Announce Type: new 
Abstract: We present SAM4EM, a novel approach for 3D segmentation of complex neural structures in electron microscopy (EM) data by leveraging the Segment Anything Model (SAM) alongside advanced fine-tuning strategies. Our contributions include the development of a prompt-free adapter for SAM using two stage mask decoding to automatically generate prompt embeddings, a dual-stage fine-tuning method based on Low-Rank Adaptation (LoRA) for enhancing segmentation with limited annotated data, and a 3D memory attention mechanism to ensure segmentation consistency across 3D stacks. We further release a unique benchmark dataset for the segmentation of astrocytic processes and synapses. We evaluated our method on challenging neuroscience segmentation benchmarks, specifically targeting mitochondria, glia, and synapses, with significant accuracy improvements over state-of-the-art (SOTA) methods, including recent SAM-based adapters developed for the medical domain and other vision transformer-based approaches. Experimental results indicate that our approach outperforms existing solutions in the segmentation of complex processes like glia and post-synaptic densities. Our code and models are available at https://github.com/Uzshah/SAM4EM.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models</title>
<link>https://arxiv.org/abs/2504.21559</link>
<guid>https://arxiv.org/abs/2504.21559</guid>
<content:encoded><![CDATA[
<div> visual prompting, object hallucination, large vision language models, black-box, engineering <br />
Summary: <br />
The article introduces the concept of using visual prompts to mitigate object hallucination in Large Vision Language Models (LVLMs). Simple visual cues overlaid on images can significantly reduce object hallucination, but the effectiveness varies among different visual prompts (VPs). To address this, the Black-Box Visual Prompt Engineering (BBVPE) framework is proposed, which identifies optimal VPs without requiring access to model internals. By training a router model to dynamically select the most effective VP for each input image, BBVPE offers a model-agnostic solution applicable to both open-source and proprietary LVLMs. Evaluations on benchmarks like POPE and CHAIR show that BBVPE successfully reduces object hallucination, enhancing the reliability of LVLMs. <div>
arXiv:2504.21559v1 Announce Type: new 
Abstract: Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting -- overlaying visual cues (e.g., bounding box, circle) on images -- can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals. Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image. This black-box approach is model-agnostic, making it applicable to both open-source and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR demonstrate that BBVPE effectively reduces object hallucination.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Trajectory Exploration for Multimodal Agents</title>
<link>https://arxiv.org/abs/2504.21561</link>
<guid>https://arxiv.org/abs/2504.21561</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal agents, online self-exploration, preference optimization, task synthesis, step-wise learning

Summary:
Multimodal agents with integrated controllers have shown great capabilities in complex tasks but require expert data for fine-tuning. The paper introduces SPORT, an online self-exploration method for these agents. SPORT uses step-wise preference optimization to refine agent trajectories without expert annotation. It operates through task synthesis, step sampling, step verification, and preference tuning components. Tasks are generated using language models, and a search scheme alternates step sampling and verification to solve them. An AI verifier provides feedback for constructing preference data used to update the controller's policy. The SPORT Agent evolves and improves its capabilities through interaction with real environments. Evaluation on benchmarks demonstrates significant improvements in performance. The method showcases generalization and effectiveness in agent learning.<br /><br />Summary: <div>
arXiv:2504.21561v1 Announce Type: new 
Abstract: Multimodal agents, which integrate a controller (e.g., a large language model) with external tools, have demonstrated remarkable capabilities in tackling complex tasks. However, existing agents need to collect a large number of expert data for fine-tuning to adapt to new environments. In this paper, we propose an online self-exploration method for multimodal agents, namely SPORT, via step-wise preference optimization to refine the trajectories of agents, which automatically generates tasks and learns from solving the generated tasks, without any expert annotation. SPORT operates through four iterative components: task synthesis, step sampling, step verification, and preference tuning. First, we synthesize multi-modal tasks using language models. Then, we introduce a novel search scheme, where step sampling and step verification are executed alternately to solve each generated task. We employ a verifier to provide AI feedback to construct step-wise preference data. The data is subsequently used to update the controller's policy through preference tuning, producing a SPORT Agent. By interacting with real environments, the SPORT Agent evolves into a more refined and capable system. Evaluation in the GTA and GAIA benchmarks show that the SPORT Agent achieves 6.41\% and 3.64\% improvements, underscoring the generalization and effectiveness introduced by our method. The project page is https://SPORT-Agents.github.io.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes</title>
<link>https://arxiv.org/abs/2504.21562</link>
<guid>https://arxiv.org/abs/2504.21562</guid>
<content:encoded><![CDATA[
<div> Keywords: Wireless Capsule Endoscopy, Neural Cellular Automata, Bleeding Segmentation, Depth Estimation, ESP32 microcontroller

Summary:
Wireless Capsule Endoscopy is a non-invasive imaging method for the gastrointestinal tract, providing pain-free imaging. The study focuses on utilizing Neural Cellular Automata (NCA) for bleeding segmentation and depth estimation in capsule endoscopic images. The NCA models are trained and optimized to run efficiently on the ESP32 microcontroller, enabling image processing on small hardware like camera capsules. The NCA models show higher accuracy in segmentation compared to other portable models, while requiring significantly fewer parameters. The depth estimation results of NCA on the microcontroller are visually impressive and outperform pseudo ground truth in some cases. Runtime optimizations on the ESP32-S3 further improve the inference speed. This research marks the first successful implementation of reliable bleeding segmentation and depth estimation on a miniaturized device, opening up possibilities for precise diagnosis and capsule localization on the capsule itself.<br /><br />Summary: <div>
arXiv:2504.21562v1 Announce Type: new 
Abstract: Wireless Capsule Endoscopy is a non-invasive imaging method for the entire gastrointestinal tract, and is a pain-free alternative to traditional endoscopy. It generates extensive video data that requires significant review time, and localizing the capsule after ingestion is a challenge. Techniques like bleeding detection and depth estimation can help with localization of pathologies, but deep learning models are typically too large to run directly on the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and depth estimation are trained on capsule endoscopic images. For monocular depth estimation, we distill a large foundation model into the lean NCA architecture, by treating the outputs of the foundation model as pseudo ground truth. We then port the trained NCA to the ESP32 microcontroller, enabling efficient image processing on hardware as small as a camera capsule. NCA are more accurate (Dice) than other portable segmentation models, while requiring more than 100x fewer parameters stored in memory than other small-scale models. The visual results of NCA depth estimation look convincing, and in some cases beat the realism and detail of the pseudo ground truth. Runtime optimizations on the ESP32-S3 accelerate the average inference speed significantly, by more than factor 3. With several algorithmic adjustments and distillation, it is possible to eNCApsulate NCA models into microcontrollers that fit into wireless capsule endoscopes. This is the first work that enables reliable bleeding segmentation and depth estimation on a miniaturized device, paving the way for precise diagnosis combined with visual odometry as a means of precise localization of the capsule -- on the capsule.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascade Detector Analysis and Application to Biomedical Microscopy</title>
<link>https://arxiv.org/abs/2504.21598</link>
<guid>https://arxiv.org/abs/2504.21598</guid>
<content:encoded><![CDATA[
<div> cascade detectors, computer vision, biomedical datasets, multiresolution images, classifier calls

Summary:
cascade detectors are used to efficiently identify sparse objects in multiresolution images, taking into account object prevalence and detector accuracies. The study provides a framework to calculate accuracy and expected classifier calls for cascade detectors, applicable across dimensions and cascade levels. Comparison between one- and two-level detectors in various microscopy tasks like fluorescent cell detection, organelle segmentation, and tissue segmentation shows that the multi-level detector achieves similar performance in significantly less time (30-75% less). The efficiency of multi-level detectors makes them a compelling choice for tasks in computer vision models and biomedical datasets. <br /><br />Summary: <div>
arXiv:2504.21598v1 Announce Type: new 
Abstract: As both computer vision models and biomedical datasets grow in size, there is an increasing need for efficient inference algorithms. We utilize cascade detectors to efficiently identify sparse objects in multiresolution images. Given an object's prevalence and a set of detectors at different resolutions with known accuracies, we derive the accuracy, and expected number of classifier calls by a cascade detector. These results generalize across number of dimensions and number of cascade levels. Finally, we compare one- and two-level detectors in fluorescent cell detection, organelle segmentation, and tissue segmentation across various microscopy modalities. We show that the multi-level detector achieves comparable performance in 30-75% less time. Our work is compatible with a variety of computer vision models and data domains.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection</title>
<link>https://arxiv.org/abs/2504.21614</link>
<guid>https://arxiv.org/abs/2504.21614</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, data selection, Intelligent Transportation Systems, open-source, Mcity Data Engine

Summary:
The article introduces the Mcity Data Engine, an open-source system designed to assist researchers in the selection and labeling of appropriate samples for machine learning model training, particularly focusing on rare and novel classes in the field of Intelligent Transportation Systems (ITS). With the increasing availability of data in this domain, the challenge lies in detecting long-tail classes of interest within vast amounts of unlabeled data. The Mcity Data Engine addresses this issue by providing modules that cover the entire data development cycle, from acquisition to model deployment, with a specific emphasis on rare classes through an open-vocabulary data selection process. Importantly, the system is openly available on GitHub under an MIT license, enabling researchers and the open-source community to benefit from its functionalities in optimizing data selection and model training processes for ITS applications. Overall, the Mcity Data Engine offers a valuable tool for enhancing the efficiency and effectiveness of machine learning projects within the realm of Intelligent Transportation Systems. 

<br /><br />Summary: <div>
arXiv:2504.21614v1 Announce Type: new 
Abstract: With an ever-increasing availability of data, it has become more and more challenging to select and label appropriate samples for the training of machine learning models. It is especially difficult to detect long-tail classes of interest in large amounts of unlabeled data. This holds especially true for Intelligent Transportation Systems (ITS), where vehicle fleets and roadside perception systems generate an abundance of raw data. While industrial, proprietary data engines for such iterative data selection and model training processes exist, researchers and the open-source community suffer from a lack of an openly available system. We present the Mcity Data Engine, which provides modules for the complete data-based development cycle, beginning at the data acquisition phase and ending at the model deployment stage. The Mcity Data Engine focuses on rare and novel classes through an open-vocabulary data selection process. All code is publicly available on GitHub under an MIT license: https://github.com/mcity/mcity_data_engine
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection</title>
<link>https://arxiv.org/abs/2504.21646</link>
<guid>https://arxiv.org/abs/2504.21646</guid>
<content:encoded><![CDATA[
<div> diffusion-based adversarial identity manipulation, privacy, face recognition, transferability, black-box attack

Summary: 
The paper introduces the DiffAIM method for generating natural and highly transferable adversarial faces to protect privacy against face recognition systems. The approach manipulates facial identity within a diffusion model's low-dimensional latent space by injecting gradient-based adversarial guidance during the reverse diffusion process. This results in the generation of desired adversarial faces that can effectively impersonate while maintaining visual naturalness. The method also includes structure-preserving regularization to ensure consistency in facial structure during manipulation. Experimental results show that DiffAIM outperforms existing methods in terms of black-box attack transferability and visual quality. The approach is tested on face verification and identification tasks, showcasing its effectiveness against commercial face recognition APIs such as Face++ and Aliyun. <div>
arXiv:2504.21646v1 Announce Type: new 
Abstract: The success of face recognition (FR) systems has led to serious privacy concerns due to potential unauthorized surveillance and user tracking on social networks. Existing methods for enhancing privacy fail to generate natural face images that can protect facial privacy. In this paper, we propose diffusion-based adversarial identity manipulation (DiffAIM) to generate natural and highly transferable adversarial faces against malicious FR systems. To be specific, we manipulate facial identity within the low-dimensional latent space of a diffusion model. This involves iteratively injecting gradient-based adversarial identity guidance during the reverse diffusion process, progressively steering the generation toward the desired adversarial faces. The guidance is optimized for identity convergence towards a target while promoting semantic divergence from the source, facilitating effective impersonation while maintaining visual naturalness. We further incorporate structure-preserving regularization to preserve facial structure consistency during manipulation. Extensive experiments on both face verification and identification tasks demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger black-box attack transferability while maintaining superior visual quality. We also demonstrate the effectiveness of the proposed approach for commercial FR APIs, including Face++ and Aliyun.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation</title>
<link>https://arxiv.org/abs/2504.21650</link>
<guid>https://arxiv.org/abs/2504.21650</guid>
<content:encoded><![CDATA[
<div> Video Diffusion Models, VR, AR, Panoramic Videos, 4D Scene Reconstruction

Summary:
The article introduces HoloTime, a framework that combines video diffusion models and 360-degree 4D scene reconstruction to enhance user experiences in VR and AR applications. The framework utilizes the 360World dataset to generate high-quality panoramic videos using the Panoramic Animator model. This is followed by the Panoramic Space-Time Reconstruction method, which transforms the videos into 4D point clouds for spatial and temporal consistency. Comparative analysis shows the superiority of the proposed method in panoramic video generation and 4D scene reconstruction, leading to more engaging and realistic immersive environments for users. This advancement has the potential to revolutionize the application of VR and AR technologies by providing truly immersive experiences through scene-level 4D assets. 

<br /><br />Summary: <div>
arXiv:2504.21650v1 Announce Type: new 
Abstract: The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Text Processing: A Comprehensive Review and Unified Evaluation</title>
<link>https://arxiv.org/abs/2504.21682</link>
<guid>https://arxiv.org/abs/2504.21682</guid>
<content:encoded><![CDATA[
arXiv:2504.21682v1 Announce Type: new 
Abstract: Visual text is a crucial component in both document and scene images, conveying rich semantic information and attracting significant attention in the computer vision community. Beyond traditional tasks such as text detection and recognition, visual text processing has witnessed rapid advancements driven by the emergence of foundation models, including text image reconstruction and text image manipulation. Despite significant progress, challenges remain due to the unique properties that differentiate text from general objects. Effectively capturing and leveraging these distinct textual characteristics is essential for developing robust visual text processing models. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in visual text processing, focusing on two key questions: (1) What textual features are most suitable for different visual text processing tasks? (2) How can these distinctive text features be effectively incorporated into processing frameworks? Furthermore, we introduce VTPBench, a new benchmark that encompasses a broad range of visual text processing datasets. Leveraging the advanced visual quality assessment capabilities of multimodal large language models (MLLMs), we propose VTPScore, a novel evaluation metric designed to ensure fair and reliable evaluation. Our empirical study with more than 20 specific models reveals substantial room for improvement in the current techniques. Our aim is to establish this work as a fundamental resource that fosters future exploration and innovation in the dynamic field of visual text processing. The relevant repository is available at https://github.com/shuyansy/Visual-Text-Processing-survey.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction</title>
<link>https://arxiv.org/abs/2504.21692</link>
<guid>https://arxiv.org/abs/2504.21692</guid>
<content:encoded><![CDATA[
arXiv:2504.21692v1 Announce Type: new 
Abstract: Successful video analysis relies on accurate recognition of pixels across frames, and frame reconstruction methods based on video correspondence learning are popular due to their efficiency. Existing frame reconstruction methods, while efficient, neglect the value of direct involvement of multiple reference frames for reconstruction and decision-making aspects, especially in complex situations such as occlusion or fast movement. In this paper, we introduce a Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple reference frames to concisely and directly enhance frame reconstruction. Its core component is a Reference Frame Memory Engine that dynamically selects frames based on object pixel features to improve tracking accuracy. In addition, a Bidirectional Target Prediction Network is built to utilize multiple reference frames to improve the robustness of the model. Through experiments, our algorithm outperforms the state-of-the-art self-supervised techniques on two fine-grained video object tracking tasks: object segmentation and keypoint tracking.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining</title>
<link>https://arxiv.org/abs/2504.21699</link>
<guid>https://arxiv.org/abs/2504.21699</guid>
<content:encoded><![CDATA[
arXiv:2504.21699v1 Announce Type: new 
Abstract: Sensor degradation poses a significant challenge in autonomous driving. During heavy rainfall, the interference from raindrops can adversely affect the quality of LiDAR point clouds, resulting in, for instance, inaccurate point measurements. This, in turn, can potentially lead to safety concerns if autonomous driving systems are not weather-aware, i.e., if they are unable to discern such changes. In this study, we release a new, large-scale, multi-modal emulated rain dataset, REHEARSE-3D, to promote research advancements in 3D point cloud de-raining. Distinct from the most relevant competitors, our dataset is unique in several respects. First, it is the largest point-wise annotated dataset, and second, it is the only one with high-resolution LiDAR data (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and nighttime conditions in a controlled weather environment. Furthermore, REHEARSE-3D involves rain-characteristic information, which is of significant value not only for sensor noise modeling but also for analyzing the impact of weather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop detection and removal in fused LiDAR and 4D Radar point clouds. Our comprehensive study further evaluates the performance of various statistical and deep-learning models. Upon publication, the dataset and benchmark models will be made publicly available at: https://sporsho.github.io/REHEARSE3D.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformers in Precision Agriculture: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2504.21706</link>
<guid>https://arxiv.org/abs/2504.21706</guid>
<content:encoded><![CDATA[
arXiv:2504.21706v1 Announce Type: new 
Abstract: Detecting plant diseases is a crucial aspect of modern agriculture - it plays a key role in maintaining crop health and increasing overall yield. Traditional approaches, though still valuable, often rely on manual inspection or conventional machine learning techniques, both of which face limitations in scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as a promising alternative, offering benefits such as improved handling of long-range dependencies and better scalability for visual tasks. This survey explores the application of ViTs in precision agriculture, covering tasks from classification to detection and segmentation. We begin by introducing the foundational architecture of ViTs and discuss their transition from Natural Language Processing (NLP) to computer vision. The discussion includes the concept of inductive bias in traditional models like Convolutional Neural Networks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive review of recent literature, focusing on key methodologies, datasets, and performance metrics. The survey also includes a comparative analysis of CNNs and ViTs, with a look at hybrid models and performance enhancements. Technical challenges - such as data requirements, computational demands, and model interpretability - are addressed alongside potential solutions. Finally, we outline potential research directions and technological advancements that could further support the integration of ViTs in real-world agricultural settings. Our goal with this study is to offer practitioners and researchers a deeper understanding of how ViTs are poised to transform smart and precision agriculture.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction</title>
<link>https://arxiv.org/abs/2504.21718</link>
<guid>https://arxiv.org/abs/2504.21718</guid>
<content:encoded><![CDATA[
arXiv:2504.21718v1 Announce Type: new 
Abstract: Generating responsive listener head dynamics with nuanced emotions and expressive reactions is crucial for practical dialogue modeling in various virtual avatar animations. Previous studies mainly focus on the direct short-term production of listener behavior. They overlook the fine-grained control over motion variations and emotional intensity, especially in long-sequence modeling. Moreover, the lack of long-term and large-scale paired speaker-listener corpora including head dynamics and fine-grained multi-modality annotations (e.g., text-based expression descriptions, emotional intensity) also limits the application of dialogue modeling.Therefore, we first newly collect a large-scale multi-turn dataset of 3D dyadic conversation containing more than 1.4M valid frames for multi-modal responsive interaction, dubbed ListenerX. Additionally, we propose VividListener, a novel framework enabling fine-grained, expressive and controllable listener dynamics modeling. This framework leverages multi-modal conditions as guiding principles for fostering coherent interactions between speakers and listeners.Specifically, we design the Responsive Interaction Module (RIM) to adaptively represent the multi-modal interactive embeddings. RIM ensures the listener dynamics achieve fine-grained semantic coordination with textual descriptions and adjustments, while preserving expressive reaction with speaker behavior. Meanwhile, we design the Emotional Intensity Tags (EIT) for emotion intensity editing with multi-modal information integration, applying to both text descriptions and listener motion amplitude.Extensive experiments conducted on our newly collected ListenerX dataset demonstrate that VividListener achieves state-of-the-art performance, realizing expressive and controllable listener dynamics.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space</title>
<link>https://arxiv.org/abs/2504.21749</link>
<guid>https://arxiv.org/abs/2504.21749</guid>
<content:encoded><![CDATA[
arXiv:2504.21749v1 Announce Type: new 
Abstract: 3D morphable models (3DMMs) are a powerful tool to represent the possible shapes and appearances of an object category. Given a single test image, 3DMMs can be used to solve various tasks, such as predicting the 3D shape, pose, semantic correspondence, and instance segmentation of an object. Unfortunately, 3DMMs are only available for very few object categories that are of particular interest, like faces or human bodies, as they require a demanding 3D data acquisition and category-specific training process. In contrast, we introduce a new method, Common3D, that learns 3DMMs of common objects in a fully self-supervised manner from a collection of object-centric videos. For this purpose, our model represents objects as a learned 3D template mesh and a deformation field that is parameterized as an image-conditioned neural network. Different from prior works, Common3D represents the object appearance with neural features instead of RGB colors, which enables the learning of more generalizable representations through an abstraction from pixel intensities. Importantly, we train the appearance features using a contrastive objective by exploiting the correspondences defined through the deformable template mesh. This leads to higher quality correspondence features compared to related works and a significantly improved model performance at estimating 3D object pose and semantic correspondence. Common3D is the first completely self-supervised method that can solve various vision tasks in a zero-shot manner.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anatomical Similarity as a New Metric to Evaluate Brain Generative Models</title>
<link>https://arxiv.org/abs/2504.21771</link>
<guid>https://arxiv.org/abs/2504.21771</guid>
<content:encoded><![CDATA[
arXiv:2504.21771v1 Announce Type: new 
Abstract: Generative models enhance neuroimaging through data augmentation, quality improvement, and rare condition studies. Despite advances in realistic synthetic MRIs, evaluations focus on texture and perception, lacking sensitivity to crucial anatomical fidelity. This study proposes a new metric, called WASABI (Wasserstein-Based Anatomical Brain Index), to assess the anatomical realism of synthetic brain MRIs. WASABI leverages \textit{SynthSeg}, a deep learning-based brain parcellation tool, to derive volumetric measures of brain regions in each MRI and uses the multivariate Wasserstein distance to compare distributions between real and synthetic anatomies. Based on controlled experiments on two real datasets and synthetic MRIs from five generative models, WASABI demonstrates higher sensitivity in quantifying anatomical discrepancies compared to traditional image-level metrics, even when synthetic images achieve near-perfect visual quality. Our findings advocate for shifting the evaluation paradigm beyond visual inspection and conventional metrics, emphasizing anatomical fidelity as a crucial benchmark for clinically meaningful brain MRI synthesis. Our code is available at https://github.com/BahramJafrasteh/wasabi-mri.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation</title>
<link>https://arxiv.org/abs/2504.21789</link>
<guid>https://arxiv.org/abs/2504.21789</guid>
<content:encoded><![CDATA[
arXiv:2504.21789v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI) plays an important role in identifying clinically significant prostate cancer (csPCa), yet automated methods face challenges such as data imbalance, variable tumor sizes, and a lack of annotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which incorporates anomaly maps derived from biparametric MRI sequences into a deep learning-based segmentation framework to improve csPCa identification. We conduct a comparative analysis of anomaly detection methods and evaluate the integration of anomaly maps into the segmentation pipeline. Anomaly maps, generated using Fixed-Point GAN reconstruction, highlight deviations from normal prostate tissue, guiding the segmentation model to potential cancerous regions. We compare the performance by using the average score, computed as the mean of the AUROC and Average Precision (AP). On the external test set, adU-Net achieves the best average score of 0.618, outperforming the baseline nnU-Net model (0.605). The results demonstrate that incorporating anomaly detection into segmentation improves generalization and performance, particularly with ADC-based anomaly maps, offering a promising direction for automated csPCa identification.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A simple and effective approach for body part recognition on CT scans based on projection estimation</title>
<link>https://arxiv.org/abs/2504.21810</link>
<guid>https://arxiv.org/abs/2504.21810</guid>
<content:encoded><![CDATA[
arXiv:2504.21810v1 Announce Type: new 
Abstract: It is well known that machine learning models require a high amount of annotated data to obtain optimal performance. Labelling Computed Tomography (CT) data can be a particularly challenging task due to its volumetric nature and often missing and$/$or incomplete associated meta-data. Even inspecting one CT scan requires additional computer software, or in the case of programming languages $-$ additional programming libraries. This study proposes a simple, yet effective approach based on 2D X-ray-like estimation of 3D CT scans for body region identification. Although body region is commonly associated with the CT scan, it often describes only the focused major body region neglecting other anatomical regions present in the observed CT. In the proposed approach, estimated 2D images were utilized to identify 14 distinct body regions, providing valuable information for constructing a high-quality medical dataset. To evaluate the effectiveness of the proposed method, it was compared against 2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed the others, where it came on top with statistical significance and F1-Score for the best-performing model EffNet-B0 of 0.980 $\pm$ 0.016 in comparison to the 0.840 $\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\pm$ 0.096 (3D VoxCNN), and 0.852 $\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three different clinical centers and counted 15,622 CT scans (44,135 labels).
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields</title>
<link>https://arxiv.org/abs/2504.21814</link>
<guid>https://arxiv.org/abs/2504.21814</guid>
<content:encoded><![CDATA[
arXiv:2504.21814v1 Announce Type: new 
Abstract: The rapid development of AIGC foundation models has revolutionized the paradigm of image compression, which paves the way for the abandonment of most pixel-level transform and coding, compelling us to ask: why compress what you can generate if the AIGC foundation model is powerful enough to faithfully generate intricate structure and fine-grained details from nothing more than some compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o image generation of OpenAI has achieved impressive cross-modality generation, editing, and design capabilities, which motivates us to answer the above question by exploring its potential in image compression fields. In this work, we investigate two typical compression paradigms: textual coding and multimodal coding (i.e., text + extremely low-resolution image), where all/most pixel-level information is generated instead of compressing via the advanced GPT-4o image generation function. The essential challenge lies in how to maintain semantic and structure consistency during the decoding process. To overcome this, we propose a structure raster-scan prompt engineering mechanism to transform the image into textual space, which is compressed as the condition of GPT-4o image generation. Extensive experiments have shown that the combination of our designed structural raster-scan prompts and GPT-4o's image generation function achieved the impressive performance compared with recent multimodal/generative image compression at ultra-low bitrate, further indicating the potential of AIGC generation in image compression fields.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization</title>
<link>https://arxiv.org/abs/2504.21831</link>
<guid>https://arxiv.org/abs/2504.21831</guid>
<content:encoded><![CDATA[
arXiv:2504.21831v1 Announce Type: new 
Abstract: We introduce DEEVISum (Distilled Early Exit Vision language model for Summarization), a lightweight, efficient, and scalable vision language model designed for segment wise video summarization. Leveraging multi modal prompts that combine textual and audio derived signals, DEEVISum incorporates Multi Stage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance between performance and efficiency. MSKD offers a 1.33% absolute F1 improvement over baseline distillation (0.5%), while EE reduces inference time by approximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset, our best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing the performance of significantly larger models, all while maintaining a lower computational footprint. We publicly release our code and processed dataset to support further research.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Stylization via Large Reconstruction Model</title>
<link>https://arxiv.org/abs/2504.21836</link>
<guid>https://arxiv.org/abs/2504.21836</guid>
<content:encoded><![CDATA[
arXiv:2504.21836v1 Announce Type: new 
Abstract: With the growing success of text or image guided 3D generators, users demand more control over the generation process, appearance stylization being one of them. Given a reference image, this requires adapting the appearance of a generated 3D asset to reflect the visual style of the reference while maintaining visual consistency from multiple viewpoints. To tackle this problem, we draw inspiration from the success of 2D stylization methods that leverage the attention mechanisms in large image generation models to capture and transfer visual style. In particular, we probe if large reconstruction models, commonly used in the context of 3D generation, has a similar capability. We discover that the certain attention blocks in these models capture the appearance specific features. By injecting features from a visual style image to such blocks, we develop a simple yet effective 3D appearance stylization method. Our method does not require training or test time optimization. Through both quantitative and qualitative evaluations, we demonstrate that our approach achieves superior results in terms of 3D appearance stylization, significantly improving efficiency while maintaining high-quality visual outcomes.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Light Modulation to Counter Manipulation of Speech Visual Content</title>
<link>https://arxiv.org/abs/2504.21846</link>
<guid>https://arxiv.org/abs/2504.21846</guid>
<content:encoded><![CDATA[
arXiv:2504.21846v1 Announce Type: new 
Abstract: High-profile speech videos are prime targets for falsification, owing to their accessibility and influence. This work proposes Spotlight, a low-overhead and unobtrusive system for protecting live speech videos from visual falsification of speaker identity and lip and facial motion. Unlike predominant falsification detection methods operating in the digital domain, Spotlight creates dynamic physical signatures at the event site and embeds them into all video recordings via imperceptible modulated light. These physical signatures encode semantically-meaningful features unique to the speech event, including the speaker's identity and facial motion, and are cryptographically-secured to prevent spoofing. The signatures can be extracted from any video downstream and validated against the portrayed speech content to check its integrity. Key elements of Spotlight include (1) a framework for generating extremely compact (i.e., 150-bit), pose-invariant speech video features, based on locality-sensitive hashing; and (2) an optical modulation scheme that embeds >200 bps into video while remaining imperceptible both in video and live. Prototype experiments on extensive video datasets show Spotlight achieves AUCs $\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified videos. Further, Spotlight is highly robust across recording conditions, video post-processing techniques, and white-box adversarial attacks on its video feature extraction methodologies.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Room Acoustic Rendering with Multi-View Vision Priors</title>
<link>https://arxiv.org/abs/2504.21847</link>
<guid>https://arxiv.org/abs/2504.21847</guid>
<content:encoded><![CDATA[
arXiv:2504.21847v1 Announce Type: new 
Abstract: An immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. However, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic Rendering (AV-DAR), a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. Experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from 16.6% to 50.9% when trained at the same scale.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning</title>
<link>https://arxiv.org/abs/2504.21850</link>
<guid>https://arxiv.org/abs/2504.21850</guid>
<content:encoded><![CDATA[
arXiv:2504.21850v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Interactive Generative Video</title>
<link>https://arxiv.org/abs/2504.21853</link>
<guid>https://arxiv.org/abs/2504.21853</guid>
<content:encoded><![CDATA[
arXiv:2504.21853v1 Announce Type: new 
Abstract: Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction</title>
<link>https://arxiv.org/abs/2504.21855</link>
<guid>https://arxiv.org/abs/2504.21855</guid>
<content:encoded><![CDATA[
arXiv:2504.21855v1 Announce Type: new 
Abstract: In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality</title>
<link>https://arxiv.org/abs/2504.21033</link>
<guid>https://arxiv.org/abs/2504.21033</guid>
<content:encoded><![CDATA[
arXiv:2504.21033v1 Announce Type: cross 
Abstract: Traditional 3D modeling requires technical expertise, specialized software, and time-intensive processes, making it inaccessible for many users. Our research aims to lower these barriers by combining generative AI and augmented reality (AR) into a cohesive system that allows users to easily generate, manipulate, and interact with 3D models in real time, directly within AR environments. Utilizing cutting-edge AI models like Shap-E, we address the complex challenges of transforming 2D images into 3D representations in AR environments. Key challenges such as object isolation, handling intricate backgrounds, and achieving seamless user interaction are tackled through advanced object detection methods, such as Mask R-CNN. Evaluation results from 35 participants reveal an overall System Usability Scale (SUS) score of 69.64, with participants who engaged with AR/VR technologies more frequently rating the system significantly higher, at 80.71. This research is particularly relevant for applications in gaming, education, and AR-based e-commerce, offering intuitive, model creation for users without specialized skills.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction</title>
<link>https://arxiv.org/abs/2504.21067</link>
<guid>https://arxiv.org/abs/2504.21067</guid>
<content:encoded><![CDATA[
arXiv:2504.21067v1 Announce Type: cross 
Abstract: This research tackles the challenge of real-time active view selection and uncertainty quantification on visual quality for active 3D reconstruction. Visual quality is a critical aspect of 3D reconstruction. Recent advancements such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have notably enhanced the image rendering quality of reconstruction models. Nonetheless, the efficient and effective acquisition of input images for reconstruction-specifically, the selection of the most informative viewpoint-remains an open challenge, which is crucial for active reconstruction. Existing studies have primarily focused on evaluating geometric completeness and exploring unobserved or unknown regions, without direct evaluation of the visual uncertainty within the reconstruction model. To address this gap, this paper introduces a probabilistic model that quantifies visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we formulate a criterion, Gaussian Splatting Shannon Mutual Information (GauSS-MI), for real-time assessment of visual mutual information from novel viewpoints, facilitating the selection of next best view. GauSS-MI is implemented within an active reconstruction system integrated with a view and motion planner. Extensive experiments across various simulated and real-world scenes showcase the superior visual quality and reconstruction efficiency performance of the proposed system.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Light Weight CNN for classification of Brain Tumors from MRI Images</title>
<link>https://arxiv.org/abs/2504.21188</link>
<guid>https://arxiv.org/abs/2504.21188</guid>
<content:encoded><![CDATA[
arXiv:2504.21188v1 Announce Type: cross 
Abstract: This study presents a convolutional neural network (CNN)-based approach for the multi-class classification of brain tumors using magnetic resonance imaging (MRI) scans. We utilize a publicly available dataset containing MRI images categorized into four classes: glioma, meningioma, pituitary tumor, and no tumor. Our primary objective is to build a light weight deep learning model that can automatically classify brain tumor types with high accuracy. To achieve this goal, we incorporate image preprocessing steps, including normalization, data augmentation, and a cropping technique designed to reduce background noise and emphasize relevant regions. The CNN architecture is optimized through hyperparameter tuning using Keras Tuner, enabling systematic exploration of network parameters. To ensure reliable evaluation, we apply 5-fold cross-validation, where each hyperparameter configuration is evaluated across multiple data splits to mitigate overfitting. Experimental results demonstrate that the proposed model achieves a classification accuracy of 98.78%, indicating its potential as a diagnostic aid in clinical settings. The proposed method offers a low-complexity yet effective solution for assisting in early brain tumor diagnosis.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets</title>
<link>https://arxiv.org/abs/2504.21227</link>
<guid>https://arxiv.org/abs/2504.21227</guid>
<content:encoded><![CDATA[
arXiv:2504.21227v1 Announce Type: cross 
Abstract: Deep learning models have great potential in medical imaging, including orthodontics and skeletal maturity assessment. However, applying a model to data different from its training set can lead to unreliable predictions that may impact patient care. To address this, we propose a comprehensive verification framework that evaluates model suitability through multiple complementary strategies. First, we introduce a Gradient Attention Map (GAM)-based approach that analyzes attention patterns using Grad-CAM and compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance. Second, we extend verification to early convolutional feature maps, capturing structural mis-alignments missed by attention alone. Finally, we incorporate an additional garbage class into the classification model to explicitly reject out-of-distribution inputs. Experimental results demonstrate that these combined methods effectively identify unsuitable models and inputs, promoting safer and more reliable deployment of deep learning in medical imaging.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Space Group Determination from EBSD Patterns: The Role of Deep Learning and High-throughput Dynamical Simulations</title>
<link>https://arxiv.org/abs/2504.21331</link>
<guid>https://arxiv.org/abs/2504.21331</guid>
<content:encoded><![CDATA[
arXiv:2504.21331v1 Announce Type: cross 
Abstract: The design of novel materials hinges on the understanding of structure-property relationships. However, our capability to synthesize a large number of materials has outpaced the ability and speed needed to characterize them. While the overall chemical constituents can be readily known during synthesis, the structural evolution and characterization of newly synthesized samples remains a bottleneck for the ultimate goal of high throughput nanomaterials discovery. Thus, scalable methods for crystal symmetry determination that can analyze a large volume of material samples within a short time-frame are especially needed. Kikuchi diffraction in the SEM is a promising technique for this due to its sensitivity to dynamical scattering, which may provide information beyond just the seven crystal systems and fourteen Bravais lattices. After diffraction patterns are collected from material samples, deep learning methods may be able to classify the space group symmetries using the patterns as input, which paired with the elemental composition, would help enable the determination of the crystal structure. To investigate the feasibility of this solution, neural networks were trained to predict the space group type of background corrected EBSD patterns. Our networks were first trained and tested on an artificial dataset of EBSD patterns of 5,148 different cubic phases, created through physics-based dynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised deep learning-based domain adaptation method, was utilized to train neural networks to make predictions for experimental EBSD patterns. We introduce a relabeling scheme, which enables our models to achieve accuracy scores higher than 90% on simulated and experimental data, suggesting that neural networks are capable of making predictions of crystal symmetry from an EBSD pattern.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse-to-Sparse Training of Diffusion Models</title>
<link>https://arxiv.org/abs/2504.21380</link>
<guid>https://arxiv.org/abs/2504.21380</guid>
<content:encoded><![CDATA[
arXiv:2504.21380v1 Announce Type: cross 
Abstract: Diffusion models (DMs) are a powerful type of generative models that have achieved state-of-the-art results in various image synthesis tasks and have shown potential in other domains, such as natural language processing and temporal data modeling. Despite their stable training dynamics and ability to produce diverse high-quality samples, DMs are notorious for requiring significant computational resources, both in the training and inference stages. Previous work has focused mostly on increasing the efficiency of model inference. This paper introduces, for the first time, the paradigm of sparse-to-sparse training to DMs, with the aim of improving both training and inference efficiency. We focus on unconditional generation and train sparse DMs from scratch (Latent Diffusion and ChiroDiff) on six datasets using three different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of sparsity in model performance. Our experiments show that sparse DMs are able to match and often outperform their Dense counterparts, while substantially reducing the number of trainable parameters and FLOPs. We also identify safe and effective values to perform sparse-to-sparse training of DMs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAV-VLN: End-to-End Vision Language guided Navigation for UAVs</title>
<link>https://arxiv.org/abs/2504.21432</link>
<guid>https://arxiv.org/abs/2504.21432</guid>
<content:encoded><![CDATA[
arXiv:2504.21432v1 Announce Type: cross 
Abstract: A core challenge in AI-guided autonomy is enabling agents to navigate realistically and effectively in previously unseen environments based on natural language commands. We propose UAV-VLN, a novel end-to-end Vision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs) that seamlessly integrates Large Language Models (LLMs) with visual perception to facilitate human-interactive navigation. Our system interprets free-form natural language instructions, grounds them into visual observations, and plans feasible aerial trajectories in diverse environments.
  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse high-level semantic goals, while a vision model detects and localizes semantically relevant objects in the environment. By fusing these modalities, the UAV can reason about spatial relationships, disambiguate references in human instructions, and plan context-aware behaviors with minimal task-specific supervision. To ensure robust and interpretable decision-making, the framework includes a cross-modal grounding mechanism that aligns linguistic intent with visual context.
  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios, demonstrating its ability to generalize to novel instructions and environments with minimal task-specific training. Our results show significant improvements in instruction-following accuracy and trajectory efficiency, highlighting the potential of LLM-driven vision-language interfaces for safe, intuitive, and generalizable UAV autonomy.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboGround: Robotic Manipulation with Grounded Vision-Language Priors</title>
<link>https://arxiv.org/abs/2504.21530</link>
<guid>https://arxiv.org/abs/2504.21530</guid>
<content:encoded><![CDATA[
arXiv:2504.21530v1 Announce Type: cross 
Abstract: Recent advancements in robotic manipulation have highlighted the potential of intermediate representations for improving policy generalization. In this work, we explore grounding masks as an effective intermediate representation, balancing two key advantages: (1) effective spatial guidance that specifies target objects and placement areas while also conveying information about object shape and size, and (2) broad generalization potential driven by large-scale vision-language models pretrained on diverse grounding datasets. We introduce RoboGround, a grounding-aware robotic manipulation system that leverages grounding masks as an intermediate representation to guide policy networks in object manipulation tasks. To further explore and enhance generalization, we propose an automated pipeline for generating large-scale, simulated data with a diverse set of objects and instructions. Extensive experiments show the value of our dataset and the effectiveness of grounding masks as intermediate guidance, significantly enhancing the generalization abilities of robot policies.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cert-SSB: Toward Certified Sample-Specific Backdoor Defense</title>
<link>https://arxiv.org/abs/2504.21730</link>
<guid>https://arxiv.org/abs/2504.21730</guid>
<content:encoded><![CDATA[
arXiv:2504.21730v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample's certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at https://github.com/NcepuQiaoTing/Cert-SSB.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.21731</link>
<guid>https://arxiv.org/abs/2504.21731</guid>
<content:encoded><![CDATA[
arXiv:2504.21731v1 Announce Type: cross 
Abstract: Mixed Reality (MR) could assist users' tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users' poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms</title>
<link>https://arxiv.org/abs/2504.21778</link>
<guid>https://arxiv.org/abs/2504.21778</guid>
<content:encoded><![CDATA[
arXiv:2504.21778v1 Announce Type: cross 
Abstract: Current learned image compression models typically exhibit high complexity, which demands significant computational resources. To overcome these challenges, we propose an innovative approach that employs hierarchical feature extraction transforms to significantly reduce complexity while preserving bit rate reduction efficiency. Our novel architecture achieves this by using fewer channels for high spatial resolution inputs/feature maps. On the other hand, feature maps with a large number of channels have reduced spatial dimensions, thereby cutting down on computational load without sacrificing performance. This strategy effectively reduces the forward pass complexity from \(1256 \, \text{kMAC/Pixel}\) to just \(270 \, \text{kMAC/Pixel}\). As a result, the reduced complexity model can open the way for learned image compression models to operate efficiently across various devices and pave the way for the development of new architectures in image compression technology.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Is a Surprisingly Effective Domain Adaptation Baseline in Handwriting Recognition</title>
<link>https://arxiv.org/abs/2302.06308</link>
<guid>https://arxiv.org/abs/2302.06308</guid>
<content:encoded><![CDATA[
arXiv:2302.06308v2 Announce Type: replace 
Abstract: In many machine learning tasks, a large general dataset and a small specialized dataset are available. In such situations, various domain adaptation methods can be used to adapt a general model to the target dataset. We show that in the case of neural networks trained for handwriting recognition using CTC, simple fine-tuning with data augmentation works surprisingly well in such scenarios and that it is resistant to overfitting even for very small target domain datasets. We evaluated the behavior of fine-tuning with respect to augmentation, training data size, and quality of the pre-trained network, both in writer-dependent and writer-independent settings. On a large real-world dataset, fine-tuning on new writers provided an average relative CER improvement of 25 % for 16 text lines and 50 % for 256 text lines.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Writing Style Adaptation in Handwriting Recognition</title>
<link>https://arxiv.org/abs/2302.06318</link>
<guid>https://arxiv.org/abs/2302.06318</guid>
<content:encoded><![CDATA[
arXiv:2302.06318v2 Announce Type: replace 
Abstract: One of the challenges of handwriting recognition is to transcribe a large number of vastly different writing styles. State-of-the-art approaches do not explicitly use information about the writer's style, which may be limiting overall accuracy due to various ambiguities. We explore models with writer-dependent parameters which take the writer's identity as an additional input. The proposed models can be trained on datasets with partitions likely written by a single author (e.g. single letter, diary, or chronicle). We propose a Writer Style Block (WSB), an adaptive instance normalization layer conditioned on learned embeddings of the partitions. We experimented with various placements and settings of WSB and contrastively pre-trained embeddings. We show that our approach outperforms a baseline with no WSB in a writer-dependent scenario and that it is possible to estimate embeddings for new writers. However, domain adaptation using simple fine-tuning in a writer-independent setting provides superior accuracy at a similar computational cost. The proposed approach should be further investigated in terms of training stability and embedding regularization to overcome such a baseline.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions</title>
<link>https://arxiv.org/abs/2303.12675</link>
<guid>https://arxiv.org/abs/2303.12675</guid>
<content:encoded><![CDATA[
arXiv:2303.12675v2 Announce Type: replace 
Abstract: Font design is of vital importance in the digital content design and modern printing industry. Developing algorithms capable of automatically synthesizing vector fonts can significantly facilitate the font design process. However, existing methods mainly concentrate on raster image generation, and only a few approaches can directly synthesize vector fonts. This paper proposes an end-to-end trainable method, VecFontSDF, to reconstruct and synthesize high-quality vector fonts using signed distance functions (SDFs). Specifically, based on the proposed SDF-based implicit shape representation, VecFontSDF learns to model each glyph as shape primitives enclosed by several parabolic curves, which can be precisely converted to quadratic B\'ezier curves that are widely used in vector font products. In this manner, most image generation methods can be easily extended to synthesize vector fonts. Qualitative and quantitative experiments conducted on a publicly-available dataset demonstrate that our method obtains high-quality results on several tasks, including vector font reconstruction, interpolation, and few-shot vector font synthesis, markedly outperforming the state of the art.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Synthetic Image Detection via Language-guided Contrastive Learning</title>
<link>https://arxiv.org/abs/2305.13800</link>
<guid>https://arxiv.org/abs/2305.13800</guid>
<content:encoded><![CDATA[
arXiv:2305.13800v2 Announce Type: replace 
Abstract: The heightened realism of AI-generated images can be attributed to the rapid development of synthetic models, including generative adversarial networks (GANs) and diffusion models (DMs). The malevolent use of synthetic images, such as the dissemination of fake news or the creation of fake profiles, however, raises significant concerns regarding the authenticity of images. Though many forensic algorithms have been developed for detecting synthetic images, their performance, especially the generalization capability, is still far from being adequate to cope with the increasing number of synthetic models. In this work, we propose a simple yet very effective synthetic image detection method via a language-guided contrastive learning. Specifically, we augment the training images with carefully-designed textual labels, enabling us to use a joint visual-language contrastive supervision for learning a forensic feature space with better generalization. It is shown that our proposed LanguAge-guided SynThEsis Detection (LASTED) model achieves much improved generalizability to unseen image generation models and delivers promising performance that far exceeds state-of-the-art competitors over four datasets. The code is available at https://github.com/HighwayWu/LASTED.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Positive Pairs for Enhancing Visual Representation Learning of Instance Discrimination Methods</title>
<link>https://arxiv.org/abs/2306.16122</link>
<guid>https://arxiv.org/abs/2306.16122</guid>
<content:encoded><![CDATA[
arXiv:2306.16122v3 Announce Type: replace 
Abstract: Self-supervised learning algorithms (SSL) based on instance discrimination have shown promising results, performing competitively or even outperforming supervised learning counterparts in some downstream tasks. Such approaches employ data augmentation to create two views of the same instance (i.e., positive pairs) and encourage the model to learn good representations by attracting these views closer in the embedding space without collapsing to the trivial solution. However, data augmentation is limited in representing positive pairs, and the repulsion process between the instances during contrastive learning may discard important features for instances that have similar categories. To address this issue, we propose an approach to identify those images with similar semantic content and treat them as positive instances, thereby reducing the chance of discarding important features during representation learning and increasing the richness of the latent representation. Our approach is generic and could work with any self-supervised instance discrimination frameworks such as MoCo and SimSiam. To evaluate our method, we run experiments on three benchmark datasets: ImageNet, STL-10 and CIFAR-10 with different instance discrimination SSL approaches. The experimental results show that our approach consistently outperforms the baseline methods across all three datasets; for instance, we improve upon the vanilla MoCo-v2 by 4.1% on ImageNet under a linear evaluation protocol over 800 epochs. We also report results on semi-supervised learning, transfer learning on downstream tasks, and object detection.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation</title>
<link>https://arxiv.org/abs/2308.13505</link>
<guid>https://arxiv.org/abs/2308.13505</guid>
<content:encoded><![CDATA[
arXiv:2308.13505v2 Announce Type: replace 
Abstract: Current prevailing Video Object Segmentation methods follow the pipeline of extraction-then-matching, which first extracts features on current and reference frames independently, and then performs dense matching between them. This decoupled pipeline limits information propagation between frames to high-level features, hindering fine-grained details for matching. Furthermore, the pixel-wise matching lacks holistic target understanding, making it prone to disturbance by similar distractors. To address these issues, we propose a unified VOS framework, coined as JointFormer, for jointly modeling feature extraction, correspondence matching, and a compressed memory. The core Joint Modeling Block leverages attention to simultaneously extract and propagate the target information from the reference frame to the current frame and a compressed memory token. This joint scheme enables extensive multi-layer propagation beyond high-level feature space and facilitates robust instance-distinctive feature learning. To incorporate the long-term and holistic target information, we introduce a compressed memory token with a customized online updating mechanism, which aggregates target features and facilitates temporal information propagation in a frame-wise manner, enhancing global modeling consistency. Our JointFormer achieves a new state-of-the-art performance on the DAVIS 2017 val/test-dev (89.7\% and 87.6\%) benchmarks and the YouTube-VOS 2018/2019 val (87.0\% and 87.0\%) benchmarks, outperforming the existing works. To demonstrate the generalizability of our model, it is further evaluated on four new benchmarks with various difficulties, including MOSE for complex scenes, VISOR for egocentric videos, VOST for complex transformations, and LVOS for long-term videos.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignDiff: Diffusion Model for American Sign Language Production</title>
<link>https://arxiv.org/abs/2308.16082</link>
<guid>https://arxiv.org/abs/2308.16082</guid>
<content:encoded><![CDATA[
arXiv:2308.16082v4 Announce Type: replace 
Abstract: In this paper, we propose a dual-condition diffusion pre-training model named SignDiff that can generate human sign language speakers from a skeleton pose. SignDiff has a novel Frame Reinforcement Network called FR-Net, similar to dense human pose estimation work, which enhances the correspondence between text lexical symbols and sign language dense pose frames, reduces the occurrence of multiple fingers in the diffusion model. In addition, we propose a new method for American Sign Language Production (ASLP), which can generate ASL skeletal pose videos from text input, integrating two new improved modules and a new loss function to improve the accuracy and quality of sign language skeletal posture and enhance the ability of the model to train on large-scale data. We propose the first baseline for ASL production and report the scores of 17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We evaluated our model on the previous mainstream dataset PHOENIX14T, and the experiments achieved the SOTA results. In addition, our image quality far exceeds all previous results by 10 percentage points in terms of SSIM.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images</title>
<link>https://arxiv.org/abs/2309.06129</link>
<guid>https://arxiv.org/abs/2309.06129</guid>
<content:encoded><![CDATA[
arXiv:2309.06129v4 Announce Type: replace 
Abstract: Deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. This problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. While synthetic datasets can be a solution, their creation is both time and resource-intensive. To address this problem, we present a framework called Light Eyes or "LEyes" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. LEyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. We demonstrate that models trained using LEyes are consistently on-par or outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets. In addition, a LEyes trained model outperforms the industry standard eye tracker using significantly more cost-effective hardware. Going forward, we are confident that LEyes will revolutionize synthetic data generation for gaze estimation models, and lead to significant improvements of the next generation video-based eye trackers.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EyePreserve: Identity-Preserving Iris Synthesis</title>
<link>https://arxiv.org/abs/2312.12028</link>
<guid>https://arxiv.org/abs/2312.12028</guid>
<content:encoded><![CDATA[
arXiv:2312.12028v4 Announce Type: replace 
Abstract: Synthesis of same-identity biometric iris images, both for existing and non-existing identities while preserving the identity across a wide range of pupil sizes, is complex due to the intricate iris muscle constriction mechanism, requiring a precise model of iris non-linear texture deformations to be embedded into the synthesis pipeline. This paper presents the first method of fully data-driven, identity-preserving, pupil size-varying synthesis of iris images. This approach is capable of synthesizing images of irises with different pupil sizes representing non-existing identities, as well as non-linearly deforming the texture of iris images of existing subjects given the segmentation mask of the target iris image. Iris recognition experiments suggest that the proposed deformation model both preserves the identity when changing the pupil size, and offers better similarity between same-identity iris samples with significant differences in pupil size, compared to state-of-the-art linear and non-linear (bio-mechanical-based) iris deformation models. Two immediate applications of the proposed approach are: (a) synthesis of, or enhancement of the existing biometric datasets for iris recognition, mimicking those acquired with iris sensors, and (b) helping forensic human experts examine iris image pairs with significant differences in pupil dilation. Images considered in this work conform to selected ISO/IEC 29794-6 quality metrics to make them applicable in biometric systems. The source codes and model weights are offered with this paper.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DDM: A Metric for Comparing 3D Shapes Using Directional Distance Fields</title>
<link>https://arxiv.org/abs/2401.09736</link>
<guid>https://arxiv.org/abs/2401.09736</guid>
<content:encoded><![CDATA[
arXiv:2401.09736v5 Announce Type: replace 
Abstract: Qualifying the discrepancy between 3D geometric models, which could be represented with either point clouds or triangle meshes, is a pivotal issue with board applications. Existing methods mainly focus on directly establishing the correspondence between two models and then aggregating point-wise distance between corresponding points, resulting in them being either inefficient or ineffective. In this paper, we propose DDM, an efficient, effective, robust, and differentiable distance metric for 3D geometry data. Specifically, we construct DDM based on the proposed implicit representation of 3D models, namely directional distance field (DDF), which defines the directional distances of 3D points to a model to capture its local surface geometry. We then transfer the discrepancy between two 3D geometric models as the discrepancy between their DDFs defined on an identical domain, naturally establishing model correspondence. To demonstrate the advantage of our DDM, we explore various distance metric-driven 3D geometric modeling tasks, including template surface fitting, rigid registration, non-rigid registration, scene flow estimation and human pose optimization. Extensive experiments show that our DDM achieves significantly higher accuracy under all tasks. As a generic distance metric, DDM has the potential to advance the field of 3D geometric modeling. The source code is available at https://github.com/rsy6318/DDM.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MeDSLIP: Medical Dual-Stream Language-Image Pre-training with Pathology-Anatomy Semantic Alignment</title>
<link>https://arxiv.org/abs/2403.10635</link>
<guid>https://arxiv.org/abs/2403.10635</guid>
<content:encoded><![CDATA[
arXiv:2403.10635v2 Announce Type: replace 
Abstract: Pathology and anatomy are two essential groups of semantics in medical data. Pathology describes what the diseases are, while anatomy explains where the diseases occur. They describe diseases from different perspectives, providing complementary insights into diseases. Thus, properly understanding these semantics and their relationships can enhance medical vision-language models (VLMs). However, pathology and anatomy semantics are usually entangled in medical data, hindering VLMs from explicitly modeling these semantics and their relationships. To address this challenge, we propose MeDSLIP, a novel Medical Dual-Stream Language-Image Pre-training pipeline, to disentangle pathology and anatomy semantics and model the relationships between them. We introduce a dual-stream mechanism in MeDSLIP to explicitly disentangle medical semantics into pathology-relevant and anatomy-relevant streams and align visual and textual information within each stream. Furthermore, we propose an interaction modeling module with prototypical contrastive learning loss and intra-image contrastive learning loss to regularize the relationships between pathology and anatomy semantics. We apply MeDSLIP to chest X-ray analysis and conduct comprehensive evaluations with four benchmark datasets: NIH CXR14, RSNA Pneumonia, SIIM-ACR Pneumothorax, and COVIDx CXR-4. The results demonstrate MeDSLIP's superior generalizability and transferability across different scenarios. The code is available at https://github.com/Shef-AIRE/MeDSLIP, and the pre-trained model is released at https://huggingface.co/pykale/MeDSLIP.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Garment3DGen: 3D Garment Stylization and Texture Generation</title>
<link>https://arxiv.org/abs/2403.18816</link>
<guid>https://arxiv.org/abs/2403.18816</guid>
<content:encoded><![CDATA[
arXiv:2403.18816v3 Announce Type: replace 
Abstract: We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance. Our proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text prompts. The generated assets can be directly draped and simulated on human bodies. We leverage the recent progress of image-to-3D diffusion methods to generate 3D garment geometries. However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target. Carefully designed losses allow the base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated. Finally, we generate high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets. With Garment3DGen users can generate the simulation-ready 3D garment of their choice without the need of artist intervention. We present a plethora of quantitative and qualitative comparisons on various assets and demonstrate that Garment3DGen unlocks key applications ranging from sketch-to-simulated garments or interacting with the garments in VR. Code is publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignLLM: Sign Language Production Large Language Models</title>
<link>https://arxiv.org/abs/2405.10718</link>
<guid>https://arxiv.org/abs/2405.10718</guid>
<content:encoded><![CDATA[
arXiv:2405.10718v3 Announce Type: replace 
Abstract: In this paper, we propose SignLLM, a multilingual Sign Language Production (SLP) large language model, which includes two novel multilingual SLP modes MLSF and Prompt2LangGloss that allow sign language gestures generation from query texts input and question-style prompts input respectively. Both modes can use a new RL loss based on reinforcement learning and a new RL module named Priority Learning Channel. These RL components can accelerate the training by enhancing the model's capability to sample high-quality data. To train SignLLM, we introduce Prompt2Sign, a comprehensive multilingual sign language dataset, which builds from public data, including American Sign Language (ASL) and seven others. This dataset standardizes information by extracting pose information from sign language videos into a unified compressed format. We extensively evaluate SignLLM, demonstrating that our model achieves state-of-the-art performance on SLP tasks across eight sign languages.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals</title>
<link>https://arxiv.org/abs/2405.20152</link>
<guid>https://arxiv.org/abs/2405.20152</guid>
<content:encoded><![CDATA[
arXiv:2405.20152v2 Announce Type: replace 
Abstract: With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images, producing over 57 million responses from popular models. Our multi-dimensional bias evaluation framework reveals that social attributes such as perceived race, gender, and physical characteristics depicted in images can significantly influence the generation of toxic content, competency-associated words, harmful stereotypes, and numerical ratings of individuals.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty for SVBRDF Acquisition using Frequency Analysis</title>
<link>https://arxiv.org/abs/2406.17774</link>
<guid>https://arxiv.org/abs/2406.17774</guid>
<content:encoded><![CDATA[
arXiv:2406.17774v2 Announce Type: replace 
Abstract: This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view captures. Under uncontrolled illumination and unstructured viewpoints, there is no guarantee that the observations contain enough information to reconstruct the appearance properties of a captured object. We study this ambiguity, or uncertainty, using entropy and accelerate the analysis by using the frequency domain, rather than the domain of incoming and outgoing viewing angles. The result is a method that computes a map of uncertainty over an entire object within a millisecond. We find that the frequency model allows us to recover SVBRDF parameters with competitive performance, that the accelerated entropy computation matches results with a physically-based path tracer, and that there is a positive correlation between error and uncertainty. We then show that the uncertainty map can be applied to improve SVBRDF acquisition using capture guidance, sharing information on the surface, and using a diffusion model to inpaint uncertain regions. Our code is available at https://github.com/rubenwiersma/svbrdf_uncertainty.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents</title>
<link>https://arxiv.org/abs/2407.05679</link>
<guid>https://arxiv.org/abs/2407.05679</guid>
<content:encoded><![CDATA[
arXiv:2407.05679v3 Announce Type: replace 
Abstract: World models have attracted increasing attention in autonomous driving for their ability to forecast potential future scenarios. In this paper, we propose BEVWorld, a novel framework that transforms multimodal sensor inputs into a unified and compact Bird's Eye View (BEV) latent space for holistic environment modeling. The proposed world model consists of two main components: a multi-modal tokenizer and a latent BEV sequence diffusion model. The multi-modal tokenizer first encodes heterogeneous sensory data, and its decoder reconstructs the latent BEV tokens into LiDAR and surround-view image observations via ray-casting rendering in a self-supervised manner. This enables joint modeling and bidirectional encoding-decoding of panoramic imagery and point cloud data within a shared spatial representation. On top of this, the latent BEV sequence diffusion model performs temporally consistent forecasting of future scenes, conditioned on high-level action tokens, enabling scene-level reasoning over time. Extensive experiments demonstrate the effectiveness of BEVWorld on autonomous driving benchmarks, showcasing its capability in realistic future scene generation and its benefits for downstream tasks such as perception and motion prediction.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Recovery for Image Generation Models: A Comparative Study of Discrete Optimizers</title>
<link>https://arxiv.org/abs/2408.06502</link>
<guid>https://arxiv.org/abs/2408.06502</guid>
<content:encoded><![CDATA[
arXiv:2408.06502v2 Announce Type: replace 
Abstract: Recovering natural language prompts for image generation models, solely based on the generated images is a difficult discrete optimization problem. In this work, we present the first head-to-head comparison of recent discrete optimization techniques for the problem of prompt inversion. We evaluate Greedy Coordinate Gradients (GCG), PEZ , Random Search, AutoDAN and BLIP2's image captioner across various evaluation metrics related to the quality of inverted prompts and the quality of the images generated by the inverted prompts. We find that focusing on the CLIP similarity between the inverted prompts and the ground truth image acts as a poor proxy for the similarity between ground truth image and the image generated by the inverted prompts. While the discrete optimizers effectively minimize their objectives, simply using responses from a well-trained captioner often leads to generated images that more closely resemble those produced by the original prompts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Kinematics and Kinetics Between OpenCap and a Marker-Based Motion Capture System in Cycling</title>
<link>https://arxiv.org/abs/2409.03766</link>
<guid>https://arxiv.org/abs/2409.03766</guid>
<content:encoded><![CDATA[
arXiv:2409.03766v3 Announce Type: replace 
Abstract: This study evaluates the agreement of marker-based and markerless (OpenCap) motion capture systems in assessing joint kinematics and kinetics during cycling. Markerless systems, such as OpenCap, offer the advantage of capturing natural movements without physical markers, making them more practical for real-world applications. However, the agreement of OpenCap with a marker-based system, particularly in cycling, remains underexplored. Ten participants cycled at varying speeds and resistances while motion data were recorded using both systems. Key metrics, including joint angles, moments, and joint reaction loads, were computed using OpenSim and compared using root mean squared error (RMSE) per trial across participants, Pearson correlation coefficients (r) per trial across participants and repeated measures Bland-Altman to control trials dependency within subject. Results revealed very strong agreement (r GT 0.9) for hip (flexion/extension), knee (flexion/extension), and ankle (dorsiflexion/plantarflexion) joint angles.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underwater Image Enhancement via Dehazing and Color Restoration</title>
<link>https://arxiv.org/abs/2409.09779</link>
<guid>https://arxiv.org/abs/2409.09779</guid>
<content:encoded><![CDATA[
arXiv:2409.09779v2 Announce Type: replace 
Abstract: Underwater visual imaging is crucial for marine engineering, but it suffers from low contrast, blurriness, and color degradation, which hinders downstream analysis. Existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process, neglecting their inherent independence while overlooking their synergistic relationship. To overcome this limitation, we propose a Vision Transformer (ViT)-based network (referred to as WaterFormer) to improve underwater image quality. WaterFormer contains three major components: a dehazing block (DehazeFormer Block) to capture the self-correlated haze features and extract deep-level features, a Color Restoration Block (CRB) to capture self-correlated color cast features, and a Channel Fusion Block (CFB) that dynamically integrates these decoupled features to achieve comprehensive enhancement. To ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. Further, a Chromatic Consistency Loss and Sobel Color Loss are designed to respectively preserve color fidelity and enhance structural details during network training. Comprehensive experimental results demonstrate that WaterFormer outperforms other state-of-the-art methods in enhancing underwater images.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WARP-LCA: Efficient Convolutional Sparse Coding with Locally Competitive Algorithm</title>
<link>https://arxiv.org/abs/2410.18794</link>
<guid>https://arxiv.org/abs/2410.18794</guid>
<content:encoded><![CDATA[
arXiv:2410.18794v2 Announce Type: replace 
Abstract: The locally competitive algorithm (LCA) can solve sparse coding problems across a wide range of use cases. Recently, convolution-based LCA approaches have been shown to be highly effective for enhancing robustness for image recognition tasks in vision pipelines. To additionally maximize representational sparsity, LCA with hard-thresholding can be applied. While this combination often yields very good solutions satisfying an $\ell_0$ sparsity criterion, it comes with significant drawbacks for practical application: (i) LCA is very inefficient, typically requiring hundreds of optimization cycles for convergence; (ii) the use of hard-thresholding results in a non-convex loss function, which might lead to suboptimal minima. To address these issues, we propose the Locally Competitive Algorithm with State Warm-up via Predictive Priming (WARP-LCA), which leverages a predictor network to provide a suitable initial guess of the LCA state based on the current input. Our approach significantly improves both convergence speed and the quality of solutions, while maintaining and even enhancing the overall strengths of LCA. We demonstrate that WARP-LCA converges faster by orders of magnitude and reaches better minima compared to conventional LCA. Moreover, the learned representations are more sparse and exhibit superior properties in terms of reconstruction and denoising quality as well as robustness when applied in deep recognition pipelines. Furthermore, we apply WARP-LCA to image denoising tasks, showcasing its robustness and practical effectiveness. Our findings confirm that the naive use of LCA with hard-thresholding results in suboptimal minima, whereas initializing LCA with a predictive guess results in better outcomes. This research advances the field of biologically inspired deep learning by providing a novel approach to convolutional sparse coding.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreCM: The Padding-based Rotation Equivariant Convolution Mode for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2411.01624</link>
<guid>https://arxiv.org/abs/2411.01624</guid>
<content:encoded><![CDATA[
arXiv:2411.01624v2 Announce Type: replace 
Abstract: Semantic segmentation is an important branch of image processing and computer vision. With the popularity of deep learning, various convolutional neural networks have been proposed for pixel-level classification and segmentation tasks. In practical scenarios, however, imaging angles are often arbitrary, encompassing instances such as water body images from remote sensing and capillary and polyp images in the medical domain, where prior orientation information is typically unavailable to guide these networks to extract more effective features. In this case, learning features from objects with diverse orientation information poses a significant challenge, as the majority of CNN-based semantic segmentation networks lack rotation equivariance to resist the disturbance from orientation information. To address this challenge, this paper first constructs a universal convolution-group framework aimed at more fully utilizing orientation information and equipping the network with rotation equivariance. Subsequently, we mathematically design a padding-based rotation equivariant convolution mode (PreCM), which is not only applicable to multi-scale images and convolutional kernels but can also serve as a replacement component for various types of convolutions, such as dilated convolutions, transposed convolutions, and asymmetric convolution. To quantitatively assess the impact of image rotation in semantic segmentation tasks, we also propose a new evaluation metric, Rotation Difference (RD). The replacement experiments related to six existing semantic segmentation networks on three datasets show that, the average Intersection Over Union (IOU) of their PreCM-based versions respectively improve 6.91%, 10.63%, 4.53%, 5.93%, 7.48%, 8.33% compared to their original versions in terms of random angle rotation. And the average RD values are decreased by 3.58%, 4.56%, 3.47%, 3.66%, 3.47%, 3.43% respectively.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Frequency Enhanced Hybrid Neural Representation for Video Compression</title>
<link>https://arxiv.org/abs/2411.06685</link>
<guid>https://arxiv.org/abs/2411.06685</guid>
<content:encoded><![CDATA[
arXiv:2411.06685v2 Announce Type: replace 
Abstract: Neural Representations for Videos (NeRV) have simplified the video codec process and achieved swift decoding speeds by encoding video content into a neural network, presenting a promising solution for video compression. However, existing work overlooks the crucial issue that videos reconstructed by these methods lack high-frequency details. To address this problem, this paper introduces a High-Frequency Enhanced Hybrid Neural Representation Network. Our method focuses on leveraging high-frequency information to improve the synthesis of fine details by the network. Specifically, we design a wavelet high-frequency encoder that incorporates Wavelet Frequency Decomposer (WFD) blocks to generate high-frequency feature embeddings. Next, we design the High-Frequency Feature Modulation (HFM) block, which leverages the extracted high-frequency embeddings to enhance the fitting process of the decoder. Finally, with the refined Harmonic decoder block and a Dynamic Weighted Frequency Loss, we further reduce the potential loss of high-frequency information. Experiments on the Bunny and UVG datasets demonstrate that our method outperforms other methods, showing notable improvements in detail preservation and compression performance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ColorEdit: Training-free Image-Guided Color editing with diffusion model</title>
<link>https://arxiv.org/abs/2411.10232</link>
<guid>https://arxiv.org/abs/2411.10232</guid>
<content:encoded><![CDATA[
arXiv:2411.10232v2 Announce Type: replace 
Abstract: Text-to-image (T2I) diffusion models, with their impressive generative capabilities, have been adopted for image editing tasks, demonstrating remarkable efficacy. However, due to attention leakage and collision between the cross-attention map of the object and the new color attribute from the text prompt, text-guided image editing methods may fail to change the color of an object, resulting in a misalignment between the resulting image and the text prompt. In this paper, we conduct an in-depth analysis on the process of text-guided image synthesizing and what semantic information different cross-attention blocks have learned. We observe that the visual representation of an object is determined in the up-block of the diffusion model in the early stage of the denoising process, and color adjustment can be achieved through value matrices alignment in the cross-attention layer. Based on our findings, we propose a straightforward, yet stable, and effective image-guided method to modify the color of an object without requiring any additional fine-tuning or training. Lastly, we present a benchmark dataset called COLORBENCH, the first benchmark to evaluate the performance of color change methods. Extensive experiments validate the effectiveness of our method in object-level color editing and surpass the performance of popular text-guided image editing approaches in both synthesized and real images.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Contrast-Agnostic Method for Ultra-High Resolution Claustrum Segmentation</title>
<link>https://arxiv.org/abs/2411.15388</link>
<guid>https://arxiv.org/abs/2411.15388</guid>
<content:encoded><![CDATA[
arXiv:2411.15388v2 Announce Type: replace 
Abstract: The claustrum is a band-like gray matter structure located between putamen and insula whose exact functions are still actively researched. Its sheet-like structure makes it barely visible in in vivo Magnetic Resonance Imaging (MRI) scans at typical resolutions and neuroimaging tools for its study, including methods for automatic segmentation, are currently very limited. In this paper, we propose a contrast- and resolution-agnostic method for claustrum segmentation at ultra-high resolution (0.35 mm isotropic); the method is based on the SynthSeg segmentation framework (Billot et al., 2023), which leverages the use of synthetic training intensity images to achieve excellent generalization. In particular, SynthSeg requires only label maps to be trained, since corresponding intensity images are synthesized on the fly with random contrast and resolution. We trained a deep learning network for automatic claustrum segmentation, using claustrum manual labels obtained from 18 ultra-high resolution MRI scans (mostly ex vivo). We demonstrated the method to work on these 18 high resolution cases (Dice score = 0.632, mean surface distance = 0.458 mm, and volumetric similarity = 0.867 using 6-fold Cross Validation (CV)), and also on in vivo T1-weighted MRI scans at typical resolutions (~1 mm isotropic). We also demonstrated that the method is robust in a test-retest setting and when applied to multimodal imaging (T2-weighted, Proton Density and quantitative T1 scans). To the best of our knowledge this is the first accurate method for automatic ultra-high resolution claustrum segmentation, which is robust against changes in contrast and resolution. The method is released at https://github.com/chiara-mauri/claustrum_segmentation and as part of the neuroimaging package Freesurfer (Fischl, 2012).
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages</title>
<link>https://arxiv.org/abs/2411.16508</link>
<guid>https://arxiv.org/abs/2411.16508</guid>
<content:encoded><![CDATA[
arXiv:2411.16508v3 Announce Type: replace 
Abstract: Existing Large Multimodal Models (LMMs) generally focus on only a few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. In pursuit of culturally diverse global multimodal models, our proposed All Languages Matter Benchmark (ALM-bench) represents the largest and most comprehensive effort to date for evaluating LMMs across 100 languages. ALM-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in LMM research. The benchmark offers a robust and nuanced evaluation framework featuring various question formats, including true/false, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. ALM-bench design ensures a comprehensive assessment of a model's ability to handle varied levels of difficulty in visual and linguistic reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. Through this, ALM-bench not only provides a rigorous testing ground for state-of-the-art open and closed-source LMMs but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. Our benchmark is publicly available.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Type-R: Automatically Retouching Typos for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2411.18159</link>
<guid>https://arxiv.org/abs/2411.18159</guid>
<content:encoded><![CDATA[
arXiv:2411.18159v2 Announce Type: replace 
Abstract: While recent text-to-image models can generate photorealistic images from text prompts that reflect detailed instructions, they still face significant challenges in accurately rendering words in the image. In this paper, we propose to retouch erroneous text renderings in the post-processing pipeline. Our approach, called Type-R, identifies typographical errors in the generated image, erases the erroneous text, regenerates text boxes for missing words, and finally corrects typos in the rendered words. Through extensive experiments, we show that Type-R, in combination with the latest text-to-image models such as Stable Diffusion or Flux, achieves the highest text rendering accuracy while maintaining image quality and also outperforms text-focused generation baselines in terms of balancing text accuracy and image quality.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos</title>
<link>https://arxiv.org/abs/2411.19167</link>
<guid>https://arxiv.org/abs/2411.19167</guid>
<content:encoded><![CDATA[
arXiv:2411.19167v2 Announce Type: replace 
Abstract: We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head Synthesis</title>
<link>https://arxiv.org/abs/2411.19509</link>
<guid>https://arxiv.org/abs/2411.19509</guid>
<content:encoded><![CDATA[
arXiv:2411.19509v3 Announce Type: replace 
Abstract: Recent advances in diffusion models have endowed talking head synthesis with subtle expressions and vivid head movements, but have also led to slow inference speed and insufficient control over generated results. To address these issues, we propose Ditto, a diffusion-based talking head framework that enables fine-grained controls and real-time inference. Specifically, we utilize an off-the-shelf motion extractor and devise a diffusion transformer to generate representations in a specific motion space. We optimize the model architecture and training strategy to address the issues in generating motion representations, including insufficient disentanglement between motion and identity, and large internal discrepancies within the representation. Besides, we employ diverse conditional signals while establishing a mapping between motion representation and facial semantics, enabling control over the generation process and correction of the results. Moreover, we jointly optimize the holistic framework to enable streaming processing, real-time inference, and low first-frame delay, offering functionalities crucial for interactive applications such as AI assistants. Extensive experimental results demonstrate that Ditto generates compelling talking head videos and exhibits superiority in both controllability and real-time performance.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation Models</title>
<link>https://arxiv.org/abs/2412.04204</link>
<guid>https://arxiv.org/abs/2412.04204</guid>
<content:encoded><![CDATA[
arXiv:2412.04204v2 Announce Type: replace 
Abstract: Geospatial Foundation Models (GFMs) have emerged as powerful tools for extracting representations from Earth observation data, but their evaluation remains inconsistent and narrow. Existing works often evaluate on suboptimal downstream datasets and tasks, that are often too easy or too narrow, limiting the usefulness of the evaluations to assess the real-world applicability of GFMs. Additionally, there is a distinct lack of diversity in current evaluation protocols, which fail to account for the multiplicity of image resolutions, sensor types, and temporalities, which further complicates the assessment of GFM performance. In particular, most existing benchmarks are geographically biased towards North America and Europe, questioning the global applicability of GFMs. To overcome these challenges, we introduce PANGAEA, a standardized evaluation protocol that covers a diverse set of datasets, tasks, resolutions, sensor modalities, and temporalities. It establishes a robust and widely applicable benchmark for GFMs. We evaluate the most popular GFMs openly available on this benchmark and analyze their performance across several domains. In particular, we compare these models to supervised baselines (e.g. UNet and vanilla ViT), and assess their effectiveness when faced with limited labeled data. Our findings highlight the limitations of GFMs, under different scenarios, showing that they do not consistently outperform supervised models. PANGAEA is designed to be highly extensible, allowing for the seamless inclusion of new datasets, models, and tasks in future research. By releasing the evaluation code and benchmark, we aim to enable other researchers to replicate our experiments and build upon our work, fostering a more principled evaluation protocol for large pre-trained geospatial models. The code is available at https://github.com/VMarsocci/pangaea-bench.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not Just Text: Uncovering Vision Modality Typographic Threats in Image Generation Models</title>
<link>https://arxiv.org/abs/2412.05538</link>
<guid>https://arxiv.org/abs/2412.05538</guid>
<content:encoded><![CDATA[
arXiv:2412.05538v2 Announce Type: replace 
Abstract: Current image generation models can effortlessly produce high-quality, highly realistic images, but this also increases the risk of misuse. In various Text-to-Image or Image-to-Image tasks, attackers can generate a series of images containing inappropriate content by simply editing the language modality input. To mitigate this security concern, numerous guarding or defensive strategies have been proposed, with a particular emphasis on safeguarding language modality. However, in practical applications, threats in the vision modality, particularly in tasks involving the editing of real-world images, present heightened security risks as they can easily infringe upon the rights of the image owner. Therefore, this paper employs a method named typographic attack to reveal that various image generation models are also susceptible to threats within the vision modality. Furthermore, we also evaluate the defense performance of various existing methods when facing threats in the vision modality and uncover their ineffectiveness. Finally, we propose the Vision Modal Threats in Image Generation Models (VMT-IGMs) dataset, which would serve as a baseline for evaluating the vision modality vulnerability of various image generation models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FILA: Fine-Grained Vision Language Models</title>
<link>https://arxiv.org/abs/2412.08378</link>
<guid>https://arxiv.org/abs/2412.08378</guid>
<content:encoded><![CDATA[
arXiv:2412.08378v3 Announce Type: replace 
Abstract: Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into smaller sub-images, which are then fed into a vision encoder that was pre-trained on lower-resolution images. However, this cropping approach often truncates objects and connected areas in the original image, causing semantic breaks. To address this limitation, we introduce HyViLM, designed to process images of any resolution while retaining the overall context during encoding. Specifically, we: (i) Design a new visual encoder called Hybrid Encoder that not only encodes individual sub-images but also interacts with detailed global visual features, significantly improving the model's ability to encode high-resolution images. (ii) Propose an optimal feature fusion strategy for the dynamic cropping approach, effectively leveraging information from different layers of the vision encoder. Compared with the state-of-the-art MLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out of ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance on the TextVQA task and a 6.9% enhancement on the DocVQA task.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos</title>
<link>https://arxiv.org/abs/2412.09621</link>
<guid>https://arxiv.org/abs/2412.09621</guid>
<content:encoded><![CDATA[
arXiv:2412.09621v2 Announce Type: replace 
Abstract: Learning to understand dynamic 3D scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. Yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3D motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. We present a system for mining high-quality 4D reconstructions from internet stereoscopic, wide-angle videos. Our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions. We use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate the utility of this data by training a variant of DUSt3R to predict structure and 3D motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes. Project page and data at: https://stereo4d.github.io
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optical aberrations in autonomous driving: Physics-informed parameterized temperature scaling for neural network uncertainty calibration</title>
<link>https://arxiv.org/abs/2412.13695</link>
<guid>https://arxiv.org/abs/2412.13695</guid>
<content:encoded><![CDATA[
arXiv:2412.13695v2 Announce Type: replace 
Abstract: 'A trustworthy representation of uncertainty is desirable and should be considered as a key feature of any machine learning method' (Huellermeier and Waegeman, 2021). This conclusion of Huellermeier et al. underpins the importance of calibrated uncertainties. Since AI-based algorithms are heavily impacted by dataset shifts, the automotive industry needs to safeguard its system against all possible contingencies. One important but often neglected dataset shift is caused by optical aberrations induced by the windshield. For the verification of the perception system performance, requirements on the AI performance need to be translated into optical metrics by a bijective mapping. Given this bijective mapping it is evident that the optical system characteristics add additional information about the magnitude of the dataset shift. As a consequence, we propose to incorporate a physical inductive bias into the neural network calibration architecture to enhance the robustness and the trustworthiness of the AI target application, which we demonstrate by using a semantic segmentation task as an example. By utilizing the Zernike coefficient vector of the optical system as a physical prior we can significantly reduce the mean expected calibration error in case of optical aberrations. As a result, we pave the way for a trustworthy uncertainty representation and for a holistic verification strategy of the perception chain.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Deep Learning and Model-Checking Framework for Accurate Brain Tumor Detection and Validation</title>
<link>https://arxiv.org/abs/2501.01991</link>
<guid>https://arxiv.org/abs/2501.01991</guid>
<content:encoded><![CDATA[
arXiv:2501.01991v2 Announce Type: replace 
Abstract: Model checking, a formal verification technique, ensures systems meet predefined requirements, playing a crucial role in minimizing errors and enhancing quality during development. This paper introduces a novel hybrid framework integrating model checking with deep learning for brain tumor detection and validation in medical imaging. By combining model-checking principles with CNN-based feature extraction and K-FCM clustering for segmentation, the proposed approach enhances the reliability of tumor detection and segmentation. Experimental results highlight the framework's effectiveness, achieving 98\% accuracy, 96.15\% precision, and 100\% recall, demonstrating its potential as a robust tool for advanced medical image analysis.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with Multi-modality Refinement Module</title>
<link>https://arxiv.org/abs/2501.08659</link>
<guid>https://arxiv.org/abs/2501.08659</guid>
<content:encoded><![CDATA[
arXiv:2501.08659v4 Announce Type: replace 
Abstract: Visual odometry (VO) plays a crucial role in autonomous driving, robotic navigation, and other related tasks by estimating the position and orientation of a camera based on visual input. Significant progress has been made in data-driven VO methods, particularly those leveraging deep learning techniques to extract image features and estimate camera poses. However, these methods often struggle in low-light conditions because of the reduced visibility of features and the increased difficulty of matching keypoints. To address this limitation, we introduce BrightVO, a novel VO model based on Transformer architecture, which not only performs front-end visual feature extraction, but also incorporates a multi-modality refinement module in the back-end that integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization, this module iteratively refines pose estimates to reduce errors and improve both accuracy and robustness. Furthermore, we create a synthetic low-light dataset, KiC4R, which includes a variety of lighting conditions to facilitate the training and evaluation of VO frameworks in challenging environments. Experimental results demonstrate that BrightVO achieves state-of-the-art performance on both the KiC4R dataset and the KITTI benchmarks. Specifically, it provides an average improvement of 20% in pose estimation accuracy in normal outdoor environments and 259% in low-light conditions, outperforming existing methods. For widespread use and further development, the research work is fully open-source at https://github.com/Anastasiawd/BrightVO.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction</title>
<link>https://arxiv.org/abs/2501.11124</link>
<guid>https://arxiv.org/abs/2501.11124</guid>
<content:encoded><![CDATA[
arXiv:2501.11124v2 Announce Type: replace 
Abstract: Pseudo-label learning methods have been widely applied in weakly-supervised temporal action localization. Existing works directly utilize weakly-supervised base model to generate instance-level pseudo-labels for training the fully-supervised detection head. We argue that the noise in pseudo-labels would interfere with the learning of fully-supervised detection head, leading to significant performance leakage. Issues with noisy labels include:(1) inaccurate boundary localization; (2) undetected short action clips; (3) multiple adjacent segments incorrectly detected as one segment. To target these issues, we introduce a two-stage noisy label learning strategy to harness every potential useful signal in noisy labels. First, we propose a frame-level pseudo-label generation model with a context-aware denoising algorithm to refine the boundaries. Second, we introduce an online-revised teacher-student framework with a missing instance compensation module and an ambiguous instance correction module to solve the short-action-missing and many-to-one problems. Besides, we apply a high-quality pseudo-label mining loss in our online-revised teacher-student framework to add different weights to the noisy labels to train more effectively. Our model outperforms the previous state-of-the-art method in detection accuracy and inference speed greatly upon the THUMOS14 and ActivityNet v1.2 benchmarks.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2501.16289</link>
<guid>https://arxiv.org/abs/2501.16289</guid>
<content:encoded><![CDATA[
arXiv:2501.16289v3 Announce Type: replace 
Abstract: Point cloud representation has recently become a research hotspot in the field of computer vision and has been utilized for autonomous vehicles. However, adapting deep learning networks for point cloud data recognition is challenging due to the variability in datasets and sensor technologies. This variability underscores the necessity for adaptive techniques to maintain accuracy under different conditions. In this paper, we present the Multi-View Structural Convolution Network (MSCN) designed for domain-invariant point cloud recognition. MSCN comprises Structural Convolution Layers (SCL) that extract local context geometric features from point clouds and Structural Aggregation Layers (SAL) that extract and aggregate both local and overall context features from point clouds. Additionally, our MSCN enhances feature representation robustness by training with unseen domain point clouds derived from source domain point clouds. This method acquires domain-invariant features and exhibits robust, consistent performance across various point cloud datasets, ensuring compatibility with diverse sensor configurations without the need for parameter adjustments. This highlights MSCN's potential to significantly improve the reliability and domain invariant features in different environments. Our code is available at https://github.com/MLMLab/MSCN.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment</title>
<link>https://arxiv.org/abs/2501.17690</link>
<guid>https://arxiv.org/abs/2501.17690</guid>
<content:encoded><![CDATA[
arXiv:2501.17690v2 Announce Type: replace 
Abstract: We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding Depth Perception in Foveated Rendering</title>
<link>https://arxiv.org/abs/2501.18635</link>
<guid>https://arxiv.org/abs/2501.18635</guid>
<content:encoded><![CDATA[
arXiv:2501.18635v2 Announce Type: replace 
Abstract: The true vision for real-time virtual and augmented reality is reproducing our visual reality in its entirety on immersive displays. To this end, foveated rendering leverages the limitations of spatial acuity in human peripheral vision to allocate computational resources to the fovea while reducing quality in the periphery. Such methods are often derived from studies on the spatial resolution of the human visual system and its ability to perceive blur in the periphery, enabling the potential for high spatial quality in real-time. However, the effects of blur on other visual cues that depend on luminance contrast, such as depth, remain largely unexplored. It is critical to understand this interplay, as accurate depth representation is a fundamental aspect of visual realism. In this paper, we present the first evaluation exploring the effects of foveated rendering on stereoscopic depth perception. We design a psychovisual experiment to quantitatively study the effects of peripheral blur on depth perception. Our analysis demonstrates that stereoscopic acuity remains unaffected (or even improves) by high levels of peripheral blur. Based on our studies, we derive a simple perceptual model that determines the amount of foveation that does not affect stereoacuity. Furthermore, we analyze the model in the context of common foveation practices reported in literature. The findings indicate that foveated rendering does not impact stereoscopic depth perception, and stereoacuity remains unaffected with up to 2x stronger foveation than commonly used. Finally, we conduct a validation experiment and show that our findings hold for complex natural stimuli.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning</title>
<link>https://arxiv.org/abs/2502.02454</link>
<guid>https://arxiv.org/abs/2502.02454</guid>
<content:encoded><![CDATA[
arXiv:2502.02454v4 Announce Type: replace 
Abstract: Using extensive training data from SA-1B, the Segment Anything Model (SAM) has demonstrated exceptional generalization and zero-shot capabilities, attracting widespread attention in areas such as medical image segmentation and remote sensing image segmentation. However, its performance in the field of image manipulation detection remains largely unexplored and unconfirmed. There are two main challenges in applying SAM to image manipulation detection: a) reliance on manual prompts, and b) the difficulty of single-view information in supporting cross-dataset generalization. To address these challenges, we develops a cross-view prompt learning paradigm called IMDPrompter based on SAM. Benefiting from the design of automated prompts, IMDPrompter no longer relies on manual guidance, enabling automated detection and localization. Additionally, we propose components such as Cross-view Feature Perception, Optimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate cross-view perceptual learning and guide SAM to generate accurate masks. Extensive experimental results from five datasets (CASIA, Columbia, Coverage, IMD2020, and NIST16) validate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Adaptive Prompting for Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2502.20292</link>
<guid>https://arxiv.org/abs/2502.20292</guid>
<content:encoded><![CDATA[
arXiv:2502.20292v3 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language</title>
<link>https://arxiv.org/abs/2503.01453</link>
<guid>https://arxiv.org/abs/2503.01453</guid>
<content:encoded><![CDATA[
arXiv:2503.01453v2 Announce Type: replace 
Abstract: Most existing works in image caption synthesis use computation heavy deep neural networks and generates image descriptions in English language. This often restricts this important assistive tool for widespread use across language and accessibility barriers. This work presents AC-Lite, a computationally efficient model for image captioning in low-resource Assamese language. AC-Lite reduces computational requirements by replacing computation-heavy deep network components with lightweight alternatives. The AC-Lite model is designed through extensive ablation experiments with different image feature extractor networks and language decoders. A combination of ShuffleNetv2x1.5 with GRU based language decoder along with bilinear attention is found to provide the best performance with minimum compute. AC-Lite was observed to achieve an 82.3 CIDEr score on the COCO-AC dataset with 2.45 GFLOPs and 22.87M parameters.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies</title>
<link>https://arxiv.org/abs/2503.02891</link>
<guid>https://arxiv.org/abs/2503.02891</guid>
<content:encoded><![CDATA[
arXiv:2503.02891v2 Announce Type: replace 
Abstract: In recent years, vision transformers (ViTs) have emerged as powerful and promising techniques for computer vision tasks such as image classification, object detection, and segmentation. Unlike convolutional neural networks (CNNs), which rely on hierarchical feature extraction, ViTs treat images as sequences of patches and leverage self-attention mechanisms. However, their high computational complexity and memory demands pose significant challenges for deployment on resource-constrained edge devices. To address these limitations, extensive research has focused on model compression techniques and hardware-aware acceleration strategies. Nonetheless, a comprehensive review that systematically categorizes these techniques and their trade-offs in accuracy, efficiency, and hardware adaptability for edge deployment remains lacking. This survey bridges this gap by providing a structured analysis of model compression techniques, software tools for inference on edge, and hardware acceleration strategies for ViTs. We discuss their impact on accuracy, efficiency, and hardware adaptability, highlighting key challenges and emerging research directions to advance ViT deployment on edge platforms, including graphics processing units (GPUs), application-specific integrated circuit (ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire further research with a contemporary guide on optimizing ViTs for efficient deployment on edge devices.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning</title>
<link>https://arxiv.org/abs/2503.12026</link>
<guid>https://arxiv.org/abs/2503.12026</guid>
<content:encoded><![CDATA[
arXiv:2503.12026v2 Announce Type: replace 
Abstract: Self-supervised video correspondence learning depends on the ability to accurately associate pixels between video frames that correspond to the same visual object. However, achieving reliable pixel matching without supervision remains a major challenge. To address this issue, recent research has focused on feature learning techniques that aim to encode unique pixel representations for matching. Despite these advances, existing methods still struggle to achieve exact pixel correspondences and often suffer from false matches, limiting their effectiveness in self-supervised settings.
  To this end, we explore an efficient self-supervised Video Correspondence Learning framework (MER) that aims to accurately extract object details from unlabeled videos. First, we design a dedicated Motion Enhancement Engine that emphasizes capturing the dynamic motion of objects in videos. In addition, we introduce a flexible sampling strategy for inter-pixel correspondence information (Multi-Cluster Sampler) that enables the model to pay more attention to the pixel changes of important objects in motion. Through experiments, our algorithm outperforms the state-of-the-art competitors on video correspondence learning tasks such as video object segmentation and video object keypoint tracking.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes</title>
<link>https://arxiv.org/abs/2503.13435</link>
<guid>https://arxiv.org/abs/2503.13435</guid>
<content:encoded><![CDATA[
arXiv:2503.13435v2 Announce Type: replace 
Abstract: With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D, which generates stable and high-quality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. Project: https://github.com/Gen-Verse/WideRange4D
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection between Point and Text Prompts</title>
<link>https://arxiv.org/abs/2503.19769</link>
<guid>https://arxiv.org/abs/2503.19769</guid>
<content:encoded><![CDATA[
arXiv:2503.19769v2 Announce Type: replace 
Abstract: Segmentation is a fundamental task in computer vision, with prompt-driven methods gaining prominence due to their flexibility. The Segment Anything Model (SAM) excels at point-prompted segmentation, while text-based models, often leveraging powerful multimodal encoders like BEIT-3, provide rich semantic understanding. However, effectively combining these complementary modalities remains a challenge. This paper introduces BiPrompt-SAM, a novel dual-modal prompt segmentation framework employing an explicit selection mechanism. We leverage SAM's ability to generate multiple mask candidates from a single point prompt and use a text-guided mask (generated via EVF-SAM with BEIT-3) to select the point-generated mask that best aligns spatially, measured by Intersection over Union (IoU). This approach, interpretable as a simplified Mixture of Experts (MoE), effectively fuses spatial precision and semantic context without complex model modifications. Notably, our method achieves strong zero-shot performance on the Endovis17 medical dataset (89.55% mDice, 81.46% mIoU) using only a single point prompt per instance. This significantly reduces annotation burden compared to bounding boxes and aligns better with practical clinical workflows, demonstrating the method's effectiveness without domain-specific training. On the RefCOCO series, BiPrompt-SAM attained 87.1%, 86.5%, and 85.8% IoU, significantly outperforming existing approaches. Experiments show BiPrompt-SAM excels in scenarios requiring both spatial accuracy and semantic disambiguation, offering a simple, effective, and interpretable perspective on multi-modal prompt fusion.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GISE-TTT:A Framework for Global InformationSegmentation and Enhancement</title>
<link>https://arxiv.org/abs/2504.00879</link>
<guid>https://arxiv.org/abs/2504.00879</guid>
<content:encoded><![CDATA[
arXiv:2504.00879v2 Announce Type: replace 
Abstract: This paper addresses the challenge of capturing global temporaldependencies in long video sequences for Video Object Segmentation (VOS). Existing architectures often fail to effectively model these dependencies acrossextended temporal horizons. To overcome this limitation, we introduce GISE-TTT, anovel architecture that integrates Temporal Transformer (TTT) layers intotransformer-based frameworks through a co-designed hierarchical approach.The TTTlayer systematically condenses historical temporal information into hidden states thatencode globally coherent contextual representations. By leveraging multi-stagecontextual aggregation through hierarchical concatenation, our frameworkprogressively refines spatiotemporal dependencies across network layers. This designrepresents the first systematic empirical evidence that distributing global informationacross multiple network layers is critical for optimal dependency utilization in videosegmentation tasks.Ablation studies demonstrate that incorporating TTT modules athigh-level feature stages significantly enhances global modeling capabilities, therebyimproving the network's ability to capture long-range temporal relationships. Extensive experiments on DAVIS 2017 show that GISE-TTT achieves a 3.2%improvement in segmentation accuracy over the baseline model, providingcomprehensive evidence that global information should be strategically leveragedthroughout the network architecture.The code will be made available at:https://github.com/uuool/GISE-TTT.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes</title>
<link>https://arxiv.org/abs/2504.09948</link>
<guid>https://arxiv.org/abs/2504.09948</guid>
<content:encoded><![CDATA[
arXiv:2504.09948v2 Announce Type: replace 
Abstract: Dish images play a crucial role in the digital era, with the demand for culturally distinctive dish images continuously increasing due to the digitization of the food industry and e-commerce. In general cases, existing text-to-image generation models excel in producing high-quality images; however, they struggle to capture diverse characteristics and faithful details of specific domains, particularly Chinese dishes. To address this limitation, we propose Omni-Dish, the first text-to-image generation model specifically tailored for Chinese dishes. We develop a comprehensive dish curation pipeline, building the largest dish dataset to date. Additionally, we introduce a recaption strategy and employ a coarse-to-fine training scheme to help the model better learn fine-grained culinary nuances. During inference, we enhance the user's textual input using a pre-constructed high-quality caption library and a large language model, enabling more photorealistic and faithful image generation. Furthermore, to extend our model's capability for dish editing tasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish editing dataset and train a specialized editing model. Extensive experiments demonstrate the superiority of our methods.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*</title>
<link>https://arxiv.org/abs/2504.11014</link>
<guid>https://arxiv.org/abs/2504.11014</guid>
<content:encoded><![CDATA[
arXiv:2504.11014v4 Announce Type: replace 
Abstract: The emerging trend in computer vision emphasizes developing universal models capable of simultaneously addressing multiple diverse tasks. Such universality typically requires joint training across multi-domain datasets to ensure effective generalization. However, monocular 3D object detection presents unique challenges in multi-domain training due to the scarcity of datasets annotated with accurate 3D ground-truth labels, especially beyond typical road-based autonomous driving contexts. To address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels. Current pretrained models often struggle to accurately detect pedestrians in non-road environments due to inherent dataset biases. Unlike generalized image-based 2D object detection models, achieving similar generalization in monocular 3D detection remains largely unexplored. In this paper, we propose GATE3D, a novel framework designed specifically for generalized monocular 3D object detection via weak supervision. GATE3D effectively bridges domain gaps by employing consistency losses between 2D and 3D predictions. Remarkably, our model achieves competitive performance on the KITTI benchmark as well as on an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework. Our results demonstrate that GATE3D significantly accelerates learning from limited annotated data through effective pre-training strategies, highlighting substantial potential for broader impacts in robotics, augmented reality, and virtual reality applications. Project page: https://ies0411.github.io/GATE3D/
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video</title>
<link>https://arxiv.org/abs/2504.15122</link>
<guid>https://arxiv.org/abs/2504.15122</guid>
<content:encoded><![CDATA[
arXiv:2504.15122v2 Announce Type: replace 
Abstract: We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a physically-inspired Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both global camera and local object motion. Our MoBGS framework ensures the temporal consistency of unseen latent timestamps and robust motion decomposition of static and dynamic regions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent advanced methods (DyBluRF and Deblur4DGS), achieving state-of-the-art performance for dynamic NVS under motion blur.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding</title>
<link>https://arxiv.org/abs/2301.11564</link>
<guid>https://arxiv.org/abs/2301.11564</guid>
<content:encoded><![CDATA[
arXiv:2301.11564v3 Announce Type: replace-cross 
Abstract: Robotic grasping is a fundamental ability for a robot to interact with the environment. Current methods focus on how to obtain a stable and reliable grasping pose in object level, while little work has been studied on part (shape)-wise grasping which is related to fine-grained grasping and robotic affordance. Parts can be seen as atomic elements to compose an object, which contains rich semantic knowledge and a strong correlation with affordance. However, lacking a large part-wise 3D robotic dataset limits the development of part representation learning and downstream applications. In this paper, we propose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to promote 3D part-level affordance and grasping ability learning. From the perspective of robotic cognition, we design a two-stage fine-grained robotic grasping framework (named LangPartGPD), including a novel 3D part language grounding model and a part-aware grasp pose detection model, in which explicit language input from human or large language models (LLMs) could guide a robot to generate part-level 6-DoF grasping pose with textual explanation. Our method combines the advantages of human-robot collaboration and LLMs' planning ability using explicit language as a symbolic intermediate. To evaluate the effectiveness of our proposed method, we perform 3D part grounding and fine-grained grasp detection experiments on both simulation and physical robot settings, following language instructions across different degrees of textual complexity. Results show our method achieves competitive performance in 3D geometry fine-grained grounding, object affordance inference, and 3D part-aware grasping tasks. Our dataset and code are available on our project website https://sites.google.com/view/lang-shape
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games</title>
<link>https://arxiv.org/abs/2312.02312</link>
<guid>https://arxiv.org/abs/2312.02312</guid>
<content:encoded><![CDATA[
arXiv:2312.02312v2 Announce Type: replace-cross 
Abstract: Video games have served as useful benchmarks for the decision-making community, but going beyond Atari games towards modern games has been prohibitively expensive for the vast majority of the research community. Prior work in modern video games typically relied on game-specific integration to obtain game features and enable online training, or on existing large datasets. An alternative approach is to train agents using imitation learning to play video games purely from images. However, this setting poses a fundamental question: which visual encoders obtain representations that retain information critical for decision making? To answer this question, we conduct a systematic study of imitation learning with publicly available pre-trained visual encoders compared to the typical task-specific end-to-end training approach in Minecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our results show that end-to-end training can be effective with comparably low-resolution images and only minutes of demonstrations, but significant improvements can be gained by utilising pre-trained encoders such as DINOv2 depending on the game. In addition to enabling effective decision making, we show that pre-trained encoders can make decision-making research in video games more accessible by significantly reducing the cost of training.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Redshift: Random Networks are not Random Functions</title>
<link>https://arxiv.org/abs/2403.02241</link>
<guid>https://arxiv.org/abs/2403.02241</guid>
<content:encoded><![CDATA[
arXiv:2403.02241v3 Announce Type: replace-cross 
Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.
  Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inherit all these properties from their building blocks.
  Implications. We provide a fresh explanation for the success of deep learning independent from gradient-based training. It points at promising avenues for controlling the solutions implemented by trained models.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObjectFinder: An Open-Vocabulary Assistive System for Interactive Object Search by Blind People</title>
<link>https://arxiv.org/abs/2412.03118</link>
<guid>https://arxiv.org/abs/2412.03118</guid>
<content:encoded><![CDATA[
arXiv:2412.03118v2 Announce Type: replace-cross 
Abstract: Searching for objects in unfamiliar scenarios is a challenging task for blind people. It involves specifying the target object, detecting it, and then gathering detailed information according to the user's intent. However, existing description- and detection-based assistive technologies do not sufficiently support the multifaceted nature of interactive object search tasks. We present ObjectFinder, an open-vocabulary wearable assistive system for interactive object search by blind people. ObjectFinder allows users to query target objects using flexible wording. Once the target object is detected, it provides egocentric localization information in real-time, including distance and direction. Users can then initiate different branches to gather detailed information based on their intent towards the target object, such as navigating to it or perceiving its surroundings. ObjectFinder is powered by a seamless combination of open-vocabulary models, namely an open-vocabulary object detector and a multimodal large language model. The ObjectFinder design concept and its development were carried out in collaboration with a blind co-designer. To evaluate ObjectFinder, we conducted an exploratory user study with eight blind participants. We compared ObjectFinder to BeMyAI and Google Lookout, popular description- and detection-based assistive applications. Our findings indicate that most participants felt more independent with ObjectFinder and preferred it for object search, as it enhanced scene context gathering and navigation, and allowed for active target identification. Finally, we discuss the implications for future assistive systems to support interactive object search.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate Segmentation of COVID-19 Lung Infections from CT Images</title>
<link>https://arxiv.org/abs/2412.06314</link>
<guid>https://arxiv.org/abs/2412.06314</guid>
<content:encoded><![CDATA[
arXiv:2412.06314v2 Announce Type: replace-cross 
Abstract: Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has emerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical settings, the segmentation of lung infections from computed tomography images enables rapid and accurate quantification and diagnosis of COVID-19. Segmentation of COVID-19 infections in the lungs poses a formidable challenge, primarily due to the indistinct boundaries and limited contrast presented by ground glass opacity manifestations. Moreover, the confounding similarity between infiltrates, lung tissues, and lung walls further complicates this segmentation task. To address these challenges, this paper introduces a novel deep network architecture, called CAD-Unet, for segmenting COVID-19 lung infections. In this architecture, capsule networks are incorporated into the existing Unet framework. Capsule networks represent a novel network architecture that differs from traditional convolutional neural networks. They utilize vectors for information transfer among capsules, facilitating the extraction of intricate lesion spatial information. Additionally, we design a capsule encoder path and establish a coupling path between the unet encoder and the capsule encoder. This design maximizes the complementary advantages of both network structures while achieving efficient information fusion. \noindent Finally, extensive experiments are conducted on four publicly available datasets, encompassing binary segmentation tasks and multi-class segmentation tasks. The experimental results demonstrate the superior segmentation performance of the proposed model. The code has been released at: https://github.com/AmanoTooko-jie/CAD-Unet.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis</title>
<link>https://arxiv.org/abs/2412.19723</link>
<guid>https://arxiv.org/abs/2412.19723</guid>
<content:encoded><![CDATA[
arXiv:2412.19723v2 Announce Type: replace-cross 
Abstract: Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explorations of the Softmax Space: Knowing When the Neural Network Doesn't Know</title>
<link>https://arxiv.org/abs/2502.00456</link>
<guid>https://arxiv.org/abs/2502.00456</guid>
<content:encoded><![CDATA[
arXiv:2502.00456v2 Announce Type: replace-cross 
Abstract: Ensuring the reliability of automated decision-making based on neural networks will be crucial as Artificial Intelligence systems are deployed more widely in critical situations. This paper proposes a new approach for measuring confidence in the predictions of any neural network that relies on the predictions of a softmax layer. We identify that a high-accuracy trained network may have certain outputs for which there should be low confidence. In such cases, decisions should be deferred and it is more appropriate for the network to provide a \textit{not known} answer to a corresponding classification task. Our approach clusters the vectors in the softmax layer to measure distances between cluster centroids and network outputs. We show that a cluster with centroid calculated simply as the mean softmax output for all correct predictions can serve as a suitable proxy in the evaluation of confidence. Defining a distance threshold for a class as the smallest distance from an incorrect prediction to the given class centroid offers a simple approach to adding \textit{not known} answers to any network classification falling outside of the threshold. We evaluate the approach on the MNIST and CIFAR-10 datasets using a Convolutional Neural Network and a Vision Transformer, respectively. The results show that our approach is consistent across datasets and network models, and indicate that the proposed distance metric can offer an efficient way of determining when automated predictions are acceptable and when they should be deferred to human operators.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Elucidating the Preconditioning in Consistency Distillation</title>
<link>https://arxiv.org/abs/2502.02922</link>
<guid>https://arxiv.org/abs/2502.02922</guid>
<content:encoded><![CDATA[
arXiv:2502.02922v3 Announce Type: replace-cross 
Abstract: Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (PF) ordinary differential equation (ODE) trajectory determined by the teacher model. Preconditioning is a vital technique for stabilizing consistency distillation, by linear combining the input data and the network output with pre-defined coefficients as the consistency function. It imposes the boundary condition of consistency functions without restricting the form and expressiveness of the neural network. However, previous preconditionings are hand-crafted and may be suboptimal choices. In this work, we offer the first theoretical insights into the preconditioning in consistency distillation, by elucidating its design criteria and the connection to the teacher ODE trajectory. Based on these analyses, we further propose a principled way dubbed \textit{Analytic-Precond} to analytically optimize the preconditioning according to the consistency gap (defined as the gap between the teacher denoiser and the optimal student denoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond can facilitate the learning of trajectory jumpers, enhance the alignment of the student trajectory with the teacher's, and achieve $2\times$ to $3\times$ training acceleration of consistency trajectory models in multi-step generation across various datasets.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image Generation</title>
<link>https://arxiv.org/abs/2502.08528</link>
<guid>https://arxiv.org/abs/2502.08528</guid>
<content:encoded><![CDATA[
arXiv:2502.08528v2 Announce Type: replace-cross 
Abstract: The properties of black holes and accretion flows can be inferred by fitting Event Horizon Telescope (EHT) data to simulated images generated through general relativistic ray tracing (GRRT). However, due to the computationally intensive nature of GRRT, the efficiency of generating specific radiation flux images needs to be improved. This paper introduces the Branch Correction Denoising Diffusion Model (BCDDM), which uses a branch correction mechanism and a weighted mixed loss function to improve the accuracy of generated black hole images based on seven physical parameters of the radiatively inefficient accretion flow (RIAF) model. Our experiments show a strong correlation between the generated images and their physical parameters. By enhancing the GRRT dataset with BCDDM-generated images and using ResNet50 for parameter regression, we achieve significant improvements in parameter prediction performance. This approach reduces computational costs and provides a faster, more efficient method for dataset expansion, parameter estimation, and model fitting.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ScaleFusionNet: Transformer-Guided Multi-Scale Feature Fusion for Skin Lesion Segmentation</title>
<link>https://arxiv.org/abs/2503.03327</link>
<guid>https://arxiv.org/abs/2503.03327</guid>
<content:encoded><![CDATA[
arXiv:2503.03327v2 Announce Type: replace-cross 
Abstract: Melanoma is a malignant tumor originating from skin cell lesions. Accurate and efficient segmentation of skin lesions is essential for quantitative medical analysis but remains challenging. To address this, we propose ScaleFusionNet, a segmentation model that integrates Cross-Attention Transformer Module (CATM) and AdaptiveFusionBlock to enhance feature extraction and fusion. The model employs a hybrid architecture encoder that effectively captures both local and global features. We introduce CATM, which utilizes Swin Transformer Blocks and Cross Attention Fusion (CAF) to adaptively refine encoder-decoder feature fusion, reducing semantic gaps and improving segmentation accuracy. Additionally, the AdaptiveFusionBlock is improved by integrating adaptive multi-scale fusion, where Swin Transformer-based attention complements deformable convolution-based multi-scale feature extraction. This enhancement refines lesion boundaries and preserves fine-grained details. ScaleFusionNet achieves Dice scores of 92.94% and 91.65% on ISIC-2016 and ISIC-2018 datasets, respectively, demonstrating its effectiveness in skin lesion analysis. Our code implementation is publicly available at GitHub.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems</title>
<link>https://arxiv.org/abs/2503.06669</link>
<guid>https://arxiv.org/abs/2503.06669</guid>
<content:encoded><![CDATA[
arXiv:2503.06669v3 Announce Type: replace-cross 
Abstract: We explore how scalable robot data can address real-world challenges for generalized robotic manipulation. Introducing AgiBot World, a large-scale platform comprising over 1 million trajectories across 217 tasks in five deployment scenarios, we achieve an order-of-magnitude increase in data scale compared to existing datasets. Accelerated by a standardized collection pipeline with human-in-the-loop verification, AgiBot World guarantees high-quality and diverse data distribution. It is extensible from grippers to dexterous hands and visuo-tactile sensors for fine-grained skill acquisition. Building on top of data, we introduce Genie Operator-1 (GO-1), a novel generalist policy that leverages latent action representations to maximize data utilization, demonstrating predictable performance scaling with increased data volume. Policies pre-trained on our dataset achieve an average performance improvement of 30% over those trained on Open X-Embodiment, both in in-domain and out-of-distribution scenarios. GO-1 exhibits exceptional capability in real-world dexterous and long-horizon tasks, achieving over 60% success rate on complex tasks and outperforming prior RDT approach by 32%. By open-sourcing the dataset, tools, and models, we aim to democratize access to large-scale, high-quality robot data, advancing the pursuit of scalable and general-purpose intelligence.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding</title>
<link>https://arxiv.org/abs/2503.18578</link>
<guid>https://arxiv.org/abs/2503.18578</guid>
<content:encoded><![CDATA[
arXiv:2503.18578v2 Announce Type: replace-cross 
Abstract: Modern vision-language models (VLMs) develop patch embedding and convolution backbone within vector space, especially Euclidean ones, at the very founding. When expanding VLMs to a galaxy scale for understanding astronomical phenomena, the integration of spherical space for planetary orbits and hyperbolic spaces for black holes raises two formidable challenges. a) The current pre-training model is confined to Euclidean space rather than a comprehensive geometric embedding. b) The predominant architecture lacks suitable backbones for anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a geometry-aware VLM, for the universe-level vision understanding tasks. We proposed the geometry prompt that generates geometry tokens by random walks across diverse spaces on a multi-scale physical graph, along with a geometry adapter that compresses and reshapes the space anisotropy in a mixture-of-experts manner. Extensive experiments demonstrate the effectiveness of our approach, with Galaxy-Walker achieving state-of-the-art performance in both galaxy property estimation ($R^2$ scores up to $0.91$) and morphology classification tasks (up to $+0.17$ F1 improvement in challenging features), significantly outperforming both domain-specific models and general-purpose VLMs.
]]></content:encoded>
<pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Frame Extraction: A Novel Approach Through Frame Similarity and Surgical Tool Tracking for Video Segmentation</title>
<link>https://arxiv.org/abs/2501.11153</link>
<guid>https://arxiv.org/abs/2501.11153</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, surgical procedures, video analysis, Kinematics Adaptive Frame Recognition, dataset reduction

Summary:
The article discusses the growing interest in using Artificial Intelligence (AI) to analyze surgical videos for performance assessment. Due to the length of operative videos, AI models face challenges in learning effectively from them. To address this issue, the authors propose a novel technique called Kinematics Adaptive Frame Recognition (KAFR) that eliminates redundant frames, reducing dataset size and computation time while maintaining accuracy. The process involves tracking surgical tools, computing similarities between frames, and training a CNN for classification. The effectiveness of KAFR is evaluated using datasets from Gastrojejunostomy (GJ) and Pancreaticojejunostomy (PJ) procedures at referral centers. The results showcase the potential of KAFR in enhancing the analysis of surgical videos, paving the way for more efficient and accurate AI-assisted surgical procedures.<br /><br />Summary: <div>
arXiv:2501.11153v3 Announce Type: replace 
Abstract: The interest in leveraging Artificial Intelligence (AI) for surgical procedures to automate analysis has witnessed a significant surge in recent years. One of the primary tools for recording surgical procedures and conducting subsequent analyses, such as performance assessment, is through videos. However, these operative videos tend to be notably lengthy compared to other fields, spanning from thirty minutes to several hours, which poses a challenge for AI models to effectively learn from them. Despite this challenge, the foreseeable increase in the volume of such videos in the near future necessitates the development and implementation of innovative techniques to tackle this issue effectively. In this article, we propose a novel technique called Kinematics Adaptive Frame Recognition (KAFR) that can efficiently eliminate redundant frames to reduce dataset size and computation time while retaining useful frames to improve accuracy. Specifically, we compute the similarity between consecutive frames by tracking the movement of surgical tools. Our approach follows these steps: $i)$ Tracking phase: a YOLOv8 model is utilized to detect tools presented in the scene, $ii)$ Similarity phase: Similarities between consecutive frames are computed by estimating variation in the spatial positions and velocities of the tools, $iii$) Classification phase: An X3D CNN is trained to classify segmentation. We evaluate the effectiveness of our approach by analyzing datasets obtained through retrospective reviews of cases at two referral centers. The newly annotated Gastrojejunostomy (GJ) dataset covers procedures performed between 2017 and 2021, while the previously annotated Pancreaticojejunostomy (PJ) dataset spans from 2011 to 2022 at the same centers.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Approach for Identification of Potato Leaf Diseases Using Wrapper Feature Selection and Feature Concatenation</title>
<link>https://arxiv.org/abs/2502.03370</link>
<guid>https://arxiv.org/abs/2502.03370</guid>
<content:encoded><![CDATA[
<div> Keywords: potato, late blight disease, image processing, machine learning, SVM classifier

Summary:
The study focuses on detecting late blight disease in potato plants using image processing and machine learning techniques. Late blight is a common leaf disease that affects potato crops and can significantly decrease yields. The proposed method includes four phases: improving image quality with Histogram Equalization, extracting features using a Deep CNN model, performing feature selection with wrapper-based selection, and classifying using SVM and its variants. The method achieved a high accuracy of 99% by selecting 550 features. This autonomous approach could help farmers detect late blight early, leading to enhanced crop yield and management of the disease. <div>
arXiv:2502.03370v4 Announce Type: replace 
Abstract: The potato is a widely grown crop in many regions of the world. In recent decades, potato farming has gained incredible traction in the world. Potatoes are susceptible to several illnesses that stunt their development. This plant seems to have significant leaf disease. Early Blight and Late Blight are two prevalent leaf diseases that affect potato plants. The early detection of these diseases would be beneficial for enhancing the yield of this crop. The ideal solution is to use image processing to identify and analyze these disorders. Here, we present an autonomous method based on image processing and machine learning to detect late blight disease affecting potato leaves. The proposed method comprises four different phases: (1) Histogram Equalization is used to improve the quality of the input image; (2) feature extraction is performed using a Deep CNN model, then these extracted features are concatenated; (3) feature selection is performed using wrapper-based feature selection; (4) classification is performed using an SVM classifier and its variants. This proposed method achieves the highest accuracy of 99% using SVM by selecting 550 features.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Geometry Save Central Views for Sports Field Registration?</title>
<link>https://arxiv.org/abs/2504.20052</link>
<guid>https://arxiv.org/abs/2504.20052</guid>
<content:encoded><![CDATA[
<div> circle correspondences, sports field registration, image annotation, geometric method, bottom-up 

Summary:
In the field of sports field registration, where accurate extraction of 3D information from broadcast videos is important for various applications such as sports analytics and fan engagement, existing methods often struggle with close-up camera views that show only line and circle markings on sports fields. This is due to the sparse and uneven distribution of field markings, which pose a challenge for including circle correspondences in linear equations. To address this issue, a novel geometric method has been proposed in this work that derives points and lines from circle correspondences, allowing for improved sports field registration and image annotation. Experimental results demonstrate the effectiveness of this bottom-up approach in complementing top-performing detectors and enabling registration in challenging scenarios. <div>
arXiv:2504.20052v1 Announce Type: new 
Abstract: Single-frame sports field registration often serves as the foundation for extracting 3D information from broadcast videos, enabling applications related to sports analytics, refereeing, or fan engagement. As sports fields have rigorous specifications in terms of shape and dimensions of their line, circle and point components, sports field markings are commonly used as calibration targets for this task. However, because of the sparse and uneven distribution of field markings, close-up camera views around central areas of the field often depict only line and circle markings. On these views, sports field registration is challenging for the vast majority of existing methods, as they focus on leveraging line field markings and their intersections. It is indeed a challenge to include circle correspondences in a set of linear equations. In this work, we propose a novel method to derive a set of points and lines from circle correspondences, enabling the exploitation of circle correspondences for both sports field registration and image annotation. In our experiments, we illustrate the benefits of our bottom-up geometric method against top-performing detectors and show that our method successfully complements them, enabling sports field registration in difficult scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment</title>
<link>https://arxiv.org/abs/2504.20054</link>
<guid>https://arxiv.org/abs/2504.20054</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, image-text alignment, multi-object editing, multi-agent reasoning, object counting

Summary:
Marmot is a novel framework designed to improve the accuracy of diffusion models in handling complex multi-object scenes. By employing Multi-Agent Reasoning for Multi-Object Self-Correcting, Marmot enhances image-text alignment and facilitates coherent multi-object image editing. The framework divides the self-correction task into object-level subtasks focusing on counting, attributes, and spatial relationships. A multi-agent editing system is implemented to mitigate interference between objects and improve editing reliability. To integrate subtask results effectively, a Pixel-Domain Stitching Smoother is introduced, enabling parallel processing and enhancing runtime efficiency. Experimental results demonstrate that Marmot significantly enhances accuracy in object counting, attribute assignment, and spatial relationships in image generation tasks. <div>
arXiv:2504.20054v1 Announce Type: new 
Abstract: While diffusion models excel at generating high-quality images, they often struggle with accurate counting, attributes, and spatial relationships in complex multi-object scenes. To address these challenges, we propose Marmot, a novel and generalizable framework that employs Multi-Agent Reasoning for Multi-Object Self-Correcting, enhancing image-text alignment and facilitating more coherent multi-object image editing. Our framework adopts a divide-and-conquer strategy that decomposes the self-correction task into three critical dimensions (counting, attributes, and spatial relationships), and further divided into object-level subtasks. We construct a multi-agent editing system featuring a decision-execution-verification mechanism, effectively mitigating inter-object interference and enhancing editing reliability. To resolve the problem of subtask integration, we propose a Pixel-Domain Stitching Smoother that employs mask-guided two-stage latent space optimization. This innovation enables parallel processing of subtask results, thereby enhancing runtime efficiency while eliminating multi-stage distortion accumulation. Extensive experiments demonstrate that Marmot significantly improves accuracy in object counting, attribute assignment, and spatial relationships for image generation tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Based Learning for Improved Classification Under Adversarial Noise</title>
<link>https://arxiv.org/abs/2504.20077</link>
<guid>https://arxiv.org/abs/2504.20077</guid>
<content:encoded><![CDATA[
<div> Keywords: Adversarial noise, Fast Gradient Sign Method (FGSM), Image classification, Edge features, Deep learning models

Summary: 
- The study analyzed the impact of adversarial noise on image classification, particularly focusing on the effects of the Fast Gradient Sign Method (FGSM).
- Training deep learning models on a combination of clean and noisy images improved recognition accuracy after adversarial attacks.
- Edge features in images were found to be relatively stable and provided essential structural information for classification.
- Models trained exclusively on edge-based representations demonstrated greater resilience to adversarial attacks compared to those trained on original images.
- While adversarial noise perturbs non-edge regions more, leveraging edge-based learning can enhance the robustness of deep learning models against adversarial perturbations.<br /><br /> <div>
arXiv:2504.20077v1 Announce Type: new 
Abstract: Adversarial noise introduces small perturbations in images, misleading deep learning models into misclassification and significantly impacting recognition accuracy. In this study, we analyzed the effects of Fast Gradient Sign Method (FGSM) adversarial noise on image classification and investigated whether training on specific image features can improve robustness. We hypothesize that while adversarial noise perturbs various regions of an image, edges may remain relatively stable and provide essential structural information for classification. To test this, we conducted a series of experiments using brain tumor and COVID datasets. Initially, we trained the models on clean images and then introduced subtle adversarial perturbations, which caused deep learning models to significantly misclassify the images. Retraining on a combination of clean and noisy images led to improved performance. To evaluate the robustness of the edge features, we extracted edges from the original/clean images and trained the models exclusively on edge-based representations. When noise was introduced to the images, the edge-based models demonstrated greater resilience to adversarial attacks compared to those trained on the original or clean images. These results suggest that while adversarial noise is able to exploit complex non-edge regions significantly more than edges, the improvement in the accuracy after retraining is marginally more in the original data as compared to the edges. Thus, leveraging edge-based learning can improve the resilience of deep learning models against adversarial perturbations.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoMultiAgents: A Multi-Agent Framework for Video Question Answering</title>
<link>https://arxiv.org/abs/2504.20091</link>
<guid>https://arxiv.org/abs/2504.20091</guid>
<content:encoded><![CDATA[
<div> framework, multimodal reasoning, VideoMultiAgents, caption generation, state-of-the-art performance

Summary:
The article introduces VideoMultiAgents, a framework for Video Question Answering (VQA) that utilizes specialized agents for vision, scene graph analysis, and text processing to enhance video understanding through complementary multimodal reasoning. This approach allows for a deeper understanding of video content by integrating visual, temporal, and linguistic cues. Additionally, the method incorporates question-guided caption generation, which produces captions highlighting objects, actions, and temporal transitions relevant to the query, thereby improving answer accuracy. Experimental results show significant performance improvements on various VQA datasets, including Intent-QA, EgoSchema subset, and NExT-QA, achieving state-of-the-art results. <div>
arXiv:2504.20091v1 Announce Type: new 
Abstract: Video Question Answering (VQA) inherently relies on multimodal reasoning, integrating visual, temporal, and linguistic cues to achieve a deeper understanding of video content. However, many existing methods rely on feeding frame-level captions into a single model, making it difficult to adequately capture temporal and interactive contexts. To address this limitation, we introduce VideoMultiAgents, a framework that integrates specialized agents for vision, scene graph analysis, and text processing. It enhances video understanding leveraging complementary multimodal reasoning from independently operating agents. Our approach is also supplemented with a question-guided caption generation, which produces captions that highlight objects, actions, and temporal transitions directly relevant to a given query, thus improving the answer accuracy. Experimental results demonstrate that our method achieves state-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA), EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Distance Field Demonstration of Imaging-Free Drone Identification in Intracity Environments</title>
<link>https://arxiv.org/abs/2504.20097</link>
<guid>https://arxiv.org/abs/2504.20097</guid>
<content:encoded><![CDATA[
<div> Residual Neural Networks, D2SP2-LiDAR, long-range detection, small objects, drone<br />
Summary:<br />
The article introduces a novel integration of residual neural networks with D2SP2-LiDAR for detecting small objects like drones over long distances up to 5 kilometers. This method offers a cost-effective and efficient alternative to traditional imaging-based approaches. By combining ResNet with D2SP2-LiDAR, the system achieves high-accuracy identification of drone poses and types, outperforming conventional imaging-based recognition systems. The experimental results demonstrate exceptional performance with 94.93% pose identification accuracy and 97.99% type classification accuracy, even under weak signal conditions with long distances and low signal-to-noise ratios. This innovation has significant implications for security, surveillance, environmental monitoring, and autonomous systems. The study highlights the feasibility and effectiveness of imaging-free methods for robust long-range detection of small targets in real-world scenarios.<br /> <div>
arXiv:2504.20097v1 Announce Type: new 
Abstract: Detecting small objects, such as drones, over long distances presents a significant challenge with broad implications for security, surveillance, environmental monitoring, and autonomous systems. Traditional imaging-based methods rely on high-resolution image acquisition, but are often constrained by range, power consumption, and cost. In contrast, data-driven single-photon-single-pixel light detection and ranging (\text{D\textsuperscript{2}SP\textsuperscript{2}-LiDAR}) provides an imaging-free alternative, directly enabling target identification while reducing system complexity and cost. However, its detection range has been limited to a few hundred meters. Here, we introduce a novel integration of residual neural networks (ResNet) with \text{D\textsuperscript{2}SP\textsuperscript{2}-LiDAR}, incorporating a refined observation model to extend the detection range to 5~\si{\kilo\meter} in an intracity environment while enabling high-accuracy identification of drone poses and types. Experimental results demonstrate that our approach not only outperforms conventional imaging-based recognition systems, but also achieves 94.93\% pose identification accuracy and 97.99\% type classification accuracy, even under weak signal conditions with long distances and low signal-to-noise ratios (SNRs). These findings highlight the potential of imaging-free methods for robust long-range detection of small targets in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An on-production high-resolution longitudinal neonatal fingerprint database in Brazil</title>
<link>https://arxiv.org/abs/2504.20104</link>
<guid>https://arxiv.org/abs/2504.20104</guid>
<content:encoded><![CDATA[
<div> Keywords: neonatal period, biometric identification, deep learning, fingerprint database, growth patterns

Summary:<br />
The neonatal period is crucial for survival and early identification is essential for interventions. Biometric solutions can enhance child protection but face challenges due to physiological changes in infants. Scaling factors have been used in previous studies but fail to capture the complex growth patterns of newborns. The lack of comprehensive biometric datasets hinders progress in this area. This study focuses on developing a high-quality database of neonatal fingerprints at various early life stages to train and evaluate machine learning models. The dataset aims to improve accuracy in predicting changes in biometric features due to growth, potentially leading to more reliable biometric identification systems tailored to newborns.<br /> 

Summary: <div>
arXiv:2504.20104v1 Announce Type: new 
Abstract: The neonatal period is critical for survival, requiring accurate and early identification to enable timely interventions such as vaccinations, HIV treatment, and nutrition programs. Biometric solutions offer potential for child protection by helping to prevent baby swaps, locate missing children, and support national identity systems. However, developing effective biometric identification systems for newborns remains a major challenge due to the physiological variability caused by finger growth, weight changes, and skin texture alterations during early development. Current literature has attempted to address these issues by applying scaling factors to emulate growth-induced distortions in minutiae maps, but such approaches fail to capture the complex and non-linear growth patterns of infants. A key barrier to progress in this domain is the lack of comprehensive, longitudinal biometric datasets capturing the evolution of neonatal fingerprints over time. This study addresses this gap by focusing on designing and developing a high-quality biometric database of neonatal fingerprints, acquired at multiple early life stages. The dataset is intended to support the training and evaluation of machine learning models aimed at emulating the effects of growth on biometric features. We hypothesize that such a dataset will enable the development of more robust and accurate Deep Learning-based models, capable of predicting changes in the minutiae map with higher fidelity than conventional scaling-based methods. Ultimately, this effort lays the groundwork for more reliable biometric identification systems tailored to the unique developmental trajectory of newborns.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image</title>
<link>https://arxiv.org/abs/2504.20111</link>
<guid>https://arxiv.org/abs/2504.20111</guid>
<content:encoded><![CDATA[
<div> watermarking, adversarial attack, diffusion models, image perturbations, watermark removal
<br />
Summary: 
An innovative black-box adversarial attack on watermarking schemes for diffusion models is proposed in this paper. The attack does not require access to diffusion model weights and is based on the concept of a many-to-one mapping between images and initial noises. By introducing perturbations to images, the attacker can forge or remove watermarks by entering or exiting specific regions in the clean image latent space. The effectiveness of the attack is demonstrated on various watermarking schemes across different diffusion models, revealing vulnerabilities and the need for improvement in existing watermarking methods. This research sheds light on the potential weaknesses in current watermarking techniques and highlights the importance of developing more robust and secure methods for protecting intellectual property. 
<br /><br />Summary: <div>
arXiv:2504.20111v1 Announce Type: new 
Abstract: Watermarking techniques are vital for protecting intellectual property and preventing fraudulent use of media. Most previous watermarking schemes designed for diffusion models embed a secret key in the initial noise. The resulting pattern is often considered hard to remove and forge into unrelated images. In this paper, we propose a black-box adversarial attack without presuming access to the diffusion model weights. Our attack uses only a single watermarked example and is based on a simple observation: there is a many-to-one mapping between images and initial noises. There are regions in the clean image latent space pertaining to each watermark that get mapped to the same initial noise when inverted. Based on this intuition, we propose an adversarial attack to forge the watermark by introducing perturbations to the images such that we can enter the region of watermarked images. We show that we can also apply a similar approach for watermark removal by learning perturbations to exit this region. We report results on multiple watermarking schemes (Tree-Ring, RingID, WIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0). Our results demonstrate the effectiveness of the attack and expose vulnerabilities in the watermarking methods, motivating future research on improving them.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals</title>
<link>https://arxiv.org/abs/2504.20178</link>
<guid>https://arxiv.org/abs/2504.20178</guid>
<content:encoded><![CDATA[
<div> Keywords: crowd-counting, multimodal fusion, Transformer networks, Convolutional Neural Networks, accuracy

Summary:
TransFusion is a new crowd-counting model that integrates Channel State Information (CSI) with image data through the use of Transformer networks. By combining these two distinct data modalities, TransFusion captures global contextual information essential for accurate crowd estimation. While Transformer networks excel at capturing global features, they may struggle with identifying detailed local features necessary for precise counting. To address this, Convolutional Neural Networks (CNNs) are incorporated into the model architecture to extract local features and complement the global context provided by Transformers. Experimental evaluations show that TransFusion achieves high accuracy with minimal counting errors while maintaining efficiency. This multimodal fusion approach enhances crowd counting performance by leveraging both global and local information derived from CSI and image data. <br /><br />Summary: <div>
arXiv:2504.20178v1 Announce Type: new 
Abstract: Current crowd-counting models often rely on single-modal inputs, such as visual images or wireless signal data, which can result in significant information loss and suboptimal recognition performance. To address these shortcomings, we propose TransFusion, a novel multimodal fusion-based crowd- counting model that integrates Channel State Information (CSI) with image data. By leveraging the powerful capabilities of Transformer networks, TransFusion effectively combines these two distinct data modalities, enabling the capture of comprehen- sive global contextual information that is critical for accurate crowd estimation. However, while transformers are well capable of capturing global features, they potentially fail to identify finer- grained, local details essential for precise crowd counting. To mitigate this, we incorporate Convolutional Neural Networks (CNNs) into the model architecture, enhancing its ability to extract detailed local features that complement the global context provided by the Transformer. Extensive experimental evaluations demonstrate that TransFusion achieves high accuracy with minimal counting errors while maintaining superior efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integration Flow Models</title>
<link>https://arxiv.org/abs/2504.20179</link>
<guid>https://arxiv.org/abs/2504.20179</guid>
<content:encoded><![CDATA[
<div> ODE-based generative models, Integration Flow, trajectory paths, reverse-time dynamics, stability. 

Summary: 
Integration Flow introduces a novel approach to ODE-based generative models by learning trajectory paths' integrals directly, eliminating the discretization errors of numerical solvers. By incorporating the target state as an anchor, Integration Flow enhances stability and accuracy. It is the first model to estimate ODE-based generative models with a unified structure and demonstrate the exact straightness of 1-Rectified Flow without reflow. Through theoretical analysis and empirical evaluations, Integration Flows outperform existing models like diffusion models, Rectified Flows, and PFGM++. Achieving remarkable FIDs in one-step generation on datasets like CIFAR10 and ImageNet, Integration Flow showcases its effectiveness in improving sample quality and training stability. <div>
arXiv:2504.20179v1 Announce Type: new 
Abstract: Ordinary differential equation (ODE) based generative models have emerged as a powerful approach for producing high-quality samples in many applications. However, the ODE-based methods either suffer the discretization error of numerical solvers of ODE, which restricts the quality of samples when only a few NFEs are used, or struggle with training instability. In this paper, we proposed Integration Flow, which directly learns the integral of ODE-based trajectory paths without solving the ODE functions. Moreover, Integration Flow explicitly incorporates the target state $\mathbf{x}_0$ as the anchor state in guiding the reverse-time dynamics. We have theoretically proven this can contribute to both stability and accuracy. To the best of our knowledge, Integration Flow is the first model with a unified structure to estimate ODE-based generative models and the first to show the exact straightness of 1-Rectified Flow without reflow. Through theoretical analysis and empirical evaluations, we show that Integration Flows achieve improved performance when it is applied to existing ODE-based models, such as diffusion models, Rectified Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model, 3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without reflow and 4.15 for PFGM++.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains</title>
<link>https://arxiv.org/abs/2504.20199</link>
<guid>https://arxiv.org/abs/2504.20199</guid>
<content:encoded><![CDATA[
<div> Focus-Centric Visual Chain, VLMs, multi-image scenarios, data synthesis, VISC-150K<br />
<br />
Summary: <br />
Vision-language models (VLMs) excel in single-image tasks but struggle with multi-image scenarios. The proposed Focus-Centric Visual Chain paradigm aims to enhance VLMs' perception, comprehension, and reasoning in such complex settings. The approach involves Focus-Centric Data Synthesis, creating the large-scale dataset VISC-150K with reasoning paths. Experimental results on seven benchmarks show an average performance improvement of 3.16% and 2.24% for different model architectures, maintaining general vision-language capabilities. This work signifies progress towards more robust vision-language systems capable of handling intricate visual scenarios.<br /> <div>
arXiv:2504.20199v1 Announce Type: new 
Abstract: Vision-language models (VLMs) achieve remarkable success in single-image tasks. However, real-world scenarios often involve intricate multi-image inputs, leading to a notable performance decline as models struggle to disentangle critical information scattered across complex visual features. In this work, we propose Focus-Centric Visual Chain, a novel paradigm that enhances VLMs'perception, comprehension, and reasoning abilities in multi-image scenarios. To facilitate this paradigm, we propose Focus-Centric Data Synthesis, a scalable bottom-up approach for synthesizing high-quality data with elaborate reasoning paths. Through this approach, We construct VISC-150K, a large-scale dataset with reasoning data in the form of Focus-Centric Visual Chain, specifically designed for multi-image tasks. Experimental results on seven multi-image benchmarks demonstrate that our method achieves average performance gains of 3.16% and 2.24% across two distinct model architectures, without compromising the general vision-language capabilities. our study represents a significant step toward more robust and capable vision-language systems that can handle complex visual scenarios.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies</title>
<link>https://arxiv.org/abs/2504.20203</link>
<guid>https://arxiv.org/abs/2504.20203</guid>
<content:encoded><![CDATA[
<div> Dataset, Flood detection, Remote Sensing, Deep Neural Networks, Augmentation <br />
<br />
Summary: 
The study focuses on utilizing Remote Sensing images for accurate flood detection by employing Deep Neural Networks trained on the BlessemFlood21 dataset. Different augmentation strategies, from basic to complex techniques including optical distortion, are explored to enhance the training process of state-of-the-art Deep Learning segmentation networks. Timely and precise flood detection is crucial for effective response to flood incidents worldwide. The study highlights the significance of accurate information for quick and efficient flood response. Utilizing specific detection methods is essential in improving flood detection accuracy. The research aims to identify effective augmentation strategies to refine the training process of Deep Learning networks for river flood detection in RGB imagery. Through this exploration, the study contributes to advancing flood detection methods for better disaster management. <div>
arXiv:2504.20203v1 Announce Type: new 
Abstract: Floods cause serious problems around the world. Responding quickly and effectively requires accurate and timely information about the affected areas. The effective use of Remote Sensing images for accurate flood detection requires specific detection methods. Typically, Deep Neural Networks are employed, which are trained on specific datasets. For the purpose of river flood detection in RGB imagery, we use the BlessemFlood21 dataset. We here explore the use of different augmentation strategies, ranging from basic approaches to more complex techniques, including optical distortion. By identifying effective strategies, we aim to refine the training process of state-of-the-art Deep Learning segmentation networks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations</title>
<link>https://arxiv.org/abs/2504.20222</link>
<guid>https://arxiv.org/abs/2504.20222</guid>
<content:encoded><![CDATA[
<div> Implicit surface representation; neural networks; 3D reconstruction; frequency stratification; blending

Summary:
Frebis is a novel neural implicit surface representation technique designed to address the challenges faced by traditional methods in complex scenes. The approach involves stratifying the scene based on surface frequency levels and encoding each level with a dedicated encoder. This allows FreBIS to capture a wide range of surface information more effectively compared to single encoder approaches. The method promotes mutual dissimilarity of encoded features through a redundancy-aware weighting module, further enhancing the quality of reconstructed 3D surfaces. Empirical evaluations on the BlendedMVS dataset demonstrate significant improvements in both surface reconstruction quality and rendering fidelity from various viewpoints. Overall, FreBIS offers a promising solution for advanced technologies like augmented reality, virtual reality, digital twins, and autonomous navigation by pushing the boundaries of neural implicit surface representation techniques. 

Summary: <div>
arXiv:2504.20222v1 Announce Type: new 
Abstract: Neural implicit surface representation techniques are in high demand for advancing technologies in augmented reality/virtual reality, digital twins, autonomous navigation, and many other fields. With their ability to model object surfaces in a scene as a continuous function, such techniques have made remarkable strides recently, especially over classical 3D surface reconstruction methods, such as those that use voxels or point clouds. However, these methods struggle with scenes that have varied and complex surfaces principally because they model any given scene with a single encoder network that is tasked to capture all of low through high-surface frequency information in the scene simultaneously. In this work, we propose a novel, neural implicit surface representation approach called FreBIS to overcome this challenge. FreBIS works by stratifying the scene based on the frequency of surfaces into multiple frequency levels, with each level (or a group of levels) encoded by a dedicated encoder. Moreover, FreBIS encourages these encoders to capture complementary information by promoting mutual dissimilarity of the encoded features via a novel, redundancy-aware weighting module. Empirical evaluations on the challenging BlendedMVS dataset indicate that replacing the standard encoder in an off-the-shelf neural surface reconstruction method with our frequency-stratified encoders yields significant improvements. These enhancements are evident both in the quality of the reconstructed 3D surfaces and in the fidelity of their renderings from any viewpoint.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters</title>
<link>https://arxiv.org/abs/2504.20234</link>
<guid>https://arxiv.org/abs/2504.20234</guid>
<content:encoded><![CDATA[
<div> Drone-based crowd monitoring, surveillance, public safety, event management, tracking continuity, consistency.<br />
Summary:<br />
Drone-based crowd monitoring is crucial for various applications, but traditional tracking methods face challenges like false positives, false negatives, and identity switches. This paper presents a point-oriented online tracking algorithm that enhances trajectory continuity and counting reliability. By incorporating camera motion compensation, altitude-aware assignment, and classification-based trajectory validation, along with Deep Discriminative Correlation Filters (DDCF), noise in tracking is reduced, missed detections are handled, and tracking accuracy is improved. The algorithm is evaluated on DroneCrowd and UP-COUNT-TRACK datasets, showing a significant decrease in counting errors and identity switches while outperforming baseline tracking methods. <div>
arXiv:2504.20234v1 Announce Type: new 
Abstract: Drone-based crowd monitoring is the key technology for applications in surveillance, public safety, and event management. However, maintaining tracking continuity and consistency remains a significant challenge. Traditional detection-assignment tracking methods struggle with false positives, false negatives, and frequent identity switches, leading to degraded counting accuracy and making in-depth analysis impossible. This paper introduces a point-oriented online tracking algorithm that improves trajectory continuity and counting reliability in drone-based crowd monitoring. Our method builds on the Simple Online and Real-time Tracking (SORT) framework, replacing the original bounding-box assignment with a point-distance metric. The algorithm is enhanced with three cost-effective techniques: camera motion compensation, altitude-aware assignment, and classification-based trajectory validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use spatial feature maps from localisation algorithms for increased computational efficiency through neural network resource sharing are integrated to refine object tracking by reducing noise and handling missed detections. The proposed method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets, demonstrating substantial improvements in tracking metrics, reducing counting errors to 23% and 15%, respectively. The results also indicate a significant reduction of identity switches while maintaining high tracking accuracy, outperforming baseline online trackers and even an offline greedy optimisation method.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts</title>
<link>https://arxiv.org/abs/2504.20241</link>
<guid>https://arxiv.org/abs/2504.20241</guid>
<content:encoded><![CDATA[
<div> ship detection, SAR imagery, wake signatures, physics-based simulation, diffusion model

Summary:
This study introduces a novel approach for efficient and end-to-end synthetic aperture radar (SAR) ship wake simulation. The research addresses the challenge of limited annotated data availability by utilizing a diffusion model trained on data generated by a physics-based simulator. By pairing images from the simulator with text prompts derived from simulation parameters, the model successfully generates realistic Kelvin wake patterns. This method not only accelerates the process of wake image generation but also enables more controlled simulations compared to traditional physics-based approaches. The experimental results demonstrate the effectiveness of the diffusion model in producing accurate wake signatures, showcasing its potential for enhancing downstream tasks in maritime SAR analysis. This innovative technique opens up new avenues for improving ship presence detection in SAR imagery through faster and more controllable wake image generation. 

<br /><br />Summary: <div>
arXiv:2504.20241v1 Announce Type: new 
Abstract: Detecting ship presence via wake signatures in SAR imagery is attracting considerable research interest, but limited annotated data availability poses significant challenges for supervised learning. Physics-based simulations are commonly used to address this data scarcity, although they are slow and constrain end-to-end learning. In this work, we explore a new direction for more efficient and end-to-end SAR ship wake simulation using a diffusion model trained on data generated by a physics-based simulator. The training dataset is built by pairing images produced by the simulator with text prompts derived from simulation parameters. Experimental result show that the model generates realistic Kelvin wake patterns and achieves significantly faster inference than the physics-based simulator. These results highlight the potential of diffusion models for fast and controllable wake image generation, opening new possibilities for end-to-end downstream tasks in maritime SAR analysis.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Interpolation with Score-based Riemannian Metrics of Diffusion Models</title>
<link>https://arxiv.org/abs/2504.20288</link>
<guid>https://arxiv.org/abs/2504.20288</guid>
<content:encoded><![CDATA[
<div> diffusion models, content generation, Riemannian manifold, metric, image interpolations <br />
<br />
Summary: 
This paper introduces a new framework that views the data space of pre-trained diffusion models as a Riemannian manifold with a metric derived from the score function. By treating the data space in this way, the framework leverages the manifold's geometry to improve content generation and editing. Experiments conducted using MNIST and Stable Diffusion datasets show that the geometry-aware approach leads to more realistic and less noisy image interpolations. These interpolations are also found to be more faithful to prompts compared to existing methods. The results indicate the potential of this novel framework for enhancing content generation tasks by utilizing the intrinsic geometry of the data manifold in diffusion models. <br /><br /> <div>
arXiv:2504.20288v1 Announce Type: new 
Abstract: Diffusion models excel in content generation by implicitly learning the data manifold, yet they lack a practical method to leverage this manifold - unlike other deep generative models equipped with latent spaces. This paper introduces a novel framework that treats the data space of pre-trained diffusion models as a Riemannian manifold, with a metric derived from the score function. Experiments with MNIST and Stable Diffusion show that this geometry-aware approach yields image interpolations that are more realistic, less noisy, and more faithful to prompts than existing methods, demonstrating its potential for improved content generation and editing.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes</title>
<link>https://arxiv.org/abs/2504.20303</link>
<guid>https://arxiv.org/abs/2504.20303</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, deep learning, computer vision, Andean archaeology, self-supervised learning

Summary:
DeepAndes is a new transformer-based vision foundation model designed for Andean archaeology, specifically trained on multi-spectral satellite images. The model incorporates a customized DINOv2 self-supervised learning algorithm optimized for 8-band multi-spectral imagery. The study evaluated DeepAndes' performance in various image understanding tasks, showing superior results in few-shot learning scenarios compared to models trained from scratch or pre-trained on smaller datasets. The effectiveness of large-scale self-supervised pre-training in archaeological remote sensing was highlighted. The code for DeepAndes will be made available on GitHub. This innovation opens up new possibilities for archaeologists to map sites at large scales, generate insights into long-term demographic trends, inter-regional social networks, and past adaptations to climate change using remote sensing data and deep learning techniques. <br /><br />Summary: <div>
arXiv:2504.20303v1 Announce Type: new 
Abstract: By mapping sites at large scales using remotely sensed data, archaeologists can generate unique insights into long-term demographic trends, inter-regional social networks, and past adaptations to climate change. Remote sensing surveys complement field-based approaches, and their reach can be especially great when combined with deep learning and computer vision techniques. However, conventional supervised deep learning methods face challenges in annotating fine-grained archaeological features at scale. While recent vision foundation models have shown remarkable success in learning large-scale remote sensing data with minimal annotations, most off-the-shelf solutions are designed for RGB images rather than multi-spectral satellite imagery, such as the 8-band data used in our study. In this paper, we introduce DeepAndes, a transformer-based vision foundation model trained on three million multi-spectral satellite images, specifically tailored for Andean archaeology. DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm optimized for 8-band multi-spectral imagery, marking the first foundation model designed explicitly for the Andes region. We evaluate its image understanding performance through imbalanced image classification, image instance retrieval, and pixel-level semantic segmentation tasks. Our experiments show that DeepAndes achieves superior F1 scores, mean average precision, and Dice scores in few-shot learning scenarios, significantly outperforming models trained from scratch or pre-trained on smaller datasets. This underscores the effectiveness of large-scale self-supervised pre-training in archaeological remote sensing. Codes will be available on https://github.com/geopacha/DeepAndes.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Contextual Attention Network: Transforming Spatial Representations into Adaptive Insights for Endoscopic Polyp Diagnosis</title>
<link>https://arxiv.org/abs/2504.20306</link>
<guid>https://arxiv.org/abs/2504.20306</guid>
<content:encoded><![CDATA[
<div> Keywords: Colorectal polyps, early detection, endoscopic imaging, Dynamic Contextual Attention Network, colorectal cancer<br />
Summary:<br />
The article introduces the Dynamic Contextual Attention Network (DCAN) as a novel approach to improving the detection of colorectal polyps, which are important indicators for early diagnosis of colorectal cancer. Traditional endoscopic imaging methods often struggle with accurate polyp localization and lack comprehensive contextual awareness, limiting the interpretability of diagnoses. DCAN addresses these issues by transforming spatial representations into adaptive contextual insights using an attention mechanism. This mechanism enhances focus on critical polyp regions without the need for explicit localization modules, ultimately improving decision interpretability in the classification process. By integrating contextual awareness into the diagnostic process, DCAN shows promise in enhancing the accuracy and reliability of colorectal cancer detection, potentially leading to better patient outcomes.<br />
Summary: 213 <br /> <div>
arXiv:2504.20306v1 Announce Type: new 
Abstract: Colorectal polyps are key indicators for early detection of colorectal cancer. However, traditional endoscopic imaging often struggles with accurate polyp localization and lacks comprehensive contextual awareness, which can limit the explainability of diagnoses. To address these issues, we propose the Dynamic Contextual Attention Network (DCAN). This novel approach transforms spatial representations into adaptive contextual insights, using an attention mechanism that enhances focus on critical polyp regions without explicit localization modules. By integrating contextual awareness into the classification process, DCAN improves decision interpretability and overall diagnostic performance. This advancement in imaging could lead to more reliable colorectal cancer detection, enabling better patient outcomes.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training</title>
<link>https://arxiv.org/abs/2504.20322</link>
<guid>https://arxiv.org/abs/2504.20322</guid>
<content:encoded><![CDATA[
<div> framework, fine-grained visual classification, meta-information, cross-contrastive pre-training, NABirds dataset <br />
Summary: <br />
The article introduces a novel framework for fine-grained visual classification that incorporates meta-information to improve identification accuracy. The proposed framework utilizes cross-contrastive pre-training to align embeddings of visual, text, and meta-information encoders, enhancing representation quality. By fine-tuning the image and meta-information encoders, the framework effectively leverages meta-information for classification tasks. Experimental results on the NABirds dataset show significant performance improvement, surpassing baseline accuracy by 7.83% and achieving an overall accuracy of 84.44%. This approach demonstrates the potential of utilizing meta-information in enhancing fine-grained recognition performance, outperforming existing state-of-the-art methods that also leverage meta-information. <br /> <div>
arXiv:2504.20322v1 Announce Type: new 
Abstract: Fine-grained visual classification aims to recognize objects belonging to multiple subordinate categories within a super-category. However, this remains a challenging problem, as appearance information alone is often insufficient to accurately differentiate between fine-grained visual categories. To address this, we propose a novel and unified framework that leverages meta-information to assist fine-grained identification. We tackle the joint learning of visual and meta-information through cross-contrastive pre-training. In the first stage, we employ three encoders for images, text, and meta-information, aligning their projected embeddings to achieve better representations. We then fine-tune the image and meta-information encoders for the classification task. Experiments on the NABirds dataset demonstrate that our framework effectively utilizes meta-information to enhance fine-grained recognition performance. With the addition of meta-information, our framework surpasses the current baseline on NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the NABirds dataset, outperforming many existing state-of-the-art approaches that utilize meta-information.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation</title>
<link>https://arxiv.org/abs/2504.20343</link>
<guid>https://arxiv.org/abs/2504.20343</guid>
<content:encoded><![CDATA[
<div> vision-language mixture-of-experts, medical image reporting, multimodal alignment, fine-grained feature extraction, state-of-the-art results<br />
Summary:<br />
MicarVLMoE is a new model for medical image reporting that addresses challenges in feature extraction, multimodal alignment, and generalization across different types of imaging. The model includes a multiscale vision encoder for capturing anatomical details, a multihead dual-branch latent attention module for aligning vision and language, and a modulated mixture-of-experts decoder for expert specialization. It extends medical image reporting to various types of imaging and achieves state-of-the-art results on multiple datasets. Experiments and ablations demonstrate improved clinical accuracy, cross-modal alignment, and model interpretability. The code is available on GitHub for further exploration and development. <br />Summary: <div>
arXiv:2504.20343v1 Announce Type: new 
Abstract: Medical image reporting (MIR) aims to generate structured clinical descriptions from radiological images. Existing methods struggle with fine-grained feature extraction, multimodal alignment, and generalization across diverse imaging types, often relying on vanilla transformers and focusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language mixture-of-experts model with gated cross-aligned fusion, designed to address these limitations. Our architecture includes: (i) a multiscale vision encoder (MSVE) for capturing anatomical details at varying resolutions, (ii) a multihead dual-branch latent attention (MDLA) module for vision-language alignment through latent bottleneck representations, and (iii) a modulated mixture-of-experts (MoE) decoder for adaptive expert specialization. We extend MIR to CT scans, retinal imaging, MRI scans, and gross pathology images, reporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets. Extensive experiments and ablations confirm improved clinical accuracy, cross-modal alignment, and model interpretability. Code is available at https://github.com/AI-14/micar-vl-moe.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots</title>
<link>https://arxiv.org/abs/2504.20362</link>
<guid>https://arxiv.org/abs/2504.20362</guid>
<content:encoded><![CDATA[
<div> Test-Time Training, image fusion, multimodal medical images, surgical robots, real-time performance

Summary: 
TTTFusion is a novel image fusion strategy that utilizes Test-Time Training to dynamically adjust model parameters during inference, leading to more efficient fusion of multimodal medical images. This approach addresses challenges in real-time performance, fine-grained feature extraction, and edge preservation, which traditional fusion methods struggle with. By adapting the model based on input data during the test phase, TTTFusion significantly improves fusion accuracy and detail preservation, especially in fine-grained feature extraction and edge preservation. Experimental results show that TTTFusion enhances the fusion quality of multimodal images, providing a technical solution for real-time image processing in surgical robots. <div>
arXiv:2504.20362v1 Announce Type: new 
Abstract: With the increasing use of surgical robots in clinical practice, enhancing their ability to process multimodal medical images has become a key research challenge. Although traditional medical image fusion methods have made progress in improving fusion accuracy, they still face significant challenges in real-time performance, fine-grained feature extraction, and edge preservation.In this paper, we introduce TTTFusion, a Test-Time Training (TTT)-based image fusion strategy that dynamically adjusts model parameters during inference to efficiently fuse multimodal medical images. By adapting the model during the test phase, our method optimizes the parameters based on the input image data, leading to improved accuracy and better detail preservation in the fusion results.Experimental results demonstrate that TTTFusion significantly enhances the fusion quality of multimodal images compared to traditional fusion methods, particularly in fine-grained feature extraction and edge preservation. This approach not only improves image fusion accuracy but also offers a novel technical solution for real-time image processing in surgical robots.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems</title>
<link>https://arxiv.org/abs/2504.20376</link>
<guid>https://arxiv.org/abs/2504.20376</guid>
<content:encoded><![CDATA[
<div> memory mechanism, online text-to-image generation, jailbreak attack, multi-turn interactions, security analysis

Summary:
This paper explores the security implications of the memory mechanism in online text-to-image (T2I) generation systems, specifically focusing on jailbreak attacks. The proposed Inception attack strategy targets the memory mechanism by embedding malicious content into the chat session turn by turn. This attack method involves segmenting the unsafe prompt into chunks and feeding them to the system in multiple turns to optimize for malicious outcomes. By utilizing segmentation and recursion techniques, Inception successfully generates images that appear benign based on request prompts but possess malicious intent. Experimental results on the DALLE 3 system demonstrate the effectiveness of Inception, outperforming existing attack methods by a notable margin. This study highlights the need for robust security measures in T2I generation systems to mitigate the risk of jailbreak attacks.<br /><br />Summary: <div>
arXiv:2504.20376v1 Announce Type: new 
Abstract: Currently, the memory mechanism has been widely and successfully exploited in online text-to-image (T2I) generation systems ($e.g.$, DALL$\cdot$E 3) for alleviating the growing tokenization burden and capturing key information in multi-turn interactions. Despite its practicality, its security analyses have fallen far behind. In this paper, we reveal that this mechanism exacerbates the risk of jailbreak attacks. Different from previous attacks that fuse the unsafe target prompt into one ultimate adversarial prompt, which can be easily detected or may generate non-unsafe images due to under- or over-optimization, we propose Inception, the first multi-turn jailbreak attack against the memory mechanism in real-world text-to-image generation systems. Inception embeds the malice at the inception of the chat session turn by turn, leveraging the mechanism that T2I generation systems retrieve key information in their memory. Specifically, Inception mainly consists of two modules. It first segments the unsafe prompt into chunks, which are subsequently fed to the system in multiple turns, serving as pseudo-gradients for directive optimization. Specifically, we develop a series of segmentation policies that ensure the images generated are semantically consistent with the target prompt. Secondly, after segmentation, to overcome the challenge of the inseparability of minimum unsafe words, we propose recursion, a strategy that makes minimum unsafe words subdivisible. Collectively, segmentation and recursion ensure that all the request prompts are benign but can lead to malicious outcomes. We conduct experiments on the real-world text-to-image generation system ($i.e.$, DALL$\cdot$E 3) to validate the effectiveness of Inception. The results indicate that Inception surpasses the state-of-the-art by a 14\% margin in attack success rate.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views</title>
<link>https://arxiv.org/abs/2504.20378</link>
<guid>https://arxiv.org/abs/2504.20378</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, surface reconstruction, sparse input views, Multi-view Stereo, 3D points<br />
<br />
Summary:<br />
A new method called Sparse2DGS is introduced for surface reconstruction using sparse input views. Traditional methods struggle with sparse Structure-from-Motion points but Sparse2DGS, incorporating MVS, Gaussian Splatting, and geometric-prioritized enhancement schemes, overcomes this challenge. This approach enables direct and robust geometric learning in ill-posed conditions, resulting in complete and accurate reconstructions. Sparse2DGS outperforms existing methods by a significant margin and is also faster than NeRF-based fine-tuning. <div>
arXiv:2504.20378v1 Announce Type: new 
Abstract: We present a Gaussian Splatting method for surface reconstruction using sparse input views. Previous methods relying on dense views struggle with extremely sparse Structure-from-Motion points for initialization. While learning-based Multi-view Stereo (MVS) provides dense 3D points, directly combining it with Gaussian Splatting leads to suboptimal results due to the ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS, an MVS-initialized Gaussian Splatting pipeline for complete and accurate reconstruction. Our key insight is to incorporate the geometric-prioritized enhancement schemes, allowing for direct and robust geometric learning under ill-posed conditions. Sparse2DGS outperforms existing methods by notable margins while being ${2}\times$ faster than the NeRF-based fine-tuning approach.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2504.20379</link>
<guid>https://arxiv.org/abs/2504.20379</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, localizing, query image, RGBD image, pose estimation <br />
Summary: <br />
The paper introduces a method for localizing a query image using a precomputed 3D Gaussian Splatting (3DGS) scene representation. It starts by rendering a synthetic RGBD image based on an initial pose estimate using 3DGS. Then, it establishes 2D-2D correspondences between the query image and the synthetic image. Next, it lifts these correspondences to 2D-3D correspondences using the depth map and solves a perspective-n-point (PnP) problem to refine the pose estimate. The method shows significant reductions in inference time and estimation error compared to baseline methods, achieving final pose errors of less than 5 in rotation and 0.05 units in translation on most images from evaluated datasets. It also demonstrates robustness to large errors in initial pose estimates, making it a promising approach for accurate and efficient localization tasks. <br /> <div>
arXiv:2504.20379v1 Announce Type: new 
Abstract: In this paper, we present a method for localizing a query image with respect to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the method uses 3DGS to render a synthetic RGBD image at some initial pose estimate. Second, it establishes 2D-2D correspondences between the query image and this synthetic image. Third, it uses the depth map to lift the 2D-2D correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP) problem to produce a final pose estimate. Results from evaluation across three existing datasets with 38 scenes and over 2,700 test images show that our method significantly reduces both inference time (by over two orders of magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation error compared to baseline methods that use photometric loss minimization. Results also show that our method tolerates large errors in the initial pose estimate of up to 55{\deg} in rotation and 1.1 units in translation (normalized by scene scale), achieving final pose errors of less than 5{\deg} in rotation and 0.05 units in translation on 90% of images from the Synthetic NeRF and Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and Temples dataset.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Stereo Video Compression with Hybrid Disparity Compensation</title>
<link>https://arxiv.org/abs/2504.20383</link>
<guid>https://arxiv.org/abs/2504.20383</guid>
<content:encoded><![CDATA[
<div> Keywords: Disparity compensation, Stereo video compression, Cross-attention mechanism, Neural network, End-to-end optimization 

Summary: 
The paper introduces a hybrid Disparity Compensation (HDC) strategy for stereo video compression, combining explicit horizontal shifting and implicit cross-attention mechanisms. HDC leverages explicit pixel displacement for optimization simplification and captures a wider range of disparity information. It computes a similarity map from horizontally shifted cross-view features to generate an explicit pixel-wise attention score, aligning features across views. The proposed end-to-end optimized neural stereo video compression framework integrates HDC modules for key coding operations, improving performance over neural and traditional methods. Experimental results on benchmark datasets validate the framework's effectiveness for autonomous driving and general scenes. <div>
arXiv:2504.20383v1 Announce Type: new 
Abstract: Disparity compensation represents the primary strategy in stereo video compression (SVC) for exploiting cross-view redundancy. These mechanisms can be broadly categorized into two types: one that employs explicit horizontal shifting, and another that utilizes an implicit cross-attention mechanism to reduce cross-view disparity redundancy. In this work, we propose a hybrid disparity compensation (HDC) strategy that leverages explicit pixel displacement as a robust prior feature to simplify optimization and perform implicit cross-attention mechanisms for subsequent warping operations, thereby capturing a broader range of disparity information. Specifically, HDC first computes a similarity map by fusing the horizontally shifted cross-view features to capture pixel displacement information. This similarity map is then normalized into an "explicit pixel-wise attention score" to perform the cross-attention mechanism, implicitly aligning features from one view to another. Building upon HDC, we introduce a novel end-to-end optimized neural stereo video compression framework, which integrates HDC-based modules into key coding operations, including cross-view feature extraction and reconstruction (HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on SVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both autonomous driving and general scenes, demonstrate that our framework outperforms both neural and traditional SVC methodologies.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding</title>
<link>https://arxiv.org/abs/2504.20384</link>
<guid>https://arxiv.org/abs/2504.20384</guid>
<content:encoded><![CDATA[
<div> framework, video understanding, language models, feature compression, keyframe selection 
Summary:
FiLA-Video introduces a novel framework, leveraging a lightweight dynamic-weight multi-frame fusion strategy to integrate multiple frames efficiently in video understanding within language models. The framework prioritizes essential video features, reducing redundant inter-frame information and computational costs. It also includes a keyframe selection strategy to identify informative frames for improved summarization. Additionally, a simple yet effective long-video training data generation strategy enhances model performance without extensive manual annotation. Experimental results show that FiLA-Video outperforms existing methods in long-video comprehension in terms of efficiency and accuracy.<br /><br />Summary: <div>
arXiv:2504.20384v1 Announce Type: new 
Abstract: Recent advancements in video understanding within visual large language models (VLLMs) have led to notable progress. However, the complexity of video data and contextual processing limitations still hinder long-video comprehension. A common approach is video feature compression to reduce token input to large language models, yet many methods either fail to prioritize essential features, leading to redundant inter-frame information, or introduce computationally expensive modules.To address these issues, we propose FiLA(Fine-grained Vision Language Model)-Video, a novel framework that leverages a lightweight dynamic-weight multi-frame fusion strategy, which adaptively integrates multiple frames into a single representation while preserving key video information and reducing computational costs. To enhance frame selection for fusion, we introduce a keyframe selection strategy, effectively identifying informative frames from a larger pool for improved summarization. Additionally, we present a simple yet effective long-video training data generation strategy, boosting model performance without extensive manual annotation. Experimental results demonstrate that FiLA-Video achieves superior efficiency and accuracy in long-video comprehension compared to existing methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation</title>
<link>https://arxiv.org/abs/2504.20409</link>
<guid>https://arxiv.org/abs/2504.20409</guid>
<content:encoded><![CDATA[
<div> autoregressive modeling, 3D garments, garment reconstruction, parametric representation, GarmentX dataset

Summary:<br />
- GarmentX is a novel framework that generates diverse, high-fidelity, and wearable 3D garments from a single input image.
- Traditional garment reconstruction methods often result in physically implausible garment structures due to overly unconstrained predictions.
- GarmentX introduces a structured and editable parametric representation compatible with GarmentCode, ensuring valid, simulation-ready 3D garments that can be easily modified in shape and style.
- The framework utilizes a masked autoregressive model to sequentially predict garment parameters, improving structured generation while avoiding inconsistencies in direct pattern prediction.
- The GarmentX dataset, consisting of 378,682 garment parameter-image pairs, is introduced, enabling the synthesis of diverse and high-quality garment images conditioned on parametric representations. 

Summary: <div>
arXiv:2504.20409v1 Announce Type: new 
Abstract: This work presents GarmentX, a novel framework for generating diverse, high-fidelity, and wearable 3D garments from a single input image. Traditional garment reconstruction methods directly predict 2D pattern edges and their connectivity, an overly unconstrained approach that often leads to severe self-intersections and physically implausible garment structures. In contrast, GarmentX introduces a structured and editable parametric representation compatible with GarmentCode, ensuring that the decoded sewing patterns always form valid, simulation-ready 3D garments while allowing for intuitive modifications of garment shape and style. To achieve this, we employ a masked autoregressive model that sequentially predicts garment parameters, leveraging autoregressive modeling for structured generation while mitigating inconsistencies in direct pattern prediction. Additionally, we introduce GarmentX dataset, a large-scale dataset of 378,682 garment parameter-image pairs, constructed through an automatic data generation pipeline that synthesizes diverse and high-quality garment images conditioned on parametric garment representations. Through integrating our method with GarmentX dataset, we achieve state-of-the-art performance in geometric fidelity and input image alignment, significantly outperforming prior approaches. We will release GarmentX dataset upon publication.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2504.20419</link>
<guid>https://arxiv.org/abs/2504.20419</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Vision Language Models, LLMs, CNNs, Disease Detection

Summary:
- The study combines multimodal Large Language Models (LLMs) like GPT-4o with Convolutional Neural Networks (CNNs) for automated plant disease classification using leaf imagery.
- Model performance was evaluated across zero-shot, few-shot, and progressive fine-tuning scenarios using the PlantVillage dataset.
- Fine-tuned GPT-4o models showed slightly better performance than ResNet-50, achieving up to 98.12% accuracy on apple leaf images.
- Zero-shot performance of GPT-4o was lower, highlighting the need for minimal training.
- Evaluations on cross-resolution and cross-plant generalization demonstrated adaptability and limitations in new domains.
<br /><br /> <div>
arXiv:2504.20419v1 Announce Type: new 
Abstract: Automation in agriculture plays a vital role in addressing challenges related to crop monitoring and disease management, particularly through early detection systems. This study investigates the effectiveness of combining multimodal Large Language Models (LLMs), specifically GPT-4o, with Convolutional Neural Networks (CNNs) for automated plant disease classification using leaf imagery. Leveraging the PlantVillage dataset, we systematically evaluate model performance across zero-shot, few-shot, and progressive fine-tuning scenarios. A comparative analysis between GPT-4o and the widely used ResNet-50 model was conducted across three resolutions (100, 150, and 256 pixels) and two plant species (apple and corn). Results indicate that fine-tuned GPT-4o models achieved slightly better performance compared to the performance of ResNet-50, achieving up to 98.12% classification accuracy on apple leaf images, compared to 96.88% achieved by ResNet-50, with improved generalization and near-zero training loss. However, zero-shot performance of GPT-4o was significantly lower, underscoring the need for minimal training. Additional evaluations on cross-resolution and cross-plant generalization revealed the models' adaptability and limitations when applied to new domains. The findings highlight the promise of integrating multimodal LLMs into automated disease detection pipelines, enhancing the scalability and intelligence of precision agriculture systems while reducing the dependence on large, labeled datasets and high-resolution sensor infrastructure. Large Language Models, Vision Language Models, LLMs and CNNs, Disease Detection with Vision Language Models, VLMs
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Assisted Cervical Cancer Screening for Cytology Samples in Developing Countries</title>
<link>https://arxiv.org/abs/2504.20435</link>
<guid>https://arxiv.org/abs/2504.20435</guid>
<content:encoded><![CDATA[
<div> Keywords: cervical cancer, AI algorithms, whole-slide analysis, UNet-based model, CvT-based classification model

Summary: 
This paper presents a novel approach for cervical cancer screening using a combination of low-cost biological microscopes and AI algorithms. The system combines a motorized microscope with AI algorithms for automated whole-slide analysis, including image stitching, cell segmentation, and classification. The AI models used in the system, a lightweight UNet-based model for cell segmentation and a CvT-based classification model, have been trained with minimal ROIs and the SIPaKMeD dataset. The system offers improved accuracy and efficiency in screening compared to traditional methods, making it a promising solution for the high incidence and mortality rates of cervical cancer, particularly in transitioning countries. The study demonstrates the effectiveness of the system through various evaluation metrics, highlighting its potential to enhance cervical cancer screening worldwide.  

Summary: <div>
arXiv:2504.20435v1 Announce Type: new 
Abstract: Cervical cancer remains a significant health challenge, with high incidence and mortality rates, particularly in transitioning countries. Conventional Liquid-Based Cytology(LBC) is a labor-intensive process, requires expert pathologists and is highly prone to errors, highlighting the need for more efficient screening methods. This paper introduces an innovative approach that integrates low-cost biological microscopes with our simple and efficient AI algorithms for automated whole-slide analysis. Our system uses a motorized microscope to capture cytology images, which are then processed through an AI pipeline involving image stitching, cell segmentation, and classification. We utilize the lightweight UNet-based model involving human-in-the-loop approach to train our segmentation model with minimal ROIs. CvT-based classification model, trained on the SIPaKMeD dataset, accurately categorizes five cell types. Our framework offers enhanced accuracy and efficiency in cervical cancer screening compared to various state-of-art methods, as demonstrated by different evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PixelHacker: Image Inpainting with Structural and Semantic Consistency</title>
<link>https://arxiv.org/abs/2504.20438</link>
<guid>https://arxiv.org/abs/2504.20438</guid>
<content:encoded><![CDATA[
<div> Keywords: Image inpainting, latent categories guidance, PixelHacker, attention mechanisms, denoising process

Summary:
PixelHacker is a novel image inpainting model that utilizes a latent categories guidance paradigm. The approach involves encoding foreground and background representations separately through fixed-size embeddings and incorporating them into the denoising process via linear attention. A dataset containing 14 million image-mask pairs was used for training, with annotations for foreground and background categories. Through pre-training on the constructed dataset and fine-tuning on benchmark datasets (Places2, CelebA-HQ, FFHQ), PixelHacker surpasses state-of-the-art methods in inpainting tasks. The model excels in handling complex structures and semantics, producing high-quality inpainted images with improved consistency in texture, shape, spatial relations, color consistency, object restoration, and logical correctness. The simplicity and effectiveness of the proposed approach demonstrate promising results in the field of image generation and editing. Visit the project page at https://hustvl.github.io/projects/PixelHacker. 

<br /><br />Summary: <div>
arXiv:2504.20438v1 Announce Type: new 
Abstract: Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at https://hustvl.github.io/projects/PixelHacker.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs</title>
<link>https://arxiv.org/abs/2504.20466</link>
<guid>https://arxiv.org/abs/2504.20466</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, 3D human faces, quality assessment, LMME3DHF, distortion-aware saliency

Summary: 
<br />
The study focuses on assessing the quality and realism of AI-generated 3D human faces, which are used in various applications such as media production and healthcare. A benchmark called Gen3DHF is introduced, containing videos of AI-generated 3D human faces, Mean Opinion Scores (MOS), saliency maps, and distortion descriptions. A metric called LMME3DHF is proposed for evaluating the quality and authenticity of these faces, including score prediction, visual question answering, and saliency prediction. Experimental results show that LMME3DHF outperforms existing methods in predicting quality scores and identifying distortion-aware salient regions and types. The model aligns well with human perceptual judgments. The Gen3DHF database and LMME3DHF metric will be publicly released upon publication. 
<br /><br />Summary: <div>
arXiv:2504.20466v1 Announce Type: new 
Abstract: The rapid advancement in generative artificial intelligence have enabled the creation of 3D human faces (HFs) for applications including media production, virtual reality, security, healthcare, and game development, etc. However, assessing the quality and realism of these AI-generated 3D human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. To this end, we conduct a comprehensive study on the quality assessment of AI-generated 3D human faces. We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS) collected across two dimensions, i.e., quality and authenticity, 2,000 distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF, we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating 3DHF capable of quality and authenticity score prediction, distortion-aware visual question answering, and distortion-aware saliency prediction. Experimental results show that LMME3DHF achieves state-of-the-art performance, surpassing existing methods in both accurately predicting quality scores for AI-generated 3D human faces and effectively identifying distortion-aware salient regions and distortion types, while maintaining strong alignment with human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be released upon the publication.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception</title>
<link>https://arxiv.org/abs/2504.20468</link>
<guid>https://arxiv.org/abs/2504.20468</guid>
<content:encoded><![CDATA[
<div> framework, LVLMs, counterfactual, hallucination, benchmark
Summary:
The paper introduces "Antidote", a synthetic data-driven framework designed to address the issue of object perception hallucinations in Large Vision-Language Models (LVLMs). These models often struggle with counterfactual presupposition questions (CPQs), producing inaccurate responses due to accepting presuppositions. "Antidote" leverages synthetic data to incorporate factual priors into questions, correcting the models' tendencies towards hallucinatory responses. Additionally, the paper introduces a benchmark called "CP-Bench" to evaluate LVLMs' performance on CPQs and factual response generation. By implementing "Antidote" on the LLaVA series, significant improvements in performance on CP-Bench as well as other tasks are achieved, without the need for external supervision or human feedback. This approach effectively mitigates object perception hallucinations in LVLMs and improves their overall accuracy in addressing CPQs. <br /><br /> <div>
arXiv:2504.20468v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have achieved impressive results across various cross-modal tasks. However, hallucinations, i.e., the models generating counterfactual responses, remain a challenge. Though recent studies have attempted to alleviate object perception hallucinations, they focus on the models' response generation, and overlooking the task question itself. This paper discusses the vulnerability of LVLMs in solving counterfactual presupposition questions (CPQs), where the models are prone to accept the presuppositions of counterfactual objects and produce severe hallucinatory responses. To this end, we introduce "Antidote", a unified, synthetic data-driven post-training framework for mitigating both types of hallucination above. It leverages synthetic data to incorporate factual priors into questions to achieve self-correction, and decouple the mitigation process into a preference optimization problem. Furthermore, we construct "CP-Bench", a novel benchmark to evaluate LVLMs' ability to correctly handle CPQs and produce factual responses. Applied to the LLaVA series, Antidote can simultaneously enhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR by 30-50%, all without relying on external supervision from stronger LVLMs or human feedback and introducing noticeable catastrophic forgetting issues.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-scale visual SLAM for in-the-wild videos</title>
<link>https://arxiv.org/abs/2504.20496</link>
<guid>https://arxiv.org/abs/2504.20496</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scene reconstruction, visual SLAM, deep learning, camera pose estimation, dynamic objects<br />
Summary:
Accurate and robust 3D scene reconstruction from uncontrolled videos is a challenging task. Existing visual SLAM methods struggle with real-world footage due to various issues such as rapid motion, textureless regions, and dynamic objects. This paper introduces a robust pipeline for improving 3D reconstruction from casual videos by leveraging deep visual odometry methods. The system automatically recovers camera intrinsics using structure-from-motion and masks dynamic objects and less-constrained areas with a predictive model. Monocular depth estimates are utilized to regularize bundle adjustment, reducing errors in low-parallax situations. Place recognition and loop closure are integrated to minimize long-term drift and refine both intrinsics and pose estimates through global bundle adjustment. The proposed system establishes a new baseline for visual reconstruction from online casual videos, producing more consistent reconstructions over longer sequences compared to baseline methods. <br /><br />Summary: <div>
arXiv:2504.20496v1 Announce Type: new 
Abstract: Accurate and robust 3D scene reconstruction from casual, in-the-wild videos can significantly simplify robot deployment to new environments. However, reliable camera pose estimation and scene reconstruction from such unconstrained videos remains an open challenge. Existing visual-only SLAM methods perform well on benchmark datasets but struggle with real-world footage which often exhibits uncontrolled motion including rapid rotations and pure forward movements, textureless regions, and dynamic objects. We analyze the limitations of current methods and introduce a robust pipeline designed to improve 3D reconstruction from casual videos. We build upon recent deep visual odometry methods but increase robustness in several ways. Camera intrinsics are automatically recovered from the first few frames using structure-from-motion. Dynamic objects and less-constrained areas are masked with a predictive model. Additionally, we leverage monocular depth estimates to regularize bundle adjustment, mitigating errors in low-parallax situations. Finally, we integrate place recognition and loop closure to reduce long-term drift and refine both intrinsics and pose estimates through global bundle adjustment. We demonstrate large-scale contiguous 3D models from several online videos in various environments. In contrast, baseline methods typically produce locally inconsistent results at several points, producing separate segments or distorted maps. In lieu of ground-truth pose data, we evaluate map consistency, execution time and visual accuracy of re-rendered NeRF models. Our proposed system establishes a new baseline for visual reconstruction from casual uncontrolled videos found online, demonstrating more consistent reconstructions over longer sequences of in-the-wild videos than previously achieved.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection</title>
<link>https://arxiv.org/abs/2504.20498</link>
<guid>https://arxiv.org/abs/2504.20498</guid>
<content:encoded><![CDATA[
<div> Style-Adaptive Detection Transformer, SDG, object detection, domain generalization, contrastive learning<br />
Summary:<br />
The article introduces a new method called Style-Adaptive Detection Transformer (SA-DETR) for Single-source Domain Generalization (SDG) in object detection. It aims to improve generalization capability across unseen target domains by dynamically adapting the style representation of the target domain during training. SA-DETR utilizes a domain style adapter and object-aware contrastive learning module to extract domain-invariant features through contrastive learning. The object-aware gating masks help constrain feature aggregation in spatial and semantic dimensions, facilitating cross-domain contrast of instance-level features. Experimental results show that SA-DETR outperforms existing methods across five different weather scenarios. The code for SA-DETR is publicly available for further research and application. <br /><br />Summary: <div>
arXiv:2504.20498v1 Announce Type: new 
Abstract: Single-source Domain Generalization (SDG) in object detection aims to develop a detector using only data from a source domain that can exhibit strong generalization capability when applied to unseen target domains. Existing methods are built upon CNN-based detectors and primarily improve robustness by employing carefully designed data augmentation strategies integrated with feature alignment techniques. However, data augmentation methods have inherent drawbacks; they are only effective when the augmented sample distribution approximates or covers the unseen scenarios, thus failing to enhance generalization across all unseen domains. Furthermore, while the recent Detection Transformer (DETR) has demonstrated superior generalization capability in domain adaptation tasks due to its efficient global information extraction, its potential in SDG tasks remains unexplored. To this end, we introduce a strong DETR-based detector named the Style-Adaptive Detection Transformer (SA-DETR) for SDG in object detection. Specifically, we present a domain style adapter that projects the style representation of the unseen target domain into the training domain, enabling dynamic style adaptation. Then, we propose an object-aware contrastive learning module to guide the detector in extracting domain-invariant features through contrastive learning. By using object-aware gating masks to constrain feature aggregation in both spatial and semantic dimensions, this module achieves cross-domain contrast of instance-level features, thereby enhancing generalization. Extensive experiments demonstrate the superior performance and generalization capability of SA-DETR across five different weather scenarios. Code is released at https://github.com/h751410234/SA-DETR.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2504.20509</link>
<guid>https://arxiv.org/abs/2504.20509</guid>
<content:encoded><![CDATA[
<div> Keywords: MambaMoE, hyperspectral image classification, spectral-spatial modeling, mixture-of-experts framework, uncertainty-guided corrective learning <br />
Summary: <br />
- The study introduces MambaMoE, a novel framework for hyperspectral image classification that incorporates spectral-spatial characteristics and utilizes a mixture-of-experts approach. 
- A Mixture of Mamba Expert Block (MoMEB) is designed to enable adaptive spectral-spatial modeling through sparse expert activation. 
- The framework includes an uncertainty-guided corrective learning (UGCL) strategy to focus on regions with prediction ambiguity, improving model performance. 
- Extensive experiments on public HSI datasets demonstrate that MambaMoE outperforms existing methods, especially Mamba-based approaches, in terms of accuracy and efficiency. 
- The research contributes to advancing HSI classification techniques and provides code for reproducibility and further development. <br /> 
Summary: <div>
arXiv:2504.20509v1 Announce Type: new 
Abstract: The Mamba model has recently demonstrated strong potential in hyperspectral image (HSI) classification, owing to its ability to perform context modeling with linear computational complexity. However, existing Mamba-based methods usually neglect the spectral and spatial directional characteristics related to heterogeneous objects in hyperspectral scenes, leading to limited classification performance. To address these issues, we propose MambaMoE, a novel spectral-spatial mixture-of-experts framework, representing the first MoE-based approach in the HSI classification community. Specifically, we design a Mixture of Mamba Expert Block (MoMEB) that leverages sparse expert activation to enable adaptive spectral-spatial modeling. Furthermore, we introduce an uncertainty-guided corrective learning (UGCL) strategy to encourage the model's attention toward complex regions prone to prediction ambiguity. Extensive experiments on multiple public HSI benchmarks demonstrate that MambaMoE achieves state-of-the-art performance in both accuracy and efficiency compared to existing advanced approaches, especially for Mamba-based methods. Code will be released.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SteelBlastQC: Shot-blasted Steel Surface Dataset with Interpretable Detection of Surface Defects</title>
<link>https://arxiv.org/abs/2504.20510</link>
<guid>https://arxiv.org/abs/2504.20510</guid>
<content:encoded><![CDATA[
<div> dataset, steel surfaces, computer vision models, quality control, defect detection

Summary:
The study introduces a dataset of 1654 labeled RGB images of shot-blasted steel surfaces for quality control. The images capture real-world defects like discoloration and corrosion, suitable for training computer vision models. Three classification approaches were evaluated: Compact Convolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50 feature extraction, and Convolutional Autoencoder (CAE). The supervised methods (CCT and SVM) achieve 95% accuracy in classifying surfaces. The CCT utilizes transformer-based attention mechanisms while SVM offers a computationally efficient alternative. The CAE approach establishes a baseline for unsupervised quality control. The study provides interpretable decision-making by all models, enabling industry users to identify problematic regions visually and understand the model's reasoning. The release of the dataset and baseline codes aims to drive further research in defect detection, promote interpretable computer vision models for quality control, and advocate for automated inspection systems in industries. 

<br /><br />Summary: <div>
arXiv:2504.20510v1 Announce Type: new 
Abstract: Automating the quality control of shot-blasted steel surfaces is crucial for improving manufacturing efficiency and consistency. This study presents a dataset of 1654 labeled RGB images (512x512) of steel surfaces, classified as either "ready for paint" or "needs shot-blasting." The dataset captures real-world surface defects, including discoloration, welding lines, scratches and corrosion, making it well-suited for training computer vision models. Additionally, three classification approaches were evaluated: Compact Convolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50 feature extraction, and a Convolutional Autoencoder (CAE). The supervised methods (CCT and SVM) achieve 95% classification accuracy on the test set, with CCT leveraging transformer-based attention mechanisms and SVM offering a computationally efficient alternative. The CAE approach, while less effective, establishes a baseline for unsupervised quality control. We present interpretable decision-making by all three neural networks, allowing industry users to visually pinpoint problematic regions and understand the model's rationale. By releasing the dataset and baseline codes, this work aims to support further research in defect detection, advance the development of interpretable computer vision models for quality control, and encourage the adoption of automated inspection systems in industrial applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2504.20518</link>
<guid>https://arxiv.org/abs/2504.20518</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image diffusion models, backdoor attacks, dynamic attention analysis, attention maps, detection methods

Summary: 
Dynamic Attention Analysis (DAA) is introduced as a novel perspective for detecting backdoor attacks in text-to-image diffusion models. The study focuses on the dynamic characteristics of these models, showing that backdoor samples exhibit distinct feature evolution patterns at the $<$EOS$>$ token. The DAA approach includes two models: DAA-I, which measures dynamic features using the Frobenius norm, and DAA-S, which uses a dynamical system-based approach to capture spatial correlations among attention maps. The global asymptotic stability of DAA-S is theoretically analyzed. Experimental results across five backdoor attack scenarios demonstrate the effectiveness of the proposed approach, with an average F1 Score of 79.49% and an AUC of 87.67%. The code for DAA is available on GitHub at https://github.com/Robin-WZQ/DAA.<br /><br />Summary: Dynamic Attention Analysis is a new method that leverages the dynamic characteristics of attention maps in identifying backdoor attacks in text-to-image diffusion models. The approach outperforms existing detection methods, showing promising results in detecting and mitigating such vulnerabilities in AI systems. <div>
arXiv:2504.20518v1 Announce Type: new 
Abstract: Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the $<$EOS$>$ token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens' attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across five representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.49% and an AUC of 87.67%. The code is available at https://github.com/Robin-WZQ/DAA.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-aware Temporal Aggregation Network for Monocular 3D Lane Detection</title>
<link>https://arxiv.org/abs/2504.20525</link>
<guid>https://arxiv.org/abs/2504.20525</guid>
<content:encoded><![CDATA[
<div> Temporal Geometry Enhancement Module, Temporal Instance-aware Query Generation, Monocular 3D lane detection, GTA-Net, geometric consistency, lane integrity<br />
Summary:<br />
The article introduces GTA-Net, a novel approach for monocular 3D lane detection that overcomes existing limitations. The GTA-Net consists of the Temporal Geometry Enhancement Module (TGEM) for leveraging geometric consistency across frames to improve scene perception. It also includes the Temporal Instance-aware Query Generation (TIQG) for generating queries that incorporate temporal cues to enhance instance information. Through these modules, GTA-Net achieves state-of-the-art results in monocular 3D lane detection by addressing issues related to geometric accuracy and lane integrity. The method effectively utilizes multiple input frames to enhance geometry perception and lane integrity, surpassing previous solutions. <div>
arXiv:2504.20525v1 Announce Type: new 
Abstract: Monocular 3D lane detection aims to estimate 3D position of lanes from frontal-view (FV) images. However, current monocular 3D lane detection methods suffer from two limitations, including inaccurate geometric information of the predicted 3D lanes and difficulties in maintaining lane integrity. To address these issues, we seek to fully exploit the potential of multiple input frames. First, we aim at enhancing the ability to perceive the geometry of scenes by leveraging temporal geometric consistency. Second, we strive to improve the integrity of lanes by revealing more instance information from temporal sequences. Therefore, we propose a novel Geometry-aware Temporal Aggregation Network (GTA-Net) for monocular 3D lane detection. On one hand, we develop the Temporal Geometry Enhancement Module (TGEM), which exploits geometric consistency across successive frames, facilitating effective geometry perception. On the other hand, we present the Temporal Instance-aware Query Generation (TIQG), which strategically incorporates temporal cues into query generation, thereby enabling the exploration of comprehensive instance information. Experiments demonstrate that our GTA-Net achieves SoTA results, surpassing existing monocular 3D lane detection solutions.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Horizon: Decoupling UAVs Multi-View Action Recognition via Partial Order Transfer</title>
<link>https://arxiv.org/abs/2504.20530</link>
<guid>https://arxiv.org/abs/2504.20530</guid>
<content:encoded><![CDATA[
<div> altitude variations, action recognition, unmanned aerial vehicles, multi-view formulation, partial order
  
Summary:<br />
- The article addresses the challenges of action recognition in unmanned aerial vehicles (UAVs) due to significant view variations along the vertical spatial axis and varying altitudes.<br />
- A multi-view formulation is introduced to account for these variations, with a partial order observed among views where recognition accuracy decreases with altitude.<br />
- The Partial Order Guided Multi-View Network (POG-MVNet) is proposed to model the hierarchical structure of UAV views and improve recognition performance across altitudes.<br />
- POG-MVNet comprises three key components: a View Partition module, an Order-aware Feature Decoupling module, and an Action Partial Order Guide.<br />
- Experimental results on various datasets show that POG-MVNet significantly outperforms existing methods, with improvements of 4.7% on the Drone-Action dataset and 3.5% on the UAV dataset compared to state-of-the-art methods ASAT and FAR.<br /> 

Summary: <div>
arXiv:2504.20530v1 Announce Type: new 
Abstract: Action recognition in unmanned aerial vehicles (UAVs) poses unique challenges due to significant view variations along the vertical spatial axis. Unlike traditional ground-based settings, UAVs capture actions from a wide range of altitudes, resulting in considerable appearance discrepancies. We introduce a multi-view formulation tailored to varying UAV altitudes and empirically observe a partial order among views, where recognition accuracy consistently decreases as the altitude increases. This motivates a novel approach that explicitly models the hierarchical structure of UAV views to improve recognition performance across altitudes. To this end, we propose the Partial Order Guided Multi-View Network (POG-MVNet), designed to address drastic view variations by effectively leveraging view-dependent information across different altitude levels. The framework comprises three key components: a View Partition (VP) module, which uses the head-to-body ratio to group views by altitude; an Order-aware Feature Decoupling (OFD) module, which disentangles action-relevant and view-specific features under partial order guidance; and an Action Partial Order Guide (APOG), which leverages the partial order to transfer informative knowledge from easier views to support learning in more challenging ones. We conduct experiments on Drone-Action, MOD20, and UAV datasets, demonstrating that POG-MVNet significantly outperforms competing methods. For example, POG-MVNet achieves a 4.7% improvement on Drone-Action dataset and a 3.5% improvement on UAV dataset compared to state-of-the-art methods ASAT and FAR. The code for POG-MVNet will be made available soon.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study</title>
<link>https://arxiv.org/abs/2504.20541</link>
<guid>https://arxiv.org/abs/2504.20541</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, point clouds, WiFi Channel State Information, autoencoder, environmental mapping

Summary:
This paper presents a novel deep learning framework for generating point clouds from WiFi Channel State Information (CSI) data. The approach utilizes a two-stage autoencoder method, consisting of a PointNet autoencoder with convolutional layers for point cloud creation and a Convolutional Neural Network autoencoder for mapping CSI data to a corresponding latent space. By aligning these latent spaces, the method effectively reconstructs environmental point clouds from WiFi data with high accuracy. Experimental results confirm the efficacy of the proposed approach, demonstrating its potential for wireless sensing applications and environmental mapping tasks. The framework offers a promising solution for leveraging WiFi data to create detailed and precise representations of physical environments, which can be invaluable for various real-world applications. 

<br /><br />Summary: <div>
arXiv:2504.20541v1 Announce Type: new 
Abstract: This paper introduces a deep learning framework for generating point clouds from WiFi Channel State Information data. We employ a two-stage autoencoder approach: a PointNet autoencoder with convolutional layers for point cloud generation, and a Convolutional Neural Network autoencoder to map CSI data to a matching latent space. By aligning these latent spaces, our method enables accurate environmental point cloud reconstruction from WiFi data. Experimental results validate the effectiveness of our approach, highlighting its potential for wireless sensing and environmental mapping applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PartHOI: Part-based Hand-Object Interaction Transfer via Generalized Cylinders</title>
<link>https://arxiv.org/abs/2504.20599</link>
<guid>https://arxiv.org/abs/2504.20599</guid>
<content:encoded><![CDATA[
<div> Keywords: hand-object interactions, part-based transfer, geometric correspondence, cross-category transfer, high-fidelity results

Summary:
PartHOI introduces a novel method for transferring hand poses between objects based on the objects' specific semantic parts. By using a generalized cylinder representation to parameterize object parts' geometry, PartHOI establishes a robust geometric correspondence between parts, enabling the transfer of contact points. This approach addresses the challenge of transferring poses across different object categories by focusing on consistent shapes of semantic parts. PartHOI optimizes hand poses to fit the target object well, resulting in high-fidelity results that outperform existing methods. The method generalizes well for cross-category objects, showcasing its effectiveness in understanding and modeling hand-object interactions through part-based transfer. <div>
arXiv:2504.20599v1 Announce Type: new 
Abstract: Learning-based methods to understand and model hand-object interactions (HOI) require a large amount of high-quality HOI data. One way to create HOI data is to transfer hand poses from a source object to another based on the objects' geometry. However, current methods for transferring hand poses between objects rely on shape matching, limiting the ability to transfer poses across different categories due to differences in their shapes and sizes. We observe that HOI often involves specific semantic parts of objects, which often have more consistent shapes across categories. In addition, constructing size-invariant correspondences between these parts is important for cross-category transfer. Based on these insights, we introduce a novel method PartHOI for part-based HOI transfer. Using a generalized cylinder representation to parameterize an object parts' geometry, PartHOI establishes a robust geometric correspondence between object parts, and enables the transfer of contact points. Given the transferred points, we optimize a hand pose to fit the target object well. Qualitative and quantitative results demonstrate that our method can generalize HOI transfers well even for cross-category objects, and produce high-fidelity results that are superior to the existing methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Purifying, Labeling, and Utilizing: A High-Quality Pipeline for Small Object Detection</title>
<link>https://arxiv.org/abs/2504.20602</link>
<guid>https://arxiv.org/abs/2504.20602</guid>
<content:encoded><![CDATA[
<div> Keywords: Small object detection, Pipeline-style process, Feature purifying, Label assignment, Multi-scale scenarios

Summary:<br />
Small object detection is typically approached as a pipeline-style process involving upstream image processing, midstream sample selection, and downstream classification and regression. The PLUSNet framework optimizes three key aspects - Purifying, Labeling, and Utilizing - to improve small object detection performance. The framework includes the Hierarchical Feature Purifier (HFP) for refining upstream features, Multiple Criteria Label Assignment (MCLA) for enhancing midstream training samples, and Frequency Decoupled Head (FDHead) for better information utilization in downstream tasks. PLUS modules are versatile and can be integrated into various object detectors for improved performance in multi-scale scenarios. Experimental results show that PLUSNet consistently outperforms existing methods across multiple datasets for small object detection.<br /> <div>
arXiv:2504.20602v1 Announce Type: new 
Abstract: Small object detection is a broadly investigated research task and is commonly conceptualized as a "pipeline-style" engineering process. In the upstream, images serve as raw materials for processing in the detection pipeline, where pre-trained models are employed to generate initial feature maps. In the midstream, an assigner selects training positive and negative samples. Subsequently, these samples and features are fed into the downstream for classification and regression. Previous small object detection methods often focused on improving isolated stages of the pipeline, thereby neglecting holistic optimization and consequently constraining overall performance gains. To address this issue, we have optimized three key aspects, namely Purifying, Labeling, and Utilizing, in this pipeline, proposing a high-quality Small object detection framework termed PLUSNet. Specifically, PLUSNet comprises three sequential components: the Hierarchical Feature Purifier (HFP) for purifying upstream features, the Multiple Criteria Label Assignment (MCLA) for improving the quality of midstream training samples, and the Frequency Decoupled Head (FDHead) for more effectively exploiting information to accomplish downstream tasks. The proposed PLUS modules are readily integrable into various object detectors, thus enhancing their detection capabilities in multi-scale scenarios. Extensive experiments demonstrate the proposed PLUSNet consistently achieves significant and consistent improvements across multiple datasets for small object detection.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian</title>
<link>https://arxiv.org/abs/2504.20607</link>
<guid>https://arxiv.org/abs/2504.20607</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, 3D human body reconstruction, Articulated 2D Gaussian, pose calibration, dynamic surface planes

Summary:<br /><br />
EfficientHuman introduces a novel approach to reconstructing dynamic 3D human bodies quickly and accurately using Articulated 2D Gaussian surfels. By encoding Gaussian splats as surfels and using Linear Blend Skinning for pose transformations, the model can efficiently fit dynamic human poses while ensuring view-consistent geometries. The model includes a pose calibration module and an optimization module to achieve precise fitting of poses and reduce redundant Gaussians in the reconstruction process. Experimental results on the ZJU-MoCap dataset demonstrate that EfficientHuman can achieve rapid reconstruction in less than a minute on average, outperforming current state-of-the-art methods by 20 seconds. This innovative approach addresses the challenges of multi-view inconsistency and redundant Gaussians in dynamic surface reconstruction. <div>
arXiv:2504.20607v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in scene reconstruction and novel view synthesis. Recent work on reconstructing the 3D human body using 3DGS attempts to leverage prior information on human pose to enhance rendering quality and improve training speed. However, it struggles to effectively fit dynamic surface planes due to multi-view inconsistency and redundant Gaussians. This inconsistency arises because Gaussian ellipsoids cannot accurately represent the surfaces of dynamic objects, which hinders the rapid reconstruction of the dynamic human body. Meanwhile, the prevalence of redundant Gaussians means that the training time of these works is still not ideal for quickly fitting a dynamic human body. To address these, we propose EfficientHuman, a model that quickly accomplishes the dynamic reconstruction of the human body using Articulated 2D Gaussian while ensuring high rendering quality. The key innovation involves encoding Gaussian splats as Articulated 2D Gaussian surfels in canonical space and then transforming them to pose space via Linear Blend Skinning (LBS) to achieve efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian surfels can quickly conform to the dynamic human body while ensuring view-consistent geometries. Additionally, we introduce a pose calibration module and an LBS optimization module to achieve precise fitting of dynamic human poses, enhancing the model's performance. Extensive experiments on the ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic human reconstruction in less than a minute on average, which is 20 seconds faster than the current state-of-the-art method, while also reducing the number of redundant Gaussians.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation</title>
<link>https://arxiv.org/abs/2504.20629</link>
<guid>https://arxiv.org/abs/2504.20629</guid>
<content:encoded><![CDATA[
<div> keyword1 keyword2 keyword3 keyword4 keyword5 
<br />
Summary: 
This paper introduces AlignDiT, a multimodal Aligned Diffusion Transformer for synthesizing high-quality speech from various input modalities. The task of multimodal-to-speech generation is addressed, focusing on text, video, and reference audio inputs. AlignDiT utilizes in-context learning of the DiT architecture and employs effective strategies to align multimodal representations, improving speech intelligibility, audio-video synchronization, naturalness, and voice similarity to the reference speaker. A novel classifier-free guidance mechanism enables adaptive information balancing from each modality during synthesis. Extensive experiments demonstrate superior performance of AlignDiT in quality, synchronization, and speaker similarity compared to existing methods. The model shows strong generalization across different multimodal tasks and achieves state-of-the-art results in video-to-speech synthesis and visual forced alignment. The research provides a significant advancement in multimodal speech synthesis, with a demo available for further exploration. <div>
arXiv:2504.20629v1 Announce Type: new 
Abstract: In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT .
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping</title>
<link>https://arxiv.org/abs/2504.20645</link>
<guid>https://arxiv.org/abs/2504.20645</guid>
<content:encoded><![CDATA[
<div> Keywords: Polygonal road outline extraction, High-resolution aerial images, LDPoly, Dual-Latent Diffusion Model, Map2ImLas

Summary:
LDPoly is a new framework designed specifically for extracting polygonal road outlines from high-resolution aerial images. It introduces a Dual-Latent Diffusion Model with a Channel-Embedded Fusion Module to generate road masks and vertex heatmaps simultaneously. The framework then applies a tailored polygonization method to produce accurate vectorized road polygons with minimal vertex redundancy. Evaluation on the Map2ImLas dataset, containing detailed polygonal annotations for various topographic objects in Dutch regions, shows that LDPoly outperforms existing methods in terms of pixel-level coverage, vertex efficiency, polygon regularity, and road connectivity. Two new metrics, assessing polygon simplicity and boundary smoothness, are also introduced. This work marks the first application of diffusion models for extracting precise vectorized object outlines from remote-sensing imagery, setting a foundation for future advancements in this field.

<br /><br />Summary: <div>
arXiv:2504.20645v1 Announce Type: new 
Abstract: Polygonal road outline extraction from high-resolution aerial images is an important task in large-scale topographic mapping, where roads are represented as vectorized polygons, capturing essential geometric features with minimal vertex redundancy. Despite its importance, no existing method has been explicitly designed for this task. While polygonal building outline extraction has been extensively studied, the unique characteristics of roads, such as branching structures and topological connectivity, pose challenges to these methods. To address this gap, we introduce LDPoly, the first dedicated framework for extracting polygonal road outlines from high-resolution aerial images. Our method leverages a novel Dual-Latent Diffusion Model with a Channel-Embedded Fusion Module, enabling the model to simultaneously generate road masks and vertex heatmaps. A tailored polygonization method is then applied to obtain accurate vectorized road polygons with minimal vertex redundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which contains detailed polygonal annotations for various topographic objects in several Dutch regions. Our experiments include both in-region and cross-region evaluations, with the latter designed to assess the model's generalization performance on unseen regions. Quantitative and qualitative results demonstrate that LDPoly outperforms state-of-the-art polygon extraction methods across various metrics, including pixel-level coverage, vertex efficiency, polygon regularity, and road connectivity. We also design two new metrics to assess polygon simplicity and boundary smoothness. Moreover, this work represents the first application of diffusion models for extracting precise vectorized object outlines without redundant vertices from remote-sensing imagery, paving the way for future advancements in this field.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data</title>
<link>https://arxiv.org/abs/2504.20648</link>
<guid>https://arxiv.org/abs/2504.20648</guid>
<content:encoded><![CDATA[
<div> Vision-language models, spatial reasoning, dataset construction, performance improvement, real-world applications<br />
Summary: <br />
Vision-language models (VLMs) excel in various tasks but struggle with spatial reasoning, a crucial human skill. Existing datasets lack diverse spatial relationships necessary for VLMs' understanding. To address this gap, a synthetic VQA dataset focusing on spatial reasoning was created using detailed image descriptions. The dataset contains 455k samples with 3.4 million QA pairs. By training on this dataset, Spatial-Reasoning Enhanced (SpaRE) VLMs showed significant performance gains, reaching up to 49% improvement on spatial reasoning benchmarks like What's Up while maintaining strong results on general tasks. This work bridges the gap between human and VLM spatial reasoning, enhancing VLMs' capabilities for real-world applications such as robotics and navigation.<br /> <div>
arXiv:2504.20648v1 Announce Type: new 
Abstract: Vision-language models (VLMs) work well in tasks ranging from image captioning to visual question answering (VQA), yet they struggle with spatial reasoning, a key skill for understanding our physical world that humans excel at. We find that spatial relations are generally rare in widely used VL datasets, with only a few being well represented, while most form a long tail of underrepresented relations. This gap leaves VLMs ill-equipped to handle diverse spatial relationships. To bridge it, we construct a synthetic VQA dataset focused on spatial reasoning generated from hyper-detailed image descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset consists of 455k samples containing 3.4 million QA pairs. Trained on this dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements on spatial reasoning benchmarks, achieving up to a 49% performance gain on the What's Up benchmark, while maintaining strong results on general tasks. Our work narrows the gap between human and VLM spatial reasoning and makes VLMs more capable in real-world tasks such as robotics and navigation.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image deidentification in the XNAT ecosystem: use cases and solutions</title>
<link>https://arxiv.org/abs/2504.20657</link>
<guid>https://arxiv.org/abs/2504.20657</guid>
<content:encoded><![CDATA[
<div> Keywords: XNAT, deidentification, DICOM data, machine-learning models, address recognition<br />
Summary:<br />
The article discusses a deidentification workflow for DICOM data using XNAT and other tools. The authors participated in the MIDI-B challenge and initially achieved a score of 97.91%, which improved to 99.61% post-feedback. A rule-based approach successfully removed name-related information but had challenges with addresses. Machine-learning models showed promise in removing addresses but were overly aggressive on other data. Future work will focus on improving address recognition and removing identifiable data from image pixels. Discussions with challenge organizers are ongoing regarding deidentification failures, estimated at 0.19%. The study highlights the importance of addressing technical challenges and improving deidentification techniques for medical image datasets.<br /><br />Summary: <div>
arXiv:2504.20657v1 Announce Type: new 
Abstract: XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT "ecosystem". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be "over-aggressive" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the "answer key" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\%. (Abridged from original for arXiv submission)
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advance Fake Video Detection via Vision Transformers</title>
<link>https://arxiv.org/abs/2504.20669</link>
<guid>https://arxiv.org/abs/2504.20669</guid>
<content:encoded><![CDATA[
<div> ViT, fake image detection, video, generative techniques, AI-generated media detection<br />
Summary: <br />
- The paper discusses the rise of hyper-realistic AI-generated multimedia and the potential for misinformation.
- It emphasizes the need for accurate and generalizable detection methods.
- The proposed framework extends ViT-based fake image detection to videos.
- The method integrates ViT embeddings over time for enhanced detection performance.
- Results show promising accuracy, generalization, and few-shot learning capabilities across diverse datasets. <div>
arXiv:2504.20669v1 Announce Type: new 
Abstract: Recent advancements in AI-based multimedia generation have enabled the creation of hyper-realistic images and videos, raising concerns about their potential use in spreading misinformation. The widespread accessibility of generative techniques, which allow for the production of fake multimedia from prompts or existing media, along with their continuous refinement, underscores the urgent need for highly accurate and generalizable AI-generated media detection methods, underlined also by new regulations like the European Digital AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based fake image detection and extend this idea to video. We propose an {original} %innovative framework that effectively integrates ViT embeddings over time to enhance detection performance. Our method shows promising accuracy, generalization, and few-shot learning capabilities across a new, large and diverse dataset of videos generated using five open source generative techniques from the state-of-the-art, as well as a separate dataset containing videos produced by proprietary generative methods.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection</title>
<link>https://arxiv.org/abs/2504.20670</link>
<guid>https://arxiv.org/abs/2504.20670</guid>
<content:encoded><![CDATA[
<div> detect drone aerial image, real-time detectors, small target detection, efficiency, performance

Summary:
The paper introduces FBRT-YOLO, a new family of real-time detectors designed to enhance small target detection efficiency in aerial images. This method includes the Feature Complementary Mapping Module (FCM) and the Multi-Kernel Perception Unit (MKP). FCM addresses the information imbalance issue by integrating spatial positional information of targets more deeply into the network to improve target localization. MKP uses convolutions with different kernel sizes to enhance relationships between targets of various scales, improving target perception. Experimental results on Visdrone, UAVDT, and AI-TOD datasets show that FBRT-YOLO outperforms other real-time detectors in terms of both performance and speed. <div>
arXiv:2504.20670v1 Announce Type: new 
Abstract: Embedded flight devices with visual capabilities have become essential for a wide range of applications. In aerial image detection, while many existing methods have partially addressed the issue of small target detection, challenges remain in optimizing small target detection and balancing detection accuracy with efficiency. These issues are key obstacles to the advancement of real-time aerial image detection. In this paper, we propose a new family of real-time detectors for aerial image detection, named FBRT-YOLO, to address the imbalance between detection accuracy and efficiency. Our method comprises two lightweight modules: Feature Complementary Mapping Module (FCM) and Multi-Kernel Perception Unit(MKP), designed to enhance object perception for small targets in aerial images. FCM focuses on alleviating the problem of information imbalance caused by the loss of small target information in deep networks. It aims to integrate spatial positional information of targets more deeply into the network,better aligning with semantic information in the deeper layers to improve the localization of small targets. We introduce MKP, which leverages convolutions with kernels of different sizes to enhance the relationships between targets of various scales and improve the perception of targets at different scales. Extensive experimental results on three major aerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that FBRT-YOLO outperforms various real-time detectors in terms of performance and speed.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Occlusion-aware Driver Monitoring System using the Driver Monitoring Dataset</title>
<link>https://arxiv.org/abs/2504.20677</link>
<guid>https://arxiv.org/abs/2504.20677</guid>
<content:encoded><![CDATA[
<div> Keywords: Driver monitoring system, occlusion-aware, gaze estimation, face occlusion detection, RGB and IR images 

Summary: 
This paper introduces a robust driver monitoring system that takes into account occlusions and varying lighting conditions. The system utilizes the Driver Monitoring Dataset (DMD) and performs driver identification, gaze estimation by regions, and face occlusion detection. By incorporating EuroNCAP recommendations, the system enhances situational awareness and trustworthiness by detecting potential performance degradation. Two separate algorithms, trained on RGB and infrared images, work together to ensure reliable functioning. The paper discusses the challenges of integrating these algorithms into a cohesive pipeline, especially when dealing with different sensors and real-world implementation. Evaluation on the DMD and in realistic scenarios validates the effectiveness of the system, highlighting the superior performance of the RGB-based models and the pioneering role of robust occlusion detection in driver monitoring systems.<br /><br />Summary: <div>
arXiv:2504.20677v1 Announce Type: new 
Abstract: This paper presents a robust, occlusion-aware driver monitoring system (DMS) utilizing the Driver Monitoring Dataset (DMD). The system performs driver identification, gaze estimation by regions, and face occlusion detection under varying lighting conditions, including challenging low-light scenarios. Aligned with EuroNCAP recommendations, the inclusion of occlusion detection enhances situational awareness and system trustworthiness by indicating when the system's performance may be degraded. The system employs separate algorithms trained on RGB and infrared (IR) images to ensure reliable functioning. We detail the development and integration of these algorithms into a cohesive pipeline, addressing the challenges of working with different sensors and real-car implementation. Evaluation on the DMD and in real-world scenarios demonstrates the effectiveness of the proposed system, highlighting the superior performance of RGB-based models and the pioneering contribution of robust occlusion detection in DMS.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation</title>
<link>https://arxiv.org/abs/2504.20682</link>
<guid>https://arxiv.org/abs/2504.20682</guid>
<content:encoded><![CDATA[
<div> model, table recognition, document analysis, spatial coordinates, dataset <br />
Summary: <br />
Table structure recognition in document analysis is challenging due to geometric deformation in tables. The OG-HFYOLO model proposed in this work improves cell localization accuracy by enhancing edge response, employing a Heterogeneous Kernel Cross Fusion module, and using a scale-aware loss function. A mask-driven non-maximal suppression approach is introduced for post-processing, replacing traditional bounding box suppression. A new dataset named Deformation Wired Table (DWTAL) is also introduced to address the lack of fine-grained deformation table cell spatial coordinate localization data. Experimental results demonstrate the model's high segmentation accuracy on various instance segmentation models. The dataset and source code are available for further research and development. <div>
arXiv:2504.20682v1 Announce Type: new 
Abstract: Table structure recognition is a key task in document analysis. However, the geometric deformation in deformed tables causes a weak correlation between content information and structure, resulting in downstream tasks not being able to obtain accurate content information. To obtain fine-grained spatial coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge response by Gradient Orientation-aware Extractor, combines a Heterogeneous Kernel Cross Fusion module and a scale-aware loss function to adapt to multi-scale objective features, and introduces mask-driven non-maximal suppression in the post-processing, which replaces the traditional bounding box suppression mechanism. Furthermore, we also propose a data generator, filling the gap in the dataset for fine-grained deformation table cell spatial coordinate localization, and derive a large-scale dataset named Deformation Wired Table (DWTAL). Experiments show that our proposed model demonstrates excellent segmentation accuracy on all mainstream instance segmentation models. The dataset and the source code are open source: https://github.com/justliulong/OGHFYOLO.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion</title>
<link>https://arxiv.org/abs/2504.20685</link>
<guid>https://arxiv.org/abs/2504.20685</guid>
<content:encoded><![CDATA[
arXiv:2504.20685v1 Announce Type: new 
Abstract: Generating realistic listener facial motions in dyadic conversations remains challenging due to the high-dimensional action space and temporal dependency requirements. Existing approaches usually consider extracting 3D Morphable Model (3DMM) coefficients and modeling in the 3DMM space. However, this makes the computational speed of the 3DMM a bottleneck, making it difficult to achieve real-time interactive responses. To tackle this problem, we propose Facial Action Diffusion (FAD), which introduces the diffusion methods from the field of image generation to achieve efficient facial action generation. We further build the Efficient Listener Network (ELNet) specially designed to accommodate both the visual and audio information of the speaker as input. Considering of FAD and ELNet, the proposed method learns effective listener facial motion representations and leads to improvements of performance over the state-of-the-art methods while reducing 99% computational time.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer</title>
<link>https://arxiv.org/abs/2504.20690</link>
<guid>https://arxiv.org/abs/2504.20690</guid>
<content:encoded><![CDATA[
arXiv:2504.20690v1 Announce Type: new 
Abstract: Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine Transform Map and Keypoint for Human-Centric Pretraining</title>
<link>https://arxiv.org/abs/2504.20800</link>
<guid>https://arxiv.org/abs/2504.20800</guid>
<content:encoded><![CDATA[
arXiv:2504.20800v1 Announce Type: new 
Abstract: Human-centric perception is the core of diverse computer vision tasks and has been a long-standing research focus. However, previous research studied these human-centric tasks individually, whose performance is largely limited to the size of the public task-specific datasets. Recent human-centric methods leverage the additional modalities, e.g., depth, to learn fine-grained semantic information, which limits the benefit of pretraining models due to their sensitivity to camera views and the scarcity of RGB-D data on the Internet. This paper improves the data scalability of human-centric pretraining methods by discarding depth information and exploring semantic information of RGB images in the frequency space by Discrete Cosine Transform (DCT). We further propose new annotation denoising auxiliary tasks with keypoints and DCT maps to enforce the RGB image extractor to learn fine-grained semantic information of human bodies. Our extensive experiments show that when pretrained on large-scale datasets (COCO and AIC datasets) without depth annotation, our model achieves better performance than state-of-the-art methods by +0.5 mAP on COCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by +4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on SHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for crowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for person ReID. We also validate the effectiveness of our method on MPII+NTURGBD datasets
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion</title>
<link>https://arxiv.org/abs/2504.20829</link>
<guid>https://arxiv.org/abs/2504.20829</guid>
<content:encoded><![CDATA[
arXiv:2504.20829v1 Announce Type: new 
Abstract: As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene representation and novel view synthesis, its rapid adoption in safety-critical domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of potential security vulnerabilities. This paper presents the first systematic study of backdoor threats in 3DGS pipelines. We identify that adversaries may implant backdoor views to induce malicious scene confusion during inference, potentially leading to environmental misperception in autonomous navigation or spatial distortion in immersive environments. To uncover this risk, we propose GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap injects malicious views at specific attack viewpoints while preserving high-quality rendering in non-target views, ensuring minimal detectability and maximizing potential harm. Specifically, the proposed method consists of a three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing attack efficacy and perceptual realism to expose security risks in 3D rendering. Extensive experiments on both synthetic and real-world datasets demonstrate that GuassTrap can effectively embed imperceptible yet harmful backdoor views while maintaining high-quality rendering in normal views, validating its robustness, adaptability, and practical applicability.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation</title>
<link>https://arxiv.org/abs/2504.20830</link>
<guid>https://arxiv.org/abs/2504.20830</guid>
<content:encoded><![CDATA[
arXiv:2504.20830v1 Announce Type: new 
Abstract: While accurate and user-friendly Computer-Aided Design (CAD) is crucial for industrial design and manufacturing, existing methods still struggle to achieve this due to their over-simplified representations or architectures incapable of supporting multimodal design requirements. In this paper, we attempt to tackle this problem from both methods and datasets aspects. First, we propose a cascade MAR with topology predictor (CMT), the first multimodal framework for CAD generation based on Boundary Representation (B-Rep). Specifically, the cascade MAR can effectively capture the ``edge-counters-surface'' priors that are essential in B-Reps, while the topology predictor directly estimates topology in B-Reps from the compact tokens in MAR. Second, to facilitate large-scale training, we develop a large-scale multimodal CAD dataset, mmABC, which includes over 1.3 million B-Rep models with multimodal annotations, including point clouds, text descriptions, and multi-view images. Extensive experiments show the superior of CMT in both conditional and unconditional CAD generation tasks. For example, we improve Coverage and Valid ratio by +10.68% and +10.3%, respectively, compared to state-of-the-art methods on ABC in unconditional generation. CMT also improves +4.01 Chamfer on image conditioned CAD generation on mmABC. The dataset, code and pretrained network shall be released.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadSAM: Segmenting 3D radiological images with a 2D promptable model</title>
<link>https://arxiv.org/abs/2504.20837</link>
<guid>https://arxiv.org/abs/2504.20837</guid>
<content:encoded><![CDATA[
arXiv:2504.20837v1 Announce Type: new 
Abstract: Medical image segmentation is a crucial and time-consuming task in clinical care, where mask precision is extremely important. The Segment Anything Model (SAM) offers a promising approach, as it provides an interactive interface based on visual prompting and edition to refine an initial segmentation. This model has strong generalization capabilities, does not rely on predefined classes, and adapts to diverse objects; however, it is pre-trained on natural images and lacks the ability to process medical data effectively. In addition, this model is built for 2D images, whereas a whole medical domain is based on 3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging are based on 2D models, thus requiring one prompt per slice to segment 3D objects, making the segmentation process tedious. They also lack important features such as editing. To bridge this gap, we propose RadSAM, a novel method for segmenting 3D objects with a 2D model from a single prompt. In practice, we train a 2D model using noisy masks as initial prompts, in addition to bounding boxes and points. We then use this novel prompt type with an iterative inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a benchmark to evaluate the model's ability to segment 3D objects in CT images from a single prompt and evaluate the models' out-of-domain transfer and edition capabilities. We demonstrate the effectiveness of our approach against state-of-the-art models on this benchmark using the AMOS abdominal organ segmentation dataset.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.20860</link>
<guid>https://arxiv.org/abs/2504.20860</guid>
<content:encoded><![CDATA[
arXiv:2504.20860v1 Announce Type: new 
Abstract: Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated learning by tuning lightweight input tokens (or prompts) on local client data, while keeping network weights frozen. Post training, only the prompts are shared by the clients with the central server for aggregation. However, textual prompt tuning often struggles with overfitting to known concepts and may be overly reliant on memorized text features, limiting its adaptability to unseen concepts. To address this limitation, we propose Federated Multimodal Visual Prompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual information -- image-conditioned features and textual attribute features of a class -- that is multimodal in nature. At the core of FedMVP is a PromptFormer module that synergistically aligns textual and visual features through cross-attention, enabling richer contexual integration. The dynamically generated multimodal visual prompts are then input to the frozen vision encoder of CLIP, and trained with a combination of CLIP similarity loss and a consistency loss. Extensive evaluation on 20 datasets spanning three generalization settings demonstrates that FedMVP not only preserves performance on in-distribution classes and domains, but also displays higher generalizability to unseen classes and domains when compared to state-of-the-art methods. Codes will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2504.20865</link>
<guid>https://arxiv.org/abs/2504.20865</guid>
<content:encoded><![CDATA[
arXiv:2504.20865v1 Announce Type: new 
Abstract: The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLIM-based Salient Object Detection Networks with Adaptive Decoders</title>
<link>https://arxiv.org/abs/2504.20872</link>
<guid>https://arxiv.org/abs/2504.20872</guid>
<content:encoded><![CDATA[
arXiv:2504.20872v1 Announce Type: new 
Abstract: Salient Object Detection (SOD) methods can locate objects that stand out in an image, assign higher values to their pixels in a saliency map, and binarize the map outputting a predicted segmentation mask. A recent tendency is to investigate pre-trained lightweight models rather than deep neural networks in SOD tasks, coping with applications under limited computational resources. In this context, we have investigated lightweight networks using a methodology named Feature Learning from Image Markers (FLIM), which assumes that the encoder's kernels can be estimated from marker pixels on discriminative regions of a few representative images. This work proposes flyweight networks, hundreds of times lighter than lightweight models, for SOD by combining a FLIM encoder with an adaptive decoder, whose weights are estimated for each input image by a given heuristic function. Such FLIM networks are trained from three to four representative images only and without backpropagation, making the models suitable for applications under labeled data constraints as well. We study five adaptive decoders; two of them are introduced here. Differently from the previous ones that rely on one neuron per pixel with shared weights, the heuristic functions of the new adaptive decoders estimate the weights of each neuron per pixel. We compare FLIM models with adaptive decoders for two challenging SOD tasks with three lightweight networks from the state-of-the-art, two FLIM networks with decoders trained by backpropagation, and one FLIM network whose labeled markers define the decoder's weights. The experiments demonstrate the advantages of the proposed networks over the baselines, revealing the importance of further investigating such methods in new applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers</title>
<link>https://arxiv.org/abs/2504.20902</link>
<guid>https://arxiv.org/abs/2504.20902</guid>
<content:encoded><![CDATA[
arXiv:2504.20902v1 Announce Type: new 
Abstract: A person downloading a pre-trained model from the web should be aware of its biases. Existing approaches for bias identification rely on datasets containing labels for the task of interest, something that a non-expert may not have access to, or may not have the necessary resources to collect: this greatly limits the number of tasks where model biases can be identified. In this work, we present Classifier-to-Bias (C2B), the first bias discovery framework that works without access to any labeled data: it only relies on a textual description of the classification task to identify biases in the target classification model. This description is fed to a large language model to generate bias proposals and corresponding captions depicting biases together with task-specific target labels. A retrieval model collects images for those captions, which are then used to assess the accuracy of the model w.r.t. the given biases. C2B is training-free, does not require any annotations, has no constraints on the list of biases, and can be applied to any pre-trained model on any classification task. Experiments on two publicly available datasets show that C2B discovers biases beyond those of the original datasets and outperforms a recent state-of-the-art bias detection baseline that relies on task-specific annotations, being a promising first step toward addressing task-agnostic unsupervised bias detection.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition</title>
<link>https://arxiv.org/abs/2504.20948</link>
<guid>https://arxiv.org/abs/2504.20948</guid>
<content:encoded><![CDATA[
arXiv:2504.20948v1 Announce Type: new 
Abstract: Given the severe challenges confronting the global growth security of economic crops, precise identification and prevention of plant diseases has emerged as a critical issue in artificial intelligence-enabled agricultural technology. To address the technical challenges in plant disease recognition, including small-sample learning, leaf occlusion, illumination variations, and high inter-class similarity, this study innovatively proposes a Dynamic Dual-Stream Fusion Network (DS_FusionNet). The network integrates a dual-backbone architecture, deformable dynamic fusion modules, and bidirectional knowledge distillation strategy, significantly enhancing recognition accuracy. Experimental results demonstrate that DS_FusionNet achieves classification accuracies exceeding 90% using only 10% of the PlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the complex PlantWild dataset, exhibiting exceptional generalization capabilities. This research not only provides novel technical insights for fine-grained image classification but also establishes a robust foundation for precise identification and management of agricultural diseases.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features</title>
<link>https://arxiv.org/abs/2504.20970</link>
<guid>https://arxiv.org/abs/2504.20970</guid>
<content:encoded><![CDATA[
arXiv:2504.20970v1 Announce Type: new 
Abstract: Accurate and early diagnosis of pneumonia through X-ray imaging is essential for effective treatment and improved patient outcomes. Recent advancements in machine learning have enabled automated diagnostic tools that assist radiologists in making more reliable and efficient decisions. In this work, we propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework for multi-class pneumonia classification, leveraging powerful feature representations from state-of-the-art self-supervised and transfer learning models. Rather than relying on computationally expensive gradient based fine-tuning, we employ a closed-form, non-iterative classification approach that ensures efficiency without compromising accuracy. Experimental results demonstrate that SVD-LS achieves competitive performance while offering significantly reduced computational costs, making it a viable alternative for real-time medical imaging applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TesserAct: Learning 4D Embodied World Models</title>
<link>https://arxiv.org/abs/2504.20995</link>
<guid>https://arxiv.org/abs/2504.20995</guid>
<content:encoded><![CDATA[
arXiv:2504.20995v1 Announce Type: new 
Abstract: This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Fusion: Introducing New Modality to Frozen Large Language Models</title>
<link>https://arxiv.org/abs/2504.20996</link>
<guid>https://arxiv.org/abs/2504.20996</guid>
<content:encoded><![CDATA[
arXiv:2504.20996v1 Announce Type: new 
Abstract: We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YoChameleon: Personalized Vision and Language Generation</title>
<link>https://arxiv.org/abs/2504.20998</link>
<guid>https://arxiv.org/abs/2504.20998</guid>
<content:encoded><![CDATA[
arXiv:2504.20998v1 Announce Type: new 
Abstract: Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive" image generation approach to enhance image quality in a few-shot setting.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports</title>
<link>https://arxiv.org/abs/2504.20220</link>
<guid>https://arxiv.org/abs/2504.20220</guid>
<content:encoded><![CDATA[
arXiv:2504.20220v1 Announce Type: cross 
Abstract: Despite the growing adoption of electronic health records, many processes still rely on paper documents, reflecting the heterogeneous real-world conditions in which healthcare is delivered. The manual transcription process is time-consuming and prone to errors when transferring paper-based data to digital formats. To streamline this workflow, this study presents an open-source pipeline that extracts and categorizes checkbox data from scanned documents. Demonstrated on transfusion reaction reports, the design supports adaptation to other checkbox-rich document types. The proposed method integrates checkbox detection, multilingual optical character recognition (OCR) and multilingual vision-language models (VLMs). The pipeline achieves high precision and recall compared against annually compiled gold-standards from 2017 to 2024. The result is a reduction in administrative workload and accurate regulatory reporting. The open-source availability of this pipeline encourages self-hosted parsing of checkbox forms.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRO: Doppler-Aware Direct Radar Odometry</title>
<link>https://arxiv.org/abs/2504.20339</link>
<guid>https://arxiv.org/abs/2504.20339</guid>
<content:encoded><![CDATA[
arXiv:2504.20339v1 Announce Type: cross 
Abstract: A renaissance in radar-based sensing for mobile robotic applications is underway. Compared to cameras or lidars, millimetre-wave radars have the ability to `see' through thin walls, vegetation, and adversarial weather conditions such as heavy rain, fog, snow, and dust. In this paper, we propose a novel SE(2) odometry approach for spinning frequency-modulated continuous-wave radars. Our method performs scan-to-local-map registration of the incoming radar data in a direct manner using all the radar intensity information without the need for feature or point cloud extraction. The method performs locally continuous trajectory estimation and accounts for both motion and Doppler distortion of the radar scans. If the radar possesses a specific frequency modulation pattern that makes radial Doppler velocities observable, an additional Doppler-based constraint is formulated to improve the velocity estimate and enable odometry in geometrically feature-deprived scenarios (e.g., featureless tunnels). Our method has been validated on over 250km of on-road data sourced from public datasets (Boreas and MulRan) and collected using our automotive platform. With the aid of a gyroscope, it outperforms state-of-the-art methods and achieves an average relative translation error of 0.26% on the Boreas leaderboard. When using data with the appropriate Doppler-enabling frequency modulation pattern, the translation error is reduced to 0.18% in similar environments. We also benchmarked our algorithm using 1.5 hours of data collected with a mobile robot in off-road environments with various levels of structure to demonstrate its versatility. Our real-time implementation is publicly available: https://github.com/utiasASRL/dro.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks</title>
<link>https://arxiv.org/abs/2504.20340</link>
<guid>https://arxiv.org/abs/2504.20340</guid>
<content:encoded><![CDATA[
arXiv:2504.20340v1 Announce Type: cross 
Abstract: With AI-generated content becoming ubiquitous across the web, social media, and other digital platforms, it is vital to examine how such content are inspired and generated. The creation of AI-generated images often involves refining the input prompt iteratively to achieve desired visual outcomes. This study focuses on the relatively underexplored concept of image regeneration using AI, in which a human operator attempts to closely recreate a specific target image by iteratively refining their prompt. Image regeneration is distinct from normal image generation, which lacks any predefined visual reference. A separate challenge lies in determining whether existing image similarity metrics (ISMs) can provide reliable, objective feedback in iterative workflows, given that we do not fully understand if subjective human judgments of similarity align with these metrics. Consequently, we must first validate their alignment with human perception before assessing their potential as a feedback mechanism in the iterative prompt refinement process. To address these research gaps, we present a structured user study evaluating how iterative prompt refinement affects the similarity of regenerated images relative to their targets, while also examining whether ISMs capture the same improvements perceived by human observers. Our findings suggest that incremental prompt adjustments substantially improve alignment, verified through both subjective evaluations and quantitative measures, underscoring the broader potential of iterative workflows to enhance generative AI content creation across various application domains.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting</title>
<link>https://arxiv.org/abs/2504.20403</link>
<guid>https://arxiv.org/abs/2504.20403</guid>
<content:encoded><![CDATA[
arXiv:2504.20403v1 Announce Type: cross 
Abstract: Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses</title>
<link>https://arxiv.org/abs/2504.20405</link>
<guid>https://arxiv.org/abs/2504.20405</guid>
<content:encoded><![CDATA[
arXiv:2504.20405v1 Announce Type: cross 
Abstract: While deep learning has shown strong performance in musculoskeletal imaging, existing work has largely focused on pathologies where diagnosis is not a clinical challenge, leaving more difficult problems underexplored, such as detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard MRIs. Diagnosing these lesions is challenging due to their subtle imaging features, often leading to reliance on invasive MRI arthrograms (MRAs). This study introduces ScopeMRI, the first publicly available, expert-annotated dataset for shoulder pathologies, and presents a deep learning (DL) framework for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes 586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent arthroscopy. Ground truth labels were derived from intraoperative findings, the gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were trained using a combination of CNNs and transformers. Predictions from sagittal, axial, and coronal views were ensembled to optimize performance. The models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71 standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83% and 94%, and specificity of 91% and 86% for standard MRIs and MRAs, respectively. Notably, model performance on non-invasive standard MRIs matched or surpassed radiologists interpreting MRAs. External validation demonstrated initial generalizability across imaging protocols. This study demonstrates that DL models can achieve radiologist-level diagnostic performance on standard MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular codebase for training and evaluating deep learning models on 3D medical imaging data, we aim to accelerate research in musculoskeletal imaging and support the development of new datasets for clinically challenging diagnostic tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight</title>
<link>https://arxiv.org/abs/2504.20454</link>
<guid>https://arxiv.org/abs/2504.20454</guid>
<content:encoded><![CDATA[
arXiv:2504.20454v1 Announce Type: cross 
Abstract: This study integrates PET metabolic information with CT anatomical structures to establish a 3D multimodal segmentation dataset for lymphoma based on whole-body FDG PET/CT examinations, which bridges the gap of the lack of standardised multimodal segmentation datasets in the field of haematological malignancies. We retrospectively collected 483 examination datasets acquired between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were rigorously de-identified. Complete 3D structural information was preserved during data acquisition, preprocessing and annotation, and a high-quality dataset was constructed based on the nnUNet format. By systematic technical validation and evaluation of the preprocessing process, annotation quality and automatic segmentation algorithm, the deep learning model trained based on this dataset is verified to achieve accurate segmentation of lymphoma lesions in PET/CT images with high accuracy, good robustness and reproducibility, which proves the applicability and stability of this dataset in accurate segmentation and quantitative analysis. The deep fusion of PET/CT images achieved with this dataset not only significantly improves the accurate portrayal of the morphology, location and metabolic features of tumour lesions, but also provides solid data support for early diagnosis, clinical staging and personalized treatment, and promotes the development of automated image segmentation and precision medicine based on deep learning. The dataset and related resources are available at https://github.com/SuperD0122/LymphAtlas-.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2504.20501</link>
<guid>https://arxiv.org/abs/2504.20501</guid>
<content:encoded><![CDATA[
arXiv:2504.20501v1 Announce Type: cross 
Abstract: One-shot medical image segmentation (MIS) is crucial for medical analysis due to the burden of medical experts on manual annotation. The recent emergence of the segment anything model (SAM) has demonstrated remarkable adaptation in MIS but cannot be directly applied to one-shot medical image segmentation (MIS) due to its reliance on labor-intensive user interactions and the high computational cost. To cope with these limitations, we propose a novel SAM-guided robust representation learning framework, named RRL-MedSAM, to adapt SAM to one-shot 3D MIS, which exploits the strong generalization capabilities of the SAM encoder to learn better feature representation. We devise a dual-stage knowledge distillation (DSKD) strategy to distill general knowledge between natural and medical images from the foundation model to train a lightweight encoder, and then adopt a mutual exponential moving average (mutual-EMA) to update the weights of the general lightweight encoder and medical-specific encoder. Specifically, pseudo labels from the registration network are used to perform mutual supervision for such two encoders. Moreover, we introduce an auto-prompting (AP) segmentation decoder which adopts the mask generated from the general lightweight model as a prompt to assist the medical-specific model in boosting the final segmentation performance. Extensive experiments conducted on three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed RRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both segmentation and registration tasks. Especially, our lightweight encoder uses only 3\% of the parameters compared to the encoder of SAM-Base.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hydra: Marker-Free RGB-D Hand-Eye Calibration</title>
<link>https://arxiv.org/abs/2504.20584</link>
<guid>https://arxiv.org/abs/2504.20584</guid>
<content:encoded><![CDATA[
arXiv:2504.20584v1 Announce Type: cross 
Abstract: This work presents an RGB-D imaging-based approach to marker-free hand-eye calibration using a novel implementation of the iterative closest point (ICP) algorithm with a robust point-to-plane (PTP) objective formulated on a Lie algebra. Its applicability is demonstrated through comprehensive experiments using three well known serial manipulators and two RGB-D cameras. With only three randomly chosen robot configurations, our approach achieves approximately 90% successful calibrations, demonstrating 2-3x higher convergence rates to the global optimum compared to both marker-based and marker-free baselines. We also report 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9 robot configurations over other marker-free methods. Our method exhibits significantly improved accuracy (5 mm in task space) over classical approaches (7 mm in task space) whilst being marker-free. The benchmarking dataset and code are open sourced under Apache 2.0 License, and a ROS 2 integration with robot abstraction is provided to facilitate deployment.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks</title>
<link>https://arxiv.org/abs/2504.20658</link>
<guid>https://arxiv.org/abs/2504.20658</guid>
<content:encoded><![CDATA[
arXiv:2504.20658v1 Announce Type: cross 
Abstract: AI-generated synthetic media are increasingly used in real-world scenarios, often with the purpose of spreading misinformation and propaganda through social media platforms, where compression and other processing can degrade fake detection cues. Currently, many forensic tools fail to account for these in-the-wild challenges. In this work, we introduce TrueFake, a large-scale benchmarking dataset of 600,000 images including top notch generative techniques and sharing via three different social networks. This dataset allows for rigorous evaluation of state-of-the-art fake image detectors under very realistic and challenging conditions. Through extensive experimentation, we analyze how social media sharing impacts detection performance, and identify current most effective detection and training strategies. Our findings highlight the need for evaluating forensic models in conditions that mirror real-world use.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning a General Model: Folding Clothing with Topological Dynamics</title>
<link>https://arxiv.org/abs/2504.20720</link>
<guid>https://arxiv.org/abs/2504.20720</guid>
<content:encoded><![CDATA[
arXiv:2504.20720v1 Announce Type: cross 
Abstract: The high degrees of freedom and complex structure of garments present significant challenges for clothing manipulation. In this paper, we propose a general topological dynamics model to fold complex clothing. By utilizing the visible folding structure as the topological skeleton, we design a novel topological graph to represent the clothing state. This topological graph is low-dimensional and applied for complex clothing in various folding states. It indicates the constraints of clothing and enables predictions regarding clothing movement. To extract graphs from self-occlusion, we apply semantic segmentation to analyze the occlusion relationships and decompose the clothing structure. The decomposed structure is then combined with keypoint detection to generate the topological graph. To analyze the behavior of the topological graph, we employ an improved Graph Neural Network (GNN) to learn the general dynamics. The GNN model can predict the deformation of clothing and is employed to calculate the deformation Jacobi matrix for control. Experiments using jackets validate the algorithm's effectiveness to recognize and fold complex clothing with self-occlusion.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities</title>
<link>https://arxiv.org/abs/2504.20734</link>
<guid>https://arxiv.org/abs/2504.20734</guid>
<content:encoded><![CDATA[
arXiv:2504.20734v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Event-based Optical Marker Systems</title>
<link>https://arxiv.org/abs/2504.20736</link>
<guid>https://arxiv.org/abs/2504.20736</guid>
<content:encoded><![CDATA[
arXiv:2504.20736v1 Announce Type: cross 
Abstract: The advent of event-based cameras, with their low latency, high dynamic range, and reduced power consumption, marked a significant change in robotic vision and machine perception. In particular, the combination of these neuromorphic sensors with widely-available passive or active optical markers (e.g. AprilTags, arrays of blinking LEDs), has recently opened up a wide field of possibilities. This survey paper provides a comprehensive review on Event-Based Optical Marker Systems (EBOMS). We analyze the basic principles and technologies on which these systems are based, with a special focus on their asynchronous operation and robustness against adverse lighting conditions. We also describe the most relevant applications of EBOMS, including object detection and tracking, pose estimation, and optical communication. The article concludes with a discussion of possible future research directions in this rapidly-emerging and multidisciplinary field.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2504.20898</link>
<guid>https://arxiv.org/abs/2504.20898</guid>
<content:encoded><![CDATA[
arXiv:2504.20898v1 Announce Type: cross 
Abstract: Advancements in generative Artificial Intelligence (AI) hold great promise for automating radiology workflows, yet challenges in interpretability and reliability hinder clinical adoption. This paper presents an automated radiology report generation framework that combines Concept Bottleneck Models (CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge AI performance with clinical explainability. CBMs map chest X-ray features to human-understandable clinical concepts, enabling transparent disease classification. Meanwhile, the RAG system integrates multi-agent collaboration and external knowledge to produce contextually rich, evidence-based reports. Our demonstration showcases the system's ability to deliver interpretable predictions, mitigate hallucinations, and generate high-quality, tailored reports with an interactive interface addressing accuracy, trust, and usability challenges. This framework provides a pathway to improving diagnostic consistency and empowering radiologists with actionable insights.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation</title>
<link>https://arxiv.org/abs/2504.20923</link>
<guid>https://arxiv.org/abs/2504.20923</guid>
<content:encoded><![CDATA[
arXiv:2504.20923v1 Announce Type: cross 
Abstract: Audio deepfakes represent a growing threat to digital security and trust, leveraging advanced generative models to produce synthetic speech that closely mimics real human voices. Detecting such manipulations is especially challenging under open-world conditions, where spoofing methods encountered during testing may differ from those seen during training. In this work, we propose an end-to-end deep learning framework for audio deepfake detection that operates directly on raw waveforms. Our model, RawNetLite, is a lightweight convolutional-recurrent architecture designed to capture both spectral and temporal features without handcrafted preprocessing. To enhance robustness, we introduce a training strategy that combines data from multiple domains and adopts Focal Loss to emphasize difficult or ambiguous samples. We further demonstrate that incorporating codec-based manipulations and applying waveform-level audio augmentations (e.g., pitch shifting, noise, and time stretching) leads to significant generalization improvements under realistic acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging out-of-distribution test set (AVSpoof2021 + CodecFake). These findings highlight the importance of diverse training data, tailored objective functions and audio augmentations in building resilient and generalizable audio forgery detectors. Code and pretrained models are available at https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification</title>
<link>https://arxiv.org/abs/2504.20930</link>
<guid>https://arxiv.org/abs/2504.20930</guid>
<content:encoded><![CDATA[
arXiv:2504.20930v1 Announce Type: cross 
Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2303.01903</link>
<guid>https://arxiv.org/abs/2303.01903</guid>
<content:encoded><![CDATA[
arXiv:2303.01903v4 Announce Type: replace 
Abstract: Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the \emph{blind} LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. The two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. Prophet is general that can be instantiated with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones). Moreover, Prophet can also be integrated with modern large multimodal models in different stages, which is named Prophet++, to further improve the capabilities on knowledge-based VQA tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical solutions to the relative pose of three calibrated cameras</title>
<link>https://arxiv.org/abs/2303.16078</link>
<guid>https://arxiv.org/abs/2303.16078</guid>
<content:encoded><![CDATA[
arXiv:2303.16078v4 Announce Type: replace 
Abstract: We study the challenging problem of estimating the relative pose of three calibrated cameras from four point correspondences. We propose novel efficient solutions to this problem that are based on the simple idea of using four correspondences to estimate an approximate geometry of the first two views. We model this geometry either as an affine or a fully perspective geometry estimated using one additional approximate correspondence. We generate such an approximate correspondence using a very simple and efficient strategy, where the new point is the mean point of three corresponding input points. The new solvers are efficient and easy to implement, since they are based on existing efficient minimal solvers, i.e., the 4-point affine fundamental matrix, the well-known 5-point relative pose solver, and the P3P solver. Extensive experiments on real data show that the proposed solvers, when properly coupled with local optimization, achieve state-of-the-art results, with the novel solver based on approximate mean-point correspondences being more robust and accurate than the affine-based solver.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions</title>
<link>https://arxiv.org/abs/2306.07520</link>
<guid>https://arxiv.org/abs/2306.07520</guid>
<content:encoded><![CDATA[
arXiv:2306.07520v5 Announce Type: replace 
Abstract: Human intelligence can retrieve any person according to both visual and language descriptions. However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world. This paper strives to resolve this problem by proposing a new instruct-ReID task that requires the model to retrieve images according to the given image or language instructions. Our instruct-ReID is a more general ReID setting, where existing 6 ReID tasks can be viewed as special cases by designing different instructions. We propose a large-scale OmniReID benchmark and an adaptive triplet loss as a baseline method to facilitate research in this new setting. Experimental results show that the proposed multi-purpose ReID model, trained on our OmniReID benchmark without fine-tuning, can improve +0.5%, +0.6%, +7.7% mAP on Market1501, MSMT17, CUHK03 for traditional ReID, +6.4%, +7.1%, +11.2% mAP on PRCC, VC-Clothes, LTCC for clothes-changing ReID, +11.7% mAP on COCAS+ real2 for clothes template based clothes-changing ReID when using only RGB images, +24.9% mAP on COCAS+ real2 for our newly defined language-instructed ReID, +4.3% on LLCM for visible-infrared ReID, +2.6% on CUHK-PEDES for text-to-image ReID. The datasets, the model, and code will be available at https://github.com/hwz-zju/Instruct-ReID.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition</title>
<link>https://arxiv.org/abs/2310.18511</link>
<guid>https://arxiv.org/abs/2310.18511</guid>
<content:encoded><![CDATA[
arXiv:2310.18511v3 Announce Type: replace 
Abstract: In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the part-instance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized at CVPR2023, showcasing the winning method's utilization of a modified PointNet$^{++}$ model trained on 6D inputs, and exploring alternative techniques for GCR enhancement. We hope our work will help ease future research on compositional 3D Vision.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement</title>
<link>https://arxiv.org/abs/2403.16184</link>
<guid>https://arxiv.org/abs/2403.16184</guid>
<content:encoded><![CDATA[
arXiv:2403.16184v3 Announce Type: replace 
Abstract: Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between objects. This complexity and diversity in SGG leads to underrepresentation, where parts of triplet labels are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose integrating the pretrained Vision-language Models to enhance representation. However, due to the gap between pretraining and SGG, direct inference of pretrained VLMs on SGG leads to severe bias, which stems from the imbalanced predicates distribution in the pretraining language set. To alleviate the bias, we introduce a novel LM Estimation to approximate the unattainable predicates distribution. Finally, we ensemble the debiased VLMs with SGG models to enhance the representation, where we design a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our training-free method effectively addresses the predicates bias in pretrained VLMs, enhances SGG's representation, and significantly improve the performance.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruct-ReID++: Towards Universal Purpose Instruction-Guided Person Re-identification</title>
<link>https://arxiv.org/abs/2405.17790</link>
<guid>https://arxiv.org/abs/2405.17790</guid>
<content:encoded><![CDATA[
arXiv:2405.17790v2 Announce Type: replace 
Abstract: Human intelligence can retrieve any person according to both visual and language descriptions. However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world. This paper strives to resolve this problem by proposing a novel instruct-ReID task that requires the model to retrieve images according to the given image or language instructions. Instruct-ReID is the first exploration of a general ReID setting, where existing 6 ReID tasks can be viewed as special cases by assigning different instructions. To facilitate research in this new instruct-ReID task, we propose a large-scale OmniReID++ benchmark equipped with diverse data and comprehensive evaluation methods e.g., task specific and task-free evaluation settings. In the task-specific evaluation setting, gallery sets are categorized according to specific ReID tasks. We propose a novel baseline model, IRM, with an adaptive triplet loss to handle various retrieval tasks within a unified framework. For task-free evaluation setting, where target person images are retrieved from task-agnostic gallery sets, we further propose a new method called IRM++ with novel memory bank-assisted learning. Extensive evaluations of IRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our proposed methods, achieving state-of-the-art performance on 10 test sets. The datasets, the model, and the code will be available at https://github.com/hwz-zju/Instruct-ReID
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation</title>
<link>https://arxiv.org/abs/2408.13509</link>
<guid>https://arxiv.org/abs/2408.13509</guid>
<content:encoded><![CDATA[
arXiv:2408.13509v3 Announce Type: replace 
Abstract: The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. Moreover, the generated mask is usually not aligned with the generated anomaly. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of diversity, realism and the accuracy of mask. Overall, our approach significantly improves the performance of downstream anomaly inspection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many-Worlds Inverse Rendering</title>
<link>https://arxiv.org/abs/2408.16005</link>
<guid>https://arxiv.org/abs/2408.16005</guid>
<content:encoded><![CDATA[
arXiv:2408.16005v4 Announce Type: replace 
Abstract: Discontinuous visibility changes remain a major bottleneck when optimizing surfaces within a physically-based inverse renderer. Many previous works have proposed sophisticated algorithms and data structures to sample visibility silhouettes more efficiently.
  Our work presents another solution: instead of differentiating a tentative surface locally, we differentiate a volumetric perturbation of a surface. We refer this as a many-worlds representation because it models a non-interacting superposition of conflicting explanations (worlds) of the input dataset. Each world is optically isolated from others, leading to a new transport law that distinguishes our method from prior work based on exponential random media.
  The resulting Monte Carlo algorithm is simpler and more efficient than prior methods. We demonstrate that our method promotes rapid convergence, both in terms of the total iteration count and the cost per iteration.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generic Objects as Pose Probes for Few-shot View Synthesis</title>
<link>https://arxiv.org/abs/2408.16690</link>
<guid>https://arxiv.org/abs/2408.16690</guid>
<content:encoded><![CDATA[
arXiv:2408.16690v4 Announce Type: replace 
Abstract: Radiance fields including NeRFs and 3D Gaussians demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as inputs. COLMAP is frequently employed for preprocessing to estimate poses, while it necessitates a large number of feature matches to operate effectively, and it struggles with scenes characterized by sparse features, large baselines between images, or a limited number of input images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images. Traditional methods often use calibration boards but they are not common in images. We propose a novel idea of utilizing everyday objects, commonly found in both images and real life, as "pose probes". The probe object is automatically segmented by SAM, whose shape is initialized from a cube. We apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. Specifically, object poses of two views are first estimated by PnP matching in an SDF representation, which serves as initial poses. PnP matching, requiring only a few features, is suitable for feature-sparse scenes. Additional views are incrementally incorporated to refine poses from preceding views. In experiments, PoseProbe achieves state-of-the-art performance in both pose estimation and novel view synthesis across multiple datasets. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance. Our project page is available at: \href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this https URL}
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underwater Camouflaged Object Tracking Meets Vision-Language SAM2</title>
<link>https://arxiv.org/abs/2409.16902</link>
<guid>https://arxiv.org/abs/2409.16902</guid>
<content:encoded><![CDATA[
arXiv:2409.16902v4 Announce Type: replace 
Abstract: Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale datasets. However, these datasets have primarily focused on open-air scenarios and have largely overlooked underwater animal tracking-especially the complex challenges posed by camouflaged marine animals. To bridge this gap, we take a step forward by proposing the first large-scale multi-modal underwater camouflaged object tracking dataset, namely UW-COT220. Based on the proposed dataset, this work first comprehensively evaluates current advanced visual object tracking methods, including SAM- and SAM2-based trackers, in challenging underwater environments, \eg, coral reefs. Our findings highlight the improvements of SAM2 over SAM, demonstrating its enhanced ability to handle the complexities of underwater camouflaged objects. Furthermore, we propose a novel vision-language tracking framework called VL-SAM2, based on the video foundation model SAM2. Experimental results demonstrate that our VL-SAM2 achieves state-of-the-art performance on the UW-COT220 dataset. The dataset and codes are available at~\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\color{magenta}{here}}.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Role of Infrastructure in Collaborative Perception</title>
<link>https://arxiv.org/abs/2410.11259</link>
<guid>https://arxiv.org/abs/2410.11259</guid>
<content:encoded><![CDATA[
arXiv:2410.11259v2 Announce Type: replace 
Abstract: Collaborative Perception (CP) is a process in which an ego agent receives and fuses sensor information from surrounding vehicles and infrastructure to enhance its perception capability. To evaluate the need for infrastructure equipped with sensors, extensive and quantitative analysis of the role of infrastructure data in CP is crucial, yet remains underexplored. To address this gap, we first quantitatively assess the importance of infrastructure data in existing vehicle-centric CP, where the ego agent is a vehicle. Furthermore, we compare vehicle-centric CP with infra-centric CP, where the ego agent is now the infrastructure, to evaluate the effectiveness of each approach. Our results demonstrate that incorporating infrastructure data improves 3D detection accuracy by up to 10.30%, and infra-centric CP shows enhanced noise robustness and increases accuracy by up to 46.47% compared with vehicle-centric CP.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion</title>
<link>https://arxiv.org/abs/2410.11838</link>
<guid>https://arxiv.org/abs/2410.11838</guid>
<content:encoded><![CDATA[
arXiv:2410.11838v3 Announce Type: replace 
Abstract: Despite the recent progress, existing frame interpolation methods still struggle with processing extremely high resolution input and handling challenging cases such as repetitive textures, thin objects, and large motion. To address these issues, we introduce a patch-based cascaded pixel diffusion model for high resolution frame interpolation, HiFI, that excels in these scenarios while achieving competitive performance on standard benchmarks. Cascades, which generate a series of images from low to high resolution, can help significantly with large or complex motion that require both global context for a coarse solution and detailed context for high resolution output. However, contrary to prior work on cascaded diffusion models which perform diffusion on increasingly large resolutions, we use a single model that always performs diffusion at the same resolution and upsamples by processing patches of the inputs and the prior solution. At inference time, this drastically reduces memory usage and allows a single model, solving both frame interpolation (base model's task) and spatial up-sampling, saving training cost as well. HiFI excels at high-resolution images and complex repeated textures that require global context, achieving comparable or state-of-the-art performance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We further introduce a new dataset, LaMoR, that focuses on particularly challenging cases, and HiFI significantly outperforms other baselines. Please visit our project page for video results: https://hifi-diffusion.github.io
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose-Based Sign Language Appearance Transfer</title>
<link>https://arxiv.org/abs/2410.13675</link>
<guid>https://arxiv.org/abs/2410.13675</guid>
<content:encoded><![CDATA[
arXiv:2410.13675v2 Announce Type: replace 
Abstract: We introduce a method for transferring the signer's appearance in sign language skeletal poses while preserving the sign content. Using estimated poses, we transfer the appearance of one signer to another, maintaining natural movements and transitions. This approach improves pose-based rendering and sign stitching while obfuscating identity. Our experiments show that while the method reduces signer identification accuracy, it slightly harms sign recognition performance, highlighting a tradeoff between privacy and utility. Our code is available at https://github.com/sign-language-processing/pose-anonymization.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniVST: A Unified Framework for Training-free Localized Video Style Transfer</title>
<link>https://arxiv.org/abs/2410.20084</link>
<guid>https://arxiv.org/abs/2410.20084</guid>
<content:encoded><![CDATA[
arXiv:2410.20084v4 Announce Type: replace 
Abstract: This paper presents UniVST, a unified framework for localized video style transfer based on diffusion models. It operates without the need for training, offering a distinct advantage over existing diffusion methods that transfer style across entire videos. The endeavors of this paper comprise: (1) A point-matching mask propagation strategy that leverages the feature maps from the DDIM inversion. This streamlines the model's architecture by obviating the need for tracking models. (2) A training-free AdaIN-guided localized video stylization mechanism that operates at both the latent and attention levels. This balances content fidelity and style richness, mitigating the loss of localized details commonly associated with direct video stylization. (3) A sliding-window consistent smoothing scheme that harnesses optical flow within the pixel representation and refines predicted noise to update the latent space. This significantly enhances temporal consistency and diminishes artifacts in stylized video. Our proposed UniVST has been validated to be superior to existing methods in quantitative and qualitative metrics. It adeptly addresses the challenges of preserving the primary object's style while ensuring temporal consistency and detail preservation. Our code is available at https://github.com/QuanjianSong/UniVST.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence Aware Learning for Reliable Face Anti-spoofing</title>
<link>https://arxiv.org/abs/2411.01263</link>
<guid>https://arxiv.org/abs/2411.01263</guid>
<content:encoded><![CDATA[
arXiv:2411.01263v2 Announce Type: replace 
Abstract: Current Face Anti-spoofing (FAS) models tend to make overly confident predictions even when encountering unfamiliar scenarios or unknown presentation attacks, which leads to serious potential risks. To solve this problem, we propose a Confidence Aware Face Anti-spoofing (CA-FAS) model, which is aware of its capability boundary, thus achieving reliable liveness detection within this boundary. To enable the CA-FAS to "know what it doesn't know", we propose to estimate its confidence during the prediction of each sample. Specifically, we build Gaussian distributions for both the live faces and the known attacks. The prediction confidence for each sample is subsequently assessed using the Mahalanobis distance between the sample and the Gaussians for the "known data". We further introduce the Mahalanobis distance-based triplet mining to optimize the parameters of both the model and the constructed Gaussians as a whole. Extensive experiments show that the proposed CA-FAS can effectively recognize samples with low prediction confidence and thus achieve much more reliable performance than other FAS models by filtering out samples that are beyond its reliable range.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Global Floods with 10 Years of Satellite Radar Data</title>
<link>https://arxiv.org/abs/2411.01411</link>
<guid>https://arxiv.org/abs/2411.01411</guid>
<content:encoded><![CDATA[
arXiv:2411.01411v3 Announce Type: replace 
Abstract: Floods cause extensive global damage annually, making effective monitoring essential. While satellite observations have proven invaluable for flood detection and tracking, comprehensive global flood datasets spanning extended time periods remain scarce. In this study, we introduce a novel deep learning flood detection model that leverages the cloud-penetrating capabilities of Sentinel-1 Synthetic Aperture Radar (SAR) satellite imagery, enabling consistent flood extent mapping in through cloud cover and in both day and night conditions. By applying this model to 10 years of SAR data, we create a unique, longitudinal global flood extent dataset with predictions unaffected by cloud coverage, offering comprehensive and consistent insights into historically flood-prone areas over the past decade. We use our model predictions to identify historically flood-prone areas in Ethiopia and demonstrate real-time disaster response capabilities during the May 2024 floods in Kenya. Additionally, our longitudinal analysis reveals potential increasing trends in global flood extent over time, although further validation is required to explore links to climate change. To maximize impact, we provide public access to both our model predictions and a code repository, empowering researchers and practitioners worldwide to advance flood monitoring and enhance disaster response strategies.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Children with Autism Spectrum Disorder based on Script-Centric Behavior Understanding with Emotional Enhancement</title>
<link>https://arxiv.org/abs/2411.09413</link>
<guid>https://arxiv.org/abs/2411.09413</guid>
<content:encoded><![CDATA[
arXiv:2411.09413v2 Announce Type: replace 
Abstract: The early diagnosis of autism spectrum disorder (ASD) is critically dependent on systematic observation and analysis of children's social behaviors. While current methodologies predominantly utilize supervised learning approaches, their clinical adoption faces two principal limitations: insufficient ASD diagnostic samples and inadequate interpretability of the detection outcomes. This paper presents a novel zero-shot ASD detection framework based on script-centric behavioral understanding with emotional enhancement, which is designed to overcome the aforementioned clinical constraints. The proposed pipeline automatically converts audio-visual data into structured behavioral text scripts through computer vision techniques, subsequently capitalizing on the generalization capabilities of large language models (LLMs) for zero-shot/few-shot ASD detection. Three core technical contributions are introduced: (1) A multimodal script transcription module transforming behavioral cues into structured textual representations. (2) An emotion textualization module encoding emotional dynamics as the contextual features to augment behavioral understanding. (3) A domain-specific prompt engineering strategy enables the injection of clinical knowledge into LLMs. Our method achieves an F1-score of 95.24\% in diagnosing ASD in children with an average age of two years while generating interpretable detection rationales. This work opens up new avenues for leveraging the power of LLMs in analyzing and understanding ASD-related human behavior, thereby enhancing the accuracy of assisted autism diagnosis.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses</title>
<link>https://arxiv.org/abs/2411.10013</link>
<guid>https://arxiv.org/abs/2411.10013</guid>
<content:encoded><![CDATA[
arXiv:2411.10013v2 Announce Type: replace 
Abstract: Stereo depth estimation is a fundamental component in augmented reality (AR), which requires low latency for real-time processing. However, preprocessing such as rectification and non-ML computations such as cost volume require significant amount of latency exceeding that of an ML model itself, which hinders the real-time processing required by AR. Therefore, we develop alternative approaches to the rectification and cost volume that consider ML acceleration (GPU and NPUs) in recent hardware. For pre-processing, we eliminate it by introducing homography matrix prediction network with a rectification positional encoding (RPE), which delivers both low latency and robustness to unrectified images. For cost volume, we replace it with a group-pointwise convolution-based operator and approximation of cosine similarity based on layernorm and dot product. Based on our approaches, we develop MultiHeadDepth (replacing cost volume) and HomoDepth (MultiHeadDepth + removing pre-processing) models. MultiHeadDepth provides 11.8-30.3% improvements in accuracy and 22.9-25.2% reduction in latency compared to a state-of-the-art depth estimation model for AR glasses from industry. HomoDepth, which can directly process unrectified images, reduces the end-to-end latency by 44.5%. We also introduce a multi-task learning method to handle misaligned stereo inputs on HomoDepth, which reduces the AbsRel error by 10.0-24.3%. The overall results demonstrate the efficacy of our approaches, which not only reduce the inference latency but also improve the model performance. Our code is available at https://github.com/UCI-ISA-Lab/MultiHeadDepth-HomoDepth
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild</title>
<link>https://arxiv.org/abs/2411.14280</link>
<guid>https://arxiv.org/abs/2411.14280</guid>
<content:encoded><![CDATA[
arXiv:2411.14280v4 Announce Type: replace 
Abstract: Our work aims to reconstruct hand-object interactions from a single-view image, which is a fundamental but ill-posed task. Unlike methods that reconstruct from videos, multi-view images, or predefined 3D templates, single-view reconstruction faces significant challenges due to inherent ambiguities and occlusions. These challenges are further amplified by the diverse nature of hand poses and the vast variety of object shapes and sizes. Our key insight is that current foundational models for segmentation, inpainting, and 3D reconstruction robustly generalize to in-the-wild images, which could provide strong visual and geometric priors for reconstructing hand-object interactions. Specifically, given a single image, we first design a novel pipeline to estimate the underlying hand pose and object shape using off-the-shelf large models. Furthermore, with the initial reconstruction, we employ a prior-guided optimization scheme, which optimizes hand pose to comply with 3D physical constraints and the 2D input image content. We perform experiments across several datasets and show that our method consistently outperforms baselines and faithfully reconstructs a diverse set of hand-object interactions. Here is the link of our project page: https://lym29.github.io/EasyHOI-page/
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-IML: Towards Unified Image Manipulation Localization</title>
<link>https://arxiv.org/abs/2411.14823</link>
<guid>https://arxiv.org/abs/2411.14823</guid>
<content:encoded><![CDATA[
arXiv:2411.14823v2 Announce Type: replace 
Abstract: Existing Image Manipulation Localization (IML) methods mostly rely heavily on task-specific designs, making them perform well only on the target IML task, while joint training on multiple IML tasks causes significant performance degradation, hindering real applications.
  To this end, we propose Omni-IML, the first generalist model designed to unify IML across diverse tasks.
  Specifically, Omni-IML achieves generalization through three key components: (1) a Modal Gate Encoder, which adaptively selects the optimal encoding modality per sample, (2) a Dynamic Weight Decoder, which dynamically adjusts decoder filters to the task at hand, and (3) an Anomaly Enhancement module that leverages box supervision to highlight the tampered regions and facilitate the learning of task-agnostic features.
  Beyond localization, to support interpretation of the tampered images, we construct Omni-273k, a large high-quality dataset that includes natural language descriptions of tampered artifact. It is annotated through our automatic, chain-of-thoughts annotation technique.
  We also design a simple-yet-effective interpretation module to better utilize these descriptive annotations.
  Our extensive experiments show that our single Omni-IML model achieves state-of-the-art performance across all four major IML tasks, providing a valuable solution for practical deployment and a promising direction of generalist models in image forensics. Our code and dataset will be publicly available.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HANDI: Hand-Centric Text-and-Image Conditioned Video Generation</title>
<link>https://arxiv.org/abs/2412.04189</link>
<guid>https://arxiv.org/abs/2412.04189</guid>
<content:encoded><![CDATA[
arXiv:2412.04189v4 Announce Type: replace 
Abstract: Despite the recent strides in video generation, state-of-the-art methods still struggle with elements of visual detail. One particularly challenging case is the class of videos in which the intricate motion of the hand coupled with a mostly stable and otherwise distracting environment is necessary to convey the execution of some complex action and its effects. To address these challenges, we introduce a new method for video generation that focuses on hand-centric actions. Our diffusion-based method incorporates two distinct innovations. First, we propose an automatic method to generate the motion area -- the region in the video in which the detailed activities occur -- guided by both the visual context and the action text prompt, rather than assuming this region can be provided manually as is now commonplace. Second, we introduce a critical Hand Refinement Loss to guide the diffusion model to focus on smooth and consistent hand poses. We evaluate our method on challenging augmented datasets based on EpicKitchens and Ego4D, demonstrating significant improvements over state-of-the-art methods in terms of action clarity, especially of the hand motion in the target region, across diverse environments and actions. Video results can be found in https://zhicaoisexcited.github.io/project_page
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement</title>
<link>https://arxiv.org/abs/2412.18565</link>
<guid>https://arxiv.org/abs/2412.18565</guid>
<content:encoded><![CDATA[
arXiv:2412.18565v2 Announce Type: replace 
Abstract: Despite advances in neural rendering, due to the scarcity of high-quality 3D datasets and the inherent limitations of multi-view diffusion models, view synthesis and 3D model generation are restricted to low resolutions with suboptimal multi-view consistency. In this study, we present a novel 3D enhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent diffusion model to enhance coarse 3D inputs while preserving multi-view consistency. Our method includes a pose-aware encoder and a diffusion-based denoiser to refine low-quality multi-view images, along with data augmentation and a multi-view attention module with epipolar aggregation to maintain consistent, high-quality 3D outputs across views. Unlike existing video-based approaches, our model supports seamless multi-view enhancement with improved coherence across diverse viewing angles. Extensive evaluations show that 3DEnhancer significantly outperforms existing methods, boosting both multi-view enhancement and per-instance 3D optimization tasks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images</title>
<link>https://arxiv.org/abs/2501.09552</link>
<guid>https://arxiv.org/abs/2501.09552</guid>
<content:encoded><![CDATA[
arXiv:2501.09552v3 Announce Type: replace 
Abstract: De-identification of medical images is a critical step to ensure privacy during data sharing in research and clinical settings. The initial step in this process involves detecting Protected Health Information (PHI), which can be found in image metadata or imprinted within image pixels. Despite the importance of such systems, there has been limited evaluation of existing AI-based solutions, creating barriers to the development of reliable and robust tools. In this study, we present an AI-based pipeline for PHI detection, comprising three key components: text detection, text extraction, and text analysis. We benchmark three models, YOLOv11, EasyOCR, and GPT-4o, across different setups corresponding to these components, evaluating the performance based on precision, recall, F1 score, and accuracy. All setups demonstrate excellent PHI detection, with all metrics exceeding 0.9. The combination of YOLOv11 for text localization and GPT-4o for extraction and analysis yields the best results. However, this setup incurs higher costs due to GPT-4o's token generation. Conversely, an end-to-end pipeline that relies solely on GPT-4o shows lower performance but highlights the potential of multimodal models for complex tasks. We recommend fine-tuning a dedicated object detection model and utilizing built-in OCR tools to achieve optimal performance and cost-effectiveness. Additionally, leveraging language models such as GPT-4o can facilitate thorough and flexible analysis of text content.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models</title>
<link>https://arxiv.org/abs/2501.12433</link>
<guid>https://arxiv.org/abs/2501.12433</guid>
<content:encoded><![CDATA[
arXiv:2501.12433v2 Announce Type: replace 
Abstract: Animal stereotypes are deeply embedded in human culture and language. They often shape our perceptions and expectations of various species. Our study investigates how animal stereotypes manifest in vision-language models during the task of image generation. Through targeted prompts, we explore whether DALL-E perpetuates stereotypical representations of animals, such as "owls as wise," "foxes as unfaithful," etc. Our findings reveal significant stereotyped instances where the model consistently generates images aligned with cultural biases. The current work is the first of its kind to examine animal stereotyping in vision-language models systematically and to highlight a critical yet underexplored dimension of bias in AI-generated visual content.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Class-Agnostic Counting: Advancements from Reference-Based to Open-World Text-Guided Approaches</title>
<link>https://arxiv.org/abs/2501.19184</link>
<guid>https://arxiv.org/abs/2501.19184</guid>
<content:encoded><![CDATA[
arXiv:2501.19184v3 Announce Type: replace 
Abstract: Visual object counting has recently shifted towards class-agnostic counting (CAC), which addresses the challenge of counting objects across arbitrary categories -- a crucial capability for flexible and generalizable counting systems. Unlike humans, who effortlessly identify and count objects from diverse categories without prior knowledge, most existing counting methods are restricted to enumerating instances of known classes, requiring extensive labeled datasets for training and struggling in open-vocabulary settings. In contrast, CAC aims to count objects belonging to classes never seen during training, operating in a few-shot setting. In this paper, we present the first comprehensive review of CAC methodologies. We propose a taxonomy to categorize CAC approaches into three paradigms based on how target object classes can be specified: reference-based, reference-less, and open-world text-guided. Reference-based approaches achieve state-of-the-art performance by relying on exemplar-guided mechanisms. Reference-less methods eliminate exemplar dependency by leveraging inherent image patterns. Finally, open-world text-guided methods use vision-language models, enabling object class descriptions via textual prompts, offering a flexible and promising solution. Based on this taxonomy, we provide an overview of the architectures of 29 CAC approaches and report their results on gold-standard benchmarks. We compare their performance and discuss their strengths and limitations. Specifically, we present results on the FSC-147 dataset, setting a leaderboard using gold-standard metrics, and on the CARPK dataset to assess generalization capabilities. Finally, we offer a critical discussion of persistent challenges, such as annotation dependency and generalization, alongside future directions. We believe this survey will be a valuable resource, showcasing CAC advancements and guiding future research.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations</title>
<link>https://arxiv.org/abs/2502.03629</link>
<guid>https://arxiv.org/abs/2502.03629</guid>
<content:encoded><![CDATA[
arXiv:2502.03629v2 Announce Type: replace 
Abstract: Existing image editing models struggle to meet real-world demands. Despite excelling in academic benchmarks, they have yet to be widely adopted for real user needs. Datasets that power these models use artificial edits, lacking the scale and ecological validity necessary to address the true diversity of user requests. We introduce REALEDIT, a large-scale image editing dataset with authentic user requests and human-made edits sourced from Reddit. REALEDIT includes a test set of 9300 examples to evaluate models on real user requests. Our results show that existing models fall short on these tasks, highlighting the need for realistic training data. To address this, we introduce 48K training examples and train our REALEDIT model, achieving substantial gains - outperforming competitors by up to 165 Elo points in human judgment and 92 percent relative improvement on the automated VIEScore metric. We deploy our model on Reddit, testing it on new requests, and receive positive feedback. Beyond image editing, we explore REALEDIT's potential in detecting edited images by partnering with a deepfake detection non-profit. Finetuning their model on REALEDIT data improves its F1-score by 14 percentage points, underscoring the dataset's value for broad applications.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds</title>
<link>https://arxiv.org/abs/2502.05857</link>
<guid>https://arxiv.org/abs/2502.05857</guid>
<content:encoded><![CDATA[
arXiv:2502.05857v2 Announce Type: replace 
Abstract: This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, which prevents them from learning from each other. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions within a single transformer. EgoAgent introduces two innovations to learn from the causal and temporally intertwined nature of these abilities: (1) Interleaved sequential modeling of states and actions with the causal attention mechanism, and (2) A joint embedding-action-prediction architecture featuring temporal asymmetric predictor-observer branches. Integrating these designs based on JEPA, EgoAgent unifies these capabilities in a cohesive learning framework. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary</title>
<link>https://arxiv.org/abs/2503.00063</link>
<guid>https://arxiv.org/abs/2503.00063</guid>
<content:encoded><![CDATA[
arXiv:2503.00063v4 Announce Type: replace 
Abstract: Adversarial attacks exploit the vulnerability of deep models against adversarial samples. Existing point cloud attackers are tailored to specific models, iteratively optimizing perturbations based on gradients in either a white-box or black-box setting. Despite their promising attack performance, they often struggle to produce transferable adversarial samples due to overfitting the specific parameters of surrogate models. To overcome this issue, we shift our focus to the data distribution itself and introduce a novel approach named NoPain, which employs optimal transport (OT) to identify the inherent singular boundaries of the data manifold for cross-network point cloud attacks. Specifically, we first calculate the OT mapping from noise to the target feature space, then identify singular boundaries by locating non-differentiable positions. Finally, we sample along singular boundaries to generate adversarial point clouds. Once the singular boundaries are determined, NoPain can efficiently produce adversarial samples without the need of iterative updates or guidance from the surrogate classifiers. Extensive experiments demonstrate that the proposed end-to-end method outperforms baseline approaches in terms of both transferability and efficiency, while also maintaining notable advantages even against defense strategies. Code and model are available at https://github.com/cognaclee/nopain
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAA-SNN: Spatial-Temporal Attention Aggregator for Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2503.02689</link>
<guid>https://arxiv.org/abs/2503.02689</guid>
<content:encoded><![CDATA[
arXiv:2503.02689v3 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) have gained significant attention due to their biological plausibility and energy efficiency, making them promising alternatives to Artificial Neural Networks (ANNs). However, the performance gap between SNNs and ANNs remains a substantial challenge hindering the widespread adoption of SNNs. In this paper, we propose a Spatial-Temporal Attention Aggregator SNN (STAA-SNN) framework, which dynamically focuses on and captures both spatial and temporal dependencies. First, we introduce a spike-driven self-attention mechanism specifically designed for SNNs. Additionally, we pioneeringly incorporate position encoding to integrate latent temporal relationships into the incoming features. For spatial-temporal information aggregation, we employ step attention to selectively amplify relevant features at different steps. Finally, we implement a time-step random dropout strategy to avoid local optima. As a result, STAA-SNN effectively captures both spatial and temporal dependencies, enabling the model to analyze complex patterns and make accurate predictions. The framework demonstrates exceptional performance across diverse datasets and exhibits strong generalization capabilities. Notably, STAA-SNN achieves state-of-the-art results on neuromorphic datasets CIFAR10-DVS, with remarkable performances of 97.14%, 82.05% and 70.40% on the static datasets CIFAR-10, CIFAR-100 and ImageNet, respectively. Furthermore, our model exhibits improved performance ranging from 0.33\% to 2.80\% with fewer time steps. The code for the model is available on GitHub.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Separation with Entropy Regularization for Knowledge Distillation in Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2503.03144</link>
<guid>https://arxiv.org/abs/2503.03144</guid>
<content:encoded><![CDATA[
arXiv:2503.03144v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs), inspired by the human brain, offer significant computational efficiency through discrete spike-based information transfer. Despite their potential to reduce inference energy consumption, a performance gap persists between SNNs and Artificial Neural Networks (ANNs), primarily due to current training methods and inherent model limitations. While recent research has aimed to enhance SNN learning by employing knowledge distillation (KD) from ANN teacher networks, traditional distillation techniques often overlook the distinctive spatiotemporal properties of SNNs, thus failing to fully leverage their advantages. To overcome these challenge, we propose a novel logit distillation method characterized by temporal separation and entropy regularization. This approach improves existing SNN distillation techniques by performing distillation learning on logits across different time steps, rather than merely on aggregated output features. Furthermore, the integration of entropy regularization stabilizes model optimization and further boosts the performance. Extensive experimental results indicate that our method surpasses prior SNN distillation strategies, whether based on logit distillation, feature distillation, or a combination of both. The code will be available on GitHub.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation</title>
<link>https://arxiv.org/abs/2503.04606</link>
<guid>https://arxiv.org/abs/2503.04606</guid>
<content:encoded><![CDATA[
arXiv:2503.04606v3 Announce Type: replace 
Abstract: Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\sim$14,000$\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>4D mmWave Radar for Sensing Enhancement in Adverse Environments: Advances and Challenges</title>
<link>https://arxiv.org/abs/2503.24091</link>
<guid>https://arxiv.org/abs/2503.24091</guid>
<content:encoded><![CDATA[
arXiv:2503.24091v2 Announce Type: replace 
Abstract: Intelligent transportation systems require accurate and reliable sensing. However, adverse environments, such as rain, snow, and fog, can significantly degrade the performance of LiDAR and cameras. In contrast, 4D mmWave radar not only provides 3D point clouds and velocity measurements but also maintains robustness in challenging conditions. Recently, research on 4D mmWave radar under adverse environments has been growing, but a comprehensive review is still lacking. To bridge this gap, this work reviews the current research on 4D mmWave radar under adverse environments. First, we present an overview of existing 4D mmWave radar datasets encompassing diverse weather and lighting scenarios. Subsequently, we analyze existing learning-based methods leveraging 4D mmWave radar to enhance performance according to different adverse conditions. Finally, the challenges and potential future directions are discussed for advancing 4D mmWave radar applications in harsh environments. To the best of our knowledge, this is the first review specifically concentrating on 4D mmWave radar in adverse environments. The related studies are listed at: https://github.com/XiangyPeng/4D-mmWave-Radar-in-Adverse-Environments.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-stage deep learning framework for the restoration of incomplete-ring PET images</title>
<link>https://arxiv.org/abs/2504.00816</link>
<guid>https://arxiv.org/abs/2504.00816</guid>
<content:encoded><![CDATA[
arXiv:2504.00816v2 Announce Type: replace 
Abstract: Positron Emission Tomography (PET) is an important molecular imaging tool widely used in medicine. Traditional PET systems rely on complete detector rings for full angular coverage and reliable data collection. However, incomplete-ring PET scanners have emerged due to hardware failures, cost constraints, or specific clinical needs. Standard reconstruction algorithms often suffer from performance degradation with these systems because of reduced data completeness and geometric inconsistencies. We present a two-stage deep-learning framework that, without incorporating any time-of-flight (TOF) information, restores high-quality images from data with about 50% missing coincidences - double the loss levels previously addressed by CNN-based methods. The pipeline operates in two stages: a projection-domain Attention U-Net first predicts the missing sections of the sinogram by leveraging spatial context from neighbouring slices, after which the completed data are reconstructed with OSEM algorithm and passed to a U-Net-diffusion module that removes residual artefacts while reinstating high-frequency detail. Using 206 brain volumes from a public dataset, the result shows that our model successfully preserves most anatomical structures and tracer distribution features with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher inference speed, thus providing an effective solution for incomplete-ring PET imaging.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-Bench: Human-Aligned Video Generation Benchmark</title>
<link>https://arxiv.org/abs/2504.04907</link>
<guid>https://arxiv.org/abs/2504.04907</guid>
<content:encoded><![CDATA[
arXiv:2504.04907v2 Announce Type: replace 
Abstract: Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency. To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experiments on advanced models including Sora demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions. Moreover, in instances where our framework's assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing and Inducing Combinational Creativity in Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.13120</link>
<guid>https://arxiv.org/abs/2504.13120</guid>
<content:encoded><![CDATA[
arXiv:2504.13120v2 Announce Type: replace 
Abstract: The ability to combine existing concepts into novel ideas stands as a fundamental hallmark of human intelligence. Recent advances in Vision-Language Models (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their outputs reflect combinational creativity--defined by M. A. Boden (1998) as synthesizing novel ideas through combining existing concepts--or sophisticated pattern matching of training data. Drawing inspiration from cognitive science, we investigate the combinational creativity of VLMs from the lens of concept blending. We propose the Identification-Explanation-Implication (IEI) framework, which decomposes creative processes into three levels: identifying input spaces, extracting shared attributes, and deriving novel semantic implications. To validate this framework, we curate CreativeMashup, a high-quality dataset of 666 artist-generated visual mashups annotated according to the IEI framework. Through extensive experiments, we demonstrate that in comprehension tasks, best VLMs have surpassed average human performance while falling short of expert-level understanding; in generation tasks, incorporating our IEI framework into the generation pipeline significantly enhances the creative quality of VLMs' outputs. Our findings establish both a theoretical foundation for evaluating artificial creativity and practical guidelines for improving creative generation in VLMs.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception Encoder: The best visual embeddings are not at the output of the network</title>
<link>https://arxiv.org/abs/2504.13181</link>
<guid>https://arxiv.org/abs/2504.13181</guid>
<content:encoded><![CDATA[
arXiv:2504.13181v2 Announce Type: replace 
Abstract: We introduce Perception Encoder (PE), a state-of-the-art vision encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods: language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together, our PE family of models achieves best-in-class results on a wide variety of tasks, including (1) zero-shot image and video classification and retrieval, simultaneously obtaining 86.6 average zero-shot ImageNet robustness and 76.9 zero-shot Kinetics-400 video classification; (2) document, image, and video Q&amp;A, enabling 94.6 DocVQA, 80.9 InfographicVQA, and 82.7 PerceptionTest with an 8B LLM; and (3) spatial tasks such as detection, tracking, and depth estimation, setting a new COCO state-of-the-art of 66.0 box mAP. To foster further research, we release our models, code, and novel dataset of synthetically and human-annotated videos: https://github.com/facebookresearch/perception_models
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis</title>
<link>https://arxiv.org/abs/2504.13754</link>
<guid>https://arxiv.org/abs/2504.13754</guid>
<content:encoded><![CDATA[
arXiv:2504.13754v2 Announce Type: replace 
Abstract: Neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. Timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole slide images is critical for patient prognosis. However, current diagnostic practices primarily rely on subjective manual examination by pathologists, leading to inconsistent accuracy. Existing automated whole slide image classification methods encounter challenges such as poor interpretability, limited feature extraction capabilities, and high computational costs, restricting their practical clinical deployment. To overcome these limitations, we propose CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model tailored for pathological image classification, which enhances the Swin Transformer architecture by integrating a Kernel Activation Network within its multilayer perceptron and classification head modules, significantly improving both interpretability and accuracy. By fusing multi-scale features and leveraging contrastive learning strategies, CMSwinKAN mimics clinicians' comprehensive approach, effectively capturing global and local tissue characteristics. Additionally, we introduce a heuristic soft voting mechanism guided by clinical insights to seamlessly bridge patch-level predictions to whole slide image-level classifications. We validate CMSwinKAN on the PpNTs dataset, which was collaboratively established with our partner hospital and the publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN performs better than existing state-of-the-art pathology-specific models pre-trained on large datasets. Our source code is available at https://github.com/JSLiam94/CMSwinKAN.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction</title>
<link>https://arxiv.org/abs/2411.17982</link>
<guid>https://arxiv.org/abs/2411.17982</guid>
<content:encoded><![CDATA[
arXiv:2411.17982v2 Announce Type: replace-cross 
Abstract: We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast and accurate monocular scene reconstruction using only RGB input. Existing Neural SLAM or 3DGS-based SLAM methods often trade off between rendering quality and geometry accuracy, our research demonstrates that both can be achieved simultaneously with RGB input alone. The key idea of our approach is to enhance the ability for geometry estimation by combining easy-to-obtain monocular priors with learning-based dense SLAM, and then using 3D Gaussian splatting as our core map representation to efficiently model the scene. Upon loop closure, our method ensures on-the-fly global consistency through efficient pose graph bundle adjustment and instant map updates by explicitly deforming the 3D Gaussian units based on anchored keyframe updates. Furthermore, we introduce a grid-based scale alignment strategy to maintain improved scale consistency in prior depths for finer depth details. Through extensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate significant improvements over existing Neural SLAM methods and even surpass RGB-D-based methods in both reconstruction and rendering quality. The project page and source code will be made available at https://hi-slam2.github.io/.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets</title>
<link>https://arxiv.org/abs/2412.07775</link>
<guid>https://arxiv.org/abs/2412.07775</guid>
<content:encoded><![CDATA[
arXiv:2412.07775v4 Announce Type: replace-cross 
Abstract: While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training methods for reward finetuning of diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and/or slow convergence in finetuning. In response to this challenge, we take inspiration from recent successes in generative flow networks (GFlowNets) and propose a reinforcement learning method for diffusion model finetuning, dubbed Nabla-GFlowNet (abbreviated as $\nabla$-GFlowNet), that leverages the rich signal in reward gradients for probabilistic diffusion finetuning. We show that our proposed method achieves fast yet diversity- and prior-preserving finetuning of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Radiance Surfaces: Optimizing Surface Representations with a 5D Radiance Field Loss</title>
<link>https://arxiv.org/abs/2501.18627</link>
<guid>https://arxiv.org/abs/2501.18627</guid>
<content:encoded><![CDATA[
arXiv:2501.18627v2 Announce Type: replace-cross 
Abstract: We present a fast and simple technique to convert images into a radiance surface-based scene representation. Building on existing radiance volume reconstruction algorithms, we introduce a subtle yet impactful modification of the loss function requiring changes to only a few lines of code: instead of integrating the radiance field along rays and supervising the resulting images, we project the training images into the scene to directly supervise the spatio-directional radiance field.
  The primary outcome of this change is the complete removal of alpha blending and ray marching from the image formation model, instead moving these steps into the loss computation. In addition to promoting convergence to surfaces, this formulation assigns explicit semantic meaning to 2D subsets of the radiance field, turning them into well-defined radiance surfaces. We finally extract a level set from this representation, which results in a high-quality radiance surface model.
  Our method retains much of the speed and quality of the baseline algorithm. For instance, a suitably modified variant of Instant NGP maintains comparable computational efficiency, while achieving an average PSNR that is only 0.1 dB lower. Most importantly, our method generates explicit surfaces in place of an exponential volume, doing so with a level of simplicity not seen in prior work.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model Approach</title>
<link>https://arxiv.org/abs/2502.00114</link>
<guid>https://arxiv.org/abs/2502.00114</guid>
<content:encoded><![CDATA[
arXiv:2502.00114v2 Announce Type: replace-cross 
Abstract: Hand-drawn maps can be used to convey navigation instructions between humans and robots in a natural and efficient manner. However, these maps can often contain inaccuracies such as scale distortions and missing landmarks which present challenges for mobile robot navigation. This paper introduces a novel Hand-drawn Map Navigation (HAM-Nav) architecture that leverages pre-trained vision language models (VLMs) for robot navigation across diverse environments, hand-drawing styles, and robot embodiments, even in the presence of map inaccuracies. HAM-Nav integrates a unique Selective Visual Association Prompting approach for topological map-based position estimation and navigation planning as well as a Predictive Navigation Plan Parser to infer missing landmarks. Extensive experiments were conducted in photorealistic simulated environments, using both wheeled and legged robots, demonstrating the effectiveness of HAM-Nav in terms of navigation success rates and Success weighted by Path Length. Furthermore, a user study in real-world environments highlighted the practical utility of hand-drawn maps for robot navigation as well as successful navigation outcomes compared against a non-hand-drawn map approach.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D ReX: Causal Explanations in 3D Neuroimaging Classification</title>
<link>https://arxiv.org/abs/2502.12181</link>
<guid>https://arxiv.org/abs/2502.12181</guid>
<content:encoded><![CDATA[
arXiv:2502.12181v3 Announce Type: replace-cross 
Abstract: Explainability remains a significant problem for AI models in medical imaging, making it challenging for clinicians to trust AI-driven predictions. We introduce 3D ReX, the first causality-based post-hoc explainability tool for 3D models. 3D ReX uses the theory of actual causality to generate responsibility maps which highlight the regions most crucial to the model's decision. We test 3D ReX on a stroke detection model, providing insight into the spatial distribution of features relevant to stroke.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RGB-Thermal Infrared Fusion for Robust Depth Estimation in Complex Environments</title>
<link>https://arxiv.org/abs/2503.04821</link>
<guid>https://arxiv.org/abs/2503.04821</guid>
<content:encoded><![CDATA[
arXiv:2503.04821v2 Announce Type: replace-cross 
Abstract: Depth estimation in complex real-world scenarios is a challenging task, especially when relying solely on a single modality such as visible light or thermal infrared (THR) imagery. This paper proposes a novel multimodal depth estimation model, RTFusion, which enhances depth estimation accuracy and robustness by integrating the complementary strengths of RGB and THR data. The RGB modality provides rich texture and color information, while the THR modality captures thermal patterns, ensuring stability under adverse lighting conditions such as extreme illumination. The model incorporates a unique fusion mechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA) module for cross-modal feature alignment and the Edge Saliency Enhancement Module (ESEM) to improve edge detail preservation. Comprehensive experiments on the MS2 and ViViD++ datasets demonstrate that the proposed model consistently produces high-quality depth maps across various challenging environments, including nighttime, rainy, and high-glare conditions. The experimental results highlight the potential of the proposed method in applications requiring reliable depth estimation, such as autonomous driving, robotics, and augmented reality.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization</title>
<link>https://arxiv.org/abs/2503.06698</link>
<guid>https://arxiv.org/abs/2503.06698</guid>
<content:encoded><![CDATA[
arXiv:2503.06698v2 Announce Type: replace-cross 
Abstract: Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation</title>
<link>https://arxiv.org/abs/2503.15358</link>
<guid>https://arxiv.org/abs/2503.15358</guid>
<content:encoded><![CDATA[
arXiv:2503.15358v2 Announce Type: replace-cross 
Abstract: Idiomatic expressions present a unique challenge in NLP, as their meanings are often not directly inferable from their constituent words. Despite recent advancements in Large Language Models (LLMs), idiomaticity remains a significant obstacle to robust semantic representation. We present datasets and tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity Representation), which challenges the community to assess and improve models' ability to interpret idiomatic expressions in multimodal contexts and in multiple languages. Participants competed in two subtasks: ranking images based on their alignment with idiomatic or literal meanings, and predicting the next image in a sequence. The most effective methods achieved human-level performance by leveraging pretrained LLMs and vision-language models in mixture-of-experts settings, with multiple queries used to smooth over the weaknesses in these models' representations of idiomaticity.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOLAQUA: SINTEF Ocean Large Aquaculture Robotics Dataset</title>
<link>https://arxiv.org/abs/2504.01790</link>
<guid>https://arxiv.org/abs/2504.01790</guid>
<content:encoded><![CDATA[
arXiv:2504.01790v2 Announce Type: replace-cross 
Abstract: This paper presents a dataset gathered with an underwater robot in a sea-based aquaculture setting. Data was gathered from an operational fish farm and includes data from sensors such as the Waterlinked A50 DVL, the Nortek Nucleus 1000 DVL, Sonardyne Micro Ranger 2 USBL, Sonoptix Mulitbeam Sonar, mono and stereo cameras, and vehicle sensor data such as power usage, IMU, pressure, temperature, and more. Data acquisition is performed during both manual and autonomous traversal of the net pen structure. The collected vision data is of undamaged nets with some fish and marine growth presence, and it is expected that both the research community and the aquaculture industry will benefit greatly from the utilization of the proposed SOLAQUA dataset.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation</title>
<link>https://arxiv.org/abs/2504.07072</link>
<guid>https://arxiv.org/abs/2504.07072</guid>
<content:encoded><![CDATA[
arXiv:2504.07072v2 Announce Type: replace-cross 
Abstract: The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning</title>
<link>https://arxiv.org/abs/2504.10143</link>
<guid>https://arxiv.org/abs/2504.10143</guid>
<content:encoded><![CDATA[
arXiv:2504.10143v3 Announce Type: replace-cross 
Abstract: Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize misalignment by introducing two specific mechanisms: selection bias, where some semantic variables are missing, and perturbation bias, where semantic variables are distorted -- both affecting latent variables shared across modalities. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings through extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of misalignment on multimodal representation learning.
]]></content:encoded>
<pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>U-Shape Mamba: State Space Model for faster diffusion</title>
<link>https://arxiv.org/abs/2504.13499</link>
<guid>https://arxiv.org/abs/2504.13499</guid>
<content:encoded><![CDATA[
<div> diffusion model, image generation, U-Shape Mamba, computational cost, generative capabilities<br />
<br />
Summary: <br />
Diffusion models are widely used for high-quality image generation but face challenges due to their high computational cost. To address this issue, a new model called U-Shape Mamba (USM) is proposed. USM utilizes Mamba-based layers in a U-Net-like structure to reduce computational overhead by adjusting sequence length in the encoder and decoder through Mamba blocks. Compared to the efficient Zigma model, USM achieves a lower GFlops requirement, decreased memory usage, and faster performance while maintaining high image quality. It outperforms Zigma in terms of Frechet Inception Distance (FID) on various datasets like AFHQ, CelebAHQ, and COCO, showing an improvement in image quality. USM presents an efficient and scalable solution for diffusion-based generative models, making advanced image synthesis more accessible to researchers and reducing computational costs. <div>
arXiv:2504.13499v2 Announce Type: replace 
Abstract: Diffusion models have become the most popular approach for high-quality image generation, but their high computational cost still remains a significant challenge. To address this problem, we propose U-Shape Mamba (USM), a novel diffusion model that leverages Mamba-based layers within a U-Net-like hierarchical structure. By progressively reducing sequence length in the encoder and restoring it in the decoder through Mamba blocks, USM significantly lowers computational overhead while maintaining strong generative capabilities. Experimental results against Zigma, which is currently the most efficient Mamba-based diffusion model, demonstrate that USM achieves one-third the GFlops, requires less memory and is faster, while outperforming Zigma in image quality. Frechet Inception Distance (FID) is improved by 15.3, 0.84 and 2.7 points on AFHQ, CelebAHQ and COCO datasets, respectively. These findings highlight USM as a highly efficient and scalable solution for diffusion-based generative models, making high-quality image synthesis more accessible to the research community while reducing computational costs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compile Scene Graphs with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13617</link>
<guid>https://arxiv.org/abs/2504.13617</guid>
<content:encoded><![CDATA[
<div> Keywords: next token prediction, large language models (LLMs), reinforcement learning, multimodal LLM, scene graphs

Summary:<br /><br />Next token prediction is crucial for training large language models (LLMs), with reinforcement learning further enhancing reasoning performance. A novel approach is introduced, R1-SGG, a multimodal LLM trained via supervised fine-tuning (SFT) on the scene graph dataset, refined using reinforcement learning to generate scene graphs directly. The model uses a graph-centric reward function combining node-level rewards, edge-level rewards, and format consistency rewards to guide reinforcement learning. Experimental results show that rule-based reinforcement learning significantly improves model performance, achieving zero failure rate in scene graph generation, which supervised fine-tuning struggles with. This approach opens up new possibilities for end-to-end extraction of structured visual representations using multimodal LLMs. <div>
arXiv:2504.13617v2 Announce Type: replace 
Abstract: Next token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. Given the structured nature of scene graphs, we design a graph-centric reward function that integrates node-level rewards, edge-level rewards, and a format consistency reward. Our experiments demonstrate that rule-based RL substantially enhances model performance in the SGG task, achieving a zero failure rate--unlike supervised fine-tuning (SFT), which struggles to generalize effectively. Our code is available at https://github.com/gpt4vision/R1-SGG.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decade of You Only Look Once (YOLO) for Object Detection</title>
<link>https://arxiv.org/abs/2504.18586</link>
<guid>https://arxiv.org/abs/2504.18586</guid>
<content:encoded><![CDATA[
<div> YOLO, object detection, real-time, review, architecture<br />
<br />
Summary:<br />
The review examines the evolution and impact of the You Only Look Once (YOLO) framework in real-time object detection over the past decade. It discusses how YOLO has developed into a versatile family of architectures known for efficiency, scalability, and adaptability across domains. The paper provides a technical overview of different YOLO versions, highlighting key architectural trends and diverse application areas where YOLO is utilized. It also addresses evaluation methodologies, ethical considerations, and potential future directions for YOLO's advancement. The analysis offers a comprehensive perspective on YOLOs journey and ongoing improvements, underscoring its significance in the field of object detection. <div>
arXiv:2504.18586v1 Announce Type: new 
Abstract: This review marks the tenth anniversary of You Only Look Once (YOLO), one of the most influential frameworks in real-time object detection. Over the past decade, YOLO has evolved from a streamlined detector into a diverse family of architectures characterized by efficient design, modular scalability, and cross-domain adaptability. The paper presents a technical overview of the main versions, highlights key architectural trends, and surveys the principal application areas in which YOLO has been adopted. It also addresses evaluation practices, ethical considerations, and potential future directions for the framework's continued development. The analysis aims to provide a comprehensive and critical perspective on YOLO's trajectory and ongoing transformation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency</title>
<link>https://arxiv.org/abs/2504.18589</link>
<guid>https://arxiv.org/abs/2504.18589</guid>
<content:encoded><![CDATA[
<div> LVLM, Vision-Language Models, VCBENCH, multimodal mathematical reasoning, visual dependencies <br />
Summary:
The article introduces VCBENCH, a benchmark for multimodal mathematical reasoning with explicit visual dependencies. It includes 1,720 problems across six cognitive domains, requiring models to reason across multiple images and incorporate commonsense knowledge. 26 state-of-the-art LVLMs were evaluated on VCBENCH, with none exceeding 50% accuracy, revealing challenges in visual-mathematical integration. The benchmark highlights the importance of discerning, integrating, and reasoning across multiple images for fundamental mathematical reasoning in LVLMs. The study suggests avenues for future advancements in LVLMs and emphasizes the need to improve the models' ability to reason about elementary-level math problems with explicit visual dependencies. <br /><br />Summary: <div>
arXiv:2504.18589v1 Announce Type: new 
Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-Training with Active Contrastive Learning and Meta-Pseudo-Labeling on 2D Projections for Deep Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2504.18666</link>
<guid>https://arxiv.org/abs/2504.18666</guid>
<content:encoded><![CDATA[
<div> active-DeepFA, image classification, semi-supervised learning, active learning, deep learning<br />
Summary:<br />
active-DeepFA is a new method that combines active learning, semi-supervised learning, and deep learning to train CNN architectures for image classification with limited labeled data. It utilizes a co-training setup with two networks to reduce confirmation bias from pseudo-labels. The method starts with supervised contrastive learning and incorporates label propagation on 2D projections of deep features. Reliable pseudo-labels are exchanged between networks, and informative samples are added to the labeled set. The networks minimize an objective loss function that includes supervised and semi-supervised components to improve image classification representations. active-DeepFA outperforms six state-of-the-art methods on three biological image datasets using only 5% of labeled samples and reduces annotation effort by achieving comparable results with just 3% of labeled data. <div>
arXiv:2504.18666v1 Announce Type: new 
Abstract: A major challenge that prevents the training of DL models is the limited availability of accurately labeled data. This shortcoming is highlighted in areas where data annotation becomes a time-consuming and error-prone task. In this regard, SSL tackles this challenge by capitalizing on scarce labeled and abundant unlabeled data; however, SoTA methods typically depend on pre-trained features and large validation sets to learn effective representations for classification tasks. In addition, the reduced set of labeled data is often randomly sampled, neglecting the selection of more informative samples. Here, we present active-DeepFA, a method that effectively combines CL, teacher-student-based meta-pseudo-labeling and AL to train non-pretrained CNN architectures for image classification in scenarios of scarcity of labeled and abundance of unlabeled data. It integrates DeepFA into a co-training setup that implements two cooperative networks to mitigate confirmation bias from pseudo-labels. The method starts with a reduced set of labeled samples by warming up the networks with supervised CL. Afterward and at regular epoch intervals, label propagation is performed on the 2D projections of the networks' deep features. Next, the most reliable pseudo-labels are exchanged between networks in a cross-training fashion, while the most meaningful samples are annotated and added into the labeled set. The networks independently minimize an objective loss function comprising supervised contrastive, supervised and semi-supervised loss components, enhancing the representations towards image classification. Our approach is evaluated on three challenging biological image datasets using only 5% of labeled samples, improving baselines and outperforming six other SoTA methods. In addition, it reduces annotation effort by achieving comparable results to those of its counterparts with only 3% of labeled data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models</title>
<link>https://arxiv.org/abs/2504.18684</link>
<guid>https://arxiv.org/abs/2504.18684</guid>
<content:encoded><![CDATA[
<div> Keywords: object-referential language, 3D grounding, spatial reasoning, large language models, zero-shot generalization 

Summary: 
SORT3D is a novel approach designed to interpret object-referential language and ground objects in 3D environments. By leveraging rich object attributes from 2D data and combining heuristic-based spatial reasoning with large language models (LLMs), SORT3D achieves state-of-the-art performance on complex grounding tasks in different environments without the need for text-to-3D training data. The method enables zero-shot generalization to unseen environments, making it highly versatile. Additionally, SORT3D has been successfully implemented in real-time on an autonomous vehicle, demonstrating its practical applicability for object-goal navigation in real-world settings. Through the publicly released source code, researchers and developers can access and utilize the SORT3D pipeline for various applications in robotics and artificial intelligence. 

Summary: <div>
arXiv:2504.18684v1 Announce Type: new 
Abstract: Interpreting object-referential language and grounding objects in 3D with spatial relations and attributes is essential for robots operating alongside humans. However, this task is often challenging due to the diversity of scenes, large number of fine-grained objects, and complex free-form nature of language references. Furthermore, in the 3D domain, obtaining large amounts of natural language training data is difficult. Thus, it is important for methods to learn from little data and zero-shot generalize to new environments. To address these challenges, we propose SORT3D, an approach that utilizes rich object attributes from 2D data and merges a heuristics-based spatial reasoning toolbox with the ability of large language models (LLMs) to perform sequential reasoning. Importantly, our method does not require text-to-3D data for training and can be applied zero-shot to unseen environments. We show that SORT3D achieves state-of-the-art performance on complex view-dependent grounding tasks on two benchmarks. We also implement the pipeline to run real-time on an autonomous vehicle and demonstrate that our approach can be used for object-goal navigation on previously unseen real-world environments. All source code for the system pipeline is publicly released at https://github.com/nzantout/SORT3D .
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HierSum: A Global and Local Attention Mechanism for Video Summarization</title>
<link>https://arxiv.org/abs/2504.18689</link>
<guid>https://arxiv.org/abs/2504.18689</guid>
<content:encoded><![CDATA[
<div> HierSum, video summarization, instructional videos, segment identification, most replayed statistic <br />
Summary: <br />
HierSum is a hierarchical approach for summarizing instructional videos by breaking them down into meaningful segments corresponding to essential steps. It integrates local cues from subtitles with global contextual information from video-level instructions and uses the "most replayed" statistic to identify critical segments. HierSum outperforms existing methods on benchmark datasets like TVSum, BLiSS, Mr.HiSum, and the WikiHow test set in metrics like F1-score and rank correlation. A new multi-modal dataset is curated using WikiHow and EHow videos and associated articles, enhancing summarization on target datasets. Extensive ablation studies demonstrate that training on this dataset significantly improves summarization performance. <br /> <div>
arXiv:2504.18689v1 Announce Type: new 
Abstract: Video summarization creates an abridged version (i.e., a summary) that provides a quick overview of the video while retaining pertinent information. In this work, we focus on summarizing instructional videos and propose a method for breaking down a video into meaningful segments, each corresponding to essential steps in the video. We propose \textbf{HierSum}, a hierarchical approach that integrates fine-grained local cues from subtitles with global contextual information provided by video-level instructions. Our approach utilizes the ``most replayed" statistic as a supervisory signal to identify critical segments, thereby improving the effectiveness of the summary. We evaluate on benchmark datasets such as TVSum, BLiSS, Mr.HiSum, and the WikiHow test set, and show that HierSum consistently outperforms existing methods in key metrics such as F1-score and rank correlation. We also curate a new multi-modal dataset using WikiHow and EHow videos and associated articles containing step-by-step instructions. Through extensive ablation studies, we demonstrate that training on this dataset significantly enhances summarization on the target datasets.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of 3D Object Detection with Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.18738</link>
<guid>https://arxiv.org/abs/2504.18738</guid>
<content:encoded><![CDATA[
<div> Object Detection, Vision-Language Models, Agents, VLMs, LLMs

Summary:
This review analyzes the field of 3D object detection with vision-language models (VLMs) by examining over 100 research papers. It highlights the unique challenges in 3D object detection with VLMs, emphasizing differences from 2D detection. Traditional approaches using point clouds and voxel grids are compared to modern vision-language frameworks like CLIP and 3D LLMs. Key architectures, pretraining strategies, and prompt engineering methods align textual and 3D features for effective detection. Visualization examples and benchmarks illustrate performance. Current challenges include limited 3D-language datasets and computational demands. Future research directions are proposed to advance 3D object detection with VLMs. <div>
arXiv:2504.18738v1 Announce Type: new 
Abstract: This review provides a systematic analysis of comprehensive survey of 3D object detection with vision-language models(VLMs) , a rapidly advancing area at the intersection of 3D vision and multimodal AI. By examining over 100 research papers, we provide the first systematic analysis dedicated to 3D object detection with vision-language models. We begin by outlining the unique challenges of 3D object detection with vision-language models, emphasizing differences from 2D detection in spatial reasoning and data complexity. Traditional approaches using point clouds and voxel grids are compared to modern vision-language frameworks like CLIP and 3D LLMs, which enable open-vocabulary detection and zero-shot generalization. We review key architectures, pretraining strategies, and prompt engineering methods that align textual and 3D features for effective 3D object detection with vision-language models. Visualization examples and evaluation benchmarks are discussed to illustrate performance and behavior. Finally, we highlight current challenges, such as limited 3D-language datasets and computational demands, and propose future research directions to advance 3D object detection with vision-language models. >Object Detection, Vision-Language Models, Agents, VLMs, LLMs, AI
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dream-Box: Object-wise Outlier Generation for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2504.18746</link>
<guid>https://arxiv.org/abs/2504.18746</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep neural networks, out-of-distribution detection, outlier generation, diffusion models, object detection

Summary:
Deep neural networks have excelled in tasks within the same distribution but struggle with out-of-distribution (OOD) detection. Traditional OOD methods have been surpassed by methods utilizing synthetic outliers for training an outlier detector. While feature space driven methods perform well, they lack visualization of training outliers. Pixel space outlier generation techniques using diffusion models have improved OOD detection, yet adapting them to object detection remains unexplored. This study introduces Dream-Box, bridging object-wise outlier generation in pixel space for OOD detection. By employing diffusion models to generate object-wise outliers, the method trains an object detector for in-distribution tasks and OOD detection. Dream-Box showcases similar performance to previous methods while providing concrete visualization of generated OOD objects, a first in the field. <br /><br />Summary: Deep neural networks excel in same-distribution tasks but struggle with OOD detection. Synthetic outlier-based methods have surpassed traditional ones, yet feature and pixel space techniques each have their limitations. Dream-Box introduces object-wise outlier generation in pixel space, using diffusion models for improved OOD detection and visualization in object detection tasks. <div>
arXiv:2504.18746v1 Announce Type: new 
Abstract: Deep neural networks have demonstrated great generalization capabilities for tasks whose training and test sets are drawn from the same distribution. Nevertheless, out-of-distribution (OOD) detection remains a challenging task that has received significant attention in recent years. Specifically, OOD detection refers to the detection of instances that do not belong to the training distribution, while still having good performance on the in-distribution task (e.g., classification or object detection). Recent work has focused on generating synthetic outliers and using them to train an outlier detector, generally achieving improved OOD detection than traditional OOD methods. In this regard, outliers can be generated either in feature or pixel space. Feature space driven methods have shown strong performance on both the classification and object detection tasks, at the expense that the visualization of training outliers remains unknown, making further analysis on OOD failure modes challenging. On the other hand, pixel space outlier generation techniques enabled by diffusion models have been used for image classification using, providing improved OOD detection performance and outlier visualization, although their adaption to the object detection task is as yet unexplored. We therefore introduce Dream-Box, a method that provides a link to object-wise outlier generation in the pixel space for OOD detection. Specifically, we use diffusion models to generate object-wise outliers that are used to train an object detector for an in-distribution task and OOD detection. Our method achieves comparable performance to previous traditional methods while being the first technique to provide concrete visualization of generated OOD objects.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stage Boundary-Aware Transformer Network for Action Segmentation in Untrimmed Surgical Videos</title>
<link>https://arxiv.org/abs/2504.18756</link>
<guid>https://arxiv.org/abs/2504.18756</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical workflows, action segmentation, Transformer Network, boundary detection, F1 scores

Summary:
The study focuses on understanding actions in surgical workflows to evaluate post-operative outcomes. Capturing long sequences of actions in surgical settings is challenging due to the variability in individual surgeons' approaches. The Multi-Stage Boundary-Aware Transformer Network (MSBATN) with hierarchical sliding window attention is introduced to improve action segmentation by addressing issues of over-segmentation and under-segmentation. A novel unified loss function is utilized to treat action classification and boundary detection as interdependent tasks, enhancing accuracy. The boundary voting mechanism accurately identifies start and end points by leveraging contextual information, improving boundary detection. Extensive experiments on three surgical datasets demonstrate the proposed method's superior performance, achieving state-of-the-art results in F1 scores at different thresholds. The MSBATN approach offers a promising solution for precise action segmentation in surgical videos. 

<br /><br />Summary: <div>
arXiv:2504.18756v1 Announce Type: new 
Abstract: Understanding actions within surgical workflows is essential for evaluating post-operative outcomes. However, capturing long sequences of actions performed in surgical settings poses challenges, as individual surgeons have their unique approaches shaped by their expertise, leading to significant variability. To tackle this complex problem, we focused on segmentation with precise boundaries, a demanding task due to the inherent variability in action durations and the subtle transitions often observed in untrimmed videos. These transitions, marked by ambiguous starting and ending points, complicate the segmentation process. Traditional models, such as MS-TCN, which depend on large receptive fields, frequently face challenges of over-segmentation (resulting in fragmented segments) or under-segmentation (merging distinct actions). Both of these issues negatively impact the quality of segmentation. To overcome these challenges, we present the Multi-Stage Boundary-Aware Transformer Network (MSBATN) with hierarchical sliding window attention, designed to enhance action segmentation. Our proposed approach incorporates a novel unified loss function that treats action classification and boundary detection as distinct yet interdependent tasks. Unlike traditional binary boundary detection methods, our boundary voting mechanism accurately identifies start and end points by leveraging contextual information. Extensive experiments using three challenging surgical datasets demonstrate the superior performance of the proposed method, achieving state-of-the-art results in F1 scores at thresholds of 25% and 50%, while also delivering comparable performance in other metrics.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data</title>
<link>https://arxiv.org/abs/2504.18770</link>
<guid>https://arxiv.org/abs/2504.18770</guid>
<content:encoded><![CDATA[
<div> Keywords: PyViT-FUSE, earth observation data, multi-modal imagery, attention mechanism, vision transformers

Summary: 
PyViT-FUSE is a new foundation model designed for handling multi-modal earth observation data. It utilizes an attention mechanism to fuse mixed-resolution input bands into a single representation. The model consists of a novel pyramidal structure of vision transformers that process learned patch tokens. It is trained on a globally sampled dataset in a self-supervised manner, drawing on concepts from the SwAV algorithm. The fusion mechanism's interpretability is demonstrated through visualization of attention scores. The model's effectiveness for downstream tasks is also highlighted in the study. Overall, PyViT-FUSE offers a versatile approach to integrating different types of imagery data and shows promise for various applications in the field of earth observation. 

<br /><br />Summary: <div>
arXiv:2504.18770v1 Announce Type: new 
Abstract: We propose PyViT-FUSE, a foundation model for earth observation data explicitly designed to handle multi-modal imagery by learning to fuse an arbitrary number of mixed-resolution input bands into a single representation through an attention mechanism. The learned patch tokens are further processed by a stack of vision transformers with a novel pyramidal structure. We train the model on a globally sampled dataset in a self-supervised manner, leveraging core concepts of the SwAV algorithm. We show the interpretability of the fusion mechanism by visualization of the attention scores and the models applicability to downstream tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth as Points: Center Point-based Depth Estimation</title>
<link>https://arxiv.org/abs/2504.18773</link>
<guid>https://arxiv.org/abs/2504.18773</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, virtual datasets, depth estimation, CenterDepth, computational efficiency <br />
Summary:<br />
The article introduces a novel approach for generating virtual datasets efficiently for autonomous driving perception tasks. The method allows for the rapid creation of task-specific datasets, leading to the development of the VirDepth dataset, designed for depth estimation in urban scenarios. The proposed CenterDepth architecture is a lightweight model for monocular depth estimation, emphasizing operational efficiency and accuracy. It incorporates global semantic information using the Center FC-CRFs algorithm, integrates multi-scale features based on object key points, and enables detection-based depth estimation. Experimental results demonstrate that CenterDepth outperforms existing methods in terms of computational speed and prediction accuracy, making it a valuable tool for autonomous driving applications. <div>
arXiv:2504.18773v1 Announce Type: new 
Abstract: The perception of vehicles and pedestrians in urban scenarios is crucial for autonomous driving. This process typically involves complicated data collection, imposes high computational and hardware demands. To address these limitations, we first develop a highly efficient method for generating virtual datasets, which enables the creation of task- and scenario-specific datasets in a short time. Leveraging this method, we construct the virtual depth estimation dataset VirDepth, a large-scale, multi-task autonomous driving dataset. Subsequently, we propose CenterDepth, a lightweight architecture for monocular depth estimation that ensures high operational efficiency and exhibits superior performance in depth estimation tasks with highly imbalanced height-scale distributions. CenterDepth integrates global semantic information through the innovative Center FC-CRFs algorithm, aggregates multi-scale features based on object key points, and enables detection-based depth estimation of targets. Experiments demonstrate that our proposed method achieves superior performance in terms of both computational speed and prediction accuracy.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IoT Botnet Detection: Application of Vision Transformer to Classification of Network Flow Traffic</title>
<link>https://arxiv.org/abs/2504.18781</link>
<guid>https://arxiv.org/abs/2504.18781</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer models, IoT network flow packets, ViT, botnet attack detection, LSTM

Summary:
This work introduces a novel preprocessing method to adapt transformer models, specifically the vision transformer (ViT), for IoT botnet attack detection using network flow packets. The approach involves feature extraction from .pcap files and transforming each instance into a 1-channel 2D image shape to enable ViT-based classification. Additionally, enhancements were made to the ViT model to allow for the use of classifiers beyond the Multilayer Perceptron (MLP) originally used. The study evaluated various models including Deep Neural Network (DNN), LSTM, and Bidirectional-LSTM (BLSTM) for multiclass-based attack detection on two IoT attack datasets. The models demonstrated competitive performance in terms of precision, recall, and F1-score, showcasing the potential of transformer models for enhancing IoT security. 

<br /><br />Summary: <div>
arXiv:2504.18781v1 Announce Type: new 
Abstract: Despite the demonstrated effectiveness of transformer models in NLP, and image and video classification, the available tools for extracting features from captured IoT network flow packets fail to capture sequential patterns in addition to the absence of spatial patterns consequently limiting transformer model application. This work introduces a novel preprocessing method to adapt transformer models, the vision transformer (ViT) in particular, for IoT botnet attack detection using network flow packets. The approach involves feature extraction from .pcap files and transforming each instance into a 1-channel 2D image shape, enabling ViT-based classification. Also, the ViT model was enhanced to allow use any classifier besides Multilayer Perceptron (MLP) that was deployed in the initial ViT paper. Models including the conventional feed forward Deep Neural Network (DNN), LSTM and Bidirectional-LSTM (BLSTM) demonstrated competitive performance in terms of precision, recall, and F1-score for multiclass-based attack detection when evaluated on two IoT attack datasets.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMeL: Cross-modality Adaptive Meta-Learning for Text-based Person Retrieval</title>
<link>https://arxiv.org/abs/2504.18782</link>
<guid>https://arxiv.org/abs/2504.18782</guid>
<content:encoded><![CDATA[
<div> Keywords: text-based person retrieval, domain-agnostic pretraining, Cross-modality Adaptive Meta-Learning, multi-task adaptation, error sample memory unit

Summary:
<br />
Text-based person retrieval aims to identify individuals in image databases based on textual descriptions. To address the bias in synthesized data used for pretraining, a domain-agnostic pretraining framework called Cross-modality Adaptive Meta-Learning (CAMeL) is introduced. CAMeL enhances model generalization by incorporating diverse tasks reflective of real-world scenarios and includes a dynamic error sample memory unit to learn from past mistakes. An adaptive dual-speed update strategy ensures efficient adaptation to new tasks while maintaining stability on historical tasks. This approach outperforms existing methods on benchmarks like CUHK-PEDES and showcases robustness in handling biased synthetic images and noisy text annotations. The code for this framework is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2504.18782v1 Announce Type: new 
Abstract: Text-based person retrieval aims to identify specific individuals within an image database using textual descriptions. Due to the high cost of annotation and privacy protection, researchers resort to synthesized data for the paradigm of pretraining and fine-tuning. However, these generated data often exhibit domain biases in both images and textual annotations, which largely compromise the scalability of the pre-trained model. Therefore, we introduce a domain-agnostic pretraining framework based on Cross-modality Adaptive Meta-Learning (CAMeL) to enhance the model generalization capability during pretraining to facilitate the subsequent downstream tasks. In particular, we develop a series of tasks that reflect the diversity and complexity of real-world scenarios, and introduce a dynamic error sample memory unit to memorize the history for errors encountered within multiple tasks. To further ensure multi-task adaptation, we also adopt an adaptive dual-speed update strategy, balancing fast adaptation to new tasks and slow weight updates for historical tasks. Albeit simple, our proposed model not only surpasses existing state-of-the-art methods on real-world benchmarks, including CUHK-PEDES, ICFG-PEDES, and RSTPReid, but also showcases robustness and scalability in handling biased synthetic images and noisy text annotations. Our code is available at https://github.com/Jahawn-Wen/CAMeL-reID.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video CLIP Model for Multi-View Echocardiography Interpretation</title>
<link>https://arxiv.org/abs/2504.18800</link>
<guid>https://arxiv.org/abs/2504.18800</guid>
<content:encoded><![CDATA[
<div> Video-language model, echocardiography, ultrasound, interpretation, multiple views
Summary:
- Echocardiography involves recording videos of the heart using ultrasound for evaluation.
- Large-scale vision-language models aim to automate interpretation of echocardiographic videos.
- Existing models rely on single-frame inputs, leading to lower accuracy in diagnosing conditions identifiable through cardiac motion.
- Incorporating multiple views in the model can improve accuracy in interpreting specific conditions.
- A video-language model trained on multiple views and full video sequences achieved higher interpretation accuracy than models trained with single-view videos or still images.<br /><br />Summary: Echocardiography entails using ultrasound to record heart videos for evaluation. Vision-language models automate interpretation, but single-frame models have lower accuracy. Multiple views improve accuracy for specific conditions. A video-language model trained on multiple views and full sequences outperformed single-view or image-trained models. <div>
arXiv:2504.18800v1 Announce Type: new 
Abstract: Echocardiography involves recording videos of the heart using ultrasound, enabling clinicians to evaluate its condition. Recent advances in large-scale vision-language models (VLMs) have garnered attention for automating the interpretation of echocardiographic videos. However, most existing VLMs proposed for medical interpretation thus far rely on single-frame (i.e., image) inputs. Consequently, these image-based models often exhibit lower diagnostic accuracy for conditions identifiable through cardiac motion. Moreover, echocardiographic videos are recorded from various views that depend on the direction of ultrasound emission, and certain views are more suitable than others for interpreting specific conditions. Incorporating multiple views could potentially yield further improvements in accuracy. In this study, we developed a video-language model that takes five different views and full video sequences as input, training it on pairs of echocardiographic videos and clinical reports from 60,747 cases. Our experiments demonstrate that this expanded approach achieves higher interpretation accuracy than models trained with only single-view videos or with still images.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning</title>
<link>https://arxiv.org/abs/2504.18810</link>
<guid>https://arxiv.org/abs/2504.18810</guid>
<content:encoded><![CDATA[
<div> audio-lip synchronization, visual quality, uncertainty learning, talking face video generation, Joint Uncertainty Learning Network (JULNet) <br />
Summary:
- The article discusses the challenge of generating talking face videos with arbitrary speech audio, emphasizing the importance of audio-lip synchronization and visual quality.
- Existing systems lack consideration for visual uncertainty, leading to inconsistent quality and performance under different input conditions.
- A Joint Uncertainty Learning Network (JULNet) is proposed to address this issue, incorporating uncertainty representation related to visual error.
- An uncertainty module predicts error and uncertainty maps after image generation, enhancing model performance and robustness through joint optimization.
- Extensive experiments show that JULNet achieves superior high-fidelity and audio-lip synchronization in talking face video generation compared to previous methods. <br /> 
Summary: <div>
arXiv:2504.18810v1 Announce Type: new 
Abstract: Talking face video generation with arbitrary speech audio is a significant challenge within the realm of digital human technology. The previous studies have emphasized the significance of audio-lip synchronization and visual quality. Currently, limited attention has been given to the learning of visual uncertainty, which creates several issues in existing systems, including inconsistent visual quality and unreliable performance across different input conditions. To address the problem, we propose a Joint Uncertainty Learning Network (JULNet) for high-quality talking face video generation, which incorporates a representation of uncertainty that is directly related to visual error. Specifically, we first design an uncertainty module to individually predict the error map and uncertainty map after obtaining the generated image. The error map represents the difference between the generated image and the ground truth image, while the uncertainty map is used to predict the probability of incorrect estimates. Furthermore, to match the uncertainty distribution with the error distribution through a KL divergence term, we introduce a histogram technique to approximate the distributions. By jointly optimizing error and uncertainty, the performance and robustness of our model can be enhanced. Extensive experiments demonstrate that our method achieves superior high-fidelity and audio-lip synchronization in talking face video generation compared to previous methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation</title>
<link>https://arxiv.org/abs/2504.18856</link>
<guid>https://arxiv.org/abs/2504.18856</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational Pathology, Vision-Language Models, Multi-resolution, Whole Slide Images, Histology

Summary:<br /><br />In this study, a novel approach is proposed in Computational Pathology (CPath) using Vision-Language Models (VLMs) for multi-resolution analysis of histology images. By leveraging Whole Slide Images (WSIs) to extract patches at various resolutions and generating textual descriptions, a more detailed representation of the images is obtained. The model introduces visual-textual alignment at multiple resolutions and cross-resolution alignment to enhance the effectiveness of text-guided visual representations. Through pre-training on a TCGA dataset with millions of image-language pairs, the model outperforms existing methodologies in tasks such as cancer subtype classification and tissue phenotyping. The code for the model is publicly available on GitHub. This innovative approach enhances feature representation, improves discriminative ability, and demonstrates superior performance in Computational Pathology tasks. <div>
arXiv:2504.18856v1 Announce Type: new 
Abstract: In Computational Pathology (CPath), the introduction of Vision-Language Models (VLMs) has opened new avenues for research, focusing primarily on aligning image-text pairs at a single magnification level. However, this approach might not be sufficient for tasks like cancer subtype classification, tissue phenotyping, and survival analysis due to the limited level of detail that a single-resolution image can provide. Addressing this, we propose a novel multi-resolution paradigm leveraging Whole Slide Images (WSIs) to extract histology patches at multiple resolutions and generate corresponding textual descriptions through advanced CPath VLM. We introduce visual-textual alignment at multiple resolutions as well as cross-resolution alignment to establish more effective text-guided visual representations. Cross-resolution alignment using a multimodal encoder enhances the model's ability to capture context from multiple resolutions in histology images. Our model aims to capture a broader range of information, supported by novel loss functions, enriches feature representation, improves discriminative ability, and enhances generalization across different resolutions. Pre-trained on a comprehensive TCGA dataset with 34 million image-language pairs at various resolutions, our fine-tuned model outperforms state-of-the-art (SOTA) counterparts across multiple datasets and tasks, demonstrating its effectiveness in CPath. The code is available on GitHub at: https://github.com/BasitAlawode/MR-PLIP
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras</title>
<link>https://arxiv.org/abs/2504.18864</link>
<guid>https://arxiv.org/abs/2504.18864</guid>
<content:encoded><![CDATA[
<div> Keywords: Particle Image Velocimetry, spike cameras, deep learning framework, fluid dynamics, PIV dataset <br />
Summary: <br />
- The study explores the use of spike cameras for flow measurement, introducing a deep learning framework called Spike Imaging Velocimetry (SIV) tailored for turbulent flow fields.
- A Detail-Preserving Hierarchical Transform module is incorporated to capture motion features from the spike stream effectively and minimize information loss.
- A Graph Encoder is introduced to extract contextual features from complex fluid flows, enhancing the understanding of intricate flow patterns.
- The development of a spike-based PIV dataset, Particle Scenes with Spike and Displacement (PSSD), provides labeled data for challenging fluid dynamics scenarios, enabling improved algorithm performance evaluation.
- The proposed SIV method demonstrates superior performance over existing baseline methods on the PSSD dataset, showcasing its effectiveness in accurately estimating fluid motion in complex flow fields. The datasets and implementation of SIV are available as open-source resources for further research and development. <br /> 

Summary: <div>
arXiv:2504.18864v1 Announce Type: new 
Abstract: The need for accurate and non-intrusive flow measurement methods has led to the widespread adoption of Particle Image Velocimetry (PIV), a powerful diagnostic tool in fluid motion estimation. This study investigates the tremendous potential of spike cameras (a type of ultra-high-speed, high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike Imaging Velocimetry (SIV), designed specifically for highly turbulent and intricate flow fields. To aggregate motion features from the spike stream while minimizing information loss, we incorporate a Detail-Preserving Hierarchical Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to extract contextual features from highly complex fluid flows. Furthermore, we present a spike-based PIV dataset, Particle Scenes with Spike and Displacement (PSSD), which provides labeled data for three challenging fluid dynamics scenarios. Our proposed method achieves superior performance compared to existing baseline methods on PSSD. The datasets and our implementation of SIV are open-sourced in the supplementary materials.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance</title>
<link>https://arxiv.org/abs/2504.18866</link>
<guid>https://arxiv.org/abs/2504.18866</guid>
<content:encoded><![CDATA[
<div> Keywords: weakly supervised, video violence detection, hierarchical modeling, hyperbolic geometry, contrastive loss

Summary: 
PiercingEye introduces a novel dual-space learning framework that combines Euclidean and hyperbolic geometries to enhance feature representation for video violence detection. The method incorporates a hyperbolic aggregation strategy and cross-space attention mechanism to model event hierarchies and facilitate feature interactions. To address the lack of ambiguous training samples, PiercingEye leverages language models to generate ambiguous event descriptions and uses a hyperbolic vision-language contrastive loss for explicit supervision. Experimental results on XD-Violence and UCF-Crime benchmarks demonstrate state-of-the-art performance, particularly on ambiguous event subsets, showcasing its effectiveness in fine-grained violence detection.<br /><br />Summary: <div>
arXiv:2504.18866v1 Announce Type: new 
Abstract: Existing weakly supervised video violence detection (VVD) methods primarily rely on Euclidean representation learning, which often struggles to distinguish visually similar yet semantically distinct events due to limited hierarchical modeling and insufficient ambiguous training samples. To address this challenge, we propose PiercingEye, a novel dual-space learning framework that synergizes Euclidean and hyperbolic geometries to enhance discriminative feature representation. Specifically, PiercingEye introduces a layer-sensitive hyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to progressively model event hierarchies, and a cross-space attention mechanism to facilitate complementary feature interactions between Euclidean and hyperbolic spaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage large language models to generate logic-guided ambiguous event descriptions, enabling explicit supervision through a hyperbolic vision-language contrastive loss that prioritizes high-confusion samples via dynamic similarity-aware weighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks demonstrate that PiercingEye achieves state-of-the-art performance, with particularly strong results on a newly curated ambiguous event subset, validating its superior capability in fine-grained violence detection.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic Localization System</title>
<link>https://arxiv.org/abs/2504.18870</link>
<guid>https://arxiv.org/abs/2504.18870</guid>
<content:encoded><![CDATA[
<div> Localization, LiDAR, Automated loading, Vehicle compartment, Positioning <br />
<br />
Summary: 
The study introduces a new wide field-of-view 3-D LiDAR system for precise automatic positioning of key points in truck compartments. The system addresses limitations of existing methods by adapting to various truck compartment sizes, establishing a unified coordinate system, and improving reliability in cluttered environments. It leverages high-density point clouds generated by LiDAR to accurately locate corner points in large, medium, and small fence-style truck compartments. A compartment key point positioning algorithm is proposed to identify stackable spatial regions based on geometric features. Experimental results demonstrate the system's reliable accuracy and reduced computational resource consumption, making it suitable for applications in logistics automation and enhancing operational efficiency and safety. <br /> <div>
arXiv:2504.18870v1 Announce Type: new 
Abstract: As an essential component of logistics automation, the automated loading system is becoming a critical technology for enhancing operational efficiency and safety. Precise automatic positioning of the truck compartment, which serves as the loading area, is the primary step in automated loading. However, existing methods have difficulty adapting to truck compartments of various sizes, do not establish a unified coordinate system for LiDAR and mobile manipulators, and often exhibit reliability issues in cluttered environments. To address these limitations, our study focuses on achieving precise automatic positioning of key points in large, medium, and small fence-style truck compartments in cluttered scenarios. We propose an innovative wide field-of-view 3-D LiDAR vehicle compartment automatic localization system. For vehicles of various sizes, this system leverages the LiDAR to generate high-density point clouds within an extensive field-of-view range. By incorporating parking area constraints, our vehicle point cloud segmentation method more effectively segments vehicle point clouds within the scene. Our compartment key point positioning algorithm utilizes the geometric features of the compartments to accurately locate the corner points, providing stackable spatial regions. Extensive experiments on our collected data and public datasets demonstrate that this system offers reliable positioning accuracy and reduced computational resource consumption, leading to its application and promotion in relevant fields.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Multiple Representations: 3D Face Biometrics Fusion with Application to Surveillance</title>
<link>https://arxiv.org/abs/2504.18886</link>
<guid>https://arxiv.org/abs/2504.18886</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D face reconstruction, face recognition systems, fusion methods, biometric recognition, ensemble method 

Summary: 
The study explores the use of multiple state-of-the-art 3D face reconstruction (3DFR) algorithms to enhance face recognition systems in challenging uncontrolled scenarios. By analyzing various parametric and non-parametric fusion methods, the research aims to improve biometric recognition robustness across different conditions like varying distances and camera setups. Results indicate that leveraging the strengths of different 3DFR algorithms can enhance the generalization capability over diverse application scenarios. The study showcases the effectiveness of advanced fusion strategies in boosting the reliability of 3DFR-based face recognition systems. The proposed ensemble method demonstrates potential for enhancing face recognition performance in real-world applications. While experiments focus on face verification setups, the fusion-based 3DFR methods have broader applicability beyond identity recognition tasks in face biometrics. 

<br /><br />Summary: <div>
arXiv:2504.18886v1 Announce Type: new 
Abstract: 3D face reconstruction (3DFR) algorithms are based on specific assumptions tailored to the limits and characteristics of the different application scenarios. In this study, we investigate how multiple state-of-the-art 3DFR algorithms can be used to generate a better representation of subjects, with the final goal of improving the performance of face recognition systems in challenging uncontrolled scenarios. We also explore how different parametric and non-parametric score-level fusion methods can exploit the unique strengths of multiple 3DFR algorithms to enhance biometric recognition robustness. With this goal, we propose a comprehensive analysis of several face recognition systems across diverse conditions, such as varying distances and camera setups, intra-dataset and cross-dataset, to assess the robustness of the proposed ensemble method. The results demonstrate that the distinct information provided by different 3DFR algorithms can alleviate the problem of generalizing over multiple application scenarios. In addition, the present study highlights the potential of advanced fusion strategies to enhance the reliability of 3DFR-based face recognition systems, providing the research community with key insights to exploit them in real-world applications effectively. Although the experiments are carried out in a specific face verification setup, our proposed fusion-based 3DFR methods may be applied to other tasks around face biometrics that are not strictly related to identity recognition.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness</title>
<link>https://arxiv.org/abs/2504.18906</link>
<guid>https://arxiv.org/abs/2504.18906</guid>
<content:encoded><![CDATA[
<div> watermarking, screen capturing, security threats, noise distribution, simulation-to-real<br />
<br />
Summary: <br />
Unauthorized screen capturing and dissemination can lead to data leakage and theft. Existing watermarking methods for tracking copyright of Screen-Camera images rely on heuristic mathematical modeling or supervised neural network fitting as the noise layer. However, these approaches have limitations in accurately approximating SC noise. To address this, the Simulation-to-Real (S2R) method is proposed, using unsupervised learning with unpaired data to bridge the gap between simulated noise distribution and real-world SC noise distribution. This approach proves to be more effective in enhancing watermark robustness and generalization compared to current methods through extensive experimental validation. <div>
arXiv:2504.18906v1 Announce Type: new 
Abstract: Unauthorized screen capturing and dissemination pose severe security threats such as data leakage and information theft. Several studies propose robust watermarking methods to track the copyright of Screen-Camera (SC) images, facilitating post-hoc certification against infringement. These techniques typically employ heuristic mathematical modeling or supervised neural network fitting as the noise layer, to enhance watermarking robustness against SC. However, both strategies cannot fundamentally achieve an effective approximation of SC noise. Mathematical simulation suffers from biased approximations due to the incomplete decomposition of the noise and the absence of interdependence among the noise components. Supervised networks require paired data to train the noise-fitting model, and it is difficult for the model to learn all the features of the noise. To address the above issues, we propose Simulation-to-Real (S2R). Specifically, an unsupervised noise layer employs unpaired data to learn the discrepancy between the modeling simulated noise distribution and the real-world SC noise distribution, rather than directly learning the mapping from sharp images to real-world images. Learning this transformation from simulation to reality is inherently simpler, as it primarily involves bridging the gap in noise distributions, instead of the complex task of reconstructing fine-grained image details. Extensive experimental results validate the efficacy of the proposed method, demonstrating superior watermark robustness and generalization compared to those of state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kinship Verification through a Forest Neural Network</title>
<link>https://arxiv.org/abs/2504.18910</link>
<guid>https://arxiv.org/abs/2504.18910</guid>
<content:encoded><![CDATA[
<div> Keywords: kinship verification, graph neural network, face representations, joint representation, center loss

Summary: 
This study introduces a new approach to kinship verification using graph neural network concepts to effectively utilize face representations. The proposed method outperforms traditional methods that rely solely on face representations. By incorporating joint representations of parents' and children's facial images and designing a novel classification module, the approach shows promising results on datasets such as KinFaceW-I and II. The introduction of a new combination of losses, including center loss, improves the network training process and enhances the overall performance. Experimental results demonstrate a significant improvement in kinship verification accuracy on KinFaceW-II, with an average increase of nearly 1.6 for all kinship types. The code for the proposed approach is publicly available, allowing for further research and development in the field of kinship verification. 

<br /><br />Summary: <div>
arXiv:2504.18910v1 Announce Type: new 
Abstract: Early methods used face representations in kinship verification, which are less accurate than joint representations of parents' and children's facial images learned from scratch. We propose an approach featuring graph neural network concepts to utilize face representations and have comparable results to joint representation algorithms. Moreover, we designed the structure of the classification module and introduced a new combination of losses to engage the center loss gradually in training our network. Additionally, we conducted experiments on KinFaceW-I and II, demonstrating the effectiveness of our approach. We achieved the best result on KinFaceW-II, an average improvement of nearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The code is available at https://github.com/ali-nazari/Kinship-Verification
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Sparse R-CNN: SAR Ship Detection Based on Background-Aware Sparse Learnable Proposals</title>
<link>https://arxiv.org/abs/2504.18959</link>
<guid>https://arxiv.org/abs/2504.18959</guid>
<content:encoded><![CDATA[
<div> sparse, oriented ship detection, Synthetic Aperture Radar (SAR), background-aware proposals (BAPs), Dual-Context Pooling (DCP) 

Summary: 
R-Sparse R-CNN introduces a novel pipeline for oriented ship detection in SAR images by leveraging sparse learnable proposals called BAPs. This approach eliminates the need for proposal generators and post-processing, streamlining the pipeline. BAPs enrich object representation by integrating ship and background features for more accurate ship detection in complex environments. The Dual-Context Pooling (DCP) strategy extracts ship and background features efficiently in a unified operation, improving contextual relationship learning. The unified design of DCP eliminates redundant computation and provides aligned features for better learning. Additionally, an Interaction Module, based on transformers, models the relationships between ship and background features with proposal features. Experimental results demonstrate the superior accuracy of R-Sparse R-CNN, outperforming state-of-the-art models on SSDD and RSDD-SAR inshore datasets. This framework proves to be effective and competitive for oriented ship detection in SAR imagery. <div>
arXiv:2504.18959v1 Announce Type: new 
Abstract: We introduce R-Sparse R-CNN, a novel pipeline for oriented ship detection in Synthetic Aperture Radar (SAR) images that leverages sparse learnable proposals enriched with background contextual information, termed background-aware proposals (BAPs). The adoption of sparse proposals streamlines the pipeline by eliminating the need for proposal generators and post-processing for overlapping predictions. The proposed BAPs enrich object representation by integrating ship and background features, allowing the model to learn their contextual relationships for more accurate distinction of ships in complex environments. To complement BAPs, we propose Dual-Context Pooling (DCP), a novel strategy that jointly extracts ship and background features in a single unified operation. This unified design improves efficiency by eliminating redundant computation inherent in separate pooling. Moreover, by ensuring that ship and background features are pooled from the same feature map level, DCP provides aligned features that improve contextual relationship learning. Finally, as a core component of contextual relationship learning in R-Sparse R-CNN, we design a dedicated transformer-based Interaction Module. This module interacts pooled ship and background features with corresponding proposal features and models their relationships. Experimental results show that R-Sparse R-CNN delivers outstanding accuracy, surpassing state-of-the-art models by margins of up to 12.8% and 11.9% on SSDD and RSDD-SAR inshore datasets, respectively. These results demonstrate the effectiveness and competitiveness of R-Sparse R-CNN as a robust framework for oriented ship detection in SAR imagery. The code is available at: www.github.com/ka-mirul/R-Sparse-R-CNN.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DPyranet Features Fusion for Spatio-temporal Feature Learning</title>
<link>https://arxiv.org/abs/2504.18977</link>
<guid>https://arxiv.org/abs/2504.18977</guid>
<content:encoded><![CDATA[
<div> Keywords: Convolutional neural network, 3DPyraNet, spatio-temporal feature learning, human actions, dynamic scenes

Summary: 
3DPyraNet is a 3D pyramidal neural network that introduces a new weighting scheme for spatio-temporal feature learning from multiple adjacent frames. It maintains a biological plausible structure, reduces parameters, and has lower computational costs compared to traditional CNNs. The 3DPyraNet-F approach extracts feature maps from the highest layer of the network, fuses them, and inputs them to a linear-SVM classifier for enhanced recognition of human actions and dynamic scenes in videos. The results show promising performance in real-world scenarios, especially with camera-induced motion. 3DPyraNet-F outperforms state-of-the-art methods on three benchmark datasets and performs comparably on the fourth. This innovative approach could potentially advance the field of action and scene recognition in video analysis.<br /><br />Summary: <div>
arXiv:2504.18977v1 Announce Type: new 
Abstract: Convolutional neural network (CNN) slides a kernel over the whole image to produce an output map. This kernel scheme reduces the number of parameters with respect to a fully connected neural network (NN). While CNN has proven to be an effective model in recognition of handwritten characters and traffic signal sign boards, etc. recently, its deep variants have proven to be effective in similar as well as more challenging applications like object, scene and action recognition. Deep CNN add more layers and kernels to the classical CNN, increasing the number of parameters, and partly reducing the main advantage of CNN which is less parameters. In this paper, a 3D pyramidal neural network called 3DPyraNet and a discriminative approach for spatio-temporal feature learning based on it, called 3DPyraNet-F, are proposed. 3DPyraNet introduces a new weighting scheme which learns features from both spatial and temporal dimensions analyzing multiple adjacent frames and keeping a biological plausible structure. It keeps the spatial topology of the input image and presents fewer parameters and lower computational and memory costs compared to both fully connected NNs and recent deep CNNs. 3DPyraNet-F extract the features maps of the highest layer of the learned network, fuse them in a single vector, and provide it as input in such a way to a linear-SVM classifier that enhances the recognition of human actions and dynamic scenes from the videos. Encouraging results are reported with 3DPyraNet in real-world environments, especially in the presence of camera induced motion. Further, 3DPyraNet-F clearly outperforms the state-of-the-art on three benchmark datasets and shows comparable result for the fourth.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MediAug: Exploring Visual Augmentation in Medical Imaging</title>
<link>https://arxiv.org/abs/2504.18983</link>
<guid>https://arxiv.org/abs/2504.18983</guid>
<content:encoded><![CDATA[
<div> Data augmentation, medical imaging, classification accuracy, convolutional backbone, transformer backbone <br />
Summary: <br />
The article discusses the importance of data augmentation in medical imaging to improve classification accuracy, lesion detection, and organ segmentation. Two main challenges are identified: the domain gap between natural photographs and medical images, and the fragmented nature of augmentation studies in medical imaging. To address these challenges, the authors propose a unified evaluation framework called MediAug, which includes six mix-based augmentation methods evaluated with both convolutional and transformer backbones on brain tumor MRI and eye disease fundus datasets. Results show that MixUp yields the greatest improvement on brain tumor classification with a ResNet-50 backbone, while SnapMix achieves the highest accuracy with a ViT-B backbone. Similarly, YOCO and CutMix show significant improvements for eye disease classification tasks with different backbone architectures. The code for the evaluation framework will be available on GitHub. <div>
arXiv:2504.18983v1 Announce Type: new 
Abstract: Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISUALCENT: Visual Human Analysis using Dynamic Centroid Representation</title>
<link>https://arxiv.org/abs/2504.19032</link>
<guid>https://arxiv.org/abs/2504.19032</guid>
<content:encoded><![CDATA[
<div> Keywords: VISUALCENT, human pose, instance segmentation, keypoint detection, mAP scores

Summary:
VISUALCENT is a new framework that combines human pose estimation and instance segmentation to enhance generalizability and scalability in multi-person visual human analysis. It utilizes a centroid-based bottom-up keypoint detection approach, incorporating Disk Representation and KeyCentroid to identify optimal keypoint coordinates. This framework introduces MaskCentroid as a dynamic centroid for unified segmentation, effectively clustering pixels to specific human instances in dynamic or occluded environments. Experimental results on COCO and OCHuman datasets show VISUALCENT's superior accuracy and real-time performance compared to existing methods, achieving higher mAP scores and execution frame rates. The implementation code is available for reference on the project page. 

<br /><br />Summary: 
- VISUALCENT is a unified framework for human pose estimation and instance segmentation.
- It utilizes centroid-based keypoint detection and introduces MaskCentroid for dynamic segmentation.
- Experimental results demonstrate VISUALCENT's high accuracy and real-time performance.
- The framework outperforms existing methods in terms of mAP scores and execution frame rates.
- Implementation code is available on the project page for reference. <div>
arXiv:2504.19032v1 Announce Type: new 
Abstract: We introduce VISUALCENT, a unified human pose and instance segmentation framework to address generalizability and scalability limitations to multi person visual human analysis. VISUALCENT leverages centroid based bottom up keypoint detection paradigm and uses Keypoint Heatmap incorporating Disk Representation and KeyCentroid to identify the optimal keypoint coordinates. For the unified segmentation task, an explicit keypoint is defined as a dynamic centroid called MaskCentroid to swiftly cluster pixels to specific human instance during rapid changes in human body movement or significantly occluded environment. Experimental results on COCO and OCHuman datasets demonstrate VISUALCENTs accuracy and real time performance advantages, outperforming existing methods in mAP scores and execution frame rate per second. The implementation is available on the project page.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2504.19056</link>
<guid>https://arxiv.org/abs/2504.19056</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Character Animation, Facial Animation, Gesture Modeling, Motion Synthesis

Summary:<br /><br />Generative AI has revolutionized the field of character animation by reducing production time and costs. This survey provides a comprehensive overview of the latest advancements in various areas of character animation, including facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. The survey highlights leading research, practical applications, commonly used datasets, and emerging trends in each of these areas. Additionally, it offers a background section to introduce foundational models and evaluation metrics for newcomers to the field. The survey also discusses open challenges and outlines future research directions to advance AI-driven character-animation technologies. This resource is designed to benefit researchers and developers entering the field of generative AI animation and related fields. <div>
arXiv:2504.19056v1 Announce Type: new 
Abstract: Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype</title>
<link>https://arxiv.org/abs/2504.19074</link>
<guid>https://arxiv.org/abs/2504.19074</guid>
<content:encoded><![CDATA[
<div> Keywords: Convolutional neural networks, hyperspectral image classification, few-shot learning, domain shift, spectral features

Summary: 
This article introduces a dual-branch residual network for hyperspectral image classification using convolutional neural networks. The network integrates spatial and spectral features through parallel branches, improving generalization in few-shot scenarios. A regulation term is introduced to enhance prototype quality, leading to more robust performance with limited samples. Additionally, a kernel probability matching strategy is used to align source and target domain features, reducing domain shift caused by sensor differences and environmental variations. Experimental results on four publicly available HSI datasets demonstrate the superiority of the proposed method over existing approaches. Overall, the dual-branch residual network effectively addresses the challenges of computational costs, limited generalization, and domain shift in HSI classification tasks. 

<br /><br />Summary: <div>
arXiv:2504.19074v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) are effective for hyperspectral image (HSI) classification, but their 3D convolutional structures introduce high computational costs and limited generalization in few-shot scenarios. Domain shifts caused by sensor differences and environmental variations further hinder cross-dataset adaptability. Metric-based few-shot learning (FSL) prototype networks mitigate this problem, yet their performance is sensitive to prototype quality, especially with limited samples. To overcome these challenges, a dual-branch residual network that integrates spatial and spectral features via parallel branches is proposed in this letter. Additionally, more robust refined prototypes are obtained through a regulation term. Furthermore, a kernel probability matching strategy aligns source and target domain features, alleviating domain shift. Experiments on four publicly available HSI datasets illustrate that the proposal achieves superior performance compared to other methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2504.19075</link>
<guid>https://arxiv.org/abs/2504.19075</guid>
<content:encoded><![CDATA[
<div> knowledge-driven, multimodal data, Alzheimer's disease, HoloDx, diagnostic accuracy

Summary:
HoloDx is a framework designed to improve Alzheimer's disease diagnosis by effectively integrating domain knowledge with multimodal clinical data. The model includes a knowledge injection module that utilizes large language models (LLMs) and clinical expertise to dynamically integrate domain-specific insights. Additionally, a memory injection module with prototypical memory attention helps the model retain and retrieve subject-specific information for consistent decision-making. By combining these mechanisms, HoloDx enhances interpretability, robustness, and alignment of prior knowledge with current subject data. Evaluation on five AD datasets shows that HoloDx outperforms existing methods, achieving higher diagnostic accuracy and strong generalization across diverse cohorts. The source code for HoloDx will be made available upon acceptance for publication.<br /><br />Summary: <div>
arXiv:2504.19075v1 Announce Type: new 
Abstract: Accurate diagnosis of Alzheimer's disease (AD) requires effectively integrating multimodal data and clinical expertise. However, existing methods often struggle to fully utilize multimodal information and lack structured mechanisms to incorporate dynamic domain knowledge. To address these limitations, we propose HoloDx, a knowledge- and data-driven framework that enhances AD diagnosis by aligning domain knowledge with multimodal clinical data. HoloDx incorporates a knowledge injection module with a knowledge-aware gated cross-attention, allowing the model to dynamically integrate domain-specific insights from both large language models (LLMs) and clinical expertise. Also, a memory injection module with a designed prototypical memory attention enables the model to retain and retrieve subject-specific information, ensuring consistency in decision-making. By jointly leveraging these mechanisms, HoloDx enhances interpretability, improves robustness, and effectively aligns prior knowledge with current subject data. Evaluations on five AD datasets demonstrate that HoloDx outperforms state-of-the-art methods, achieving superior diagnostic accuracy and strong generalization across diverse cohorts. The source code will be released upon publication acceptance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Drive from a World Model</title>
<link>https://arxiv.org/abs/2504.19077</link>
<guid>https://arxiv.org/abs/2504.19077</guid>
<content:encoded><![CDATA[
<div> Keywords: self-driving systems, end-to-end training architecture, real driving data, on-policy simulator, advanced driver-assistance system

Summary:
Self-driving systems typically rely on hand-coded perception outputs and driving rules. This study introduces an end-to-end training architecture that utilizes real driving data to train a driving policy within an on-policy simulator. Two simulation methods, reprojective simulation and a learned world model, are employed to train a policy without predefined driving rules. The performance of these policies is evaluated in closed-loop simulations and real-world advanced driver-assistance systems. The approach aims to simplify the training process and improve scalability with compute and data. By learning directly from human driving data, the proposed method offers a more adaptive and efficient way to train driving policies. <div>
arXiv:2504.19077v1 Announce Type: new 
Abstract: Most self-driving systems rely on hand-coded perception outputs and engineered driving rules. Learning directly from human driving data with an end-to-end method can allow for a training architecture that is simpler and scales well with compute and data.
  In this work, we propose an end-to-end training architecture that uses real driving data to train a driving policy in an on-policy simulator. We show two different methods of simulation, one with reprojective simulation and one with a learned world model. We show that both methods can be used to train a policy that learns driving behavior without any hand-coded driving rules. We evaluate the performance of these policies in a closed-loop simulation and when deployed in a real-world advanced driver-assistance system.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore</title>
<link>https://arxiv.org/abs/2504.19080</link>
<guid>https://arxiv.org/abs/2504.19080</guid>
<content:encoded><![CDATA[
<div> Keywords: Attention mechanisms, deep learning, feature representation, MIA-Mind, MindSpore framework 

Summary: 
MIA-Mind is introduced as a Multidimensional Interactive Attention Mechanism that addresses the interdependence between channel importance and spatial saliency. It offers a unified cross-attentive fusion strategy to fine-tune feature recalibration with low computational overhead. The performance of MIA-Mind is evaluated on CIFAR-10, ISBI2012, and CIC-IDS2017 datasets, achieving accuracies of 82.9%, 78.7%, and 91.9% respectively. The study highlights the lightweight design, versatility, and generalization capability of MIA-Mind across diverse tasks. Future research directions include scaling up to large datasets, developing adaptive attention fusion methods, and exploring distributed deployment for improved scalability and robustness.<br /><br />Summary: <div>
arXiv:2504.19080v1 Announce Type: new 
Abstract: Attention mechanisms have significantly advanced deep learning by enhancing feature representation through selective focus. However, existing approaches often independently model channel importance and spatial saliency, overlooking their inherent interdependence and limiting their effectiveness. To address this limitation, we propose MIA-Mind, a lightweight and modular Multidimensional Interactive Attention Mechanism, built upon the MindSpore framework. MIA-Mind jointly models spatial and channel features through a unified cross-attentive fusion strategy, enabling fine-grained feature recalibration with minimal computational overhead. Extensive experiments are conducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an accuracy of 82.9\%; on ISBI2012, it achieves an accuracy of 78.7\%; and on CIC-IDS2017, it achieves an accuracy of 91.9\%. These results validate the versatility, lightweight design, and generalization ability of MIA-Mind across heterogeneous tasks. Future work will explore the extension of MIA-Mind to large-scale datasets, the development of ada,ptive attention fusion strategies, and distributed deployment to further enhance scalability and robustness.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Single-domain Generalized Object Detection via Vision-Language Knowledge Interaction</title>
<link>https://arxiv.org/abs/2504.19086</link>
<guid>https://arxiv.org/abs/2504.19086</guid>
<content:encoded><![CDATA[
<div> method, object detection, cross-modal feature learning, region-aware feature interaction, proposal refining 

Summary: 
The article introduces a new method for Single-Domain Generalized Object Detection (S-DGOD) to improve generalization to diverse target domains. The proposed method focuses on cross-modal feature learning to capture generalized and discriminative regional features. It incorporates a mechanism for Cross-modal and Region-aware Feature Interaction to learn inter-modal and intra-modal regional invariance simultaneously. Additionally, a strategy called Cross-domain Proposal Refining and Mixing is used to align and diversify region proposals across domains, enhancing detector localization in unseen scenarios. The method achieves state-of-the-art results on benchmark datasets, with significant improvements in mPC metrics on Cityscapes-C and DWD datasets over baselines, demonstrating its effectiveness in S-DGOD tasks. <div>
arXiv:2504.19086v1 Announce Type: new 
Abstract: Single-Domain Generalized Object Detection~(S-DGOD) aims to train an object detector on a single source domain while generalizing well to diverse unseen target domains, making it suitable for multimedia applications that involve various domain shifts, such as intelligent video surveillance and VR/AR technologies. With the success of large-scale Vision-Language Models, recent S-DGOD approaches exploit pre-trained vision-language knowledge to guide invariant feature learning across visual domains. However, the utilized knowledge remains at a coarse-grained level~(e.g., the textual description of adverse weather paired with the image) and serves as an implicit regularization for guidance, struggling to learn accurate region- and object-level features in varying domains. In this work, we propose a new cross-modal feature learning method, which can capture generalized and discriminative regional features for S-DGOD tasks. The core of our method is the mechanism of Cross-modal and Region-aware Feature Interaction, which simultaneously learns both inter-modal and intra-modal regional invariance through dynamic interactions between fine-grained textual and visual features. Moreover, we design a simple but effective strategy called Cross-domain Proposal Refining and Mixing, which aligns the position of region proposals across multiple domains and diversifies them, enhancing the localization ability of detectors in unseen scenarios. Our method achieves new state-of-the-art results on S-DGOD benchmark datasets, with improvements of +8.8\%~mPC on Cityscapes-C and +7.9\%~mPC on DWD over baselines, demonstrating its efficacy.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Latency-Aware 3D Streaming Perception for Autonomous Driving</title>
<link>https://arxiv.org/abs/2504.19115</link>
<guid>https://arxiv.org/abs/2504.19115</guid>
<content:encoded><![CDATA[
<div> benchmark, latency-aware, 3D perception, edge devices, online evaluation

Summary:
The article introduces a new benchmark focused on runtime latency for 3D perception algorithms deployed on edge devices. The proposed framework, Latency-Aware 3D Streaming Perception (LASP), addresses latency challenges through two primary components. Firstly, the latency-aware history integration ensures the continuous integration of historical features regardless of varying latency. Secondly, the latency-aware predictive detection module compensates detection results with predicted trajectory and accessed latency information. By incorporating these mechanisms, the method demonstrates generalization across different latency levels and achieves an online performance on the Jetson AGX Orin that closely matches its offline evaluation without the use of acceleration techniques. The LASP framework showcases significant improvements in performance for 3D perception tasks on edge devices, with a focus on reducing runtime latency. 

<br /><br />Summary: <div>
arXiv:2504.19115v1 Announce Type: new 
Abstract: Although existing 3D perception algorithms have demonstrated significant improvements in performance, their deployment on edge devices continues to encounter critical challenges due to substantial runtime latency. We propose a new benchmark tailored for online evaluation by considering runtime latency. Based on the benchmark, we build a Latency-Aware 3D Streaming Perception (LASP) framework that addresses the latency issue through two primary components: 1) latency-aware history integration, which extends query propagation into a continuous process, ensuring the integration of historical feature regardless of varying latency; 2) latency-aware predictive detection, a module that compensates the detection results with the predicted trajectory and the posterior accessed latency. By incorporating the latency-aware mechanism, our method shows generalization across various latency levels, achieving an online performance that closely aligns with 80\% of its offline evaluation on the Jetson AGX Orin without any acceleration techniques.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blind Source Separation Based on Sparsity</title>
<link>https://arxiv.org/abs/2504.19124</link>
<guid>https://arxiv.org/abs/2504.19124</guid>
<content:encoded><![CDATA[
<div> Sparse Representation, Blind Source Separation, Morphological Component Analysis, Multichannel MCA, Dictionary Learning<br />
Summary:<br />
This report delves into Blind Source Separation (BSS) techniques, exploring classical Independent Component Analysis (ICA) and sparsity-based methods. It introduces the concept of sparse representation and decomposition, detailing a block coordinate relaxation MCA algorithm, Multichannel MCA (MMCA), and Generalized MCA (GMCA). The report also discusses local dictionary learning using K-SVD, proposing an enhanced algorithm, SAC+BK-SVD, for learning a block-sparsifying dictionary. Implementation involves experiments on image segmentation and blind image source separation. Simulation results demonstrate the superiority of the proposed block-sparse dictionary learning algorithm in improving blind image separation quality. <div>
arXiv:2504.19124v1 Announce Type: new 
Abstract: Blind source separation (BSS) is a key technique in array processing and data analysis, aiming to recover unknown sources from observed mixtures without knowledge of the mixing matrix. Classical independent component analysis (ICA) methods rely on the assumption that sources are mutually independent. To address limitations of ICA, sparsity-based methods have been introduced, which decompose source signals sparsely in a predefined dictionary. Morphological Component Analysis (MCA), based on sparse representation theory, assumes that a signal is a linear combination of components with distinct geometries, each sparsely representable in one dictionary and not in others. This approach has recently been applied to BSS with promising results.
  This report reviews key approaches derived from classical ICA and explores sparsity-based methods for BSS. It introduces the theory of sparse representation and decomposition, followed by a block coordinate relaxation MCA algorithm, whose variants are used in Multichannel MCA (MMCA) and Generalized MCA (GMCA). A local dictionary learning method using K-SVD is then presented. Finally, we propose an improved algorithm, SAC+BK-SVD, which enhances K-SVD by learning a block-sparsifying dictionary that clusters and updates similar atoms in blocks.
  The implementation includes experiments on image segmentation and blind image source separation using the discussed techniques. We also compare the proposed block-sparse dictionary learning algorithm with K-SVD. Simulation results demonstrate that our method yields improved blind image separation quality.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSPG: Exploring Deep Semantic Prior Guidance for Low-light Image Enhancement with Multimodal Learning</title>
<link>https://arxiv.org/abs/2504.19127</link>
<guid>https://arxiv.org/abs/2504.19127</guid>
<content:encoded><![CDATA[
<div> semantic knowledge, low-light image enhancement, multimodal learning, deep learning, Retinex image decomposition

Summary:
The article introduces a new deep semantic prior-guided framework (DeepSPG) for low-light image enhancement (LLIE), aiming to improve performance by incorporating semantic information. The framework utilizes a pre-trained semantic segmentation model and multimodal learning to guide the enhancement process. It incorporates both image-level and text-level semantic priors to facilitate effective semantic feature incorporation. Additionally, a multi-scale semantic-aware structure is introduced to enhance semantic feature integration. The DeepSPG framework outperforms existing methods on five benchmark datasets, demonstrating superior performance in LLIE tasks. The implementation details and code for DeepSPG are publicly available on GitHub for further exploration and research. <div>
arXiv:2504.19127v1 Announce Type: new 
Abstract: There has long been a belief that high-level semantics learning can benefit various downstream computer vision tasks. However, in the low-light image enhancement (LLIE) community, existing methods learn a brutal mapping between low-light and normal-light domains without considering the semantic information of different regions, especially in those extremely dark regions that suffer from severe information loss. To address this issue, we propose a new deep semantic prior-guided framework (DeepSPG) based on Retinex image decomposition for LLIE to explore informative semantic knowledge via a pre-trained semantic segmentation model and multimodal learning. Notably, we incorporate both image-level semantic prior and text-level semantic prior and thus formulate a multimodal learning framework with combinatorial deep semantic prior guidance for LLIE. Specifically, we incorporate semantic knowledge to guide the enhancement process via three designs: an image-level semantic prior guidance by leveraging hierarchical semantic features from a pre-trained semantic segmentation model; a text-level semantic prior guidance by integrating natural language semantic constraints via a pre-trained vision-language model; a multi-scale semantic-aware structure that facilitates effective semantic feature incorporation. Eventually, our proposed DeepSPG demonstrates superior performance compared to state-of-the-art methods across five benchmark datasets. The implementation details and code are publicly available at https://github.com/Wenyuzhy/DeepSPG.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification</title>
<link>https://arxiv.org/abs/2504.19136</link>
<guid>https://arxiv.org/abs/2504.19136</guid>
<content:encoded><![CDATA[
<div> Frequency-aware framework, Phase-Amplitude Decoupling, SAR, RGB, land cover classification

Summary:
Phase-Amplitude Decoupling (PAD) is a novel framework for fusing Synthetic Aperture Radar (SAR) and RGB imagery for land cover classification. It addresses the challenges of modality heterogeneity and the underutilization of spectral complementarity by separating phase (modality-shared) and amplitude (modality-specific) components in the Fourier domain. PAD consists of two key components: Phase Spectrum Correction (PSC) and Amplitude Spectrum Fusion (ASF). PSC aligns cross-modal phase features for enhanced geometric consistency, while ASF dynamically integrates high-frequency details and low-frequency structures using frequency-adaptive multilayer perceptrons. Extensive experiments on WHU-OPT-SAR and DDHR-SK datasets show state-of-the-art performance, leveraging SAR's sensitivity to morphological features and RGB's spectral richness. This work establishes a new paradigm for physics-aware multi-modal fusion in remote sensing.

<br /><br />Summary: <div>
arXiv:2504.19136v1 Announce Type: new 
Abstract: The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover classification remains challenging due to modality heterogeneity and the underutilization of spectral complementarity. Existing methods often fail to decouple shared structural features from modality-specific radiometric attributes, leading to feature conflicts and information loss. To address this issue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework that separates phase (modality-shared) and amplitude (modality-specific) components in the Fourier domain. Specifically, PAD consists of two key components: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase features through convolution-guided scaling to enhance geometric consistency, and 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates high-frequency details and low-frequency structures using frequency-adaptive multilayer perceptrons. This approach leverages SAR's sensitivity to morphological features and RGB's spectral richness. Extensive experiments on WHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our work establishes a new paradigm for physics-aware multi-modal fusion in remote sensing. The code will be available at https://github.com/RanFeng2/PAD.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadioFormer: A Multiple-Granularity Radio Map Estimation Transformer with 1\textpertenthousand Spatial Sampling</title>
<link>https://arxiv.org/abs/2504.19161</link>
<guid>https://arxiv.org/abs/2504.19161</guid>
<content:encoded><![CDATA[
<div> Transformer, radio map estimation, sparse observations, deep learning, electromagnetic spectrum

Summary:
The article introduces RadioFormer, a multiple-granularity transformer designed to improve radio map estimation in scenarios with extreme spatial sparsity. RadioFormer uses a dual-stream self-attention module to capture correlations in pixel-wise signal power observations and patch-wise building geometries. It then integrates these into multi-scale representations of radio maps through a cross stream cross-attention module. Experiments on the RadioMapSeer dataset show that RadioFormer outperforms existing methods in radio map estimation with lower computational costs. It also demonstrates strong generalization capabilities and robust zero-shot performance. The approach has the potential to advance radio map estimation in practical settings with very limited observation nodes.<br /><br />Summary: <div>
arXiv:2504.19161v1 Announce Type: new 
Abstract: The task of radio map estimation aims to generate a dense representation of electromagnetic spectrum quantities, such as the received signal strength at each grid point within a geographic region, based on measurements from a subset of spatially distributed nodes (represented as pixels). Recently, deep vision models such as the U-Net have been adapted to radio map estimation, whose effectiveness can be guaranteed with sufficient spatial observations (typically 0.01% to 1% of pixels) in each map, to model local dependency of observed signal power. However, such a setting of sufficient measurements can be less practical in real-world scenarios, where extreme sparsity in spatial sampling can be widely encountered. To address this challenge, we propose RadioFormer, a novel multiple-granularity transformer designed to handle the constraints posed by spatial sparse observations. Our RadioFormer, through a dual-stream self-attention (DSA) module, can respectively discover the correlation of pixel-wise observed signal power and also learn patch-wise buildings' geometries in a style of multiple granularities, which are integrated into multi-scale representations of radio maps by a cross stream cross-attention (CCA) module. Extensive experiments on the public RadioMapSeer dataset demonstrate that RadioFormer outperforms state-of-the-art methods in radio map estimation while maintaining the lowest computational cost. Furthermore, the proposed approach exhibits exceptional generalization capabilities and robust zero-shot performance, underscoring its potential to advance radio map estimation in a more practical setting with very limited observation nodes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IM-Portrait: Learning 3D-aware Video Diffusion for PhotorealisticTalking Heads from Monocular Videos</title>
<link>https://arxiv.org/abs/2504.19165</link>
<guid>https://arxiv.org/abs/2504.19165</guid>
<content:encoded><![CDATA[
<div> 3D-aware diffusion-based method, photorealistic talking head videos, Multiplane Images, immersive viewing experiences, monocular videos<br />
Summary:<br />
The article introduces a novel 3D-aware diffusion-based method for generating photorealistic talking head videos using a single identity image and control signals. The method utilizes Multiplane Images (MPIs) to ensure geometric consistency, making it suitable for immersive viewing experiences like binocular videos for VR headsets. Unlike existing methods, this approach directly generates the final output through a single denoising process without the need for post-processing steps. A unique training mechanism reconstructs the output MPI randomly in either the target or reference camera space, enabling the model to simultaneously capture sharp image details and underlying 3D information. The method demonstrates competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data.<br /> <div>
arXiv:2504.19165v1 Announce Type: new 
Abstract: We propose a novel 3D-aware diffusion-based method for generating photorealistic talking head videos directly from a single identity image and explicit control signals (e.g., expressions). Our method generates Multiplane Images (MPIs) that ensure geometric consistency, making them ideal for immersive viewing experiences like binocular videos for VR headsets. Unlike existing methods that often require a separate stage or joint optimization to reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach directly generates the final output through a single denoising process, eliminating the need for post-processing steps to render novel views efficiently. To effectively learn from monocular videos, we introduce a training mechanism that reconstructs the output MPI randomly in either the target or the reference camera space. This approach enables the model to simultaneously learn sharp image details and underlying 3D information. Extensive experiments demonstrate the effectiveness of our method, which achieves competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segmenting Objectiveness and Task-awareness Unknown Region for Autonomous Driving</title>
<link>https://arxiv.org/abs/2504.19183</link>
<guid>https://arxiv.org/abs/2504.19183</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer-based architectures, road scene perception, anomaly detection, autonomous driving, segmentation

Summary:
The paper introduces a new framework called Segmenting Objectiveness and Task-Awareness (SOTA) for autonomous driving scenes. SOTA aims to improve the segmentation of objectiveness in road scene perception while filtering out anomalies irrelevant to autonomous driving tasks. The framework includes a Semantic Fusion Block (SFB) to enhance objectiveness segmentation and a Scene-understanding Guided Prompt-Context Adaptor (SG-PCA) to eliminate irrelevant anomalies. By addressing the limitations of existing methods that rely on image inpainting and OOD distribution detection, SOTA significantly enhances OOD detection performance across diverse detectors. Empirical evaluations on benchmark datasets such as Fishyscapes Lost and Found, Segment-Me-If-You-Can, and RoadAnomaly demonstrate the effectiveness of the proposed framework in achieving robust and accurate segmentation outcomes. Overall, SOTA provides a promising approach to improving road scene perception and anomaly detection in the field of autonomous driving. 

<br /><br />Summary: <div>
arXiv:2504.19183v1 Announce Type: new 
Abstract: With the emergence of transformer-based architectures and large language models (LLMs), the accuracy of road scene perception has substantially advanced. Nonetheless, current road scene segmentation approaches are predominantly trained on closed-set data, resulting in insufficient detection capabilities for out-of-distribution (OOD) objects. To overcome this limitation, road anomaly detection methods have been proposed. However, existing methods primarily depend on image inpainting and OOD distribution detection techniques, facing two critical issues: (1) inadequate consideration of the objectiveness attributes of anomalous regions, causing incomplete segmentation when anomalous objects share similarities with known classes, and (2) insufficient attention to environmental constraints, leading to the detection of anomalies irrelevant to autonomous driving tasks. In this paper, we propose a novel framework termed Segmenting Objectiveness and Task-Awareness (SOTA) for autonomous driving scenes. Specifically, SOTA enhances the segmentation of objectiveness through a Semantic Fusion Block (SFB) and filters anomalies irrelevant to road navigation tasks using a Scene-understanding Guided Prompt-Context Adaptor (SG-PCA). Extensive empirical evaluations on multiple benchmark datasets, including Fishyscapes Lost and Found, Segment-Me-If-You-Can, and RoadAnomaly, demonstrate that the proposed SOTA consistently improves OOD detection performance across diverse detectors, achieving robust and accurate segmentation outcomes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition</title>
<link>https://arxiv.org/abs/2504.19186</link>
<guid>https://arxiv.org/abs/2504.19186</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, place recognition, LiDAR, radar, fusion

Summary:
In autonomous driving, accurate place recognition is essential for global localization in challenging environments where GPS may not work. The paper introduces LRFusionPR, a method that combines LiDAR with single-chip or scanning radar data to improve recognition accuracy and robustness. A dual-branch network is used to fuse different modalities within a unified polar coordinate bird's eye view representation. Cross-attention is employed for cross-modality feature interactions, and knowledge from the fusion branch is transferred to the distillation branch to enhance robustness. The resulting multimodal global descriptor enables precise place retrieval. Extensive evaluations on various datasets show that LRFusionPR achieves accurate place recognition and maintains robustness even in adverse weather conditions. The code for LRFusionPR will be released as open-source. 

<br /><br />Summary: <div>
arXiv:2504.19186v1 Announce Type: new 
Abstract: In autonomous driving, place recognition is critical for global localization in GPS-denied environments. LiDAR and radar-based place recognition methods have garnered increasing attention, as LiDAR provides precise ranging, whereas radar excels in adverse weather resilience. However, effectively leveraging LiDAR-radar fusion for place recognition remains challenging. The noisy and sparse nature of radar data limits its potential to further improve recognition accuracy. In addition, heterogeneous radar configurations complicate the development of unified cross-modality fusion frameworks. In this paper, we propose LRFusionPR, which improves recognition accuracy and robustness by fusing LiDAR with either single-chip or scanning radar. Technically, a dual-branch network is proposed to fuse different modalities within the unified polar coordinate bird's eye view (BEV) representation. In the fusion branch, cross-attention is utilized to perform cross-modality feature interactions. The knowledge from the fusion branch is simultaneously transferred to the distillation branch, which takes radar as its only input to further improve the robustness. Ultimately, the descriptors from both branches are concatenated, producing the multimodal global descriptor for place retrieval. Extensive evaluations on multiple datasets demonstrate that our LRFusionPR achieves accurate place recognition, while maintaining robustness under varying weather conditions. Our open-source code will be released at https://github.com/QiZS-BIT/LRFusionPR.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Dual-domain Learning for Underwater Image Enhancement</title>
<link>https://arxiv.org/abs/2504.19198</link>
<guid>https://arxiv.org/abs/2504.19198</guid>
<content:encoded><![CDATA[
<div> MCSS, SWSA, SS-block, FWL, UIE <br />
Summary: 
The article introduces a novel Underwater Image Enhancement (UIE) method called SS-UIE that addresses challenges faced by existing learning-based methods. The SS-UIE method utilizes spatial-wise Multi-scale Cycle Selective Scan (MCSS) and Spectral-Wise Self-Attention (SWSA) modules to handle degradation levels in different spatial regions and spectral bands. These modules are combined to form a Spatial-Spectral block (SS-block) for dual-domain adaptive UIE. The SS-UIE network, built by stacking multiple SS-blocks, incorporates a Frequency-Wise Loss (FWL) to focus on regions with high-frequency details. Experimental results show that the SS-UIE technique outperforms state-of-the-art UIE methods while being computationally efficient and cost-effective. <br /> <div>
arXiv:2504.19198v1 Announce Type: new 
Abstract: Recently, learning-based Underwater Image Enhancement (UIE) methods have demonstrated promising performance. However, existing learning-based methods still face two challenges. 1) They rarely consider the inconsistent degradation levels in different spatial regions and spectral bands simultaneously. 2) They treat all regions equally, ignoring that the regions with high-frequency details are more difficult to reconstruct. To address these challenges, we propose a novel UIE method based on spatial-spectral dual-domain adaptive learning, termed SS-UIE. Specifically, we first introduce a spatial-wise Multi-scale Cycle Selective Scan (MCSS) module and a Spectral-Wise Self-Attention (SWSA) module, both with linear complexity, and combine them in parallel to form a basic Spatial-Spectral block (SS-block). Benefiting from the global receptive field of MCSS and SWSA, SS-block can effectively model the degradation levels of different spatial regions and spectral bands, thereby enabling degradation level-based dual-domain adaptive UIE. By stacking multiple SS-blocks, we build our SS-UIE network. Additionally, a Frequency-Wise Loss (FWL) is introduced to narrow the frequency-wise discrepancy and reinforce the model's attention on the regions with high-frequency details. Extensive experiments validate that the SS-UIE technique outperforms state-of-the-art UIE methods while requiring cheaper computational and memory costs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexPara: Flexible Neural Surface Parameterization</title>
<link>https://arxiv.org/abs/2504.19210</link>
<guid>https://arxiv.org/abs/2504.19210</guid>
<content:encoded><![CDATA[
<div> Keywords: Surface parameterization, 3D assets, neural optimization, global parameterization, multi-chart parameterization

Summary:
FlexPara is a novel unsupervised neural optimization framework for surface parameterization, essential in geometry processing. It enables global and multi-chart parameterizations by establishing point-wise mappings between 3D surface points and adaptively-deformed 2D UV coordinates. The framework combines various sub-networks to handle cutting, deforming, unwrapping, and wrapping processes, eliminating the need for manually specified cutting seams. Additionally, it offers a multi-chart parameterization framework with adaptively-learned chart assignment. Experiment results showcase the effectiveness and potential of FlexPara in achieving optimal surface parameterizations for different surface structures and task requirements. The code for FlexPara will be publicly available, facilitating its adoption and further research in the field. <br /><br />Summary: <div>
arXiv:2504.19210v1 Announce Type: new 
Abstract: Surface parameterization is a fundamental geometry processing task, laying the foundations for the visual presentation of 3D assets and numerous downstream shape analysis scenarios. Conventional parameterization approaches demand high-quality mesh triangulation and are restricted to certain simple topologies unless additional surface cutting and decomposition are provided. In practice, the optimal configurations (e.g., type of parameterization domains, distribution of cutting seams, number of mapping charts) may vary drastically with different surface structures and task characteristics, thus requiring more flexible and controllable processing pipelines. To this end, this paper introduces FlexPara, an unsupervised neural optimization framework to achieve both global and multi-chart surface parameterizations by establishing point-wise mappings between 3D surface points and adaptively-deformed 2D UV coordinates. We ingeniously design and combine a series of geometrically-interpretable sub-networks, with specific functionalities of cutting, deforming, unwrapping, and wrapping, to construct a bi-directional cycle mapping framework for global parameterization without the need for manually specified cutting seams. Furthermore, we construct a multi-chart parameterization framework with adaptively-learned chart assignment. Extensive experiments demonstrate the universality, superiority, and inspiring potential of our neural surface parameterization paradigm. The code will be publicly available at https://github.com/AidenZhao/FlexPara
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes</title>
<link>https://arxiv.org/abs/2504.19212</link>
<guid>https://arxiv.org/abs/2504.19212</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake technology, multimodal capsule network, image editing, detection accuracy, adversarial attacks

Summary: 
The article discusses the threat posed by rapidly advancing deepfake technology, specifically in instruction-guided image manipulation. These manipulations, undetectable by humans and existing systems, highlight the need for improved defenses. The proposed solution, CapsFake, is a multimodal capsule network that integrates visual, textual, and frequency-domain features to detect deepfake edits. By utilizing competitive routing mechanisms and high-level capsules, CapsFake achieves superior detection accuracy compared to existing methods. Evaluation on various datasets shows up to a 20% improvement in accuracy. Ablation studies confirm the robustness of CapsFake, with detection rates above 94% under natural perturbations and 96% against adversarial attacks. The model also demonstrates excellent generalization to unseen editing scenarios, establishing a powerful framework for combating sophisticated image manipulations. 

<br /><br />Summary: <div>
arXiv:2504.19212v1 Announce Type: new 
Abstract: The rapid evolution of deepfake technology, particularly in instruction-guided image editing, threatens the integrity of digital images by enabling subtle, context-aware manipulations. Generated conditionally from real images and textual prompts, these edits are often imperceptible to both humans and existing detection systems, revealing significant limitations in current defenses. We propose a novel multimodal capsule network, CapsFake, designed to detect such deepfake image edits by integrating low-level capsules from visual, textual, and frequency-domain modalities. High-level capsules, predicted through a competitive routing mechanism, dynamically aggregate local features to identify manipulated regions with precision. Evaluated on diverse datasets, including MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits, CapsFake outperforms state-of-the-art methods by up to 20% in detection accuracy. Ablation studies validate its robustness, achieving detection rates above 94% under natural perturbations and 96% against adversarial attacks, with excellent generalization to unseen editing scenarios. This approach establishes a powerful framework for countering sophisticated image manipulations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis</title>
<link>https://arxiv.org/abs/2504.19223</link>
<guid>https://arxiv.org/abs/2504.19223</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral imaging, camera-agnostic representation learning, RGB, multispectral, hyperspectral

Summary:<br /><br />
The article introduces CARL, a model for Camera-Agnostic Representation Learning for spectral imaging modalities such as RGB, multispectral, and hyperspectral. This model addresses the issue of variability in channel dimensionality and captured wavelengths among spectral cameras, which hinders the development of AI-driven methodologies. CARL utilizes wavelength positional encoding and a self-attention-cross-attention mechanism to compress spectral information into learned query representations, enabling the conversion of spectral images with any channel dimensionality to a camera-agnostic embedding. Spectral-spatial pre-training is achieved using a novel spectral self-supervised JEPA-inspired strategy tailored to CARL. Large-scale experiments across medical imaging, autonomous driving, and satellite imaging domains demonstrate CARL's robustness to spectral heterogeneity, outperforming on datasets with simulated and real-world cross-camera spectral variations. The proposed approach's scalability and versatility make it a potential backbone for future spectral foundation models. 

Summary: <div>
arXiv:2504.19223v1 Announce Type: new 
Abstract: Spectral imaging offers promising applications across diverse domains, including medicine and urban scene understanding, and is already established as a critical modality in remote sensing. However, variability in channel dimensionality and captured wavelengths among spectral cameras impede the development of AI-driven methodologies, leading to camera-specific models with limited generalizability and inadequate cross-camera applicability. To address this bottleneck, we introduce $\textbf{CARL}$, a model for $\textbf{C}$amera-$\textbf{A}$gnostic $\textbf{R}$epresentation $\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging modalities. To enable the conversion of a spectral image with any channel dimensionality to a camera-agnostic embedding, we introduce wavelength positional encoding and a self-attention-cross-attention mechanism to compress spectral information into learned query representations. Spectral-spatial pre-training is achieved with a novel spectral self-supervised JEPA-inspired strategy tailored to CARL. Large-scale experiments across the domains of medical imaging, autonomous driving, and satellite imaging demonstrate our model's unique robustness to spectral heterogeneity, outperforming on datasets with simulated and real-world cross-camera spectral variations. The scalability and versatility of the proposed approach position our model as a backbone for future spectral foundation models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised 2D-3D lifting of non-rigid objects using local constraints</title>
<link>https://arxiv.org/abs/2504.19227</link>
<guid>https://arxiv.org/abs/2504.19227</guid>
<content:encoded><![CDATA[
<div> predicting 3D shape, non-rigid objects, low-rank constraints, high capacity models, unsupervised loss 
Summary: 
- The challenge of predicting 3D shapes from 2D keypoint observations in non-rigid objects is addressed.
- Specialized models with low-rank constraints have been used but are difficult to train and limited in reconstruction quality.
- Generic, high-capacity models trained with unsupervised loss allow for more accurate predicted shapes.
- Applying low-rank constraints to localized subsets of the full shape helps constrain the high capacity suitably.
- The state-of-the-art reconstruction error on the S-Up3D dataset is reduced by over 70%. 

<br /><br />Summary: <div>
arXiv:2504.19227v1 Announce Type: new 
Abstract: For non-rigid objects, predicting the 3D shape from 2D keypoint observations is ill-posed due to occlusions, and the need to disentangle changes in viewpoint and changes in shape. This challenge has often been addressed by embedding low-rank constraints into specialized models. These models can be hard to train, as they depend on finding a canonical way of aligning observations, before they can learn detailed geometry. These constraints have limited the reconstruction quality. We show that generic, high capacity models, trained with an unsupervised loss, allow for more accurate predicted shapes. In particular, applying low-rank constraints to localized subsets of the full shape allows the high capacity to be suitably constrained. We reduce the state-of-the-art reconstruction error on the S-Up3D dataset by over 70%.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID</title>
<link>https://arxiv.org/abs/2504.19244</link>
<guid>https://arxiv.org/abs/2504.19244</guid>
<content:encoded><![CDATA[
<div> Keywords: Unsupervised person re-identification, Visible-infrared, Fine-grained patterns, Semantic-aligned learning, Collaborative refinement<br />
Summary:<br />
The paper introduces the Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework for unsupervised visible-infrared person re-identification. It addresses the limitations of existing methods by emphasizing fine-grained patterns and cross-modality variations in feature representation. The framework consists of a Dual Association with Global Learning (DAGI) module for unifying cross-modality pseudo-labels, a Fine-Grained Semantic-Aligned Learning (FGSAL) module for exploring part-level semantic-aligned patterns, and a Global-Part Collaborative Refinement (GPCR) module for refining inter-instance relationships. By optimizing specific fine-grained patterns and label distributions of different modalities, SALCR achieves superior performance compared to state-of-the-art methods. The proposed solution effectively mines reliable positive sample sets to mitigate the effects of noisy pseudo-labels. The code for implementing SALCR is available on GitHub for further research and application. <br /><br />Summary: <div>
arXiv:2504.19244v1 Announce Type: new 
Abstract: Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to match pedestrian images of the same individual across different modalities without human annotations for model learning. Previous methods unify pseudo-labels of cross-modality images through label association algorithms and then design contrastive learning framework for global feature learning. However, these methods overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. This insight results in insufficient modality-shared learning when only global features are optimized. To address this issue, we propose a Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework, which builds up optimization objective for specific fine-grained patterns emphasized by each modality, thereby achieving complementary alignment between the label distributions of different modalities. Specifically, we first introduce a Dual Association with Global Learning (DAGI) module to unify the pseudo-labels of cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances. Optimization objective is then formulated based on the semantic-aligned features and their corresponding label space. To alleviate the side-effects arising from noisy pseudo-labels, we propose a Global-Part Collaborative Refinement (GPCR) module to mine reliable positive sample sets for the global and part features dynamically and optimize the inter-instance relationships. Extensive experiments demonstrate the effectiveness of the proposed method, which achieves superior performances to state-of-the-art methods. Our code is available at \href{https://github.com/FranklinLingfeng/code-for-SALCR}.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ODExAI: A Comprehensive Object Detection Explainable AI Evaluation</title>
<link>https://arxiv.org/abs/2504.19249</link>
<guid>https://arxiv.org/abs/2504.19249</guid>
<content:encoded><![CDATA[
<div> Keywords: Object Detection, Explainable Artificial Intelligence, Evaluation, Localization Accuracy, Model Faithfulness

Summary:
Object Detection Explainable AI Evaluation (ODExAI) framework assesses XAI methods in object detection based on localization accuracy, model faithfulness, and computational complexity. The study benchmarks XAI methods on YOLOX and Faster R-CNN with MS-COCO and PASCAL VOC datasets. Region-based methods like D-CLOSE show strong localization and model faithfulness but have high computational overhead. CAM-based methods like G-CAME achieve superior localization with lower runtime but reduced faithfulness. The findings highlight trade-offs among XAI approaches and emphasize the importance of task-specific evaluation in object detection pipelines. The implementation and evaluation benchmarks are publicly available at https://github.com/Analytics-Everywhere-Lab/odexai.

Summary:<br /><br />Object Detection Explainable AI Evaluation (ODExAI) framework evaluates XAI methods in object detection based on localization accuracy, model faithfulness, and computational complexity. Results show trade-offs between region-based methods like D-CLOSE with strong localization and model faithfulness but high computational overhead, and CAM-based methods like G-CAME with superior localization and lower runtime but reduced faithfulness. Task-specific evaluation is necessary for deploying XAI approaches in object detection pipelines. The implementation and evaluation benchmarks are publicly accessible at https://github.com/Analytics-Everywhere-Lab/odexai. <div>
arXiv:2504.19249v1 Announce Type: new 
Abstract: Explainable Artificial Intelligence (XAI) techniques for interpreting object detection models remain in an early stage, with no established standards for systematic evaluation. This absence of consensus hinders both the comparative analysis of methods and the informed selection of suitable approaches. To address this gap, we introduce the Object Detection Explainable AI Evaluation (ODExAI), a comprehensive framework designed to assess XAI methods in object detection based on three core dimensions: localization accuracy, faithfulness to model behavior, and computational complexity. We benchmark a set of XAI methods across two widely used object detectors (YOLOX and Faster R-CNN) and standard datasets (MS-COCO and PASCAL VOC). Empirical results demonstrate that region-based methods (e.g., D-CLOSE) achieve strong localization (PG = 88.49%) and high model faithfulness (OA = 0.863), though with substantial computational overhead (Time = 71.42s). On the other hand, CAM-based methods (e.g., G-CAME) achieve superior localization (PG = 96.13%) and significantly lower runtime (Time = 0.54s), but at the expense of reduced faithfulness (OA = 0.549). These findings demonstrate critical trade-offs among existing XAI approaches and reinforce the need for task-specific evaluation when deploying them in object detection pipelines. Our implementation and evaluation benchmarks are publicly available at: https://github.com/Analytics-Everywhere-Lab/odexai.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LM-MCVT: A Lightweight Multi-modal Multi-view Convolutional-Vision Transformer Approach for 3D Object Recognition</title>
<link>https://arxiv.org/abs/2504.19256</link>
<guid>https://arxiv.org/abs/2504.19256</guid>
<content:encoded><![CDATA[
<div> Convolutional-Vision Transformer, Lightweight, Object Recognition, Robotics, 3D<br />
<br />
Summary: <br />
The paper introduces a novel Lightweight Multi-modal Multi-view Convolutional-Vision Transformer network (LM-MCVT) for improving 3D object recognition in human-centered environments. The approach utilizes the Globally Entropy-based Embeddings Fusion (GEEF) method to effectively integrate multi-views. The LM-MCVT architecture includes pre- and mid-level convolutional encoders, along with local and global transformers, to enhance feature extraction and recognition accuracy. Achieving a recognition accuracy of 95.6% on the synthetic ModelNet40 dataset with a four-view setup, it outperforms existing state-of-the-art methods. Validation on the real-world OmniObject3D dataset through 5-fold cross-validation consistently demonstrates superior performance, highlighting the method's robustness in 3D object recognition across synthetic and real-world 3D data. <div>
arXiv:2504.19256v1 Announce Type: new 
Abstract: In human-centered environments such as restaurants, homes, and warehouses, robots often face challenges in accurately recognizing 3D objects. These challenges stem from the complexity and variability of these environments, including diverse object shapes. In this paper, we propose a novel Lightweight Multi-modal Multi-view Convolutional-Vision Transformer network (LM-MCVT) to enhance 3D object recognition in robotic applications. Our approach leverages the Globally Entropy-based Embeddings Fusion (GEEF) method to integrate multi-views efficiently. The LM-MCVT architecture incorporates pre- and mid-level convolutional encoders and local and global transformers to enhance feature extraction and recognition accuracy. We evaluate our method on the synthetic ModelNet40 dataset and achieve a recognition accuracy of 95.6% using a four-view setup, surpassing existing state-of-the-art methods. To further validate its effectiveness, we conduct 5-fold cross-validation on the real-world OmniObject3D dataset using the same configuration. Results consistently show superior performance, demonstrating the method's robustness in 3D object recognition across synthetic and real-world 3D data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion</title>
<link>https://arxiv.org/abs/2504.19258</link>
<guid>https://arxiv.org/abs/2504.19258</guid>
<content:encoded><![CDATA[
<div> Keywords: LiDAR, place recognition, autonomous navigation, cross-modal localization, OpenStreetMap

Summary:
OPAL is a new network for LiDAR place recognition that utilizes OpenStreetMap as a lightweight and up-to-date prior. The network addresses the domain disparity between sparse LiDAR scans and structured OSM data through a cross-modal visibility mask and adaptive radial fusion module. These components guide feature learning and consolidate multiscale radial features to create discriminative global descriptors. OPAL outperforms existing approaches on the augmented KITTI and KITTI-360 datasets, achieving a 15.98% higher recall at @1m threshold for top-1 retrieved matches and operating at 12x faster inference speeds. The code and datasets are available on GitHub for public access. The integration of LiDAR data with OpenStreetMap enhances the efficiency and accuracy of place recognition in large-scale outdoor environments, contributing to advancements in autonomous navigation and cross-modal localization technologies.

<br /><br />Summary: <div>
arXiv:2504.19258v1 Announce Type: new 
Abstract: LiDAR place recognition is a critical capability for autonomous navigation and cross-modal localization in large-scale outdoor environments. Existing approaches predominantly depend on pre-built 3D dense maps or aerial imagery, which impose significant storage overhead and lack real-time adaptability. In this paper, we propose OPAL, a novel network for LiDAR place recognition that leverages OpenStreetMap as a lightweight and up-to-date prior. Our key innovation lies in bridging the domain disparity between sparse LiDAR scans and structured OSM data through two carefully designed components: a cross-modal visibility mask that identifies maximal observable regions from both modalities to guide feature learning, and an adaptive radial fusion module that dynamically consolidates multiscale radial features into discriminative global descriptors. Extensive experiments on the augmented KITTI and KITTI-360 datasets demonstrate OPAL's superiority, achieving 15.98% higher recall at @1m threshold for top-1 retrieved matches while operating at 12x faster inference speeds compared to state-of-the-art approaches. Code and datasets are publicly available at: https://github.com/WHU-USI3DV/OPAL .
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting</title>
<link>https://arxiv.org/abs/2504.19261</link>
<guid>https://arxiv.org/abs/2504.19261</guid>
<content:encoded><![CDATA[
<div> scene view synthesis, renderability field-guided gaussian splatting, input inhomogeneity, image restoration model, wide-baseline pseudo-views <br />
Summary:
The paper introduces a new approach, renderability field-guided gaussian splatting (RF-GS), for scene view synthesis. This method addresses the challenge of rendering stable images in complex environments with non-uniform observations. RF-GS quantifies input inhomogeneity using a renderability field and guides pseudo-view sampling for improved visual consistency. An image restoration model is trained to enhance the quality of wide-baseline pseudo-views by mapping point projections to visible-light styles. A hybrid data optimization strategy effectively fuses information from pseudo-view angles and source view textures. Comparative experiments on simulated and real-world data demonstrate that RF-GS outperforms existing methods in rendering stability. <div>
arXiv:2504.19261v1 Announce Type: new 
Abstract: Scene view synthesis, which generates novel views from limited perspectives, is increasingly vital for applications like virtual reality, augmented reality, and robotics. Unlike object-based tasks, such as generating 360{\deg} views of a car, scene view synthesis handles entire environments where non-uniform observations pose unique challenges for stable rendering quality. To address this issue, we propose a novel approach: renderability field-guided gaussian splatting (RF-GS). This method quantifies input inhomogeneity through a renderability field, guiding pseudo-view sampling to enhanced visual consistency. To ensure the quality of wide-baseline pseudo-views, we train an image restoration model to map point projections to visible-light styles. Additionally, our validated hybrid data optimization strategy effectively fuses information of pseudo-view angles and source view textures. Comparative experiments on simulated and real-world data show that our method outperforms existing approaches in rendering stability.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenFusion++: An Open-vocabulary Real-time Scene Understanding System</title>
<link>https://arxiv.org/abs/2504.19266</link>
<guid>https://arxiv.org/abs/2504.19266</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time, open-vocabulary, scene understanding, 3D perception, semantic-geometric reconstruction <br />
Summary: <br />
OpenFusion++ introduces a real-time 3D semantic-geometric reconstruction system for open-vocabulary scene understanding. The system enhances 3D point clouds by fusing confidence maps from foundational models, updates global semantic labels dynamically using an adaptive cache based on instance area, and utilizes a dual-path encoding framework integrating object attributes with environmental context for accurate query responses. Experimental results on various datasets demonstrate that OpenFusion++ surpasses the baseline in both semantic accuracy and query responsiveness. The system addresses issues such as imprecise instance segmentation, static semantic updates, and limited handling of complex queries, making it beneficial for applications like vision-language navigation, embodied intelligence, and augmented reality. <div>
arXiv:2504.19266v1 Announce Type: new 
Abstract: Real-time open-vocabulary scene understanding is essential for efficient 3D perception in applications such as vision-language navigation, embodied intelligence, and augmented reality. However, existing methods suffer from imprecise instance segmentation, static semantic updates, and limited handling of complex queries. To address these issues, we present OpenFusion++, a TSDF-based real-time 3D semantic-geometric reconstruction system. Our approach refines 3D point clouds by fusing confidence maps from foundational models, dynamically updates global semantic labels via an adaptive cache based on instance area, and employs a dual-path encoding framework that integrates object attributes with environmental context for precise query responses. Experiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate that OpenFusion++ significantly outperforms the baseline in both semantic accuracy and query responsiveness.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VI3NR: Variance Informed Initialization for Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2504.19270</link>
<guid>https://arxiv.org/abs/2504.19270</guid>
<content:encoded><![CDATA[
arXiv:2504.19270v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs) are a versatile and powerful tool for encoding various forms of data, including images, videos, sound, and 3D shapes. A critical factor in the success of INRs is the initialization of the network, which can significantly impact the convergence and accuracy of the learned model. Unfortunately, commonly used neural network initializations are not widely applicable for many activation functions, especially those used by INRs. In this paper, we improve upon previous initialization methods by deriving an initialization that has stable variance across layers, and applies to any activation function. We show that this generalizes many previous initialization methods, and has even better stability for well studied activations. We also show that our initialization leads to improved results with INR activation functions in multiple signal modalities. Our approach is particularly effective for Gaussian INRs, where we demonstrate that the theory of our initialization matches with task performance in multiple experiments, allowing us to achieve improvements in image, audio, and 3D surface reconstruction.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Multi-Modal Saliency and Fusion for Gaze Target Detection</title>
<link>https://arxiv.org/abs/2504.19271</link>
<guid>https://arxiv.org/abs/2504.19271</guid>
<content:encoded><![CDATA[
arXiv:2504.19271v1 Announce Type: new 
Abstract: Gaze target detection (GTD) is the task of predicting where a person in an image is looking. This is a challenging task, as it requires the ability to understand the relationship between the person's head, body, and eyes, as well as the surrounding environment. In this paper, we propose a novel method for GTD that fuses multiple pieces of information extracted from an image. First, we project the 2D image into a 3D representation using monocular depth estimation. We then extract a depth-infused saliency module map, which highlights the most salient (\textit{attention-grabbing}) regions in image for the subject in consideration. We also extract face and depth modalities from the image, and finally fuse all the extracted modalities to identify the gaze target. We quantitatively evaluated our method, including the ablation analysis on three publicly available datasets, namely VideoAttentionTarget, GazeFollow and GOO-Real, and showed that it outperforms other state-of-the-art methods. This suggests that our method is a promising new approach for GTD.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Hyperspectral Undersampling Strategy for Satellite Imaging</title>
<link>https://arxiv.org/abs/2504.19279</link>
<guid>https://arxiv.org/abs/2504.19279</guid>
<content:encoded><![CDATA[
arXiv:2504.19279v1 Announce Type: new 
Abstract: Hyperspectral image (HSI) classification presents significant challenges due to the high dimensionality, spectral redundancy, and limited labeled data typically available in real-world applications. To address these issues and optimize classification performance, we propose a novel band selection strategy known as Iterative Wavelet-based Gradient Sampling (IWGS). This method incrementally selects the most informative spectral bands by analyzing gradients within the wavelet-transformed domain, enabling efficient and targeted dimensionality reduction. Unlike traditional selection methods, IWGS leverages the multi-resolution properties of wavelets to better capture subtle spectral variations relevant for classification. The iterative nature of the approach ensures that redundant or noisy bands are systematically excluded while maximizing the retention of discriminative features. We conduct comprehensive experiments on two widely-used benchmark HSI datasets: Houston 2013 and Indian Pines. Results demonstrate that IWGS consistently outperforms state-of-the-art band selection and classification techniques in terms of both accuracy and computational efficiency. These improvements make our method especially suitable for deployment in edge devices or other resource-constrained environments, where memory and processing power are limited. In particular, IWGS achieved an overall accuracy up to 97.8% on Indian Pines for selected classes, confirming its effectiveness and generalizability across different HSI scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marine Snow Removal Using Internally Generated Pseudo Ground Truth</title>
<link>https://arxiv.org/abs/2504.19289</link>
<guid>https://arxiv.org/abs/2504.19289</guid>
<content:encoded><![CDATA[
arXiv:2504.19289v1 Announce Type: new 
Abstract: Underwater videos often suffer from degraded quality due to light absorption, scattering, and various noise sources. Among these, marine snow, which is suspended organic particles appearing as bright spots or noise, significantly impacts machine vision tasks, particularly those involving feature matching. Existing methods for removing marine snow are ineffective due to the lack of paired training data. To address this challenge, this paper proposes a novel enhancement framework that introduces a new approach for generating paired datasets from raw underwater videos. The resulting dataset consists of paired images of generated snowy and snow, free underwater videos, enabling supervised training for video enhancement. We describe the dataset creation process, highlight its key characteristics, and demonstrate its effectiveness in enhancing underwater image restoration in the absence of ground truth.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusionNet: Multi-model Linear Fusion Framework for Low-light Image Enhancement</title>
<link>https://arxiv.org/abs/2504.19295</link>
<guid>https://arxiv.org/abs/2504.19295</guid>
<content:encoded><![CDATA[
arXiv:2504.19295v1 Announce Type: new 
Abstract: The advent of Deep Neural Networks (DNNs) has driven remarkable progress in low-light image enhancement (LLIE), with diverse architectures (e.g., CNNs and Transformers) and color spaces (e.g., sRGB, HSV, HVI) yielding impressive results. Recent efforts have sought to leverage the complementary strengths of these paradigms, offering promising solutions to enhance performance across varying degradation scenarios. However, existing fusion strategies are hindered by challenges such as parameter explosion, optimization instability, and feature misalignment, limiting further improvements. To overcome these issues, we introduce FusionNet, a novel multi-model linear fusion framework that operates in parallel to effectively capture global and local features across diverse color spaces. By incorporating a linear fusion strategy underpinned by Hilbert space theoretical guarantees, FusionNet mitigates network collapse and reduces excessive training costs. Our method achieved 1st place in the CVPR2025 NTIRE Low Light Enhancement Challenge. Extensive experiments conducted on synthetic and real-world benchmark datasets demonstrate that the proposed method significantly outperforms state-of-the-art methods in terms of both quantitative and qualitative results, delivering robust enhancement under diverse low-light conditions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Myocardial Region-guided Feature Aggregation Net for Automatic Coronary artery Segmentation and Stenosis Assessment using Coronary Computed Tomography Angiography</title>
<link>https://arxiv.org/abs/2504.19300</link>
<guid>https://arxiv.org/abs/2504.19300</guid>
<content:encoded><![CDATA[
arXiv:2504.19300v1 Announce Type: new 
Abstract: Coronary artery disease (CAD) remains a leading cause of mortality worldwide, requiring accurate segmentation and stenosis detection using Coronary Computed Tomography angiography (CCTA). Existing methods struggle with challenges such as low contrast, morphological variability and small vessel segmentation. To address these limitations, we propose the Myocardial Region-guided Feature Aggregation Net, a novel U-shaped dual-encoder architecture that integrates anatomical prior knowledge to enhance robustness in coronary artery segmentation. Our framework incorporates three key innovations: (1) a Myocardial Region-guided Module that directs attention to coronary regions via myocardial contour expansion and multi-scale feature fusion, (2) a Residual Feature Extraction Encoding Module that combines parallel spatial channel attention with residual blocks to enhance local-global feature discrimination, and (3) a Multi-scale Feature Fusion Module for adaptive aggregation of hierarchical vascular features. Additionally, Monte Carlo dropout f quantifies prediction uncertainty, supporting clinical interpretability. For stenosis detection, a morphology-based centerline extraction algorithm separates the vascular tree into anatomical branches, enabling cross-sectional area quantification and stenosis grading. The superiority of MGFA-Net was demonstrated by achieving an Dice score of 85.04%, an accuracy of 84.24%, an HD95 of 6.1294 mm, and an improvement of 5.46% in true positive rate for stenosis detection compared to3D U-Net. The integrated segmentation-to-stenosis pipeline provides automated, clinically interpretable CAD assessment, bridging deep learning with anatomical prior knowledge for precision medicine. Our code is publicly available at http://github.com/chenzhao2023/MGFA_CCTA
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Platonic Grounding for Efficient Multimodal Language Models</title>
<link>https://arxiv.org/abs/2504.19327</link>
<guid>https://arxiv.org/abs/2504.19327</guid>
<content:encoded><![CDATA[
arXiv:2504.19327v1 Announce Type: new 
Abstract: The hyperscaling of data and parameter count in Transformer-based models is yielding diminishing performance improvement, especially when weighed against training costs. Such plateauing indicates the importance of methods for more efficient finetuning and inference, while retaining similar performance. This is especially relevant for multimodal learning paradigms, where inference costs of processing multimodal tokens can determine the model's practical viability. At the same time, research on representations and mechanistic interpretability has improved our understanding of the inner workings of Transformer-based models; one such line of work reveals an implicit alignment in the deeper layers of pretrained models, across modalities. Taking inspiration from this, we motivate and propose a simple modification to existing multimodal frameworks that rely on aligning pretrained models. We demonstrate that our approach maintains and, in some cases, even improves performance of baseline methods while achieving significant gains in both training and inference-time compute. Our work also has implications for combining pretrained models into larger systems efficiently.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing seeding efficiency using a computer vision system to monitor furrow quality in real-time</title>
<link>https://arxiv.org/abs/2504.19334</link>
<guid>https://arxiv.org/abs/2504.19334</guid>
<content:encoded><![CDATA[
arXiv:2504.19334v1 Announce Type: new 
Abstract: Effective seed sowing in precision agriculture is hindered by challenges such as residue accumulation, low soil temperatures, and hair pinning (crop residue pushed in the trench by furrow opener), which obstruct optimal trench formation. Row cleaners are employed to mitigate these issues, but there is a lack of quantitative methods to assess trench cleanliness. In this study, a novel computer vision-based method was developed to evaluate row cleaner performance. Multiple air seeders were equipped with a video acquisition system to capture trench conditions after row cleaner operation, enabling an effective comparison of the performance of each row cleaner. The captured data were used to develop a segmentation model that analyzed key elements such as soil, straw, and machinery. Using the results from the segmentation model, an objective method was developed to quantify row cleaner performance. The results demonstrated the potential of this method to improve row cleaner selection and enhance seeding efficiency in precision agriculture.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Small Drone Detection Through Multi-Scale Processing and Data Augmentation</title>
<link>https://arxiv.org/abs/2504.19347</link>
<guid>https://arxiv.org/abs/2504.19347</guid>
<content:encoded><![CDATA[
arXiv:2504.19347v1 Announce Type: new 
Abstract: Detecting small drones, often indistinguishable from birds, is crucial for modern surveillance. This work introduces a drone detection methodology built upon the medium-sized YOLOv11 object detection model. To enhance its performance on small targets, we implemented a multi-scale approach in which the input image is processed both as a whole and in segmented parts, with subsequent prediction aggregation. We also utilized a copy-paste data augmentation technique to enrich the training dataset with diverse drone and bird examples. Finally, we implemented a post-processing technique that leverages frame-to-frame consistency to mitigate missed detections. The proposed approach attained a top-3 ranking in the 8th WOSDETC Drone-vsBird Detection Grand Challenge, held at the 2025 International Joint Conference on Neural Networks (IJCNN), showcasing its capability to detect drones in complex environments effectively.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MERA: Multimodal and Multiscale Self-Explanatory Model with Considerably Reduced Annotation for Lung Nodule Diagnosis</title>
<link>https://arxiv.org/abs/2504.19357</link>
<guid>https://arxiv.org/abs/2504.19357</guid>
<content:encoded><![CDATA[
arXiv:2504.19357v1 Announce Type: new 
Abstract: Lung cancer, a leading cause of cancer-related deaths globally, emphasises the importance of early detection for better patient outcomes. Pulmonary nodules, often early indicators of lung cancer, necessitate accurate, timely diagnosis. Despite Explainable Artificial Intelligence (XAI) advances, many existing systems struggle providing clear, comprehensive explanations, especially with limited labelled data. This study introduces MERA, a Multimodal and Multiscale self-Explanatory model designed for lung nodule diagnosis with considerably Reduced Annotation requirements. MERA integrates unsupervised and weakly supervised learning strategies (self-supervised learning techniques and Vision Transformer architecture for unsupervised feature extraction) and a hierarchical prediction mechanism leveraging sparse annotations via semi-supervised active learning in the learned latent space. MERA explains its decisions on multiple levels: model-level global explanations via semantic latent space clustering, instance-level case-based explanations showing similar instances, local visual explanations via attention maps, and concept explanations using critical nodule attributes. Evaluations on the public LIDC dataset show MERA's superior diagnostic accuracy and self-explainability. With only 1% annotated samples, MERA achieves diagnostic accuracy comparable to or exceeding state-of-the-art methods requiring full annotation. The model's inherent design delivers comprehensive, robust, multilevel explanations aligned closely with clinical practice, enhancing trustworthiness and transparency. Demonstrated viability of unsupervised and weakly supervised learning lowers the barrier to deploying diagnostic AI in broader medical domains. Our complete code is open-source available: https://github.com/diku-dk/credanno.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization</title>
<link>https://arxiv.org/abs/2504.19370</link>
<guid>https://arxiv.org/abs/2504.19370</guid>
<content:encoded><![CDATA[
arXiv:2504.19370v1 Announce Type: new 
Abstract: The urging societal demand for fair AI systems has put pressure on the research community to develop predictive models that are not only globally accurate but also meet new fairness criteria, reflecting the lack of disparate mistreatment with respect to sensitive attributes ($\textit{e.g.}$ gender, ethnicity, age). In particular, the variability of the errors made by certain Facial Recognition (FR) systems across specific segments of the population compromises the deployment of the latter, and was judged unacceptable by regulatory authorities. Designing fair FR systems is a very challenging problem, mainly due to the complex and functional nature of the performance measure used in this domain ($\textit{i.e.}$ ROC curves) and because of the huge heterogeneity of the face image datasets usually available for training. In this paper, we propose a novel post-processing approach to improve the fairness of pre-trained FR models by optimizing a regression loss which acts on centroid-based scores. Beyond the computational advantages of the method, we present numerical experiments providing strong empirical evidence of the gain in fairness and of the ability to preserve global accuracy.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumMorph: Generalized Dynamic Human Neural Fields from Few Views</title>
<link>https://arxiv.org/abs/2504.19390</link>
<guid>https://arxiv.org/abs/2504.19390</guid>
<content:encoded><![CDATA[
arXiv:2504.19390v1 Announce Type: new 
Abstract: We introduce HumMorph, a novel generalized approach to free-viewpoint rendering of dynamic human bodies with explicit pose control. HumMorph renders a human actor in any specified pose given a few observed views (starting from just one) in arbitrary poses. Our method enables fast inference as it relies only on feed-forward passes through the model. We first construct a coarse representation of the actor in the canonical T-pose, which combines visual features from individual partial observations and fills missing information using learned prior knowledge. The coarse representation is complemented by fine-grained pixel-aligned features extracted directly from the observed views, which provide high-resolution appearance information. We show that HumMorph is competitive with the state-of-the-art when only a single input view is available, however, we achieve results with significantly better visual quality given just 2 monocular observations. Moreover, previous generalized methods assume access to accurate body shape and pose parameters obtained using synchronized multi-camera setups. In contrast, we consider a more practical scenario where these body parameters are noisily estimated directly from the observed views. Our experimental results demonstrate that our architecture is more robust to errors in the noisy parameters and clearly outperforms the state of the art in this setting.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Arthroscopic Navigation System for Anterior Cruciate Ligament Reconstruction Based on Multi-level Memory Architecture</title>
<link>https://arxiv.org/abs/2504.19398</link>
<guid>https://arxiv.org/abs/2504.19398</guid>
<content:encoded><![CDATA[
arXiv:2504.19398v1 Announce Type: new 
Abstract: This paper presents a dynamic arthroscopic navigation system based on multi-level memory architecture for anterior cruciate ligament (ACL) reconstruction surgery. The system extends our previously proposed markerless navigation method from static image matching to dynamic video sequence tracking. By integrating the Atkinson-Shiffrin memory model's three-level architecture (sensory memory, working memory, and long-term memory), our system maintains continuous tracking of the femoral condyle throughout the surgical procedure, providing stable navigation support even in complex situations involving viewpoint changes, instrument occlusion, and tissue deformation. Unlike existing methods, our system operates in real-time on standard arthroscopic equipment without requiring additional tracking hardware, achieving 25.3 FPS with a latency of only 39.5 ms, representing a 3.5-fold improvement over our previous static system. For extended sequences (1000 frames), the dynamic system maintained an error of 5.3 plus-minus 1.5 pixels, compared to the static system's 12.6 plus-minus 3.7 pixels - an improvement of approximately 45 percent. For medium-length sequences (500 frames) and short sequences (100 frames), the system achieved approximately 35 percent and 19 percent accuracy improvements, respectively. Experimental results demonstrate the system overcomes limitations of traditional static matching methods, providing new technical support for improving surgical precision in ACL reconstruction.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2504.19402</link>
<guid>https://arxiv.org/abs/2504.19402</guid>
<content:encoded><![CDATA[
arXiv:2504.19402v1 Announce Type: new 
Abstract: While the availability of open 3D medical shape datasets is increasing, offering substantial benefits to the research community, we have found that many of these datasets are, unfortunately, disorganized and contain artifacts. These issues limit the development and training of robust models, particularly for accurate 3D reconstruction tasks. In this paper, we examine the current state of available 3D liver shape datasets and propose a solution using diffusion models combined with implicit neural representations (INRs) to augment and expand existing datasets. Our approach utilizes the generative capabilities of diffusion models to create realistic, diverse 3D liver shapes, capturing a wide range of anatomical variations and addressing the problem of data scarcity. Experimental results indicate that our method enhances dataset diversity, providing a scalable solution to improve the accuracy and reliability of 3D liver reconstruction and generation in medical applications. Finally, we suggest that diffusion models can also be applied to other downstream tasks in 3D medical imaging.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability</title>
<link>https://arxiv.org/abs/2504.19414</link>
<guid>https://arxiv.org/abs/2504.19414</guid>
<content:encoded><![CDATA[
arXiv:2504.19414v1 Announce Type: new 
Abstract: The Vision Transformer (ViT) has made significant advancements in computer vision, utilizing self-attention mechanisms to achieve state-of-the-art performance across various tasks, including image classification, object detection, and segmentation. Its architectural flexibility and capabilities have made it a preferred choice among researchers and practitioners. However, the intricate multi-head attention mechanism of ViT presents significant challenges to interpretability, as the underlying prediction process remains opaque. A critical limitation arises from an observation commonly noted in transformer architectures: "Not all attention heads are equally meaningful." Overlooking the relative importance of specific heads highlights the limitations of existing interpretability methods. To address these challenges, we introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel method that quantifies the importance of each attention head using gradient-based scores. These scores are normalized to derive a weighted aggregate attention score, effectively capturing the relative contributions of individual heads. GMAR clarifies the role of each head in the prediction process, enabling more precise interpretability at the head level. Experimental results demonstrate that GMAR consistently outperforms traditional attention rollout techniques. This work provides a practical contribution to transformer-based architectures, establishing a robust framework for enhancing the interpretability of Vision Transformer models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Real-Time Event-Based Normal Flow Estimator</title>
<link>https://arxiv.org/abs/2504.19417</link>
<guid>https://arxiv.org/abs/2504.19417</guid>
<content:encoded><![CDATA[
arXiv:2504.19417v1 Announce Type: new 
Abstract: This paper presents a real-time, asynchronous, event-based normal flow estimator. It follows the same algorithm as Learning Normal Flow Directly From Event Neighborhoods, but with a more optimized implementation. The original method treats event slices as 3D point clouds, encodes each event's local geometry into a fixed-length vector, and uses a multi-layer perceptron to predict normal flow. It constructs representations by multiplying an adjacency matrix with a feature matrix, resulting in quadratic time complexity with respect to the number of events. In contrast, we leverage the fact that event coordinates are integers and reformulate the representation step as a pooling operation. This achieves the same effect as the adjacency matrix but with much lower computational cost. As a result, our method supports real-time normal flow prediction on event cameras. Our estimator uses 1 GB of CUDA memory and runs at 4 million normal flows per second on an RTX 3070, or 6 million per second on an RTX A5000. We release the CUDA implementation along with a Python interface at https://github.com/dhyuan99/VecKM_flow_cpp.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation</title>
<link>https://arxiv.org/abs/2504.19432</link>
<guid>https://arxiv.org/abs/2504.19432</guid>
<content:encoded><![CDATA[
arXiv:2504.19432v1 Announce Type: new 
Abstract: Satellite imagery and maps, as two fundamental data modalities in remote sensing, offer direct observations of the Earth's surface and human-interpretable geographic abstractions, respectively. The task of bidirectional translation between satellite images and maps (BSMT) holds significant potential for applications in urban planning and disaster response. However, this task presents two major challenges: first, the absence of precise pixel-wise alignment between the two modalities substantially complicates the translation process; second, it requires achieving both high-level abstraction of geographic features and high-quality visual synthesis, which further elevates the technical complexity. To address these limitations, we introduce EarthMapper, a novel autoregressive framework for controllable bidirectional satellite-map translation. EarthMapper employs geographic coordinate embeddings to anchor generation, ensuring region-specific adaptability, and leverages multi-scale feature alignment within a geo-conditioned joint scale autoregression (GJSA) process to unify bidirectional translation in a single training cycle. A semantic infusion (SI) mechanism is introduced to enhance feature-level consistency, while a key point adaptive guidance (KPAG) mechanism is proposed to dynamically balance diversity and precision during inference. We further contribute CNSatMap, a large-scale dataset comprising 302,132 precisely aligned satellite-map pairs across 38 Chinese cities, enabling robust benchmarking. Extensive experiments on CNSatMap and the New York dataset demonstrate EarthMapper's superior performance, achieving significant improvements in visual realism, semantic consistency, and structural fidelity over state-of-the-art methods. Additionally, EarthMapper excels in zero-shot tasks like in-painting, out-painting and coordinate-conditional generation, underscoring its versatility.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIP-KOA: Enhancing Knee Osteoarthritis Diagnosis with Multi-Modal Learning and Symmetry-Aware Loss Functions</title>
<link>https://arxiv.org/abs/2504.19443</link>
<guid>https://arxiv.org/abs/2504.19443</guid>
<content:encoded><![CDATA[
arXiv:2504.19443v1 Announce Type: new 
Abstract: Knee osteoarthritis (KOA) is a universal chronic musculoskeletal disorders worldwide, making early diagnosis crucial. Currently, the Kellgren and Lawrence (KL) grading system is widely used to assess KOA severity. However, its high inter-observer variability and subjectivity hinder diagnostic consistency. To address these limitations, automated diagnostic techniques using deep learning have been actively explored in recent years. In this study, we propose a CLIP-based framework (CLIP-KOA) to enhance the consistency and reliability of KOA grade prediction. To achieve this, we introduce a learning approach that integrates image and text information and incorporate Symmetry Loss and Consistency Loss to ensure prediction consistency between the original and flipped images. CLIP-KOA achieves state-of-the-art accuracy of 71.86\% on KOA severity prediction task, and ablation studies show that CLIP-KOA has 2.36\% improvement in accuracy over the standard CLIP model due to our contribution. This study shows a novel direction for data-driven medical prediction not only to improve reliability of fine-grained diagnosis and but also to explore multimodal methods for medical image analysis. Our code is available at https://github.com/anonymized-link.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Language Prompting for Generative Data Augmentation in Few-shot Fashion Style Recognition</title>
<link>https://arxiv.org/abs/2504.19455</link>
<guid>https://arxiv.org/abs/2504.19455</guid>
<content:encoded><![CDATA[
arXiv:2504.19455v1 Announce Type: new 
Abstract: Constructing dataset for fashion style recognition is challenging due to the inherent subjectivity and ambiguity of style concepts. Recent advances in text-to-image models have facilitated generative data augmentation by synthesizing images from labeled data, yet existing methods based solely on class names or reference captions often fail to balance visual diversity and style consistency. In this work, we propose \textbf{Masked Language Prompting (MLP)}, a novel prompting strategy that masks selected words in a reference caption and leverages large language models to generate diverse yet semantically coherent completions. This approach preserves the structural semantics of the original caption while introducing attribute-level variations aligned with the intended style, enabling style-consistent and diverse image generation without fine-tuning. Experimental results on the FashionStyle14 dataset demonstrate that our MLP-based augmentation consistently outperforms class-name and caption-based baselines, validating its effectiveness for fashion style recognition under limited supervision.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video</title>
<link>https://arxiv.org/abs/2504.19475</link>
<guid>https://arxiv.org/abs/2504.19475</guid>
<content:encoded><![CDATA[
arXiv:2504.19475v1 Announce Type: new 
Abstract: Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CasaGPT: Cuboid Arrangement and Scene Assembly for Interior Design</title>
<link>https://arxiv.org/abs/2504.19478</link>
<guid>https://arxiv.org/abs/2504.19478</guid>
<content:encoded><![CDATA[
arXiv:2504.19478v1 Announce Type: new 
Abstract: We present a novel approach for indoor scene synthesis, which learns to arrange decomposed cuboid primitives to represent 3D objects within a scene. Unlike conventional methods that use bounding boxes to determine the placement and scale of 3D objects, our approach leverages cuboids as a straightforward yet highly effective alternative for modeling objects. This allows for compact scene generation while minimizing object intersections. Our approach, coined CasaGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive model to sequentially arrange cuboids, producing physically plausible scenes. By applying rejection sampling during the fine-tuning stage to filter out scenes with object collisions, our model further reduces intersections and enhances scene quality. Additionally, we introduce a refined dataset, 3DFRONT-NC, which eliminates significant noise presented in the original dataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our dataset demonstrate that our approach consistently outperforms the state-of-the-art methods, enhancing the realism of generated scenes, and providing a promising direction for 3D scene synthesis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.19500</link>
<guid>https://arxiv.org/abs/2504.19500</guid>
<content:encoded><![CDATA[
arXiv:2504.19500v1 Announce Type: new 
Abstract: Open-vocabulary 3D scene understanding is pivotal for enhancing physical intelligence, as it enables embodied agents to interpret and interact dynamically within real-world environments. This paper introduces MPEC, a novel Masked Point-Entity Contrastive learning method for open-vocabulary 3D semantic segmentation that leverages both 3D entity-language alignment and point-entity consistency across different point cloud views to foster entity-specific feature representations. Our method improves semantic discrimination and enhances the differentiation of unique instances, achieving state-of-the-art results on ScanNet for open-vocabulary 3D semantic segmentation and demonstrating superior zero-shot scene understanding capabilities. Extensive fine-tuning experiments on 8 datasets, spanning from low-level perception to high-level reasoning tasks, showcase the potential of learned 3D features, driving consistent performance gains across varied 3D scene understanding tasks. Project website: https://mpec-3d.github.io/
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynergyAmodal: Deocclude Anything with Text Control</title>
<link>https://arxiv.org/abs/2504.19506</link>
<guid>https://arxiv.org/abs/2504.19506</guid>
<content:encoded><![CDATA[
arXiv:2504.19506v1 Announce Type: new 
Abstract: Image deocclusion (or amodal completion) aims to recover the invisible regions (\ie, shape and appearance) of occluded instances in images. Despite recent advances, the scarcity of high-quality data that balances diversity, plausibility, and fidelity remains a major obstacle. To address this challenge, we identify three critical elements: leveraging in-the-wild image data for diversity, incorporating human expertise for plausibility, and utilizing generative priors for fidelity. We propose SynergyAmodal, a novel framework for co-synthesizing in-the-wild amodal datasets with comprehensive shape and appearance annotations, which integrates these elements through a tripartite data-human-model collaboration. First, we design an occlusion-grounded self-supervised learning algorithm to harness the diversity of in-the-wild image data, fine-tuning an inpainting diffusion model into a partial completion diffusion model. Second, we establish a co-synthesis pipeline to iteratively filter, refine, select, and annotate the initial deocclusion results of the partial completion diffusion model, ensuring plausibility and fidelity through human expert guidance and prior model constraints. This pipeline generates a high-quality paired amodal dataset with extensive category and scale diversity, comprising approximately 16K pairs. Finally, we train a full completion diffusion model on the synthesized dataset, incorporating text prompts as conditioning signals. Extensive experiments demonstrate the effectiveness of our framework in achieving zero-shot generalization and textual controllability. Our code, dataset, and models will be made publicly available at https://github.com/imlixinyang/SynergyAmodal.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding</title>
<link>https://arxiv.org/abs/2504.19514</link>
<guid>https://arxiv.org/abs/2504.19514</guid>
<content:encoded><![CDATA[
arXiv:2504.19514v1 Announce Type: new 
Abstract: Figure skating, known as the "Art on Ice," is among the most artistic sports, challenging to understand due to its blend of technical elements (like jumps and spins) and overall artistic expression. Existing figure skating datasets mainly focus on single tasks, such as action recognition or scoring, lacking comprehensive annotations for both technical and artistic evaluation. Current sports research is largely centered on ball games, with limited relevance to artistic sports like figure skating. To address this, we introduce FSAnno, a large-scale dataset advancing artistic sports understanding through figure skating. FSAnno includes an open-access training and test dataset, alongside a benchmark dataset, FSBench, for fair model evaluation. FSBench consists of FSBench-Text, with multiple-choice questions and explanations, and FSBench-Motion, containing multimodal data and Question and Answer (QA) pairs, supporting tasks from technical analysis to performance commentary. Initial tests on FSBench reveal significant limitations in existing models' understanding of artistic sports. We hope FSBench will become a key tool for evaluating and enhancing model comprehension of figure skating.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning</title>
<link>https://arxiv.org/abs/2504.19524</link>
<guid>https://arxiv.org/abs/2504.19524</guid>
<content:encoded><![CDATA[
arXiv:2504.19524v1 Announce Type: new 
Abstract: Industrial Anomaly Detection (IAD) is critical for ensuring product quality by identifying defects. Traditional methods such as feature embedding and reconstruction-based approaches require large datasets and struggle with scalability. Existing vision-language models (VLMs) and Multimodal Large Language Models (MLLMs) address some limitations but rely on mask annotations, leading to high implementation costs and false positives. Additionally, industrial datasets like MVTec-AD and VisA suffer from severe class imbalance, with defect samples constituting only 23.8% and 11.1% of total data respectively. To address these challenges, we propose a reward function that dynamically prioritizes rare defect patterns during training to handle class imbalance. We also introduce a mask-free reasoning framework using Chain of Thought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms, enabling anomaly detection directly from raw images without annotated masks. This approach generates interpretable step-by-step explanations for defect localization. Our method achieves state-of-the-art performance, outperforming prior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating mask dependency and reducing costs while providing explainable outputs, this work advances industrial anomaly detection and supports scalable quality control in manufacturing. Code to reproduce the experiment is available at https://github.com/LilaKen/LR-IAD.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Shallow Watermarking</title>
<link>https://arxiv.org/abs/2504.19529</link>
<guid>https://arxiv.org/abs/2504.19529</guid>
<content:encoded><![CDATA[
arXiv:2504.19529v1 Announce Type: new 
Abstract: Recent advances in digital watermarking make use of deep neural networks for message embedding and extraction. They typically follow the ``encoder-noise layer-decoder''-based architecture. By deliberately establishing a differentiable noise layer to simulate the distortion of the watermarked signal, they jointly train the deep encoder and decoder to fit the noise layer to guarantee robustness. As a result, they are usually weak against unknown distortions that are not used in their training pipeline. In this paper, we propose a novel watermarking framework to resist unknown distortions, namely Adversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder that is randomly parameterized and designed to be insensitive to distortions for watermarking extraction. During the watermark embedding, ASW freezes the shallow decoder and adversarially optimizes a host image until its updated version (i.e., the watermarked image) stably triggers the shallow decoder to output the watermark message. During the watermark extraction, it accurately recovers the message from the watermarked image by leveraging the insensitive nature of the shallow decoder against arbitrary distortions. Our ASW is training-free, encoder-free, and noise layer-free. Experiments indicate that the watermarked images created by ASW have strong robustness against various unknown distortions. Compared to the existing ``encoder-noise layer-decoder'' approaches, ASW achieves comparable results on known distortions and better robustness on unknown distortions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction</title>
<link>https://arxiv.org/abs/2504.19545</link>
<guid>https://arxiv.org/abs/2504.19545</guid>
<content:encoded><![CDATA[
arXiv:2504.19545v1 Announce Type: new 
Abstract: Quad meshes are essential in geometric modeling and computational mechanics. Although learning-based methods for triangle mesh demonstrate considerable advancements, quad mesh generation remains less explored due to the challenge of ensuring coplanarity, convexity, and quad-only meshes. In this paper, we present Point2Quad, the first learning-based method for quad-only mesh generation from point clouds. The key idea is learning to identify quad mesh with fused pointwise and facewise features. Specifically, Point2Quad begins with a k-NN-based candidate generation considering the coplanarity and squareness. Then, two encoders are followed to extract geometric and topological features that address the challenge of quad-related constraints, especially by combining in-depth quadrilaterals-specific characteristics. Subsequently, the extracted features are fused to train the classifier with a designed compound loss. The final results are derived after the refinement by a quad-specific post-processing. Extensive experiments on both clear and noise data demonstrate the effectiveness and superiority of Point2Quad, compared to baseline methods under comprehensive metrics.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crowd Detection Using Very-Fine-Resolution Satellite Imagery</title>
<link>https://arxiv.org/abs/2504.19546</link>
<guid>https://arxiv.org/abs/2504.19546</guid>
<content:encoded><![CDATA[
arXiv:2504.19546v1 Announce Type: new 
Abstract: Accurate crowd detection (CD) is critical for public safety and historical pattern analysis, yet existing methods relying on ground and aerial imagery suffer from limited spatio-temporal coverage. The development of very-fine-resolution (VFR) satellite sensor imagery (e.g., ~0.3 m spatial resolution) provides unprecedented opportunities for large-scale crowd activity analysis, but it has never been considered for this task. To address this gap, we proposed CrowdSat-Net, a novel point-based convolutional neural network, which features two innovative components: Dual-Context Progressive Attention Network (DCPAN) to improve feature representation of individuals by aggregating scene context and local individual characteristics, and High-Frequency Guided Deformable Upsampler (HFGDU) that recovers high-frequency information during upsampling through frequency-domain guided deformable convolutions. To validate the effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR satellite imagery dataset designed specifically for CD tasks, comprising over 120k manually labeled individuals from multi-source satellite platforms (Beijing-3N, Jilin-1 Gaofen-04A and Google Earth) across China. In the experiments, CrowdSat-Net was compared with five state-of-the-art point-based CD methods (originally designed for ground or aerial imagery) using CrowdSat and achieved the largest F1-score of 66.12% and Precision of 73.23%, surpassing the second-best method by 1.71% and 2.42%, respectively. Moreover, extensive ablation experiments validated the importance of the DCPAN and HFGDU modules. Furthermore, cross-regional evaluation further demonstrated the spatial generalizability of CrowdSat-Net. This research advances CD capability by providing both a newly developed network architecture for CD and a pioneering benchmark dataset to facilitate future CD development.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEEMO: De-identity Multimodal Emotion Recognition and Reasoning</title>
<link>https://arxiv.org/abs/2504.19549</link>
<guid>https://arxiv.org/abs/2504.19549</guid>
<content:encoded><![CDATA[
arXiv:2504.19549v1 Announce Type: new 
Abstract: Emotion understanding is a critical yet challenging task. Most existing approaches rely heavily on identity-sensitive information, such as facial expressions and speech, which raises concerns about personal privacy. To address this, we introduce the De-identity Multimodal Emotion Recognition and Reasoning (DEEMO), a novel task designed to enable emotion understanding using de-identified video and audio inputs. The DEEMO dataset consists of two subsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body Language (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion Recognition and Reasoning using identity-free cues. This design supports emotion understanding without compromising identity privacy. In addition, we propose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates de-identified audio, video, and textual information to enhance both emotion recognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves state-of-the-art performance on both tasks, outperforming existing MLLMs by a significant margin, achieving 74.49% accuracy and 74.45% F1-score in de-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap in de-identity emotion reasoning. Our work contributes to ethical AI by advancing privacy-preserving emotion understanding and promoting responsible affective computing.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes</title>
<link>https://arxiv.org/abs/2504.19557</link>
<guid>https://arxiv.org/abs/2504.19557</guid>
<content:encoded><![CDATA[
arXiv:2504.19557v1 Announce Type: new 
Abstract: Current point-based approaches encounter limitations in scalability and rendering quality when using large 3D point cloud maps because using them directly for novel view synthesis (NVS) leads to degraded visualizations. We identify the primary issue behind these low-quality renderings as a visibility mismatch between geometry and appearance, stemming from using these two modalities together. To address this problem, we present CE-NPBG, a new approach for novel view synthesis (NVS) in large-scale autonomous driving scenes. Our method is a neural point-based technique that leverages two modalities: posed images (cameras) and synchronized raw 3D point clouds (LiDAR). We first employ a connectivity relationship graph between appearance and geometry, which retrieves points from a large 3D point cloud map observed from the current camera perspective and uses them for rendering. By leveraging this connectivity, our method significantly improves rendering quality and enhances run-time and scalability by using only a small subset of points from the large 3D point cloud map. Our approach associates neural descriptors with the points and uses them to synthesize views. To enhance the encoding of these descriptors and elevate rendering quality, we propose a joint adversarial and point rasterization training. During training, we pair an image-synthesizer network with a multi-resolution discriminator. At inference, we decouple them and use the image-synthesizer to generate novel views. We also integrate our proposal into the recent 3D Gaussian Splatting work to highlight its benefits for improved rendering and scalability.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Category-Level and Open-Set Object Pose Estimation for Robotics</title>
<link>https://arxiv.org/abs/2504.19572</link>
<guid>https://arxiv.org/abs/2504.19572</guid>
<content:encoded><![CDATA[
arXiv:2504.19572v1 Announce Type: new 
Abstract: Object pose estimation enables a variety of tasks in computer vision and robotics, including scene understanding and robotic grasping. The complexity of a pose estimation task depends on the unknown variables related to the target object. While instance-level methods already excel for opaque and Lambertian objects, category-level and open-set methods, where texture, shape, and size are partially or entirely unknown, still struggle with these basic material properties. Since texture is unknown in these scenarios, it cannot be used for disambiguating object symmetries, another core challenge of 6D object pose estimation. The complexity of estimating 6D poses with such a manifold of unknowns led to various datasets, accuracy metrics, and algorithmic solutions. This paper compares datasets, accuracy metrics, and algorithms for solving 6D pose estimation on the category-level. Based on this comparison, we analyze how to bridge category-level and open-set object pose estimation to reach generalization and provide actionable recommendations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DG-DETR: Toward Domain Generalized Detection Transformer</title>
<link>https://arxiv.org/abs/2504.19574</link>
<guid>https://arxiv.org/abs/2504.19574</guid>
<content:encoded><![CDATA[
arXiv:2504.19574v1 Announce Type: new 
Abstract: End-to-end Transformer-based detectors (DETRs) have demonstrated strong detection performance. However, domain generalization (DG) research has primarily focused on convolutional neural network (CNN)-based detectors, while paying little attention to enhancing the robustness of DETRs. In this letter, we introduce a Domain Generalized DEtection TRansformer (DG-DETR), a simple, effective, and plug-and-play method that improves out-of-distribution (OOD) robustness for DETRs. Specifically, we propose a novel domain-agnostic query selection strategy that removes domain-induced biases from object queries via orthogonal projection onto the instance-specific style space. Additionally, we leverage a wavelet decomposition to disentangle features into domain-invariant and domain-specific components, enabling synthesis of diverse latent styles while preserving the semantic features of objects. Experimental results validate the effectiveness of DG-DETR. Our code is available at https://github.com/sminhwang/DG-DETR.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity</title>
<link>https://arxiv.org/abs/2504.19581</link>
<guid>https://arxiv.org/abs/2504.19581</guid>
<content:encoded><![CDATA[
arXiv:2504.19581v1 Announce Type: new 
Abstract: Driven by the increasing demand for accurate and efficient representation of 3D data in various domains, point cloud sampling has emerged as a pivotal research topic in 3D computer vision. Recently, learning-to-sample methods have garnered growing interest from the community, particularly for their ability to be jointly trained with downstream tasks. However, previous learning-based sampling methods either lead to unrecognizable sampling patterns by generating a new point cloud or biased sampled results by focusing excessively on sharp edge details. Moreover, they all overlook the natural variations in point distribution across different shapes, applying a similar sampling strategy to all point clouds. In this paper, we propose a Sparse Attention Map and Bin-based Learning method (termed SAMBLE) to learn shape-specific sampling strategies for point cloud shapes. SAMBLE effectively achieves an improved balance between sampling edge points for local details and preserving uniformity in the global shape, resulting in superior performance across multiple common point cloud downstream tasks, even in scenarios with few-point sampling.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShowMak3r: Compositional TV Show Reconstruction</title>
<link>https://arxiv.org/abs/2504.19584</link>
<guid>https://arxiv.org/abs/2504.19584</guid>
<content:encoded><![CDATA[
arXiv:2504.19584v1 Announce Type: new 
Abstract: Reconstructing dynamic radiance fields from video clips is challenging, especially when entertainment videos like TV shows are given. Many challenges make the reconstruction difficult due to (1) actors occluding with each other and having diverse facial expressions, (2) cluttered stages, and (3) small baseline views or sudden shot changes. To address these issues, we present ShowMak3r, a comprehensive reconstruction pipeline that allows the editing of scenes like how video clips are made in a production control room. In ShowMak3r, a 3DLocator module locates recovered actors on the stage using depth prior and estimates unseen human poses via interpolation. The proposed ShotMatcher module then tracks the actors under shot changes. Furthermore, ShowMak3r introduces a face-fitting network that dynamically recovers the actors' expressions. Experiments on Sitcoms3D dataset show that our pipeline can reassemble TV show scenes with new cameras at different timestamps. We also demonstrate that ShowMak3r enables interesting applications such as synthetic shot-making, actor relocation, insertion, deletion, and pose manipulation. Project page : https://nstar1125.github.io/showmak3r
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Magnifier: A Multi-grained Neural Network-based Architecture for Burned Area Delineation</title>
<link>https://arxiv.org/abs/2504.19589</link>
<guid>https://arxiv.org/abs/2504.19589</guid>
<content:encoded><![CDATA[
arXiv:2504.19589v1 Announce Type: new 
Abstract: In crisis management and remote sensing, image segmentation plays a crucial role, enabling tasks like disaster response and emergency planning by analyzing visual data. Neural networks are able to analyze satellite acquisitions and determine which areas were affected by a catastrophic event. The problem in their development in this context is the data scarcity and the lack of extensive benchmark datasets, limiting the capabilities of training large neural network models. In this paper, we propose a novel methodology, namely Magnifier, to improve segmentation performance with limited data availability. The Magnifier methodology is applicable to any existing encoder-decoder architecture, as it extends a model by merging information at different contextual levels through a dual-encoder approach: a local and global encoder. Magnifier analyzes the input data twice using the dual-encoder approach. In particular, the local and global encoders extract information from the same input at different granularities. This allows Magnifier to extract more information than the other approaches given the same set of input images. Magnifier improves the quality of the results of +2.65% on average IoU while leading to a restrained increase in terms of the number of trainable parameters compared to the original model. We evaluated our proposed approach with state-of-the-art burned area segmentation models, demonstrating, on average, comparable or better performances in less than half of the GFLOPs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural network task specialization via domain constraining</title>
<link>https://arxiv.org/abs/2504.19592</link>
<guid>https://arxiv.org/abs/2504.19592</guid>
<content:encoded><![CDATA[
arXiv:2504.19592v1 Announce Type: new 
Abstract: This paper introduces a concept of neural network specialization via task-specific domain constraining, aimed at enhancing network performance on data subspace in which the network operates. The study presents experiments on training specialists for image classification and object detection tasks. The results demonstrate that specialization can enhance a generalist's accuracy even without additional data or changing training regimes: solely by constraining class label space in which the network performs. Theoretical and experimental analyses indicate that effective specialization requires modifying traditional fine-tuning methods and constraining data space to semantically coherent subsets. The specialist extraction phase before tuning the network is proposed for maximal performance gains. We also provide analysis of the evolution of the feature space during specialization. This study paves way to future research for developing more advanced dynamically configurable image analysis systems, where computations depend on the specific input. Additionally, the proposed methods can help improve system performance in scenarios where certain data domains should be excluded from consideration of the generalist network.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2504.19598</link>
<guid>https://arxiv.org/abs/2504.19598</guid>
<content:encoded><![CDATA[
arXiv:2504.19598v1 Announce Type: new 
Abstract: Deep learning methods have shown promising performances in remote sensing image change detection (CD). However, existing methods usually train a dataset-specific deep network for each dataset. Due to the significant differences in the data distribution and labeling between various datasets, the trained dataset-specific deep network has poor generalization performances on other datasets. To solve this problem, this paper proposes a change adapter network (CANet) for a more universal and generalized CD. CANet contains dataset-shared and dataset-specific learning modules. The former explores the discriminative features of images, and the latter designs a lightweight adapter model, to deal with the characteristics of different datasets in data distribution and labeling. The lightweight adapter can quickly generalize the deep network for new CD tasks with a small computation cost. Specifically, this paper proposes an interesting change region mask (ICM) in the adapter, which can adaptively focus on interested change objects and decrease the influence of labeling differences in various datasets. Moreover, CANet adopts a unique batch normalization layer for each dataset to deal with data distribution differences. Compared with existing deep learning methods, CANet can achieve satisfactory CD performances on various datasets simultaneously. Experimental results on several public datasets have verified the effectiveness and advantages of the proposed CANet on CD. CANet has a stronger generalization ability, smaller training costs (merely updating 4.1%-7.7% parameters), and better performances under limited training datasets than other deep learning methods, which also can be flexibly inserted with existing deep models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Generation Method Based on Heat Diffusion Models</title>
<link>https://arxiv.org/abs/2504.19600</link>
<guid>https://arxiv.org/abs/2504.19600</guid>
<content:encoded><![CDATA[
arXiv:2504.19600v1 Announce Type: new 
Abstract: Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image generation without adversarial training, but they process images as a whole. Since adjacent pixels are highly likely to belong to the same object, we propose the Heat Diffusion Model (HDM) to further preserve image details and generate more realistic images. HDM is a model that incorporates pixel-level operations while maintaining the same training process as DDPM. In HDM, the discrete form of the two-dimensional heat equation is integrated into the diffusion and generation formulas of DDPM, enabling the model to compute relationships between neighboring pixels during image processing. Our experiments demonstrate that HDM can generate higher-quality samples compared to models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion Models (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN).
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer</title>
<link>https://arxiv.org/abs/2504.19614</link>
<guid>https://arxiv.org/abs/2504.19614</guid>
<content:encoded><![CDATA[
arXiv:2504.19614v1 Announce Type: new 
Abstract: Collecting multi-view driving scenario videos to enhance the performance of 3D visual perception tasks presents significant challenges and incurs substantial costs, making generative models for realistic data an appealing alternative. Yet, the videos generated by recent works suffer from poor quality and spatiotemporal consistency, undermining their utility in advancing perception tasks under driving scenarios. To address this gap, we propose DiVE, a diffusion transformer-based generative framework meticulously engineered to produce high-fidelity, temporally coherent, and cross-view consistent multi-view videos, aligning seamlessly with bird's-eye view layouts and textual descriptions. DiVE leverages a unified cross-attention and a SketchFormer to exert precise control over multimodal data, while incorporating a view-inflated attention mechanism that adds no extra parameters, thereby guaranteeing consistency across views. Despite these advancements, synthesizing high-resolution videos under multimodal constraints introduces dual challenges: investigating the optimal classifier-free guidance coniguration under intricate multi-condition inputs and mitigating excessive computational latency in high-resolution rendering--both of which remain underexplored in prior researches. To resolve these limitations, we introduce two innovations: Multi-Control Auxiliary Branch Distillation, which streamlines multi-condition CFG selection while circumventing high computational overhead, and Resolution Progressive Sampling, a training-free acceleration strategy that staggers resolution scaling to reduce high latency due to high resolution. These innovations collectively achieve a 2.62x speedup with minimal quality degradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance in multi-view video generation, yielding photorealistic outputs with exceptional temporal and cross-view coherence.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NSegment : Noisy Segment Improves Remote Sensing Image Segmentation</title>
<link>https://arxiv.org/abs/2504.19634</link>
<guid>https://arxiv.org/abs/2504.19634</guid>
<content:encoded><![CDATA[
arXiv:2504.19634v1 Announce Type: new 
Abstract: Labeling errors in remote sensing (RS) image segmentation datasets often remain implicit and subtle due to ambiguous class boundaries, mixed pixels, shadows, complex terrain features, and subjective annotator bias. Furthermore, the scarcity of annotated RS data due to high image acquisition and labeling costs complicates training noise-robust models. While sophisticated mechanisms such as label selection or noise correction might address this issue, they tend to increase training time and add implementation complexity. In this letter, we propose NSegment-a simple yet effective data augmentation solution to mitigate this issue. Unlike traditional methods, it applies elastic transformations only to segmentation labels, varying deformation intensity per sample in each training epoch to address annotation inconsistencies. Experimental results demonstrate that our approach improves the performance of RS image segmentation on various state-of-the-art models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Inter-Sample Correlation and Intra-Sample Redundancy for Partially Relevant Video Retrieval</title>
<link>https://arxiv.org/abs/2504.19637</link>
<guid>https://arxiv.org/abs/2504.19637</guid>
<content:encoded><![CDATA[
arXiv:2504.19637v1 Announce Type: new 
Abstract: Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video that is partially relevant to the text query. The primary challenge in PRVR arises from the semantic asymmetry between textual and visual modalities, as videos often contain substantial content irrelevant to the query. Existing methods coarsely align paired videos and text queries to construct the semantic space, neglecting the critical cross-modal dual nature inherent in this task: inter-sample correlation and intra-sample redundancy. To this end, we propose a novel PRVR framework to systematically exploit these two characteristics. Our framework consists of three core modules. First, the Inter Correlation Enhancement (ICE) module captures inter-sample correlation by identifying semantically similar yet unpaired text queries and video moments, combining them to form pseudo-positive pairs for more robust semantic space construction. Second, the Intra Redundancy Mining (IRM) module mitigates intra-sample redundancy by mining redundant video moment features and treating them as hard negative samples, thereby encouraging the model to learn more discriminative representations. Finally, to reinforce these modules, we introduce the Temporal Coherence Prediction (TCP) module, which enhances feature discrimination by training the model to predict the original temporal order of randomly shuffled video frames and moments. Extensive experiments on three datasets demonstrate the superiority of our approach compared to previous methods, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BARIS: Boundary-Aware Refinement with Environmental Degradation Priors for Robust Underwater Instance Segmentation</title>
<link>https://arxiv.org/abs/2504.19643</link>
<guid>https://arxiv.org/abs/2504.19643</guid>
<content:encoded><![CDATA[
arXiv:2504.19643v1 Announce Type: new 
Abstract: Underwater instance segmentation is challenging due to adverse visual conditions such as light attenuation, scattering, and color distortion, which degrade model performance. In this work, we propose BARIS-Decoder (Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that enhances segmentation accuracy through feature refinement. To address underwater degradations, we introduce the Environmental Robust Adapter (ERA), which efficiently models underwater degradation patterns while reducing trainable parameters by over 90\% compared to full fine-tuning. The integration of BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves state-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B backbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the effectiveness of BARIS-ERA in advancing underwater instance segmentation, providing a robust and efficient solution.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xEdgeFace: Efficient Cross-Spectral Face Recognition for Edge Devices</title>
<link>https://arxiv.org/abs/2504.19646</link>
<guid>https://arxiv.org/abs/2504.19646</guid>
<content:encoded><![CDATA[
arXiv:2504.19646v1 Announce Type: new 
Abstract: Heterogeneous Face Recognition (HFR) addresses the challenge of matching face images across different sensing modalities, such as thermal to visible or near-infrared to visible, expanding the applicability of face recognition systems in real-world, unconstrained environments. While recent HFR methods have shown promising results, many rely on computation-intensive architectures, limiting their practicality for deployment on resource-constrained edge devices. In this work, we present a lightweight yet effective HFR framework by adapting a hybrid CNN-Transformer architecture originally designed for face recognition. Our approach enables efficient end-to-end training with minimal paired heterogeneous data while preserving strong performance on standard RGB face recognition tasks. This makes it a compelling solution for both homogeneous and heterogeneous scenarios. Extensive experiments across multiple challenging HFR and face recognition benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches while maintaining a low computational overhead.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification</title>
<link>https://arxiv.org/abs/2504.19682</link>
<guid>https://arxiv.org/abs/2504.19682</guid>
<content:encoded><![CDATA[
arXiv:2504.19682v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as an efficient alternative to convolutional approaches for vision tasks such as image classification, leveraging patch-based representations instead of raw pixels. These methods construct graphs where image patches serve as nodes, and edges are established based on patch similarity or classification relevance. Despite their efficiency, the explainability of GNN-based vision models remains underexplored, even though graphs are naturally interpretable. In this work, we analyze the semantic consistency of the graphs formed at different layers of GNN-based image classifiers, focusing on how well they preserve object structures and meaningful relationships. A comprehensive analysis is presented by quantifying the extent to which inter-layer graph connections reflect semantic similarity and spatial coherence. Explanations from standard and adversarial settings are also compared to assess whether they reflect the classifiers' robustness. Additionally, we visualize the flow of information across layers through heatmap-based visualization techniques, thereby highlighting the models' explainability. Our findings demonstrate that the decision-making processes of these models can be effectively explained, while also revealing that their reasoning does not necessarily align with human perception, especially in deeper layers.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery</title>
<link>https://arxiv.org/abs/2504.19684</link>
<guid>https://arxiv.org/abs/2504.19684</guid>
<content:encoded><![CDATA[
arXiv:2504.19684v1 Announce Type: new 
Abstract: Accurate weather classification from low-quality traffic camera imagery remains a challenging task, particularly under adverse nighttime conditions. In this study, we propose a scalable framework that combines generative domain adaptation with efficient contrastive learning to enhance classification performance. Using CycleGAN-based domain translation, we improve the quality of nighttime images, enabling better feature extraction by downstream models. While the baseline EVA-02 model employing CLIP-based contrastive loss achieves an overall accuracy of 96.55\%, it exhibits a significant performance gap between daytime (97.21\%) and nighttime conditions (63.40\%). Replacing CLIP with the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive overall accuracy of 94.00\%, with substantial improvements in nighttime performance (85.90\% accuracy). The combination of Vision-SigLIP-2, Text-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime accuracy (85.90\%) among all models tested, while EVA-02 with CycleGAN maintains the highest overall accuracy (97.01\%) and per-class accuracies. These findings demonstrate the potential of combining domain adaptation and efficient contrastive learning to build practical, resource-efficient weather classification systems for intelligent transportation infrastructure.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network for Low-Dose CT MAR</title>
<link>https://arxiv.org/abs/2504.19687</link>
<guid>https://arxiv.org/abs/2504.19687</guid>
<content:encoded><![CDATA[
arXiv:2504.19687v1 Announce Type: new 
Abstract: Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it will potentially degrade image quality, even yields metal artifacts at the case of metallic implants. For simultaneous LDCT reconstruction and metal artifact reduction (LDMAR), existing deep learning-based efforts face two main limitations: i) the network design neglects multi-scale and within-scale information; ii) training a distinct model for each dose necessitates significant storage space for multiple doses. To fill these gaps, we propose a prompt guiding multi-scale adaptive sparse representation-driven network, abbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet inspired from multi-scale sparsifying frames, and it can simultaneously employ within-scale characteristics and cross-scale complementarity owing to an elaborated prompt guiding scale-adaptive threshold generator (PSATG) and a built multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively capture multiple contextual information to generate more faithful thresholds, achieved by fusing features from local, regional, and global levels. Furthermore, we elaborate a model interpretable dual domain LDMAR framework called PDuMSRNet, and train single model with a prompt guiding strategy for multiple dose levels. We build a prompt guiding module, whose input contains dose level, metal mask and input instance, to provide various guiding information, allowing a single model to accommodate various CT dose settings. Extensive experiments at various dose levels demonstrate that the proposed methods outperform the state-of-the-art LDMAR methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SubGrapher: Visual Fingerprinting of Chemical Structures</title>
<link>https://arxiv.org/abs/2504.19695</link>
<guid>https://arxiv.org/abs/2504.19695</guid>
<content:encoded><![CDATA[
arXiv:2504.19695v1 Announce Type: new 
Abstract: Automatic extraction of chemical structures from scientific literature plays a crucial role in accelerating research across fields ranging from drug discovery to materials science. Patent documents, in particular, contain molecular information in visual form, which is often inaccessible through traditional text-based searches. In this work, we introduce SubGrapher, a method for the visual fingerprinting of chemical structure images. Unlike conventional Optical Chemical Structure Recognition (OCSR) models that attempt to reconstruct full molecular graphs, SubGrapher focuses on extracting molecular fingerprints directly from chemical structure images. Using learning-based instance segmentation, SubGrapher identifies functional groups and carbon backbones, constructing a substructure-based fingerprint that enables chemical structure retrieval. Our approach is evaluated against state-of-the-art OCSR and fingerprinting methods, demonstrating superior retrieval performance and robustness across diverse molecular depictions. The dataset, models, and code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-set Anomaly Segmentation in Complex Scenarios</title>
<link>https://arxiv.org/abs/2504.19706</link>
<guid>https://arxiv.org/abs/2504.19706</guid>
<content:encoded><![CDATA[
arXiv:2504.19706v1 Announce Type: new 
Abstract: Precise segmentation of out-of-distribution (OoD) objects, herein referred to as anomalies, is crucial for the reliable deployment of semantic segmentation models in open-set, safety-critical applications, such as autonomous driving. Current anomalous segmentation benchmarks predominantly focus on favorable weather conditions, resulting in untrustworthy evaluations that overlook the risks posed by diverse meteorological conditions in open-set environments, such as low illumination, dense fog, and heavy rain. To bridge this gap, this paper introduces the ComsAmy, a challenging benchmark specifically designed for open-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide spectrum of adverse weather conditions, dynamic driving environments, and diverse anomaly types to comprehensively evaluate the model performance in realistic open-world scenarios. Our extensive evaluation of several state-of-the-art anomalous segmentation models reveals that existing methods demonstrate significant deficiencies in such challenging scenarios, highlighting their serious safety risks for real-world deployment. To solve that, we propose a novel energy-entropy learning (EEL) strategy that integrates the complementary information from energy and entropy to bolster the robustness of anomaly segmentation under complex open-world environments. Additionally, a diffusion-based anomalous training data synthesizer is proposed to generate diverse and high-quality anomalous images to enhance the existing copy-paste training data synthesizer. Extensive experimental results on both public and ComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer with energy and entropy learning (DiffEEL) serves as an effective and generalizable plug-and-play method to enhance existing models, yielding an average improvement of around 4.96% in $\rm{AUPRC}$ and 9.87% in $\rm{FPR}_{95}$.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A computer vision method to estimate ventilation rate of Atlantic salmon in sea fish farms</title>
<link>https://arxiv.org/abs/2504.19719</link>
<guid>https://arxiv.org/abs/2504.19719</guid>
<content:encoded><![CDATA[
arXiv:2504.19719v1 Announce Type: new 
Abstract: The increasing demand for aquaculture production necessitates the development of innovative, intelligent tools to effectively monitor and manage fish health and welfare. While non-invasive video monitoring has become a common practice in finfish aquaculture, existing intelligent monitoring methods predominantly focus on assessing body condition or fish swimming patterns and are often developed and evaluated in controlled tank environments, without demonstrating their applicability to real-world aquaculture settings in open sea farms. This underscores the necessity for methods that can monitor physiological traits directly within the production environment of sea fish farms. To this end, we have developed a computer vision method for monitoring ventilation rates of Atlantic salmon (Salmo salar), which was specifically designed for videos recorded in the production environment of commercial sea fish farms using the existing infrastructure. Our approach uses a fish head detection model, which classifies the mouth state as either open or closed using a convolutional neural network. This is followed with multiple object tracking to create temporal sequences of fish swimming across the field of view of the underwater video camera to estimate ventilation rates. The method demonstrated high efficiency, achieving a Pearson correlation coefficient of 0.82 between ground truth and predicted ventilation rates in a test set of 100 fish collected independently of the training data. By accurately identifying pens where fish exhibit signs of respiratory distress, our method offers broad applicability and the potential to transform fish health and welfare monitoring in finfish aquaculture.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The ATLAS of Traffic Lights: A Reliable Perception Framework for Autonomous Driving</title>
<link>https://arxiv.org/abs/2504.19722</link>
<guid>https://arxiv.org/abs/2504.19722</guid>
<content:encoded><![CDATA[
arXiv:2504.19722v1 Announce Type: new 
Abstract: Traffic light perception is an essential component of the camera-based perception system for autonomous vehicles, enabling accurate detection and interpretation of traffic lights to ensure safe navigation through complex urban environments. In this work, we propose a modularized perception framework that integrates state-of-the-art detection models with a novel real-time association and decision framework, enabling seamless deployment into an autonomous driving stack. To address the limitations of existing public datasets, we introduce the ATLAS dataset, which provides comprehensive annotations of traffic light states and pictograms across diverse environmental conditions and camera setups. This dataset is publicly available at https://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art traffic light detection architectures on ATLAS, demonstrating significant performance improvements in both accuracy and robustness. Finally, we evaluate the framework in real-world scenarios by deploying it in an autonomous vehicle to make decisions at traffic light-controlled intersections, highlighting its reliability and effectiveness for real-time operation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepText: Rendering Visual Text via Replicating</title>
<link>https://arxiv.org/abs/2504.19724</link>
<guid>https://arxiv.org/abs/2504.19724</guid>
<content:encoded><![CDATA[
arXiv:2504.19724v1 Announce Type: new 
Abstract: Although contemporary text-to-image generation models have achieved remarkable breakthroughs in producing visually appealing images, their capacity to generate precise and flexible typographic elements, especially non-Latin alphabets, remains constrained. To address these limitations, we start from an naive assumption that text understanding is only a sufficient condition for text rendering, but not a necessary condition. Based on this, we present RepText, which aims to empower pre-trained monolingual text-to-image generation models with the ability to accurately render, or more precisely, replicate, multilingual visual text in user-specified fonts, without the need to really understand them. Specifically, we adopt the setting from ControlNet and additionally integrate language agnostic glyph and position of rendered text to enable generating harmonized visual text, allowing users to customize text content, font and position on their needs. To improve accuracy, a text perceptual loss is employed along with the diffusion loss. Furthermore, to stabilize rendering process, at the inference phase, we directly initialize with noisy glyph latent instead of random initialization, and adopt region masks to restrict the feature injection to only the text region to avoid distortion of the background. We conducted extensive experiments to verify the effectiveness of our RepText relative to existing works, our approach outperforms existing open-source methods and achieves comparable results to native multi-language closed-source models. To be more fair, we also exhaustively discuss its limitations in the end.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Train Driver Performance as Key to Approval of Driverless Trains</title>
<link>https://arxiv.org/abs/2504.19735</link>
<guid>https://arxiv.org/abs/2504.19735</guid>
<content:encoded><![CDATA[
arXiv:2504.19735v1 Announce Type: new 
Abstract: Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation (EU) No. 402/2013 allow a simplified approach for the safety approval of computer vision systems for driverless trains, if they have 'similar' functions and interfaces as the replaced human driver. The human driver is not replaced one-to-one by a technical system - only a limited set of cognitive functions are replaced. However, performance in the most challenging function, obstacle detection, is difficult to quantify due to the deficiency of published measurement results. This article summarizes the data published so far. This article also goes a long way to remedy this situation by providing a new public and anonymized dataset of 711 train driver performance measurements from controlled experiments. The measurements are made for different speeds, obstacle sizes, train protection systems and obstacle color contrasts respectively. The measured values are reaction time and distance to the obstacle. The goal of this paper is an unbiased and exhaustive description of the presented dataset for research, standardization and regulation. Further project related information including the dataset and source code is available at https://atosense-02371c.usercontent.opencode.de/
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis</title>
<link>https://arxiv.org/abs/2504.19737</link>
<guid>https://arxiv.org/abs/2504.19737</guid>
<content:encoded><![CDATA[
arXiv:2504.19737v1 Announce Type: new 
Abstract: Global variations in terrain appearance raise a major challenge for satellite image analysis, leading to poor model performance when training on locations that differ from those encountered at test time. This remains true even with recent large global datasets. To address this challenge, we propose a novel domain-generalization framework for satellite images. Instead of trying to learn a single generalizable model, we train one expert model per training domain, while learning experts' similarity and encouraging similar experts to be consistent. A model selection module then identifies the most suitable experts for a given test sample and aggregates their predictions. Experiments on four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent gains over existing domain generalization and adaptation methods. Our code is publicly available at https://github.com/Abhishek19009/CoDEx.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Language-Image Learning with Augmented Textual Prompts for 3D/4D FER Using Vision-Language Model</title>
<link>https://arxiv.org/abs/2504.19739</link>
<guid>https://arxiv.org/abs/2504.19739</guid>
<content:encoded><![CDATA[
arXiv:2504.19739v1 Announce Type: new 
Abstract: In this paper, we introduce AffectVLM, a vision-language model designed to integrate multiviews for a semantically rich and visually comprehensive understanding of facial emotions from 3D/4D data. To effectively capture visual features, we propose a joint representation learning framework paired with a novel gradient-friendly loss function that accelerates model convergence towards optimal feature representation. Additionally, we introduce augmented textual prompts to enhance the model's linguistic capabilities and employ mixed view augmentation to expand the visual dataset. We also develop a Streamlit app for a real-time interactive inference and enable the model for distributed learning. Extensive experiments validate the superior performance of AffectVLM across multiple benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoWikiRS: Learning Ecological Representation of Satellite Images from Weak Supervision with Species Observations and Wikipedia</title>
<link>https://arxiv.org/abs/2504.19742</link>
<guid>https://arxiv.org/abs/2504.19742</guid>
<content:encoded><![CDATA[
arXiv:2504.19742v1 Announce Type: new 
Abstract: The presence of species provides key insights into the ecological properties of a location such as land cover, climatic conditions or even soil properties. We propose a method to predict such ecological properties directly from remote sensing (RS) images by aligning them with species habitat descriptions. We introduce the EcoWikiRS dataset, consisting of high-resolution aerial images, the corresponding geolocated species observations, and, for each species, the textual descriptions of their habitat from Wikipedia. EcoWikiRS offers a scalable way of supervision for RS vision language models (RS-VLMs) for ecology. This is a setting with weak and noisy supervision, where, for instance, some text may describe properties that are specific only to part of the species' niche or is irrelevant to a specific image. We tackle this by proposing WINCEL, a weighted version of the InfoNCE loss. We evaluate our model on the task of ecosystem zero-shot classification by following the habitat definitions from the European Nature Information System (EUNIS). Our results show that our approach helps in understanding RS images in a more ecologically meaningful manner. The code and the dataset are available at https://github.com/eceo-epfl/EcoWikiRS.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STCOcc: Sparse Spatial-Temporal Cascade Renovation for 3D Occupancy and Scene Flow Prediction</title>
<link>https://arxiv.org/abs/2504.19749</link>
<guid>https://arxiv.org/abs/2504.19749</guid>
<content:encoded><![CDATA[
arXiv:2504.19749v1 Announce Type: new 
Abstract: 3D occupancy and scene flow offer a detailed and dynamic representation of 3D scene. Recognizing the sparsity and complexity of 3D space, previous vision-centric methods have employed implicit learning-based approaches to model spatial and temporal information. However, these approaches struggle to capture local details and diminish the model's spatial discriminative ability. To address these challenges, we propose a novel explicit state-based modeling method designed to leverage the occupied state to renovate the 3D features. Specifically, we propose a sparse occlusion-aware attention mechanism, integrated with a cascade refinement strategy, which accurately renovates 3D features with the guidance of occupied state information. Additionally, we introduce a novel method for modeling long-term dynamic interactions, which reduces computational costs and preserves spatial information. Compared to the previous state-of-the-art methods, our efficient explicit renovation strategy not only delivers superior performance in terms of RayIoU and mAVE for occupancy and scene flow prediction but also markedly reduces GPU memory usage during training, bringing it down to 8.7GB. Our code is available on https://github.com/lzzzzzm/STCOcc
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment</title>
<link>https://arxiv.org/abs/2504.19755</link>
<guid>https://arxiv.org/abs/2504.19755</guid>
<content:encoded><![CDATA[
arXiv:2504.19755v1 Announce Type: new 
Abstract: Liver cirrhosis is an insidious condition involving the substitution of normal liver tissue with fibrous scar tissue and causing major health complications. The conventional method of diagnosis using liver biopsy is invasive and, therefore, inconvenient for use in regular screening. In this paper,we present a hybrid model that combines machine learning techniques with clinical data and ultrasoundscans to improve liver fibrosis and cirrhosis detection accuracy is presented. The model integrates fixed blood test probabilities with deep learning model predictions (DenseNet-201) for ultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The findings establish the viability of the combined model in enhancing diagnosis accuracy and supporting early intervention in liver disease care.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video</title>
<link>https://arxiv.org/abs/2504.19819</link>
<guid>https://arxiv.org/abs/2504.19819</guid>
<content:encoded><![CDATA[
arXiv:2504.19819v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRF) has demonstrated its superior capability to represent 3D geometry but require accurately precomputed camera poses during training. To mitigate this requirement, existing methods jointly optimize camera poses and NeRF often relying on good pose initialisation or depth priors. However, these approaches struggle in challenging scenarios, such as large rotations, as they map each camera to a world coordinate system. We propose a novel method that eliminates prior dependencies by modeling continuous camera motions as time-dependent angular velocity and velocity. Relative motions between cameras are learned first via velocity integration, while camera poses can be obtained by aggregating such relative motions up to a world coordinate system defined at a single time step within the video. Specifically, accurate continuous camera movements are learned through a time-dependent NeRF, which captures local scene geometry and motion by training from neighboring frames for each time step. The learned motions enable fine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D and Scannet show our approach achieves superior camera pose and depth estimation and comparable novel-view synthesis performance compared to state-of-the-art methods. Our code is available at https://github.com/HoangChuongNguyen/cope-nerf.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming the Randomness: Towards Label-Preserving Cropping in Contrastive Learning</title>
<link>https://arxiv.org/abs/2504.19824</link>
<guid>https://arxiv.org/abs/2504.19824</guid>
<content:encoded><![CDATA[
arXiv:2504.19824v1 Announce Type: new 
Abstract: Contrastive learning (CL) approaches have gained great recognition as a very successful subset of self-supervised learning (SSL) methods. SSL enables learning from unlabeled data, a crucial step in the advancement of deep learning, particularly in computer vision (CV), given the plethora of unlabeled image data. CL works by comparing different random augmentations (e.g., different crops) of the same image, thus achieving self-labeling. Nevertheless, randomly augmenting images and especially random cropping can result in an image that is semantically very distant from the original and therefore leads to false labeling, hence undermining the efficacy of the methods. In this research, two novel parameterized cropping methods are introduced that increase the robustness of self-labeling and consequently increase the efficacy. The results show that the use of these methods significantly improves the accuracy of the model by between 2.7\% and 12.4\% on the downstream task of classifying CIFAR-10, depending on the crop size compared to that of the non-parameterized random cropping method.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination</title>
<link>https://arxiv.org/abs/2504.19828</link>
<guid>https://arxiv.org/abs/2504.19828</guid>
<content:encoded><![CDATA[
arXiv:2504.19828v1 Announce Type: new 
Abstract: We present HOIGaze - a novel learning-based approach for gaze estimation during hand-object interactions (HOI) in extended reality (XR). HOIGaze addresses the challenging HOI setting by building on one key insight: The eye, hand, and head movements are closely coordinated during HOIs and this coordination can be exploited to identify samples that are most useful for gaze estimator training - as such, effectively denoising the training data. This denoising approach is in stark contrast to previous gaze estimation methods that treated all training samples as equal. Specifically, we propose: 1) a novel hierarchical framework that first recognises the hand currently visually attended to and then estimates gaze direction based on the attended hand; 2) a new gaze estimator that uses cross-modal Transformers to fuse head and hand-object features extracted using a convolutional neural network and a spatio-temporal graph convolutional network; and 3) a novel eye-head coordination loss that upgrades training samples belonging to the coordinated eye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin (ADT) datasets and show that it significantly outperforms state-of-the-art methods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in mean angular error. To demonstrate the potential of our method, we further report significant performance improvements for the sample downstream task of eye-based activity recognition on ADT. Taken together, our results underline the significant information content available in eye-hand-head coordination and, as such, open up an exciting new direction for learning-based gaze estimation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnimateAnywhere: Rouse the Background in Human Image Animation</title>
<link>https://arxiv.org/abs/2504.19834</link>
<guid>https://arxiv.org/abs/2504.19834</guid>
<content:encoded><![CDATA[
arXiv:2504.19834v1 Announce Type: new 
Abstract: Human image animation aims to generate human videos of given characters and backgrounds that adhere to the desired pose sequence. However, existing methods focus more on human actions while neglecting the generation of background, which typically leads to static results or inharmonious movements. The community has explored camera pose-guided animation tasks, yet preparing the camera trajectory is impractical for most entertainment applications and ordinary users. As a remedy, we present an AnimateAnywhere framework, rousing the background in human image animation without requirements on camera trajectories. In particular, based on our key insight that the movement of the human body often reflects the motion of the background, we introduce a background motion learner (BML) to learn background motions from human pose sequences. To encourage the model to learn more accurate cross-frame correspondences, we further deploy an epipolar constraint on the 3D attention map. Specifically, the mask used to suppress geometrically unreasonable attention is carefully constructed by combining an epipolar mask and the current 3D attention map. Extensive experiments demonstrate that our AnimateAnywhere effectively learns the background motion from human pose sequences, achieving state-of-the-art performance in generating human animation results with vivid and realistic backgrounds. The source code and model will be available at https://github.com/liuxiaoyu1104/AnimateAnywhere.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail UHR Satellite Image Segmentation</title>
<link>https://arxiv.org/abs/2504.19839</link>
<guid>https://arxiv.org/abs/2504.19839</guid>
<content:encoded><![CDATA[
arXiv:2504.19839v1 Announce Type: new 
Abstract: The long-tail problem presents a significant challenge to the advancement of semantic segmentation in ultra-high-resolution (UHR) satellite imagery. While previous efforts in UHR semantic segmentation have largely focused on multi-branch network architectures that emphasize multi-scale feature extraction and fusion, they have often overlooked the importance of addressing the long-tail issue. In contrast to prior UHR methods that focused on independent feature extraction, we emphasize data augmentation and multimodal feature fusion to alleviate the long-tail problem. In this paper, we introduce SRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our approach addresses the long-tail class distribution by incorporating a multi-scale cropping technique alongside a data augmentation strategy based on semantic reordering and resampling. To further enhance model performance, we propose a multimodal fusion-based general representation knowledge injection method, which, for the first time, fuses text and visual features without the need for individual region text descriptions, extracting more robust features. Extensive experiments on the URUR, GID, and FBP datasets demonstrate that our method improves mIoU by 3.33\%, 0.66\%, and 0.98\%, respectively, achieving state-of-the-art performance. Code is available at: https://github.com/BinSpa/SRMF.git.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration</title>
<link>https://arxiv.org/abs/2504.19847</link>
<guid>https://arxiv.org/abs/2504.19847</guid>
<content:encoded><![CDATA[
arXiv:2504.19847v1 Announce Type: new 
Abstract: In this work, we introduce Segmentation to Human-Object Interaction (\textit{\textbf{Seg2HOI}}) approach, a novel framework that integrates segmentation-based vision foundation models with the human-object interaction task, distinguished from traditional detection-based Human-Object Interaction (HOI) methods. Our approach enhances HOI detection by not only predicting the standard triplets but also introducing quadruplets, which extend HOI triplets by including segmentation masks for human-object pairs. More specifically, Seg2HOI inherits the properties of the vision foundation model (e.g., promptable and interactive mechanisms) and incorporates a decoder that applies these attributes to HOI task. Despite training only for HOI, without additional training mechanisms for these properties, the framework demonstrates that such features still operate efficiently. Extensive experiments on two public benchmark datasets demonstrate that Seg2HOI achieves performance comparable to state-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that Seg2HOI can generate HOI quadruplets and interactive HOI segmentation from novel text and visual prompts that were not used during training, making it versatile for a wide range of applications by leveraging this flexibility.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoherenDream: Boosting Holistic Text Coherence in 3D Generation via Multimodal Large Language Models Feedback</title>
<link>https://arxiv.org/abs/2504.19860</link>
<guid>https://arxiv.org/abs/2504.19860</guid>
<content:encoded><![CDATA[
arXiv:2504.19860v1 Announce Type: new 
Abstract: Score Distillation Sampling (SDS) has achieved remarkable success in text-to-3D content generation. However, SDS-based methods struggle to maintain semantic fidelity for user prompts, particularly when involving multiple objects with intricate interactions. While existing approaches often address 3D consistency through multiview diffusion model fine-tuning on 3D datasets, this strategy inadvertently exacerbates text-3D alignment degradation. The limitation stems from SDS's inherent accumulation of view-independent biases during optimization, which progressively diverges from the ideal text alignment direction. To alleviate this limitation, we propose a novel SDS objective, dubbed as Textual Coherent Score Distillation (TCSD), which integrates alignment feedback from multimodal large language models (MLLMs). Our TCSD leverages cross-modal understanding capabilities of MLLMs to assess and guide the text-3D correspondence during the optimization. We further develop 3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text alignment in 3D generations. Additionally, we introduce an LLM-layout initialization that significantly accelerates optimization convergence through semantic-aware spatial configuration. Comprehensive evaluations demonstrate that our framework, CoherenDream, establishes state-of-the-art performance in text-aligned 3D generation across multiple benchmarks, including T$^3$Bench and TIFA subset. Qualitative results showcase the superior performance of CoherenDream in preserving textual consistency and semantic interactions. As the first study to incorporate MLLMs into SDS optimization, we also conduct extensive ablation studies to explore optimal MLLM adaptations for 3D generation tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast Videos via Physically Grounded Synthetic-to-Real Transfer</title>
<link>https://arxiv.org/abs/2504.19863</link>
<guid>https://arxiv.org/abs/2504.19863</guid>
<content:encoded><![CDATA[
arXiv:2504.19863v1 Announce Type: new 
Abstract: Analyzing a player's technique in table tennis requires knowledge of the ball's 3D trajectory and spin. While, the spin is not directly observable in standard broadcasting videos, we show that it can be inferred from the ball's trajectory in the video. We present a novel method to infer the initial spin and 3D trajectory from the corresponding 2D trajectory in a video. Without ground truth labels for broadcast videos, we train a neural network solely on synthetic data. Due to the choice of our input data representation, physically correct synthetic training data, and using targeted augmentations, the network naturally generalizes to real data. Notably, these simple techniques are sufficient to achieve generalization. No real data at all is required for training. To the best of our knowledge, we are the first to present a method for spin and trajectory prediction in simple monocular broadcast videos, achieving an accuracy of 92.0% in spin classification and a 2D reprojection error of 0.19% of the image diagonal.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images</title>
<link>https://arxiv.org/abs/2504.19876</link>
<guid>https://arxiv.org/abs/2504.19876</guid>
<content:encoded><![CDATA[
arXiv:2504.19876v1 Announce Type: new 
Abstract: This paper introduces DeeCLIP, a novel framework for detecting AI-generated images using CLIP-ViT and fusion learning. Despite significant advancements in generative models capable of creating highly photorealistic images, existing detection methods often struggle to generalize across different models and are highly sensitive to minor perturbations. To address these challenges, DeeCLIP incorporates DeeFuser, a fusion module that combines high-level and low-level features, improving robustness against degradations such as compression and blurring. Additionally, we apply triplet loss to refine the embedding space, enhancing the model's ability to distinguish between real and synthetic content. To further enable lightweight adaptation while preserving pre-trained knowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation (LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot learning without sacrificing generalization. Trained exclusively on 4-class ProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets composed of generative adversarial network (GAN) and diffusion models. Despite having fewer trainable parameters, DeeCLIP outperforms existing methods, demonstrating superior robustness against various generative models and real-world distortions. The code is publicly available at https://github.com/Mamadou-Keita/DeeCLIP for research purposes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Fixed and Mobile Eye Tracking to Understand How Visitors View Art in a Museum: A Study at the Bowes Museum, County Durham, UK</title>
<link>https://arxiv.org/abs/2504.19881</link>
<guid>https://arxiv.org/abs/2504.19881</guid>
<content:encoded><![CDATA[
arXiv:2504.19881v1 Announce Type: new 
Abstract: The following paper describes a collaborative project involving researchers at Durham University, and professionals at the Bowes Museum, Barnard Castle, County Durham, UK, during which we used fixed and mobile eye tracking to understand how visitors view art. Our study took place during summer 2024 and builds on work presented at DH2017 (Bailey-Ross et al., 2017). Our interdisciplinary team included researchers from digital humanities, psychology, art history and computer science, working in collaboration with professionals from the museum. We used fixed and mobile eye tracking to understand how museum visitors view art in a physical gallery setting. This research will enable us to make recommendations about how the Museum's collections could be more effectively displayed, encouraging visitors to engage with them more fully.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Out-of-Distribution Generalization: A Causal Augmentation View</title>
<link>https://arxiv.org/abs/2504.19882</link>
<guid>https://arxiv.org/abs/2504.19882</guid>
<content:encoded><![CDATA[
arXiv:2504.19882v1 Announce Type: new 
Abstract: Federated learning aims to collaboratively model by integrating multi-source information to obtain a model that can generalize across all client data. Existing methods often leverage knowledge distillation or data augmentation to mitigate the negative impact of data bias across clients. However, the limited performance of teacher models on out-of-distribution samples and the inherent quality gap between augmented and original data hinder their effectiveness and they typically fail to leverage the advantages of incorporating rich contextual information. To address these limitations, this paper proposes a Federated Causal Augmentation method, termed FedCAug, which employs causality-inspired data augmentation to break the spurious correlation between attributes and categories. Specifically, it designs a causal region localization module to accurately identify and decouple the background and objects in the image, providing rich contextual information for causal data augmentation. Additionally, it designs a causality-inspired data augmentation module that integrates causal features and within-client context to generate counterfactual samples. This significantly enhances data diversity, and the entire process does not require any information sharing between clients, thereby contributing to the protection of data privacy. Extensive experiments conducted on three datasets reveal that FedCAug markedly reduces the model's reliance on background to predict sample labels, achieving superior performance compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing breast cancer detection on screening mammogram using self-supervised learning and a hybrid deep model of Swin Transformer and Convolutional Neural Network</title>
<link>https://arxiv.org/abs/2504.19888</link>
<guid>https://arxiv.org/abs/2504.19888</guid>
<content:encoded><![CDATA[
arXiv:2504.19888v1 Announce Type: new 
Abstract: Purpose: The scarcity of high-quality curated labeled medical training data remains one of the major limitations in applying artificial intelligence (AI) systems to breast cancer diagnosis. Deep models for mammogram analysis and mass (or micro-calcification) detection require training with a large volume of labeled images, which are often expensive and time-consuming to collect. To reduce this challenge, we proposed a novel method that leverages self-supervised learning (SSL) and a deep hybrid model, named \textbf{HybMNet}, which combines local self-attention and fine-grained feature extraction to enhance breast cancer detection on screening mammograms.
  Approach: Our method employs a two-stage learning process: (1) SSL Pretraining: We utilize EsViT, a SSL technique, to pretrain a Swin Transformer (Swin-T) using a limited set of mammograms. The pretrained Swin-T then serves as the backbone for the downstream task. (2) Downstream Training: The proposed HybMNet combines the Swin-T backbone with a CNN-based network and a novel fusion strategy. The Swin-T employs local self-attention to identify informative patch regions from the high-resolution mammogram, while the CNN-based network extracts fine-grained local features from the selected patches. A fusion module then integrates global and local information from both networks to generate robust predictions. The HybMNet is trained end-to-end, with the loss function combining the outputs of the Swin-T and CNN modules to optimize feature extraction and classification performance.
  Results: The proposed method was evaluated for its ability to detect breast cancer by distinguishing between benign (normal) and malignant mammograms. Leveraging SSL pretraining and the HybMNet model, it achieved AUC of 0.864 (95% CI: 0.852, 0.875) on the CMMD dataset and 0.889 (95% CI: 0.875, 0.903) on the INbreast dataset, highlighting its effectiveness.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition</title>
<link>https://arxiv.org/abs/2504.19894</link>
<guid>https://arxiv.org/abs/2504.19894</guid>
<content:encoded><![CDATA[
arXiv:2504.19894v1 Announce Type: new 
Abstract: We present CineVerse, a novel framework for the task of cinematic scene composition. Similar to traditional multi-shot generation, our task emphasizes the need for consistency and continuity across frames. However, our task also focuses on addressing challenges inherent to filmmaking, such as multiple characters, complex interactions, and visual cinematic effects. In order to learn to generate such content, we first create the CineVerse dataset. We use this dataset to train our proposed two-stage approach. First, we prompt a large language model (LLM) with task-specific instructions to take in a high-level scene description and generate a detailed plan for the overall setting and characters, as well as the individual shots. Then, we fine-tune a text-to-image generation model to synthesize high-quality visual keyframes. Experimental results demonstrate that CineVerse yields promising improvements in generating visually coherent and contextually rich movie scenes, paving the way for further exploration in cinematic video synthesis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning</title>
<link>https://arxiv.org/abs/2504.19900</link>
<guid>https://arxiv.org/abs/2504.19900</guid>
<content:encoded><![CDATA[
arXiv:2504.19900v1 Announce Type: new 
Abstract: Accurate detection of breast cancer from high-resolution mammograms is crucial for early diagnosis and effective treatment planning. Previous studies have shown the potential of using single-view mammograms for breast cancer detection. However, incorporating multi-view data can provide more comprehensive insights. Multi-view classification, especially in medical imaging, presents unique challenges, particularly when dealing with large-scale, high-resolution data. In this work, we propose a novel Multi-view Visual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening mammograms. We first pretrain a robust single-view classification model on high-resolution mammograms and then innovatively adapt multi-view feature learning into a task-specific prompt tuning process. This technique selectively tunes a minimal set of trainable parameters (7\%) while retaining the robustness of the pre-trained single-view model, enabling efficient integration of multi-view data without the need for aggressive downsampling. Our approach offers an efficient alternative to traditional feature fusion methods, providing a more robust, scalable, and efficient solution for high-resolution mammogram analysis. Experimental results on a large multi-institution dataset demonstrate that our method outperforms conventional approaches while maintaining detection efficiency, achieving an AUROC of 0.852 for distinguishing between Benign, DCIS, and Invasive classes. This work highlights the potential of MVPT-NET for medical imaging tasks and provides a scalable solution for integrating multi-view data in breast cancer detection.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI</title>
<link>https://arxiv.org/abs/2504.19918</link>
<guid>https://arxiv.org/abs/2504.19918</guid>
<content:encoded><![CDATA[
arXiv:2504.19918v1 Announce Type: new 
Abstract: The automatic summarization of surgical videos is essential for enhancing procedural documentation, supporting surgical training, and facilitating post-operative analysis. This paper presents a novel method at the intersection of artificial intelligence and medicine, aiming to develop machine learning models with direct real-world applications in surgical contexts. We propose a multi-modal framework that leverages recent advancements in computer vision and large language models to generate comprehensive video summaries. %
The approach is structured in three key stages. First, surgical videos are divided into clips, and visual features are extracted at the frame level using visual transformers. This step focuses on detecting tools, tissues, organs, and surgical actions. Second, the extracted features are transformed into frame-level captions via large language models. These are then combined with temporal features, captured using a ViViT-based encoder, to produce clip-level summaries that reflect the broader context of each video segment. Finally, the clip-level descriptions are aggregated into a full surgical report using a dedicated LLM tailored for the summarization task. %
We evaluate our method on the CholecT50 dataset, using instrument and action annotations from 50 laparoscopic videos. The results show strong performance, achieving 96\% precision in tool detection and a BERT score of 0.74 for temporal context summarization. This work contributes to the advancement of AI-assisted tools for surgical reporting, offering a step toward more intelligent and reliable clinical documentation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Quality for VVC Compressed Videos with Omniscient Quality Enhancement Model</title>
<link>https://arxiv.org/abs/2504.19935</link>
<guid>https://arxiv.org/abs/2504.19935</guid>
<content:encoded><![CDATA[
arXiv:2504.19935v1 Announce Type: new 
Abstract: The latest video coding standard H.266/VVC has shown its great improvement in terms of compression performance when compared to its predecessor HEVC standard. Though VVC was implemented with many advanced techniques, it still met the same challenges as its predecessor due to the need for even higher perceptual quality demand at the decoder side as well as the compression performance at the encoder side. The advancement of Artificial Intelligence (AI) technology, notably the deep learning-based video quality enhancement methods, was shown to be a promising approach to improving the perceptual quality experience. In this paper, we propose a novel Omniscient video quality enhancement Network for VVC compressed Videos. The Omniscient Network for compressed video quality enhancement was originally designed for HEVC compressed videos in which not only the spatial-temporal features but also cross-frequencies information were employed to augment the visual quality. Inspired by this work, we propose a modification of the OVQE model and integrate it into the lasted STD-VVC (Standard Versatile Video Coding) decoder architecture. As assessed in a rich set of test conditions, the proposed OVQE-VVC solution is able to achieve significant PSNR improvement, notably around 0.74 dB and up to 1.2 dB with respect to the original STD-VVC codec. This also corresponds to around 19.6% of bitrate saving while keeping a similar quality observation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mesh-Learner: Texturing Mesh with Spherical Harmonics</title>
<link>https://arxiv.org/abs/2504.19938</link>
<guid>https://arxiv.org/abs/2504.19938</guid>
<content:encoded><![CDATA[
arXiv:2504.19938v1 Announce Type: new 
Abstract: In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at https://github.com/hku-mars/Mesh-Learner.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shopformer: Transformer-Based Framework for Detecting Shoplifting via Human Pose</title>
<link>https://arxiv.org/abs/2504.19970</link>
<guid>https://arxiv.org/abs/2504.19970</guid>
<content:encoded><![CDATA[
arXiv:2504.19970v1 Announce Type: new 
Abstract: Shoplifting remains a costly issue for the retail sector, but traditional surveillance systems, which are mostly based on human monitoring, are still largely ineffective, with only about 2% of shoplifters being arrested. Existing AI-based approaches rely on pixel-level video analysis which raises privacy concerns, is sensitive to environmental variations, and demands significant computational resources. To address these limitations, we introduce Shopformer, a novel transformer-based model that detects shoplifting by analyzing pose sequences rather than raw video. We propose a custom tokenization strategy that converts pose sequences into compact embeddings for efficient transformer processing. To the best of our knowledge, this is the first pose-sequence-based transformer model for shoplifting detection. Evaluated on real-world pose data, our method outperforms state-of-the-art anomaly detection models, offering a privacy-preserving, and scalable solution for real-time retail surveillance. The code base for this work is available at https://github.com/TeCSAR-UNCC/Shopformer.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data</title>
<link>https://arxiv.org/abs/2504.19991</link>
<guid>https://arxiv.org/abs/2504.19991</guid>
<content:encoded><![CDATA[
arXiv:2504.19991v1 Announce Type: new 
Abstract: Effective weed management is crucial for improving agricultural productivity, as weeds compete with crops for vital resources like nutrients and water. Accurate maps of weed management methods are essential for policymakers to assess farmer practices, evaluate impacts on vegetation health, biodiversity, and climate, as well as ensure compliance with policies and subsidies. However, monitoring weed management methods is challenging as commonly rely on on-ground field surveys, which are often costly, time-consuming and subject to delays. In order to tackle this problem, we leverage Earth Observation (EO) data and Machine Learning (ML). Specifically, we developed an ML approach for mapping four distinct weed management methods (Mowing, Tillage, Chemical-spraying, and No practice) in orchards using satellite image time series (SITS) data from two different sources: Sentinel-2 (S2) and PlanetScope (PS). The findings demonstrate the potential of ML-driven remote sensing to enhance the efficiency and accuracy of weed management mapping in orchards.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery</title>
<link>https://arxiv.org/abs/2504.19996</link>
<guid>https://arxiv.org/abs/2504.19996</guid>
<content:encoded><![CDATA[
arXiv:2504.19996v1 Announce Type: new 
Abstract: The widespread use of Exogenous Organic Matter in agriculture necessitates monitoring to assess its effects on soil and crop health. This study evaluates optical Sentinel-2 satellite imagery for detecting digestate application, a practice that enhances soil fertility but poses environmental risks like microplastic contamination and nitrogen losses. In the first instance, Sentinel-2 satellite image time series (SITS) analysis of specific indices (EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after application on the soils of four different crop types in Thessaly, Greece. Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient Boosting and a Feed-Forward Neural Network), were used to investigate digestate presence detection, achieving F1-scores up to 0.85. The findings highlight the potential of combining remote sensing and ML for scalable and cost-effective monitoring of EOM applications, supporting precision agriculture and sustainability.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning</title>
<link>https://arxiv.org/abs/2504.20024</link>
<guid>https://arxiv.org/abs/2504.20024</guid>
<content:encoded><![CDATA[
arXiv:2504.20024v1 Announce Type: new 
Abstract: Recent studies in 3D spatial reasoning explore data-driven approaches and achieve enhanced spatial reasoning performance with reinforcement learning (RL). However, these methods typically perform spatial reasoning in an implicit manner, and it remains underexplored whether the acquired 3D knowledge generalizes to unseen question types at any stage of the training. In this work we introduce SpatialReasoner, a novel large vision-language model (LVLM) that address 3D spatial reasoning with explicit 3D representations shared between stages -- 3D perception, computation, and reasoning. Explicit 3D representations provide a coherent interface that supports advanced 3D spatial reasoning and enable us to study the factual errors made by LVLMs. Results show that our SpatialReasoner achieve improved performance on a variety of spatial reasoning benchmarks and generalizes better when evaluating on novel 3D spatial reasoning questions. Our study bridges the 3D parsing capabilities of prior visual foundation models with the powerful reasoning abilities of large language models, opening new directions for 3D spatial reasoning.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields</title>
<link>https://arxiv.org/abs/2504.20026</link>
<guid>https://arxiv.org/abs/2504.20026</guid>
<content:encoded><![CDATA[
arXiv:2504.20026v1 Announce Type: new 
Abstract: We present Large Inverse Rendering Model (LIRM), a transformer architecture that jointly reconstructs high-quality shape, materials, and radiance fields with view-dependent effects in less than a second. Our model builds upon the recent Large Reconstruction Models (LRMs) that achieve state-of-the-art sparse-view reconstruction quality. However, existing LRMs struggle to reconstruct unseen parts accurately and cannot recover glossy appearance or generate relightable 3D contents that can be consumed by standard Graphics engines. To address these limitations, we make three key technical contributions to build a more practical multi-view 3D reconstruction framework. First, we introduce an update model that allows us to progressively add more input views to improve our reconstruction. Second, we propose a hexa-plane neural SDF representation to better recover detailed textures, geometry and material parameters. Third, we develop a novel neural directional-embedding mechanism to handle view-dependent effects. Trained on a large-scale shape and material dataset with a tailored coarse-to-fine training scheme, our model achieves compelling results. It compares favorably to optimization-based dense-view inverse rendering methods in terms of geometry and relighting accuracy, while requiring only a fraction of the inference time.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Clear, More Flexible, More Precise: A Comprehensive Oriented Object Detection benchmark for UAV</title>
<link>https://arxiv.org/abs/2504.20032</link>
<guid>https://arxiv.org/abs/2504.20032</guid>
<content:encoded><![CDATA[
arXiv:2504.20032v1 Announce Type: new 
Abstract: Applications of unmanned aerial vehicle (UAV) in logistics, agricultural automation, urban management, and emergency response are highly dependent on oriented object detection (OOD) to enhance visual perception. Although existing datasets for OOD in UAV provide valuable resources, they are often designed for specific downstream tasks.Consequently, they exhibit limited generalization performance in real flight scenarios and fail to thoroughly demonstrate algorithm effectiveness in practical environments. To bridge this critical gap, we introduce CODrone, a comprehensive oriented object detection dataset for UAVs that accurately reflects real-world conditions. It also serves as a new benchmark designed to align with downstream task requirements, ensuring greater applicability and robustness in UAV-based OOD.Based on application requirements, we identify four key limitations in current UAV OOD datasets-low image resolution, limited object categories, single-view imaging, and restricted flight altitudes-and propose corresponding improvements to enhance their applicability and robustness.Furthermore, CODrone contains a broad spectrum of annotated images collected from multiple cities under various lighting conditions, enhancing the realism of the benchmark. To rigorously evaluate CODrone as a new benchmark and gain deeper insights into the novel challenges it presents, we conduct a series of experiments based on 22 classical or SOTA methods.Our evaluation not only assesses the effectiveness of CODrone in real-world scenarios but also highlights key bottlenecks and opportunities to advance OOD in UAV applications.Overall, CODrone fills the data gap in OOD from UAV perspective and provides a benchmark with enhanced generalization capability, better aligning with practical applications and future algorithm development.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Catastrophic Forgetting in the Incremental Learning of Medical Images</title>
<link>https://arxiv.org/abs/2504.20033</link>
<guid>https://arxiv.org/abs/2504.20033</guid>
<content:encoded><![CDATA[
arXiv:2504.20033v1 Announce Type: new 
Abstract: This paper proposes an Incremental Learning (IL) approach to enhance the accuracy and efficiency of deep learning models in analyzing T2-weighted (T2w) MRI medical images prostate cancer detection using the PI-CAI dataset. We used multiple health centers' artificial intelligence and radiology data, focused on different tasks that looked at prostate cancer detection using MRI (PI-CAI). We utilized Knowledge Distillation (KD), as it employs generated images from past tasks to guide the training of models for subsequent tasks. The approach yielded improved performance and faster convergence of the models. To demonstrate the versatility and robustness of our approach, we evaluated it on the PI-CAI dataset, a diverse set of medical imaging modalities including OCT and PathMNIST, and the benchmark continual learning dataset CIFAR-10. Our results indicate that KD can be a promising technique for IL in medical image analysis in which data is sourced from individual health centers and the storage of large datasets is not feasible. By using generated images from prior tasks, our method enables the model to retain and apply previously acquired knowledge without direct access to the original data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion</title>
<link>https://arxiv.org/abs/2504.20040</link>
<guid>https://arxiv.org/abs/2504.20040</guid>
<content:encoded><![CDATA[
arXiv:2504.20040v1 Announce Type: new 
Abstract: While Structure-from-Motion (SfM) has seen much progress over the years, state-of-the-art systems are prone to failure when facing extreme viewpoint changes in low-overlap, low-parallax or high-symmetry scenarios. Because capturing images that avoid these pitfalls is challenging, this severely limits the wider use of SfM, especially by non-expert users. We overcome these limitations by augmenting the classical SfM paradigm with monocular depth and normal priors inferred by deep neural networks. Thanks to a tight integration of monocular and multi-view constraints, our approach significantly outperforms existing ones under extreme viewpoint changes, while maintaining strong performance in standard conditions. We also show that monocular priors can help reject faulty associations due to symmetries, which is a long-standing problem for SfM. This makes our approach the first capable of reliably reconstructing challenging indoor environments from few images. Through principled uncertainty propagation, it is robust to errors in the priors, can handle priors inferred by different models with little tuning, and will thus easily benefit from future progress in monocular depth and normal estimation. Our code is publicly available at https://github.com/cvg/mpsfm.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Streaming Video Representation via Multitask Training</title>
<link>https://arxiv.org/abs/2504.20041</link>
<guid>https://arxiv.org/abs/2504.20041</guid>
<content:encoded><![CDATA[
arXiv:2504.20041v1 Announce Type: new 
Abstract: Understanding continuous video streams plays a fundamental role in real-time applications including embodied AI and autonomous driving. Unlike offline video understanding, streaming video understanding requires the ability to process video streams frame by frame, preserve historical information, and make low-latency decisions.To address these challenges, our main contributions are three-fold. (i) We develop a novel streaming video backbone, termed as StreamFormer, by incorporating causal temporal attention into a pre-trained vision transformer. This enables efficient streaming video processing while maintaining image representation capability.(ii) To train StreamFormer, we propose to unify diverse spatial-temporal video understanding tasks within a multitask visual-language alignment framework. Hence, StreamFormer learns global semantics, temporal dynamics, and fine-grained spatial relationships simultaneously. (iii) We conduct extensive experiments on online action detection, online video instance segmentation, and video question answering. StreamFormer achieves competitive results while maintaining efficiency, demonstrating its potential for real-time applications.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompleteMe: Reference-based Human Image Completion</title>
<link>https://arxiv.org/abs/2504.20042</link>
<guid>https://arxiv.org/abs/2504.20042</guid>
<content:encoded><![CDATA[
arXiv:2504.20042v1 Announce Type: new 
Abstract: Recent methods for human image completion can reconstruct plausible body shapes but often fail to preserve unique details, such as specific clothing patterns or distinctive accessories, without explicit reference images. Even state-of-the-art reference-based inpainting approaches struggle to accurately capture and integrate fine-grained details from reference images. To address this limitation, we propose CompleteMe, a novel reference-based human image completion framework. CompleteMe employs a dual U-Net architecture combined with a Region-focused Attention (RFA) Block, which explicitly guides the model's attention toward relevant regions in reference images. This approach effectively captures fine details and ensures accurate semantic correspondence, significantly improving the fidelity and consistency of completed images. Additionally, we introduce a challenging benchmark specifically designed for evaluating reference-based human image completion tasks. Extensive experiments demonstrate that our proposed method achieves superior visual quality and semantic consistency compared to existing techniques. Project page: https://liagm.github.io/CompleteMe/
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Visual Complaints through a test battery in Acquired Brain Injury Patients: A Detailed Analysis of the DiaNAH Dataset</title>
<link>https://arxiv.org/abs/2504.18540</link>
<guid>https://arxiv.org/abs/2504.18540</guid>
<content:encoded><![CDATA[
arXiv:2504.18540v1 Announce Type: cross 
Abstract: This study investigated visual impairment complaints in a sample of 948 Acquired Brain Injury (ABI) patients using the DiaNAH dataset, emphasizing advanced machine learning techniques for managing missing data. Patients completed a CVS questionnaire capturing eight types of visual symptoms, including blurred vision and altered contrast perception. Due to incomplete data, 181 patients were excluded, resulting in an analytical subset of 767 individuals. To address the challenge of missing data, an automated machine learning (AutoML) approach was employed for data imputation, preserving the distributional characteristics of the original dataset. Patients were grouped according to singular and combined complaint clusters derived from the 40,320 potential combinations identified through the CVS questionnaire. A linear correlation analysis revealed minimal to no direct relationship between patient-reported visual complaints and standard visual perceptual function tests. This study represents an initial systematic attempt to understand the complex relationship between subjective visual complaints and objective visual perceptual assessments in ABI patients. Given the limitations of sample size and variability, further studies with larger populations are recommended to robustly explore these complaint clusters and their implications for visual perception following brain injury.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Bit Integerization of Vision Transformers using Operand Reodering for Efficient Hardware</title>
<link>https://arxiv.org/abs/2504.18547</link>
<guid>https://arxiv.org/abs/2504.18547</guid>
<content:encoded><![CDATA[
arXiv:2504.18547v1 Announce Type: cross 
Abstract: Pre-trained vision transformers have achieved remarkable performance across various visual tasks but suffer from expensive computational and memory costs. While model quantization reduces memory usage by lowering precision, these models still incur significant computational overhead due to the dequantization before matrix operations. In this work, we analyze the computation graph and propose an integerization process based on operation reordering. Specifically, the process delays dequantization until after matrix operations. This enables integerized matrix multiplication and linear module by directly processing the quantized input. To validate our approach, we synthesize the self-attention module of ViT on a systolic array-based hardware. Experimental results show that our low-bit inference reduces per-PE power consumption for linear layer and matrix multiplication, bridging the gap between quantized models and efficient inference.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Modality Computational Ophthalmic Imaging with Deep Learning and Coaxial Optical Design</title>
<link>https://arxiv.org/abs/2504.18549</link>
<guid>https://arxiv.org/abs/2504.18549</guid>
<content:encoded><![CDATA[
arXiv:2504.18549v1 Announce Type: cross 
Abstract: The growing burden of myopia and retinal diseases necessitates more accessible and efficient eye screening solutions. This study presents a compact, dual-function optical device that integrates fundus photography and refractive error detection into a unified platform. The system features a coaxial optical design using dichroic mirrors to separate wavelength-dependent imaging paths, enabling simultaneous alignment of fundus and refraction modules. A Dense-U-Net-based algorithm with customized loss functions is employed for accurate pupil segmentation, facilitating automated alignment and focusing. Experimental evaluations demonstrate the system's capability to achieve high-precision pupil localization (EDE = 2.8 px, mIoU = 0.931) and reliable refractive estimation with a mean absolute error below 5%. Despite limitations due to commercial lens components, the proposed framework offers a promising solution for rapid, intelligent, and scalable ophthalmic screening, particularly suitable for community health settings.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoor Defense in Diffusion Models via Spatial Attention Unlearning</title>
<link>https://arxiv.org/abs/2504.18563</link>
<guid>https://arxiv.org/abs/2504.18563</guid>
<content:encoded><![CDATA[
arXiv:2504.18563v1 Announce Type: cross 
Abstract: Text-to-image diffusion models are increasingly vulnerable to backdoor attacks, where malicious modifications to the training data cause the model to generate unintended outputs when specific triggers are present. While classification models have seen extensive development of defense mechanisms, generative models remain largely unprotected due to their high-dimensional output space, which complicates the detection and mitigation of subtle perturbations. Defense strategies for diffusion models, in particular, remain under-explored. In this work, we propose Spatial Attention Unlearning (SAU), a novel technique for mitigating backdoor attacks in diffusion models. SAU leverages latent space manipulation and spatial attention mechanisms to isolate and remove the latent representation of backdoor triggers, ensuring precise and efficient removal of malicious effects. We evaluate SAU across various types of backdoor attacks, including pixel-based and style-based triggers, and demonstrate its effectiveness in achieving 100% trigger removal accuracy. Furthermore, SAU achieves a CLIP score of 0.7023, outperforming existing methods while preserving the model's ability to generate high-quality, semantically aligned images. Our results show that SAU is a robust, scalable, and practical solution for securing text-to-image diffusion models against backdoor attacks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations</title>
<link>https://arxiv.org/abs/2504.18591</link>
<guid>https://arxiv.org/abs/2504.18591</guid>
<content:encoded><![CDATA[
arXiv:2504.18591v1 Announce Type: cross 
Abstract: Recent advances in Neural Fields have enabled powerful, discretization-invariant methods for learning neural operators that approximate solutions of Partial Differential Equations (PDEs) on general geometries. Building on these developments, we introduce enf2enf, an encoder--decoder methodology for predicting steady-state Partial Differential Equations with non-parameterized geometric variability, based on recently proposed Equivariant Neural Field architectures. In enf2enf, input geometries are encoded into latent point cloud embeddings that inherently preserve geometric grounding and capture local phenomena. The resulting representations are then combined with global parameters and directly decoded into continuous output fields, thus efficiently modeling the coupling between geometry and physics. By leveraging the inductive biases of locality and translation invariance, our approach is able to capture fine-scale physical features as well as complex shape variations, thereby enhancing generalization and physical compliance. Extensive experiments on a high-fidelity aerodynamic dataset, a hyper-elastic material benchmark, and multi-element airfoil geometries, demonstrate that the proposed model achieves superior or competitive performance compared to state-of-the-art graph based, operator learning, and neural field methods. Notably, our method supports real time inference and zero-shot super-resolution, enabling efficient training on low-resolution meshes while maintaining high accuracy on full-scale discretizations.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Validation and Calibration of Semi-Analytical Models for the Event Horizon Telescope Observations of Sagittarius A*</title>
<link>https://arxiv.org/abs/2504.18624</link>
<guid>https://arxiv.org/abs/2504.18624</guid>
<content:encoded><![CDATA[
arXiv:2504.18624v1 Announce Type: cross 
Abstract: The Event Horizon Telescope (EHT) enables the exploration of black hole accretion flows at event-horizon scales. Fitting ray-traced physical models to EHT observations requires the generation of synthetic images, a task that is computationally demanding. This study leverages \alinet, a generative machine learning model, to efficiently produce radiatively inefficient accretion flow (RIAF) images as a function of the specified physical parameters. \alinet has previously been shown to be able to interpolate black hole images and their associated physical parameters after training on a computationally tractable set of library images. We utilize this model to estimate the uncertainty introduced by a number of anticipated unmodeled physical effects, including interstellar scattering and intrinsic source variability. We then use this to calibrate physical parameter estimates and their associated uncertainties from RIAF model fits to mock EHT data via a library of general relativistic magnetohydrodynamics models.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians</title>
<link>https://arxiv.org/abs/2504.18768</link>
<guid>https://arxiv.org/abs/2504.18768</guid>
<content:encoded><![CDATA[
arXiv:2504.18768v1 Announce Type: cross 
Abstract: The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis</title>
<link>https://arxiv.org/abs/2504.18802</link>
<guid>https://arxiv.org/abs/2504.18802</guid>
<content:encoded><![CDATA[
arXiv:2504.18802v1 Announce Type: cross 
Abstract: Urban roads and infrastructure, vital to city operations, face growing threats from subsurface anomalies like cracks and cavities. Ground Penetrating Radar (GPR) effectively visualizes underground conditions employing electromagnetic (EM) waves; however, accurate anomaly detection via GPR remains challenging due to limited labeled data, varying subsurface conditions, and indistinct target boundaries. Although visually image-like, GPR data fundamentally represent EM waves, with variations within and between waves critical for identifying anomalies. Addressing these, we propose the Reservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework exploiting both visual discernibility and wave-changing properties of GPR data. Res-SAM initially identifies apparent candidate anomaly regions given minimal prompts, and further refines them by analyzing anomaly-induced changing information within and between EM waves in local GPR data, enabling precise and complete anomaly region extraction and category determination. Real-world experiments demonstrate that Res-SAM achieves high detection accuracy (>85%) and outperforms state-of-the-art. Notably, Res-SAM requires only minimal accessible non-target data, avoids intensive training, and incorporates simple human interaction to enhance reliability. Our research provides a scalable, resource-efficient solution for rapid subsurface anomaly detection across diverse environments, improving urban safety monitoring while reducing manual effort and computational cost.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy</title>
<link>https://arxiv.org/abs/2504.18829</link>
<guid>https://arxiv.org/abs/2504.18829</guid>
<content:encoded><![CDATA[
arXiv:2504.18829v1 Announce Type: cross 
Abstract: Generalizable dexterous grasping with suitable grasp types is a fundamental skill for intelligent robots. Developing such skills requires a large-scale and high-quality dataset that covers numerous grasp types (i.e., at least those categorized by the GRASP taxonomy), but collecting such data is extremely challenging. Existing automatic grasp synthesis methods are often limited to specific grasp types or object categories, hindering scalability. This work proposes an efficient pipeline capable of synthesizing contact-rich, penetration-free, and physically plausible grasps for any grasp type, object, and articulated hand. Starting from a single human-annotated template for each hand and grasp type, our pipeline tackles the complicated synthesis problem with two stages: optimize the object to fit the hand template first, and then locally refine the hand to fit the object in simulation. To validate the synthesized grasps, we introduce a contact-aware control strategy that allows the hand to apply the appropriate force at each contact point to the object. Those validated grasps can also be used as new grasp templates to facilitate future synthesis. Experiments show that our method significantly outperforms previous type-unaware grasp synthesis baselines in simulation. Using our algorithm, we construct a dataset containing 10.7k objects and 9.5M grasps, covering 31 grasp types in the GRASP taxonomy. Finally, we train a type-conditional generative model that successfully performs the desired grasp type from single-view object point clouds, achieving an 82.3% success rate in real-world experiments. Project page: https://pku-epic.github.io/Dexonomy.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Surgeons vs. Computer Vision: A comparative analysis on surgical phase recognition capabilities</title>
<link>https://arxiv.org/abs/2504.18954</link>
<guid>https://arxiv.org/abs/2504.18954</guid>
<content:encoded><![CDATA[
arXiv:2504.18954v1 Announce Type: cross 
Abstract: Purpose: Automated Surgical Phase Recognition (SPR) uses Artificial Intelligence (AI) to segment the surgical workflow into its key events, functioning as a building block for efficient video review, surgical education as well as skill assessment. Previous research has focused on short and linear surgical procedures and has not explored if temporal context influences experts' ability to better classify surgical phases. This research addresses these gaps, focusing on Robot-Assisted Partial Nephrectomy (RAPN) as a highly non-linear procedure. Methods: Urologists of varying expertise were grouped and tasked to indicate the surgical phase for RAPN on both single frames and video snippets using a custom-made web platform. Participants reported their confidence levels and the visual landmarks used in their decision-making. AI architectures without and with temporal context as trained and benchmarked on the Cholec80 dataset were subsequently trained on this RAPN dataset. Results: Video snippets and presence of specific visual landmarks improved phase classification accuracy across all groups. Surgeons displayed high confidence in their classifications and outperformed novices, who struggled discriminating phases. The performance of the AI models is comparable to the surgeons in the survey, with improvements when temporal context was incorporated in both cases. Conclusion: SPR is an inherently complex task for expert surgeons and computer vision, where both perform equally well when given the same context. Performance increases when temporal information is provided. Surgical tools and organs form the key landmarks for human interpretation and are expected to shape the future of automated SPR.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REED-VAE: RE-Encode Decode Training for Iterative Image Editing with Diffusion Models</title>
<link>https://arxiv.org/abs/2504.18989</link>
<guid>https://arxiv.org/abs/2504.18989</guid>
<content:encoded><![CDATA[
arXiv:2504.18989v1 Announce Type: cross 
Abstract: While latent diffusion models achieve impressive image editing results, their application to iterative editing of the same image is severely restricted. When trying to apply consecutive edit operations using current models, they accumulate artifacts and noise due to repeated transitions between pixel and latent spaces. Some methods have attempted to address this limitation by performing the entire edit chain within the latent space, sacrificing flexibility by supporting only a limited, predetermined set of diffusion editing operations. We present a RE-encode decode (REED) training scheme for variational autoencoders (VAEs), which promotes image quality preservation even after many iterations. Our work enables multi-method iterative image editing: users can perform a variety of iterative edit operations, with each operation building on the output of the previous one using both diffusion-based operations and conventional editing techniques. We demonstrate the advantage of REED-VAE across a range of image editing scenarios, including text-based and mask-based editing frameworks. In addition, we show how REED-VAE enhances the overall editability of images, increasing the likelihood of successful and precise edit operations. We hope that this work will serve as a benchmark for the newly introduced task of multi-method image editing. Our code and models will be available at https://github.com/galmog/REED-VAE
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation</title>
<link>https://arxiv.org/abs/2504.19002</link>
<guid>https://arxiv.org/abs/2504.19002</guid>
<content:encoded><![CDATA[
arXiv:2504.19002v1 Announce Type: cross 
Abstract: This paper introduces a novel deep learning-based multimodal fusion architecture aimed at enhancing the perception capabilities of autonomous navigation robots in complex environments. By utilizing innovative feature extraction modules, adaptive fusion strategies, and time-series modeling mechanisms, the system effectively integrates RGB images and LiDAR data. The key contributions of this work are as follows: a. the design of a lightweight feature extraction network to enhance feature representation; b. the development of an adaptive weighted cross-modal fusion strategy to improve system robustness; and c. the incorporation of time-series information modeling to boost dynamic scene perception accuracy. Experimental results on the KITTI dataset demonstrate that the proposed approach increases navigation and positioning accuracy by 3.5% and 2.2%, respectively, while maintaining real-time performance. This work provides a novel solution for autonomous robot navigation in complex environments.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation</title>
<link>https://arxiv.org/abs/2504.19174</link>
<guid>https://arxiv.org/abs/2504.19174</guid>
<content:encoded><![CDATA[
arXiv:2504.19174v1 Announce Type: cross 
Abstract: We introduce CLR-Wire, a novel framework for 3D curve-based wireframe generation that integrates geometry and topology into a unified Continuous Latent Representation. Unlike conventional methods that decouple vertices, edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along with their topological connectivity into a continuous and fixed-length latent space using an attention-driven variational autoencoder (VAE). This unified approach facilitates joint learning and generation of both geometry and topology. To generate wireframes, we employ a flow matching model to progressively map Gaussian noise to these latents, which are subsequently decoded into complete 3D wireframes. Our method provides fine-grained modeling of complex shapes and irregular topologies, and supports both unconditional generation and generation conditioned on point cloud or image inputs. Experimental results demonstrate that, compared with state-of-the-art generative approaches, our method achieves substantial improvements in accuracy, novelty, and diversity, offering an efficient and comprehensive solution for CAD design, geometric reconstruction, and 3D content creation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</title>
<link>https://arxiv.org/abs/2504.19189</link>
<guid>https://arxiv.org/abs/2504.19189</guid>
<content:encoded><![CDATA[
arXiv:2504.19189v1 Announce Type: cross 
Abstract: Storyboarding is widely used for creating 3D animations. Animators use the 2D sketches in storyboards as references to craft the desired 3D animations through a trial-and-error process. The traditional approach requires exceptional expertise and is both labor-intensive and time-consuming. Consequently, there is a high demand for automated methods that can directly translate 2D storyboard sketches into 3D animations. This task is under-explored to date and inspired by the significant advancements of motion diffusion models, we propose to address it from the perspective of conditional motion synthesis. We thus present Sketch2Anim, composed of two key modules for sketch constraint understanding and motion generation. Specifically, due to the large domain gap between the 2D sketch and 3D motion, instead of directly conditioning on 2D inputs, we design a 3D conditional motion generator that simultaneously leverages 3D keyposes, joint trajectories, and action words, to achieve precise and fine-grained motion control. Then, we invent a neural mapper dedicated to aligning user-provided 2D sketches with their corresponding 3D keyposes and trajectories in a shared embedding space, enabling, for the first time, direct 2D control of motion generation. Our approach successfully transfers storyboards into high-quality 3D motions and inherently supports direct 3D animation editing, thanks to the flexibility of our multi-conditional motion generator. Comprehensive experiments and evaluations, and a user perceptual study demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography</title>
<link>https://arxiv.org/abs/2504.19200</link>
<guid>https://arxiv.org/abs/2504.19200</guid>
<content:encoded><![CDATA[
arXiv:2504.19200v1 Announce Type: cross 
Abstract: In situ synchrotron X-ray computed tomography enables dynamic material studies, but automated segmentation remains challenging due to complex imaging artefacts and limited training data. We present a methodology for deep learning-based segmentation by transforming high-quality ex situ laboratory data to train models for binary segmentation of in situ synchrotron data, demonstrated through copper oxide dissolution studies. Using a modified SegFormer architecture, our approach achieves high segmentation performance on unseen data while reducing processing time from hours to seconds per 3D dataset. The method maintains consistent performance over significant morphological changes during experiments, despite training only on static specimens. This methodology can be readily applied to diverse materials systems, accelerating the analysis of time-resolved tomographic data across scientific disciplines.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction</title>
<link>https://arxiv.org/abs/2504.19203</link>
<guid>https://arxiv.org/abs/2504.19203</guid>
<content:encoded><![CDATA[
arXiv:2504.19203v1 Announce Type: cross 
Abstract: Knee osteoarthritis (KOA) is a common joint disease that causes pain and mobility issues. While MRI-based deep learning models have demonstrated superior performance in predicting total knee replacement (TKR) and disease progression, their generalizability remains challenging, particularly when applied to imaging data from different sources. In this study, we have shown that replacing batch normalization with instance normalization, using data augmentation, and applying contrastive loss improves model generalization in a baseline deep learning model for knee osteoarthritis (KOA) prediction. We trained and evaluated our model using MRI data from the Osteoarthritis Initiative (OAI) database, considering sagittal fat-suppressed intermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain and sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state (DESS) images as the target domain. The results demonstrate a statistically significant improvement in classification accuracy across both domains, with our approach outperforming the baseline model.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantitative evaluation of brain-inspired vision sensors in high-speed robotic perception</title>
<link>https://arxiv.org/abs/2504.19253</link>
<guid>https://arxiv.org/abs/2504.19253</guid>
<content:encoded><![CDATA[
arXiv:2504.19253v1 Announce Type: cross 
Abstract: Perception systems in robotics encounter significant challenges in high-speed and dynamic conditions when relying on traditional cameras, where motion blur can compromise spatial feature integrity and task performance. Brain-inspired vision sensors (BVS) have recently gained attention as an alternative, offering high temporal resolution with reduced bandwidth and power requirements. Here, we present the first quantitative evaluation framework for two representative classes of BVSs in variable-speed robotic sensing, including event-based vision sensors (EVS) that detect asynchronous temporal contrasts, and the primitive-based sensor Tianmouc that employs a complementary mechanism to encode both spatiotemporal changes and intensity. A unified testing protocol is established, including crosssensor calibrations, standardized testing platforms, and quality metrics to address differences in data modality. From an imaging standpoint, we evaluate the effects of sensor non-idealities, such as motion-induced distortion, on the capture of structural information. For functional benchmarking, we examine task performance in corner detection and motion estimation under different rotational speeds. Results indicate that EVS performs well in highspeed, sparse scenarios and in modestly fast, complex scenes, but exhibits performance limitations in high-speed, cluttered settings due to pixel-level bandwidth variations and event rate saturation. In comparison, Tianmouc demonstrates consistent performance across sparse and complex scenarios at various speeds, supported by its global, precise, high-speed spatiotemporal gradient samplings. These findings offer valuable insights into the applicationdependent suitability of BVS technologies and support further advancement in this area.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?</title>
<link>https://arxiv.org/abs/2504.19267</link>
<guid>https://arxiv.org/abs/2504.19267</guid>
<content:encoded><![CDATA[
arXiv:2504.19267v1 Announce Type: cross 
Abstract: Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading</title>
<link>https://arxiv.org/abs/2504.19362</link>
<guid>https://arxiv.org/abs/2504.19362</guid>
<content:encoded><![CDATA[
arXiv:2504.19362v1 Announce Type: cross 
Abstract: Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one of the primary causes of vision loss among retinal vascular diseases. Deep learning methods have been extensively applied in the grading of diabetic retinopathy (DR). However, their performance declines significantly when applied to data outside the training distribution due to domain shifts. Domain generalization (DG) has emerged as a solution to this challenge. However, most existing DG methods overlook lesion-specific features, resulting in insufficient accuracy. In this paper, we propose a novel approach that enhances existing DG methods by incorporating structural priors, inspired by the observation that DR grading is heavily dependent on vessel and lesion structures. We introduce Low-rank Adaptive Structural Priors (LoASP), a plug-and-play framework designed for seamless integration with existing DG models. LoASP improves generalization by learning adaptive structural representations that are finely tuned to the complexities of DR diagnosis. Extensive experiments on eight diverse datasets validate its effectiveness in both single-source and multi-source domain scenarios. Furthermore, visualizations reveal that the learned structural priors intuitively align with the intricate architecture of the vessels and lesions, providing compelling insights into their interpretability and diagnostic relevance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Innovative Integration of 4D Cardiovascular Reconstruction and Hologram: A New Visualization Tool for Coronary Artery Bypass Grafting Planning</title>
<link>https://arxiv.org/abs/2504.19401</link>
<guid>https://arxiv.org/abs/2504.19401</guid>
<content:encoded><![CDATA[
arXiv:2504.19401v1 Announce Type: cross 
Abstract: Background: Coronary artery bypass grafting (CABG) planning requires advanced spatial visualization and consideration of coronary artery depth, calcification, and pericardial adhesions. Objective: To develop and evaluate a dynamic cardiovascular holographic visualization tool for preoperative CABG planning. Methods: Using 4D cardiac computed tomography angiography data from 14 CABG candidates, we developed a semi-automated workflow for time-resolved segmentation of cardiac structures, epicardial adipose tissue (EAT), and coronary arteries with calcium scoring. The workflow incorporated methods for cardiac segmentation, coronary calcification quantification, visualization of coronary depth within EAT, and pericardial adhesion assessment through motion analysis. Dynamic cardiovascular holograms were displayed using the Looking Glass platform. Thirteen cardiac surgeons evaluated the tool using a Likert scale. Additionally, pericardial adhesion scores from holograms of 21 patients (including seven undergoing secondary cardiac surgeries) were compared with intraoperative findings. Results: Surgeons rated the visualization tool highly for preoperative planning utility (mean Likert score: 4.57/5.0). Hologram-based pericardial adhesion scoring strongly correlated with intraoperative findings (r=0.786, P<0.001). Conclusion: This study establishes a visualization framework for CABG planning that produces clinically relevant dynamic holograms from patient-specific data, with clinical feedback confirming its effectiveness for preoperative planning.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting</title>
<link>https://arxiv.org/abs/2504.19408</link>
<guid>https://arxiv.org/abs/2504.19408</guid>
<content:encoded><![CDATA[
arXiv:2504.19408v1 Announce Type: cross 
Abstract: Making accurate weather predictions can be particularly challenging for localized storms or events that evolve on hourly timescales, such as thunderstorms. Hence, our goal for the project was to model Weather Nowcasting for making highly localized and accurate predictions that apply to the immediate future replacing the current numerical weather models and data assimilation systems with Deep Learning approaches. A significant advantage of machine learning is that inference is computationally cheap given an already-trained model, allowing forecasts that are nearly instantaneous and in the native high resolution of the input data. In this work we developed a novel method that employs Transformer-based machine learning models to forecast precipitation. This approach works by leveraging axial attention mechanisms to learn complex patterns and dynamics from time series frames. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings data. This paper represents an initial research on the dataset used in the domain of next frame prediciton, and hence, we demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67, SSIM = 0.9943) used for the given dataset using UNet with Axial Transformer.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Attention Driven Lumbar Magnetic Resonance Image Feature Enhancement and Automatic Diagnosis of Herniation</title>
<link>https://arxiv.org/abs/2504.19438</link>
<guid>https://arxiv.org/abs/2504.19438</guid>
<content:encoded><![CDATA[
arXiv:2504.19438v1 Announce Type: cross 
Abstract: Lumbar disc herniation (LDH) is a common musculoskeletal disease that requires magnetic resonance imaging (MRI) for effective clinical management. However, the interpretation of MRI images heavily relies on the expertise of radiologists, leading to delayed diagnosis and high costs for training physicians. Therefore, this paper proposes an innovative automated LDH classification framework. To address these key issues, the framework utilizes T1-weighted and T2-weighted MRI images from 205 people. The framework extracts clinically actionable LDH features and generates standardized diagnostic outputs by leveraging data augmentation and channel and spatial attention mechanisms. These outputs can help physicians make confident and time-effective care decisions when needed. The proposed framework achieves an area under the receiver operating characteristic curve (AUC-ROC) of 0.969 and an accuracy of 0.9486 for LDH detection. The experimental results demonstrate the performance of the proposed framework. Our framework only requires a small number of datasets for training to demonstrate high diagnostic accuracy. This is expected to be a solution to enhance the LDH detection capabilities of primary hospitals.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution</title>
<link>https://arxiv.org/abs/2504.19595</link>
<guid>https://arxiv.org/abs/2504.19595</guid>
<content:encoded><![CDATA[
arXiv:2504.19595v1 Announce Type: cross 
Abstract: Synthetic image source attribution is an open challenge, with an increasing number of image generators being released yearly. The complexity and the sheer number of available generative techniques, as well as the scarcity of high-quality open source datasets of diverse nature for this task, make training and benchmarking synthetic image source attribution models very challenging. WILD is a new in-the-Wild Image Linkage Dataset designed to provide a powerful training and benchmarking tool for synthetic image attribution models. The dataset is built out of a closed set of 10 popular commercial generators, which constitutes the training base of attribution models, and an open set of 10 additional generators, simulating a real-world in-the-wild scenario. Each generator is represented by 1,000 images, for a total of 10,000 images in the closed set and 10,000 images in the open set. Half of the images are post-processed with a wide range of operators. WILD allows benchmarking attribution models in a wide range of tasks, including closed and open set identification and verification, and robust attribution with respect to post-processing and adversarial attacks. Models trained on WILD are expected to benefit from the challenging scenario represented by the dataset itself. Moreover, an assessment of seven baseline methodologies on closed and open set attribution is presented, including robustness tests with respect to post-processing.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.19627</link>
<guid>https://arxiv.org/abs/2504.19627</guid>
<content:encoded><![CDATA[
arXiv:2504.19627v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient compared to humans who analyze information and generate content at the conceptual level, extracting relevant visual concepts with minimal effort. This inefficiency, stemming from the lack of a visual concept model, limits LVLMs' usability in real-world applications. To address this, we propose VCM, an end-to-end self-supervised visual concept modeling framework. VCM leverages implicit contrastive learning across multiple sampled instances and vision-language fine-tuning to construct a visual concept model without requiring costly concept-level annotations. Our results show that VCM significantly reduces computational costs (e.g., 85\% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance across diverse image understanding tasks. Moreover, VCM enhances visual encoders' capabilities in classic visual concept perception tasks. Extensive quantitative and qualitative experiments validate the effectiveness and efficiency of VCM.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation</title>
<link>https://arxiv.org/abs/2504.19718</link>
<guid>https://arxiv.org/abs/2504.19718</guid>
<content:encoded><![CDATA[
arXiv:2504.19718v1 Announce Type: cross 
Abstract: Face registration deforms a template mesh to closely fit a 3D face scan, the quality of which commonly degrades in non-skin regions (e.g., hair, beard, accessories), because the optimized template-to-scan distance pulls the template mesh towards the noisy scan surface. Improving registration quality requires a clean separation of skin and non-skin regions on the scan mesh. Existing image-based (2D) or scan-based (3D) segmentation methods however perform poorly. Image-based segmentation outputs multi-view inconsistent masks, and they cannot account for scan inaccuracies or scan-image misalignment, while scan-based methods suffer from lower spatial resolution compared to images. In this work, we introduce a novel method that accurately separates skin from non-skin geometry on 3D human head scans. For this, our method extracts features from multi-view images using a frozen image foundation model and aggregates these features in 3D. These lifted 2D features are then fused with 3D geometric features extracted from the scan mesh, to then predict a segmentation mask directly on the scan mesh. We show that our segmentations improve the registration accuracy over pure 2D or 3D segmentation methods by 8.89% and 14.3%, respectively. Although trained only on synthetic data, our model generalizes well to real data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Brenier Potentials with Convex Generative Adversarial Neural Networks</title>
<link>https://arxiv.org/abs/2504.19779</link>
<guid>https://arxiv.org/abs/2504.19779</guid>
<content:encoded><![CDATA[
arXiv:2504.19779v1 Announce Type: cross 
Abstract: Brenier proved that under certain conditions on a source and a target probability measure there exists a strictly convex function such that its gradient is a transport map from the source to the target distribution. This function is called the Brenier potential. Furthermore, detailed information on the H\"older regularity of the Brenier potential is available. In this work we develop the statistical learning theory of generative adversarial neural networks that learn the Brenier potential. As by the transformation of densities formula, the density of the generated measure depends on the second derivative of the Brenier potential, we develop the universal approximation theory of ReCU networks with cubic activation $\mathtt{ReCU}(x)=\max\{0,x\}^3$ that combines the favorable approximation properties of H\"older functions with a Lipschitz continuous density. In order to assure the convexity of such general networks, we introduce an adversarial training procedure for a potential function represented by the ReCU networks that combines the classical discriminator cross entropy loss with a penalty term that enforces (strict) convexity. We give a detailed decomposition of learning errors and show that for a suitable high penalty parameter all networks chosen in the adversarial min-max optimization problem are strictly convex. This is further exploited to prove the consistency of the learning procedure for (slowly) expanding network capacity. We also implement the described learning algorithm and apply it to a number of standard test cases from Gaussian mixture to image data as target distributions. As predicted in theory, we observe that the convexity loss becomes inactive during the training process and the potentials represented by the neural networks have learned convexity.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mj\"olnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density</title>
<link>https://arxiv.org/abs/2504.19822</link>
<guid>https://arxiv.org/abs/2504.19822</guid>
<content:encoded><![CDATA[
arXiv:2504.19822v1 Announce Type: cross 
Abstract: Recent advances in AI-based weather forecasting models, such as FourCastNet, Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep learning to emulate complex atmospheric dynamics. Building on this momentum, we propose Mj\"olnir, a novel deep learning-based framework for global lightning flash density parameterization. Trained on ERA5 atmospheric predictors and World Wide Lightning Location Network (WWLLN) observations at a daily temporal resolution and 1 degree spatial resolution, Mj\"olnir captures the nonlinear mapping between large-scale environmental conditions and lightning activity. The model architecture is based on the InceptionNeXt backbone with SENet, and a multi-task learning strategy to simultaneously predict lightning occurrence and magnitude. Extensive evaluations yield that Mollnir accurately reproduces the global distribution, seasonal variability, and regional characteristics of lightning activity, achieving a global Pearson correlation coefficient of 0.96 for annual mean fields. These results suggest that Mj\"olnir serves not only as an effective data-driven global lightning parameterization but also as a promising AI-based scheme for next-generation Earth system models (AI-ESMs).
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks</title>
<link>https://arxiv.org/abs/2504.19854</link>
<guid>https://arxiv.org/abs/2504.19854</guid>
<content:encoded><![CDATA[
arXiv:2504.19854v1 Announce Type: cross 
Abstract: Existing Visual-Language-Action (VLA) models have shown promising performance in zero-shot scenarios, demonstrating impressive task execution and reasoning capabilities. However, a significant challenge arises from the limitations of visual encoding, which can result in failures during tasks such as object grasping. Moreover, these models typically suffer from high computational overhead due to their large sizes, often exceeding 7B parameters. While these models excel in reasoning and task planning, the substantial computational overhead they incur makes them impractical for real-time robotic environments, where speed and efficiency are paramount. To address the limitations of existing VLA models, we propose NORA, a 3B-parameter model designed to reduce computational overhead while maintaining strong task performance. NORA adopts the Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior visual-semantic understanding to enhance visual reasoning and action grounding. Additionally, our \model{} is trained on 970k real-world robot demonstrations and equipped with the FAST+ tokenizer for efficient action sequence generation. Experimental results demonstrate that NORA outperforms existing large-scale VLA models, achieving better task performance with significantly reduced computational overhead, making it a more practical solution for real-time robotic autonomy.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated 3D-3D rigid registration of echocardiographic images obtained from apical window using particle filter</title>
<link>https://arxiv.org/abs/2504.19930</link>
<guid>https://arxiv.org/abs/2504.19930</guid>
<content:encoded><![CDATA[
arXiv:2504.19930v1 Announce Type: cross 
Abstract: The perfect alignment of 3D echocardiographic images captured from various angles has improved image quality and broadened the field of view. This study proposes an accelerated sequential Monte Carlo (SMC) algorithm for 3D-3D rigid registration of transthoracic echocardiographic images with significant and limited overlap taken from apical window that is robust to the noise and intensity variation in ultrasound images. The algorithm estimates the translational and rotational components of the rigid transform through an iterative process and requires an initial approximation of the rotation and translation limits. We perform registration in two ways: the image-based registration computes the transform to align the end-diastolic frame of the apical nonstandard image to the apical standard image and applies the same transform to all frames of the cardiac cycle, whereas the mask-based registration approach uses the binary masks of the left ventricle in the same way. The SMC and exhaustive search (EX) algorithms were evaluated for 4D temporal sequences recorded from 7 volunteers who participated in a study conducted at the Mazankowski Alberta Heart Institute. The evaluations demonstrate that the mask-based approach of the accelerated SMC yielded a Dice score value of 0.819 +/- 0.045 for the left ventricle and gained 16.7x speedup compared to the CPU version of the SMC algorithm.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SST-DUNet: Automated preclinical functional MRI skull stripping using Smart Swin Transformer and Dense UNet</title>
<link>https://arxiv.org/abs/2504.19937</link>
<guid>https://arxiv.org/abs/2504.19937</guid>
<content:encoded><![CDATA[
arXiv:2504.19937v1 Announce Type: cross 
Abstract: Skull stripping is a common preprocessing step that is often performed manually in Magnetic Resonance Imaging (MRI) pipelines, including functional MRI (fMRI). This manual process is time-consuming and operator dependent. Automating this process is challenging for preclinical data due to variations in brain geometry, resolution, and tissue contrast. While existing methods for MRI skull stripping exist, they often struggle with the low resolution and varying slice sizes in preclinical fMRI data. This study proposes a novel method called SST-DUNet, that integrates a dense UNet-based architecture with a feature extractor based on Smart Swin Transformer (SST) for fMRI skull stripping. The Smart Shifted Window Multi-Head Self-Attention (SSW-MSA) module in SST is adapted to replace the mask-based module in the Swin Transformer (ST), enabling the learning of distinct channel-wise features while focusing on relevant dependencies within brain structures. This modification allows the model to better handle the complexities of fMRI skull stripping, such as low resolution and variable slice sizes. To address the issue of class imbalance in preclinical data, a combined loss function using Focal and Dice loss is utilized. The model was trained on rat fMRI images and evaluated across three in-house datasets with a Dice similarity score of 98.65%, 97.86%, and 98.04%. The fMRI results obtained through automatic skull stripping using the SST-DUNet model closely align with those from manual skull stripping for both seed-based and independent component analyses. These results indicate that the SST-DUNet can effectively substitute manual brain extraction in rat fMRI analysis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage</title>
<link>https://arxiv.org/abs/2504.20007</link>
<guid>https://arxiv.org/abs/2504.20007</guid>
<content:encoded><![CDATA[
arXiv:2504.20007v1 Announce Type: cross 
Abstract: This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating video, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. We present our methodology, computational techniques, and findings, outlining a practical approach for law enforcement while advancing the frontiers of knowledge discovery from police BWC data.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory Regulation and Alignment toward Generalizer RGB-Infrared Person</title>
<link>https://arxiv.org/abs/2109.08843</link>
<guid>https://arxiv.org/abs/2109.08843</guid>
<content:encoded><![CDATA[
arXiv:2109.08843v2 Announce Type: replace 
Abstract: The domain shift, coming from unneglectable modality gap and non-overlapped identity classes between training and test sets, is a major issue of RGB-Infrared person re-identification. A key to tackle the inherent issue -- domain shift -- is to enforce the data distributions of the two domains to be similar. However, RGB-IR ReID always demands discriminative features, leading to over-rely feature sensitivity of seen classes, \textit{e.g.}, via attention-based feature alignment or metric learning. Therefore, predicting the unseen query category from predefined training classes may not be accurate and leads to a sub-optimal adversarial gradient. In this paper, we uncover it in a more explainable way and propose a novel multi-granularity memory regulation and alignment module (MG-MRA) to solve this issue. By explicitly incorporating a latent variable attribute, from fine-grained to coarse semantic granularity, into intermediate features, our method could alleviate the over-confidence of the model about discriminative features of seen classes. Moreover, instead of matching discriminative features by traversing nearest neighbor, sparse attributes, \textit{i.e.}, global structural pattern, are recollected with respect to features and assigned to measure pair-wise image similarity in hashing. Extensive experiments on RegDB \cite{RegDB} and SYSU-MM01 \cite{SYSU} show the superiority of the proposed method that outperforms existing state-of-the-art methods. Our code is available in https://github.com/Chenfeng1271/MGMRA.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Injecting Image Details into CLIP's Feature Space</title>
<link>https://arxiv.org/abs/2208.14649</link>
<guid>https://arxiv.org/abs/2208.14649</guid>
<content:encoded><![CDATA[
arXiv:2208.14649v5 Announce Type: replace 
Abstract: Although CLIP-like Visual Language Models provide a functional joint feature space for image and text, due to the limitation of the CILP-like model's image input size (e.g., 224), subtle details are lost in the feature representation if we input high-resolution images (e.g., 2240). In this work, we introduce an efficient framework that can produce a single feature representation for a high-resolution image that injects image details and shares the same semantic space as the original CLIP. In the framework, we train a feature fusing model based on CLIP features extracted from a carefully designed image patch method that can cover objects of any scale, weakly supervised by image-agnostic class prompted queries. We validate our framework by retrieving images from class prompted queries on the real world and synthetic datasets, showing significant performance improvement on these tasks. Furthermore, to fully demonstrate our framework's detail retrieval ability, we construct a CLEVR-like synthetic dataset called CLVER-DS, which is fully annotated and has a controllable object scale.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEVICE: Depth and Visual Concepts Aware Transformer for OCR-based Image Captioning</title>
<link>https://arxiv.org/abs/2302.01540</link>
<guid>https://arxiv.org/abs/2302.01540</guid>
<content:encoded><![CDATA[
arXiv:2302.01540v4 Announce Type: replace 
Abstract: OCR-based image captioning is an important but under-explored task, aiming to generate descriptions containing visual objects and scene text. Recent studies have made encouraging progress, but they are still suffering from a lack of overall understanding of scenes and generating inaccurate captions. One possible reason is that current studies mainly focus on constructing the plane-level geometric relationship of scene text without depth information. This leads to insufficient scene text relational reasoning so that models may describe scene text inaccurately. The other possible reason is that existing methods fail to generate fine-grained descriptions of some visual objects. In addition, they may ignore essential visual objects, leading to the scene text belonging to these ignored objects not being utilized. To address the above issues, we propose a Depth and Visual Concepts Aware Transformer (DEVICE) for OCR-based image captinong. Concretely, to construct three-dimensional geometric relations, we introduce depth information and propose a depth-enhanced feature updating module to ameliorate OCR token features. To generate more precise and comprehensive captions, we introduce semantic features of detected visual concepts as auxiliary information, and propose a semantic-guided alignment module to improve the model's ability to utilize visual concepts. Our DEVICE is capable of comprehending scenes more comprehensively and boosting the accuracy of described visual entities. Sufficient experiments demonstrate the effectiveness of our proposed DEVICE, which outperforms state-of-the-art models on the TextCaps test set.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local-Global Temporal Difference Learning for Satellite Video Super-Resolution</title>
<link>https://arxiv.org/abs/2304.04421</link>
<guid>https://arxiv.org/abs/2304.04421</guid>
<content:encoded><![CDATA[
arXiv:2304.04421v3 Announce Type: replace 
Abstract: Optical-flow-based and kernel-based approaches have been extensively explored for temporal compensation in satellite Video Super-Resolution (VSR). However, these techniques are less generalized in large-scale or complex scenarios, especially in satellite videos. In this paper, we propose to exploit the well-defined temporal difference for efficient and effective temporal compensation. To fully utilize the local and global temporal information within frames, we systematically modeled the short-term and long-term temporal discrepancies since we observed that these discrepancies offer distinct and mutually complementary properties. Specifically, we devise a Short-term Temporal Difference Module (S-TDM) to extract local motion representations from RGB difference maps between adjacent frames, which yields more clues for accurate texture representation. To explore the global dependency in the entire frame sequence, a Long-term Temporal Difference Module (L-TDM) is proposed, where the differences between forward and backward segments are incorporated and activated to guide the modulation of the temporal feature, leading to a holistic global compensation. Moreover, we further propose a Difference Compensation Unit (DCU) to enrich the interaction between the spatial distribution of the target frame and temporal compensated results, which helps maintain spatial consistency while refining the features to avoid misalignment. Rigorous objective and subjective evaluations conducted across five mainstream video satellites demonstrate that our method performs favorably against state-of-the-art approaches. Code will be available at https://github.com/XY-boy/LGTD
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEDORA: Flying Event Dataset fOr Reactive behAvior</title>
<link>https://arxiv.org/abs/2305.14392</link>
<guid>https://arxiv.org/abs/2305.14392</guid>
<content:encoded><![CDATA[
arXiv:2305.14392v3 Announce Type: replace 
Abstract: The ability of resource-constrained biological systems such as fruitflies to perform complex and high-speed maneuvers in cluttered environments has been one of the prime sources of inspiration for developing vision-based autonomous systems. To emulate this capability, the perception pipeline of such systems must integrate information cues from tasks including optical flow and depth estimation, object detection and tracking, and segmentation, among others. However, the conventional approach of employing slow, synchronous inputs from standard frame-based cameras constrains these perception capabilities, particularly during high-speed maneuvers. Recently, event-based sensors have emerged as low latency and low energy alternatives to standard frame-based cameras for capturing high-speed motion, effectively speeding up perception and hence navigation. For coherence, all the perception tasks must be trained on the same input data. However, present-day datasets are curated mainly for a single or a handful of tasks and are limited in the rate of the provided ground truths. To address these limitations, we present Flying Event Dataset fOr Reactive behAviour (FEDORA) - a fully synthetic dataset for perception tasks, with raw data from frame-based cameras, event-based cameras, and Inertial Measurement Units (IMU), along with ground truths for depth, pose, and optical flow at a rate much higher than existing datasets.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-Tailed Continual Learning For Visual Food Recognition</title>
<link>https://arxiv.org/abs/2307.00183</link>
<guid>https://arxiv.org/abs/2307.00183</guid>
<content:encoded><![CDATA[
arXiv:2307.00183v2 Announce Type: replace 
Abstract: Deep learning-based food recognition has made significant progress in predicting food types from eating occasion images. However, two key challenges hinder real-world deployment: (1) continuously learning new food classes without forgetting previously learned ones, and (2) handling the long-tailed distribution of food images, where a few common classes and many more rare classes. To address these, food recognition methods should focus on long-tailed continual learning. In this work, We introduce a dataset that encompasses 186 American foods along with comprehensive annotations. We also introduce three new benchmark datasets, VFN186-LT, VFN186-INSULIN and VFN186-T2D, which reflect real-world food consumption for healthy populations, insulin takers and individuals with type 2 diabetes without taking insulin. We propose a novel end-to-end framework that improves the generalization ability for instance-rare food classes using a knowledge distillation-based predictor to avoid misalignment of representation during continual learning. Additionally, we introduce an augmentation technique by integrating class-activation-map (CAM) and CutMix to improve generalization on instance-rare food classes. Our method, evaluated on Food101-LT, VFN-LT, VFN186-LT, VFN186-INSULIN, and VFN186-T2DM, shows significant improvements over existing methods. An ablation study highlights further performance enhancements, demonstrating its potential for real-world food recognition applications.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OccluTrack: Rethinking Awareness of Occlusion for Enhancing Multiple Pedestrian Tracking</title>
<link>https://arxiv.org/abs/2309.10360</link>
<guid>https://arxiv.org/abs/2309.10360</guid>
<content:encoded><![CDATA[
arXiv:2309.10360v2 Announce Type: replace 
Abstract: Multiple pedestrian tracking is crucial for enhancing safety and efficiency in intelligent transport and autonomous driving systems by predicting movements and enabling adaptive decision-making in dynamic environments. It optimizes traffic flow, facilitates human interaction, and ensures compliance with regulations. However, it faces the challenge of tracking pedestrians in the presence of occlusion. Existing methods overlook effects caused by abnormal detections during partial occlusion. Subsequently, these abnormal detections can lead to inaccurate motion estimation, unreliable appearance features, and unfair association. To address these issues, we propose an adaptive occlusion-aware multiple pedestrian tracker, OccluTrack, to mitigate the effects caused by partial occlusion. Specifically, we first introduce a plug-and-play abnormal motion suppression mechanism into the Kalman Filter to adaptively detect and suppress outlier motions caused by partial occlusion. Second, we develop a pose-guided re-identification (Re-ID) module to extract discriminative part features for partially occluded pedestrians. Last, we develop a new occlusion-aware association method towards fair Intersection over Union (IoU) and appearance embedding distance measurement for occluded pedestrians. Extensive evaluation results demonstrate that our method outperforms state-of-the-art methods on MOTChallenge and DanceTrack datasets. Particularly, the performance improvements on IDF1 and ID Switches, as well as visualized results, demonstrate the effectiveness of our method in multiple pedestrian tracking.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shape-centered Representation Learning for Visible-Infrared Person Re-identification</title>
<link>https://arxiv.org/abs/2310.17952</link>
<guid>https://arxiv.org/abs/2310.17952</guid>
<content:encoded><![CDATA[
arXiv:2310.17952v3 Announce Type: replace 
Abstract: Visible-Infrared Person Re-Identification (VI-ReID) plays a critical role in all-day surveillance systems. However, existing methods primarily focus on learning appearance features while overlooking body shape features, which not only complement appearance features but also exhibit inherent robustness to modality variations. Despite their potential, effectively integrating shape and appearance features remains challenging. Appearance features are highly susceptible to modality variations and background noise, while shape features often suffer from inaccurate infrared shape estimation due to the limitations of auxiliary models. To address these challenges, we propose the Shape-centered Representation Learning (ScRL) framework, which enhances VI-ReID performance by innovatively integrating shape and appearance features. Specifically, we introduce Infrared Shape Restoration (ISR) to restore inaccuracies in infrared body shape representations at the feature level by leveraging infrared appearance features. In addition, we propose Shape Feature Propagation (SFP), which enables the direct extraction of shape features from original images during inference with minimal computational complexity. Furthermore, we design Appearance Feature Enhancement (AFE), which utilizes shape features to emphasize shape-related appearance features while effectively suppressing identity-unrelated noise. Benefiting from the effective integration of shape and appearance features, ScRL demonstrates superior performance through extensive experiments. On the SYSU-MM01, HITSZ-VCM, and RegDB datasets, it achieves Rank-1 (mAP) accuracies of 76.1% (72.6%), 71.2% (52.9%), and 92.4% (86.7%), respectively, surpassing existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MENTOR: Human Perception-Guided Pretraining for Increased Generalization</title>
<link>https://arxiv.org/abs/2310.19545</link>
<guid>https://arxiv.org/abs/2310.19545</guid>
<content:encoded><![CDATA[
arXiv:2310.19545v3 Announce Type: replace 
Abstract: Leveraging human perception into training of convolutional neural networks (CNN) has boosted generalization capabilities of such models in open-set recognition tasks. One of the active research questions is where (in the model architecture or training pipeline) and how to efficiently incorporate always limited human perceptual data into training strategies of models. In this paper, we introduce MENTOR (huMan pErceptioN-guided preTraining fOr increased geneRalization), which addresses this question through two unique rounds of training CNNs tasked with open-set anomaly detection. First, we train an autoencoder to learn human saliency maps given an input image, without any class labels. The autoencoder is thus tasked with discovering domain-specific salient features which mimic human perception. Second, we remove the decoder part, add a classification layer on top of the encoder, and train this new model conventionally, now using class labels. We show that MENTOR successfully raises the generalization performance across three different CNN backbones in a variety of anomaly detection tasks (demonstrated for detection of unknown iris presentation attacks, synthetically-generated faces, and anomalies in chest X-ray images) compared to traditional pretraining methods (e.g., sourcing the weights from ImageNet), and as well as state-of-the-art methods that incorporate human perception guidance into training. In addition, we demonstrate that MENTOR can be flexibly applied to existing human perception-guided methods and subsequently increasing their generalization with no architectural modifications.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infusion: internal diffusion for inpainting of dynamic textures and complex motion</title>
<link>https://arxiv.org/abs/2311.01090</link>
<guid>https://arxiv.org/abs/2311.01090</guid>
<content:encoded><![CDATA[
arXiv:2311.01090v4 Announce Type: replace 
Abstract: Video inpainting is the task of filling a region in a video in a visually convincing manner. It is very challenging due to the high dimensionality of the data and the temporal consistency required for obtaining convincing results. Recently, diffusion models have shown impressive results in modeling complex data distributions, including images and videos. Such models remain nonetheless very expensive to train and to perform inference with, which strongly reduce their applicability to videos, and yields unreasonable computational loads. We show that in the case of video inpainting, thanks to the highly auto-similar nature of videos, the training data of a diffusion model can be restricted to the input video and still produce very satisfying results. With this internal learning approach, where the training data is limited to a single video, our lightweight models perform very well with only half a million parameters, in contrast to the very large networks with billions of parameters typically found in the literature. We also introduce a new method for efficient training and inference of diffusion models in the context of internal learning, by splitting the diffusion process into different learning intervals corresponding to different noise levels of the diffusion process. We show qualitative and quantitative results, demonstrating that our method reaches or exceeds state of the art performance in the case of dynamic textures and complex dynamic backgrounds
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Syntactic Discrepancy in Images (SSDI): Learning Meaning and Order of Features from Natural Images</title>
<link>https://arxiv.org/abs/2401.17515</link>
<guid>https://arxiv.org/abs/2401.17515</guid>
<content:encoded><![CDATA[
arXiv:2401.17515v2 Announce Type: replace 
Abstract: Despite considerable progress in image classification tasks, classification models seem unaffected by the images that significantly deviate from those that appear natural to human eyes. Specifically, while human perception can easily identify abnormal appearances or compositions in images, classification models overlook any alterations in the arrangement of object parts as long as they are present in any order, even if unnatural. Hence, this work exposes the vulnerability of having semantic and syntactic discrepancy in images (SSDI) in the form of corruptions that remove or shuffle image patches or present images in the form of puzzles. To address this vulnerability, we propose the concept of "image grammar", comprising "image semantics" and "image syntax". Image semantics pertains to the interpretation of parts or patches within an image, whereas image syntax refers to the arrangement of these parts to form a coherent object. We present a semi-supervised two-stage method for learning the image grammar of visual elements and environments solely from natural images. While the first stage learns the semantic meaning of individual object parts, the second stage learns how their relative arrangement constitutes an entire object. The efficacy of the proposed approach is then demonstrated by achieving SSDI detection rates ranging from 70% to 90% on corruptions generated from CelebA and SUN-RGBD datasets. Code is publicly available at: https://github.com/ChunTao1999/SSDI/
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIR: Multi-view Inverse Rendering with Decomposable Shadow Under Indoor Intense Lighting</title>
<link>https://arxiv.org/abs/2402.06136</link>
<guid>https://arxiv.org/abs/2402.06136</guid>
<content:encoded><![CDATA[
arXiv:2402.06136v4 Announce Type: replace 
Abstract: We propose SIR, an efficient method to decompose differentiable shadows for inverse rendering on indoor scenes using multi-view data, addressing the challenges in accurately decomposing the materials and lighting conditions. Unlike previous methods that struggle with shadow fidelity in complex lighting environments, our approach explicitly learns shadows for enhanced realism in material estimation under unknown light positions. Utilizing posed HDR images as input, SIR employs an SDF-based neural radiance field for comprehensive scene representation. Then, SIR integrates a shadow term with a three-stage material estimation approach to improve SVBRDF quality. Specifically, SIR is designed to learn a differentiable shadow, complemented by BRDF regularization, to optimize inverse rendering accuracy. Extensive experiments on both synthetic and real-world indoor scenes demonstrate the superior performance of SIR over existing methods in both quantitative metrics and qualitative analysis. The significant decomposing ability of SIR enables sophisticated editing capabilities like free-view relighting, object insertion, and material replacement. The code and data are available at https://xiaokangwei.github.io/SIR/.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit Motion Handling and Interactive Prompting for Video Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2403.01968</link>
<guid>https://arxiv.org/abs/2403.01968</guid>
<content:encoded><![CDATA[
arXiv:2403.01968v2 Announce Type: replace 
Abstract: Camouflage poses challenges in distinguishing a static target, whereas any movement of the target can break this disguise. Existing video camouflaged object detection (VCOD) approaches take noisy motion estimation as input or model motion implicitly, restricting detection performance in complex dynamic scenes. In this paper, we propose a novel Explicit Motion handling and Interactive Prompting framework for VCOD, dubbed EMIP, which handles motion cues explicitly using a frozen pre-trained optical flow fundamental model. EMIP is characterized by a two-stream architecture for simultaneously conducting camouflaged segmentation and optical flow estimation. Interactions across the dual streams are realized in an interactive prompting way that is inspired by emerging visual prompt learning. Two learnable modules, i.e., the camouflaged feeder and motion collector, are designed to incorporate segmentation-to-motion and motion-to-segmentation prompts, respectively, and enhance outputs of the both streams. The prompt fed to the motion stream is learned by supervising optical flow in a self-supervised manner. Furthermore, we show that long-term historical information can also be incorporated as a prompt into EMIP and achieve more robust results with temporal consistency. Experimental results demonstrate that our EMIP achieves new state-of-the-art records on popular VCOD benchmarks. Our code is made publicly available at https://github.com/zhangxin06/EMIP.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Image Segmentation via Hybrid Convolutional-Transformer Architecture Search</title>
<link>https://arxiv.org/abs/2403.10413</link>
<guid>https://arxiv.org/abs/2403.10413</guid>
<content:encoded><![CDATA[
arXiv:2403.10413v2 Announce Type: replace 
Abstract: Image segmentation is one of the most fundamental problems in computer vision and has drawn a lot of attention due to its vast applications in image understanding and autonomous driving. However, designing effective and efficient segmentation neural architectures is a labor-intensive process that may require numerous trials by human experts. In this paper, we address the challenge of integrating multi-head self-attention into high-resolution representation CNNs efficiently by leveraging architecture search. Manually replacing convolution layers with multi-head self-attention is non-trivial due to the costly overhead in memory to maintain high resolution. By contrast, we develop a multi-target multi-branch supernet method, which not only fully utilizes the advantages of high-resolution features but also finds the proper location for placing the multi-head self-attention module. Our search algorithm is optimized towards multiple objectives (e.g., latency and mIoU) and is capable of finding architectures on the Pareto frontier with an arbitrary number of branches in a single search. We further present a series of models via the Hybrid Convolutional-Transformer Architecture Search (HyCTAS) method that searches for the best hybrid combination of light-weight convolution layers and memory-efficient self-attention layers between branches from different resolutions and fuses them to high resolution for both efficiency and effectiveness. Extensive experiments demonstrate that HyCTAS outperforms previous methods in both semantic segmentation and panoptic segmentation tasks. Code and models are available at https://github.com/MarvinYu1995/HyCTAS.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2403.19103</link>
<guid>https://arxiv.org/abs/2403.19103</guid>
<content:encoded><![CDATA[
arXiv:2403.19103v3 Announce Type: replace 
Abstract: Prompt engineering is an effective but labor-intensive way to control text-to-image (T2I) generative models. Its time-intensive nature and complexity have spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, or produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically produces human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompt distribution built upon the reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles, and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models</title>
<link>https://arxiv.org/abs/2403.20331</link>
<guid>https://arxiv.org/abs/2403.20331</guid>
<content:encoded><![CDATA[
arXiv:2403.20331v3 Announce Type: replace 
Abstract: This paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed $\textbf{Unsolvable Problem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. For the evaluation, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. A detailed analysis shows that LMMs have different bottlenecks and chain-of-thought and self-reflection improved performance for LMMs with the bottleneck in their LLM capability. We hope our insights will enhance the broader understanding and development of more reliable LMMs.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaneCorrect: Self-supervised Lane Detection</title>
<link>https://arxiv.org/abs/2404.14671</link>
<guid>https://arxiv.org/abs/2404.14671</guid>
<content:encoded><![CDATA[
arXiv:2404.14671v2 Announce Type: replace 
Abstract: Lane detection has evolved highly functional autonomous driving system to understand driving scenes even under complex environments. In this paper, we work towards developing a generalized computer vision system able to detect lanes without using any annotation. We make the following contributions: (i) We illustrate how to perform unsupervised 3D lane segmentation by leveraging the distinctive intensity of lanes on the LiDAR point cloud frames, and then obtain the noisy lane labels in the 2D plane by projecting the 3D points; (ii) We propose a novel self-supervised training scheme, dubbed LaneCorrect, that automatically corrects the lane label by learning geometric consistency and instance awareness from the adversarial augmentations; (iii) With the self-supervised pre-trained model, we distill to train a student network for arbitrary target lane (e.g., TuSimple) detection without any human labels; (iv) We thoroughly evaluate our self-supervised method on four major lane detection benchmarks (including TuSimple, CULane, CurveLanes and LLAMAS) and demonstrate excellent performance compared with existing supervised counterpart, whilst showing more effective results on alleviating the domain gap, i.e., training on CULane and test on TuSimple.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComFace: Facial Representation Learning with Synthetic Data for Comparing Faces</title>
<link>https://arxiv.org/abs/2405.16016</link>
<guid>https://arxiv.org/abs/2405.16016</guid>
<content:encoded><![CDATA[
arXiv:2405.16016v3 Announce Type: replace 
Abstract: Daily monitoring of intra-personal facial changes associated with health and emotional conditions has great potential to be useful for medical, healthcare, and emotion recognition fields. However, the approach for capturing intra-personal facial changes is relatively unexplored due to the difficulty of collecting temporally changing face images. In this paper, we propose a facial representation learning method using synthetic images for comparing faces, called ComFace, which is designed to capture intra-personal facial changes. For effective representation learning, ComFace aims to acquire two feature representations, i.e., inter-personal facial differences and intra-personal facial changes. The key point of our method is the use of synthetic face images to overcome the limitations of collecting real intra-personal face images. Facial representations learned by ComFace are transferred to three extensive downstream tasks for comparing faces: estimating facial expression changes, weight changes, and age changes from two face images of the same individual. Our ComFace, trained using only synthetic data, achieves comparable to or better transfer performance than general pre-training and state-of-the-art representation learning methods trained using real images.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReDistill: Residual Encoded Distillation for Peak Memory Reduction of CNNs</title>
<link>https://arxiv.org/abs/2406.03744</link>
<guid>https://arxiv.org/abs/2406.03744</guid>
<content:encoded><![CDATA[
arXiv:2406.03744v3 Announce Type: replace 
Abstract: The expansion of neural network sizes and the enhanced resolution of modern image sensors result in heightened memory and power demands to process modern computer vision models. In order to deploy these models in extremely resource-constrained edge devices, it is crucial to reduce their peak memory, which is the maximum memory consumed during the execution of a model. A naive approach to reducing peak memory is aggressive down-sampling of feature maps via pooling with large stride, which often results in unacceptable degradation in network performance. To mitigate this problem, we propose residual encoded distillation (ReDistill) for peak memory reduction in a teacher-student framework, in which a student network with less memory is derived from the teacher network using aggressive pooling. We apply our distillation method to multiple problems in computer vision, including image classification and diffusion-based image generation. For image classification, our method yields 4x-5x theoretical peak memory reduction with less degradation in accuracy for most CNN-based architectures. For diffusion-based image generation, our proposed distillation method yields a denoising network with 4x lower theoretical peak memory while maintaining decent diversity and fidelity for image generation. Experiments demonstrate our method's superior performance compared to other feature-based and response-based distillation methods when applied to the same student network. The code is available at https://github.com/mengtang-lab/ReDistill.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Video Anomaly Detection for Anomalous Scenarios in Autonomous Driving</title>
<link>https://arxiv.org/abs/2406.06423</link>
<guid>https://arxiv.org/abs/2406.06423</guid>
<content:encoded><![CDATA[
arXiv:2406.06423v3 Announce Type: replace 
Abstract: In autonomous driving, the most challenging scenarios can only be detected within their temporal context. Most video anomaly detection approaches focus either on surveillance or traffic accidents, which are only a subfield of autonomous driving. We present HF$^2$-VAD$_{AD}$, a variation of the HF$^2$-VAD surveillance video anomaly detection method for autonomous driving. We learn a representation of normality from a vehicle's ego perspective and evaluate pixel-wise anomaly detections in rare and critical scenarios.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Machine Learning Driven Material Defect Detection</title>
<link>https://arxiv.org/abs/2406.07880</link>
<guid>https://arxiv.org/abs/2406.07880</guid>
<content:encoded><![CDATA[
arXiv:2406.07880v2 Announce Type: replace 
Abstract: Material defects (MD) represent a primary challenge affecting product performance and giving rise to safety issues in related products. The rapid and accurate identification and localization of MD constitute crucial research endeavors in addressing contemporary challenges associated with MD. In recent years, propelled by the swift advancement of machine learning (ML) technologies, particularly exemplified by deep learning, ML has swiftly emerged as the core technology and a prominent research direction for material defect detection (MDD). Through a comprehensive review of the latest literature, we systematically survey the ML techniques applied in MDD into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning. We provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques. Furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy. Finally, the survey explores potential future directions in MDD utilizing ML technologies. This survey consolidates ML-based MDD literature and provides a foundation for future research and practice.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accessible, At-Home Detection of Parkinson's Disease via Multi-task Video Analysis</title>
<link>https://arxiv.org/abs/2406.14856</link>
<guid>https://arxiv.org/abs/2406.14856</guid>
<content:encoded><![CDATA[
arXiv:2406.14856v5 Announce Type: replace 
Abstract: Limited accessibility to neurological care leads to underdiagnosed Parkinson's Disease (PD), preventing early intervention. Existing AI-based PD detection methods primarily focus on unimodal analysis of motor or speech tasks, overlooking the multifaceted nature of the disease. To address this, we introduce a large-scale, multi-task video dataset consisting of 1102 sessions (each containing videos of finger tapping, facial expression, and speech tasks captured via webcam) from 845 participants (272 with PD). We propose a novel Uncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal data to enhance diagnostic accuracy. UFNet employs independent task-specific networks, trained with Monte Carlo Dropout for uncertainty quantification, followed by self-attended fusion of features, with attention weights dynamically adjusted based on task-specific uncertainties. To ensure patient-centered evaluation, the participants were randomly split into three sets: 60% for training, 20% for model selection, and 20% for final performance evaluation. UFNet significantly outperformed single-task models in terms of accuracy, area under the ROC curve (AUROC), and sensitivity while maintaining non-inferior specificity. Withholding uncertain predictions further boosted the performance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9% sensitivity, and 92.6+-0.3% specificity, at the expense of not being able to predict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further analysis suggests that the trained model does not exhibit any detectable bias across sex and ethnic subgroups and is most effective for individuals aged between 50 and 80. Requiring only a webcam and microphone, our approach facilitates accessible home-based PD screening, especially in regions with limited healthcare resources.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GS-ROR$^2$: Bidirectional-guided 3DGS and SDF for Reflective Object Relighting and Reconstruction</title>
<link>https://arxiv.org/abs/2406.18544</link>
<guid>https://arxiv.org/abs/2406.18544</guid>
<content:encoded><![CDATA[
arXiv:2406.18544v3 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has shown a powerful capability for novel view synthesis due to its detailed expressive ability and highly efficient rendering speed. Unfortunately, creating relightable 3D assets and reconstructing faithful geometry with 3DGS is still problematic, particularly for reflective objects, as its discontinuous representation raises difficulties in constraining geometries. Volumetric signed distance field (SDF) methods provide robust geometry reconstruction, while the expensive ray marching hinders its real-time application and slows the training. Besides, these methods struggle to capture sharp geometric details. To this end, we propose to guide 3DGS and SDF bidirectionally in a complementary manner, including an SDF-aided Gaussian splatting for efficient optimization of the relighting model and a GS-guided SDF enhancement for high-quality geometry reconstruction. At the core of our SDF-aided Gaussian splatting is the mutual supervision of the depth and normal between blended Gaussians and SDF, which avoids the expensive volume rendering of SDF. Thanks to this mutual supervision, the learned blended Gaussians are well-constrained with a minimal time cost. As the Gaussians are rendered in a deferred shading mode, the alpha-blended Gaussians are smooth, while individual Gaussians may still be outliers, yielding floater artifacts. Therefore, we introduce an SDF-aware pruning strategy to remove Gaussian outliers located distant from the surface defined by SDF, avoiding floater issue. This way, our GS framework provides reasonable normal and achieves realistic relighting, while the mesh from depth is still problematic. Therefore, we design a GS-guided SDF refinement, which utilizes the blended normal from Gaussians to finetune SDF. With this enhancement, our method can further provide high-quality meshes for reflective objects at the cost of 17% extra training time.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DD-rPPGNet: De-interfering and Descriptive Feature Learning for Unsupervised rPPG Estimation</title>
<link>https://arxiv.org/abs/2407.21402</link>
<guid>https://arxiv.org/abs/2407.21402</guid>
<content:encoded><![CDATA[
arXiv:2407.21402v3 Announce Type: replace 
Abstract: Remote Photoplethysmography (rPPG) aims to measure physiological signals and Heart Rate (HR) from facial videos. Recent unsupervised rPPG estimation methods have shown promising potential in estimating rPPG signals from facial regions without relying on ground truth rPPG signals. However, these methods seem oblivious to interference existing in rPPG signals and still result in unsatisfactory performance. In this paper, we propose a novel De-interfered and Descriptive rPPG Estimation Network (DD-rPPGNet) to eliminate the interference within rPPG features for learning genuine rPPG signals. First, we investigate the characteristics of local spatial-temporal similarities of interference and design a novel unsupervised model to estimate the interference. Next, we propose an unsupervised de-interfered method to learn genuine rPPG signals with two stages. In the first stage, we estimate the initial rPPG signals by contrastive learning from both the training data and their augmented counterparts. In the second stage, we use the estimated interference features to derive de-interfered rPPG features and encourage the rPPG signals to be distinct from the interference. In addition, we propose an effective descriptive rPPG feature learning by developing a strong 3D Learnable Descriptive Convolution (3DLDC) to capture the subtle chrominance changes for enhancing rPPG estimation. Extensive experiments conducted on five rPPG benchmark datasets demonstrate that the proposed DD-rPPGNet outperforms previous unsupervised rPPG estimation methods and achieves competitive performances with state-of-the-art supervised rPPG methods. The code is available at: https://github.com/Pei-KaiHuang/TIFS2025-DD-rPPGNet
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: From Correlation to Causation: Max-Pooling-Based Multi-Instance Learning Leads to More Robust Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2408.09449</link>
<guid>https://arxiv.org/abs/2408.09449</guid>
<content:encoded><![CDATA[
arXiv:2408.09449v2 Announce Type: replace 
Abstract: Although attention-based multi-instance learning (MIL) algorithms have achieved impressive performance on slide-level whole slide image (WSI) classification tasks, they are prone to mistakenly focusing on irrelevant patterns such as staining conditions and tissue morphology, leading to incorrect patch-level predictions and unreliable interpretability. In this paper, we analyze why attention-based methods tend to rely on spurious correlations in their predictions. Furthermore, we revisit max-pooling-based approaches and examine the reasons behind the underperformance of existing methods. We argue that well-trained max-pooling-based MIL models can make predictions based on causal factors and avoid relying on spurious correlations. Building on these insights, we propose a simple yet effective max-pooling-based MIL method (FocusMIL) that outperforms existing mainstream attention-based methods on two datasets. In this position paper, we advocate renewed attention to max-pooling-based methods to achieve more robust and interpretable predictions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Attention Diffusion Networks with Object Priors for Video Change Detection</title>
<link>https://arxiv.org/abs/2408.10619</link>
<guid>https://arxiv.org/abs/2408.10619</guid>
<content:encoded><![CDATA[
arXiv:2408.10619v2 Announce Type: replace 
Abstract: We present a unified change detection pipeline that combines instance level masking, multi\-scale attention within a denoising diffusion model, and per pixel semantic classification, all refined via SSIM to match human perception. By first isolating only temporally novel objects with Mask R\-CNN, then guiding diffusion updates through hierarchical cross attention to object and global contexts, and finally categorizing each pixel into one of C change types, our method delivers detailed, interpretable multi\-class maps. It outperforms traditional differencing, Siamese CNNs, and GAN\-based detectors by 10\-25 points in F1 and IoU on both synthetic and real world benchmarks, marking a new state of the art in remote sensing change detection.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-Guided Multi-scale Interaction Network for Face Super-Resolution</title>
<link>https://arxiv.org/abs/2409.00591</link>
<guid>https://arxiv.org/abs/2409.00591</guid>
<content:encoded><![CDATA[
arXiv:2409.00591v3 Announce Type: replace 
Abstract: Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions and encoder-decoder phase feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free-DyGS: Camera-Pose-Free Scene Reconstruction for Dynamic Surgical Videos with Gaussian Splatting</title>
<link>https://arxiv.org/abs/2409.01003</link>
<guid>https://arxiv.org/abs/2409.01003</guid>
<content:encoded><![CDATA[
arXiv:2409.01003v3 Announce Type: replace 
Abstract: High-fidelity reconstruction of surgical scene is a fundamentally crucial task to support many applications, such as intra-operative navigation and surgical education. However, most existing methods assume the ideal surgical scenarios - either focus on dynamic reconstruction with deforming tissue yet assuming a given fixed camera pose, or allow endoscope movement yet reconstructing the static scenes. In this paper, we target at a more realistic yet challenging setup - free-pose reconstruction with a moving camera for highly dynamic surgical scenes. Meanwhile, we take the first step to introduce Gaussian Splitting (GS) technique to tackle this challenging setting and propose a novel GS-based framework for fast reconstruction, termed \textit{Free-DyGS}. Concretely, our model embraces a novel scene initialization in which a pre-trained Sparse Gaussian Regressor (SGR) can efficiently parameterize the initial attributes. For each subsequent frame, we propose to jointly optimize the deformation model and 6D camera poses in a frame-by-frame manner, easing training given the limited deformation differences between consecutive frames. A Scene Expansion scheme is followed to expand the GS model for the unseen regions introduced by the moving camera. Moreover, the framework is equipped with a novel Retrospective Deformation Recapitulation (RDR) strategy to preserve the entire-clip deformations throughout the frame-by-frame training scheme. The efficacy of the proposed Free-DyGS is substantiated through extensive experiments on two datasets: StereoMIS and Hamlyn datasets. The experimental outcomes underscore that Free-DyGS surpasses other advanced methods in both rendering accuracy and efficiency. Code will be available.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PatternPaint: Practical Layout Pattern Generation Using Diffusion-Based Inpainting</title>
<link>https://arxiv.org/abs/2409.01348</link>
<guid>https://arxiv.org/abs/2409.01348</guid>
<content:encoded><![CDATA[
arXiv:2409.01348v4 Announce Type: replace 
Abstract: Generating diverse VLSI layout patterns is essential for various downstream tasks in design for manufacturing, as design rules continually evolve during the development of new technology nodes. However, existing training-based methods for layout pattern generation rely on large datasets. In practical scenarios, especially when developing a new technology node, obtaining such extensive layout data is challenging. Consequently, training models with large datasets becomes impractical, limiting the scalability and adaptability of prior approaches. To this end, we propose PatternPaint, a diffusion-based framework capable of generating legal patterns with limited design-rule-compliant training samples. PatternPaint simplifies complex layout pattern generation into a series of inpainting processes with a template-based denoising scheme. Furthermore, we perform few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results show that using a sub-3nm technology node (Intel 18A), our model is the only one that can generate legal patterns in complex 2D metal interconnect design rule settings among all previous works and achieves a high diversity score. Additionally, our few-shot finetuning can boost the legality rate with 1.87X improvement compared to the original pretrained model. As a result, we demonstrate a production-ready approach for layout pattern generation in developing new technology nodes.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation</title>
<link>https://arxiv.org/abs/2409.09497</link>
<guid>https://arxiv.org/abs/2409.09497</guid>
<content:encoded><![CDATA[
arXiv:2409.09497v2 Announce Type: replace 
Abstract: Prototypical part learning is emerging as a promising approach for making semantic segmentation interpretable. The model selects real patches seen during training as prototypes and constructs the dense prediction map based on the similarity between parts of the test image and the prototypes. This improves interpretability since the user can inspect the link between the predicted output and the patterns learned by the model in terms of prototypical information. In this paper, we propose a method for interpretable semantic segmentation that leverages multi-scale image representation for prototypical part learning. First, we introduce a prototype layer that explicitly learns diverse prototypical parts at several scales, leading to multi-scale representations in the prototype activation output. Then, we propose a sparse grouping mechanism that produces multi-scale sparse groups of these scale-specific prototypical parts. This provides a deeper understanding of the interactions between multi-scale object representations while enhancing the interpretability of the segmentation model. The experiments conducted on Pascal VOC, Cityscapes, and ADE20K demonstrate that the proposed method increases model sparsity, improves interpretability over existing prototype-based methods, and narrows the performance gap with the non-interpretable counterpart models. Code is available at github.com/eceo-epfl/ScaleProtoSeg.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Integration of Task-Specific Adapters for Class Incremental Learning</title>
<link>https://arxiv.org/abs/2409.14983</link>
<guid>https://arxiv.org/abs/2409.14983</guid>
<content:encoded><![CDATA[
arXiv:2409.14983v2 Announce Type: replace 
Abstract: Non-exemplar class Incremental Learning (NECIL) enables models to continuously acquire new classes without retraining from scratch and storing old task exemplars, addressing privacy and storage issues. However, the absence of data from earlier tasks exacerbates the challenge of catastrophic forgetting in NECIL. In this paper, we propose a novel framework called Dynamic Integration of task-specific Adapters (DIA), which comprises two key components: Task-Specific Adapter Integration (TSAI) and Patch-Level Model Alignment. TSAI boosts compositionality through a patch-level adapter integration strategy, which provides a more flexible compositional solution while maintaining low computation costs. Patch-Level Model Alignment maintains feature consistency and accurate decision boundaries via two specialized mechanisms: Patch-Level Distillation Loss (PDL) and Patch-Level Feature Reconstruction method (PFR). Specifically, the PDL preserves feature-level consistency between successive models by implementing a distillation loss based on the contributions of patch tokens to new class learning. The PFR facilitates accurate classifier alignment by reconstructing old class features from previous tasks that adapt to new task knowledge. Extensive experiments validate the effectiveness of our DIA, revealing significant improvements on benchmark datasets in the NECIL setting, maintaining an optimal balance between computational complexity and accuracy.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization</title>
<link>https://arxiv.org/abs/2409.17993</link>
<guid>https://arxiv.org/abs/2409.17993</guid>
<content:encoded><![CDATA[
arXiv:2409.17993v5 Announce Type: replace 
Abstract: We propose a novel unsupervised cross-modal homography estimation learning framework, named Split Supervised Homography estimation Network (SSHNet). SSHNet reformulates the unsupervised cross-modal homography estimation into two supervised sub-problems, each addressed by its specialized network: a homography estimation network and a modality transfer network. To realize stable training, we introduce an effective split optimization strategy to train each network separately within its respective sub-problem. We also formulate an extra homography feature space supervision to enhance feature consistency, further boosting the estimation accuracy. Moreover, we employ a simple yet effective distillation training technique to reduce model parameters and improve cross-domain generalization ability while maintaining comparable performance. The training stability of SSHNet enables its cooperation with various homography estimation architectures. Experiments reveal that the SSHNet using IHN as homography estimation network, namely SSHNet-IHN, outperforms previous unsupervised approaches by a significant margin. Even compared to supervised approaches MHN and LocalTrans, SSHNet-IHN achieves 47.4% and 85.8% mean average corner errors (MACEs) reduction on the challenging OPT-SAR dataset.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness</title>
<link>https://arxiv.org/abs/2409.18125</link>
<guid>https://arxiv.org/abs/2409.18125</guid>
<content:encoded><![CDATA[
arXiv:2409.18125v3 Announce Type: replace 
Abstract: Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D scene understanding capabilities has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce a simple yet effective framework called LLaVA-3D. Leveraging the strong 2D visual understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we utilize the 3D position embeddings to enhance the 2D CLIP Patches with 3D spatial context information and construct 3D patches. By integrating the 3D position embeddings into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish a unified architecture for both 2D visual understanding and 3D scene understanding. In contrast to previous 3D LMMs, LLaVA-3D supports decoding accurate 3D spatial perception outputs, e.g., 3D bounding boxes, directly from these 3D patches, without relying on the time-consuming off-the-shelf 3D segmentors. Experimental results show that LLaVA-3D converges 3.5x faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D visual understanding and vision-language conversation capabilities with LLaVA.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Consistency Representation Learning for Lifelong Person Re-Identification</title>
<link>https://arxiv.org/abs/2409.19954</link>
<guid>https://arxiv.org/abs/2409.19954</guid>
<content:encoded><![CDATA[
arXiv:2409.19954v3 Announce Type: replace 
Abstract: Lifelong person re-identification (LReID) exhibits a contradictory relationship between intra-domain discrimination and inter-domain gaps when learning from continuous data. Intra-domain discrimination focuses on individual nuances (i.e., clothing type, accessories, etc.), while inter-domain gaps emphasize domain consistency. Achieving a trade-off between maximizing intra-domain discrimination and minimizing inter-domain gaps is a crucial challenge for improving LReID performance. Most existing methods strive to reduce inter-domain gaps through knowledge distillation to maintain domain consistency. However, they often ignore intra-domain discrimination. To address this challenge, we propose a novel domain consistency representation learning (DCR) model that explores global and attribute-wise representations as a bridge to balance intra-domain discrimination and inter-domain gaps. At the intra-domain level, we explore the complementary relationship between global and attribute-wise representations to improve discrimination among similar identities. Excessive learning intra-domain discrimination can lead to catastrophic forgetting. We further develop an attribute-oriented anti-forgetting (AF) strategy that explores attribute-wise representations to enhance inter-domain consistency, and propose a knowledge consolidation (KC) strategy to facilitate knowledge transfer. Extensive experiments show that our DCR model achieves superior performance compared to state-of-the-art LReID methods. Our code is publicly available at https://github.com/LiuShiBen/DCR.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interpreting Visual Information Processing in Vision-Language Models</title>
<link>https://arxiv.org/abs/2410.07149</link>
<guid>https://arxiv.org/abs/2410.07149</guid>
<content:encoded><![CDATA[
arXiv:2410.07149v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) are powerful tools for processing and understanding text and images. We study the processing of visual tokens in the language model component of LLaVA, a prominent VLM. Our approach focuses on analyzing the localization of object information, the evolution of visual token representations across layers, and the mechanism of integrating visual information for predictions. Through ablation studies, we demonstrated that object identification accuracy drops by over 70\% when object-specific tokens are removed. We observed that visual token representations become increasingly interpretable in the vocabulary space across layers, suggesting an alignment with textual tokens corresponding to image content. Finally, we found that the model extracts object information from these refined representations at the last token position for prediction, mirroring the process in text-only language models for factual association tasks. These findings provide crucial insights into how VLMs process and integrate visual information, bridging the gap between our understanding of language and vision models, and paving the way for more interpretable and controllable multimodal systems.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content</title>
<link>https://arxiv.org/abs/2410.08260</link>
<guid>https://arxiv.org/abs/2410.08260</guid>
<content:encoded><![CDATA[
arXiv:2410.08260v2 Announce Type: replace 
Abstract: With the continuous progress of visual generation technologies, the scale of video datasets has grown exponentially. The quality of these datasets plays a pivotal role in the performance of video generation models. We assert that temporal splitting, detailed captions, and video quality filtering are three crucial determinants of dataset quality. However, existing datasets exhibit various limitations in these areas. To address these challenges, we introduce Koala-36M, a large-scale, high-quality video dataset featuring accurate temporal splitting, detailed captions, and superior video quality. The essence of our approach lies in improving the consistency between fine-grained conditions and video content. Specifically, we employ a linear classifier on probability distributions to enhance the accuracy of transition detection, ensuring better temporal consistency. We then provide structured captions for the splitted videos, with an average length of 200 words, to improve text-video alignment. Additionally, we develop a Video Training Suitability Score (VTSS) that integrates multiple sub-metrics, allowing us to filter high-quality videos from the original corpus. Finally, we incorporate several metrics into the training process of the generation model, further refining the fine-grained conditions. Our experiments demonstrate the effectiveness of our data processing pipeline and the quality of the proposed Koala-36M dataset. Our dataset and code have been released at https://koala36m.github.io/.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Compositionality in Text-to-Image Generative Models</title>
<link>https://arxiv.org/abs/2410.16719</link>
<guid>https://arxiv.org/abs/2410.16719</guid>
<content:encoded><![CDATA[
arXiv:2410.16719v2 Announce Type: replace 
Abstract: Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing solutions have tackled these challenges by optimizing the cross-attention mechanism or learning from the caption pairs with minimal semantic changes. However, can we generate high-quality complex contrastive images that diffusion models can directly discriminate based on visual representations? In this work, we leverage large-language models (LLMs) to compose realistic, complex scenarios and harness Visual-Question Answering (VQA) systems alongside diffusion models to automatically curate a contrastive dataset, ConPair, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectively from these error cases, i.e., hard negative images, we propose EvoGen, a new multi-stage curriculum for contrastive learning of diffusion models. Through extensive experiments across a wide range of compositional scenarios, we showcase the effectiveness of our proposed framework on compositional T2I benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DivShift: Exploring Domain-Specific Distribution Shift in Large-Scale, Volunteer-Collected Biodiversity Datasets</title>
<link>https://arxiv.org/abs/2410.19816</link>
<guid>https://arxiv.org/abs/2410.19816</guid>
<content:encoded><![CDATA[
arXiv:2410.19816v4 Announce Type: replace 
Abstract: Large-scale, volunteer-collected datasets of community-identified natural world imagery like iNaturalist have enabled marked performance gains for fine-grained visual classification of species using machine learning methods. However, such data -- sometimes referred to as citizen science data -- are opportunistic and lack a structured sampling strategy. This volunteer-collected biodiversity data contains geographic, temporal, taxonomic, observers, and sociopolitical biases that can have significant effects on biodiversity model performance, but whose impacts are unclear for fine-grained species recognition performance. Here we introduce Diversity Shift (DivShift), a framework for quantifying the effects of domain-specific distribution shifts on machine learning model performance. To diagnose the performance effects of biases specific to volunteer-collected biodiversity data, we also introduce DivShift - North American West Coast (DivShift-NAWC), a curated dataset of almost 7.5 million iNaturalist images across the western coast of North America partitioned across five types of expert-verified bias. We compare species recognition performance across these bias partitions using a diverse variety of species- and ecosystem-focused accuracy metrics. We observe that these biases confound model performance less than expected from the underlying label distribution shift, and that more data leads to better model performance but the magnitude of these improvements are bias-specific. These findings imply that while the structure within natural world images provides generalization improvements for biodiversity monitoring tasks, the biases present in volunteer-collected biodiversity data can also affect model performance; thus these models should be used with caution in downstream biodiversity monitoring tasks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free-Mask: A Novel Paradigm of Integration Between the Segmentation Diffusion Model and Image Editing</title>
<link>https://arxiv.org/abs/2411.01819</link>
<guid>https://arxiv.org/abs/2411.01819</guid>
<content:encoded><![CDATA[
arXiv:2411.01819v3 Announce Type: replace 
Abstract: Current semantic segmentation models typically require a substantial amount of manually annotated data, a process that is both time-consuming and resource-intensive. Alternatively, leveraging advanced text-to-image models such as Midjourney and Stable Diffusion has emerged as an efficient strategy, enabling the automatic generation of synthetic data in place of manual annotations. However, previous methods have been limited to generating single-instance images, as the generation of multiple instances with Stable Diffusion has proven unstable. To address this limitation and expand the scope and diversity of synthetic datasets, we propose a framework \textbf{Free-Mask} that combines a Diffusion Model for segmentation with advanced image editing capabilities, allowing for the integration of multiple objects into images via text-to-image models. Our method facilitates the creation of highly realistic datasets that closely emulate open-world environments while generating accurate segmentation masks. It reduces the labor associated with manual annotation and also ensures precise mask generation. Experimental results demonstrate that synthetic data generated by \textbf{Free-Mask} enables segmentation models to outperform those trained on real data, especially in zero-shot settings. Notably, \textbf{Free-Mask} achieves new state-of-the-art results on previously unseen classes in the VOC 2012 benchmark.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision</title>
<link>https://arxiv.org/abs/2411.07199</link>
<guid>https://arxiv.org/abs/2411.07199</guid>
<content:encoded><![CDATA[
arXiv:2411.07199v2 Announce Type: replace 
Abstract: Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training</title>
<link>https://arxiv.org/abs/2411.11927</link>
<guid>https://arxiv.org/abs/2411.11927</guid>
<content:encoded><![CDATA[
arXiv:2411.11927v3 Announce Type: replace 
Abstract: Language-image pre-training faces significant challenges due to limited data in specific formats and the constrained capacities of text encoders. While prevailing methods attempt to address these issues through data augmentation and architecture modifications, they continue to struggle with processing long-form text inputs, and the inherent limitations of traditional CLIP text encoders lead to suboptimal downstream generalization. In this paper, we propose FLAME (Frozen Large lAnguage Models Enable data-efficient language-image pre-training) that leverages frozen large language models as text encoders, naturally processing long text inputs and demonstrating impressive multilingual generalization. FLAME comprises two key components: 1) a multifaceted prompt distillation technique for extracting diverse semantic representations from long captions, which better aligns with the multifaceted nature of images, and 2) a facet-decoupled attention mechanism, complemented by an offline embedding strategy, to ensure efficient computation. Extensive empirical evaluations demonstrate FLAME's superior performance. When trained on CC3M, FLAME surpasses the previous state-of-the-art by 4.9% in ImageNet top-1 accuracy. On YFCC15M, FLAME surpasses the WIT-400M-trained CLIP by 44.4\% in average image-to-text recall@1 across 36 languages, and by 34.6% in text-to-image recall@1 for long-context retrieval on Urban-1k. Code is available at https://github.com/MIV-XJTU/FLAME.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input</title>
<link>https://arxiv.org/abs/2411.11934</link>
<guid>https://arxiv.org/abs/2411.11934</guid>
<content:encoded><![CDATA[
arXiv:2411.11934v2 Announce Type: replace 
Abstract: Stereo video synthesis from a monocular input is a demanding task in the fields of spatial computing and virtual reality. The main challenges of this task lie on the insufficiency of high-quality paired stereo videos for training and the difficulty of maintaining the spatio-temporal consistency between frames. Existing methods primarily address these issues by directly applying novel view synthesis (NVS) techniques to video, while facing limitations such as the inability to effectively represent dynamic scenes and the requirement for large amounts of training data. In this paper, we introduce a novel self-supervised stereo video synthesis paradigm via a video diffusion model, termed SpatialDreamer, which meets the challenges head-on. Firstly, to address the stereo video data insufficiency, we propose a Depth based Video Generation module DVG, which employs a forward-backward rendering mechanism to generate paired videos with geometric and temporal priors. Leveraging data generated by DVG, we propose RefinerNet along with a self-supervised synthetic framework designed to facilitate efficient and dedicated training. More importantly, we devise a consistency control module, which consists of a metric of stereo deviation strength and a Temporal Interaction Learning module TIL for geometric and temporal consistency ensurance respectively. We evaluated the proposed method against various benchmark methods, with the results showcasing its superior performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lifting Motion to the 3D World via 2D Diffusion</title>
<link>https://arxiv.org/abs/2411.18808</link>
<guid>https://arxiv.org/abs/2411.18808</guid>
<content:encoded><![CDATA[
arXiv:2411.18808v2 Announce Type: replace 
Abstract: Estimating 3D motion from 2D observations is a long-standing research challenge. Prior work typically requires training on datasets containing ground truth 3D motions, limiting their applicability to activities well-represented in existing motion capture data. This dependency particularly hinders generalization to out-of-distribution scenarios or subjects where collecting 3D ground truth is challenging, such as complex athletic movements or animal motion. We introduce MVLift, a novel approach to predict global 3D motion -- including both joint rotations and root trajectories in the world coordinate system -- using only 2D pose sequences for training. Our multi-stage framework leverages 2D motion diffusion models to progressively generate consistent 2D pose sequences across multiple views, a key step in recovering accurate global 3D motion. MVLift generalizes across various domains, including human poses, human-object interactions, and animal poses. Despite not requiring 3D supervision, it outperforms prior work on five datasets, including those methods that require 3D supervision.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception of Visual Content: Differences Between Humans and Foundation Models</title>
<link>https://arxiv.org/abs/2411.18968</link>
<guid>https://arxiv.org/abs/2411.18968</guid>
<content:encoded><![CDATA[
arXiv:2411.18968v3 Announce Type: replace 
Abstract: Human-annotated content is often used to train machine learning (ML) models. However, recently, language and multi-modal foundational models have been used to replace and scale-up human annotator's efforts. This study explores the similarity between human-generated and ML-generated annotations of images across diverse socio-economic contexts (RQ1) and their impact on ML model performance and bias (RQ2). We aim to understand differences in perception and identify potential biases in content interpretation. Our dataset comprises images of people from various geographical regions and income levels, covering various daily activities and home environments. ML captions and human labels show highest similarity at a low-level, i.e., types of words that appear and sentence structures, but all annotations are consistent in how they perceive images across regions. ML Captions resulted in best overall region classification performance, while ML Objects and ML Captions performed best overall for income regression. ML annotations worked best for action categories, while human input was more effective for non-action categories. These findings highlight the notion that both human and machine annotations are important, and that human-generated annotations are yet to be replaceable.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden in the Noise: Two-Stage Robust Watermarking for Images</title>
<link>https://arxiv.org/abs/2412.04653</link>
<guid>https://arxiv.org/abs/2412.04653</guid>
<content:encoded><![CDATA[
arXiv:2412.04653v5 Announce Type: replace 
Abstract: As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.
  In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wonderland: Navigating 3D Scenes from a Single Image</title>
<link>https://arxiv.org/abs/2412.12091</link>
<guid>https://arxiv.org/abs/2412.12091</guid>
<content:encoded><![CDATA[
arXiv:2412.12091v2 Announce Type: replace 
Abstract: How can one efficiently generate high-quality, wide-scope 3D scenes from arbitrary single images? Existing methods suffer several drawbacks, such as requiring multi-view data, time-consuming per-scene optimization, distorted geometry in occluded areas, and low visual quality in backgrounds. Our novel 3D scene reconstruction pipeline overcomes these limitations to tackle the aforesaid challenge. Specifically, we introduce a large-scale reconstruction model that leverages latents from a video diffusion model to predict 3D Gaussian Splattings of scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that encode multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive learning strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets affirm that our model significantly outperforms existing single-view 3D scene generation methods, especially with out-of-domain images. Thus, we demonstrate for the first time that a 3D reconstruction model can effectively be built upon the latent space of a diffusion model in order to realize efficient 3D scene generation.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineVQ: Fine-Grained User Generated Content Video Quality Assessment</title>
<link>https://arxiv.org/abs/2412.19238</link>
<guid>https://arxiv.org/abs/2412.19238</guid>
<content:encoded><![CDATA[
arXiv:2412.19238v2 Announce Type: replace 
Abstract: The rapid growth of user-generated content (UGC) videos has produced an urgent need for effective video quality assessment (VQA) algorithms to monitor video quality and guide optimization and recommendation procedures. However, current VQA models generally only give an overall rating for a UGC video, which lacks fine-grained labels for serving video processing and recommendation applications. To address the challenges and promote the development of UGC videos, we establish the first large-scale Fine-grained Video quality assessment Database, termed FineVD, which comprises 6104 UGC videos with fine-grained quality scores and descriptions across multiple dimensions. Based on this database, we propose a Fine-grained Video Quality assessment (FineVQ) model to learn the fine-grained quality of UGC videos, with the capabilities of quality rating, quality scoring, and quality attribution. Extensive experimental results demonstrate that our proposed FineVQ can produce fine-grained video-quality results and achieve state-of-the-art performance on FineVD and other commonly used UGC-VQA datasets.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimLTD: Simple Supervised and Semi-Supervised Long-Tailed Object Detection</title>
<link>https://arxiv.org/abs/2412.20047</link>
<guid>https://arxiv.org/abs/2412.20047</guid>
<content:encoded><![CDATA[
arXiv:2412.20047v2 Announce Type: replace 
Abstract: While modern visual recognition systems have made significant advancements, many continue to struggle with the open problem of learning from few exemplars. This paper focuses on the task of object detection in the setting where object classes follow a natural long-tailed distribution. Existing methods for long-tailed detection resort to external ImageNet labels to augment the low-shot training instances. However, such dependency on a large labeled database has limited utility in practical scenarios. We propose a versatile and scalable approach to leverage optional unlabeled images, which are easy to collect without the burden of human annotations. Our SimLTD framework is straightforward and intuitive, and consists of three simple steps: (1) pre-training on abundant head classes; (2) transfer learning on scarce tail classes; and (3) fine-tuning on a sampled set of both head and tail classes. Our approach can be viewed as an improved head-to-tail model transfer paradigm without the added complexities of meta-learning or knowledge distillation, as was required in past research. By harnessing supplementary unlabeled images, without extra image labels, SimLTD establishes new record results on the challenging LVIS v1 benchmark across both supervised and semi-supervised settings.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction</title>
<link>https://arxiv.org/abs/2501.01695</link>
<guid>https://arxiv.org/abs/2501.01695</guid>
<content:encoded><![CDATA[
arXiv:2501.01695v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) leverages densely distributed Gaussian primitives for high-quality scene representation and reconstruction. While existing 3DGS methods perform well in scenes with minor view variation, large view changes from cross-view data pose optimization challenges for these methods. To address these issues, we propose a novel cross-view Gaussian Splatting method for large-scale scene reconstruction based on multi-branch construction and fusion. Our method independently reconstructs models from different sets of views as multiple independent branches to establish the baselines of Gaussian distribution, providing reliable priors for cross-view reconstruction during initialization and densification. Specifically, a gradient-aware regularization strategy is introduced to mitigate smoothing issues caused by significant view disparities. Additionally, a unique Gaussian supplementation strategy is utilized to incorporate complementary information of multi-branch into the cross-view model. Extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in novel view synthesis compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Tomato Split Anomaly Detection using Hyperspectral Imaging and Variational Autoencoders</title>
<link>https://arxiv.org/abs/2501.02921</link>
<guid>https://arxiv.org/abs/2501.02921</guid>
<content:encoded><![CDATA[
arXiv:2501.02921v2 Announce Type: replace 
Abstract: Tomato anomalies/damages pose a significant challenge in greenhouse farming. While this method of cultivation benefits from efficient resource utilization, anomalies can significantly degrade the quality of farm produce. A common anomaly associated with tomatoes is splitting, characterized by the development of cracks on the tomato skin, which degrades its quality. Detecting this type of anomaly is challenging due to dynamic variations in appearance and sizes, compounded by dataset scarcity. We address this problem in an unsupervised manner by utilizing a tailored variational autoencoder (VAE) with hyperspectral input. Preliminary analysis of the dataset enabled us to select the optimal range of wavelengths for detecting this anomaly. Our findings indicate that the 530nm - 550nm range is suitable for identifying tomato dry splits. The proposed VAE model achieved a 97% detection accuracy for tomato split anomalies in the test data. The analysis on reconstruction loss allow us to not only detect the anomalies but also to some degree estimate the anomalous regions.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training</title>
<link>https://arxiv.org/abs/2501.15579</link>
<guid>https://arxiv.org/abs/2501.15579</guid>
<content:encoded><![CDATA[
arXiv:2501.15579v2 Announce Type: replace 
Abstract: The clinical adoption of artificial intelligence (AI) in medical imaging requires models that are both diagnostically accurate and interpretable to clinicians. While current multimodal biomedical foundation models prioritize performance, their black-box nature hinders explaining the decision-making process in clinically meaningful concepts. Here, we present ConceptCLIP, the first explainable biomedical foundation model that achieves state-of-the-art diagnostic accuracy while delivering human-interpretable explanations across diverse imaging modalities. We curate MedConcept-23M, the largest pre-training dataset comprising 23 million image-text-concept triplets across diverse medical modalities, where clinical concepts are derived from the Unified Medical Language System. Leveraging this dataset, we develop ConceptCLIP through a novel dual-alignment approach that simultaneously learns global image-text representations and fine-grained region-concept associations for precise and interpretable medical image analysis. We curate the most extensive evaluation benchmark for multimodal biomedical foundation models, covering 52 clinical tasks spanning 10 imaging modalities. Extensive experiments demonstrate that ConceptCLIP outperforms existing state-of-the-art multimodal biomedical foundation models. Importantly, ConceptCLIP demonstrates superior diagnostic performance while providing human-understandable explanations validated by clinical experts. As the first precise and interpretable biomedical foundation model, ConceptCLIP represents a critical milestone toward the widespread clinical adoption of AI, thereby advancing trustworthy AI in medicine.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity</title>
<link>https://arxiv.org/abs/2502.01776</link>
<guid>https://arxiv.org/abs/2502.01776</guid>
<content:encoded><![CDATA[
arXiv:2502.01776v2 Announce Type: replace 
Abstract: Diffusion Transformers (DiTs) dominate video generation but their high computational cost severely limits real-world applicability, usually requiring tens of minutes to generate a few seconds of video even on high-performance GPUs. This inefficiency primarily arises from the quadratic computational complexity of 3D Full Attention with respect to the context length. In this paper, we propose a training-free framework termed Sparse VideoGen (SVG) that leverages the inherent sparsity in 3D Full Attention to boost inference efficiency. We reveal that the attention heads can be dynamically classified into two groups depending on distinct sparse patterns: (1) Spatial Head, where only spatially-related tokens within each frame dominate the attention output, and (2) Temporal Head, where only temporally-related tokens across different frames dominate. Based on this insight, SVG proposes an online profiling strategy to capture the dynamic sparse patterns and predicts the type of attention head. Combined with a novel hardware-efficient tensor layout transformation and customized kernel implementations, SVG achieves up to 2.28x and 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively, while preserving generation quality. Our code is open-sourced and is available at https://github.com/svg-project/Sparse-VideoGen
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerteNet -- A Multi-Context Hybrid CNN Transformer for Accurate Vertebral Landmark Localization in Lateral Spine DXA Images</title>
<link>https://arxiv.org/abs/2502.02097</link>
<guid>https://arxiv.org/abs/2502.02097</guid>
<content:encoded><![CDATA[
arXiv:2502.02097v2 Announce Type: replace 
Abstract: Lateral Spine Image (LSI) analysis is important for medical diagnosis, treatment planning, and detailed spinal health assessments. Although modalities like Computed Tomography and Digital X-ray Imaging are commonly used, Dual Energy X-ray Absorptiometry (DXA) is often preferred due to lower radiation exposure, seamless capture, and cost-effectiveness. Accurate Vertebral Landmark Localization (VLL) on LSIs is important to detect spinal conditions like kyphosis and lordosis, as well as assessing Abdominal Aortic Calcification (AAC) using Inter-Vertebral Guides (IVGs). Nonetheless, few automated VLL methodologies have concentrated on DXA LSIs. We present VerteNet, a hybrid CNN-Transformer model featuring a novel dual-resolution attention mechanism in self and cross-attention domains, referred to as Dual Resolution Self-Attention (DRSA) and Dual Resolution Cross-Attention (DRCA). These mechanisms capture the diverse frequencies in DXA images by operating at two different feature map resolutions. Additionally, we design a Multi-Context Feature Fusion Block (MCFB) that efficiently integrates the features using DRSA and DRCA. We train VerteNet on 620 DXA LSIs from various machines and achieve superior results compared to existing methods. We also design an algorithm that utilizes VerteNet's predictions in estimating the Region of Interest (ROI) to detect potential abdominal aorta cropping, where inadequate soft tissue hinders calcification assessment. Additionally, we present a small proof-of-concept study to show that IVGs generated from VLL information can improve inter-reader correlation in AAC scoring, addressing two key areas of disagreement in expert AAC-24 scoring: IVG placement and quality control for full abdominal aorta assessment. The code for this work can be found at https://github.com/zaidilyas89/VerteNet.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain Tumor Identification using Improved YOLOv8</title>
<link>https://arxiv.org/abs/2502.03746</link>
<guid>https://arxiv.org/abs/2502.03746</guid>
<content:encoded><![CDATA[
arXiv:2502.03746v2 Announce Type: replace 
Abstract: Identifying the extent of brain tumors is a significant challenge in brain cancer treatment. The main difficulty is in the approximate detection of tumor size. Magnetic resonance imaging (MRI) has become a critical diagnostic tool. However, manually detecting the boundaries of brain tumors from MRI scans is a labor-intensive task that requires extensive expertise. Deep learning and computer-aided detection techniques have led to notable advances in machine learning for this purpose. In this paper, we propose a modified You Only Look Once (YOLOv8) model to accurately detect the tumors within the MRI images. The proposed model replaced the Non-Maximum Suppression (NMS) algorithm with a Real-Time Detection Transformer (RT- DETR) in the detection head. NMS filters out redundant or overlapping bounding boxes in the detected tumors, but they are hand-designed and pre-set. RT-DETR removes hand-designed components. The second improvement was made by replacing the normal convolution block with ghost convolution. Ghost Convolution reduces computational and memory costs while maintaining high accuracy and enabling faster inference, making it ideal for resource-constrained environments and real-time applications. The third improvement was made by introducing a vision transformer block in the backbone of YOLOv8 to extract context-aware features. We used a publicly available dataset of brain tumors in the proposed model. The proposed model performed better than the original YOLOv8 model and also performed better than other object detectors (Faster R- CNN, Mask R-CNN, YOLO, YOLOv3, YOLOv4, YOLOv5, SSD, RetinaNet, EfficientDet, and DETR). The proposed model achieved 0.91 mAP (mean Average Precision)@0.5.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoRoPE: What Makes for Good Video Rotary Position Embedding?</title>
<link>https://arxiv.org/abs/2502.05173</link>
<guid>https://arxiv.org/abs/2502.05173</guid>
<content:encoded><![CDATA[
arXiv:2502.05173v2 Announce Type: replace 
Abstract: While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce \textbf{VideoRoPE}, with a \textit{3D structure} designed to preserve spatio-temporal relationships. VideoRoPE features \textit{low-frequency temporal allocation} to mitigate periodic oscillations, a \textit{diagonal layout} to maintain spatial symmetry, and \textit{adjustable temporal spacing} to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at \href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI Synthesis</title>
<link>https://arxiv.org/abs/2502.08025</link>
<guid>https://arxiv.org/abs/2502.08025</guid>
<content:encoded><![CDATA[
arXiv:2502.08025v3 Announce Type: replace 
Abstract: While functional magnetic resonance imaging (fMRI) offers valuable insights into brain activity, it is limited by high operational costs and significant infrastructural demands. In contrast, electroencephalography (EEG) provides millisecond-level precision in capturing electrical activity but lacks the spatial fidelity necessary for precise neural localization. To bridge these gaps, we propose E2fNet, a simple yet effective deep learning model for synthesizing fMRI images from low-cost EEG data. E2fNet is an encoder-decoder network specifically designed to capture and translate meaningful multi-scale features from EEG across electrode channels into accurate fMRI representations. Extensive evaluations across three public datasets demonstrate that E2fNet consistently outperforms existing CNN- and transformer-based methods, achieving state-of-the-art results in terms of the structural similarity index measure (SSIM). These results demonstrate that E2fNet is a promising, cost-effective solution for enhancing neuroimaging capabilities. The code is available at https://github.com/kgr20/E2fNet.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering</title>
<link>https://arxiv.org/abs/2502.09573</link>
<guid>https://arxiv.org/abs/2502.09573</guid>
<content:encoded><![CDATA[
arXiv:2502.09573v3 Announce Type: replace 
Abstract: In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPT's performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPT's performance without additional finetuning, offering an effective and scalable solution for improving video classification.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences</title>
<link>https://arxiv.org/abs/2502.10377</link>
<guid>https://arxiv.org/abs/2502.10377</guid>
<content:encoded><![CDATA[
arXiv:2502.10377v2 Announce Type: replace 
Abstract: We introduce ReStyle3D, a novel framework for scene-level appearance transfer from a single style image to a real-world scene represented by multiple views. The method combines explicit semantic correspondences with multi-view consistency to achieve precise and coherent stylization. Unlike conventional stylization methods that apply a reference style globally, ReStyle3D uses open-vocabulary segmentation to establish dense, instance-level correspondences between the style and real-world images. This ensures that each object is stylized with semantically matched textures. It first transfers the style to a single view using a training-free semantic-attention mechanism in a diffusion model. It then lifts the stylization to additional views via a learned warp-and-refine network guided by monocular depth and pixel-wise correspondences. Experiments show that ReStyle3D consistently outperforms prior methods in structure preservation, perceptual style similarity, and multi-view coherence. User studies further validate its ability to produce photo-realistic, semantically faithful results. Our code, pretrained models, and dataset will be publicly released, to support new applications in interior design, virtual staging, and 3D-consistent stylization.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Gaussian Inpainting with Depth-Guided Cross-View Consistency</title>
<link>https://arxiv.org/abs/2502.11801</link>
<guid>https://arxiv.org/abs/2502.11801</guid>
<content:encoded><![CDATA[
arXiv:2502.11801v2 Announce Type: replace 
Abstract: When performing 3D inpainting using novel-view rendering methods like Neural Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture and geometry consistency across camera views has been a challenge. In this paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided by the rendered depth information from each training view, our 3DGIC exploits background pixels visible across different views for updating the inpainting mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive experiments on benchmark datasets, we confirm that our 3DGIC outperforms current state-of-the-art 3D inpainting methods quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels</title>
<link>https://arxiv.org/abs/2502.20087</link>
<guid>https://arxiv.org/abs/2502.20087</guid>
<content:encoded><![CDATA[
arXiv:2502.20087v3 Announce Type: replace 
Abstract: Top-down attention plays a crucial role in the human vision system, wherein the brain initially obtains a rough overview of a scene to discover salient cues (i.e., overview first), followed by a more careful finer-grained examination (i.e., look closely next). However, modern ConvNets remain confined to a pyramid structure that successively downsamples the feature map for receptive field expansion, neglecting this crucial biomimetic principle. We present OverLoCK, the first pure ConvNet backbone architecture that explicitly incorporates a top-down attention mechanism. Unlike pyramid backbone networks, our design features a branched architecture with three synergistic sub-networks: 1) a Base-Net that encodes low/mid-level features; 2) a lightweight Overview-Net that generates dynamic top-down attention through coarse global context modeling (i.e., overview first); and 3) a robust Focus-Net that performs finer-grained perception guided by top-down attention (i.e., look closely next). To fully unleash the power of top-down attention, we further propose a novel context-mixing dynamic convolution (ContMix) that effectively models long-range dependencies while preserving inherent local inductive biases even when the input resolution increases, addressing critical limitations in existing convolutions. Our OverLoCK exhibits a notable performance improvement over existing methods. For instance, OverLoCK-T achieves a Top-1 accuracy of 84.2%, significantly surpassing ConvNeXt-B while using only around one-third of the FLOPs/parameters. On object detection, our OverLoCK-S clearly surpasses MogaNet-B by 1% in AP^b. On semantic segmentation, our OverLoCK-T remarkably improves UniRepLKNet-T by 1.7% in mIoU. Code is publicly available at https://github.com/LMMMEng/OverLoCK.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiMo: High-Speed Objects Motion Compensation in Point Clouds</title>
<link>https://arxiv.org/abs/2503.00803</link>
<guid>https://arxiv.org/abs/2503.00803</guid>
<content:encoded><![CDATA[
arXiv:2503.00803v2 Announce Type: replace 
Abstract: LiDAR point cloud is essential for autonomous vehicles, but motion distortions from dynamic objects degrade the data quality. While previous work has considered distortions caused by ego motion, distortions caused by other moving objects remain largely overlooked, leading to errors in object shape and position. This distortion is particularly pronounced in high-speed environments such as highways and in multi-LiDAR configurations, a common setup for heavy vehicles. To address this challenge, we introduce HiMo, a pipeline that repurposes scene flow estimation for non-ego motion compensation, correcting the representation of dynamic objects in point clouds. During the development of HiMo, we observed that existing self-supervised scene flow estimators often produce degenerate or inconsistent estimates under high-speed distortion. We further propose SeFlow++, a real-time scene flow estimator that achieves state-of-the-art performance on both scene flow and motion compensation. Since well-established motion distortion metrics are absent in the literature, we introduce two evaluation metrics: compensation accuracy at a point level and shape similarity of objects. We validate HiMo through extensive experiments on Argoverse 2, ZOD, and a newly collected real-world dataset featuring highway driving and multi-LiDAR-equipped heavy vehicles. Our findings show that HiMo improves the geometric consistency and visual fidelity of dynamic objects in LiDAR point clouds, benefiting downstream tasks such as semantic segmentation and 3D detection. See https://kin-zhang.github.io/HiMo for more details.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRI super-resolution reconstruction using efficient diffusion probabilistic model with residual shifting</title>
<link>https://arxiv.org/abs/2503.01576</link>
<guid>https://arxiv.org/abs/2503.01576</guid>
<content:encoded><![CDATA[
arXiv:2503.01576v2 Announce Type: replace 
Abstract: Objective:This study introduces a residual error-shifting mechanism that drastically reduces sampling steps while preserving critical anatomical details, thus accelerating MRI reconstruction. Approach:We propose a novel diffusion-based SR framework called Res-SRDiff, which integrates residual error shifting into the forward diffusion process. This enables efficient HR image reconstruction by aligning the degraded HR and LR distributions.We evaluated Res-SRDiff on ultra-high-field brain T1 MP2RAGE maps and T2-weighted prostate images, comparing it with Bicubic, Pix2pix, CycleGAN, and a conventional denoising diffusion probabilistic model with vision transformer backbone (TM-DDPM), using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), gradient magnitude similarity deviation (GMSD), and learned perceptual image patch similarity (LPIPS). Main results: Res-SRDiff significantly outperformed all comparative methods in terms of PSNR, SSIM, and GMSD across both datasets, with statistically significant improvements (p-values<<0.05). The model achieved high-fidelity image restoration with only four sampling steps, drastically reducing computational time to under one second per slice, which is substantially faster than conventional TM-DDPM with around 20 seconds per slice. Qualitative analyses further demonstrated that Res-SRDiff effectively preserved fine anatomical details and lesion morphology in both brain and pelvic MRI images. Significance: Our findings show that Res-SRDiff is an efficient and accurate MRI SR method, markedly improving computational efficiency and image quality. Integrating residual error shifting into the diffusion process allows for rapid and robust HR image reconstruction, enhancing clinical MRI workflows and advancing medical imaging research. The source at:https://github.com/mosaf/Res-SRDiff
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation</title>
<link>https://arxiv.org/abs/2503.02247</link>
<guid>https://arxiv.org/abs/2503.02247</guid>
<content:encoded><![CDATA[
arXiv:2503.02247v4 Announce Type: replace 
Abstract: Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical Flow Estimation</title>
<link>https://arxiv.org/abs/2503.10195</link>
<guid>https://arxiv.org/abs/2503.10195</guid>
<content:encoded><![CDATA[
arXiv:2503.10195v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) have emerged as a promising tool for event-based optical flow estimation tasks due to their ability to leverage spatio-temporal information and low-power capabilities. However, the performance of SNN models is often constrained, limiting their application in real-world scenarios. In this work, we address this gap by proposing a novel neural network architecture, ST-FlowNet, specifically tailored for optical flow estimation from event-based data. The ST-FlowNet architecture integrates ConvGRU modules to facilitate cross-modal feature augmentation and temporal alignment of the predicted optical flow, improving the network's ability to capture complex motion dynamics. Additionally, to overcome the challenges associated with training SNNs, we introduce a novel approach to derive SNN models from pre-trained artificial neural networks (ANNs) through ANN-to-SNN conversion or our proposed BISNN method. Notably, the BISNN method alleviates the complexities involved in biological parameter selection, further enhancing the robustness of SNNs in optical flow estimation tasks. Extensive evaluations on three benchmark event-based datasets demonstrate that the SNN-based ST-FlowNet model outperforms state-of-the-art methods, delivering superior performance in accurate optical flow estimation across a diverse range of dynamic visual scenes. Furthermore, the inherent energy efficiency of SNN models is highlighted, establishing a compelling advantage for their practical deployment. Overall, our work presents a novel framework for optical flow estimation using SNNs and event-based data, contributing to the advancement of neuromorphic vision applications.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis</title>
<link>https://arxiv.org/abs/2503.12150</link>
<guid>https://arxiv.org/abs/2503.12150</guid>
<content:encoded><![CDATA[
arXiv:2503.12150v3 Announce Type: replace 
Abstract: This paper proposes a general solution to enable point cloud recognition models to handle distribution shifts at test time. Unlike prior methods, which rely heavily on training data (often inaccessible during online inference) and are limited to recognizing a fixed set of point cloud classes predefined during training, we explore a more practical and challenging scenario: adapting the model solely based on online test data to recognize both previously seen classes and novel, unseen classes at test time. To this end, we develop \textbf{Point-Cache}, a hierarchical cache model that captures essential clues of online test samples, particularly focusing on the global structure of point clouds and their local-part details. Point-Cache, which serves as a rich 3D knowledge base, is dynamically managed to prioritize the inclusion of high-quality samples. Designed as a plug-and-play module, our method can be flexibly integrated into large multimodal 3D models to support open-vocabulary point cloud recognition. Notably, our solution operates with efficiency comparable to zero-shot inference, as it is entirely training-free. Point-Cache demonstrates substantial gains across 8 challenging benchmarks and 4 representative large 3D models, highlighting its effectiveness. Code is available at https://github.com/auniquesun/Point-Cache.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View Prediction</title>
<link>https://arxiv.org/abs/2503.12929</link>
<guid>https://arxiv.org/abs/2503.12929</guid>
<content:encoded><![CDATA[
arXiv:2503.12929v3 Announce Type: replace 
Abstract: Novel view synthesis (NVS) is a cornerstone for image-to-3d creation. However, existing works still struggle to maintain consistency between the generated views and the input views, especially when there is a significant camera pose difference, leading to poor-quality 3D geometries and textures. We attribute this issue to their treatment of all target views with equal priority according to our empirical observation that the target views closer to the input views exhibit higher fidelity. With this inspiration, we propose AR-1-to-3, a novel next-view prediction paradigm based on diffusion models that first generates views close to the input views, which are then utilized as contextual information to progressively synthesize farther views. To encode the generated view subsequences as local and global conditions for the next-view prediction, we accordingly develop a stacked local feature encoding strategy (Stacked-LE) and an LSTM-based global feature encoding strategy (LSTM-GE). Extensive experiments demonstrate that our method significantly improves the consistency between the generated views and the input views, producing high-fidelity 3D assets.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>8-Calves Image dataset</title>
<link>https://arxiv.org/abs/2503.13777</link>
<guid>https://arxiv.org/abs/2503.13777</guid>
<content:encoded><![CDATA[
arXiv:2503.13777v2 Announce Type: replace 
Abstract: We introduce the 8-Calves dataset, a benchmark for evaluating object detection and identity preservation in occlusion-rich, temporally consistent environments. Comprising a 1-hour video (67,760 frames) of eight Holstein Friesian calves with unique coat patterns and 900 static frames, the dataset emphasizes real-world challenges like prolonged occlusions, motion blur, and pose variation. By fine-tuning 28 object detectors (YOLO variants, transformers) and evaluating 23 pretrained backbones (ResNet, ConvNextV2, ViTs), we expose critical architectural trade-offs: smaller models (e.g., ConvNextV2 Nano, 15.6M parameters) excel in efficiency and retrieval accuracy, while pure vision transformers lag in occlusion-heavy settings. The dataset's structured design-fixed camera views, natural motion, and verified identities-provides a reproducible testbed for object detection challenges (mAP50:95: 56.5-66.4%), bridging synthetic simplicity and domain-specific complexity. The dataset and benchmark code are all publicly available at https://huggingface.co/datasets/tonyFang04/8-calves. Limitations include partial labeling and detector bias, addressed in later sections.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization</title>
<link>https://arxiv.org/abs/2503.13915</link>
<guid>https://arxiv.org/abs/2503.13915</guid>
<content:encoded><![CDATA[
arXiv:2503.13915v2 Announce Type: replace 
Abstract: We address the problem of semi-supervised domain generalization (SSDG), where the distributions of train and test data differ, and only a small amount of labeled data along with a larger amount of unlabeled data are available during training. Existing SSDG methods that leverage only the unlabeled samples for which the model's predictions are highly confident (confident-unlabeled samples), limit the full utilization of the available unlabeled data. To the best of our knowledge, we are the first to explore a method for incorporating the unconfident-unlabeled samples that were previously disregarded in SSDG setting. To this end, we propose UPCSC to utilize these unconfident-unlabeled samples in SSDG that consists of two modules: 1) Unlabeled Proxy-based Contrastive learning (UPC) module, treating unconfident-unlabeled samples as additional negative pairs and 2) Surrogate Class learning (SC) module, generating positive pairs for unconfident-unlabeled samples using their confusing class set. These modules are plug-and-play and do not require any domain labels, which can be easily integrated into existing approaches. Experiments on four widely used SSDG benchmarks demonstrate that our approach consistently improves performance when attached to baselines and outperforms competing plug-and-play methods. We also analyze the role of our method in SSDG, showing that it enhances class-level discriminability and mitigates domain gaps. The code is available at https://github.com/dongkwani/UPCSC.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models</title>
<link>https://arxiv.org/abs/2503.17794</link>
<guid>https://arxiv.org/abs/2503.17794</guid>
<content:encoded><![CDATA[
arXiv:2503.17794v3 Announce Type: replace 
Abstract: Text-to-image generative models often struggle with long prompts detailing complex scenes, diverse objects with distinct visual characteristics and spatial relationships. In this work, we propose SCoPE (Scheduled interpolation of Coarse-to-fine Prompt Embeddings), a training-free method to improve text-to-image alignment by progressively refining the input prompt in a coarse-to-fine-grained manner. Given a detailed input prompt, we first decompose it into multiple sub-prompts which evolve from describing broad scene layout to highly intricate details. During inference, we interpolate between these sub-prompts and thus progressively introduce finer-grained details into the generated image. Our training-free plug-and-play approach significantly enhances prompt alignment, achieves an average improvement of up to +4% in Visual Question Answering (VQA) scores over the Stable Diffusion baselines on 85% of the prompts from the GenAI-Bench dataset.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding</title>
<link>https://arxiv.org/abs/2503.18478</link>
<guid>https://arxiv.org/abs/2503.18478</guid>
<content:encoded><![CDATA[
arXiv:2503.18478v2 Announce Type: replace 
Abstract: Despite advanced token compression techniques, existing multimodal large language models (MLLMs) still struggle with hour-long video understanding. In this work, we propose Video-XL-Pro, an efficient method for extremely long video understanding, built upon Reconstructive Compression of Tokens (ReCoT), a learnable module that leverages self-supervised learning to generate comprehensive and compact video tokens. ReCoT introduces two key components: (i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from static image tokens by learning intra-token relationships, which are then used in masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively masks redundant visual tokens to facilitate more effective reconstructive learning. To improve training efficiency in MLLMs fine-tuning, we introduce a video-specific dataset pruning strategy and design a simple yet Query-aware Selector that enables the model to precisely locate query-relevant video tokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models trained on larger datasets across multiple long video understanding benchmarks. Moreover, it can process over 8K frames on a single A100 GPU while maintaining high-quality performance.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoGen-Eval: Agent-based System for Video Generation Evaluation</title>
<link>https://arxiv.org/abs/2503.23452</link>
<guid>https://arxiv.org/abs/2503.23452</guid>
<content:encoded><![CDATA[
arXiv:2503.23452v2 Announce Type: replace 
Abstract: The rapid advancement of video generation has rendered existing evaluation systems inadequate for assessing state-of-the-art models, primarily due to simple prompts that cannot showcase the model's capabilities, fixed evaluation operators struggling with Out-of-Distribution (OOD) cases, and misalignment between computed metrics and human preferences. To bridge the gap, we propose VideoGen-Eval, an agent evaluation system that integrates LLM-based content structuring, MLLM-based content judgment, and patch tools designed for temporal-dense dimensions, to achieve a dynamic, flexible, and expandable video generation evaluation. Additionally, we introduce a video generation benchmark to evaluate existing cutting-edge models and verify the effectiveness of our evaluation system. It comprises 700 structured, content-rich prompts (both T2V and I2V) and over 12,000 videos generated by 20+ models, among them, 8 cutting-edge models are selected as quantitative evaluation for the agent and human. Extensive experiments validate that our proposed agent-based evaluation system demonstrates strong alignment with human preferences and reliably completes the evaluation, as well as the diversity and richness of the benchmark.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Neural Transparency: Jacobian Maps for Explainable AI in Alzheimer's Detection</title>
<link>https://arxiv.org/abs/2504.03230</link>
<guid>https://arxiv.org/abs/2504.03230</guid>
<content:encoded><![CDATA[
arXiv:2504.03230v2 Announce Type: replace 
Abstract: Alzheimer's disease (AD) leads to progressive cognitive decline, making early detection crucial for effective intervention. While deep learning models have shown high accuracy in AD diagnosis, their lack of interpretability limits clinical trust and adoption. This paper introduces a novel pre-model approach leveraging Jacobian Maps (JMs) within a multi-modal framework to enhance explainability and trustworthiness in AD detection. By capturing localized brain volume changes, JMs establish meaningful correlations between model predictions and well-known neuroanatomical biomarkers of AD. We validate JMs through experiments comparing a 3D CNN trained on JMs versus on traditional preprocessed data, which demonstrates superior accuracy. We also employ 3D Grad-CAM analysis to provide both visual and quantitative insights, further showcasing improved interpretability and diagnostic reliability.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NCL-CIR: Noise-aware Contrastive Learning for Composed Image Retrieval</title>
<link>https://arxiv.org/abs/2504.04339</link>
<guid>https://arxiv.org/abs/2504.04339</guid>
<content:encoded><![CDATA[
arXiv:2504.04339v2 Announce Type: replace 
Abstract: Composed Image Retrieval (CIR) seeks to find a target image using a multi-modal query, which combines an image with modification text to pinpoint the target. While recent CIR methods have shown promise, they mainly focus on exploring relationships between the query pairs (image and text) through data augmentation or model design. These methods often assume perfect alignment between queries and target images, an idealized scenario rarely encountered in practice. In reality, pairs are often partially or completely mismatched due to issues like inaccurate modification texts, low-quality target images, and annotation errors. Ignoring these mismatches leads to numerous False Positive Pair (FFPs) denoted as noise pairs in the dataset, causing the model to overfit and ultimately reducing its performance. To address this problem, we propose the Noise-aware Contrastive Learning for CIR (NCL-CIR), comprising two key components: the Weight Compensation Block (WCB) and the Noise-pair Filter Block (NFB). The WCB coupled with diverse weight maps can ensure more stable token representations of multi-modal queries and target images. Meanwhile, the NFB, in conjunction with the Gaussian Mixture Model (GMM) predicts noise pairs by evaluating loss distributions, and generates soft labels correspondingly, allowing for the design of the soft-label based Noise Contrastive Estimation (NCE) loss function. Consequently, the overall architecture helps to mitigate the influence of mismatched and partially matched samples, with experimental results demonstrating that NCL-CIR achieves exceptional performance on the benchmark datasets.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Management of the False Discovery Rate in Medical Instance Segmentation Based on Conformal Risk Control</title>
<link>https://arxiv.org/abs/2504.04482</link>
<guid>https://arxiv.org/abs/2504.04482</guid>
<content:encoded><![CDATA[
arXiv:2504.04482v2 Announce Type: replace 
Abstract: Instance segmentation plays a pivotal role in medical image analysis by enabling precise localization and delineation of lesions, tumors, and anatomical structures. Although deep learning models such as Mask R-CNN and BlendMask have achieved remarkable progress, their application in high-risk medical scenarios remains constrained by confidence calibration issues, which may lead to misdiagnosis. To address this challenge, we propose a robust quality control framework based on conformal prediction theory. This framework innovatively constructs a risk-aware dynamic threshold mechanism that adaptively adjusts segmentation decision boundaries according to clinical requirements.Specifically, we design a \textbf{calibration-aware loss function} that dynamically tunes the segmentation threshold based on a user-defined risk level $\alpha$. Utilizing exchangeable calibration data, this method ensures that the expected FNR or FDR on test data remains below $\alpha$ with high probability. The framework maintains compatibility with mainstream segmentation models (e.g., Mask R-CNN, BlendMask+ResNet-50-FPN) and datasets (PASCAL VOC format) without requiring architectural modifications. Empirical results demonstrate that we rigorously bound the FDR metric marginally over the test set via our developed calibration framework.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AD-Det: Boosting Object Detection in UAV Images with Focused Small Objects and Balanced Tail Classes</title>
<link>https://arxiv.org/abs/2504.05601</link>
<guid>https://arxiv.org/abs/2504.05601</guid>
<content:encoded><![CDATA[
arXiv:2504.05601v2 Announce Type: replace 
Abstract: Object detection in Unmanned Aerial Vehicle (UAV) images poses significant challenges due to complex scale variations and class imbalance among objects. Existing methods often address these challenges separately, overlooking the intricate nature of UAV images and the potential synergy between them. In response, this paper proposes AD-Det, a novel framework employing a coherent coarse-to-fine strategy that seamlessly integrates two pivotal components: Adaptive Small Object Enhancement (ASOE) and Dynamic Class-balanced Copy-paste (DCC). ASOE utilizes a high-resolution feature map to identify and cluster regions containing small objects. These regions are subsequently enlarged and processed by a fine-grained detector. On the other hand, DCC conducts object-level resampling by dynamically pasting tail classes around the cluster centers obtained by ASOE, main-taining a dynamic memory bank for each tail class. This approach enables AD-Det to not only extract regions with small objects for precise detection but also dynamically perform reasonable resampling for tail-class objects. Consequently, AD-Det enhances the overall detection performance by addressing the challenges of scale variations and class imbalance in UAV images through a synergistic and adaptive framework. We extensively evaluate our approach on two public datasets, i.e., VisDrone and UAVDT, and demonstrate that AD-Det significantly outperforms existing competitive alternatives. Notably, AD-Det achieves a 37.5% Average Precision (AP) on the VisDrone dataset, surpassing its counterparts by at least 3.1%.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation</title>
<link>https://arxiv.org/abs/2504.06962</link>
<guid>https://arxiv.org/abs/2504.06962</guid>
<content:encoded><![CDATA[
arXiv:2504.06962v2 Announce Type: replace 
Abstract: Self-supervised learning (SSL) has enabled the development of vision foundation models for Earth Observation (EO), demonstrating strong transferability across diverse remote sensing tasks. While prior work has focused on network architectures and training strategies, the role of dataset curation, especially in balancing and diversifying pre-training datasets, remains underexplored. In EO, this challenge is amplified by the redundancy and heavy-tailed distributions common in satellite imagery, which can lead to biased representations and inefficient training.
  In this work, we propose a dynamic dataset pruning strategy designed to improve SSL pre-training by maximizing dataset diversity and balance. Our method iteratively refines the training set without requiring a pre-existing feature extractor, making it well-suited for domains where curated datasets are limited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode (WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by ocean observations. We train models from scratch on the entire Sentinel-1 WV archive spanning 10 years. Across three downstream tasks, our results show that dynamic pruning improves both computational efficiency and representation quality, leading to stronger transferability.
  We also release the weights of OceanSAR-1, the first model in the OceanSAR family, a series of foundation models for ocean observation and analysis using SAR imagery, at github.com/galeio-research/OceanSAR-models/.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniCaptioner: One Captioner to Rule Them All</title>
<link>https://arxiv.org/abs/2504.07089</link>
<guid>https://arxiv.org/abs/2504.07089</guid>
<content:encoded><![CDATA[
arXiv:2504.07089v2 Announce Type: replace 
Abstract: We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-IFEngine: Towards Multimodal Instruction Following</title>
<link>https://arxiv.org/abs/2504.07957</link>
<guid>https://arxiv.org/abs/2504.07957</guid>
<content:encoded><![CDATA[
arXiv:2504.07957v2 Announce Type: replace 
Abstract: The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2$\%$), MIA (+7.6$\%$), and IFEval (+12.3$\%$). We have fully open-sourced the datasets (both SFT and DPO), evaluation code and training scripts at https://github.com/SYuan03/MM-IFEngine.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation</title>
<link>https://arxiv.org/abs/2504.09149</link>
<guid>https://arxiv.org/abs/2504.09149</guid>
<content:encoded><![CDATA[
arXiv:2504.09149v2 Announce Type: replace 
Abstract: We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable local surface patches, each defined by a spherical distance function emanating from an anchor point. We further leverage the compactness of spherical harmonics to encode the MASH functions, combined with a generalized view cone with a parameterized base that masks the spatial extent of the spherical function to attain locality. We develop a differentiable optimization algorithm capable of converting any point cloud into a MASH representation accurately approximating ground-truth surfaces with arbitrary geometry and topology. Extensive experiments demonstrate that MASH is versatile for multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environment</title>
<link>https://arxiv.org/abs/2504.11019</link>
<guid>https://arxiv.org/abs/2504.11019</guid>
<content:encoded><![CDATA[
arXiv:2504.11019v2 Announce Type: replace 
Abstract: Reliable traffic data are essential for understanding urban mobility and developing effective traffic management strategies. This study introduces the DRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale urban traffic dataset collected systematically from synchronized drone videos at approximately 250 meters altitude, covering nine interconnected intersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle trajectories that include directional information, processed through video synchronization and orthomap alignment, resulting in a comprehensive dataset of 81,699 vehicle trajectories. Through our DRIFT dataset, researchers can simultaneously analyze traffic at multiple scales - from individual vehicle maneuvers like lane-changes and safety metrics such as time-to-collision to aggregate network flow dynamics across interconnected urban intersections. The DRIFT dataset is structured to enable immediate use without additional preprocessing, complemented by open-source models for object detection and trajectory extraction, as well as associated analytical tools. DRIFT is expected to significantly contribute to academic research and practical applications, such as traffic flow analysis and simulation studies. The dataset and related resources are publicly accessible at https://github.com/AIxMobility/The-DRIFT.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals</title>
<link>https://arxiv.org/abs/2504.12121</link>
<guid>https://arxiv.org/abs/2504.12121</guid>
<content:encoded><![CDATA[
arXiv:2504.12121v2 Announce Type: replace 
Abstract: Identifying spatial regions where biodiversity is threatened is crucial for effective ecosystem conservation and monitoring. In this stydy, we assessed varios machine learning methods to detect grazing trails automatically. We tested five semantic segmentation models combined with 14 different encoder networks. The best combination was UNet with MambaOut encoder. The solution proposed could be used as the basis for tools aiming at mapping and tracking changes in grazing trails on a continuous temporal basis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FetaFix: Automatic Fault Localization and Repair of Deep Learning Model Conversions</title>
<link>https://arxiv.org/abs/2312.15101</link>
<guid>https://arxiv.org/abs/2312.15101</guid>
<content:encoded><![CDATA[
arXiv:2312.15101v4 Announce Type: replace-cross 
Abstract: Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.
  In this paper, we propose an automated approach for fault localization and repair, FetaFix, during model conversion between deep learning frameworks. FetaFix is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion. FetaFix uses a set of fault types (mined from surveying common conversion issues reported in code repositories and forums) to localize potential conversion faults in the converted target model and then repair them appropriately, e.g., replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the dataset, comparing output label differences between the source model and the converted target model until all differences are resolved. We evaluate the effectiveness of FetaFix in fixing model conversion bugs of three widely used image recognition models converted across four different deep learning frameworks. Overall, FetaFix was able to fix $462$ out of $755$ detected conversion faults, either completely repairing or significantly improving the performance of $14$ out of the $15$ erroneous conversion cases.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An $\ell^1$-Plug-and-Play Approach for MPI Using a Zero Shot Denoiser with Evaluation on the 3D Open MPI Dataset</title>
<link>https://arxiv.org/abs/2401.00275</link>
<guid>https://arxiv.org/abs/2401.00275</guid>
<content:encoded><![CDATA[
arXiv:2401.00275v3 Announce Type: replace-cross 
Abstract: Objective: Magnetic particle imaging (MPI) is an emerging medical imaging modality which has gained increasing interest in recent years. Among the benefits of MPI are its high temporal resolution, and that the technique does not expose the specimen to any kind of ionizing radiation. It is based on the non-linear response of magnetic nanoparticles to an applied magnetic field. From the electric signal measured in receive coils, the particle concentration has to be reconstructed. Due to the ill-posedness of the reconstruction problem, various regularization methods have been proposed for reconstruction ranging from early stopping methods, via classical Tikhonov regularization and iterative methods to modern machine learning approaches. In this work, we contribute to the latter class: we propose a plug-and-play approach based on a generic zero-shot denoiser with an $\ell^1$-prior.
  Approach: We validate the reconstruction parameters of the method on a hybrid dataset and compare it with the baseline Tikhonov, DIP and the previous PP-MPI, which is a plug-and-play method with denoiser trained on MPI-friendly data.
  Main results: We offer a quantitative and qualitative evaluation of the zero-shot plug-and-play approach on the 3D Open MPI dataset. Moreover, we show the quality of the approach with different levels of preprocessing of the data.
  Significance: The proposed method employs a zero-shot denoiser which has not been trained for the MPI task and therefore saves the cost for training. Moreover, it offers a method that can be potentially applied in future MPI contexts.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperparameters in Continual Learning: A Reality Check</title>
<link>https://arxiv.org/abs/2403.09066</link>
<guid>https://arxiv.org/abs/2403.09066</guid>
<content:encoded><![CDATA[
arXiv:2403.09066v4 Announce Type: replace-cross 
Abstract: Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a CL scenario) while balancing the trade-off between plasticity (learning new tasks) and stability (retaining prior knowledge). The dominantly adopted conventional evaluation protocol for CL algorithms selects the best hyperparameters (e.g., learning rate, mini-batch size, regularization strengths, etc.) within a given scenario and then evaluates the algorithms using these hyperparameters in the same scenario. However, this protocol has significant shortcomings: it overestimates the CL capacity of algorithms and relies on unrealistic hyperparameter tuning, which is not feasible for real-world applications. From the fundamental principles of evaluation in machine learning, we argue that the evaluation of CL algorithms should focus on assessing the generalizability of their CL capacity to unseen scenarios. Based on this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP) consisting of hyperparameter tuning and evaluation phases. Both phases share the same scenario configuration (e.g., number of tasks) but are generated from different datasets. Hyperparameters of CL algorithms are tuned in the first phase and applied in the second phase to evaluate the algorithms. We apply this protocol to class-incremental learning, both with and without pretrained models. Across more than 8,000 experiments, our results show that most state-of-the-art algorithms fail to replicate their reported performance, highlighting that their CL capacity has been significantly overestimated in the conventional evaluation protocol. Our implementation can be found in https://github.com/csm9493/GTEP.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking</title>
<link>https://arxiv.org/abs/2404.08535</link>
<guid>https://arxiv.org/abs/2404.08535</guid>
<content:encoded><![CDATA[
arXiv:2404.08535v2 Announce Type: replace-cross 
Abstract: Contrastive learning has gained widespread adoption for retrieval tasks due to its minimal requirement for manual annotations. However, popular training frameworks typically learn from binary (positive/negative) relevance, making them ineffective at incorporating desired rankings. As a result, the poor ranking performance of these models forces systems to employ a re-ranker, which increases complexity, maintenance effort and inference time. To address this, we introduce Generalized Contrastive Learning (GCL), a training framework designed to learn from continuous ranking scores beyond binary relevance. GCL encodes both relevance and ranking information into a unified embedding space by applying ranking scores to the loss function. This enables a single-stage retrieval system. In addition, during our research, we identified a lack of public multi-modal datasets that benchmark both retrieval and ranking capabilities. To facilitate this and future research for ranked retrieval, we curated a large-scale MarqoGS-10M dataset using GPT-4 and Google Shopping, providing ranking scores for each of the 10 million query-document pairs. Our results show that GCL achieves a 29.3% increase in NDCG@10 for in-domain evaluations and 6.0% to 10.0% increases for cold-start evaluations compared to the finetuned CLIP baseline with MarqoGS-10M. Additionally, we evaluated GCL offline on a proprietary user interaction data. GCL shows an 11.2% gain for in-domain evaluations. The dataset and the method are available at: https://github.com/marqo-ai/GCL.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A prototype-based model for set classification</title>
<link>https://arxiv.org/abs/2408.13720</link>
<guid>https://arxiv.org/abs/2408.13720</guid>
<content:encoded><![CDATA[
arXiv:2408.13720v2 Announce Type: replace-cross 
Abstract: Classification of sets of inputs (e.g., images and texts) is an active area of research within both computer vision (CV) and natural language processing (NLP). A common way to represent a set of vectors is to model them as linear subspaces. In this contribution, we present a prototype-based approach for learning on the manifold formed from such linear subspaces, the Grassmann manifold. Our proposed method learns a set of subspace prototypes capturing the representative characteristics of classes and a set of relevance factors automating the selection of the dimensionality of the subspaces. This leads to a transparent classifier model which presents the computed impact of each input vector on its decision. Through experiments on benchmark image and text datasets, we have demonstrated the efficiency of our proposed classifier, compared to the transformer-based models in terms of not only performance and explainability but also computational resource requirements.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Devil is in Details: Locality-Aware 3D Abdominal CT Volume Generation for Self-Supervised Organ Segmentation</title>
<link>https://arxiv.org/abs/2409.20332</link>
<guid>https://arxiv.org/abs/2409.20332</guid>
<content:encoded><![CDATA[
arXiv:2409.20332v2 Announce Type: replace-cross 
Abstract: In the realm of medical image analysis, self-supervised learning (SSL) techniques have emerged to alleviate labeling demands, while still facing the challenge of training data scarcity owing to escalating resource requirements and privacy constraints. Numerous efforts employ generative models to generate high-fidelity, unlabeled 3D volumes across diverse modalities and anatomical regions. However, the intricate and indistinguishable anatomical structures within the abdomen pose a unique challenge to abdominal CT volume generation compared to other anatomical regions. To address the overlooked challenge, we introduce the Locality-Aware Diffusion (Lad), a novel method tailored for exquisite 3D abdominal CT volume generation. We design a locality loss to refine crucial anatomical regions and devise a condition extractor to integrate abdominal priori into generation, thereby enabling the generation of large quantities of high-quality abdominal CT volumes essential for SSL tasks without the need for additional data such as labels or radiology reports. Volumes generated through our method demonstrate remarkable fidelity in reproducing abdominal structures, achieving a decrease in FID score from 0.0034 to 0.0002 on AbdomenCT-1K dataset, closely mirroring authentic data and surpassing current methods. Extensive experiments demonstrate the effectiveness of our method in self-supervised organ segmentation tasks, resulting in an improvement in mean Dice scores on two abdominal datasets effectively. These results underscore the potential of synthetic data to advance self-supervised learning in medical image analysis.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning</title>
<link>https://arxiv.org/abs/2410.06140</link>
<guid>https://arxiv.org/abs/2410.06140</guid>
<content:encoded><![CDATA[
arXiv:2410.06140v3 Announce Type: replace-cross 
Abstract: QUIC, a new and increasingly used transport protocol, enhances TCP by offering improved security, performance, and stream multiplexing. These features, however, also impose challenges for network middle-boxes that need to monitor and analyze web traffic. This paper proposes a novel method to estimate the number of HTTP/3 responses in a given QUIC connection by an observer. This estimation reveals server behavior, client-server interactions, and data transmission efficiency, which is crucial for various applications such as designing a load balancing solution and detecting HTTP/3 flood attacks. The proposed scheme transforms QUIC connection traces into image sequences and uses machine learning (ML) models, guided by a tailored loss function, to predict response counts. Evaluations on more than seven million images-derived from 100,000 traces collected across 44,000 websites over four months-achieve up to 97% accuracy in both known and unknown server settings and 92% accuracy on previously unseen complete QUIC traces.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Modality-Aware Representations: Adaptive Group-wise Interaction Network for Multimodal MRI Synthesis</title>
<link>https://arxiv.org/abs/2411.14684</link>
<guid>https://arxiv.org/abs/2411.14684</guid>
<content:encoded><![CDATA[
arXiv:2411.14684v2 Announce Type: replace-cross 
Abstract: Multimodal MR image synthesis aims to generate missing modality images by effectively fusing and mapping from a subset of available MRI modalities. Most existing methods adopt an image-to-image translation paradigm, treating multiple modalities as input channels. However, these approaches often yield sub-optimal results due to the inherent difficulty in achieving precise feature- or semantic-level alignment across modalities. To address these challenges, we propose an Adaptive Group-wise Interaction Network (AGI-Net) that explicitly models both inter-modality and intra-modality relationships for multimodal MR image synthesis. Specifically, feature channels are first partitioned into predefined groups, after which an adaptive rolling mechanism is applied to conventional convolutional kernels to better capture feature and semantic correspondences between different modalities. In parallel, a cross-group attention module is introduced to enable effective feature fusion across groups, thereby enhancing the network's representational capacity. We validate the proposed AGI-Net on the publicly available IXI and BraTS2023 datasets. Experimental results demonstrate that AGI-Net achieves state-of-the-art performance in multimodal MR image synthesis tasks, confirming the effectiveness of its modality-aware interaction design. We release the relevant code at: https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Consistent Nested Diffusion Bridge for Accelerated MRI Reconstruction</title>
<link>https://arxiv.org/abs/2412.09998</link>
<guid>https://arxiv.org/abs/2412.09998</guid>
<content:encoded><![CDATA[
arXiv:2412.09998v2 Announce Type: replace-cross 
Abstract: Accelerated MRI reconstruction plays a vital role in reducing scan time while preserving image quality. While most existing methods rely on complex-valued image-space or k-space data, these formats are often inaccessible in clinical practice due to proprietary reconstruction pipelines, leaving only magnitude images stored in DICOM files. To address this gap, we focus on the underexplored task of magnitude-image-based MRI reconstruction. Recent advancements in diffusion models, particularly denoising diffusion probabilistic models (DDPMs), have demonstrated strong capabilities in modeling image priors. However, their task-agnostic denoising nature limits performance in source-to-target image translation tasks, such as MRI reconstruction. In this work, we propose a novel Self-Consistent Nested Diffusion Bridge (SC-NDB) framework that models accelerated MRI reconstruction as a bi-directional image translation process between under-sampled and fully-sampled magnitude MRI images. SC-NDB introduces a nested diffusion architecture with a self-consistency constraint and reverse bridge diffusion pathways to improve intermediate prediction fidelity and better capture the explicit priors of source images. Furthermore, we incorporate a Contour Decomposition Embedding Module (CDEM) to inject structural and textural knowledge by leveraging Laplacian pyramids and directional filter banks. Extensive experiments on the fastMRI and IXI datasets demonstrate that our method achieves state-of-the-art performance compared to both magnitude-based and non-magnitude-based diffusion models, confirming the effectiveness and clinical relevance of SC-NDB.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations</title>
<link>https://arxiv.org/abs/2501.14208</link>
<guid>https://arxiv.org/abs/2501.14208</guid>
<content:encoded><![CDATA[
arXiv:2501.14208v2 Announce Type: replace-cross 
Abstract: Bimanual robotic manipulation is a long-standing challenge of embodied intelligence due to its characteristics of dual-arm spatial-temporal coordination and high-dimensional action spaces. Previous studies rely on pre-defined action taxonomies or direct teleoperation to alleviate or circumvent these issues, often making them lack simplicity, versatility and scalability. Differently, we believe that the most effective and efficient way for teaching bimanual manipulation is learning from human demonstrated videos, where rich features such as spatial-temporal positions, dynamic postures, interaction states and dexterous transitions are available almost for free. In this work, we propose the YOTO (You Only Teach Once), which can extract and then inject patterns of bimanual actions from as few as a single binocular observation of hand movements, and teach dual robot arms various complex tasks. Furthermore, based on keyframes-based motion trajectories, we devise a subtle solution for rapidly generating training demonstrations with diverse variations of manipulated objects and their locations. These data can then be used to learn a customized bimanual diffusion policy (BiDP) across diverse scenes. In experiments, YOTO achieves impressive performance in mimicking 5 intricate long-horizon bimanual tasks, possesses strong generalization under different visual and spatial conditions, and outperforms existing visuomotor imitation learning methods in accuracy and efficiency. Our project link is https://hnuzhy.github.io/projects/YOTO.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Feasibility of Patch-based Inference for Generalized Diffusion Priors in Inverse Problems for Medical Images</title>
<link>https://arxiv.org/abs/2501.15309</link>
<guid>https://arxiv.org/abs/2501.15309</guid>
<content:encoded><![CDATA[
arXiv:2501.15309v2 Announce Type: replace-cross 
Abstract: Plug-and-play approaches to solving inverse problems such as restoration and super-resolution have recently benefited from Diffusion-based generative priors for natural as well as medical images. However, solutions often use the standard albeit computationally intensive route of training and inferring with the whole image on the diffusion prior. While patch-based approaches to evaluating diffusion priors in plug-and-play methods have received some interest, they remain an open area of study. In this work, we explore the feasibility of the usage of patches for training and inference of a diffusion prior on MRI images. We explore the minor adaptation necessary for artifact avoidance, the performance and the efficiency of memory usage of patch-based methods as well as the adaptability of whole image training to patch-based evaluation - evaluating across multiple plug-and-play methods, tasks and datasets.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge</title>
<link>https://arxiv.org/abs/2501.19259</link>
<guid>https://arxiv.org/abs/2501.19259</guid>
<content:encoded><![CDATA[
arXiv:2501.19259v2 Announce Type: replace-cross 
Abstract: The integration of human-intuitive interactions into autonomous systems has been limited. Traditional Natural Language Processing (NLP) systems struggle with context and intent understanding, severely restricting human-robot interaction. Recent advancements in Large Language Models (LLMs) have transformed this dynamic, allowing for intuitive and high-level communication through speech and text, and bridging the gap between human commands and robotic actions. Additionally, autonomous navigation has emerged as a central focus in robotics research, with artificial intelligence (AI) increasingly being leveraged to enhance these systems. However, existing AI-based navigation algorithms face significant challenges in latency-critical tasks where rapid decision-making is critical. Traditional frame-based vision systems, while effective for high-level decision-making, suffer from high energy consumption and latency, limiting their applicability in real-time scenarios. Neuromorphic vision systems, combining event-based cameras and spiking neural networks (SNNs), offer a promising alternative by enabling energy-efficient, low-latency navigation. Despite their potential, real-world implementations of these systems, particularly on physical platforms such as drones, remain scarce. In this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural language processing, Neuro-LIFT translates human speech into high-level planning commands which are then autonomously executed using event-based neuromorphic vision and physics-driven planning. Our framework demonstrates its capabilities in navigating in a dynamic environment, avoiding obstacles, and adapting to human instructions in real-time.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Deep Human-in-the-Loop Optimization for Retinal Implants Using Sighted Participants</title>
<link>https://arxiv.org/abs/2502.00177</link>
<guid>https://arxiv.org/abs/2502.00177</guid>
<content:encoded><![CDATA[
arXiv:2502.00177v2 Announce Type: replace-cross 
Abstract: Human-in-the-loop optimization (HILO) is a promising approach for personalizing visual prostheses by iteratively refining stimulus parameters based on user feedback. Previous work demonstrated HILO's efficacy in simulation, but its performance with human participants remains untested. Here we evaluate HILO using sighted participants viewing simulated prosthetic vision to assess its ability to optimize stimulation strategies under realistic conditions. Participants selected between phosphenes generated by competing encoders to iteratively refine a deep stimulus encoder (DSE). We tested HILO in three conditions: standard optimization, threshold misspecifications, and out-of-distribution parameter sampling. Participants consistently preferred HILO-generated stimuli over both a naive encoder and the DSE alone, with log odds favoring HILO across all conditions. We also observed key differences between human and simulated decision-making, highlighting the importance of validating optimization strategies with human participants. These findings support HILO as a viable approach for adapting visual prostheses to individuals. Clinical relevance: Validating HILO with sighted participants viewing simulated prosthetic vision is an important step toward personalized calibration of future visual prostheses.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Rome with Convex Optimization</title>
<link>https://arxiv.org/abs/2502.04640</link>
<guid>https://arxiv.org/abs/2502.04640</guid>
<content:encoded><![CDATA[
arXiv:2502.04640v3 Announce Type: replace-cross 
Abstract: Global bundle adjustment is made easy by depth prediction and convex optimization. We (i) propose a scaled bundle adjustment (SBA) formulation that lifts 2D keypoint measurements to 3D with learned depth, (ii) design an empirically tight convex semidfinite program (SDP) relaxation that solves SBA to certfiable global optimality, (iii) solve the SDP relaxations at extreme scale with Burer-Monteiro factorization and a CUDA-based trust-region Riemannian optimizer (dubbed XM), (iv) build a structure from motion (SfM) pipeline with XM as the optimization engine and show that XM-SfM compares favorably with existing pipelines in terms of reconstruction quality while being significantly faster, more scalable, and initialization-free.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</title>
<link>https://arxiv.org/abs/2502.19645</link>
<guid>https://arxiv.org/abs/2502.19645</guid>
<content:encoded><![CDATA[
arXiv:2502.19645v2 Announce Type: replace-cross 
Abstract: Recent vision-language-action models (VLAs) build upon pretrained vision-language models and leverage diverse robot datasets to demonstrate strong task execution, language following ability, and semantic generalization. Despite these successes, VLAs struggle with novel robot setups and require fine-tuning to achieve good performance, yet how to most effectively fine-tune them is unclear given many possible strategies. In this work, we study key VLA adaptation design choices such as different action decoding schemes, action representations, and learning objectives for fine-tuning, using OpenVLA as our representative base model. Our empirical analysis informs an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a continuous action representation, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and flexibility in the model's input-output specifications. We propose OpenVLA-OFT, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5% to 97.1% while increasing action generation throughput by 26$\times$. In real-world evaluations, our fine-tuning recipe enables OpenVLA to successfully execute dexterous, high-frequency control tasks on a bimanual ALOHA robot and outperform other VLAs ($\pi_0$ and RDT-1B) fine-tuned using their default recipes, as well as strong imitation learning policies trained from scratch (Diffusion Policy and ACT) by up to 15% (absolute) in average success rate. We release code for OFT and pretrained model checkpoints at https://openvla-oft.github.io/.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Permutation Learning with Only N Parameters: From SoftSort to Self-Organizing Gaussians</title>
<link>https://arxiv.org/abs/2503.13051</link>
<guid>https://arxiv.org/abs/2503.13051</guid>
<content:encoded><![CDATA[
arXiv:2503.13051v2 Announce Type: replace-cross 
Abstract: Sorting and permutation learning are key concepts in optimization and machine learning, especially when organizing high-dimensional data into meaningful spatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N parameters to determine a full permutation matrix, making it computationally expensive for large datasets. Low-rank matrix factorization approximations reduce memory requirements to 2NM (with M << N), but they still struggle with very large problems. SoftSort, by providing a continuous relaxation of the argsort operator, allows differentiable 1D sorting, but it faces challenges with multidimensional data and complex permutations. In this paper, we present a novel method for learning permutations using only N parameters, which dramatically reduces storage costs. Our method extends SoftSort by iteratively shuffling the N indices of the elements and applying a few SoftSort optimization steps per iteration. This modification significantly improves sorting quality, especially for multidimensional data and complex optimization criteria, and outperforms pure SoftSort. Our method offers improved memory efficiency and scalability compared to existing approaches, while maintaining high-quality permutation learning. Its dramatically reduced memory requirements make it particularly well-suited for large-scale optimization tasks, such as "Self-Organizing Gaussians", where efficient and scalable permutation learning is critical.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Electrocardiogram Generation Using Hierarchical Variational Autoencoders</title>
<link>https://arxiv.org/abs/2503.13469</link>
<guid>https://arxiv.org/abs/2503.13469</guid>
<content:encoded><![CDATA[
arXiv:2503.13469v2 Announce Type: replace-cross 
Abstract: Cardiovascular diseases (CVDs) are disorders impacting the heart and circulatory system. These disorders are the foremost and continuously escalating cause of mortality worldwide. One of the main tasks when working with CVDs is analyzing and identifying pathologies on a 12-lead electrocardiogram (ECG) with a standard 10-second duration. Using machine learning (ML) in automatic ECG analysis increases CVD diagnostics' availability, speed, and accuracy. However, the most significant difficulty in developing ML models is obtaining a sufficient training dataset. Due to the limitations of medical data usage, such as expensiveness, errors, the ambiguity of labels, imbalance of classes, and privacy issues, utilizing synthetic samples depending on specific pathologies bypasses these restrictions and improves algorithm quality. Existing solutions for the conditional generation of ECG signals are mainly built on Generative Adversarial Networks (GANs), and only a few papers consider the architectures based on Variational Autoencoders (VAEs), showing comparable results in recent works. This paper proposes the publicly available conditional Nouveau VAE model for ECG signal generation (cNVAE-ECG), which produces high-resolution ECGs with multiple pathologies. We provide an extensive comparison of the proposed model on various practical downstream tasks, including transfer learning scenarios showing an area under the receiver operating characteristic (AUROC) increase up to 2% surpassing GAN-like competitors.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs</title>
<link>https://arxiv.org/abs/2504.12909</link>
<guid>https://arxiv.org/abs/2504.12909</guid>
<content:encoded><![CDATA[
arXiv:2504.12909v2 Announce Type: replace-cross 
Abstract: Many works have succeeded in reconstructing Gaussian human avatars from multi-view videos. However, they either struggle to capture pose-dependent appearance details with a single MLP, or rely on a computationally intensive neural network to reconstruct high-fidelity appearance but with rendering performance degraded to non-real-time. We propose a novel Gaussian human avatar representation that can reconstruct high-fidelity pose-dependence appearance with details and meanwhile can be rendered in real time. Our Gaussian avatar is empowered by spatially distributed MLPs which are explicitly located on different positions on human body. The parameters stored in each Gaussian are obtained by interpolating from the outputs of its nearby MLPs based on their distances. To avoid undesired smooth Gaussian property changing during interpolation, for each Gaussian we define a set of Gaussian offset basis, and a linear combination of basis represents the Gaussian property offsets relative to the neutral properties. Then we propose to let the MLPs output a set of coefficients corresponding to the basis. In this way, although Gaussian coefficients are derived from interpolation and change smoothly, the Gaussian offset basis is learned freely without constraints. The smoothly varying coefficients combined with freely learned basis can still produce distinctly different Gaussian property offsets, allowing the ability to learn high-frequency spatial signals. We further use control points to constrain the Gaussians distributed on a surface layer rather than allowing them to be irregularly distributed inside the body, to help the human avatar generalize better when animated under novel poses. Compared to the state-of-the-art method, our method achieves better appearance quality with finer details while the rendering speed is significantly faster under novel views and novel poses.
]]></content:encoded>
<pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion</title>
<link>https://arxiv.org/abs/2504.08937</link>
<guid>https://arxiv.org/abs/2504.08937</guid>
<content:encoded><![CDATA[
<div> Few-shot training, neural networks, image fusion, deep learning, Granular Ball Significant Extraction

Summary:
The paper introduces a novel image fusion framework called GBFF, specifically designed for few-shot training with prior knowledge. The Coarse-Grained Granular Ball model is utilized to represent all pixel pairs in the fusion process. Fine-Grained Granular Balls are used to extract Non-Salient and Salient Pixel Pairs in the brightness space. Pixel-wise weights are computed to generate a pseudo-supervised image. The algorithm categorizes pixel pairs into the Positive Region or Boundary Region based on their contributions. The Granular Ball adapts to modality-aware settings to adjust the neural network's loss function. Extensive experiments prove the effectiveness of the proposed approach, showing competitiveness in fusion time and image expressiveness compared to state-of-the-art methods. <div>
arXiv:2504.08937v3 Announce Type: replace-cross 
Abstract: In image fusion tasks, the absence of real fused images as priors presents a fundamental challenge. Most deep learning-based fusion methods rely on large-scale paired datasets to extract global weighting features from raw images, thereby generating fused outputs that approximate real fused images. In contrast to previous studies, this paper explores few-shot training of neural networks under the condition of having prior knowledge. We propose a novel fusion framework named GBFF, and a Granular Ball Significant Extraction algorithm specifically designed for the few-shot prior setting. All pixel pairs involved in the fusion process are initially modeled as a Coarse-Grained Granular Ball. At the local level, Fine-Grained Granular Balls are used to slide through the brightness space to extract Non-Salient Pixel Pairs, and perform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights are then computed to generate a pseudo-supervised image. At the global level, pixel pairs with significant contributions to the fusion process are categorized into the Positive Region, while those whose contributions cannot be accurately determined are assigned to the Boundary Region. The Granular Ball performs modality-aware adaptation based on the proportion of the positive region, thereby adjusting the neural network's loss function and enabling it to complement the information of the boundary region. Extensive experiments demonstrate the effectiveness of both the proposed algorithm and the underlying theory. Compared with state-of-the-art (SOTA) methods, our approach shows strong competitiveness in terms of both fusion time and image expressiveness. Our code is publicly available at:
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Dictionary Learning for Generative Image Modeling</title>
<link>https://arxiv.org/abs/2504.17804</link>
<guid>https://arxiv.org/abs/2504.17804</guid>
<content:encoded><![CDATA[
<div> frequency, phase, amplitude, spectral basis functions, image synthesis, spectral generative model

Summary:<br />
- The proposed spectral generative model for image synthesis reconstructs images as linear combinations of learned spectral basis functions parameterized by frequency, phase, and amplitude.
- The model learns a global spectral dictionary with time-varying modulations and image-specific mixing coefficients to generate new images from a latent space.
- The deterministic approach offers interpretability and physically meaningful representations compared to stochastic methods.
- Frequency-domain loss functions are incorporated using the short-time Fourier transform to capture global structure and fine-grained spectral details.
- Experimental results on CIFAR-10 show competitive performance in reconstruction quality, perceptual fidelity, training stability, and computational efficiency.<br />Summary: <div>
arXiv:2504.17804v1 Announce Type: new 
Abstract: We propose a novel spectral generative model for image synthesis that departs radically from the common variational, adversarial, and diffusion paradigms. In our approach, images, after being flattened into one-dimensional signals, are reconstructed as linear combinations of a set of learned spectral basis functions, where each basis is explicitly parameterized in terms of frequency, phase, and amplitude. The model jointly learns a global spectral dictionary with time-varying modulations and per-image mixing coefficients that quantify the contributions of each spectral component. Subsequently, a simple probabilistic model is fitted to these mixing coefficients, enabling the deterministic generation of new images by sampling from the latent space. This framework leverages deterministic dictionary learning, offering a highly interpretable and physically meaningful representation compared to methods relying on stochastic inference or adversarial training. Moreover, the incorporation of frequency-domain loss functions, computed via the short-time Fourier transform (STFT), ensures that the synthesized images capture both global structure and fine-grained spectral details, such as texture and edge information. Experimental evaluations on the CIFAR-10 benchmark demonstrate that our approach not only achieves competitive performance in terms of reconstruction quality and perceptual fidelity but also offers improved training stability and computational efficiency. This new type of generative model opens up promising avenues for controlled synthesis, as the learned spectral dictionary affords a direct handle on the intrinsic frequency content of the images, thus providing enhanced interpretability and potential for novel applications in image manipulation and analysis.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos</title>
<link>https://arxiv.org/abs/2504.17810</link>
<guid>https://arxiv.org/abs/2504.17810</guid>
<content:encoded><![CDATA[
<div> Gaussian splatting, camera pose estimation, small-baseline videos, dynamic scenes, DINOv2 <br />
<br />
Summary: <br />
SmallGS is a camera pose estimation framework designed for small-baseline videos with minimal motion. It utilizes Gaussian splatting to reconstruct scenes and optimize sequential camera poses, providing a stable reference frame for accurate estimation. By incorporating pretrained visual features like DINOv2, SmallGS enhances the robustness of pose estimation without the need for explicit feature correspondences or strong parallax motion. The temporal consistency of Gaussian splatting allows for accurate pose estimation even in dynamic scenes with limited viewpoint changes. In experiments on TUM-Dynamics sequences, SmallGS outperformed existing methods like MonST3R and DORID-SLAM in accurately estimating camera poses in small-baseline videos. The framework's effectiveness lies in its ability to leverage Gaussian splatting and high-dimensional feature mapping to achieve impressive accuracy in pose estimation for dynamic videos with small baseline motions. <div>
arXiv:2504.17810v1 Announce Type: new 
Abstract: Dynamic videos with small baseline motions are ubiquitous in daily life, especially on social media. However, these videos present a challenge to existing pose estimation frameworks due to ambiguous features, drift accumulation, and insufficient triangulation constraints. Gaussian splatting, which maintains an explicit representation for scenes, provides a reliable novel view rasterization when the viewpoint change is small. Inspired by this, we propose SmallGS, a camera pose estimation framework that is specifically designed for small-baseline videos. SmallGS optimizes sequential camera poses using Gaussian splatting, which reconstructs the scene from the first frame in each video segment to provide a stable reference for the rest. The temporal consistency of Gaussian splatting within limited viewpoint differences reduced the requirement of sufficient depth variations in traditional camera pose estimation. We further incorporate pretrained robust visual features, e.g. DINOv2, into Gaussian splatting, where high-dimensional feature map rendering enhances the robustness of camera pose estimation. By freezing the Gaussian splatting and optimizing camera viewpoints based on rasterized features, SmallGS effectively learns camera poses without requiring explicit feature correspondences or strong parallax motion. We verify the effectiveness of SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves impressive accuracy in camera pose estimation compared to MonST3R and DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at: https://yuxinyao620.github.io/SmallGS
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object Learning and Robust 3D Reconstruction</title>
<link>https://arxiv.org/abs/2504.17812</link>
<guid>https://arxiv.org/abs/2504.17812</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network, unsupervised object segmentation, 2D scenarios, motion cues, 3D applications<br />
<br />
Summary: <br />
This thesis explores architectural designs and training methods for neural networks to perform unsupervised object segmentation in images. The focus is on distinguishing foreground objects from the background, particularly in 2D scenarios where motion cues are utilized through FlowCapsules. The research also extends to 3D applications, where dynamic objects are detected and removed using geometric consistency. Transient object masks are generated for optimizing 3D modeling in casual capture setups. By demonstrating the advantages of unsupervised object-based approaches in computer vision, the thesis aims to inspire further exploration of explicit object representations in image understanding tasks. Ultimately, the goal is to stimulate the community to delve into defining objects of interest without the need for supervision. <br /> <div>
arXiv:2504.17812v1 Announce Type: new 
Abstract: In this thesis we discuss architectural designs and training methods for a neural network to have the ability of dissecting an image into objects of interest without supervision. The main challenge in 2D unsupervised object segmentation is distinguishing between foreground objects of interest and background. FlowCapsules uses motion as a cue for the objects of interest in 2D scenarios. The last part of this thesis focuses on 3D applications where the goal is detecting and removal of the object of interest from the input images. In these tasks, we leverage the geometric consistency of scenes in 3D to detect the inconsistent dynamic objects. Our transient object masks are then used for designing robust optimization kernels to improve 3D modelling in a casual capture setup. One of our goals in this thesis is to show the merits of unsupervised object based approaches in computer vision. Furthermore, we suggest possible directions for defining objects of interest or foreground objects without requiring supervision. Our hope is to motivate and excite the community into further exploring explicit object representations in image understanding tasks.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss</title>
<link>https://arxiv.org/abs/2504.17813</link>
<guid>https://arxiv.org/abs/2504.17813</guid>
<content:encoded><![CDATA[
<div> Keywords: ordinal classification, margin-based contrastive learning, multi-margin n-pair loss, image datasets, clinical decision bias

Summary: 
The article introduces CLOC, a novel margin-based contrastive learning method for ordinal classification that considers the varying importance of margins between neighboring classes. The proposed method optimizes multiple margins using a multi-margin n-pair loss (MMNP) to learn an ordered representation. CLOC allows for flexible decision boundaries, smooth transitions between classes, and reduces the risk of overfitting to biases in training data. Experimental results on real-world image datasets and a synthetic dataset simulating clinical decision bias show that CLOC outperforms existing ordinal classification methods. The interpretability and controllability of CLOC in learning meaningful, ordered representations that align with clinical and practical needs are also demonstrated. <div>
arXiv:2504.17813v1 Announce Type: new 
Abstract: In ordinal classification, misclassifying neighboring ranks is common, yet the consequences of these errors are not the same. For example, misclassifying benign tumor categories is less consequential, compared to an error at the pre-cancerous to cancerous threshold, which could profoundly influence treatment choices. Despite this, existing ordinal classification methods do not account for the varying importance of these margins, treating all neighboring classes as equally significant. To address this limitation, we propose CLOC, a new margin-based contrastive learning method for ordinal classification that learns an ordered representation based on the optimization of multiple margins with a novel multi-margin n-pair loss (MMNP). CLOC enables flexible decision boundaries across key adjacent categories, facilitating smooth transitions between classes and reducing the risk of overfitting to biases present in the training data. We provide empirical discussion regarding the properties of MMNP and show experimental results on five real-world image datasets (Adience, Historical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic Retinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset simulating clinical decision bias. Our results demonstrate that CLOC outperforms existing ordinal classification methods and show the interpretability and controllability of CLOC in learning meaningful, ordered representations that align with clinical and practical needs.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning</title>
<link>https://arxiv.org/abs/2504.17815</link>
<guid>https://arxiv.org/abs/2504.17815</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, 3D representation, novel view synthesis, inpainting, blended content<br />
<br />
Summary: <br />
The article introduces a new method, called VISTA, for 3D Gaussian inpainting (3DGI), where masked objects in a scene are replaced seamlessly. The approach leverages visibility uncertainties of 3D points across input views to guide the inpainting process with complementary visual cues. A diffusion model based on learned scene concepts is used to fill masked objects in input images. The proposed VISTA framework combines visibility-uncertainty-guided 3DGI with scene conceptual learning to generate high-quality 3D models for artifact-free view synthesis. The method is demonstrated to outperform existing techniques on challenging datasets, including static scenes and dynamic inpainting scenarios like underwater scenes with moving fish as targets. <div>
arXiv:2504.17815v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D representation for novel view synthesis. This paper extends 3DGS capabilities to inpainting, where masked objects in a scene are replaced with new contents that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D Gaussian inpainting (3DGI) is challenging in effectively leveraging complementary visual and semantic cues from multiple input views, as occluded areas in one view may be visible in others. To address this, we propose a method that measures the visibility uncertainties of 3D points across different input views and uses them to guide 3DGI in utilizing complementary visual cues. We also employ uncertainties to learn a semantic concept of scene without the masked object and use a diffusion model to fill masked objects in input images based on the learned concept. Finally, we build a novel 3DGI framework, VISTA, by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl learning. VISTA generates high-quality 3DGS models capable of synthesizing artifact-free and naturally inpainted novel views. Furthermore, our approach extends to handling dynamic distractors arising from temporal object changes, enhancing its versatility in diverse scene reconstruction scenarios. We demonstrate the superior performance of our method over state-of-the-art techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10 diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset derived from UTB180, including fast-moving fish as inpainting targets.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subject-driven Video Generation via Disentangled Identity and Motion</title>
<link>https://arxiv.org/abs/2504.17816</link>
<guid>https://arxiv.org/abs/2504.17816</guid>
<content:encoded><![CDATA[
<div> Zero-shot learning, video customization, image-to-video training, subject-driven, temporal dynamics<br />
<br />
Summary: 
The article introduces a novel approach for training a subject-driven customized video generation model without the need for additional tuning. By decoupling subject-specific learning from temporal dynamics, the proposed method utilizes an image customization dataset to facilitate video customization. This approach involves injecting identity through the image dataset and preserving temporal modeling with a small set of unannotated videos. Random image token dropping and randomized initialization techniques are employed to address issues like copy-and-paste. Furthermore, stochastic switching during joint optimization helps mitigate catastrophic forgetting and enhances learning. The method showcases strong subject consistency and scalability, outperforming existing video customization models in zero-shot settings. The effectiveness of the framework is demonstrated through various experiments and comparisons with competitive models. <div>
arXiv:2504.17816v1 Announce Type: new 
Abstract: We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, which are computationally expensive and require extensive annotation. In contrast to the previous approach, we introduce the use of an image customization dataset directly on training video customization models, factorizing the video customization into two folds: (1) identity injection through image customization dataset and (2) temporal modeling preservation with a small set of unannotated videos through the image-to-video training method. Additionally, we employ random image token dropping with randomized image initialization during image-to-video fine-tuning to mitigate the copy-and-paste issue. To further enhance learning, we introduce stochastic switching during joint optimization of subject-specific and temporal features, mitigating catastrophic forgetting. Our method achieves strong subject consistency and scalability, outperforming existing video customization models in zero-shot settings, demonstrating the effectiveness of our framework.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Underwater Active Perception in Simulation</title>
<link>https://arxiv.org/abs/2504.17817</link>
<guid>https://arxiv.org/abs/2504.17817</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater vehicles, image quality prediction, turbidity, backscattering, active perception

Summary: 
- This article introduces a new approach for high-quality image acquisition of assets in varying water conditions using underwater vehicles.
- Consideration and assessment of water conditions, such as turbidity, are crucial for successful robotic operations in underwater inspections.
- An active perception framework involving a multi-layer perceptron (MLP) is used to predict image quality based on distance to target and artificial light intensity.
- A synthetic dataset with ten water types of different turbidity levels was generated using modified Blender software to simulate underwater light propagation properties.
- Validation in simulation showed significant improvements in visual coverage and image quality compared to traditional approaches.<br /><br />Summary: <div>
arXiv:2504.17817v1 Announce Type: new 
Abstract: When employing underwater vehicles for the autonomous inspection of assets, it is crucial to consider and assess the water conditions. Indeed, they have a significant impact on the visibility, which also affects robotic operations. Turbidity can jeopardise the whole mission as it may prevent correct visual documentation of the inspected structures. Previous works have introduced methods to adapt to turbidity and backscattering, however, they also include manoeuvring and setup constraints. We propose a simple yet efficient approach to enable high-quality image acquisition of assets in a broad range of water conditions. This active perception framework includes a multi-layer perceptron (MLP) trained to predict image quality given a distance to a target and artificial light intensity. We generated a large synthetic dataset including ten water types with different levels of turbidity and backscattering. For this, we modified the modelling software Blender to better account for the underwater light propagation properties. We validated the approach in simulation and showed significant improvements in visual coverage and quality of imagery compared to traditional approaches. The project code is available on our project page at https://roboticimaging.org/Projects/ActiveUW/.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension</title>
<link>https://arxiv.org/abs/2504.17821</link>
<guid>https://arxiv.org/abs/2504.17821</guid>
<content:encoded><![CDATA[
<div> benchmark, video comprehension, cultural diversity, multilingual, domain

Summary:<br /><br />
1) The paper introduces VideoVista-CulturalLingo, a benchmark for evaluating video comprehension of multi-modal AI systems across different cultures, languages, and domains. <br />
2) This benchmark includes diverse cultural contexts from China, North America, and Europe and presents questions in both Chinese and English. <br />
3) Results show that existing models perform better on Western-centric questions than Chinese-centric ones, especially related to Chinese history.<br />
4) Open-source models still struggle with temporal understanding, achieving a low score in Event Localization tasks.<br />
5) Mainstream models excel in general scientific questions, whereas open-source models perform poorly in mathematics.<br /> <div>
arXiv:2504.17821v1 Announce Type: new 
Abstract: Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multi-linguistics, with questions presented in Chinese and English-two of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw</title>
<link>https://arxiv.org/abs/2504.17822</link>
<guid>https://arxiv.org/abs/2504.17822</guid>
<content:encoded><![CDATA[
<div> deep learning, Arctic, permafrost, retrogressive thaw slumps, mapping

Summary: 
The study focuses on using a Cascade Mask R-CNN model with a multi-scale vision transformer backbone to detect Retrogressive Thaw Slumps (RTS) in the Arctic region. Two strategies were introduced to improve the model's performance: feature-level cross-modality attention fusion and pre-trained unimodal learning followed by multimodal fine-tuning. These strategies effectively integrate information from multiple modalities and optimize the model's predictive capabilities. Experimental results show that the proposed approach outperforms existing models, providing valuable insights into the efficient use of multimodal data for RTS mapping. This research contributes to a better understanding of permafrost landforms and their environmental impacts.
<br /><br /> <div>
arXiv:2504.17822v1 Announce Type: new 
Abstract: Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost landforms with significant environmental impacts. Mapping these RTS is crucial because their appearance serves as a clear indication of permafrost thaw. However, their small scale compared to other landform features, vague boundaries, and spatiotemporal variation pose significant challenges for accurate detection. In this paper, we employed a state-of-the-art deep learning model, the Cascade Mask R-CNN with a multi-scale vision transformer-based backbone, to delineate RTS features across the Arctic. Two new strategies were introduced to optimize multimodal learning and enhance the model's predictive performance: (1) a feature-level, residual cross-modality attention fusion strategy, which effectively integrates feature maps from multiple modalities to capture complementary information and improve the model's ability to understand complex patterns and relationships within the data; (2) pre-trained unimodal learning followed by multimodal fine-tuning to alleviate high computing demand while achieving strong model performance. Experimental results demonstrated that our approach outperformed existing models adopting data-level fusion, feature-level convolutional fusion, and various attention fusion strategies, providing valuable insights into the efficient utilization of multimodal data for RTS mapping. This research contributes to our understanding of permafrost landforms and their environmental implications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Prompting Image Restoration with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.17825</link>
<guid>https://arxiv.org/abs/2504.17825</guid>
<content:encoded><![CDATA[
<div> Keywords: image restoration, diffusion transformer, dual prompting, conditional information, image prior <br /> 
Summary: 
Dual Prompting Image Restoration (DPIR) introduces a novel approach to image restoration by combining a low-quality image conditioning branch with a dual prompting control branch. The first branch efficiently incorporates image priors into a diffusion transformer such as SD3. DPIR also incorporates a dual prompting module to provide additional visual cues, capturing both global context and local appearance, allowing for more effective restoration. The combination of textual and visual prompts as conditional controls enhances the quality of the restoration process. Experimental results demonstrate that DPIR outperforms state-of-the-art image restoration methods. <br /><br />Summary: <div>
arXiv:2504.17825v1 Announce Type: new 
Abstract: Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. In this paper, we introduce DPIR (Dual Prompting Image Restoration), a novel image restoration method that effectivly extracts conditional information of low-quality images from multiple perspectives. Specifically, DPIR consits of two branches: a low-quality image conditioning branch and a dual prompting control branch. The first branch utilizes a lightweight module to incorporate image priors into the DiT with high efficiency. More importantly, we believe that in image restoration, textual description alone cannot fully capture its rich visual characteristics. Therefore, a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance. The extracted global-local visual prompts as extra conditional control, alongside textual prompts to form dual prompts, greatly enhance the quality of the restoration. Extensive experimental results demonstrate that DPIR delivers superior image restoration performance.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model</title>
<link>https://arxiv.org/abs/2504.17826</link>
<guid>https://arxiv.org/abs/2504.17826</guid>
<content:encoded><![CDATA[
<div> multimodal, multitask, multiround, fashion assistant, vision-language models <br />
<br />
Summary: FashionM3 is a novel fashion assistant that leverages vision-language models to provide personalized recommendations and styling suggestions to users. It is built upon a VLM fine-tuned for fashion-specific tasks and offers capabilities such as personalized recommendation, alternative suggestion, product image generation, and virtual try-on simulation. The system is trained on the FashionRec dataset, which includes multimodal dialogue samples across various recommendation tasks. FashionM3 delivers contextually personalized suggestions through iterative multiround interactions, resulting in superior recommendation effectiveness. Quantitative and qualitative evaluations, as well as user studies, confirm the practical value of FashionM3 as a fashion assistant in the retail industry. <div>
arXiv:2504.17826v1 Announce Type: new 
Abstract: Fashion styling and personalized recommendations are pivotal in modern retail, contributing substantial economic value in the fashion industry. With the advent of vision-language models (VLM), new opportunities have emerged to enhance retailing through natural language and visual interactions. This work proposes FashionM3, a multimodal, multitask, and multiround fashion assistant, built upon a VLM fine-tuned for fashion-specific tasks. It helps users discover satisfying outfits by offering multiple capabilities including personalized recommendation, alternative suggestion, product image generation, and virtual try-on simulation. Fine-tuned on the novel FashionRec dataset, comprising 331,124 multimodal dialogue samples across basic, personalized, and alternative recommendation tasks, FashionM3 delivers contextually personalized suggestions with iterative refinement through multiround interactions. Quantitative and qualitative evaluations, alongside user studies, demonstrate FashionM3's superior performance in recommendation effectiveness and practical value as a fashion assistant.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VEU-Bench: Towards Comprehensive Understanding of Video Editing</title>
<link>https://arxiv.org/abs/2504.17828</link>
<guid>https://arxiv.org/abs/2504.17828</guid>
<content:encoded><![CDATA[
<div> annotation pipeline, ontology-based knowledge base, VEU-Bench dataset, Oscars, Vid-LLMs<br />
Summary:<br />
This paper introduces VEU-Bench, a benchmark for video editing understanding tasks that includes 19 fine-grained tasks categorized across recognition, reasoning, and judging stages. The benchmark covers various dimensions of video editing components, offering a comprehensive evaluation framework. An annotation pipeline integrated with an ontology-based knowledge base enhances automatic annotation of VEU data. The study finds that current Vid-LLMs face challenges in VEU tasks, leading to the development of the Oscars expert model fine-tuned on the VEU-Bench dataset. Oscars outperforms existing Vid-LLMs on VEU-Bench and shows comparable performance to commercial models like GPT-4o. Furthermore, the integration of VEU data improves the performance of Vid-LLMs on general video understanding benchmarks, contributing to an average performance increase of 8.3% across reasoning tasks.<br /><br />Summary: <div>
arXiv:2504.17828v1 Announce Type: new 
Abstract: Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions. Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses 19 fine-grained tasks across three stages: recognition, reasoning, and judging. To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base. Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice. To alleviate this issue, we develop Oscars, a VEU expert model fine-tuned on the curated VEU-Bench dataset. It outperforms existing open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves performance comparable to commercial models like GPT-4o. We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of 8.3% across nine reasoning tasks.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing</title>
<link>https://arxiv.org/abs/2504.17829</link>
<guid>https://arxiv.org/abs/2504.17829</guid>
<content:encoded><![CDATA[
<div> keyword: single-image dehazing, adversarial noise, transformer, fine-tuning strategies, remote sensing<br />
Summary:<br />
The study addresses the vulnerability of state-of-the-art image-to-image dehazing transformers to adversarial noise, which can significantly impair their performance with even minor perturbations. The researchers demonstrate that a single-pixel change can result in a decrease of up to 2.8 dB in Peak Signal-to-Noise Ratio (PSNR). To enhance the robustness of pre-trained transformers, two lightweight fine-tuning strategies are proposed, maintaining comparable clean performance while significantly increasing protection against adversarial data. These strategies are shown to be effective in remote sensing applications, particularly for out-of-distribution data. The source code for the adversarial fine-tuning and attack algorithms is available on github.com/Vladimirescu/RobustDehazing. <br /><br />Summary: <div>
arXiv:2504.17829v1 Announce Type: new 
Abstract: Single-image dehazing is an important topic in remote sensing applications, enhancing the quality of acquired images and increasing object detection precision. However, the reliability of such structures has not been sufficiently analyzed, which poses them to the risk of imperceptible perturbations that can significantly hinder their performance. In this work, we show that state-of-the-art image-to-image dehazing transformers are susceptible to adversarial noise, with even 1 pixel change being able to decrease the PSNR by as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies aimed at increasing the robustness of pre-trained transformers. Our methods results in comparable clean performance, while significantly increasing the protection against adversarial data. We further present their applicability in two remote sensing scenarios, showcasing their robust behavior for out-of-distribution data. The source code for adversarial fine-tuning and attack algorithms can be found at github.com/Vladimirescu/RobustDehazing.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Sequence Compression for Efficient Multimodal Computing</title>
<link>https://arxiv.org/abs/2504.17892</link>
<guid>https://arxiv.org/abs/2504.17892</guid>
<content:encoded><![CDATA[
<div> vision encoders, cross-modal reasoning, multimodal models, token selection, compression 

Summary:
The article discusses the inefficiencies in current vision encoders used in Large Multimodal Models (LMMs) and introduces an adaptive compression method for multimodal data. Through benchmarking and qualitative analysis, the study explores various visual token selection and merging approaches. The research finds that cluster-level token aggregation outperforms existing methods such as merging at the vision encoder level and attention-based approaches. Additionally, the study exposes redundancies in current vision encoders and uncovers intriguing trends in visual token selection principles through cross-modal attention visualizations. This work signifies a step towards more efficient encoding and processing of high-dimensional data, ultimately paving the way for scalable and sustainable multimodal systems. 

<br /><br />Summary: <div>
arXiv:2504.17892v1 Announce Type: new 
Abstract: The exponential growth of Large Multimodal Models (LMMs) has driven advancements in cross-modal reasoning but at significant computational costs. In this work, we focus on visual language models. We highlight the redundancy and inefficiency in current vision encoders, and seek to construct an adaptive compression method for multimodal data. In this work, we characterize a panoply of visual token selection and merging approaches through both benchmarking and qualitative analysis. In particular, we demonstrate that simple cluster-level token aggregation outperforms prior state-of-the-art works in token selection and merging, including merging at the vision encoder level and attention-based approaches. We underline the redundancy in current vision encoders, and shed light on several puzzling trends regarding principles of visual token selection through cross-modal attention visualizations. This work is a first effort towards more effective encoding and processing of high-dimensional data, and paves the way for more scalable and sustainable multimodal systems.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing</title>
<link>https://arxiv.org/abs/2504.17894</link>
<guid>https://arxiv.org/abs/2504.17894</guid>
<content:encoded><![CDATA[
<div> DCT coefficients, JPEG compression, image security, adversarial perturbations, diffusion models <br />
Summary: <br />
Advancements in diffusion models have made image editing easy through text prompts, but also raised concerns about image security. To address this issue, a novel optimization approach is proposed to introduce adversarial perturbations directly in the frequency domain by modifying the DCT coefficients of the input image. This method leverages the JPEG pipeline to generate adversarial images that effectively prevent malicious editing while minimizing visible artifacts. Experimental results across various tasks and datasets show that the approach introduces fewer visual artifacts, maintains similar levels of edit protection, and is robust to noise purification techniques like JPEG compression within a feasible pixel budget. <div>
arXiv:2504.17894v1 Announce Type: new 
Abstract: Advancements in diffusion models have enabled effortless image editing via text prompts, raising concerns about image security. Attackers with access to user images can exploit these tools for malicious edits. Recent defenses attempt to protect images by adding a limited noise in the pixel space to disrupt the functioning of diffusion-based editing models. However, the adversarial noise added by previous methods is easily noticeable to the human eye. Moreover, most of these methods are not robust to purification techniques like JPEG compression under a feasible pixel budget. We propose a novel optimization approach that introduces adversarial perturbations directly in the frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients of the input image. By leveraging the JPEG pipeline, our method generates adversarial images that effectively prevent malicious image editing. Extensive experiments across a variety of tasks and datasets demonstrate that our approach introduces fewer visual artifacts while maintaining similar levels of edit protection and robustness to noise purification techniques.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMU: Context Augmentation for Meme Understanding</title>
<link>https://arxiv.org/abs/2504.17902</link>
<guid>https://arxiv.org/abs/2504.17902</guid>
<content:encoded><![CDATA[
<div> framework, CAMU, social media memes, hate detection, vision-language models, multimodal understanding <br />
<br />
Summary: 
The article introduces CAMU, a novel framework that utilizes large vision-language models to enhance descriptive captions and improve hate detection in social media memes. By fine-tuning CLIP's text encoder, CAMU achieves high accuracy and F1-score on the Hateful Memes dataset, matching the current state-of-the-art performance. The framework also excels at identifying offensive memes on the MultiOFF dataset, showcasing its adaptability. The study emphasizes the importance of robust visual grounding and nuanced text representations for accurate hate and offense detection. CAMU's efficient approach offers practical advantages in real-world scenarios while maintaining high performance. The authors plan to release CAMU along with the resulting models to facilitate further research in this domain. <br /><br /> <div>
arXiv:2504.17902v1 Announce Type: new 
Abstract: Social media memes are a challenging domain for hate detection because they intertwine visual and textual cues into culturally nuanced messages. We introduce a novel framework, CAMU, which leverages large vision-language models to generate more descriptive captions, a caption-scoring neural network to emphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's text encoder for an improved multimodal understanding of memes. Experiments on publicly available hateful meme datasets show that simple projection layer fine-tuning yields modest gains, whereas selectively tuning deeper text encoder layers significantly boosts performance on all evaluation metrics. Moreover, our approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful Memes dataset, at par with the existing SoTA framework while being much more efficient, offering practical advantages in real-world scenarios that rely on fixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the MultiOFF dataset for offensive meme identification, demonstrating its generalisability. Additional analyses on benign confounders reveal that robust visual grounding and nuanced text representations are crucial for reliable hate and offence detection. We will publicly release CAMU along with the resultant models for further research.
  Disclaimer: This paper includes references to potentially disturbing, hateful, or offensive content due to the nature of the task.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked strategies for images with small objects</title>
<link>https://arxiv.org/abs/2504.17935</link>
<guid>https://arxiv.org/abs/2504.17935</guid>
<content:encoded><![CDATA[
<div> deep learning; hematology analytics; small blood components; self-supervised models; semantic segmentation<br />
Summary:<br />
The study explores the challenges in hematology analytics for detecting and classifying small blood components. It suggests using self-supervised models like masked autoencoders (MAE) to learn representations and then apply them for downstream tasks. By experimenting with different mask ratios and patch sizes, the researchers found that smaller ratios and sizes improved image reconstruction using MAE. They applied the learned encoder weights to train a U-Net Transformer for semantic segmentation, showcasing improved results for smaller-sized blood components with pre-training. Overall, the proposed method offers an efficient and effective approach for segmenting and classifying small objects in hematology analytics. <br /> <div>
arXiv:2504.17935v1 Announce Type: new 
Abstract: The hematology analytics used for detection and classification of small blood components is a significant challenge. In particular, when objects exists as small pixel-sized entities in a large context of similar objects. Deep learning approaches using supervised models with pre-trained weights, such as residual networks and vision transformers have demonstrated success for many applications. Unfortunately, when applied to images outside the domain of learned representations, these methods often result with less than acceptable performance. A strategy to overcome this can be achieved by using self-supervised models, where representations are learned and weights are then applied for downstream applications. Recently, masked autoencoders have proven to be effective to obtain representations that captures global context information. By masking regions of an image and having the model learn to reconstruct both the masked and non-masked regions, weights can be used for various applications. However, if the sizes of the objects in images are less than the size of the mask, the global context information is lost, making it almost impossible to reconstruct the image. In this study, we investigated the effect of mask ratios and patch sizes for blood components using a MAE to obtain learned ViT encoder representations. We then applied the encoder weights to train a U-Net Transformer for semantic segmentation to obtain both local and global contextual information. Our experimental results demonstrates that both smaller mask ratios and patch sizes improve the reconstruction of images using a MAE. We also show the results of semantic segmentation with and without pre-trained weights, where smaller-sized blood components benefited with pre-training. Overall, our proposed method offers an efficient and effective strategy for the segmentation and classification of small objects.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval</title>
<link>https://arxiv.org/abs/2504.17990</link>
<guid>https://arxiv.org/abs/2504.17990</guid>
<content:encoded><![CDATA[
<div> Keywords: Composed Image Retrieval, Zero-shot learning, multimodal task, visual semantic injection module, soft text alignment objective 

Summary: 
- Composed Image Retrieval (CIR) is a challenging multimodal task that retrieves a target image based on a reference image and modification text.
- Zero-shot (ZS) CIR has become popular due to the high cost of annotating datasets.
- Existing projection-based methods for ZS CIR face challenges such as token representation capacity, discrepancies between training and inference, and reliance on synthetic data.
- A two-stage framework is proposed, enhancing image-to-token learning in the first stage and optimizing text encoder in the second stage with synthetic triplet data.
- The approach achieves superior performance on public datasets by capturing richer image information, combining pseudo-word tokens with modification text, and being compatible with various data qualities. 

<br /><br />Summary: <div>
arXiv:2504.17990v1 Announce Type: new 
Abstract: Composed Image Retrieval (CIR) is a challenging multimodal task that retrieves a target image based on a reference image and accompanying modification text. Due to the high cost of annotating CIR triplet datasets, zero-shot (ZS) CIR has gained traction as a promising alternative. Existing studies mainly focus on projection-based methods, which map an image to a single pseudo-word token. However, these methods face three critical challenges: (1) insufficient pseudo-word token representation capacity, (2) discrepancies between training and inference phases, and (3) reliance on large-scale synthetic data. To address these issues, we propose a two-stage framework where the training is accomplished from mapping to composing. In the first stage, we enhance image-to-pseudo-word token learning by introducing a visual semantic injection module and a soft text alignment objective, enabling the token to capture richer and fine-grained image information. In the second stage, we optimize the text encoder using a small amount of synthetic triplet data, enabling it to effectively extract compositional semantics by combining pseudo-word tokens with modification text for accurate target image retrieval. The strong visual-to-pseudo mapping established in the first stage provides a solid foundation for the second stage, making our approach compatible with both high- and low-quality synthetic data, and capable of achieving significant performance gains with only a small amount of synthetic data. Extensive experiments were conducted on three public datasets, achieving superior performance compared to existing approaches.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation</title>
<link>https://arxiv.org/abs/2504.17991</link>
<guid>https://arxiv.org/abs/2504.17991</guid>
<content:encoded><![CDATA[
<div> Keywords: Image-goal navigation, perception-action policy, spatial relationships, fine-grained cross-correlation, direction-aware correlation

Summary: 
RSRNav proposes a new method for image-goal navigation that addresses challenges faced by current approaches. It reasons spatial relationships between the goal and current observations to provide more accurate navigation guidance. By constructing correlations between the goal and observations, the method improves directional information for more efficient action prediction. The correlations are refined using fine-grained cross-correlation and direction-aware correlation techniques. RSRNav outperforms existing methods on benchmark datasets, especially in scenarios where the goal is matched with user preferences. This highlights its potential for real-world applications and demonstrates superior navigation performance. Overall, RSRNav offers a simple yet effective solution to enhancing perception-action policies for image-goal navigation tasks.<br /><br />Summary: <div>
arXiv:2504.17991v1 Announce Type: new 
Abstract: Recent image-goal navigation (ImageNav) methods learn a perception-action policy by separately capturing semantic features of the goal and egocentric images, then passing them to a policy network. However, challenges remain: (1) Semantic features often fail to provide accurate directional information, leading to superfluous actions, and (2) performance drops significantly when viewpoint inconsistencies arise between training and application. To address these challenges, we propose RSRNav, a simple yet effective method that reasons spatial relationships between the goal and current observations as navigation guidance. Specifically, we model the spatial relationship by constructing correlations between the goal and current observations, which are then passed to the policy network for action prediction. These correlations are progressively refined using fine-grained cross-correlation and direction-aware correlation for more precise navigation. Extensive evaluation of RSRNav on three benchmark datasets demonstrates superior navigation performance, particularly in the "user-matched goal" setting, highlighting its potential for real-world applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning</title>
<link>https://arxiv.org/abs/2504.17996</link>
<guid>https://arxiv.org/abs/2504.17996</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, token pruning, Tsallis entropy, semantic segmentation, computational efficiency

Summary:
The study introduces LVTP, a progressive token pruning framework for Vision Transformers, enhancing semantic segmentation on resource-constrained devices. LVTP leverages multi-scale Tsallis entropy and low-level visual features to guide pruning, preserving critical edge information while reducing computational cost. A dynamic scoring mechanism using multi-scale Tsallis entropy weighting ensures more precise performance than traditional methods. The framework requires no architectural changes or additional training, making it easily integratable. Evaluations across multiple datasets demonstrate 20%-45% computational reductions with minimal performance loss, particularly excelling in complex edge regions. LVTP outperforms existing methods by effectively balancing cost and accuracy in semantic segmentation tasks. <br /><br />Summary: <div>
arXiv:2504.17996v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) excel in semantic segmentation but demand significant computation, posing challenges for deployment on resource-constrained devices. Existing token pruning methods often overlook fundamental visual data characteristics. This study introduces 'LVTP', a progressive token pruning framework guided by multi-scale Tsallis entropy and low-level visual features with twice clustering. It integrates high-level semantics and basic visual attributes for precise segmentation. A novel dynamic scoring mechanism using multi-scale Tsallis entropy weighting overcomes limitations of traditional single-parameter entropy. The framework also incorporates low-level feature analysis to preserve critical edge information while optimizing computational cost. As a plug-and-play module, it requires no architectural changes or additional training. Evaluations across multiple datasets show 20%-45% computational reductions with negligible performance loss, outperforming existing methods in balancing cost and accuracy, especially in complex edge regions.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Client-tailored Adapter for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2504.18020</link>
<guid>https://arxiv.org/abs/2504.18020</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Medical Image Segmentation, X-ray Images, Client-tailored Adaptive Segmentation, Federated Client-tailored Adapter <br />
Summary: 
The paper introduces a Federated Client-tailored Adapter (FCA) framework for medical image segmentation in X-ray images, addressing the limitations of centralized learning methods in distributed data scenarios. The FCA framework utilizes federated learning to achieve stable and client-tailored adaptive segmentation without sharing sensitive local data. By incorporating universal knowledge from medical foundation models, the federated adapter enhances the training process stability. Two client-tailored federated updating strategies are developed to adaptively update common and individual components of the adapter, improving the performance of the segmentation models for each client. Extensive experiments on large-scale datasets demonstrate the effectiveness and superiority of the FCA framework for federated medical segmentation. <br /><br /> <div>
arXiv:2504.18020v1 Announce Type: new 
Abstract: Medical image segmentation in X-ray images is beneficial for computer-aided diagnosis and lesion localization. Existing methods mainly fall into a centralized learning paradigm, which is inapplicable in the practical medical scenario that only has access to distributed data islands. Federated Learning has the potential to offer a distributed solution but struggles with heavy training instability due to client-wise domain heterogeneity (including distribution diversity and class imbalance). In this paper, we propose a novel Federated Client-tailored Adapter (FCA) framework for medical image segmentation, which achieves stable and client-tailored adaptive segmentation without sharing sensitive local data. Specifically, the federated adapter stirs universal knowledge in off-the-shelf medical foundation models to stabilize the federated training process. In addition, we develop two client-tailored federated updating strategies that adaptively decompose the adapter into common and individual components, then globally and independently update the parameter groups associated with common client-invariant and individual client-specific units, respectively. They further stabilize the heterogeneous federated learning process and realize optimal client-tailored instead of sub-optimal global-compromised segmentation models. Extensive experiments on three large-scale datasets demonstrate the effectiveness and superiority of the proposed FCA framework for federated medical segmentation.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShapeSpeak: Body Shape-Aware Textual Alignment for Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2504.18025</link>
<guid>https://arxiv.org/abs/2504.18025</guid>
<content:encoded><![CDATA[
<div> Keywords: Visible-Infrared Person Re-identification, Body Shape-aware Textual Alignment, Body Shape Textual Alignment, Text-Visual Consistency Regularizer, Shape-aware Representation Learning

Summary: 

The paper introduces a new framework called Body Shape-aware Textual Alignment (BSaTa) for Visible-Infrared Person Re-identification (VIReID). This framework explicitly models and utilizes body shape information to improve cross-modal matching between visible and infrared pedestrian images. The proposed Body Shape Textual Alignment (BSTA) module extracts body shape information using a human parsing model and aligns it with structured text representations. A Text-Visual Consistency Regularizer (TVCR) ensures alignment between body shape textual representations and visual body shape features. Additionally, a Shape-aware Representation Learning (SRL) mechanism combines multi-text supervision and distribution consistency constraints to guide the visual encoder in learning modality-invariant and discriminative identity features. Experimental results on SYSU-MM01 and RegDB datasets demonstrate the superior performance of the proposed method in VIReID tasks.  <div>
arXiv:2504.18025v1 Announce Type: new 
Abstract: Visible-Infrared Person Re-identification (VIReID) aims to match visible and infrared pedestrian images, but the modality differences and the complexity of identity features make it challenging. Existing methods rely solely on identity label supervision, which makes it difficult to fully extract high-level semantic information. Recently, vision-language pre-trained models have been introduced to VIReID, enhancing semantic information modeling by generating textual descriptions. However, such methods do not explicitly model body shape features, which are crucial for cross-modal matching. To address this, we propose an effective Body Shape-aware Textual Alignment (BSaTa) framework that explicitly models and utilizes body shape information to improve VIReID performance. Specifically, we design a Body Shape Textual Alignment (BSTA) module that extracts body shape information using a human parsing model and converts it into structured text representations via CLIP. We also design a Text-Visual Consistency Regularizer (TVCR) to ensure alignment between body shape textual representations and visual body shape features. Furthermore, we introduce a Shape-aware Representation Learning (SRL) mechanism that combines Multi-text Supervision and Distribution Consistency Constraints to guide the visual encoder to learn modality-invariant and discriminative identity features, thus enhancing modality invariance. Experimental results demonstrate that our method achieves superior performance on the SYSU-MM01 and RegDB datasets, validating its effectiveness.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large Vision-Language Model based Environment Perception System for Visually Impaired People</title>
<link>https://arxiv.org/abs/2504.18027</link>
<guid>https://arxiv.org/abs/2504.18027</guid>
<content:encoded><![CDATA[
<div> Keywords: visually impaired, environment perception, large vision-language model, segmentation model, wearable device

Summary: 
Visually impaired individuals face challenges in perceiving their environment, limiting their personal and social activities. This paper presents a Large Vision-Language Model (LVLM) based system designed to help visually impaired individuals understand their surroundings. The system captures the current scene using a wearable device and provides analysis results to the user. By activating LVLM output, users can receive a global scene description, access object categories through tapping or swiping, and obtain detailed object descriptions through double-tapping. The system incorporates segmentation results into LVLM input to enhance accuracy and reduce hallucination. Experimental results demonstrate improved scene description accuracy compared to existing models and effective environment perception for visually impaired individuals. <br /><br />Summary: <div>
arXiv:2504.18027v1 Announce Type: new 
Abstract: It is a challenging task for visually impaired people to perceive their surrounding environment due to the complexity of the natural scenes. Their personal and social activities are thus highly limited. This paper introduces a Large Vision-Language Model(LVLM) based environment perception system which helps them to better understand the surrounding environment, by capturing the current scene they face with a wearable device, and then letting them retrieve the analysis results through the device. The visually impaired people could acquire a global description of the scene by long pressing the screen to activate the LVLM output, retrieve the categories of the objects in the scene resulting from a segmentation model by tapping or swiping the screen, and get a detailed description of the objects they are interested in by double-tapping the screen. To help visually impaired people more accurately perceive the world, this paper proposes incorporating the segmentation result of the RGB image as external knowledge into the input of LVLM to reduce the LVLM's hallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the system could provide a more accurate description of the scene compared to Qwen-VL-Chat, exploratory experiments show that the system helps visually impaired people to perceive the surrounding environment effectively.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models</title>
<link>https://arxiv.org/abs/2504.18032</link>
<guid>https://arxiv.org/abs/2504.18032</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-image diffusion models, privacy, originality, legal complications, PRSS

Summary:
PRSS is a novel method introduced to improve text-to-image diffusion models by addressing concerns related to privacy and originality. It enhances privacy by integrating prompt re-anchoring (PR) and improves utility by incorporating semantic prompt search (SS). The method aims to strike a balance between privacy and utility, which is a critical issue in generating images aligned with user prompts. By refining the classifier-free guidance approach, PRSS offers a new state-of-the-art solution to the privacy-utility trade-off. Extensive experiments across various privacy levels demonstrate the effectiveness of PRSS in enhancing the model's performance while also protecting privacy and ensuring the originality of generated images. This research contributes to the ongoing exploration of ways to improve the capabilities of text-to-image diffusion models while responsibly addressing privacy concerns and legal implications. 

<br /><br />Summary: PRSS addresses privacy concerns in text-to-image diffusion models by integrating prompt re-anchoring and semantic prompt search, improving the privacy-utility trade-off and establishing a new state-of-the-art solution. <div>
arXiv:2504.18032v1 Announce Type: new 
Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cabbage: A Differential Growth Framework for Open Surfaces</title>
<link>https://arxiv.org/abs/2504.18040</link>
<guid>https://arxiv.org/abs/2504.18040</guid>
<content:encoded><![CDATA[
<div> differential growth, buckling behavior, 3D surfaces, mesh generation, collision detection
Summary: 
Cabbage is a novel framework for modeling buckling behavior in 3D open surfaces like flower petals, producing high-quality triangular meshes without self-intersections. It utilizes edge subdivision and shell forces to generate buckling effects over time, along with feature-aware smoothing and remeshing to maintain mesh quality. A corrective collision method prevents self-collision, even in tight spaces. The framework includes Cabbage-Collision for an approximate alternative and CAD-ready surface generation. Cabbage is the first open-source tool of its kind, surpassing state-of-the-art methods in morphological expressiveness, mesh quality, and stability, capable of generating complex patterns over hundreds of simulation steps. It serves as a valuable resource for computational modeling, digital fabrication, education, and provides annotated data for geometry processing and shape analysis. <div>
arXiv:2504.18040v1 Announce Type: new 
Abstract: We propose Cabbage, a differential growth framework to model buckling behavior in 3D open surfaces found in nature-like the curling of flower petals. Cabbage creates high-quality triangular meshes free of self-intersection. Cabbage-Shell is driven by edge subdivision which differentially increases discretization resolution. Shell forces expands the surface, generating buckling over time. Feature-aware smoothing and remeshing ensures mesh quality. Corrective collision effectively prevents self-collision even in tight spaces. We additionally provide Cabbage-Collision, and approximate alternative, followed by CAD-ready surface generation. Cabbage is the first open-source effort with this calibre and robustness, outperforming SOTA methods in its morphological expressiveness, mesh quality, and stably generates large, complex patterns over hundreds of simulation steps. It is a source not only of computational modeling, digital fabrication, education, but also high-quality, annotated data for geometry processing and shape analysis.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification</title>
<link>https://arxiv.org/abs/2504.18046</link>
<guid>https://arxiv.org/abs/2504.18046</guid>
<content:encoded><![CDATA[
<div> Siamese network, deep learning, binocular fundus image classification, multi-scale context-aware module, dual-modal feature fusion<br />
<br />
Summary: 
The article introduces DMS-Net, a dual-modal multi-scale Siamese network designed to address the limitations of traditional diagnosis methods for ophthalmic diseases that fail to account for binocular pathological correlations. The framework utilizes weight-shared Siamese ResNet-152 backbones to extract deep semantic features from paired fundus images. To overcome challenges like lesion boundary ambiguity, a Multi-Scale Context-Aware Module (MSCAM) is introduced for multi-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion (DMFF) module enhances cross-modal interaction through spatial-semantic recalibration and bidirectional attention mechanisms. Evaluated on the ODIR-5K dataset, DMS-Net achieves state-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8% Cohen's kappa, showcasing superior capability in detecting symmetric pathologies and revolutionizing clinical decision-making for ocular diseases. <br /> <div>
arXiv:2504.18046v1 Announce Type: new 
Abstract: Ophthalmic diseases pose a significant global health challenge, yet traditional diagnosis methods and existing single-eye deep learning approaches often fail to account for binocular pathological correlations. To address this, we propose DMS-Net, a dual-modal multi-scale Siamese network for binocular fundus image classification. Our framework leverages weight-shared Siamese ResNet-152 backbones to extract deep semantic features from paired fundus images. To tackle challenges such as lesion boundary ambiguity and scattered pathological distributions, we introduce a Multi-Scale Context-Aware Module (MSCAM) that integrates adaptive pooling and attention mechanisms for multi-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion (DMFF) module enhances cross-modal interaction through spatial-semantic recalibration and bidirectional attention, effectively combining global context and local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves state-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8% Cohen's kappa, demonstrating superior capability in detecting symmetric pathologies and advancing clinical decision-making for ocular diseases.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images</title>
<link>https://arxiv.org/abs/2504.18049</link>
<guid>https://arxiv.org/abs/2504.18049</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, deep learning, self-supervised learning, Vision Transformers, CNNs

Summary: 
In the realm of medical imaging, deep learning techniques, particularly convolutional neural networks (CNNs), have revolutionized the analysis of images. However, the requirement for extensive labeled data poses challenges, as label acquisition in medical imaging is costly and complex. To address this, the study explores the use of nn-MobileNet, a lightweight CNN framework, to implement a BERT-style self-supervised learning approach on unlabeled retinal fundus images from the UK Biobank. The pre-training of the network significantly enhances performance in downstream tasks like Alzheimer's disease, Parkinson's disease, and various retinal disease identification. This method combines the benefits of CNNs with advanced self-supervised learning to handle large-scale unlabeled datasets effectively, showcasing the potential of CNNs in scenarios with limited labeled data. 

Summary: 

Summary: <br />
Keywords: medical imaging, deep learning, self-supervised learning, Vision Transformers, CNNs <br />
In the realm of medical imaging, deep learning techniques, particularly convolutional neural networks (CNNs), have revolutionized the analysis of images. However, the requirement for extensive labeled data poses challenges, as label acquisition in medical imaging is costly and complex. To address this, the study explores the use of nn-MobileNet, a lightweight CNN framework, to implement a BERT-style self-supervised learning approach on unlabeled retinal fundus images from the UK Biobank. The pre-training of the network significantly enhances performance in downstream tasks like Alzheimer's disease, Parkinson's disease, and various retinal disease identification. This method combines the benefits of CNNs with advanced self-supervised learning to handle large-scale unlabeled datasets effectively, showcasing the potential of CNNs in scenarios with limited labeled data. <div>
arXiv:2504.18049v1 Announce Type: new 
Abstract: In the field of medical imaging, the advent of deep learning, especially the application of convolutional neural networks (CNNs) has revolutionized the analysis and interpretation of medical images. Nevertheless, deep learning methods usually rely on large amounts of labeled data. In medical imaging research, the acquisition of high-quality labels is both expensive and difficult. The introduction of Vision Transformers (ViT) and self-supervised learning provides a pre-training strategy that utilizes abundant unlabeled data, effectively alleviating the label acquisition challenge while broadening the breadth of data utilization. However, ViT's high computational density and substantial demand for computing power, coupled with the lack of localization characteristics of its operations on image patches, limit its efficiency and applicability in many application scenarios. In this study, we employ nn-MobileNet, a lightweight CNN framework, to implement a BERT-style self-supervised learning approach. We pre-train the network on the unlabeled retinal fundus images from the UK Biobank to improve downstream application performance. We validate the results of the pre-trained model on Alzheimer's disease (AD), Parkinson's disease (PD), and various retinal diseases identification. The results show that our approach can significantly improve performance in the downstream tasks. In summary, this study combines the benefits of CNNs with the capabilities of advanced self-supervised learning in handling large-scale unlabeled data, demonstrating the potential of CNNs in the presence of label scarcity.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POET: Prompt Offset Tuning for Continual Human Action Adaptation</title>
<link>https://arxiv.org/abs/2504.18059</link>
<guid>https://arxiv.org/abs/2504.18059</guid>
<content:encoded><![CDATA[
<div> Personalize, action recognition, extended reality, privacy-aware, prompt tuning<br />
Summary:<br /> 
The article introduces a new approach called POET for privacy-aware few-shot continual action recognition in extended reality (XR) environments. Users and developers can personalize their experience by adding new action classes to their device models in a low-shot and efficient manner without storing or replaying sensitive training data. POET utilizes prompt tuning on a lightweight backbone pretrained exclusively on base class data, employing a novel spatio-temporal learnable prompt offset tuning approach. This approach is applied to Graph Neural Networks and outperforms existing benchmarks on new benchmarks for human action recognition: NTU RGB+D dataset for activity recognition and SHREC-2017 dataset for hand gesture recognition. The proposed method enables users to continually expand the range of recognized actions on immersive computing devices. Source code for POET is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2504.18059v1 Announce Type: new 
Abstract: As extended reality (XR) is redefining how users interact with computing devices, research in human action recognition is gaining prominence. Typically, models deployed on immersive computing devices are static and limited to their default set of classes. The goal of our research is to provide users and developers with the capability to personalize their experience by adding new action classes to their device models continually. Importantly, a user should be able to add new classes in a low-shot and efficient manner, while this process should not require storing or replaying any of user's sensitive training data. We formalize this problem as privacy-aware few-shot continual action recognition. Towards this end, we propose POET: Prompt-Offset Tuning. While existing prompt tuning approaches have shown great promise for continual learning of image, text, and video modalities; they demand access to extensively pretrained transformers. Breaking away from this assumption, POET demonstrates the efficacy of prompt tuning a significantly lightweight backbone, pretrained exclusively on the base class data. We propose a novel spatio-temporal learnable prompt offset tuning approach, and are the first to apply such prompt tuning to Graph Neural Networks. We contribute two new benchmarks for our new problem setting in human action recognition: (i) NTU RGB+D dataset for activity recognition, and (ii) SHREC-2017 dataset for hand gesture recognition. We find that POET consistently outperforms comprehensive benchmarks. Source code at https://github.com/humansensinglab/POET-continual-action-recognition.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S3MOT: Monocular 3D Object Tracking with Selective State Space Model</title>
<link>https://arxiv.org/abs/2504.18068</link>
<guid>https://arxiv.org/abs/2504.18068</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-object tracking, monocular setup, Hungarian State Space Model, Fully Convolutional One-stage Embedding, VeloSSM<br />
<br />
Summary: <br />
Accurate multi-object tracking (MOT) in 3D space is crucial for robotics and computer vision applications. This work introduces three innovative techniques for enhancing monocular 3D MOT. Firstly, the Hungarian State Space Model (HSSM) compresses contextual tracking cues across multiple paths, enabling efficient assignment decisions. Secondly, Fully Convolutional One-stage Embedding (FCOE) improves object re-identification accuracy by using dense feature maps for contrastive learning. Lastly, VeloSSM enhances 6-DoF pose estimation by capturing motion dynamics with an encoder-decoder architecture. Experiments on the KITTI benchmark show the effectiveness of the proposed method, achieving a new state-of-the-art performance of 76.86 HOTA at 31 FPS. The approach outperforms previous methods significantly, demonstrating robustness and efficiency in monocular 3D MOT tasks. The code and models are publicly available. <div>
arXiv:2504.18068v1 Announce Type: new 
Abstract: Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we present three innovative techniques to enhance the fusion and exploitation of heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State Space Model (HSSM), a novel data association mechanism that compresses contextual tracking cues across multiple paths, enabling efficient and comprehensive assignment decisions with linear complexity. HSSM features a global receptive field and dynamic weights, in contrast to traditional linear assignment algorithms that rely on hand-crafted association costs. (2) We propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI pooling by directly using dense feature maps for contrastive learning, thus improving object re-identification accuracy under challenging conditions such as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation through VeloSSM, an encoder-decoder architecture that models temporal dependencies in velocity to capture motion dynamics, overcoming the limitations of frame-based 3D inference. Experiments on the KITTI public test benchmark demonstrate the effectiveness of our method, achieving a new state-of-the-art performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness and efficiency for monocular 3D MOT tasks. The code and models are available at https://github.com/bytepioneerX/s3mot.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation</title>
<link>https://arxiv.org/abs/2504.18087</link>
<guid>https://arxiv.org/abs/2504.18087</guid>
<content:encoded><![CDATA[
<div> disentangled emotion embedder, correlation-enhanced emotion conditioning module, Emotion Banks, emotion discrimination objective, diffusion process

Summary:
The article introduces a novel framework called DICE-Talk for emotional talking head generation. It addresses limitations in existing methods by disentangling identity from emotion and enhancing emotion correlations. The framework includes a disentangled emotion embedder that combines audio-visual emotional cues, a correlation-enhanced emotion conditioning module with learnable Emotion Banks, and an emotion discrimination objective for affective consistency during the diffusion process. Experimental results on MEAD and HDTF datasets show superior emotion accuracy and competitive lip-sync performance compared to state-of-the-art approaches. Qualitative results and user studies confirm the method's ability to generate identity-preserving portraits with rich, correlated emotional expressions that can adapt to unseen identities. <div>
arXiv:2504.18087v1 Announce Type: new 
Abstract: Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audio's inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose a novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop a disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identity-agnostic Gaussian distributions. Second, we introduce a correlation-enhanced emotion conditioning module with learnable Emotion Banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our method's superiority, outperforming state-of-the-art approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our method's ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Study on Real-Time Road Surface Reconstruction Using Stereo Vision</title>
<link>https://arxiv.org/abs/2504.18112</link>
<guid>https://arxiv.org/abs/2504.18112</guid>
<content:encoded><![CDATA[
<div> road surface reconstruction, autonomous driving, efficiency, accuracy, real-time

Summary:
- The paper enhances the RoadBEV framework for real-time inference on edge devices by optimizing efficiency and accuracy.
- Isomorphic Global Structured Pruning is applied to the stereo feature extraction backbone to reduce network complexity while maintaining performance.
- The head network is redesigned with an optimized hourglass structure, dynamic attention heads, reduced feature channels, mixed precision inference, and efficient probability volume computation.
- The approach improves inference speed and achieves lower reconstruction error, making it suitable for real-time road surface reconstruction in autonomous driving. 

<br /><br />Summary: <div>
arXiv:2504.18112v1 Announce Type: new 
Abstract: Road surface reconstruction plays a crucial role in autonomous driving, providing essential information for safe and smooth navigation. This paper enhances the RoadBEV [1] framework for real-time inference on edge devices by optimizing both efficiency and accuracy. To achieve this, we proposed to apply Isomorphic Global Structured Pruning to the stereo feature extraction backbone, reducing network complexity while maintaining performance. Additionally, the head network is redesigned with an optimized hourglass structure, dynamic attention heads, reduced feature channels, mixed precision inference, and efficient probability volume computation. Our approach improves inference speed while achieving lower reconstruction error, making it well-suited for real-time road surface reconstruction in autonomous driving.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network</title>
<link>https://arxiv.org/abs/2504.18127</link>
<guid>https://arxiv.org/abs/2504.18127</guid>
<content:encoded><![CDATA[
<div> Keywords: spacecraft image super-resolution, salient region, arbitrary-scale, feature fusion, deep learning<br />
<br />
Summary: <br />
Spacecraft image super-resolution aims to enhance low-resolution spacecraft images to high-resolution ones. Existing arbitrary-scale super-resolution methods often introduce noise due to overlooking the difference in features between spacecraft core regions and the black space background. To address this, a salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR) is proposed. The network utilizes features from spacecraft core salient regions to guide latent modulation for achieving arbitrary-scale super-resolution. A spacecraft core region recognition block (SCRRB) is designed to identify core salient regions, while an adaptive-weighted feature fusion enhancement mechanism (AFFEM) selectively aggregates spacecraft core region features with general image features. Experimental results show that SGSASR outperforms existing approaches in enhancing spacecraft images. <br /> <div>
arXiv:2504.18127v1 Announce Type: new 
Abstract: Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose a salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results demonstrate that the proposed SGSASR outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASF-YOLO: An Improved YOLOv11 Network for Small Object Detection on Drone View</title>
<link>https://arxiv.org/abs/2504.18136</link>
<guid>https://arxiv.org/abs/2504.18136</guid>
<content:encoded><![CDATA[
<div> UAV, object detection, computer vision, YOLOv11, VisDrone2019<br />
Summary:<br />
- The article introduces a novel object detection network called Multi-scale Context Aggregation and Scale-adaptive Fusion YOLO (MASF-YOLO), specifically designed for UAV images.
- The network addresses challenges such as small target pixels, scale variations of objects, and complex backgrounds in UAV images.
- Key modules include Multi-scale Feature Aggregation Module (MFAM) for detecting small objects, Improved Efficient Multi-scale Attention Module (IEMA) to reduce background noise interference, and Dimension-Aware Selective Integration Module (DASI) for feature fusion.
- Performance evaluations on the VisDrone2019 dataset show MASF-YOLO outperforming YOLOv11-s in mAP@0.5 and mAP@0.5:0.95 with lower parameters and computational cost.
- Comparative experiments demonstrate MASF-YOLO-s maintains competitive advantage in detection accuracy and model efficiency compared to state-of-the-art detectors.<br /><br /> <div>
arXiv:2504.18136v1 Announce Type: new 
Abstract: With the rapid advancement of Unmanned Aerial Vehicle (UAV) and computer vision technologies, object detection from UAV perspectives has emerged as a prominent research area. However, challenges for detection brought by the extremely small proportion of target pixels, significant scale variations of objects, and complex background information in UAV images have greatly limited the practical applications of UAV. To address these challenges, we propose a novel object detection network Multi-scale Context Aggregation and Scale-adaptive Fusion YOLO (MASF-YOLO), which is developed based on YOLOv11. Firstly, to tackle the difficulty of detecting small objects in UAV images, we design a Multi-scale Feature Aggregation Module (MFAM), which significantly improves the detection accuracy of small objects through parallel multi-scale convolutions and feature fusion. Secondly, to mitigate the interference of background noise, we propose an Improved Efficient Multi-scale Attention Module (IEMA), which enhances the focus on target regions through feature grouping, parallel sub-networks, and cross-spatial learning. Thirdly, we introduce a Dimension-Aware Selective Integration Module (DASI), which further enhances multi-scale feature fusion capabilities by adaptively weighting and fusing low-dimensional features and high-dimensional features. Finally, we conducted extensive performance evaluations of our proposed method on the VisDrone2019 dataset. Compared to YOLOv11-s, MASFYOLO-s achieves improvements of 4.6% in mAP@0.5 and 3.5% in mAP@0.5:0.95 on the VisDrone2019 validation set. Remarkably, MASF-YOLO-s outperforms YOLOv11-m while requiring only approximately 60% of its parameters and 65% of its computational cost. Furthermore, comparative experiments with state-of-the-art detectors confirm that MASF-YOLO-s maintains a clear competitive advantage in both detection accuracy and model efficiency.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding</title>
<link>https://arxiv.org/abs/2504.18152</link>
<guid>https://arxiv.org/abs/2504.18152</guid>
<content:encoded><![CDATA[
<div> Keywords: ActionArt, fine-grained understanding, human-centric AI, multimodal models, proxy tasks

Summary:<br />
The article introduces the ActionArt dataset, aimed at advancing research in human-centric multimodal understanding by providing detailed annotations of human actions and poses in videos. The dataset includes various scenarios and interactions, with annotations on every limb movement. Eight sub-tasks are developed to evaluate large multimodal models' capabilities for fine-grained understanding, revealing their limitations due to the scarcity of meticulously annotated data. To address this, proxy tasks are proposed to enhance model perception in spatial and temporal dimensions using automatically generated data from existing models. Experimental results demonstrate a significant improvement in bridging the performance gap between models trained on annotated data and those using proxy tasks. These findings highlight the potential of using proxy tasks to reduce reliance on manual annotations for achieving fine-grained understanding in human-centric AI applications.<br />Summary: <div>
arXiv:2504.18152v1 Announce Type: new 
Abstract: Fine-grained understanding of human actions and poses in videos is essential for human-centric AI applications. In this work, we introduce ActionArt, a fine-grained video-caption dataset designed to advance research in human-centric multimodal understanding. Our dataset comprises thousands of videos capturing a broad spectrum of human actions, human-object interactions, and diverse scenarios, each accompanied by detailed annotations that meticulously label every limb movement. We develop eight sub-tasks to evaluate the fine-grained understanding capabilities of existing large multimodal models across different dimensions. Experimental results indicate that, while current large multimodal models perform commendably on various tasks, they often fall short in achieving fine-grained understanding. We attribute this limitation to the scarcity of meticulously annotated data, which is both costly and difficult to scale manually. Since manual annotations are costly and hard to scale, we propose proxy tasks to enhance the model perception ability in both spatial and temporal dimensions. These proxy tasks are carefully crafted to be driven by data automatically generated from existing MLLMs, thereby reducing the reliance on costly manual labels. Experimental results show that the proposed proxy tasks significantly narrow the gap toward the performance achieved with manually annotated fine-grained data.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-InMeMo: Enhanced Prompting for Visual In-Context Learning</title>
<link>https://arxiv.org/abs/2504.18158</link>
<guid>https://arxiv.org/abs/2504.18158</guid>
<content:encoded><![CDATA[
<div> keywords: in-context learning, computer vision, enhanced prompting, mIoU scores, lightweight strategy
Summary: 
Enhanced Instruct Me More (E-InMeMo) is a novel approach that introduces learnable perturbations into in-context pairs to optimize prompting in computer vision tasks. By enhancing visual in-context learning, E-InMeMo outperforms existing methods in various vision tasks, such as foreground segmentation and single object detection. It significantly boosts mIoU scores, improving by 7.99 for foreground segmentation and 17.04 for single object detection compared to the baseline method without learnable prompts. E-InMeMo is a lightweight and effective strategy that demonstrates superior performance in enhancing visual in-context learning. The code for E-InMeMo is publicly available on GitHub at https://github.com/Jackieam/E-InMeMo.

Summary:<br /><br />Keywords: in-context learning, computer vision, enhanced prompting, mIoU scores, lightweight strategy<br />Enhanced Instruct Me More (E-InMeMo) introduces learnable perturbations into in-context pairs for optimized prompting in computer vision tasks. It outperforms existing methods, improving mIoU scores significantly in tasks like foreground segmentation and single object detection. E-InMeMo is a lightweight and effective strategy for enhancing visual in-context learning, with the code available on GitHub. <div>
arXiv:2504.18158v1 Announce Type: new 
Abstract: Large-scale models trained on extensive datasets have become the standard due to their strong generalizability across diverse tasks. In-context learning (ICL), widely used in natural language processing, leverages these models by providing task-specific prompts without modifying their parameters. This paradigm is increasingly being adapted for computer vision, where models receive an input-output image pair, known as an in-context pair, alongside a query image to illustrate the desired output. However, the success of visual ICL largely hinges on the quality of these prompts. To address this, we propose Enhanced Instruct Me More (E-InMeMo), a novel approach that incorporates learnable perturbations into in-context pairs to optimize prompting. Through extensive experiments on standard vision tasks, E-InMeMo demonstrates superior performance over existing state-of-the-art methods. Notably, it improves mIoU scores by 7.99 for foreground segmentation and by 17.04 for single object detection when compared to the baseline without learnable prompts. These results highlight E-InMeMo as a lightweight yet effective strategy for enhancing visual ICL. Code is publicly available at: https://github.com/Jackieam/E-InMeMo
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models</title>
<link>https://arxiv.org/abs/2504.18165</link>
<guid>https://arxiv.org/abs/2504.18165</guid>
<content:encoded><![CDATA[
<div> digital twinning, object tracking, KPIs, industrial production lines, Convolutional Neural Networks <br />
<br />
Summary: PerfCam is a new open source framework that uses camera and sensor data, 3D Gaussian Splatting, and computer vision models for digital twinning in industrial production lines. By combining 3D reconstruction and Convolutional Neural Networks, PerfCam enables semi-automated object tracking and spatial mapping, allowing for real-time monitoring of Key Performance Indicators (KPIs) like availability, performance, Overall Equipment Effectiveness (OEE), and conveyor belt rates. The framework was tested in pharmaceutical production lines and proved effective in providing actionable insights. PerfCam's digital twin capabilities make it a valuable tool for creating usable digital twins in smart manufacturing environments and extracting operational analytics. The openly published dataset accompanying the framework encourages further research and development in this area. <br /><br /> <div>
arXiv:2504.18165v1 Announce Type: new 
Abstract: We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-independent hyperparameter-free self-supervised single-view deep subspace clustering</title>
<link>https://arxiv.org/abs/2504.18179</link>
<guid>https://arxiv.org/abs/2504.18179</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep subspace clustering, layer-wise self expression loss, subspace-structured norm, sequential learning framework, relative error-based self-stopping mechanism

Summary:<br /><br />
The article introduces a novel single-view Deep Subspace Clustering (DSC) approach to address existing challenges. It focuses on utilizing information present in intermediate layers through layer-wise self expression loss and optimizing clustering quality using a subspace-structured norm. The approach implements a multi-stage sequential learning framework that eliminates the need for hyperparameter tuning using multiple regularization terms. Training termination is based on a relative error-based self-stopping mechanism, enabling training without external labels. Additionally, the method retains a fixed number of leading coefficients in the learned representation matrix based on prior knowledge. Evaluation on various datasets shows superior performance compared to linear SC algorithms with tuned hyperparameters and competitive performance with the best linear approaches. <div>
arXiv:2504.18179v1 Announce Type: new 
Abstract: Deep subspace clustering (DSC) algorithms face several challenges that hinder their widespread adoption across variois application domains. First, clustering quality is typically assessed using only the encoder's output layer, disregarding valuable information present in the intermediate layers. Second, most DSC approaches treat representation learning and subspace clustering as independent tasks, limiting their effectiveness. Third, they assume the availability of a held-out dataset for hyperparameter tuning, which is often impractical in real-world scenarios. Fourth, learning termination is commonly based on clustering error monitoring, requiring external labels. Finally, their performance often depends on post-processing techniques that rely on labeled data. To address this limitations, we introduce a novel single-view DSC approach that: (i) minimizes a layer-wise self expression loss using a joint representation matrix; (ii) optimizes a subspace-structured norm to enhance clustering quality; (iii) employs a multi-stage sequential learning framework, consisting of pre-training and fine-tuning, enabling the use of multiple regularization terms without hyperparameter tuning; (iv) incorporates a relative error-based self-stopping mechanism to terminate training without labels; and (v) retains a fixed number of leading coefficients in the learned representation matrix based on prior knowledge. We evaluate the proposed method on six datasets representing faces, digits, and objects. The results show that our method outperforms most linear SC algorithms with careffulyl tuned hyperparameters while maintaining competitive performance with the best performing linear appoaches.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is the Added Value of UDA in the VFM Era?</title>
<link>https://arxiv.org/abs/2504.18190</link>
<guid>https://arxiv.org/abs/2504.18190</guid>
<content:encoded><![CDATA[
<div> Keywords: Unsupervised Domain Adaptation, Vision Foundation Models, Semantic Segmentation, Synthetic Data, Autonomous Driving<br />
<br />
Summary: 
This study explores Unsupervised Domain Adaptation (UDA) using Vision Foundation Models (VFMs) for semantic segmentation in autonomous driving scenarios. The research investigates UDA performance with diverse and representative data, comparing synth-to-real and real-to-real scenarios. Results show that UDA's advantage over source-only fine-tuning decreases with stronger synthetic source data, but still outperforms in all synthetic data scenarios. Further, UDA with a small amount of labeled target data achieves state-of-the-art segmentation quality comparable to fully-supervised models. The study suggests that while UDA may not always provide significant benefits, it remains a valuable technique for enhancing perception models in autonomous driving applications. <div>
arXiv:2504.18190v1 Announce Type: new 
Abstract: Unsupervised Domain Adaptation (UDA) can improve a perception model's generalization to an unlabeled target domain starting from a labeled source domain. UDA using Vision Foundation Models (VFMs) with synthetic source data can achieve generalization performance comparable to fully-supervised learning with real target data. However, because VFMs have strong generalization from their pre-training, more straightforward, source-only fine-tuning can also perform well on the target. As data scenarios used in academic research are not necessarily representative for real-world applications, it is currently unclear (a) how UDA behaves with more representative and diverse data and (b) if source-only fine-tuning of VFMs can perform equally well in these scenarios. Our research aims to close these gaps and, similar to previous studies, we focus on semantic segmentation as a representative perception task. We assess UDA for synth-to-real and real-to-real use cases with different source and target data combinations. We also investigate the effect of using a small amount of labeled target data in UDA. We clarify that while these scenarios are more realistic, they are not necessarily more challenging. Our results show that, when using stronger synthetic source data, UDA's improvement over source-only fine-tuning of VFMs reduces from +8 mIoU to +2 mIoU, and when using more diverse real source data, UDA has no added value. However, UDA generalization is always higher in all synthetic data scenarios than source-only fine-tuning and, when including only 1/16 of Cityscapes labels, synthetic UDA obtains the same state-of-the-art segmentation quality of 85 mIoU as a fully-supervised model using all labels. Considering the mixed results, we discuss how UDA can best support robust autonomous driving at scale.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition</title>
<link>https://arxiv.org/abs/2504.18201</link>
<guid>https://arxiv.org/abs/2504.18201</guid>
<content:encoded><![CDATA[
<div> Keywords: intent recognition, visual clues, multi-grained features, class-specific prototypes, graph convolutional network

Summary: 
The paper introduces a novel approach called Multi-grained Compositional visual Clue Learning (MCCL) to improve image intent recognition. It addresses challenges of diverse visual clues and class-specific prototypes to handle large visual diversity. The method breaks down intent recognition into visual clue composition and integrates multi-grained features, enhancing accuracy and interpretability. The approach treats intent recognition as a multi-label classification problem and uses a graph convolutional network to incorporate prior knowledge through label embedding correlations. Demonstrated on Intentonomy and MDID datasets, the method outperforms existing approaches. It paves the way for future explorations in understanding the complex and varied forms of human expression. 

<br /><br />Summary: <div>
arXiv:2504.18201v1 Announce Type: new 
Abstract: In an era where social media platforms abound, individuals frequently share images that offer insights into their intents and interests, impacting individual life quality and societal stability. Traditional computer vision tasks, such as object detection and semantic segmentation, focus on concrete visual representations, while intent recognition relies more on implicit visual clues. This poses challenges due to the wide variation and subjectivity of such clues, compounded by the problem of intra-class variety in conveying abstract concepts, e.g. "enjoy life". Existing methods seek to solve the problem by manually designing representative features or building prototypes for each class from global features. However, these methods still struggle to deal with the large visual diversity of each intent category. In this paper, we introduce a novel approach named Multi-grained Compositional visual Clue Learning (MCCL) to address these challenges for image intent recognition. Our method leverages the systematic compositionality of human cognition by breaking down intent recognition into visual clue composition and integrating multi-grained features. We adopt class-specific prototypes to alleviate data imbalance. We treat intent recognition as a multi-label classification problem, using a graph convolutional network to infuse prior knowledge through label embedding correlations. Demonstrated by a state-of-the-art performance on the Intentonomy and MDID datasets, our approach advances the accuracy of existing methods while also possessing good interpretability. Our work provides an attempt for future explorations in understanding complex and miscellaneous forms of human expression.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring</title>
<link>https://arxiv.org/abs/2504.18203</link>
<guid>https://arxiv.org/abs/2504.18203</guid>
<content:encoded><![CDATA[
<div> Keywords: railway automation, deep learning, long-range object detection, monocular images, LiDAR data<br />
Summary:<br />
- The article introduces a deep-learning-based approach for long-range 3D object detection in railway systems, with a focus on automation in Germany.
- The automation aims to address legacy infrastructure challenges and increase train traffic safely by improving early hazard detection, such as obstacles at level crossings or pedestrians on tracks.
- The method utilizes monocular images and incorporates LiDAR data during training to enhance depth estimation for robust perception.
- The proposed pipeline includes modified YOLOv9 for 2.5D object detection, a depth estimation network, and short- and long-range 3D detection heads.
- Evaluation on the OSDaR23 dataset demonstrates the effectiveness of the approach in detecting objects up to 250 meters, showcasing its potential for enhancing railway automation with areas for further improvement identified. <br /><br />Summary: <div>
arXiv:2504.18203v1 Announce Type: new 
Abstract: Railway systems, particularly in Germany, require high levels of automation to address legacy infrastructure challenges and increase train traffic safely. A key component of automation is robust long-range perception, essential for early hazard detection, such as obstacles at level crossings or pedestrians on tracks. Unlike automotive systems with braking distances of ~70 meters, trains require perception ranges exceeding 1 km. This paper presents an deep-learning-based approach for long-range 3D object detection tailored for autonomous trains. The method relies solely on monocular images, inspired by the Faraway-Frustum approach, and incorporates LiDAR data during training to improve depth estimation. The proposed pipeline consists of four key modules: (1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation network, and (3-4) dedicated short- and long-range 3D detection heads. Evaluations on the OSDaR23 dataset demonstrate the effectiveness of the approach in detecting objects up to 250 meters. Results highlight its potential for railway automation and outline areas for future improvement.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding</title>
<link>https://arxiv.org/abs/2504.18204</link>
<guid>https://arxiv.org/abs/2504.18204</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, image generation, multi-round interactions, Visual Co-Adaptation framework, human-in-the-loop feedback

Summary: 
Generative AI has transformed industries by enabling text-driven image generation but faces challenges in producing high-resolution images that align with user preferences. This study introduces a Visual Co-Adaptation (VCA) framework that incorporates human feedback to optimize image generation based on multiple reward functions like diversity and consistency. By leveraging a well-trained reward model and multi-turn dialogue datasets, the framework fine-tunes the diffusion model through LoRA to enhance image quality. Through experiments, it is shown that the VCA framework outperforms existing models, particularly in multi-turn dialogue scenarios, by improving image consistency and alignment with user intent. User satisfaction is significantly higher when using this approach, showcasing its potential in enhancing generative AI capabilities. 

<br /><br />Summary: <div>
arXiv:2504.18204v1 Announce Type: new 
Abstract: Generative AI has significantly changed industries by enabling text-driven image generation, yet challenges remain in achieving high-resolution outputs that align with fine-grained user preferences. Consequently, multi-round interactions are necessary to ensure the generated images meet expectations. Previous methods enhanced prompts via reward feedback but did not optimize over a multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation (VCA) framework incorporating human-in-the-loop feedback, leveraging a well-trained reward model aligned with human preferences. Using a diverse multi-turn dialogue dataset, our framework applies multiple reward functions, such as diversity, consistency, and preference feedback, while fine-tuning the diffusion model through LoRA, thus optimizing image generation based on user input. We also construct multi-round dialogue datasets of prompts and image pairs aligned with user intent. Experiments demonstrate that our method outperforms state-of-the-art baselines, significantly improving image consistency and alignment with user intent. Our approach consistently surpasses competing models in user satisfaction, especially in multi-turn dialogue scenarios.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data-Centric Approach to 3D Semantic Segmentation of Railway Scenes</title>
<link>https://arxiv.org/abs/2504.18213</link>
<guid>https://arxiv.org/abs/2504.18213</guid>
<content:encoded><![CDATA[
<div> LiDAR, semantic segmentation, data augmentation, autonomous trains, OSDaR23 <br />
<br />
Summary: 
This paper introduces two novel data augmentation methods aimed at enhancing LiDAR-based semantic segmentation for autonomous trains, specifically targeting the railway-specific OSDaR23 dataset. The first method, person instance pasting, focuses on improving segmentation accuracy of pedestrians at distant ranges by introducing realistic variations into the dataset. The second method, track sparsification, redistributes point density in LiDAR scans to enhance track segmentation at far distances while maintaining close-range accuracy. These methods are evaluated using a cutting-edge 3D semantic segmentation network, showcasing notable enhancements in distant-range performance without compromising close-range predictions. By establishing a benchmark for 3D semantic segmentation on the OSDaR23 dataset, this research highlights the potential of data-centric approaches in addressing the unique challenges of autonomous train perception in railway environments. <br /> <div>
arXiv:2504.18213v1 Announce Type: new 
Abstract: LiDAR-based semantic segmentation is critical for autonomous trains, requiring accurate predictions across varying distances. This paper introduces two targeted data augmentation methods designed to improve segmentation performance on the railway-specific OSDaR23 dataset. The person instance pasting method enhances segmentation of pedestrians at distant ranges by injecting realistic variations into the dataset. The track sparsification method redistributes point density in LiDAR scans, improving track segmentation at far distances with minimal impact on close-range accuracy. Both methods are evaluated using a state-of-the-art 3D semantic segmentation network, demonstrating significant improvements in distant-range performance while maintaining robustness in close-range predictions. We establish the first 3D semantic segmentation benchmark for OSDaR23, demonstrating the potential of data-centric approaches to address railway-specific challenges in autonomous train perception.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating</title>
<link>https://arxiv.org/abs/2504.18215</link>
<guid>https://arxiv.org/abs/2504.18215</guid>
<content:encoded><![CDATA[
<div> Keywords: Monocular 3D reconstruction, End-to-end network, Anatomy Shaping Extraction module, Twins Negotiating Reconstruction U-Net, Comic Data Augmentation.

Summary: 
This paper presents a novel approach for monocular 3D clothed human reconstruction using an end-to-end network that directly predicts a 3D avatar from a single 2D image, without relying on explicit intermediate geometry. The proposed framework includes the Anatomy Shaping Extraction module, which captures shape features specific to human anatomy, and the Twins Negotiating Reconstruction U-Net, which enhances reconstruction through feature interaction between different modalities. Additionally, a Comic Data Augmentation strategy is introduced to improve model performance with over 15,000 3D human scans. Extensive experiments demonstrate the superiority of this method over state-of-the-art techniques on two test sets and various real-world scenarios. Visit the provided link for demos showcasing the effectiveness of the proposed approach. 

<br /><br />Summary: <div>
arXiv:2504.18215v1 Announce Type: new 
Abstract: Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modeling both it and the input image. This routine is constrained by the preceding model, and overlooks the integrity of the reconstruction task. To address this, this paper introduces a novel paradigm that treats human reconstruction as a holistic process, utilizing an end-to-end network for direct prediction from 2D image to 3D avatar, eliminating any explicit intermediate geometry display. Based on this, we further propose a novel reconstruction framework consisting of two core components: the Anatomy Shaping Extraction module, which captures implicit shape features taking into account the specialty of human anatomy, and the Twins Negotiating Reconstruction U-Net, which enhances reconstruction through feature interaction between two U-Nets of different modalities. Moreover, we propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to bolster model performance in more complex case input. Extensive experiments on two test sets and many in-the-wild cases show the superiority of our method over SOTA methods. Our demos can be found in : https://e2e3dgsrecon.github.io/e2e3dgsrecon/.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense Geometry Supervision for Underwater Depth Estimation</title>
<link>https://arxiv.org/abs/2504.18233</link>
<guid>https://arxiv.org/abs/2504.18233</guid>
<content:encoded><![CDATA[
<div> dataset, multi-view depth estimation, texture-depth fusion module, underwater optical imaging principles, FLSea dataset
<br />
Summary:<br />
This paper introduces a novel approach to address the limited research on monocular depth estimation for underwater scenes. By constructing an economically efficient dataset using multi-view depth estimation, the proposed method generates supervisory signals and enhanced underwater images. A texture-depth fusion module, based on underwater optical imaging principles, effectively integrates depth information from texture cues. Experimental results on the FLSea dataset show a significant improvement in accuracy and adaptability of models in underwater settings. This work provides a cost-effective solution for monocular underwater depth estimation and shows promise for practical applications. <div>
arXiv:2504.18233v1 Announce Type: new 
Abstract: The field of monocular depth estimation is continually evolving with the advent of numerous innovative models and extensions. However, research on monocular depth estimation methods specifically for underwater scenes remains limited, compounded by a scarcity of relevant data and methodological support. This paper proposes a novel approach to address the existing challenges in current monocular depth estimation methods for underwater environments. We construct an economically efficient dataset suitable for underwater scenarios by employing multi-view depth estimation to generate supervisory signals and corresponding enhanced underwater images. we introduces a texture-depth fusion module, designed according to the underwater optical imaging principles, which aims to effectively exploit and integrate depth information from texture cues. Experimental results on the FLSea dataset demonstrate that our approach significantly improves the accuracy and adaptability of models in underwater settings. This work offers a cost-effective solution for monocular underwater depth estimation and holds considerable promise for practical applications.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiasBench: A reproducible benchmark for tuning the biases of event cameras</title>
<link>https://arxiv.org/abs/2504.18235</link>
<guid>https://arxiv.org/abs/2504.18235</guid>
<content:encoded><![CDATA[
<div> Keywords: event-based cameras, biases tuning, BiasBench dataset, quality metric, RL-based method

Summary:
Event-based cameras, inspired by biological sensors, offer advantages over traditional frame-based cameras such as high temporal resolution and low latency. However, configuring the camera's settings, or biases, is crucial to output quality. While frame-based cameras have automatic configuration algorithms, there are few tools for tuning biases in event-based cameras. To address this, a new dataset called BiasBench has been created, containing scenes with settings sampled in a grid pattern. The dataset includes scenes with quality metrics for different applications. Additionally, a novel method based on reinforcement learning has been introduced to facilitate online bias adjustments. The availability of BiasBench and the new bias tuning method aim to improve the reproducibility and optimization of event-based camera settings for various applications. 

<br /><br />Summary: <div>
arXiv:2504.18235v1 Announce Type: new 
Abstract: Event-based cameras are bio-inspired sensors that detect light changes asynchronously for each pixel. They are increasingly used in fields like computer vision and robotics because of several advantages over traditional frame-based cameras, such as high temporal resolution, low latency, and high dynamic range. As with any camera, the output's quality depends on how well the camera's settings, called biases for event-based cameras, are configured. While frame-based cameras have advanced automatic configuration algorithms, there are very few such tools for tuning these biases. A systematic testing framework would require observing the same scene with different biases, which is tricky since event cameras only generate events when there is movement. Event simulators exist, but since biases heavily depend on the electrical circuit and the pixel design, available simulators are not well suited for bias tuning. To allow reproducibility, we present BiasBench, a novel event dataset containing multiple scenes with settings sampled in a grid-like pattern. We present three different scenes, each with a quality metric of the downstream application. Additionally, we present a novel, RL-based method to facilitate online bias adjustments.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Event-Based Eye Tracking. 2025 Event-based Vision Workshop</title>
<link>https://arxiv.org/abs/2504.18249</link>
<guid>https://arxiv.org/abs/2504.18249</guid>
<content:encoded><![CDATA[
<div> Keywords: Event-Based Eye Tracking, Pupil Center Prediction, Innovative Methods, Challenge, Hardware Design

Summary: 
The survey discusses the 2025 Event-Based Eye Tracking Challenge, which focused on predicting the pupil center using event camera recorded eye movement. It reviews top-ranked teams' methods, detailing accuracy, model size, and operations. The challenge results aim to drive future event-based eye tracking research. The survey also explores event-based eye tracking from a hardware design perspective, highlighting the significance of advanced methods in the field. <div>
arXiv:2504.18249v1 Announce Type: new 
Abstract: This survey serves as a review for the 2025 Event-Based Eye Tracking Challenge organized as part of the 2025 CVPR event-based vision workshop. This challenge focuses on the task of predicting the pupil center by processing event camera recorded eye movement. We review and summarize the innovative methods from teams rank the top in the challenge to advance future event-based eye tracking research. In each method, accuracy, model size, and number of operations are reported. In this survey, we also discuss event-based eye tracking from the perspective of hardware design.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology</title>
<link>https://arxiv.org/abs/2504.18256</link>
<guid>https://arxiv.org/abs/2504.18256</guid>
<content:encoded><![CDATA[
<div> Keywords: biodiversity mapping, remote sensing, self-supervised learning, phenology, dataset construction

Summary:
This study addresses the challenge of limited labeled datasets for global biodiversity mapping using remote sensing data. A new dataset, SSL4Eco, is introduced, utilizing a phenology-informed sampling strategy to capture vegetation seasonality globally. By training an existing model on this dataset with a season-contrastive objective, the study shows improved representation quality on ecological downstream tasks. The model pretrained on SSL4Eco outperforms existing models on 7 out of 8 tasks, showcasing its state-of-the-art performance. The importance of dataset construction in enhancing representation quality is emphasized, highlighting the need for unbiased and seasonally-informed datasets. The release of code, data, and model weights aims to support both macroecological and computer vision research. 
<br /><br />Summary: <div>
arXiv:2504.18256v1 Announce Type: new 
Abstract: With the exacerbation of the biodiversity and climate crises, macroecological pursuits such as global biodiversity mapping become more urgent. Remote sensing offers a wealth of Earth observation data for ecological studies, but the scarcity of labeled datasets remains a major challenge. Recently, self-supervised learning has enabled learning representations from unlabeled data, triggering the development of pretrained geospatial models with generalizable features. However, these models are often trained on datasets biased toward areas of high human activity, leaving entire ecological regions underrepresented. Additionally, while some datasets attempt to address seasonality through multi-date imagery, they typically follow calendar seasons rather than local phenological cycles. To better capture vegetation seasonality at a global scale, we propose a simple phenology-informed sampling strategy and introduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we train an existing model with a season-contrastive objective. We compare representations learned from SSL4Eco against other datasets on diverse ecological downstream tasks and demonstrate that our straightforward sampling method consistently improves representation quality, highlighting the importance of dataset construction. The model pretrained on SSL4Eco reaches state of the art performance on 7 out of 8 downstream tasks spanning (multi-label) classification and regression. We release our code, data, and model weights to support macroecological and computer vision research at https://github.com/PlekhanovaElena/ssl4eco.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator</title>
<link>https://arxiv.org/abs/2504.18283</link>
<guid>https://arxiv.org/abs/2504.18283</guid>
<content:encoded><![CDATA[
<div> Audio-Visual Generative Model, Image Generation, Mixed Audio, Audio-Visual Separator, Evaluation Metrics<br />
Summary:<br />
The article introduces the Audio-Visual Generation and Separation model (AV-GAS), which focuses on generating images from soundscapes, i.e., mixed audio containing multiple classes. The proposed model addresses the challenge of generating images from multi-class audio input using an audio-visual separator. Additionally, a new audio-visual separation task is introduced to generate separate images for each class present in mixed audio. The model is trained and evaluated on the VGGSound dataset, outperforming state-of-the-art methods by achieving higher scores in Class Representation Score (CRS) and modified R@K metrics. Overall, the AV-GAS model shows significant progress in generating plausible images from mixed audio inputs. <br /><br />Summary: <div>
arXiv:2504.18283v1 Announce Type: new 
Abstract: Recent audio-visual generative models have made substantial progress in generating images from audio. However, existing approaches focus on generating images from single-class audio and fail to generate images from mixed audio. To address this, we propose an Audio-Visual Generation and Separation model (AV-GAS) for generating images from soundscapes (mixed audio containing multiple classes). Our contribution is threefold: First, we propose a new challenge in the audio-visual generation task, which is to generate an image given a multi-class audio input, and we propose a method that solves this task using an audio-visual separator. Second, we introduce a new audio-visual separation task, which involves generating separate images for each class present in a mixed audio input. Lastly, we propose new evaluation metrics for the audio-visual generation task: Class Representation Score (CRS) and a modified R@K. Our model is trained and evaluated on the VGGSound dataset. We show that our method outperforms the state-of-the-art, achieving 7% higher CRS and 4% higher R@2* in generating plausible images with mixed audio.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis</title>
<link>https://arxiv.org/abs/2504.18286</link>
<guid>https://arxiv.org/abs/2504.18286</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic training data, material wear, aging, re-identification, dataset  
Summary:  
- The study investigates the influence of synthetic training data on predicting material wear and aging in re-identification scenarios. 
- Various experimental setups and gallery expansion strategies were tested to improve performance over time for aging re-identification subjects.
- Continuous updating of the gallery improved mean Rank-1 accuracy by 24% by considering material aging gradually.
- Models trained with 10% artificial data showed up to a 13% increase in Rank-1 accuracy compared to models trained solely on real-world data, enhancing general performance on hold-out data.
- The creation of the pallet-block-2696 dataset, containing images of Euro pallets aged and damaged over 4 months, provides a valuable resource for generating synthetic aged wooden materials. 

<br /><br />Summary: <div>
arXiv:2504.18286v1 Announce Type: new 
Abstract: This contribution explores the impact of synthetic training data usage and the prediction of material wear and aging in the context of re-identification. Different experimental setups and gallery set expanding strategies are tested, analyzing their impact on performance over time for aging re-identification subjects. Using a continuously updating gallery, we were able to increase our mean Rank-1 accuracy by 24%, as material aging was taken into account step by step. In addition, using models trained with 10% artificial training data, Rank-1 accuracy could be increased by up to 13%, in comparison to a model trained on only real-world data, significantly boosting generalized performance on hold-out data. Finally, this work introduces a novel, open-source re-identification dataset, pallet-block-2696. This dataset contains 2,696 images of Euro pallets, taken over a period of 4 months. During this time, natural aging processes occurred and some of the pallets were damaged during their usage. These wear and tear processes significantly changed the appearance of the pallets, providing a dataset that can be used to generate synthetically aged pallets or other wooden materials.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy</title>
<link>https://arxiv.org/abs/2504.18317</link>
<guid>https://arxiv.org/abs/2504.18317</guid>
<content:encoded><![CDATA[
<div> GPS signals unavailable, UAV localization, vision-based methods, compact multi-view features, orthogonally-constrained VIB encoder<br />
<br />
Summary: 
The study introduces a novel approach to precise UAV localization in urban areas without GPS signals. Inspired by mammalian spatial cognition, UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. The Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB) with automatic relevance determination (ARD) prunes non-informative features and enforces orthogonality to minimize redundancy. This task-oriented communication framework enables efficient and accurate localization with minimal transmission cost, essential for the Low Altitude Economy (LAE). Extensive evaluation on a dedicated UAV dataset demonstrates that O-VIB achieves high-precision localization under stringent bandwidth constraints. The code and dataset will be publicly available on GitHub for further research and development. <br /><br /> <div>
arXiv:2504.18317v1 Announce Type: new 
Abstract: To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles (UAVs) localization in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UAVs. Inspired by mammalian spatial cognition, we propose a task-oriented communication framework, where UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. We introduce the Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB), which incorporates automatic relevance determination (ARD) to prune non-informative features while enforcing orthogonality to minimize redundancy. This enables efficient and accurate localization with minimal transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows that O-VIB achieves high-precision localization under stringent bandwidth budgets. Code and dataset will be made publicly available: github.com/fangzr/TOC-Edge-Aerial.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2504.18318</link>
<guid>https://arxiv.org/abs/2504.18318</guid>
<content:encoded><![CDATA[
arXiv:2504.18318v1 Announce Type: new 
Abstract: Text-to-4D generation is rapidly developing and widely applied in various scenarios. However, existing methods often fail to incorporate adequate spatio-temporal modeling and prompt alignment within a unified framework, resulting in temporal inconsistencies, geometric distortions, or low-quality 4D content that deviates from the provided texts. Therefore, we propose STP4D, a novel approach that aims to integrate comprehensive spatio-temporal-prompt consistency modeling for high-quality text-to-4D generation. Specifically, STP4D employs three carefully designed modules: Time-varying Prompt Embedding, Geometric Information Enhancement, and Temporal Extension Deformation, which collaborate to accomplish this goal. Furthermore, STP4D is among the first methods to exploit the Diffusion model to generate 4D Gaussians, combining the fine-grained modeling capabilities and the real-time rendering process of 4DGS with the rapid inference speed of the Diffusion model. Extensive experiments demonstrate that STP4D excels in generating high-fidelity 4D content with exceptional efficiency (approximately 4.6s per asset), surpassing existing methods in both quality and speed.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth3DLane: Monocular 3D Lane Detection via Depth Prior Distillation</title>
<link>https://arxiv.org/abs/2504.18325</link>
<guid>https://arxiv.org/abs/2504.18325</guid>
<content:encoded><![CDATA[
arXiv:2504.18325v1 Announce Type: new 
Abstract: Monocular 3D lane detection is challenging due to the difficulty in capturing depth information from single-camera images. A common strategy involves transforming front-view (FV) images into bird's-eye-view (BEV) space through inverse perspective mapping (IPM), facilitating lane detection using BEV features. However, IPM's flat-ground assumption and loss of contextual information lead to inaccuracies in reconstructing 3D information, especially height. In this paper, we introduce a BEV-based framework to address these limitations and improve 3D lane detection accuracy. Our approach incorporates a Hierarchical Depth-Aware Head that provides multi-scale depth features, mitigating the flat-ground assumption by enhancing spatial awareness across varying depths. Additionally, we leverage Depth Prior Distillation to transfer semantic depth knowledge from a teacher model, capturing richer structural and contextual information for complex lane structures. To further refine lane continuity and ensure smooth lane reconstruction, we introduce a Conditional Random Field module that enforces spatial coherence in lane predictions. Extensive experiments validate that our method achieves state-of-the-art performance in terms of z-axis error and outperforms other methods in the field in overall performance. The code is released at: https://anonymous.4open.science/r/Depth3DLane-DCDD.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations</title>
<link>https://arxiv.org/abs/2504.18332</link>
<guid>https://arxiv.org/abs/2504.18332</guid>
<content:encoded><![CDATA[
arXiv:2504.18332v1 Announce Type: new 
Abstract: The growing applications of AR/VR increase the demand for real-time full-body pose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint signals from the head and hands, reconstructing a full-body pose remains challenging due to the unconstrained lower body. Recent advancements often rely on conventional neural networks and generative models to improve performance in this task, such as Transformers and diffusion models. However, these approaches struggle to strike a balance between achieving precise pose reconstruction and maintaining fast inference speed. To overcome these challenges, a lightweight and efficient model, SSD-Poser, is designed for robust full-body motion estimation from sparse observations. SSD-Poser incorporates a well-designed hybrid encoder, State Space Attention Encoders, to adapt the state space duality to complex motion poses and enable real-time realistic pose reconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate jitter caused by variable-frequency motion signals, remarkably enhancing the motion smoothness. Comprehensive experiments on the AMASS dataset demonstrate that SSD-Poser achieves exceptional accuracy and computational efficiency, showing outstanding inference efficiency compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning</title>
<link>https://arxiv.org/abs/2504.18348</link>
<guid>https://arxiv.org/abs/2504.18348</guid>
<content:encoded><![CDATA[
arXiv:2504.18348v1 Announce Type: new 
Abstract: For deep learning-based image steganography frameworks, in order to ensure the invisibility and recoverability of the information embedding, the loss function usually contains several losses such as embedding loss, recovery loss and steganalysis loss. In previous research works, fixed loss weights are usually chosen for training optimization, and this setting is not linked to the importance of the steganography task itself and the training process. In this paper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for balancing multinomial losses in deep learning image steganography algorithms. TSCL consists of two phases: a priori curriculum control and loss dynamics control. The first phase firstly focuses the model on learning the information embedding of the original image by controlling the loss weights in the multi-party adversarial training; secondly, it makes the model shift its learning focus to improving the decoding accuracy; and finally, it makes the model learn to generate a steganographic image that is resistant to steganalysis. In the second stage, the learning speed of each training task is evaluated by calculating the loss drop of the before and after iteration rounds to balance the learning of each task. Experimental results on three large public datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL strategy improves the quality of steganography, decoding accuracy and security.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Data Auditing in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.18349</link>
<guid>https://arxiv.org/abs/2504.18349</guid>
<content:encoded><![CDATA[
arXiv:2504.18349v1 Announce Type: new 
Abstract: With the surge of large language models (LLMs), Large Vision-Language Models (VLMs)--which integrate vision encoders with LLMs for accurate visual grounding--have shown great potential in tasks like generalist agents and robotic control. However, VLMs are typically trained on massive web-scraped images, raising concerns over copyright infringement and privacy violations, and making data auditing increasingly urgent. Membership inference (MI), which determines whether a sample was used in training, has emerged as a key auditing technique, with promising results on open-source VLMs like LLaVA (AUC > 80%). In this work, we revisit these advances and uncover a critical issue: current MI benchmarks suffer from distribution shifts between member and non-member images, introducing shortcut cues that inflate MI performance. We further analyze the nature of these shifts and propose a principled metric based on optimal transport to quantify the distribution discrepancy. To evaluate MI in realistic settings, we construct new benchmarks with i.i.d. member and non-member images. Existing MI methods fail under these unbiased conditions, performing only marginally better than chance. Further, we explore the theoretical upper bound of MI by probing the Bayes Optimality within the VLM's embedding space and find the irreducible error rate remains high. Despite this pessimistic outlook, we analyze why MI for VLMs is particularly challenging and identify three practical scenarios--fine-tuning, access to ground-truth texts, and set-based inference--where auditing becomes feasible. Our study presents a systematic view of the limits and opportunities of MI for VLMs, providing guidance for future efforts in trustworthy data auditing.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes</title>
<link>https://arxiv.org/abs/2504.18355</link>
<guid>https://arxiv.org/abs/2504.18355</guid>
<content:encoded><![CDATA[
arXiv:2504.18355v1 Announce Type: new 
Abstract: Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a "this looks like that" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization</title>
<link>https://arxiv.org/abs/2504.18361</link>
<guid>https://arxiv.org/abs/2504.18361</guid>
<content:encoded><![CDATA[
arXiv:2504.18361v1 Announce Type: new 
Abstract: Recent advancements in image manipulation have achieved unprecedented progress in generating photorealistic content, but also simultaneously eliminating barriers to arbitrary manipulation and editing, raising concerns about multimedia authenticity and cybersecurity. However, existing Image Manipulation Detection and Localization (IMDL) methodologies predominantly focus on splicing or copy-move forgeries, lacking dedicated benchmarks for inpainting-based manipulations. To bridge this gap, we present COCOInpaint, a comprehensive benchmark specifically designed for inpainting detection, with three key contributions: 1) High-quality inpainting samples generated by six state-of-the-art inpainting models, 2) Diverse generation scenarios enabled by four mask generation strategies with optional text guidance, and 3) Large-scale coverage with 258,266 inpainted images with rich semantic diversity. Our benchmark is constructed to emphasize intrinsic inconsistencies between inpainted and authentic regions, rather than superficial semantic artifacts such as object shapes. We establish a rigorous evaluation protocol using three standard metrics to assess existing IMDL approaches. The dataset will be made publicly available to facilitate future research in this area.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Autoregressive Models for Continuous Latent Generation</title>
<link>https://arxiv.org/abs/2504.18391</link>
<guid>https://arxiv.org/abs/2504.18391</guid>
<content:encoded><![CDATA[
arXiv:2504.18391v1 Announce Type: new 
Abstract: Autoregressive models have demonstrated remarkable success in sequential data generation, particularly in NLP, but their extension to continuous-domain image generation presents significant challenges. Recent work, the masked autoregressive model (MAR), bypasses quantization by modeling per-token distributions in continuous spaces using a diffusion head but suffers from slow inference due to the high computational cost of the iterative denoising process. To address this, we propose the Fast AutoRegressive model (FAR), a novel framework that replaces MAR's diffusion head with a lightweight shortcut head, enabling efficient few-step sampling while preserving autoregressive principles. Additionally, FAR seamlessly integrates with causal Transformers, extending them from discrete to continuous token generation without requiring architectural modifications. Experiments demonstrate that FAR achieves $2.3\times$ faster inference than MAR while maintaining competitive FID and IS scores. This work establishes the first efficient autoregressive paradigm for high-fidelity continuous-space image generation, bridging the critical gap between quality and scalability in visual autoregressive modeling.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization</title>
<link>https://arxiv.org/abs/2504.18397</link>
<guid>https://arxiv.org/abs/2504.18397</guid>
<content:encoded><![CDATA[
arXiv:2504.18397v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) reasoning greatly improves the interpretability and problem-solving abilities of multimodal large language models (MLLMs). However, existing approaches are focused on text CoT, limiting their ability to leverage visual cues. Visual CoT remains underexplored, and the only work is based on supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data and is hard to generalize to unseen cases. In this paper, we introduce Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT reasoning via preference optimization. UV-CoT performs preference comparisons between model-generated bounding boxes (one is preferred and the other is dis-preferred), eliminating the need for bounding-box annotations. We get such preference data by introducing an automatic data generation pipeline. Given an image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using a template prompt and then answers the question using each bounded region as input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these rankings serve as supervision to train the target MLLM with UV-CoT by minimizing negative log-likelihood losses. By emulating human perception--identifying key regions and reasoning based on them--UV-CoT can improve visual comprehension, particularly in spatial reasoning tasks where textual descriptions alone fall short. Our experiments on six datasets demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual and visual CoT methods. Our zero-shot testing on four unseen datasets shows the strong generalization of UV-CoT. The code is available in https://github.com/kesenzhao/UV-CoT.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection</title>
<link>https://arxiv.org/abs/2504.18419</link>
<guid>https://arxiv.org/abs/2504.18419</guid>
<content:encoded><![CDATA[
arXiv:2504.18419v1 Announce Type: new 
Abstract: We present a new way to detect 3D objects from multimodal inputs, leveraging both LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an RGB detection network and a 3D LiDAR detector. We exploit late fusion principles to reduce LiDAR False Positives, matching LiDAR detections with RGB ones by projecting the LiDAR bounding boxes on the image. We rely on cascade fusion principles to recover LiDAR False Negatives leveraging epipolar constraints and frustums generated by RGB detections of separate views. Our solution can be plugged on top of any underlying single-modal detectors, enabling a flexible training process that can take advantage of pre-trained LiDAR and RGB detectors, or train the two branches separately. We evaluate our results on the KITTI object detection benchmark, showing significant performance improvements, especially for the detection of Pedestrians and Cyclists.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning</title>
<link>https://arxiv.org/abs/2504.18424</link>
<guid>https://arxiv.org/abs/2504.18424</guid>
<content:encoded><![CDATA[
arXiv:2504.18424v1 Announce Type: new 
Abstract: We present layered ray intersections (LaRI), a new method for unseen geometry reasoning from a single image. Unlike conventional depth estimation that is limited to the visible surface, LaRI models multiple surfaces intersected by the camera rays using layered point maps. Benefiting from the compact and layered representation, LaRI enables complete, efficient, and view-aligned geometric reasoning to unify object- and scene-level tasks. We further propose to predict the ray stopping index, which identifies valid intersecting pixels and layers from LaRI's output. We build a complete training data generation pipeline for synthetic and real-world data, including 3D objects and scenes, with necessary data cleaning steps and coordination between rendering engines. As a generic method, LaRI's performance is validated in two scenarios: It yields comparable object-level results to the recent large generative model using 4% of its training data and 17% of its parameters. Meanwhile, it achieves scene-level occluded geometry reasoning in only one feed-forward.
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Event-based Motion Segmentation by Variational Contrast Maximization</title>
<link>https://arxiv.org/abs/2504.18447</link>
<guid>https://arxiv.org/abs/2504.18447</guid>
<content:encoded><![CDATA[
arXiv:2504.18447v1 Announce Type: new 
Abstract: Event cameras provide rich signals that are suitable for motion estimation since they respond to changes in the scene. As any visual changes in the scene produce event data, it is paramount to classify the data into different motions (i.e., motion segmentation), which is useful for various tasks such as object detection and visual servoing. We propose an iterative motion segmentation method, by classifying events into background (e.g., dominant motion hypothesis) and foreground (independent motion residuals), thus extending the Contrast Maximization framework. Experimental results demonstrate that the proposed method successfully classifies event clusters both for public and self-recorded datasets, producing sharp, motion-compensated edge-like images. The proposed method achieves state-of-the-art accuracy on moving object detection benchmarks with an improvement of over 30%, and demonstrates its possibility of applying to more complex and noisy real-world scenes. We hope this work broadens the sensitivity of Contrast Maximization with respect to both motion parameters and input events, thus contributing to theoretical advancements in event-based motion segmentation estimation. https://github.com/aoki-media-lab/event_based_segmentation_vcmax
]]></content:encoded>
<pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>