<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>


<item>
<title>WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing</title>
<link>https://arxiv.org/abs/2512.00387</link>
<guid>https://arxiv.org/abs/2512.00387</guid>
<content:encoded><![CDATA[
<div> Keywords: WiseEdit, image editing, cognitive creativity, knowledge-intensive benchmark, evaluation

<br /><br />Summary:  
The paper introduces WiseEdit, a novel knowledge-intensive benchmark designed to comprehensively evaluate advanced image editing models that incorporate cognition and creativity. Existing benchmarks are criticized for their narrow scope and inability to fully assess these sophisticated abilities. WiseEdit addresses this gap by structuring image editing tasks into three cascaded cognitive steps: Awareness, Interpretation, and Imagination, each representing distinct challenges for model evaluation. The benchmark additionally includes complex tasks that require the integrated use of these three steps, pushing models beyond simple editing capabilities. WiseEdit incorporates three fundamental types of knowledge—Declarative, Procedural, and Metacognitive—providing broad knowledge coverage to test models' reasoning and creativity. In total, the benchmark contains 1,220 test cases, enabling a detailed and objective assessment of state-of-the-art image editing models’ limitations in cognitive reasoning and creative composition. The benchmark, along with evaluation code and example generated images from various models, will be made publicly available soon. This work aims to push forward research in intelligent image editing by offering a deep and broad evaluation framework inspired by human cognitive processes in creativity. Further details and resources are accessible on the project webpage. <div>
arXiv:2512.00387v2 Announce Type: replace 
Abstract: Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction</title>
<link>https://arxiv.org/abs/2512.00960</link>
<guid>https://arxiv.org/abs/2512.00960</guid>
<content:encoded><![CDATA[
<div> 4D HOI reconstruction, human-object interaction, monocular videos, human-in-the-loop, dataset

<br /><br />Summary: This paper addresses the challenge of reconstructing accurate and scalable 4D human-object interaction (HOI) data from diverse and large-scale monocular internet videos, which are abundant but difficult to analyze. The authors propose 4DHOISolver, an efficient optimization framework that integrates sparse human-in-the-loop contact point annotations to constrain the ill-posed 4D HOI reconstruction problem, ensuring high spatio-temporal coherence and physical plausibility. Using this method, they introduce Open4DHOI, a large-scale dataset featuring 144 object types and 103 different actions, offering a rich variety for learning generalized robot interactions. The effectiveness of the reconstructed 4D interactions is validated by enabling a reinforcement learning agent to imitate the recovered human-object manipulation motions. Despite these advances, the paper highlights that current 3D foundation models fall short in automatically predicting precise human-object contact correspondences, making this an open problem for the community. This limitation justifies the human-in-the-loop annotation strategy employed. Data and code for 4DHOISolver and Open4DHOI will be made publicly available, advancing research in robotics and HOI understanding from in-the-wild videos. <div>
arXiv:2512.00960v2 Announce Type: replace 
Abstract: Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-ACT: Learn from Multimodal Parallel Generation to Act</title>
<link>https://arxiv.org/abs/2512.00975</link>
<guid>https://arxiv.org/abs/2512.00975</guid>
<content:encoded><![CDATA[
<div> Keywords: generalist robotic policy, Vision-Language-Action model, multimodal learning, cross-modal learning, robotic task performance<br /><br />Summary:<br /><br />This paper introduces MM-ACT, a unified Vision-Language-Action (VLA) model designed for generalist robotic policies that require semantic understanding and predictive interaction capabilities. The model integrates text, image, and action data within a shared token space and supports generation across these three modalities. To enhance generation efficiency, MM-ACT utilizes a re-mask parallel decoding strategy for text and image outputs and a one-step parallel decoding strategy for action generation. The novel training method, called Context-Shared Multimodal Learning, supervises generation across modalities from a unified contextual representation, boosting the quality of action generation through cross-modal information exchange. The model’s performance was validated on three benchmarks: the LIBERO simulation, real-world Franka robot tasks, and RoboTwin2.0 for bimanual task execution. MM-ACT achieved impressive success rates of 96.3% on LIBERO, 72.0% on three Franka robot tasks, and 52.38% on eight RoboTwin2.0 tasks, demonstrating strong in-domain and out-of-domain capabilities. Notably, the cross-modal learning paradigm contributed an additional 9.25% improvement in task success rates. The authors have made their codes, models, and datasets publicly available at the provided GitHub repository to facilitate further research and development in robotic generalist policies. <div>
arXiv:2512.00975v2 Announce Type: replace 
Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy</title>
<link>https://arxiv.org/abs/2512.01302</link>
<guid>https://arxiv.org/abs/2512.01302</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, Multi-Modal Diffusion Transformers, divide-and-conquer, attention masks, noise initialization<br /><br />Summary:<br /><br />This paper addresses the challenge of generating accurate long or multiple texts in text-to-image models, which typically suffer from diluted global attention. The authors propose DCText, a training-free method based on a divide-and-conquer approach that leverages Multi-Modal Diffusion Transformers known for reliable short-text generation. DCText first decomposes complex prompts by extracting and dividing the target text into segments, each assigned to specific regions in the image. To ensure each text segment is accurately rendered and the overall image remains coherent, two novel attention masks, Text-Focus and Context-Expansion, are introduced and applied sequentially during the denoising process. Furthermore, the method employs Localized Noise Initialization to enhance text accuracy and alignment of regions without increasing computational overhead. Extensive experiments on both single- and multi-sentence benchmarks demonstrate that DCText achieves superior text accuracy compared to existing approaches, while maintaining high image quality. Additionally, DCText outperforms others in generation speed, offering the lowest latency. This technique provides an effective and efficient solution to improve text rendering in text-to-image synthesis without requiring additional training. <div>
arXiv:2512.01302v2 Announce Type: replace 
Abstract: Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians</title>
<link>https://arxiv.org/abs/2512.01306</link>
<guid>https://arxiv.org/abs/2512.01306</guid>
<content:encoded><![CDATA[
<div> Gaussian Swaying, aerodynamic simulation, 3D Gaussians, surface-based framework, realistic motion<br /><br />Summary:<br /><br />This paper introduces Gaussian Swaying, a novel surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike traditional mesh-based methods that require complex and computationally expensive meshing, or particle-based techniques which depend on discrete positional data, this approach models surfaces continuously with 3D Gaussian patches. This continuous representation enables efficient and fine-grained aerodynamic interactions that enhance realism in simulating natural motions such as branches swaying, flags rippling, and boats rocking. A key advantage of the method is its unification of simulation and rendering on the same representation, where Gaussian patches facilitate both force computations for dynamics and normal calculations for lightweight shading simultaneously. The framework is tested across synthetic and real-world datasets and evaluates multiple performance metrics to demonstrate its effectiveness. Results show that Gaussian Swaying achieves state-of-the-art performance and computational efficiency, making it a scalable solution for realistic aerodynamic scene simulation. The research highlights how this method advances the realism and efficiency of aerodynamic effects critical for applications in vision and graphics. <div>
arXiv:2512.01306v2 Announce Type: replace 
Abstract: Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</title>
<link>https://arxiv.org/abs/2512.01821</link>
<guid>https://arxiv.org/abs/2512.01821</guid>
<content:encoded><![CDATA[
<div> Spatial reasoning, Multimodal Large Language Models, Implicit spatial modeling, Relative Positional Encoding, Geometry-aware dataset<br /><br />Summary:<br /><br />This paper addresses the challenge of spatial reasoning in Multimodal Large Language Models (MLLMs), highlighting the limitation of current methods that rely solely on verbal descriptive tuning, leading to visual illiteracy. To overcome this, the authors propose MILO, an implicit spatial world modeling paradigm that enables human-like spatial imagination by integrating a visual generator to provide geometry-aware feedback. This integration grounds symbolic reasoning in perceptual visual experience rather than purely textual symbols. Alongside MILO, the study introduces RePE (Relative Positional Encoding), a novel encoding method that captures relative camera-pose transformations, which proves more effective than absolute coordinate systems for spatial understanding. To facilitate training, the authors develop GeoGen, a large-scale dataset featuring around 2,241 videos and 67,827 observation-action-outcome triplets, designed to be geometry-aware and generative. Experimental results across multiple baselines and benchmarks demonstrate that this combined approach significantly improves the spatial reasoning capabilities of MLLMs, enabling a more comprehensive and holistic understanding of 3D spaces. This work represents an important advancement in bridging symbolic spatial concepts and perceptual visual grounding in multimodal AI systems. <div>
arXiv:2512.01821v2 Announce Type: replace 
Abstract: Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPTArena: A Benchmark for Agentic PowerPoint Editing</title>
<link>https://arxiv.org/abs/2512.03042</link>
<guid>https://arxiv.org/abs/2512.03042</guid>
<content:encoded><![CDATA[
<div> PowerPoint editing, benchmark, PPTArena, PPTPilot, visual fidelity<br /><br />Summary:<br /><br />1. The paper introduces PPTArena, a new benchmark designed to evaluate reliable in-place editing of real PowerPoint slides guided by natural-language instructions. <br /><br />2. Unlike previous approaches focusing on image-PDF renderings or text-to-slide generation, PPTArena emphasizes detailed edits within existing slides covering text, charts, tables, animations, and master-level styles. It comprises 100 presentation decks, 2125 slides, and over 800 targeted edits. <br /><br />3. Each task in PPTArena includes a ground-truth deck along with a fully specified target outcome and utilizes a dual vision-language model (VLM) judge pipeline that separately assesses instruction adherence and visual quality using structural differences and slide image comparisons. <br /><br />4. The authors present PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences and dynamically routes between high-level programmatic tools and deterministic XML operations for precise modifications. It verifies results through an iterative plan-edit-check loop against task constraints. <br /><br />5. Experimental results demonstrate that PPTPilot outperforms strong proprietary and state-of-the-art VLM-based systems by over 10 percentage points, particularly excelling in visual fidelity, layout-sensitive edits, and deck-wide consistency. Nonetheless, long-horizon, document-scale editing tasks in PPTArena remain challenging for all existing agents, indicating significant room for future improvement in reliable automated PPT editing. <div>
arXiv:2512.03042v2 Announce Type: replace 
Abstract: We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices</title>
<link>https://arxiv.org/abs/2512.05969</link>
<guid>https://arxiv.org/abs/2512.05969</guid>
<content:encoded><![CDATA[
<div> Keywords: video generation models, reasoning, Task Pair design, automated evaluation, reinforcement learning<br /><br />Summary:<br /><br />This paper demonstrates that video generation models have advanced to a stage where they can perform reasoning tasks. The models were tested on diverse reasoning challenges such as chess, maze navigation, Sudoku, mental rotation, and Raven’s Matrices, with leading models like Sora-2 achieving success rates around sixty percent. To facilitate systematic evaluation, the authors introduce a robust experimental paradigm called the "Task Pair" design, which pairs related tasks to test reasoning ability. They developed a code framework supporting 39 pre-integrated models and allowing easy addition of new models and tasks, ensuring scalability and extensibility. The study validates that their automated evaluation metrics strongly correlate with human judgment, indicating that this evaluation approach is reliable and scalable. With this foundation, the paper suggests that reinforcement learning can be applied to further improve the reasoning capabilities of video generation models. The authors provide open access to their raw results via a public website and share their entire evaluation codebase, VMEvalKit, on GitHub, encouraging community involvement and further research in this area. <div>
arXiv:2512.05969v1 Announce Type: new 
Abstract: We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the "Task Pair" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Dataset Quantization: A New Direction for Dataset Pruning</title>
<link>https://arxiv.org/abs/2512.05987</link>
<guid>https://arxiv.org/abs/2512.05987</guid>
<content:encoded><![CDATA[
<div> Dataset Quantization, Intra-sample Redundancy, Adaptive Quantization, Edge Devices, Compression

<br /><br />Summary:  
This paper proposes a novel dataset quantization approach aimed at reducing storage and communication costs for large-scale datasets, particularly in resource-constrained edge devices. Unlike traditional methods that address inter-sample redundancy, the presented technique focuses on compressing each image by reducing intra-sample redundancy, i.e., redundant or less informative content within individual samples while preserving essential features. The method first uses linear symmetric quantization to determine an initial quantization range and scale for each sample. Following this, an adaptive quantization allocation algorithm is employed to assign different quantization ratios according to samples' precision requirements, ensuring a constant total compression ratio across the dataset. Key contributions include being the first to represent datasets with limited bits for significant storage reduction, introducing a dataset-level quantization algorithm with adaptive ratio allocation, and performing extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K datasets. The results demonstrate that this approach maintains model training performance while achieving substantial compression, outperforming conventional quantization and dataset pruning techniques at equivalent compression ratios. This work highlights the potential for effective dataset compression strategies tailored to edge computing scenarios without sacrificing training accuracy. <div>
arXiv:2512.05987v1 Announce Type: new 
Abstract: This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VG3T: Visual Geometry Grounded Gaussian Transformer</title>
<link>https://arxiv.org/abs/2512.05988</link>
<guid>https://arxiv.org/abs/2512.05988</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D semantic occupancy, multi-view fusion, Gaussian representation, Grid-Based Sampling, Positional Refinement<br /><br />Summary:<br /><br />1. The paper addresses the challenge of creating coherent 3D scene representations from multiple images taken at different viewpoints, which is a fundamental problem in 3D reconstruction and scene understanding.<br />2. Existing methods often fail due to fragmented and inconsistent 3D representations caused by processing each view independently, limiting the overall quality and coherence of the 3D output.<br />3. The authors propose VG3T, a novel multi-view feed-forward neural network that predicts a 3D semantic occupancy by directly modeling a set of semantically labeled 3D Gaussian primitives, integrating information from all views jointly rather than view-by-view.<br />4. Two key innovations, Grid-Based Sampling and Positional Refinement, are introduced to reduce the typical distance-dependent density bias arising from pixel-aligned initializations of Gaussian primitives, improving geometric and semantic accuracy.<br />5. VG3T demonstrates superior performance on the challenging nuScenes dataset by achieving a 1.7% absolute improvement in mean Intersection over Union (mIoU) while using 46% fewer Gaussian primitives than the previous state-of-the-art, indicating enhanced efficiency and representational power. <div>
arXiv:2512.05988v1 Announce Type: new 
Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head</title>
<link>https://arxiv.org/abs/2512.05991</link>
<guid>https://arxiv.org/abs/2512.05991</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, emotional expression, Emotion-aware Gaussian Diffusion, action unit prompt, text-to-AU emotion controller<br /><br />Summary:<br /><br />This paper addresses the limitations of existing photo-realistic 3D talking head models based on 3D Gaussian Splatting, particularly in manipulating emotional expressions with fine granularity and expansive dynamics via multimodal control. The authors propose EmoDiffTalk, an editable 3D Gaussian talking head framework that introduces a novel Emotion-aware Gaussian Diffusion mechanism. This mechanism incorporates an action unit (AU) prompt Gaussian diffusion process designed for detailed facial animation, enabling precise control over subtle emotional expressions. Furthermore, EmoDiffTalk integrates an accurate text-to-AU emotion controller, allowing expressive and dynamic emotional editing driven by text input. The system has been evaluated on public datasets EmoTalk3D and RenderMe-360, where it demonstrates superior performance in emotional subtlety, lip-sync accuracy, and overall controllability compared to prior works. EmoDiffTalk sets a new standard for high-quality, diffusion-based multimodal 3D talking head synthesis. Notably, this framework is among the first to leverage 3D Gaussian Splatting for talking-head generation that supports continuous and multimodal emotional editing within the AU-based expression space, paving the way for advanced, editable 3D facial animation technologies. <div>
arXiv:2512.05991v1 Announce Type: new 
Abstract: Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology</title>
<link>https://arxiv.org/abs/2512.05993</link>
<guid>https://arxiv.org/abs/2512.05993</guid>
<content:encoded><![CDATA[
<div> Neuropathology, foundation models, neurodegenerative diseases, whole-slide images, domain-specific AI  

<br /><br />Summary:  
1. Foundation models have revolutionized computational pathology by learning from large histology datasets but are mainly trained on surgical pathology data, which does not represent nervous tissue well.  
2. Neuropathology involves distinct tissue types and pathological features, such as neurons, glia, neurofibrillary tangles, amyloid plaques, Lewy bodies, and unique neurodegeneration patterns, which are not adequately captured by general-purpose models.  
3. To bridge this domain gap, the authors developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue covering various neurodegenerative diseases.  
4. NeuroFM outperforms general foundation models in neuropathology-specific tasks including mixed dementia classification, segmentation of the hippocampal region, and identification of neurodegenerative ataxias such as cerebellar essential tremor and spinocerebellar ataxia subtypes.  
5. This study demonstrates that domain-specialized foundation models tailored to brain pathology provide more accurate and reliable analyses for diagnosing and researching neurodegenerative diseases, setting a precedent for future development of specialized AI models in digital pathology. <div>
arXiv:2512.05993v1 Announce Type: new 
Abstract: Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting</title>
<link>https://arxiv.org/abs/2512.05996</link>
<guid>https://arxiv.org/abs/2512.05996</guid>
<content:encoded><![CDATA[
<div> Keywords: Fish detection, Underwater imagery, Weak supervision, Reinforcement learning, Marine ecology

<br /><br />Summary: Analyzing underwater fish imagery presents challenges due to visual degradation and the high cost of detailed annotations. The paper introduces FishDetector-R1, a novel multi-modal large model (MLLM)-based framework designed to detect, segment, and count fish using weak supervision. Evaluated on the DeepFish dataset, FishDetector-R1 shows significant performance improvements, achieving a 20% increase in Average Precision (AP), 10% higher mean Intersection over Union (mIoU), a 30% reduction in Mean Absolute Error (MAE), and a 35% decrease in the Grid Average Mean Absolute Error (GAME). These gains are attributed to two core innovations: a detect-to-count prompt that ensures spatially consistent fish detections and counts, and a Reinforcement Learning from Verifiable Reward (RLVR) method that uses sparse point labels within a scalable reward framework. Ablation studies confirm the effectiveness of the designed reward system. Additionally, FishDetector-R1 demonstrates strong cross-domain generalization capabilities on other underwater datasets, highlighting its robustness. Overall, this approach offers a reliable and scalable solution to improve marine visual ecological monitoring through weakly supervised learning, reducing annotation costs while maintaining high accuracy. More details and resources are available at the project page: https://umfieldrobotics.github.io/FishDetector-R1. <div>
arXiv:2512.05996v1 Announce Type: new 
Abstract: Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrunedCaps: A Case For Primary Capsules Discrimination</title>
<link>https://arxiv.org/abs/2512.06003</link>
<guid>https://arxiv.org/abs/2512.06003</guid>
<content:encoded><![CDATA[
<div> Capsule Networks, Pruning, Primary Capsules, Dynamic Routing, Computational Efficiency<br /><br />Summary:<br /><br />This paper addresses the inefficiency of Capsule Networks (CapsNets) caused by the large number of Primary Capsules (PCs), which results in slow training and testing phases. The authors propose pruning the Primary Capsules to enhance the resource efficiency of CapsNets. They conduct experiments on four datasets: MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN. The results demonstrate that pruning up to 95 percent of the Capsules significantly accelerates the CapsNet, achieving up to 9.90 times faster performance compared to the original architecture without any drop in accuracy. Additionally, the pruned CapsNet reduces floating-point operations in the dynamic routing stage by more than 95.36 percent, highlighting substantial computational savings. The study further explores the varying impact of pruning on different datasets, providing insights into why some datasets benefit more from pruning than others. Overall, this research offers a practical approach to make CapsNets more resource-efficient while maintaining their advantages, contributing to faster and less resource-intensive deep learning models suitable for image classification tasks. <div>
arXiv:2512.06003v1 Announce Type: new 
Abstract: Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization</title>
<link>https://arxiv.org/abs/2512.06006</link>
<guid>https://arxiv.org/abs/2512.06006</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, code adaptation, biomedical imaging, agent design, production pipeline<br /><br />Summary:<br /><br />1. The article addresses the challenge of adapting production-level computer vision tools to specialized scientific datasets, which currently faces significant bottlenecks due to the need for large annotated datasets for fine-tuning or labor-intensive manual code changes.  
2. It proposes the use of AI agents to automate the manual coding process required for adapting these tools, aiming to streamline and accelerate the adaptation to bespoke scientific datasets.  
3. A systematic evaluation framework for agentic code optimization is introduced, enabling objective assessment of different AI agent designs on this task.  
4. The framework is applied to three biomedical imaging pipelines, demonstrating that a simple AI agent architecture can consistently generate adaptation code that outperforms solutions produced by human experts.  
5. The study finds that more complex agent architectures do not always offer advantages, offering practical insights and a roadmap for optimal agent design.  
6. The framework and approach have been open-sourced, and the authors validate their method by successfully deploying agent-generated functions into a real production pipeline, illustrating clear potential for real-world scientific impact. <div>
arXiv:2512.06006v1 Announce Type: new 
Abstract: Adapting production-level computer vision tools to bespoke scientific datasets is a critical "last mile" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Flexible Robustness Certificates for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06010</link>
<guid>https://arxiv.org/abs/2512.06010</guid>
<content:encoded><![CDATA[
<div> Keywords: certifiable robustness, semantic segmentation, Lipschitz constraints, adversarial attacks, randomized smoothing<br /><br />Summary: This paper addresses the vulnerability of deep neural networks to small adversarial perturbations, focusing on semantic segmentation tasks rather than classification. The authors introduce a new class of semantic segmentation networks incorporating built-in Lipschitz constraints to ensure certifiable robustness. These networks are efficiently trainable and achieve competitive pixel accuracy on challenging datasets like Cityscapes. A novel framework is proposed to generalize robustness certificates for semantic segmentation, demonstrating both computational efficiency and flexibility of Lipschitz networks in providing robustness guarantees. The approach enables real-time compatible certifiably robust semantic segmentation for the first time, which is a significant advancement in practical deployment. The method also allows for the computation of worst-case performance under \(\ell_2\) adversarial attacks within a specified radius \(\epsilon\), covering a variety of performance metrics. A major highlight is the certification process's speed, which is shown to be around 600 times faster than randomized smoothing methods at inference on an NVIDIA A100 GPU while maintaining comparable certificate quality. Finally, the authors validate the tightness of their worst-case robustness certificates by benchmarking them against state-of-the-art adversarial attacks, confirming the robustness and reliability of the proposed method. <div>
arXiv:2512.06010v1 Announce Type: new 
Abstract: Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $\epsilon$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing</title>
<link>https://arxiv.org/abs/2512.06012</link>
<guid>https://arxiv.org/abs/2512.06012</guid>
<content:encoded><![CDATA[
<div> Keywords: Selective Laser Melting, powder morphology, machine learning, clustering, Fourier descriptors  

<br /><br />Summary:  
1. Selective Laser Melting (SLM) is an additive manufacturing technique whose part quality is highly dependent on the morphology of the metallic powder feedstock.  
2. Traditional powder characterization methods are low-throughput and qualitative, often failing to represent the heterogeneity found in industrial-scale powder batches.  
3. The authors propose an automated, machine learning-based framework that integrates high-throughput imaging with shape extraction and clustering to profile powder morphology at scale.  
4. Three clustering pipelines are developed and evaluated: an autoencoder-based pipeline, a shape-descriptor-based pipeline, and a functional-data-based pipeline.  
5. Using a dataset of approximately 126,000 powder particle images ranging from 0.5 to 102 micrometers, internal validity metrics such as the Davies-Bouldin index and Calinski-Harabasz score identify the Fourier-descriptor combined with k-means clustering as the most effective approach.  
6. This Fourier-descriptor + k-means pipeline achieves a very low Davies-Bouldin index and high Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop, demonstrating both effectiveness and efficiency.  
7. Although this study focuses on establishing a robust morphological clustering framework, the identified shape groups provide a foundation for future investigations into how particle shape affects powder flowability, packing density, and final SLM part quality.  
8. The unsupervised learning framework enables rapid, automated morphological assessment and allows monitoring of shape changes across powder reuse cycles, offering a path toward real-time feedstock monitoring within SLM manufacturing workflows. <div>
arXiv:2512.06012v1 Announce Type: new 
Abstract: Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAT: Vision Action Transformer by Unlocking Full Representation of ViT</title>
<link>https://arxiv.org/abs/2512.06013</link>
<guid>https://arxiv.org/abs/2512.06013</guid>
<content:encoded><![CDATA[
<div> Vision Transformers, robot learning, action tokens, imitation learning, manipulation tasks<br /><br />Summary:<br /><br />1. The paper addresses a limitation in current robot learning methods that use Vision Transformers (ViTs) by relying solely on features from the final layer, which discards valuable information.  
2. It proposes a novel architecture called Vision Action Transformer (VAT), which extends ViT by incorporating action tokens processed with visual features across all transformer layers.  
3. This design enables a deep and progressive fusion of perception and action generation, utilizing the full hierarchical representation of ViT rather than only the last layer's output.  
4. Experimental evaluation on simulated manipulation tasks across four LIBERO benchmarks shows that VAT achieves a 98.15% average success rate, outperforming previous state-of-the-art approaches such as OpenVLA-OFT.  
5. The work highlights the critical importance of leveraging the entire "representation trajectory" in vision models to enhance robotic policy learning and provides publicly available code on GitHub for reproducibility and further research. <div>
arXiv:2512.06013v1 Announce Type: new 
Abstract: In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets</title>
<link>https://arxiv.org/abs/2512.06014</link>
<guid>https://arxiv.org/abs/2512.06014</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, chest X-ray, medical imaging, embedding models, model benchmarking

<br /><br />Summary:  
This study benchmarks two large-scale chest X-ray embedding models, CXR-Foundation (ELIXR v2.0) and MedImageInsight, using public datasets MIMIC-CR and NIH ChestX-ray14. Both models were evaluated through a unified preprocessing pipeline alongside fixed downstream LightGBM classifiers to ensure reproducibility. Embeddings were extracted directly from pre-trained encoders for multiple disease label prediction tasks. Performance metrics including mean AUROC and F1-score with 95% confidence intervals were reported. Results showed MedImageInsight slightly outperformed CXR-Foundation across most tasks, while CXR-Foundation demonstrated superior cross-dataset stability. Additionally, unsupervised clustering of MedImageInsight embeddings revealed well-defined disease-specific grouping consistent with quantitative performance. The study emphasizes the importance of standardizing evaluations for medical foundation models to enable fair and reproducible comparisons. Finally, it establishes reproducible baseline performances to facilitate future research, particularly in multimodal data integration and clinical applications. <div>
arXiv:2512.06014v1 Announce Type: new 
Abstract: Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation</title>
<link>https://arxiv.org/abs/2512.06020</link>
<guid>https://arxiv.org/abs/2512.06020</guid>
<content:encoded><![CDATA[
<div> Preference-conditioned image generation, multimodal large language models, visual question answering, inter-user discrimination, diffusion-based image generation<br /><br />Summary:<br /><br />This paper addresses the challenge of adapting image generation models to individual user preferences, aiming to produce images that reflect personal aesthetics beyond the textual prompt. The authors propose a novel multimodal framework that employs multimodal large language models (MLLMs) to extract detailed user representations. These representations are introduced into diffusion-based image generators to guide output personalization. To refine preference extraction, the MLLM is trained on a preference-oriented visual question answering task, enabling it to capture subtle semantic cues. The framework includes two probing tasks: inter-user discrimination, which differentiates between users, and intra-user discrimination, which distinguishes between content liked or disliked by a user. To ensure that the user embeddings align with the diffusion model’s text encoders despite modality differences, a maximum mean discrepancy-based alignment loss is designed, preserving multimodal structure and compatibility. The resulting embeddings effectively condition the image generator, allowing it to honor both the user's preferences and the given text prompts. Extensive experiments validate the approach, showing significant improvements over existing baselines in image quality and preference alignment, demonstrating the strength of enhanced representation extraction and modality alignment for personalized image generation. <div>
arXiv:2512.06020v1 Announce Type: new 
Abstract: Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing</title>
<link>https://arxiv.org/abs/2512.06024</link>
<guid>https://arxiv.org/abs/2512.06024</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, wave free surface, velocity fields, attention-augmented neural network, ocean waves<br /><br />Summary:<br /><br />1. The paper presents a novel neural network designed for precise three-dimensional reconstruction of ocean wave free surfaces and associated velocity fields, tackling the challenge of long-term ocean wave observations.<br /><br />2. The network architecture is an attention-augmented pyramid tailored to handle the multi-scale and temporally continuous nature of wave motions, improving robustness especially under visual occlusions.<br /><br />3. Physics-based constraints are integrated into the model to enable time-resolved reconstruction of nonlinear 3D velocity fields derived from the evolving free-surface boundary.<br /><br />4. Experimental results from real-sea conditions show millimetre-level accuracy in wave elevation prediction, dominant-frequency errors below 0.01 Hz, and precise estimation of high-frequency spectral power laws.<br /><br />5. The model achieves high-fidelity 3D reconstruction of nonlinear velocity fields efficiently, capable of dense reconstruction of two million points in just 1.35 seconds, outperforming conventional visual reconstruction methods and demonstrating strong generalization even with occlusions, due to global multi-scale attention and learned wave propagation dynamics. <div>
arXiv:2512.06024v1 Announce Type: new 
Abstract: Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation</title>
<link>https://arxiv.org/abs/2512.06032</link>
<guid>https://arxiv.org/abs/2512.06032</guid>
<content:encoded><![CDATA[
<div> Segment Anything Models, SAM2, SAM3, prompt-based segmentation, concept-driven segmentation<br /><br />Summary:<br /><br />This paper highlights the fundamental discontinuity between Segment Anything Model versions SAM2 and SAM3. (1) Conceptual Break: SAM2 relies on spatial prompts like points, boxes, and masks for geometric and temporal segmentation, whereas SAM3 utilizes a multimodal fusion framework incorporating vision and language for semantic, open-vocabulary, and concept-driven segmentation. (2) Architectural Divergence: SAM2 is purely vision-temporal with spatial prompt inputs, while SAM3 integrates vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and Mixture-of-Experts to manage ambiguity. (3) Dataset and Annotation Differences: SAM2 is trained on SA-V video masks focused on geometric delineation, whereas SAM3 leverages multimodal corpora annotated with rich semantic concepts. (4) Training and Hyperparameter Distinctions: SAM2’s optimization strategies are unsuitable for SAM3 due to the latter’s complex multimodal objectives and architecture. (5) Evaluation and Failure Modes: SAM2 uses geometric Intersection-over-Union (IoU) metrics, while SAM3 requires semantic, open-vocabulary evaluation to capture concept-level performance. Overall, the analysis positions SAM3 as initiating a new class of segmentation foundation models centered on concept-driven understanding, marking a shift from traditional prompt-based segmentation and indicating future research directions in this domain. <div>
arXiv:2512.06032v1 Announce Type: new 
Abstract: This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Learning for Point Cloud Understanding</title>
<link>https://arxiv.org/abs/2512.06058</link>
<guid>https://arxiv.org/abs/2512.06058</guid>
<content:encoded><![CDATA[
<div> 3D data, point cloud, supervised learning, self-supervised learning, transfer learning<br /><br />Summary:<br /><br />This dissertation addresses the growing importance of 3D data acquisition and utilization across multiple fields such as computer vision, robotics, and geospatial analysis. It highlights the diverse methods of capturing 3D data, including 3D scanners, LiDARs, and RGB-D cameras, which provide detailed geometric, shape, and scale information. The work emphasizes the benefits of combining 3D data with 2D images to enhance machine understanding of environments, which is crucial for applications like autonomous driving, robotics, remote sensing, and medical treatment. The core focus of the research lies in three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning techniques, and transfer learning by leveraging 2D pre-trained models to improve 3D network training. Importantly, the approach does not rely on simply transforming 2D data into 3D but integrates 2D knowledge to boost 3D understanding effectively. The dissertation includes extensive experiments demonstrating the effectiveness and potential of the proposed methods to advance point cloud representation learning through the integration of 2D and 3D data. <div>
arXiv:2512.06058v1 Announce Type: new 
Abstract: With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</title>
<link>https://arxiv.org/abs/2512.06065</link>
<guid>https://arxiv.org/abs/2512.06065</guid>
<content:encoded><![CDATA[
<div> Keywords: egocentric video editing, instruction-guided, real-time inference, hand-object interactions, dataset  

<br /><br />Summary:  
This work focuses on instruction-guided editing of egocentric videos, targeting interactive augmented reality (AR) applications. Unlike existing AI video editors that perform well on third-person footage, egocentric videos present unique challenges such as rapid egomotion and frequent hand-object interactions, creating a considerable domain gap that complicates editing tasks. Traditional offline editing methods also suffer from high latency, restricting their usability for real-time interactions. To overcome these challenges, the authors introduce a comprehensive ecosystem for egocentric video editing consisting of three key components. First, they create EgoEditData, a manually curated dataset tailored to egocentric editing scenarios which emphasizes preserving hand and object interactions explicitly. Second, they develop EgoEdit, an instruction-following video editor optimized for egocentric footage that supports real-time streaming inference on a single GPU, enabling interactive latency. Third, they propose EgoEditBench, an evaluation suite designed to assess instruction faithfulness, preservation of hands and interactions, and temporal stability despite rapid egomotion. Experimental results demonstrate that EgoEdit delivers temporally stable and instruction-faithful editing outcomes, outperforming existing methods on egocentric editing benchmarks while maintaining competitive performance on general video editing tasks. EgoEditData and EgoEditBench will be publicly released to facilitate further research. <div>
arXiv:2512.06065v1 Announce Type: new 
Abstract: We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light</title>
<link>https://arxiv.org/abs/2512.06080</link>
<guid>https://arxiv.org/abs/2512.06080</guid>
<content:encoded><![CDATA[
<div> Keywords: single-photon lidar, 3D scene reconstruction, multi-bounce light, multiplexed illumination, deep learning<br /><br />Summary:<br />1. This work tackles the difficult problem of reconstructing 3D scenes, especially in the presence of occluded regions and specular materials such as mirrors, using single-photon lidar technology. <br />2. Single-photon lidars estimate depth by detecting light emitted into the scene and reflected back; uniquely, they also capture multi-bounce light, which carries rich information about occluded geometry and material properties but is challenging to interpret. <br />3. Previous efforts have only succeeded when the laser illuminates one point at a time, while this paper addresses the more practical and complex scenario of simultaneous illumination of multiple scene points, inducing complicated light transport due to multiplexing, two-bounce reflections, shadows, and specularity. <br />4. Analytical inversion of this complex light transport proves impractical, so the authors develop a data-driven approach using deep learning to decompose multi-bounce signals and recover constituent contributions from each laser spot. <br />5. To train their model, they create and release a large simulated dataset with around 100,000 lidar transients from indoor environments, and experimentally validate that their method successfully infers 3D geometry in challenging scenes from a single measurement, demonstrating improved reconstruction in occluded and mirror-containing scenes. <div>
arXiv:2512.06080v1 Announce Type: new 
Abstract: 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06096</link>
<guid>https://arxiv.org/abs/2512.06096</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Multimodal Language Models, 360-degree BEV, autonomous driving, spatial reasoning  

<br /><br />Summary:  
The paper introduces BeLLA, an innovative end-to-end architecture designed for autonomous driving that integrates unified 360-degree Bird's Eye View (BEV) spatial representations with a large language model to enhance question answering capabilities. Traditional approaches often rely on single-view encoders or aggregated multi-view features, which either fail to fully utilize multi-camera spatial structures or lack a unified spatial framework, limiting effective reasoning about ego-centric directions and object relationships. BeLLA addresses these limitations by providing a consistent and comprehensive spatial representation, enabling more accurate understanding of relative object positions and contextual behavioral patterns crucial for autonomous navigation. The model is rigorously evaluated on two benchmarks—NuScenes-QA and DriveLM—where it demonstrates substantial improvements, especially in tasks demanding complex spatial reasoning, achieving up to a 9.3% absolute gain over prior methods. Additionally, BeLLA remains competitive across a broad spectrum of question types, showcasing its versatility and robustness. Overall, this work advances the integration of vision and language models in driving contexts, pushing forward the interpretability and contextual awareness of autonomous vehicle systems. <div>
arXiv:2512.06096v1 Announce Type: new 
Abstract: The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360{\deg} BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2512.06103</link>
<guid>https://arxiv.org/abs/2512.06103</guid>
<content:encoded><![CDATA[
<div> Keywords: Iris recognition, Presentation Attack Detection, Multispectral imaging, Vision Transformer, Spoofing detection

<br /><br />Summary: Iris recognition is a highly accurate biometric modality but is vulnerable to presentation attacks (PAs), making effective presentation attack detection (PAD) essential for security. Traditional iris systems typically use near-infrared (NIR) imaging, but multispectral imaging across multiple NIR bands can enhance PAD generalizability by providing complementary reflectance information. This work introduces SpectraIrisPAD, a novel deep learning framework based on a DINOv2 Vision Transformer backbone, which incorporates learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features for robust PAD. To support this, the authors developed MSIrPAD, a new comprehensive multispectral iris PAD dataset containing 18,848 images across eight diverse attack types, including textured contact lenses, print attacks, and display-based attacks, captured at five NIR wavelengths (800 nm, 830 nm, 850 nm, 870 nm, and 980 nm) using a custom multispectral sensor. Extensive experiments under unseen attack scenarios demonstrate that SpectraIrisPAD consistently outperforms state-of-the-art methods on multiple metrics, showcasing its superior robustness and generalization capabilities in detecting a wide variety of iris presentation attacks. <div>
arXiv:2512.06103v1 Announce Type: new 
Abstract: Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\,nm, 830\,nm, 850\,nm, 870\,nm, and 980\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation</title>
<link>https://arxiv.org/abs/2512.06105</link>
<guid>https://arxiv.org/abs/2512.06105</guid>
<content:encoded><![CDATA[
<div> Keywords: melanoma classification, interpretability, contrastive learning, Vision Transformer, clinical trust

<br /><br />Summary: This paper addresses the challenge of model opacity in deep learning-based melanoma classification, which hinders clinical adoption despite high accuracy. The authors propose a Cross-modal Explainable Framework for Melanoma (CEFM) that integrates clinical diagnostic criteria—Asymmetry, Border, and Color (ABC)—directly into the model's embedding space using contrastive learning. By employing dual projection heads within a Vision Transformer architecture, CEFM aligns visual features with clinical semantics, fostering interpretability. These aligned features are then converted into structured textual explanations through natural language generation, creating a transparent connection between raw images and clinical insights. Experimental results on public melanoma datasets show that CEFM achieves a classification accuracy of 92.79% and an AUC of 0.961. The framework also demonstrates significant improvements in interpretability metrics compared to baseline methods. Qualitative analyses further reveal that the spatial arrangement of learned embeddings corresponds well with clinicians’ use of the ABC rule, effectively enhancing trust and understanding of the model's decisions. Overall, CEFM successfully bridges the gap between black-box high-performance melanoma classification models and clinically reliable, interpretable tools for dermatology practice. <div>
arXiv:2512.06105v1 Announce Type: new 
Abstract: Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation</title>
<link>https://arxiv.org/abs/2512.06158</link>
<guid>https://arxiv.org/abs/2512.06158</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D generation, multi-view video diffusion, point tracking, Gaussian Splatting, dynamic objects  

<br /><br />Summary:  
Generating dynamic 4D objects from sparse multi-view inputs is challenging due to the need for preserving both appearance and motion consistency across views and over time, while avoiding artifacts and temporal drift. The paper identifies an important limitation in existing methods, namely that supervision often relies on pixel- or latent-space video diffusion losses, which do not provide explicit temporally aware, feature-level tracking guidance. To address this, the authors propose Track4DGen, a two-stage framework that integrates a multi-view video diffusion model with a foundational point tracker and a hybrid 4D Gaussian Splatting reconstructor. In the first stage, dense, feature-level point correspondences derived from the tracker are enforced within the diffusion generator to produce temporally consistent features that reduce appearance drift and improve coherence across views. In the second stage, a dynamic 4D Gaussian Splatting model is reconstructed by combining diffusion features carrying the Stage One tracking priors with Hex-plane features, augmented with 4D Spherical Harmonics to better model complex dynamics. Experimentally, Track4DGen outperforms baseline methods on benchmarks for both multi-view video generation and 4D object generation, producing temporally stable and text-editable 4D assets. Additionally, the authors contribute Sketchfab28, a new high-quality, object-centric 4D generation dataset to support future research in this domain. <div>
arXiv:2512.06158v1 Announce Type: new 
Abstract: Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection</title>
<link>https://arxiv.org/abs/2512.06171</link>
<guid>https://arxiv.org/abs/2512.06171</guid>
<content:encoded><![CDATA[
<div> Shearography, defect detection, deep learning, annotation automation, segmentation<br /><br />Summary:<br /><br />1. Shearography is an interferometric method that measures surface displacement gradients, making it highly sensitive for identifying subsurface defects in safety-critical components. 2. A significant barrier to deploying shearography in industrial environments is the absence of extensive, high-quality annotated datasets. Manual annotation is not only time-consuming but also subjective and difficult to standardize across different operators. 3. To address this challenge, the authors propose an automated workflow that leverages deep learning techniques to generate defect annotations directly from shearography data. 4. This workflow produces detailed, high-resolution segmentation masks and bounding-box labels that correspond to defects detected in shearography images. 5. Validation against expert-labeled datasets demonstrates that the automated annotation achieves a level of accuracy adequate for use in weakly supervised training schemes, minimizing the dependency on manual labeling. Overall, this approach facilitates scalable and standardized dataset creation, which in turn supports more robust and efficient defect detection in industrial shearography applications. <div>
arXiv:2512.06171v1 Announce Type: new 
Abstract: Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction</title>
<link>https://arxiv.org/abs/2512.06174</link>
<guid>https://arxiv.org/abs/2512.06174</guid>
<content:encoded><![CDATA[
<div> Keywords: shadow generation, physical modeling, deep learning, 3D geometry, illumination<br /><br />Summary:  
This paper addresses shadow generation with a focus on producing photorealistic shadows consistent with object geometry and scene illumination. It identifies a gap in existing deep-learning shadow generation methods that rarely incorporate explicit physical modeling based on the physics of shadow formation. The authors propose a novel framework embedding explicit physical modeling of both geometry and illumination into a deep learning pipeline. Starting from a single RGB image, their method estimates approximate 3D geometry represented as dense point maps and predicts a dominant light direction. Using these signals, the framework computes an initial shadow location and shape that adhere to physical principles of shadow casting. This physics-based shadow estimate is then refined within a diffusion model, enhancing realism and ensuring the final shadow maintains coherence with scene geometry and lighting. The model is trained on the DESOBAV2 dataset and demonstrates improved results over prior approaches, notably in challenging scenarios involving complex geometry or ambiguous lighting conditions. Overall, the approach successfully integrates physical insights with data-driven techniques to enhance shadow generation quality and physical correctness. <div>
arXiv:2512.06174v1 Announce Type: new 
Abstract: Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction</title>
<link>https://arxiv.org/abs/2512.06179</link>
<guid>https://arxiv.org/abs/2512.06179</guid>
<content:encoded><![CDATA[
<div> Keywords: attached shadows, cast shadows, shadow detection, light estimation, geometry-illumination reasoning  

<br /><br />Summary:  
Attached shadows occur on surfaces where light is blocked due to self-occlusion, playing a vital role in perceiving 3D structure and improving scene understanding. Despite their importance, current shadow detection research primarily focuses on cast shadows, lacking specialized datasets and models for attached shadows. To bridge this gap, the authors propose a novel framework that jointly detects both cast and attached shadows by leveraging the interplay between shadows, scene illumination, and geometry. The system comprises a shadow detection module that separately predicts cast and attached shadows, alongside a light estimation module that deduces the light direction from shadow cues. Using the estimated light direction with surface normals, the framework generates a geometry-consistent partial map, highlighting regions prone to self-occlusion. This map is fed back to refine shadow predictions, creating an iterative closed-loop process that enhances both shadow segmentation and light estimation simultaneously. The authors also constructed a dedicated dataset containing 1,458 images with distinct annotations for cast and attached shadows to facilitate robust training and evaluation. Experimental results demonstrate that this geometry-illumination iterative reasoning approach significantly improves attached shadow detection performance, achieving at least a 33% reduction in Balanced Error Rate (BER) while preserving accuracy in detecting cast and full shadows. <div>
arXiv:2512.06179v1 Announce Type: new 
Abstract: Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling</title>
<link>https://arxiv.org/abs/2512.06185</link>
<guid>https://arxiv.org/abs/2512.06185</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, fooling images, black-box attack, vision transformers, model robustness<br /><br />Summary:<br /><br />1. This study revisits the concept of "fooling images," initially proposed by Nguyen et al. (2015), which are inputs that cause deep neural networks (DNNs) to produce high-confidence incorrect classifications despite bearing no resemblance to natural images. <br /><br />2. The authors re-implement evolutionary fooling attacks using both CPPN-based and direct-encoding-based approaches on modern architectures, including convolutional neural networks and transformer models like ViT-B/16. Their findings confirm that state-of-the-art models remain vulnerable, with the ViT-B/16 transformer being the most susceptible to quick and near-certain misclassifications.<br /><br />3. Introducing SPOOF, a new minimalist and efficient black-box attack, the paper demonstrates that it can generate high-confidence fooling images with minimal pixel changes and significantly lower computation compared to previous methods.<br /><br />4. The research explores robustness improvements by retraining models with fooling images as an additional class. This method only provides limited defense, as SPOOF continues to produce effective fooling images, albeit with a slightly increased number of queries.<br /><br />5. The results highlight the persistent vulnerability and fragility of current deep learning classifiers, including advanced transformer architectures, against adversarial images crafted to deceive them consistently. <div>
arXiv:2512.06185v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the "fooling images" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying</title>
<link>https://arxiv.org/abs/2512.06190</link>
<guid>https://arxiv.org/abs/2512.06190</guid>
<content:encoded><![CDATA[
<div> Keywords: food drying, color trajectory, multi-modal prediction, drying process parameters, model accuracy<br /><br />Summary:<br /><br />Food drying is extensively employed to reduce moisture, enhance safety, and prolong the shelf life of food products. Monitoring the color evolution of food samples is crucial as it serves as an important quality indicator during drying. Existing research has mainly focused on analyzing low-dimensional color features under varying drying conditions, which limits the ability to fully characterize the complex and dynamic color changes that occur. Additionally, current modeling techniques struggle to generalize predictions to drying conditions not previously encountered. To overcome these challenges, the authors propose a novel multi-modal color-trajectory prediction approach that integrates high-dimensional temporal color data with drying process parameters. This integration enables the accurate and data-efficient prediction of color trajectories in food drying. The model’s performance was evaluated on cookie and apple drying tasks under unseen drying conditions, achieving root mean squared errors (RMSE) of 2.12 and 1.29, respectively. This represents a reduction in prediction errors by over 90% compared to baseline models. The experimental results confirm the approach’s superior accuracy, robustness, and broad applicability across different drying scenarios, highlighting its potential for improving food drying quality control. <div>
arXiv:2512.06190v1 Announce Type: new 
Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning</title>
<link>https://arxiv.org/abs/2512.06206</link>
<guid>https://arxiv.org/abs/2512.06206</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Glioma Segmentation, MRI, PID Controller, Medical Imaging  

<br /><br />Summary:  
1. The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024 focused on federated learning (FL) for segmenting glioma sub-regions in multi-parametric MRI scans.  
2. The challenge used a multi-institutional dataset from the BraTS glioma benchmark, including 1,251 training cases, 219 validation cases, and 570 hidden test cases, with segmentation labels for enhancing tumor (ET), tumor core (TC), and whole tumor (WT).  
3. Six teams participated and were evaluated under a standardized federated learning framework using a cumulative scoring system that combined segmentation accuracy (Dice Similarity Coefficient and 95th percentile Hausdorff Distance) and communication efficiency (convergence score).  
4. The winning team employed a PID-controller-based weight aggregation method, achieving the highest overall ranking with mean DSCs of 0.733 (ET), 0.761 (TC), and 0.751 (WT), alongside high HD95 values (around 32-34 mm), and the best communication efficiency with a convergence score of 0.764.  
5. Results demonstrate advancements over previous challenge iterations, highlighting the effectiveness of PID controllers in stabilizing and optimizing weight aggregation in federated learning for medical imaging. The challenge code is openly available at https://github.com/FeTS-AI/Challenge. <div>
arXiv:2512.06206v1 Announce Type: new 
Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study</title>
<link>https://arxiv.org/abs/2512.06221</link>
<guid>https://arxiv.org/abs/2512.06221</guid>
<content:encoded><![CDATA[
<div> SVD, WDR, image compression, reproducibility, JPEG2000<br /><br />Summary:<br /><br />This article presents an independent reproducibility study focused on a lossy image compression technique that combines Singular Value Decomposition (SVD) and Wavelet Difference Reduction (WDR). The original publication claimed that this combined SVD+WDR method outperforms both JPEG2000 and standalone WDR in terms of visual quality and compression ratio. To verify these claims, the author re-implemented the technique, carefully addressing missing implementation details such as quantization and threshold initialization, which were ambiguously described in the original paper. The study replicated the original experiments as closely as possible and extended the evaluation to new image datasets. Performance was assessed using standard metrics, Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM). Contrary to the original claims, findings indicate that SVD+WDR does not generally exceed JPEG2000 or WDR alone in PSNR, and only shows limited improvement in SSIM when compared to JPEG2000. The study underlines how unclear methodological details can significantly hinder reproducibility and affect the reported effectiveness of image compression methods. This highlights the importance of thorough documentation and transparency for accurate performance assessment in image compression research. <div>
arXiv:2512.06221v1 Announce Type: new 
Abstract: This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking</title>
<link>https://arxiv.org/abs/2512.06230</link>
<guid>https://arxiv.org/abs/2512.06230</guid>
<content:encoded><![CDATA[
<div> multi-target tracking, labeled random finite sets, GLMB filter, GPU acceleration, multi-detections per object<br /><br />Summary:<br /><br />This article addresses challenges in multi-target tracking by focusing on labeled random finite set (RFS) methods, particularly the Generalized Labeled Multi-Bernoulli (GLMB) filter. Labeled RFS methods are valued for maintaining temporal coherence in object labeling and providing closed-form solutions to the multi-target Bayes filter. However, these methods experience high computational complexity due to the maintenance of multiple hypotheses under standard measurement models, even with hypothesis pruning. The authors propose a variant of the GLMB filter that allows for multiple detections per object from the same sensor, addressing scenarios common in distributed networks of machine learning-based virtual sensors. This variant breaks inter-detection dependencies present in standard GLMB updates, enabling significantly enhanced parallel scalability during filter updates. The improved scalability facilitates efficient implementation on GPU hardware. The paper reports preliminary results from a GPU-accelerated GLMB tracker, demonstrating favorable run-time scalability relative to the number of tracked objects and the maximum number of retained hypotheses. Overall, the work contributes a method that reduces computational burdens and improves deployment efficiency for multi-target tracking in sensor networks leveraging advanced computing architectures. <div>
arXiv:2512.06230v1 Announce Type: new 
Abstract: Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion: Learning Intuitive Physics May Require More than Visual Data</title>
<link>https://arxiv.org/abs/2512.06232</link>
<guid>https://arxiv.org/abs/2512.06232</guid>
<content:encoded><![CDATA[
<div> Keywords: intuitive physics, video joint embedding predictive architecture, SAYCam dataset, deep learning, data distribution<br /><br />Summary:<br />1. Humans develop intuitive physics through rich internal models based on their everyday experiences and understanding of the physical world.<br />2. Current state-of-the-art deep learning models, despite being trained on enormous internet video datasets, have not reached human-level performance on intuitive physics tasks.<br />3. This study explores whether the type and distribution of training data, rather than just the quantity, can enhance learning of intuitive physics.<br />4. The authors pretrained a Video Joint Embedding Predictive Architecture (V-JEPA) on SAYCam, an egocentric video dataset that realistically captures the visual experiences of young children.<br />5. Despite representing only 0.01% of the data volume used by leading models, training on SAYCam did not produce meaningful improvements on the IntPhys2 benchmark.<br />6. The findings suggest that simply using developmentally realistic datasets is not enough for current architectures to acquire effective intuitive physics representations.<br />7. Consequently, increasing or varying visual data volume and distribution alone appears insufficient to build artificial systems with human-like intuitive physics capabilities. <div>
arXiv:2512.06232v1 Announce Type: new 
Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks</title>
<link>https://arxiv.org/abs/2512.06251</link>
<guid>https://arxiv.org/abs/2512.06251</guid>
<content:encoded><![CDATA[
<div> Keywords: Partially Supervised Multi-Task Learning, NexusFlow, invertible coupling layers, domain partition, autonomous driving<br /><br />Summary:  
Partially Supervised Multi-Task Learning (PS-MTL) addresses learning from multiple tasks with incomplete annotations. Existing methods predominantly focus on homogeneous, dense prediction tasks, lacking solutions for structurally diverse tasks. NexusFlow is introduced as a novel, lightweight, plug-and-play framework that effectively handles both homogeneous and heterogeneous task settings. Central to NexusFlow are surrogate networks with invertible coupling layers that align latent feature distributions across tasks into a unified representation. The coupling layers are bijective, preserving detailed information and preventing representational collapse while allowing alignment across structurally different tasks without compromising expressive capacity. NexusFlow is first evaluated on a domain-partitioned autonomous driving scenario involving dense map reconstruction and sparse multi-object tracking supervised in different geographic regions, which presents both structural and domain disparities. It achieves state-of-the-art performance on the nuScenes dataset, surpassing strong partially supervised baselines. To validate its general applicability, NexusFlow is also tested on the NYUv2 dataset with three homogeneous dense prediction tasks — segmentation, depth estimation, and surface normal prediction — confirming consistent task improvements. These results demonstrate NexusFlow’s broad utility and effectiveness in advancing PS-MTL across diverse real-world problems. <div>
arXiv:2512.06251v1 Announce Type: new 
Abstract: Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-driven Fine-grained Retrieval</title>
<link>https://arxiv.org/abs/2512.06255</link>
<guid>https://arxiv.org/abs/2512.06255</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained image retrieval, language-driven supervision, large language models, vision-language models, attribute-level descriptions<br /><br />Summary: Existing fine-grained image retrieval (FGIR) techniques typically use one-hot labels based on category names for supervision, which are semantically sparse and limit the ability to model detailed comparisons across categories, reducing generalization to unseen classes. To overcome this, the paper proposes LaFG, a novel framework that leverages large language models (LLMs) and vision-language models (VLMs) to transform category names into rich attribute-level supervision. LaFG treats category names as semantic anchors and prompts an LLM to generate detailed, attribute-focused descriptions for each class. To address potential omissions in these descriptions, a frozen VLM maps them into a vision-aligned space where attributes are clustered into a comprehensive dataset-wide vocabulary, enriched by attributes mined from related categories. Using this vocabulary, LaFG applies a global prompt template to select relevant attributes and aggregate them into category-specific linguistic prototypes. These prototypes then act as supervision to guide the retrieval model, enabling it to better capture and compare fine-grained details across categories, ultimately improving its ability to generalize to unseen categories in fine-grained image retrieval tasks. <div>
arXiv:2512.06255v1 Announce Type: new 
Abstract: Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs</title>
<link>https://arxiv.org/abs/2512.06258</link>
<guid>https://arxiv.org/abs/2512.06258</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision-Language Models, reasoning paths, path selection bias, Path-Select Optimization, Group Relative Policy Optimization<br /><br />Summary:  
This paper identifies a critical problem in Large Vision-Language Models (LVLMs) where models often reach correct answers through flawed or unstable reasoning paths, emphasizing that the issue stems from a path selection bias rather than knowledge gaps. Evidence for this comes from significant differences between Pass@K (large K) and Pass@1 performance, indicating misreasoning rather than ignorance is the core challenge. To address this, the authors propose a two-stage post-training framework called Path-Select Optimization (PSO). The first stage uses Group Relative Policy Optimization (GRPO) with template and answer-based rewards to encourage structured, stepwise reasoning. In the second stage, an online preference optimization process is introduced where the model self-evaluates multiple reasoning paths, aligning toward preferred trajectories and storing incorrect paths in a Negative Replay Memory (NRM) to avoid repeating mistakes. This continual refinement mechanism improves reasoning stability and accuracy. Extensive experiments confirm that PSO effectively filters out invalid reasoning trajectories, improves reasoning accuracy by an average of 7.4%, and results in more stable and consistent chains of thought. The authors will release their code publicly to facilitate further research and application. <div>
arXiv:2512.06258v1 Announce Type: new 
Abstract: We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.06269</link>
<guid>https://arxiv.org/abs/2512.06269</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, novel view synthesis, multi-view triangulation, geometry consistency, photorealistic rendering  

<br /><br />Summary:  
1. The paper addresses the limitations of 3D Gaussian Splatting in real-time novel view synthesis, particularly issues arising from relying solely on photometric loss which leads to "floater" artifacts and unstructured geometry.  
2. To overcome these challenges, the authors propose a novel method that enforces global geometry consistency by integrating constrained multi-view triangulation.  
3. Their approach establishes a consensus on the 3D representation by leveraging multiple estimated views to improve the physical accuracy of reconstructions.  
4. The optimization penalizes deviations of rendered 3D points from a robust consensus point, which is recalculated through self-supervised triangulation over neighboring views.  
5. Experimental results demonstrate state-of-the-art performance, with a notable mean Chamfer Distance of 0.50 mm on the DTU dataset, surpassing other explicit reconstruction methods.  
6. The authors plan to release their code as open-source to promote reproducibility and facilitate further research in the community. <div>
arXiv:2512.06269v1 Announce Type: new 
Abstract: 3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in "floater" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FacePhys: State of the Heart Learning</title>
<link>https://arxiv.org/abs/2512.06275</link>
<guid>https://arxiv.org/abs/2512.06275</guid>
<content:encoded><![CDATA[
<div> Keywords: remote photoplethysmography, vital sign measurement, FacePhys, real-time inference, computational efficiency<br /><br />Summary:<br /><br />1. The paper presents FacePhys, a novel memory-efficient algorithm for remote photoplethysmography (rPPG), aimed at measuring vital signs using cameras for comfortable and ubiquitous health monitoring.<br />2. FacePhys addresses the key challenges of model scalability, cross-dataset generalization, and real-time operation through a temporal-spatial state space duality approach.<br />3. It leverages a transferable heart state to capture subtle periodic variations across video frames while maintaining minimal computational overhead, enabling training on extended video sequences and low-latency inference.<br />4. FacePhys achieves a new state-of-the-art performance with a 49% reduction in error compared to existing methods.<br />5. The solution supports real-time inference with a memory footprint of only 3.6 MB and per-frame latency of 9.46 milliseconds, outperforming current models by 83% to 99%, making it suitable for practical deployment.<br />6. A live demonstration of FacePhys is available for public access at https://www.facephys.com/, showcasing its potential for reliable and efficient vital sign monitoring through cameras. <div>
arXiv:2512.06275v1 Announce Type: new 
Abstract: Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\% to 99\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2512.06276</link>
<guid>https://arxiv.org/abs/2512.06276</guid>
<content:encoded><![CDATA[
<div> Referring Expression Comprehension, Multi-modal Large Language Model, RefBench-PRO, Dynamic IoU-based GRPO, Reinforcement Learning<br /><br />Summary:  
Referring Expression Comprehension (REC) involves localizing specific image regions based on textual descriptions. Existing benchmarks mainly focus on perceptual abilities and lack interpretable scoring metrics, limiting insights into the grounding capabilities of Multi-modal Large Language Models (MLLMs) across various cognitive skills. To overcome these limitations, this work introduces RefBench-PRO, a comprehensive REC benchmark that divides referring expressions into two main dimensions: perception and reasoning. These dimensions are further broken down into six progressively challenging tasks: attribute, position, interaction, commonsense, relation, and reject. The authors develop a fully automated data generation pipeline to create diverse referring expressions across these sub-dimensions, ensuring robust evaluation. Additionally, Ref-R1, a reinforcement learning-based method incorporating Dynamic IoU-based GRPO, is proposed to enhance localization accuracy, particularly under complex reasoning scenarios, thereby establishing a stronger baseline for REC. Extensive experiments demonstrate that RefBench-PRO facilitates interpretable and nuanced evaluation of MLLMs on referring expression comprehension tasks, exposing greater challenges in both perceptual and reasoning capabilities. This benchmark and methodology collectively push forward the assessment and improvement of grounding abilities in vision-language models. <div>
arXiv:2512.06276v1 Announce Type: new 
Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.06281</link>
<guid>https://arxiv.org/abs/2512.06281</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, modality imbalance, latent visual reconstruction, masked image modeling, visual attention  

<br /><br />Summary:  
Multimodal Large Language Models (MLLMs) have achieved strong performance on tasks involving multiple data types, but they often suffer from a modality imbalance issue. This issue arises because visual information is underutilized relative to textual information in the deeper network layers, leading to reduced visual performance and hallucinations. The root cause is the training paradigm centered around next-text-token prediction, which does not provide direct supervisory signals for visual data. To address this, the authors propose Latent Visual Reconstruction (LaVer), a novel training framework that introduces masked image modeling within the joint latent semantic space of LLMs. LaVer provides explicit visual activation, which encourages the model to allocate more attention to visual inputs and maintain more discriminative visual representations throughout its layers. Extensive experiments across multiple benchmarks demonstrate LaVer's effectiveness, especially in scenarios demanding dense visual reasoning. The framework enhances the visual capability of MLLMs and reduces visual hallucination. The authors have also made the code for LaVer publicly available, facilitating further research and application development in this area. <div>
arXiv:2512.06281v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Sleep Monitoring System Based on Audio, Video and Depth Information</title>
<link>https://arxiv.org/abs/2512.06282</link>
<guid>https://arxiv.org/abs/2512.06282</guid>
<content:encoded><![CDATA[
<div> motion events, light-on/off events, noise events, infrared depth sensor, event detection algorithm<br /><br />Summary:<br /><br />This paper presents a noninvasive sleep monitoring system designed to quantitatively evaluate sleep disturbances using an event-based method. The study classifies sleep disturbances into three main event types: motion events, light-on/off events, and noise events, which are crucial for understanding the sleep environment. The monitoring device integrates an infrared depth sensor, an RGB camera, and a four-microphone array to capture multimodal data during sleep in low-light home settings. A background model leveraging depth signals is employed to quantify the magnitude of movements, while a separate background model using color images detects lighting changes, addressing the limitation of depth sensors in sensing light variations. An event detection algorithm processes the sensor data to detect the occurrences of the three classified events. Experiments conducted under actual sleep conditions demonstrate the system's reliability and effectiveness in monitoring and classifying sleep disturbances. This integrated approach offers a comprehensive and nonintrusive solution for sleep disturbance evaluation in real-world home environments, showing potential for enhancing sleep quality assessment. <div>
arXiv:2512.06282v1 Announce Type: new 
Abstract: For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification</title>
<link>https://arxiv.org/abs/2512.06290</link>
<guid>https://arxiv.org/abs/2512.06290</guid>
<content:encoded><![CDATA[
<div> Keywords: Stroke classification, reference points, Inline Sequence Attention, Cross-Ellipse Query, online handwriting  

<br /><br />Summary:  
The paper addresses the challenge of stroke classification in online handwriting, which is complicated by variations in writing styles, ambiguous content, and dynamic writing positions. The key difficulty lies in modeling the semantic relationships between strokes, particularly because stroke interactions tend to be localized, and existing deep learning models struggle to capture these fine-grained relationships. To overcome this, the authors propose representing strokes through selected reference points combined with feature vectors, balancing detail and redundancy. They introduce StrokeNet, an architecture that sequentially encodes these reference points and uses an Inline Sequence Attention (ISA) module to build contextual stroke features. Additionally, a novel Cross-Ellipse Query (CEQ) mechanism clusters reference points to extract spatial features across multiple scales. The framework also incorporates a joint optimization strategy that simultaneously predicts stroke categories via reference point regression and models semantic transitions between adjacent strokes using an Auxiliary Branch. Experimental results across several public online handwriting datasets demonstrate state-of-the-art performance. Notably, on the CASIA-onDo dataset, the accuracy of stroke classification improves significantly from 93.81% to 95.54%, validating the method’s effectiveness and robustness in handling complex stroke interactions. <div>
arXiv:2512.06290v1 Announce Type: new 
Abstract: Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\%$ to 95.54$\%$, demonstrating the effectiveness and robustness of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.06306</link>
<guid>https://arxiv.org/abs/2512.06306</guid>
<content:encoded><![CDATA[
<div> Event cameras, human pose estimation, point cloud, temporal modeling, edge enhancement<br /><br />Summary:<br /><br />This paper addresses human pose estimation using event cameras, which provide high temporal resolution and low latency beneficial for scenarios with challenging conditions. Traditional approaches usually convert event streams into dense event frames, which compromises temporal resolution and increases computational load. To overcome this, the authors propose a point cloud-based framework that directly exploits the spatiotemporal characteristics of event streams. The method introduces an Event Temporal Slicing Convolution module designed to capture short-term dependencies across event slices. Alongside this, the Event Slice Sequencing module is employed to enable structured temporal modeling of the event data. To enhance spatial details, especially in sparse event scenarios, an edge enhancement technique is applied within the point cloud-based event representation. Experimental validation on the DHP19 dataset demonstrates consistent performance improvements using the proposed approach across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer. The results confirm that leveraging event-based data in a point cloud format with temporal and edge enhancements can significantly improve human pose estimation models. <div>
arXiv:2512.06306v1 Announce Type: new 
Abstract: Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.06328</link>
<guid>https://arxiv.org/abs/2512.06328</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.06328v1

Keywords: ReCAD, reinforcement learning, parametric CAD, vision-language models, generative models<br /><br />Summary:<br /><br />1. The paper introduces ReCAD, a reinforcement learning (RL) framework designed to generate accurate parametric computer-aided design (CAD) models by leveraging pretrained large models (PLMs) and their generative capabilities from multimodal inputs.<br /><br />2. Unlike previous approaches relying mainly on supervised fine-tuning and limited editability, ReCAD accesses simple functional interfaces such as point coordinates to enable complex CAD operations like pattern replication and mirroring.<br /><br />3. The framework initiates by fine-tuning vision-language models (VLMs) on rewritten CAD scripts translated into parameterized code, which helps generate accurate textual supervision.<br /><br />4. A novel RL strategy incorporates parameterized code guidance, enhancing reasoning ability for complex CAD generation tasks, supported by a hierarchical primitive learning process that progressively builds structured, compositional skills under a unified reward function optimizing both geometric accuracy and semantic fidelity.<br /><br />5. Experimental results show that ReCAD achieves state-of-the-art performance on text-to-CAD and image-to-CAD tasks, significantly reducing mean Chamfer Distance for both in-distribution (73.47 to 29.61) and out-of-distribution (272.06 to 80.23) cases, markedly outperforming existing baselines. <div>
arXiv:2512.06328v1 Announce Type: new 
Abstract: We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening</title>
<link>https://arxiv.org/abs/2512.06330</link>
<guid>https://arxiv.org/abs/2512.06330</guid>
<content:encoded><![CDATA[
<div> Keywords: Pansharpening, Discrete Wavelet Transform, Cross-modal Interaction, Multi-scale Fusion, Spectral-Spatial Disentanglement<br /><br />Summary:<br />1. The paper introduces S2WMamba, a novel method for pansharpening that fuses high-resolution panchromatic (PAN) images with low-resolution multispectral (LRMS) images to produce high-resolution multispectral (HRMS) outputs.<br />2. A key challenge addressed is the entanglement of spatial details and spectral fidelity when jointly processing PAN and MS data; S2WMamba explicitly disentangles frequency information to overcome this.<br />3. The method employs a 2D Haar Discrete Wavelet Transform (DWT) on PAN images to localize spatial edges and textures, while a channel-wise 1D Haar DWT processes multispectral pixels as 1D signals, separating low- and high-frequency components to minimize spectral distortion.<br />4. S2WMamba features two parallel branches: the Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectral information from the 1D wavelet pyramid.<br />5. Information exchange between branches is facilitated by a Mamba-based cross-modulation mechanism that models long-range dependencies with linear complexity.<br />6. An adaptive multi-scale dynamic gate, combining multiplicative and additive operations, is used to fuse the outputs of both branches effectively.<br />7. Experiments on WV3, GF2, and QB datasets show that S2WMamba matches or surpasses recent strong baselines, achieving up to 0.23 dB PSNR improvement and a high HQNR score of 0.956 on full-resolution WV3.<br />8. Ablation studies confirm the effectiveness of the 2D/1D DWT placement, parallel dual-branch architecture, and fusion gate design.<br />9. The authors provide the code publicly for reproducibility at the specified GitHub repository. <div>
arXiv:2512.06330v1 Announce Type: new 
Abstract: Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks</title>
<link>https://arxiv.org/abs/2512.06332</link>
<guid>https://arxiv.org/abs/2512.06332</guid>
<content:encoded><![CDATA[
<div> Keywords: cryo-electron microscopy, compositional heterogeneity, transformer, hypernetwork, implicit neural representation<br /><br />Summary: Cryo-electron microscopy (cryo-EM) is a critical method for determining three-dimensional structures of dynamic biomolecular complexes, typically used for imaging a single molecular species. Current approaches mainly address conformational heterogeneity within one or a few structures, lacking the ability to resolve compositional heterogeneity arising from mixtures containing many distinct molecular species. To overcome this limitation, the paper introduces CryoHype, a novel transformer-based hypernetwork designed for cryo-EM reconstruction. CryoHype dynamically adjusts the weights of an implicit neural representation, enabling it to handle multiple molecular species simultaneously. The method achieves state-of-the-art performance on a challenging benchmark dataset consisting of 100 different biomolecular structures. Moreover, CryoHype demonstrates scalability by successfully reconstructing 1,000 distinct structures from unlabeled cryo-EM images in a fixed-pose scenario. This advancement paves the way for high-throughput, simultaneous structural determination of diverse molecular species from mixed cryo-EM data, addressing a key challenge in the field that existing methods could not effectively resolve. <div>
arXiv:2512.06332v1 Announce Type: new 
Abstract: Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate</title>
<link>https://arxiv.org/abs/2512.06344</link>
<guid>https://arxiv.org/abs/2512.06344</guid>
<content:encoded><![CDATA[
<div> Generative Image Compression, Multimodal Guidance, Semantic Consistency, Task-Aware Semantic Compression, Diffusion Decoder<br /><br />Summary:<br /><br />This paper addresses the challenge of semantic deviations in generative image compression at ultra-low bitrates (below 0.05 bpp), which hampers the deployment of such methods in bandwidth-restricted 6G semantic communication. The authors propose the Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework, which leverages three different guidance modalities to improve semantic consistency: a concise text caption to capture global semantics, a highly compressed image (HCI) for low-level visual details, and Semantic Pseudo-Words (SPWs) that provide fine-grained, task-relevant semantic information. These SPWs are produced by a novel Task-Aware Semantic Compression Module (TASCM), which uses multi-head self-attention to focus on important semantics while filtering unnecessary data. The integration of these guidance signals into the image reconstruction relies on a Multimodal-Guided Diffusion Decoder (MGDD), which utilizes a dual-path cooperative mechanism combining cross-attention and ControlNet residuals to effectively inject the multimodal information into the diffusion process. Experimental results demonstrate that MTGC significantly enhances semantic consistency, perceptual quality, and pixel-level fidelity, achieving a notable 10.59% reduction in DISTS on the DIV2K dataset, validating its effectiveness under ultra-low bitrate conditions. <div>
arXiv:2512.06344v1 Announce Type: new 
Abstract: Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLUENet: Cluster Attention Makes Neural Networks Have Eyes</title>
<link>https://arxiv.org/abs/2512.06345</link>
<guid>https://arxiv.org/abs/2512.06345</guid>
<content:encoded><![CDATA[
<div> Clustering, Attention, Visual Semantic Understanding, Interpretability, Deep Architecture<br /><br />Summary: Despite the widespread success of convolutional and attention-based models in vision tasks, these approaches face limitations due to their rigid receptive fields and complex structures, which reduce interpretability and hinder the modeling of irregular spatial patterns. To improve transparency and semantic modeling flexibility, clustering paradigms have been explored, but they typically suffer from issues such as limited accuracy, low efficiency, and gradient vanishing during training. Addressing these challenges, the paper introduces CLUster attEntion Network (CLUENet), a transparent deep learning architecture aimed at enhancing visual semantic understanding. CLUENet introduces three main innovations: (i) a Global Soft Aggregation and Hard Assignment mechanism using Temperature-Scaled Cosine Attention combined with gated residual connections for improved local feature modeling; (ii) inter-block Hard and Shared Feature Dispatching to optimize information flow; and (iii) an improved cluster pooling strategy that enhances performance. Experimental results on CIFAR-100 and Mini-ImageNet datasets demonstrate that CLUENet surpasses existing clustering-based methods and conventional visual models by achieving a remarkable balance between classification accuracy, computational efficiency, and model interpretability. This work offers a significant step toward transparent, effective visual models suitable for tasks demanding high levels of model explainability. <div>
arXiv:2512.06345v1 Announce Type: new 
Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search</title>
<link>https://arxiv.org/abs/2512.06353</link>
<guid>https://arxiv.org/abs/2512.06353</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformers, Quantization, Tree Structured Search, Environmental Noise Guidance, General Monarch Branch<br /><br />Summary: Diffusion Transformers (DiTs) have become a powerful model architecture for image generation, surpassing traditional U-Net designs in scalability and performance but facing deployment challenges due to high computational and memory requirements. This work introduces TreeQ, a novel unified framework specifically designed to address these challenges in DiT quantization. First, TreeQ implements Tree Structured Search (TSS), which exploits the linear structure of DiTs to efficiently explore the solution space in O(n) time while enhancing accuracy through comparison-based pruning. Second, it proposes Environmental Noise Guidance (ENG), a method that harmonizes the optimization objectives of Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) via a single hyperparameter, simplifying the quantization process. Third, to combat information loss in ultra-low-bit precision settings, TreeQ incorporates the General Monarch Branch (GMB), a structured sparse branch that preserves critical details in generated images. Comprehensive experiments show that TreeQ achieves state-of-the-art results on the DiT-XL/2 architecture under both W3A3 and W4A4 PTQ/PEFT configurations. Remarkably, this approach is the first to reach near-lossless 4-bit PTQ performance for DiT models, marking a significant step toward more practical deployment of DiTs. The code and trained models are publicly accessible at the provided GitHub repository. <div>
arXiv:2512.06353v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectifying Latent Space for Generative Single-Image Reflection Removal</title>
<link>https://arxiv.org/abs/2512.06358</link>
<guid>https://arxiv.org/abs/2512.06358</guid>
<content:encoded><![CDATA[
<div> Reflection removal, latent diffusion, VAE, composite image, depth-guided sampling<br /><br />Summary: This paper addresses the challenging problem of single-image reflection removal, which is ill-posed due to the complexity of separating overlapping layers in a reflection-contaminated image. The authors identify a fundamental issue with existing semantic encoder latent spaces, which do not naturally represent composite images as a linear combination of layers, limiting recovery and generalization. To overcome this, the paper introduces a novel framework based on a reflection-equivariant variational autoencoder (VAE) designed to align the latent space with the physics of reflection formation, thereby enabling more meaningful layer disentanglement. Additionally, the method employs a learnable, task-specific text embedding that offers precise guidance while avoiding ambiguity inherent in natural language instructions. The approach also incorporates a depth-guided early-branching sampling technique to leverage generative stochasticity effectively, enhancing the quality and realism of the reconstructed images. Extensive experimental validation demonstrates that this method sets a new state-of-the-art (SOTA) performance across multiple benchmark datasets, showing strong generalization to complex real-world scenarios where reflections are difficult to remove. The combination of physics-aligned latent space, precise guidance, and innovative sampling strategy collectively yields robust and high-quality reflection removal results. <div>
arXiv:2512.06358v1 Announce Type: new 
Abstract: Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection</title>
<link>https://arxiv.org/abs/2512.06363</link>
<guid>https://arxiv.org/abs/2512.06363</guid>
<content:encoded><![CDATA[
<div> Physical Presentation Attacks, Digital Forgery Attacks, Unified Attack Detection, Spoofing-aware Prompt Learning, CLIP-based Defense<br /><br />Summary:<br /><br />This paper addresses the vulnerability of face recognition systems to both physical presentation attacks (PAs) and digital forgery attacks (DFs), aiming to secure biometric data through a unified detection framework. Existing methods typically use CLIP with regularization to improve generalization across attack types but face challenges due to conflicting optimization objectives in the shared prompt space for physical and digital attacks. To resolve this, the authors propose the Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization by introducing parallel learnable prompt branches specific to physical and digital attacks. This is enhanced by an adaptive Spoofing Context Prompt Generation mechanism, allowing independent control for each attack category. Additionally, the Cues-awareness Augmentation technique leverages the dual-prompt design to create challenging sample mining tasks, which strengthen the model’s robustness against previously unseen attack techniques. Extensive experiments conducted on the large-scale UniAttackDataPlus dataset demonstrate that SPL-UAD significantly improves performance for unified attack detection tasks compared to prior approaches, confirming the effectiveness of decoupling prompt optimization and the benefits of cue-aware augmentation strategies. <div>
arXiv:2512.06363v1 Announce Type: new 
Abstract: Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos</title>
<link>https://arxiv.org/abs/2512.06368</link>
<guid>https://arxiv.org/abs/2512.06368</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular dynamic video reconstruction, SMPL human body model, geometric priors, human boundary preservation, cross-attention fusion<br /><br />Summary:<br /><br />This article addresses the challenges in monocular dynamic video reconstruction for dynamic human scenes, focusing on issues like geometric inconsistencies and resolution degradation. Existing approaches often fail due to lack of 3D human structural understanding, resulting in distorted limb proportions and unnatural fusion between humans and objects. Moreover, downsampling to reduce memory usage causes human boundaries to drift toward background geometry, reducing accuracy. To overcome these problems, the authors propose a novel method that integrates hybrid geometric priors combining SMPL human body models with monocular depth estimation. This approach leverages structured human priors to maintain surface consistency while capturing fine-grained details in human regions. The proposed Human3R framework features a hierarchical pipeline: it first processes full-resolution images to reconstruct overall scene geometry, then strategically crops and applies cross-attention fusion to enhance human-specific details. The integration of SMPL priors is achieved through a Feature Fusion Module that ensures geometrically plausible reconstructions and preserves fine boundary details of humans. Experiments on TUM Dynamics and GTA-IM datasets demonstrate that Human3R outperforms previous methods in reconstructing dynamic humans accurately and with improved detail. <div>
arXiv:2512.06368v1 Announce Type: new 
Abstract: Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06373</link>
<guid>https://arxiv.org/abs/2512.06373</guid>
<content:encoded><![CDATA[
arXiv:2512.06373v1 Announce Type: new 
Abstract: Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework</title>
<link>https://arxiv.org/abs/2512.06376</link>
<guid>https://arxiv.org/abs/2512.06376</guid>
<content:encoded><![CDATA[
arXiv:2512.06376v1 Announce Type: new 
Abstract: Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System</title>
<link>https://arxiv.org/abs/2512.06377</link>
<guid>https://arxiv.org/abs/2512.06377</guid>
<content:encoded><![CDATA[
arXiv:2512.06377v1 Announce Type: new 
Abstract: Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OCFER-Net: Recognizing Facial Expression in Online Learning System</title>
<link>https://arxiv.org/abs/2512.06379</link>
<guid>https://arxiv.org/abs/2512.06379</guid>
<content:encoded><![CDATA[
arXiv:2512.06379v1 Announce Type: new 
Abstract: Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement</title>
<link>https://arxiv.org/abs/2512.06400</link>
<guid>https://arxiv.org/abs/2512.06400</guid>
<content:encoded><![CDATA[
arXiv:2512.06400v1 Announce Type: new 
Abstract: In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Training Dynamics in Scale-wise Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.06421</link>
<guid>https://arxiv.org/abs/2512.06421</guid>
<content:encoded><![CDATA[
arXiv:2512.06421v1 Announce Type: new 
Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Perception CNN for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2512.06422</link>
<guid>https://arxiv.org/abs/2512.06422</guid>
<content:encoded><![CDATA[
arXiv:2512.06422v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DragMesh: Interactive 3D Generation Made Easy</title>
<link>https://arxiv.org/abs/2512.06424</link>
<guid>https://arxiv.org/abs/2512.06424</guid>
<content:encoded><![CDATA[
arXiv:2512.06424v1 Announce Type: new 
Abstract: While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition</title>
<link>https://arxiv.org/abs/2512.06426</link>
<guid>https://arxiv.org/abs/2512.06426</guid>
<content:encoded><![CDATA[
arXiv:2512.06426v1 Announce Type: new 
Abstract: Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening</title>
<link>https://arxiv.org/abs/2512.06434</link>
<guid>https://arxiv.org/abs/2512.06434</guid>
<content:encoded><![CDATA[
arXiv:2512.06434v1 Announce Type: new 
Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars</title>
<link>https://arxiv.org/abs/2512.06438</link>
<guid>https://arxiv.org/abs/2512.06438</guid>
<content:encoded><![CDATA[
arXiv:2512.06438v1 Announce Type: new 
Abstract: The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Stable Cross-Domain Depression Recognition under Missing Modalities</title>
<link>https://arxiv.org/abs/2512.06447</link>
<guid>https://arxiv.org/abs/2512.06447</guid>
<content:encoded><![CDATA[
arXiv:2512.06447v1 Announce Type: new 
Abstract: Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction</title>
<link>https://arxiv.org/abs/2512.06485</link>
<guid>https://arxiv.org/abs/2512.06485</guid>
<content:encoded><![CDATA[
arXiv:2512.06485v1 Announce Type: new 
Abstract: Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion</title>
<link>https://arxiv.org/abs/2512.06504</link>
<guid>https://arxiv.org/abs/2512.06504</guid>
<content:encoded><![CDATA[
arXiv:2512.06504v1 Announce Type: new 
Abstract: The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images</title>
<link>https://arxiv.org/abs/2512.06521</link>
<guid>https://arxiv.org/abs/2512.06521</guid>
<content:encoded><![CDATA[
arXiv:2512.06521v1 Announce Type: new 
Abstract: The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.
  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization</title>
<link>https://arxiv.org/abs/2512.06530</link>
<guid>https://arxiv.org/abs/2512.06530</guid>
<content:encoded><![CDATA[
arXiv:2512.06530v1 Announce Type: new 
Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images</title>
<link>https://arxiv.org/abs/2512.06531</link>
<guid>https://arxiv.org/abs/2512.06531</guid>
<content:encoded><![CDATA[
arXiv:2512.06531v1 Announce Type: new 
Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging spatial awareness and global context in medical image segmentation</title>
<link>https://arxiv.org/abs/2512.06560</link>
<guid>https://arxiv.org/abs/2512.06560</guid>
<content:encoded><![CDATA[
arXiv:2512.06560v1 Announce Type: new 
Abstract: Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities</title>
<link>https://arxiv.org/abs/2512.06562</link>
<guid>https://arxiv.org/abs/2512.06562</guid>
<content:encoded><![CDATA[
arXiv:2512.06562v1 Announce Type: new 
Abstract: Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation</title>
<link>https://arxiv.org/abs/2512.06565</link>
<guid>https://arxiv.org/abs/2512.06565</guid>
<content:encoded><![CDATA[
arXiv:2512.06565v1 Announce Type: new 
Abstract: We present GNC--Pose, a fully learning--free monocular 6D object pose estimation pipeline for textured objects that combines rendering--based initialization, geometry--aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D--3D correspondences obtained through feature matching and rendering--based alignment, our method builds upon the Graduated Non--Convexity (GNC) principle and introduces a geometry--aware, cluster--based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC--Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC--Pose achieves competitive accuracy compared with both learning-based and learning--free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules</title>
<link>https://arxiv.org/abs/2512.06575</link>
<guid>https://arxiv.org/abs/2512.06575</guid>
<content:encoded><![CDATA[
arXiv:2512.06575v1 Announce Type: new 
Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding</title>
<link>https://arxiv.org/abs/2512.06581</link>
<guid>https://arxiv.org/abs/2512.06581</guid>
<content:encoded><![CDATA[
arXiv:2512.06581v1 Announce Type: new 
Abstract: Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain</title>
<link>https://arxiv.org/abs/2512.06598</link>
<guid>https://arxiv.org/abs/2512.06598</guid>
<content:encoded><![CDATA[
arXiv:2512.06598v1 Announce Type: new 
Abstract: Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics</title>
<link>https://arxiv.org/abs/2512.06612</link>
<guid>https://arxiv.org/abs/2512.06612</guid>
<content:encoded><![CDATA[
arXiv:2512.06612v1 Announce Type: new 
Abstract: Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach</title>
<link>https://arxiv.org/abs/2512.06613</link>
<guid>https://arxiv.org/abs/2512.06613</guid>
<content:encoded><![CDATA[
arXiv:2512.06613v1 Announce Type: new 
Abstract: Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.
  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.
  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).
  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution</title>
<link>https://arxiv.org/abs/2512.06642</link>
<guid>https://arxiv.org/abs/2512.06642</guid>
<content:encoded><![CDATA[
arXiv:2512.06642v1 Announce Type: new 
Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextMamba: Scene Text Detector with Mamba</title>
<link>https://arxiv.org/abs/2512.06657</link>
<guid>https://arxiv.org/abs/2512.06657</guid>
<content:encoded><![CDATA[
arXiv:2512.06657v1 Announce Type: new 
Abstract: In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\%, 89.2\%, and 78.5\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Image Descriptions from Attention Sequences</title>
<link>https://arxiv.org/abs/2512.06662</link>
<guid>https://arxiv.org/abs/2512.06662</guid>
<content:encoded><![CDATA[
arXiv:2512.06662v1 Announce Type: new 
Abstract: People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks</title>
<link>https://arxiv.org/abs/2512.06663</link>
<guid>https://arxiv.org/abs/2512.06663</guid>
<content:encoded><![CDATA[
arXiv:2512.06663v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning</title>
<link>https://arxiv.org/abs/2512.06673</link>
<guid>https://arxiv.org/abs/2512.06673</guid>
<content:encoded><![CDATA[
arXiv:2512.06673v1 Announce Type: new 
Abstract: Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RunawayEvil: Jailbreaking the Image-to-Video Generative Models</title>
<link>https://arxiv.org/abs/2512.06674</link>
<guid>https://arxiv.org/abs/2512.06674</guid>
<content:encoded><![CDATA[
arXiv:2512.06674v1 Announce Type: new 
Abstract: Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a "Strategy-Tactic-Action" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy</title>
<link>https://arxiv.org/abs/2512.06684</link>
<guid>https://arxiv.org/abs/2512.06684</guid>
<content:encoded><![CDATA[
arXiv:2512.06684v1 Announce Type: new 
Abstract: Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation</title>
<link>https://arxiv.org/abs/2512.06689</link>
<guid>https://arxiv.org/abs/2512.06689</guid>
<content:encoded><![CDATA[
arXiv:2512.06689v1 Announce Type: new 
Abstract: Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of Entropy in Visual Grounding: Analysis and Optimization</title>
<link>https://arxiv.org/abs/2512.06726</link>
<guid>https://arxiv.org/abs/2512.06726</guid>
<content:encoded><![CDATA[
arXiv:2512.06726v1 Announce Type: new 
Abstract: Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data</title>
<link>https://arxiv.org/abs/2512.06736</link>
<guid>https://arxiv.org/abs/2512.06736</guid>
<content:encoded><![CDATA[
arXiv:2512.06736v1 Announce Type: new 
Abstract: Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.06738</link>
<guid>https://arxiv.org/abs/2512.06738</guid>
<content:encoded><![CDATA[
arXiv:2512.06738v1 Announce Type: new 
Abstract: We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2512.06746</link>
<guid>https://arxiv.org/abs/2512.06746</guid>
<content:encoded><![CDATA[
arXiv:2512.06746v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement</title>
<link>https://arxiv.org/abs/2512.06750</link>
<guid>https://arxiv.org/abs/2512.06750</guid>
<content:encoded><![CDATA[
arXiv:2512.06750v1 Announce Type: new 
Abstract: Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors</title>
<link>https://arxiv.org/abs/2512.06759</link>
<guid>https://arxiv.org/abs/2512.06759</guid>
<content:encoded><![CDATA[
arXiv:2512.06759v1 Announce Type: new 
Abstract: Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms</title>
<link>https://arxiv.org/abs/2512.06763</link>
<guid>https://arxiv.org/abs/2512.06763</guid>
<content:encoded><![CDATA[
arXiv:2512.06763v1 Announce Type: new 
Abstract: The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.06769</link>
<guid>https://arxiv.org/abs/2512.06769</guid>
<content:encoded><![CDATA[
arXiv:2512.06769v1 Announce Type: new 
Abstract: Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.06774</link>
<guid>https://arxiv.org/abs/2512.06774</guid>
<content:encoded><![CDATA[
arXiv:2512.06774v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</title>
<link>https://arxiv.org/abs/2512.06783</link>
<guid>https://arxiv.org/abs/2512.06783</guid>
<content:encoded><![CDATA[
arXiv:2512.06783v1 Announce Type: new 
Abstract: Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Geometry Encoding Volume for Real-time Stereo Matching</title>
<link>https://arxiv.org/abs/2512.06793</link>
<guid>https://arxiv.org/abs/2512.06793</guid>
<content:encoded><![CDATA[
arXiv:2512.06793v1 Announce Type: new 
Abstract: Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VDOT: Efficient Unified Video Creation via Optimal Transport Distillation</title>
<link>https://arxiv.org/abs/2512.06802</link>
<guid>https://arxiv.org/abs/2512.06802</guid>
<content:encoded><![CDATA[
arXiv:2512.06802v1 Announce Type: new 
Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06810</link>
<guid>https://arxiv.org/abs/2512.06810</guid>
<content:encoded><![CDATA[
arXiv:2512.06810v1 Announce Type: new 
Abstract: Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.06811</link>
<guid>https://arxiv.org/abs/2512.06811</guid>
<content:encoded><![CDATA[
arXiv:2512.06811v1 Announce Type: new 
Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshSplatting: Differentiable Rendering with Opaque Meshes</title>
<link>https://arxiv.org/abs/2512.06818</link>
<guid>https://arxiv.org/abs/2512.06818</guid>
<content:encoded><![CDATA[
arXiv:2512.06818v1 Announce Type: new 
Abstract: Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseCoop: Cooperative Perception with Kinematic-Grounded Queries</title>
<link>https://arxiv.org/abs/2512.06838</link>
<guid>https://arxiv.org/abs/2512.06838</guid>
<content:encoded><![CDATA[
arXiv:2512.06838v1 Announce Type: new 
Abstract: Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles</title>
<link>https://arxiv.org/abs/2512.06840</link>
<guid>https://arxiv.org/abs/2512.06840</guid>
<content:encoded><![CDATA[
arXiv:2512.06840v1 Announce Type: new 
Abstract: Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the "incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.06845</link>
<guid>https://arxiv.org/abs/2512.06845</guid>
<content:encoded><![CDATA[
arXiv:2512.06845v1 Announce Type: new 
Abstract: Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</title>
<link>https://arxiv.org/abs/2512.06849</link>
<guid>https://arxiv.org/abs/2512.06849</guid>
<content:encoded><![CDATA[
arXiv:2512.06849v1 Announce Type: new 
Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Referring Image Segmentation</title>
<link>https://arxiv.org/abs/2512.06862</link>
<guid>https://arxiv.org/abs/2512.06862</guid>
<content:encoded><![CDATA[
arXiv:2512.06862v1 Announce Type: new 
Abstract: In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training</title>
<link>https://arxiv.org/abs/2512.06864</link>
<guid>https://arxiv.org/abs/2512.06864</guid>
<content:encoded><![CDATA[
arXiv:2512.06864v1 Announce Type: new 
Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial Retrieval Augmented Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06865</link>
<guid>https://arxiv.org/abs/2512.06865</guid>
<content:encoded><![CDATA[
arXiv:2512.06865v1 Announce Type: new 
Abstract: Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.
  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior</title>
<link>https://arxiv.org/abs/2512.06866</link>
<guid>https://arxiv.org/abs/2512.06866</guid>
<content:encoded><![CDATA[
arXiv:2512.06866v1 Announce Type: new 
Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective</title>
<link>https://arxiv.org/abs/2512.06870</link>
<guid>https://arxiv.org/abs/2512.06870</guid>
<content:encoded><![CDATA[
arXiv:2512.06870v1 Announce Type: new 
Abstract: Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification</title>
<link>https://arxiv.org/abs/2512.06877</link>
<guid>https://arxiv.org/abs/2512.06877</guid>
<content:encoded><![CDATA[
arXiv:2512.06877v1 Announce Type: new 
Abstract: Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion</title>
<link>https://arxiv.org/abs/2512.06882</link>
<guid>https://arxiv.org/abs/2512.06882</guid>
<content:encoded><![CDATA[
arXiv:2512.06882v1 Announce Type: new 
Abstract: Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoPano: Unified Panorama Generation via Joint Modeling</title>
<link>https://arxiv.org/abs/2512.06885</link>
<guid>https://arxiv.org/abs/2512.06885</guid>
<content:encoded><![CDATA[
arXiv:2512.06885v1 Announce Type: new 
Abstract: Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Learning for Domain Adaptive Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06886</link>
<guid>https://arxiv.org/abs/2512.06886</guid>
<content:encoded><![CDATA[
arXiv:2512.06886v1 Announce Type: new 
Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation</title>
<link>https://arxiv.org/abs/2512.06888</link>
<guid>https://arxiv.org/abs/2512.06888</guid>
<content:encoded><![CDATA[
arXiv:2512.06888v1 Announce Type: new 
Abstract: The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Zero-Shot Reference-to-Video Generation</title>
<link>https://arxiv.org/abs/2512.06905</link>
<guid>https://arxiv.org/abs/2512.06905</guid>
<content:encoded><![CDATA[
arXiv:2512.06905v1 Announce Type: new 
Abstract: Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification</title>
<link>https://arxiv.org/abs/2512.06921</link>
<guid>https://arxiv.org/abs/2512.06921</guid>
<content:encoded><![CDATA[
arXiv:2512.06921v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology</title>
<link>https://arxiv.org/abs/2512.06949</link>
<guid>https://arxiv.org/abs/2512.06949</guid>
<content:encoded><![CDATA[
arXiv:2512.06949v1 Announce Type: new 
Abstract: Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\% to 31.25\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Masking based Self-Supervised Learning for Image Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06981</link>
<guid>https://arxiv.org/abs/2512.06981</guid>
<content:encoded><![CDATA[
arXiv:2512.06981v1 Announce Type: new 
Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues</title>
<link>https://arxiv.org/abs/2512.07034</link>
<guid>https://arxiv.org/abs/2512.07034</guid>
<content:encoded><![CDATA[
arXiv:2512.07034v1 Announce Type: new 
Abstract: Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating and Preserving High-level Fidelity in Super-Resolution</title>
<link>https://arxiv.org/abs/2512.07037</link>
<guid>https://arxiv.org/abs/2512.07037</guid>
<content:encoded><![CDATA[
arXiv:2512.07037v1 Announce Type: new 
Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.07051</link>
<guid>https://arxiv.org/abs/2512.07051</guid>
<content:encoded><![CDATA[
arXiv:2512.07051v1 Announce Type: new 
Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07052</link>
<guid>https://arxiv.org/abs/2512.07052</guid>
<content:encoded><![CDATA[
arXiv:2512.07052v1 Announce Type: new 
Abstract: Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction</title>
<link>https://arxiv.org/abs/2512.07062</link>
<guid>https://arxiv.org/abs/2512.07062</guid>
<content:encoded><![CDATA[
arXiv:2512.07062v1 Announce Type: new 
Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent Homology-Guided Frequency Filtering for Image Compression</title>
<link>https://arxiv.org/abs/2512.07065</link>
<guid>https://arxiv.org/abs/2512.07065</guid>
<content:encoded><![CDATA[
arXiv:2512.07065v1 Announce Type: new 
Abstract: Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-measure: Contextualizing Metric for Camouflage</title>
<link>https://arxiv.org/abs/2512.07076</link>
<guid>https://arxiv.org/abs/2512.07076</guid>
<content:encoded><![CDATA[
arXiv:2512.07076v1 Announce Type: new 
Abstract: Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection</title>
<link>https://arxiv.org/abs/2512.07078</link>
<guid>https://arxiv.org/abs/2512.07078</guid>
<content:encoded><![CDATA[
arXiv:2512.07078v1 Announce Type: new 
Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision</title>
<link>https://arxiv.org/abs/2512.07107</link>
<guid>https://arxiv.org/abs/2512.07107</guid>
<content:encoded><![CDATA[
arXiv:2512.07107v1 Announce Type: new 
Abstract: We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection</title>
<link>https://arxiv.org/abs/2512.07110</link>
<guid>https://arxiv.org/abs/2512.07110</guid>
<content:encoded><![CDATA[
arXiv:2512.07110v1 Announce Type: new 
Abstract: Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \textbf{representation} and \textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-free Clothing Region of Interest Self-correction for Virtual Try-On</title>
<link>https://arxiv.org/abs/2512.07126</link>
<guid>https://arxiv.org/abs/2512.07126</guid>
<content:encoded><![CDATA[
arXiv:2512.07126v1 Announce Type: new 
Abstract: VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP</title>
<link>https://arxiv.org/abs/2512.07128</link>
<guid>https://arxiv.org/abs/2512.07128</guid>
<content:encoded><![CDATA[
arXiv:2512.07128v1 Announce Type: new 
Abstract: Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07135</link>
<guid>https://arxiv.org/abs/2512.07135</guid>
<content:encoded><![CDATA[
arXiv:2512.07135v1 Announce Type: new 
Abstract: Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2512.07136</link>
<guid>https://arxiv.org/abs/2512.07136</guid>
<content:encoded><![CDATA[
arXiv:2512.07136v1 Announce Type: new 
Abstract: Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models</title>
<link>https://arxiv.org/abs/2512.07141</link>
<guid>https://arxiv.org/abs/2512.07141</guid>
<content:encoded><![CDATA[
arXiv:2512.07141v1 Announce Type: new 
Abstract: As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</title>
<link>https://arxiv.org/abs/2512.07155</link>
<guid>https://arxiv.org/abs/2512.07155</guid>
<content:encoded><![CDATA[
arXiv:2512.07155v1 Announce Type: new 
Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation</title>
<link>https://arxiv.org/abs/2512.07165</link>
<guid>https://arxiv.org/abs/2512.07165</guid>
<content:encoded><![CDATA[
arXiv:2512.07165v1 Announce Type: new 
Abstract: Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing</title>
<link>https://arxiv.org/abs/2512.07166</link>
<guid>https://arxiv.org/abs/2512.07166</guid>
<content:encoded><![CDATA[
arXiv:2512.07166v1 Announce Type: new 
Abstract: Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach</title>
<link>https://arxiv.org/abs/2512.07170</link>
<guid>https://arxiv.org/abs/2512.07170</guid>
<content:encoded><![CDATA[
arXiv:2512.07170v1 Announce Type: new 
Abstract: Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration</title>
<link>https://arxiv.org/abs/2512.07171</link>
<guid>https://arxiv.org/abs/2512.07171</guid>
<content:encoded><![CDATA[
arXiv:2512.07171v1 Announce Type: new 
Abstract: Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\underline{t}$wo stage $\underline{i}$nverse $\underline{d}$egradation $\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>START: Spatial and Textual Learning for Chart Understanding</title>
<link>https://arxiv.org/abs/2512.07186</link>
<guid>https://arxiv.org/abs/2512.07186</guid>
<content:encoded><![CDATA[
arXiv:2512.07186v1 Announce Type: new 
Abstract: Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification</title>
<link>https://arxiv.org/abs/2512.07190</link>
<guid>https://arxiv.org/abs/2512.07190</guid>
<content:encoded><![CDATA[
arXiv:2512.07190v1 Announce Type: new 
Abstract: Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction</title>
<link>https://arxiv.org/abs/2512.07191</link>
<guid>https://arxiv.org/abs/2512.07191</guid>
<content:encoded><![CDATA[
arXiv:2512.07191v1 Announce Type: new 
Abstract: Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression</title>
<link>https://arxiv.org/abs/2512.07192</link>
<guid>https://arxiv.org/abs/2512.07192</guid>
<content:encoded><![CDATA[
arXiv:2512.07192v1 Announce Type: new 
Abstract: Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07197</link>
<guid>https://arxiv.org/abs/2512.07197</guid>
<content:encoded><![CDATA[
arXiv:2512.07197v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Storytelling Images with Rich Chains-of-Reasoning</title>
<link>https://arxiv.org/abs/2512.07198</link>
<guid>https://arxiv.org/abs/2512.07198</guid>
<content:encoded><![CDATA[
arXiv:2512.07198v1 Announce Type: new 
Abstract: An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Diffusion Models via Code Execution</title>
<link>https://arxiv.org/abs/2512.07201</link>
<guid>https://arxiv.org/abs/2512.07201</guid>
<content:encoded><![CDATA[
arXiv:2512.07201v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning</title>
<link>https://arxiv.org/abs/2512.07203</link>
<guid>https://arxiv.org/abs/2512.07203</guid>
<content:encoded><![CDATA[
arXiv:2512.07203v1 Announce Type: new 
Abstract: Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT</title>
<link>https://arxiv.org/abs/2512.07206</link>
<guid>https://arxiv.org/abs/2512.07206</guid>
<content:encoded><![CDATA[
arXiv:2512.07206v1 Announce Type: new 
Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds</title>
<link>https://arxiv.org/abs/2512.07211</link>
<guid>https://arxiv.org/abs/2512.07211</guid>
<content:encoded><![CDATA[
arXiv:2512.07211v1 Announce Type: new 
Abstract: Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.
  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation</title>
<link>https://arxiv.org/abs/2512.07215</link>
<guid>https://arxiv.org/abs/2512.07215</guid>
<content:encoded><![CDATA[
arXiv:2512.07215v1 Announce Type: new 
Abstract: Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Protective Perturbation against DeepFake Face Swapping</title>
<link>https://arxiv.org/abs/2512.07228</link>
<guid>https://arxiv.org/abs/2512.07228</guid>
<content:encoded><![CDATA[
arXiv:2512.07228v1 Announce Type: new 
Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2512.07229</link>
<guid>https://arxiv.org/abs/2512.07229</guid>
<content:encoded><![CDATA[
arXiv:2512.07229v1 Announce Type: new 
Abstract: Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STRinGS: Selective Text Refinement in Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07230</link>
<guid>https://arxiv.org/abs/2512.07230</guid>
<content:encoded><![CDATA[
arXiv:2512.07230v1 Announce Type: new 
Abstract: Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07234</link>
<guid>https://arxiv.org/abs/2512.07234</guid>
<content:encoded><![CDATA[
arXiv:2512.07234v1 Announce Type: new 
Abstract: Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Camera Positional Encoding for Controlled Video Generation</title>
<link>https://arxiv.org/abs/2512.07237</link>
<guid>https://arxiv.org/abs/2512.07237</guid>
<content:encoded><![CDATA[
arXiv:2512.07237v1 Announce Type: new 
Abstract: Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture</title>
<link>https://arxiv.org/abs/2512.07241</link>
<guid>https://arxiv.org/abs/2512.07241</guid>
<content:encoded><![CDATA[
arXiv:2512.07241v1 Announce Type: new 
Abstract: Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Textual Explanations via Translating Decision-Critical Features</title>
<link>https://arxiv.org/abs/2512.07245</link>
<guid>https://arxiv.org/abs/2512.07245</guid>
<content:encoded><![CDATA[
arXiv:2512.07245v1 Announce Type: new 
Abstract: Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</title>
<link>https://arxiv.org/abs/2512.07247</link>
<guid>https://arxiv.org/abs/2512.07247</guid>
<content:encoded><![CDATA[
arXiv:2512.07247v1 Announce Type: new 
Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement</title>
<link>https://arxiv.org/abs/2512.07251</link>
<guid>https://arxiv.org/abs/2512.07251</guid>
<content:encoded><![CDATA[
arXiv:2512.07251v1 Announce Type: new 
Abstract: Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement</title>
<link>https://arxiv.org/abs/2512.07253</link>
<guid>https://arxiv.org/abs/2512.07253</guid>
<content:encoded><![CDATA[
arXiv:2512.07253v1 Announce Type: new 
Abstract: Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A graph generation pipeline for critical infrastructures based on heuristics, images and depth data</title>
<link>https://arxiv.org/abs/2512.07269</link>
<guid>https://arxiv.org/abs/2512.07269</guid>
<content:encoded><![CDATA[
arXiv:2512.07269v1 Announce Type: new 
Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation</title>
<link>https://arxiv.org/abs/2512.07273</link>
<guid>https://arxiv.org/abs/2512.07273</guid>
<content:encoded><![CDATA[
arXiv:2512.07273v1 Announce Type: new 
Abstract: Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation</title>
<link>https://arxiv.org/abs/2512.07275</link>
<guid>https://arxiv.org/abs/2512.07275</guid>
<content:encoded><![CDATA[
arXiv:2512.07275v1 Announce Type: new 
Abstract: In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery</title>
<link>https://arxiv.org/abs/2512.07276</link>
<guid>https://arxiv.org/abs/2512.07276</guid>
<content:encoded><![CDATA[
arXiv:2512.07276v1 Announce Type: new 
Abstract: Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts</title>
<link>https://arxiv.org/abs/2512.07302</link>
<guid>https://arxiv.org/abs/2512.07302</guid>
<content:encoded><![CDATA[
arXiv:2512.07302v1 Announce Type: new 
Abstract: Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset</title>
<link>https://arxiv.org/abs/2512.07305</link>
<guid>https://arxiv.org/abs/2512.07305</guid>
<content:encoded><![CDATA[
arXiv:2512.07305v1 Announce Type: new 
Abstract: This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2512.07328</link>
<guid>https://arxiv.org/abs/2512.07328</guid>
<content:encoded><![CDATA[
arXiv:2512.07328v1 Announce Type: new 
Abstract: Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers</title>
<link>https://arxiv.org/abs/2512.07331</link>
<guid>https://arxiv.org/abs/2512.07331</guid>
<content:encoded><![CDATA[
arXiv:2512.07331v1 Announce Type: new 
Abstract: Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a "U-shaped" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this "Inductive Bottleneck" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively "learning" a bottleneck to isolate semantic features.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Referring Expression Segmentation on Aerial Photos</title>
<link>https://arxiv.org/abs/2512.07338</link>
<guid>https://arxiv.org/abs/2512.07338</guid>
<content:encoded><![CDATA[
arXiv:2512.07338v1 Announce Type: new 
Abstract: Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07345</link>
<guid>https://arxiv.org/abs/2512.07345</guid>
<content:encoded><![CDATA[
arXiv:2512.07345v1 Announce Type: new 
Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition</title>
<link>https://arxiv.org/abs/2512.07348</link>
<guid>https://arxiv.org/abs/2512.07348</guid>
<content:encoded><![CDATA[
arXiv:2512.07348v1 Announce Type: new 
Abstract: In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&amp;Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&amp;Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection</title>
<link>https://arxiv.org/abs/2512.07351</link>
<guid>https://arxiv.org/abs/2512.07351</guid>
<content:encoded><![CDATA[
arXiv:2512.07351v1 Announce Type: new 
Abstract: The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.07360</link>
<guid>https://arxiv.org/abs/2512.07360</guid>
<content:encoded><![CDATA[
arXiv:2512.07360v1 Announce Type: new 
Abstract: Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency</title>
<link>https://arxiv.org/abs/2512.07379</link>
<guid>https://arxiv.org/abs/2512.07379</guid>
<content:encoded><![CDATA[
arXiv:2512.07379v1 Announce Type: new 
Abstract: This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects</title>
<link>https://arxiv.org/abs/2512.07381</link>
<guid>https://arxiv.org/abs/2512.07381</guid>
<content:encoded><![CDATA[
arXiv:2512.07381v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogicCBMs: Logic-Enhanced Concept-Based Learning</title>
<link>https://arxiv.org/abs/2512.07383</link>
<guid>https://arxiv.org/abs/2512.07383</guid>
<content:encoded><![CDATA[
arXiv:2512.07383v1 Announce Type: new 
Abstract: Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline</title>
<link>https://arxiv.org/abs/2512.07385</link>
<guid>https://arxiv.org/abs/2512.07385</guid>
<content:encoded><![CDATA[
arXiv:2512.07385v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring</title>
<link>https://arxiv.org/abs/2512.07391</link>
<guid>https://arxiv.org/abs/2512.07391</guid>
<content:encoded><![CDATA[
arXiv:2512.07391v1 Announce Type: new 
Abstract: Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstructing Objects along Hand Interaction Timelines in Egocentric Video</title>
<link>https://arxiv.org/abs/2512.07394</link>
<guid>https://arxiv.org/abs/2512.07394</guid>
<content:encoded><![CDATA[
arXiv:2512.07394v1 Announce Type: new 
Abstract: We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs</title>
<link>https://arxiv.org/abs/2512.07410</link>
<guid>https://arxiv.org/abs/2512.07410</guid>
<content:encoded><![CDATA[
arXiv:2512.07410v1 Announce Type: new 
Abstract: Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven Exploration of Mobility Interaction Patterns</title>
<link>https://arxiv.org/abs/2512.07415</link>
<guid>https://arxiv.org/abs/2512.07415</guid>
<content:encoded><![CDATA[
arXiv:2512.07415v1 Announce Type: new 
Abstract: Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When normalization hallucinates: unseen risks in AI-powered whole slide image processing</title>
<link>https://arxiv.org/abs/2512.07426</link>
<guid>https://arxiv.org/abs/2512.07426</guid>
<content:encoded><![CDATA[
arXiv:2512.07426v1 Announce Type: new 
Abstract: Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Video Editing with Temporal Reasoner</title>
<link>https://arxiv.org/abs/2512.07469</link>
<guid>https://arxiv.org/abs/2512.07469</guid>
<content:encoded><![CDATA[
arXiv:2512.07469v1 Announce Type: new 
Abstract: Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance</title>
<link>https://arxiv.org/abs/2512.07480</link>
<guid>https://arxiv.org/abs/2512.07480</guid>
<content:encoded><![CDATA[
arXiv:2512.07480v1 Announce Type: new 
Abstract: While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior</title>
<link>https://arxiv.org/abs/2512.07498</link>
<guid>https://arxiv.org/abs/2512.07498</guid>
<content:encoded><![CDATA[
arXiv:2512.07498v1 Announce Type: new 
Abstract: Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer</title>
<link>https://arxiv.org/abs/2512.07500</link>
<guid>https://arxiv.org/abs/2512.07500</guid>
<content:encoded><![CDATA[
arXiv:2512.07500v1 Announce Type: new 
Abstract: Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.07503</link>
<guid>https://arxiv.org/abs/2512.07503</guid>
<content:encoded><![CDATA[
arXiv:2512.07503v1 Announce Type: new 
Abstract: Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\times$ to $3\times$ inference latency reduction and $2\times$ to $7\times$ step compression, while preserving visual quality with no observable degradation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points</title>
<link>https://arxiv.org/abs/2512.07504</link>
<guid>https://arxiv.org/abs/2512.07504</guid>
<content:encoded><![CDATA[
arXiv:2512.07504v1 Announce Type: new 
Abstract: Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshRipple: Structured Autoregressive Generation of Artist-Meshes</title>
<link>https://arxiv.org/abs/2512.07514</link>
<guid>https://arxiv.org/abs/2512.07514</guid>
<content:encoded><![CDATA[
arXiv:2512.07514v1 Announce Type: new 
Abstract: Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images</title>
<link>https://arxiv.org/abs/2512.07527</link>
<guid>https://arxiv.org/abs/2512.07527</guid>
<content:encoded><![CDATA[
arXiv:2512.07527v1 Announce Type: new 
Abstract: City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.
  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.
  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07564</link>
<guid>https://arxiv.org/abs/2512.07564</guid>
<content:encoded><![CDATA[
arXiv:2512.07564v1 Announce Type: new 
Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation</title>
<link>https://arxiv.org/abs/2512.07568</link>
<guid>https://arxiv.org/abs/2512.07568</guid>
<content:encoded><![CDATA[
arXiv:2512.07568v1 Announce Type: new 
Abstract: Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while na\"ive fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs</title>
<link>https://arxiv.org/abs/2512.07580</link>
<guid>https://arxiv.org/abs/2512.07580</guid>
<content:encoded><![CDATA[
arXiv:2512.07580v1 Announce Type: new 
Abstract: Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by "vanishing token information", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as "information horizon", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongCat-Image Technical Report</title>
<link>https://arxiv.org/abs/2512.07584</link>
<guid>https://arxiv.org/abs/2512.07584</guid>
<content:encoded><![CDATA[
arXiv:2512.07584v1 Announce Type: new 
Abstract: We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation</title>
<link>https://arxiv.org/abs/2512.07590</link>
<guid>https://arxiv.org/abs/2512.07590</guid>
<content:encoded><![CDATA[
arXiv:2512.07590v1 Announce Type: new 
Abstract: To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery</title>
<link>https://arxiv.org/abs/2512.07596</link>
<guid>https://arxiv.org/abs/2512.07596</guid>
<content:encoded><![CDATA[
arXiv:2512.07596v1 Announce Type: new 
Abstract: The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Segment Any 3D Thing as Instance Tracking</title>
<link>https://arxiv.org/abs/2512.07599</link>
<guid>https://arxiv.org/abs/2512.07599</guid>
<content:encoded><![CDATA[
arXiv:2512.07599v1 Announce Type: new 
Abstract: Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decomposition Sampling for Efficient Region Annotations in Active Learning</title>
<link>https://arxiv.org/abs/2512.07606</link>
<guid>https://arxiv.org/abs/2512.07606</guid>
<content:encoded><![CDATA[
arXiv:2512.07606v1 Announce Type: new 
Abstract: Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation</title>
<link>https://arxiv.org/abs/2512.07628</link>
<guid>https://arxiv.org/abs/2512.07628</guid>
<content:encoded><![CDATA[
arXiv:2512.07628v1 Announce Type: new 
Abstract: Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method</title>
<link>https://arxiv.org/abs/2512.07651</link>
<guid>https://arxiv.org/abs/2512.07651</guid>
<content:encoded><![CDATA[
arXiv:2512.07651v1 Announce Type: new 
Abstract: Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research</title>
<link>https://arxiv.org/abs/2512.07652</link>
<guid>https://arxiv.org/abs/2512.07652</guid>
<content:encoded><![CDATA[
arXiv:2512.07652v1 Announce Type: new 
Abstract: Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization-Guided Diffusion for Interactive Scene Generation</title>
<link>https://arxiv.org/abs/2512.07661</link>
<guid>https://arxiv.org/abs/2512.07661</guid>
<content:encoded><![CDATA[
arXiv:2512.07661v1 Announce Type: new 
Abstract: Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset</title>
<link>https://arxiv.org/abs/2512.07668</link>
<guid>https://arxiv.org/abs/2512.07668</guid>
<content:encoded><![CDATA[
arXiv:2512.07668v1 Announce Type: new 
Abstract: We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations</title>
<link>https://arxiv.org/abs/2512.07674</link>
<guid>https://arxiv.org/abs/2512.07674</guid>
<content:encoded><![CDATA[
arXiv:2512.07674v1 Announce Type: new 
Abstract: Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only</title>
<link>https://arxiv.org/abs/2512.07698</link>
<guid>https://arxiv.org/abs/2512.07698</guid>
<content:encoded><![CDATA[
arXiv:2512.07698v1 Announce Type: new 
Abstract: Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment</title>
<link>https://arxiv.org/abs/2512.07702</link>
<guid>https://arxiv.org/abs/2512.07702</guid>
<content:encoded><![CDATA[
arXiv:2512.07702v1 Announce Type: new 
Abstract: Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PVeRA: Probabilistic Vector-Based Random Matrix Adaptation</title>
<link>https://arxiv.org/abs/2512.07703</link>
<guid>https://arxiv.org/abs/2512.07703</guid>
<content:encoded><![CDATA[
arXiv:2512.07703v1 Announce Type: new 
Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnCageNet: Tracking and Pose Estimation of Caged Animal</title>
<link>https://arxiv.org/abs/2512.07712</link>
<guid>https://arxiv.org/abs/2512.07712</guid>
<content:encoded><![CDATA[
arXiv:2512.07712v1 Announce Type: new 
Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation</title>
<link>https://arxiv.org/abs/2512.07720</link>
<guid>https://arxiv.org/abs/2512.07720</guid>
<content:encoded><![CDATA[
arXiv:2512.07720v1 Announce Type: new 
Abstract: Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving action classification with brain-inspired deep networks</title>
<link>https://arxiv.org/abs/2512.07729</link>
<guid>https://arxiv.org/abs/2512.07729</guid>
<content:encoded><![CDATA[
arXiv:2512.07729v1 Announce Type: new 
Abstract: Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination</title>
<link>https://arxiv.org/abs/2512.07730</link>
<guid>https://arxiv.org/abs/2512.07730</guid>
<content:encoded><![CDATA[
arXiv:2512.07730v1 Announce Type: new 
Abstract: Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\%p improvement in CHAIR\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery</title>
<link>https://arxiv.org/abs/2512.07733</link>
<guid>https://arxiv.org/abs/2512.07733</guid>
<content:encoded><![CDATA[
arXiv:2512.07733v1 Announce Type: new 
Abstract: Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HLTCOE Evaluation Team at TREC 2025: VQA Track</title>
<link>https://arxiv.org/abs/2512.07738</link>
<guid>https://arxiv.org/abs/2512.07738</guid>
<content:encoded><![CDATA[
arXiv:2512.07738v1 Announce Type: new 
Abstract: The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.07745</link>
<guid>https://arxiv.org/abs/2512.07745</guid>
<content:encoded><![CDATA[
arXiv:2512.07745v1 Announce Type: new 
Abstract: Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.07747</link>
<guid>https://arxiv.org/abs/2512.07747</guid>
<content:encoded><![CDATA[
arXiv:2512.07747v1 Announce Type: new 
Abstract: Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction</title>
<link>https://arxiv.org/abs/2512.07756</link>
<guid>https://arxiv.org/abs/2512.07756</guid>
<content:encoded><![CDATA[
arXiv:2512.07756v1 Announce Type: new 
Abstract: Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2512.07760</link>
<guid>https://arxiv.org/abs/2512.07760</guid>
<content:encoded><![CDATA[
arXiv:2512.07760v1 Announce Type: new 
Abstract: Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring</title>
<link>https://arxiv.org/abs/2512.07776</link>
<guid>https://arxiv.org/abs/2512.07776</guid>
<content:encoded><![CDATA[
arXiv:2512.07776v1 Announce Type: new 
Abstract: Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, "in-the-wild" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution Matching Variational AutoEncoder</title>
<link>https://arxiv.org/abs/2512.07778</link>
<guid>https://arxiv.org/abs/2512.07778</guid>
<content:encoded><![CDATA[
arXiv:2512.07778v1 Announce Type: new 
Abstract: Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \textbf{Distribution-Matching VAE} (\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory</title>
<link>https://arxiv.org/abs/2512.07802</link>
<guid>https://arxiv.org/abs/2512.07802</guid>
<content:encoded><![CDATA[
arXiv:2512.07802v1 Announce Type: new 
Abstract: Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-view Pyramid Transformer: Look Coarser to See Broader</title>
<link>https://arxiv.org/abs/2512.07806</link>
<guid>https://arxiv.org/abs/2512.07806</guid>
<content:encoded><![CDATA[
arXiv:2512.07806v1 Announce Type: new 
Abstract: We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details," MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes</title>
<link>https://arxiv.org/abs/2512.07807</link>
<guid>https://arxiv.org/abs/2512.07807</guid>
<content:encoded><![CDATA[
arXiv:2512.07807v1 Announce Type: new 
Abstract: Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling</title>
<link>https://arxiv.org/abs/2512.07821</link>
<guid>https://arxiv.org/abs/2512.07821</guid>
<content:encoded><![CDATA[
arXiv:2512.07821v1 Announce Type: new 
Abstract: Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing</title>
<link>https://arxiv.org/abs/2512.07826</link>
<guid>https://arxiv.org/abs/2512.07826</guid>
<content:encoded><![CDATA[
arXiv:2512.07826v1 Announce Type: new 
Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</title>
<link>https://arxiv.org/abs/2512.07829</link>
<guid>https://arxiv.org/abs/2512.07829</guid>
<content:encoded><![CDATA[
arXiv:2512.07829v1 Announce Type: new 
Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</title>
<link>https://arxiv.org/abs/2512.07831</link>
<guid>https://arxiv.org/abs/2512.07831</guid>
<content:encoded><![CDATA[
arXiv:2512.07831v1 Announce Type: new 
Abstract: Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Visual Similarity</title>
<link>https://arxiv.org/abs/2512.07833</link>
<guid>https://arxiv.org/abs/2512.07833</guid>
<content:encoded><![CDATA[
arXiv:2512.07833v1 Announce Type: new 
Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Voxify3D: Pixel Art Meets Volumetric Rendering</title>
<link>https://arxiv.org/abs/2512.07834</link>
<guid>https://arxiv.org/abs/2512.07834</guid>
<content:encoded><![CDATA[
arXiv:2512.07834v1 Announce Type: new 
Abstract: Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger is not better: Better Augmentations in Contrastive Learning for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.05992</link>
<guid>https://arxiv.org/abs/2512.05992</guid>
<content:encoded><![CDATA[
arXiv:2512.05992v1 Announce Type: cross 
Abstract: Self-supervised contrastive learning is among the recent representation learning methods that have shown performance gains in several downstream tasks including semantic segmentation. This paper evaluates strong data augmentation, one of the most important components for self-supervised contrastive learning's improved performance. Strong data augmentation involves applying the composition of multiple augmentation techniques on images. Surprisingly, we find that the existing data augmentations do not always improve performance for semantic segmentation for medical images. We experiment with other augmentations that provide improved performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Temporal Single-photon LiDAR</title>
<link>https://arxiv.org/abs/2512.06008</link>
<guid>https://arxiv.org/abs/2512.06008</guid>
<content:encoded><![CDATA[
arXiv:2512.06008v1 Announce Type: cross 
Abstract: Temporal single-photon (TSP-) LiDAR presents a promising solution for imaging-free target recognition over long distances with reduced size, cost, and power consumption. However, existing TSP-LiDAR approaches are ineffective in handling open-set scenarios where unknown targets emerge, and they suffer significant performance degradation under low signal-to-noise ratio (SNR) and short acquisition times (fewer photons). Here, inspired by semantic communication, we propose a semantic TSP-LiDAR based on a self-updating semantic knowledge base (SKB), in which the target recognition processing of TSP-LiDAR is formulated as a semantic communication. The results, both simulation and experiment, demonstrate that our approach surpasses conventional methods, particularly under challenging conditions of low SNR and limited acquisition time. More importantly, our self-updating SKB mechanism can dynamically update the semantic features of newly encountered targets in the SKB, enabling continuous adaptation without the need for extensive retraining of the neural network. In fact, a recognition accuracy of 89% is achieved on nine types of unknown targets in real-world experiments, compared to 66% without the updating mechanism. These findings highlight the potential of our framework for adaptive and robust target recognition in complex and dynamic environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</title>
<link>https://arxiv.org/abs/2512.06147</link>
<guid>https://arxiv.org/abs/2512.06147</guid>
<content:encoded><![CDATA[
arXiv:2512.06147v1 Announce Type: cross 
Abstract: While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\&amp;M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</title>
<link>https://arxiv.org/abs/2512.06589</link>
<guid>https://arxiv.org/abs/2512.06589</guid>
<content:encoded><![CDATA[
arXiv:2512.06589v1 Announce Type: cross 
Abstract: Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Quantization using Gaussian Variational Autoencoder</title>
<link>https://arxiv.org/abs/2512.06609</link>
<guid>https://arxiv.org/abs/2512.06609</guid>
<content:encoded><![CDATA[
arXiv:2512.06609v1 Announce Type: cross 
Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment</title>
<link>https://arxiv.org/abs/2512.06628</link>
<guid>https://arxiv.org/abs/2512.06628</guid>
<content:encoded><![CDATA[
arXiv:2512.06628v1 Announce Type: cross 
Abstract: Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network</title>
<link>https://arxiv.org/abs/2512.06648</link>
<guid>https://arxiv.org/abs/2512.06648</guid>
<content:encoded><![CDATA[
arXiv:2512.06648v1 Announce Type: cross 
Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning</title>
<link>https://arxiv.org/abs/2512.06649</link>
<guid>https://arxiv.org/abs/2512.06649</guid>
<content:encoded><![CDATA[
arXiv:2512.06649v1 Announce Type: cross 
Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods</title>
<link>https://arxiv.org/abs/2512.06665</link>
<guid>https://arxiv.org/abs/2512.06665</guid>
<content:encoded><![CDATA[
arXiv:2512.06665v1 Announce Type: cross 
Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data</title>
<link>https://arxiv.org/abs/2512.06730</link>
<guid>https://arxiv.org/abs/2512.06730</guid>
<content:encoded><![CDATA[
arXiv:2512.06730v1 Announce Type: cross 
Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v1 Announce Type: cross 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XM-ALIGN: Unified Cross-Modal Embedding Alignment for Face-Voice Association</title>
<link>https://arxiv.org/abs/2512.06757</link>
<guid>https://arxiv.org/abs/2512.06757</guid>
<content:encoded><![CDATA[
arXiv:2512.06757v1 Announce Type: cross 
Abstract: This paper introduces our solution, XM-ALIGN (Unified Cross-Modal Embedding Alignment Framework), proposed for the FAME challenge at ICASSP 2026. Our framework combines explicit and implicit alignment mechanisms, significantly improving cross-modal verification performance in both "heard" and "unheard" languages. By extracting feature embeddings from both face and voice encoders and jointly optimizing them using a shared classifier, we employ mean squared error (MSE) as the embedding alignment loss to ensure tight alignment between modalities. Additionally, data augmentation strategies are applied during model training to enhance generalization. Experimental results show that our approach demonstrates superior performance on the MAV-Celeb dataset. The code will be released at https://github.com/PunkMale/XM-ALIGN.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices</title>
<link>https://arxiv.org/abs/2512.06848</link>
<guid>https://arxiv.org/abs/2512.06848</guid>
<content:encoded><![CDATA[
arXiv:2512.06848v1 Announce Type: cross 
Abstract: Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Visual SLAM using a General 3D Prior</title>
<link>https://arxiv.org/abs/2512.06868</link>
<guid>https://arxiv.org/abs/2512.06868</guid>
<content:encoded><![CDATA[
arXiv:2512.06868v1 Announce Type: cross 
Abstract: Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[
arXiv:2512.06951v1 Announce Type: cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</title>
<link>https://arxiv.org/abs/2512.06963</link>
<guid>https://arxiv.org/abs/2512.06963</guid>
<content:encoded><![CDATA[
arXiv:2512.06963v1 Announce Type: cross 
Abstract: Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients</title>
<link>https://arxiv.org/abs/2512.06990</link>
<guid>https://arxiv.org/abs/2512.06990</guid>
<content:encoded><![CDATA[
arXiv:2512.06990v1 Announce Type: cross 
Abstract: Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis</title>
<link>https://arxiv.org/abs/2512.07040</link>
<guid>https://arxiv.org/abs/2512.07040</guid>
<content:encoded><![CDATA[
arXiv:2512.07040v1 Announce Type: cross 
Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.07130</link>
<guid>https://arxiv.org/abs/2512.07130</guid>
<content:encoded><![CDATA[
arXiv:2512.07130v1 Announce Type: cross 
Abstract: End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.07132</link>
<guid>https://arxiv.org/abs/2512.07132</guid>
<content:encoded><![CDATA[
arXiv:2512.07132v1 Announce Type: cross 
Abstract: Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search</title>
<link>https://arxiv.org/abs/2512.07142</link>
<guid>https://arxiv.org/abs/2512.07142</guid>
<content:encoded><![CDATA[
arXiv:2512.07142v1 Announce Type: cross 
Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers</title>
<link>https://arxiv.org/abs/2512.07150</link>
<guid>https://arxiv.org/abs/2512.07150</guid>
<content:encoded><![CDATA[
arXiv:2512.07150v1 Announce Type: cross 
Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics</title>
<link>https://arxiv.org/abs/2512.07224</link>
<guid>https://arxiv.org/abs/2512.07224</guid>
<content:encoded><![CDATA[
arXiv:2512.07224v1 Announce Type: cross 
Abstract: Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affine Subspace Models and Clustering for Patch-Based Image Denoising</title>
<link>https://arxiv.org/abs/2512.07259</link>
<guid>https://arxiv.org/abs/2512.07259</guid>
<content:encoded><![CDATA[
arXiv:2512.07259v1 Announce Type: cross 
Abstract: Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Geometric Unification of Concept Learning with Concept Cones</title>
<link>https://arxiv.org/abs/2512.07355</link>
<guid>https://arxiv.org/abs/2512.07355</guid>
<content:encoded><![CDATA[
arXiv:2512.07355v1 Announce Type: cross 
Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood</title>
<link>https://arxiv.org/abs/2512.07390</link>
<guid>https://arxiv.org/abs/2512.07390</guid>
<content:encoded><![CDATA[
arXiv:2512.07390v1 Announce Type: cross 
Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models</title>
<link>https://arxiv.org/abs/2512.07419</link>
<guid>https://arxiv.org/abs/2512.07419</guid>
<content:encoded><![CDATA[
arXiv:2512.07419v1 Announce Type: cross 
Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</title>
<link>https://arxiv.org/abs/2512.07437</link>
<guid>https://arxiv.org/abs/2512.07437</guid>
<content:encoded><![CDATA[
arXiv:2512.07437v1 Announce Type: cross 
Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Geometry Distribution for 3D Animation Generation</title>
<link>https://arxiv.org/abs/2512.07459</link>
<guid>https://arxiv.org/abs/2512.07459</guid>
<content:encoded><![CDATA[
arXiv:2512.07459v1 Announce Type: cross 
Abstract: Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \times$ higher user study score), achieving the best results across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces</title>
<link>https://arxiv.org/abs/2512.07509</link>
<guid>https://arxiv.org/abs/2512.07509</guid>
<content:encoded><![CDATA[
arXiv:2512.07509v1 Announce Type: cross 
Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLaX: Reasoning with Latent Exploration for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2512.07558</link>
<guid>https://arxiv.org/abs/2512.07558</guid>
<content:encoded><![CDATA[
arXiv:2512.07558v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework</title>
<link>https://arxiv.org/abs/2512.07574</link>
<guid>https://arxiv.org/abs/2512.07574</guid>
<content:encoded><![CDATA[
arXiv:2512.07574v1 Announce Type: cross 
Abstract: Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation</title>
<link>https://arxiv.org/abs/2512.07576</link>
<guid>https://arxiv.org/abs/2512.07576</guid>
<content:encoded><![CDATA[
arXiv:2512.07576v1 Announce Type: cross 
Abstract: Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs</title>
<link>https://arxiv.org/abs/2512.07687</link>
<guid>https://arxiv.org/abs/2512.07687</guid>
<content:encoded><![CDATA[
arXiv:2512.07687v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep transfer learning for image classification: a survey</title>
<link>https://arxiv.org/abs/2205.09904</link>
<guid>https://arxiv.org/abs/2205.09904</guid>
<content:encoded><![CDATA[
arXiv:2205.09904v2 Announce Type: replace 
Abstract: Deep neural networks such as convolutional neural networks (CNNs) and transformers have achieved many successes in image classification in recent years. It has been consistently demonstrated that best practice for image classification is when large deep models can be trained on abundant labelled data. However there are many real world scenarios where the requirement for large amounts of training data to get the best performance cannot be met. In these scenarios transfer learning can help improve performance. To date there have been no surveys that comprehensively review deep transfer learning as it relates to image classification overall. However, several recent general surveys of deep transfer learning and ones that relate to particular specialised target image classification tasks have been published. We believe it is important for the future progress in the field that all current knowledge is collated and the overarching patterns analysed and discussed. In this survey we formally define deep transfer learning and the problem it attempts to solve in relation to image classification. We survey the current state of the field and identify where recent progress has been made. We show where the gaps in current knowledge are and make suggestions for how to progress the field to fill in these knowledge gaps. We present a new taxonomy of the applications of transfer learning for image classification. This taxonomy makes it easier to see overarching patterns of where transfer learning has been effective and, where it has failed to fulfill its potential. This also allows us to suggest where the problems lie and how it could be used more effectively. We show that under this new taxonomy, many of the applications where transfer learning has been shown to be ineffective or even hinder performance are to be expected when taking into account the source and target datasets and the techniques used.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping</title>
<link>https://arxiv.org/abs/2303.11228</link>
<guid>https://arxiv.org/abs/2303.11228</guid>
<content:encoded><![CDATA[
arXiv:2303.11228v3 Announce Type: replace 
Abstract: Object segmentation for robotic grasping under dynamic conditions often faces challenges such as occlusion, low light conditions, motion blur and object size variance. To address these challenges, we propose a Deep Learning network that fuses two types of visual signals, event-based data and RGB frame data. The proposed Bimodal SegNet network has two distinct encoders, one for each signal input and a spatial pyramidal pooling with atrous convolutions. Encoders capture rich contextual information by pooling the concatenated features at different resolutions while the decoder obtains sharp object boundaries. The evaluation of the proposed method undertakes five unique image degradation challenges including occlusion, blur, brightness, trajectory and scale variance on the Event-based Segmentation (ESD) Dataset. The evaluation results show a 6-10\% segmentation accuracy improvement over state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. The model code is available at https://github.com/sanket0707/Bimodal-SegNet.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models for Image Restoration and Enhancement: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2308.09388</link>
<guid>https://arxiv.org/abs/2308.09388</guid>
<content:encoded><![CDATA[
arXiv:2308.09388v3 Announce Type: replace 
Abstract: Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation. Recently, the diffusion model has achieved significant advancements in the visual generation of AIGC, thereby raising an intuitive question, "whether diffusion model can boost image restoration". To answer this, some pioneering studies attempt to integrate diffusion models into the image restoration task, resulting in superior performances than previous GAN-based methods. Despite that, a comprehensive and enlightening survey on diffusion model-based image restoration remains scarce. In this paper, we are the first to present a comprehensive review of recent diffusion model-based methods on image restoration, encompassing the learning paradigm, conditional strategy, framework design, modeling strategy, and evaluation. Concretely, we first introduce the background of the diffusion model briefly and then present two prevalent workflows that exploit diffusion models in image restoration. Subsequently, we classify and emphasize the innovative designs using diffusion models for both IR and blind/real-world IR, intending to inspire future development. To evaluate existing methods thoroughly, we summarize the commonly-used dataset, implementation details, and evaluation metrics. Additionally, we present the objective comparison for open-sourced methods across three tasks, including image super-resolution, deblurring, and inpainting. Ultimately, informed by the limitations in existing works, we propose five potential and challenging directions for the future research of diffusion model-based IR, including sampling efficiency, model compression, distortion simulation and estimation, distortion invariant learning, and framework design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Roadside Monocular 3D Detection Prompted by 2D Detection</title>
<link>https://arxiv.org/abs/2404.01064</link>
<guid>https://arxiv.org/abs/2404.01064</guid>
<content:encoded><![CDATA[
arXiv:2404.01064v5 Announce Type: replace 
Abstract: Roadside monocular 3D detection requires detecting objects of predefined classes in an RGB frame and predicting their 3D attributes, such as bird's-eye-view (BEV) locations. It has broad applications in traffic control, vehicle-vehicle communication, and vehicle-infrastructure cooperative perception. To address this task, we introduce Promptable 3D Detector (Pro3D), a novel detector design that leverages 2D detections as prompts. We build our Pro3D upon two key insights. First, compared to a typical 3D detector, a 2D detector is ``easier'' to train due to fewer loss terms and performs significantly better at localizing objects w.r.t 2D metrics. Second, once 2D detections precisely locate objects in the image, a 3D detector can focus on lifting these detections into 3D BEV, especially when fixed camera pose or scene geometry provide an informative prior. To encode and incorporate 2D detections, we explore three methods: (a) concatenating features from both 2D and 3D detectors, (b) attentively fusing 2D and 3D detector features, and (c) encoding properties of predicted 2D bounding boxes \{$x$, $y$, width, height, label\} and attentively fusing them with the 3D detector feature. Interestingly, the third method significantly outperforms the others, underscoring the effectiveness of 2D detections as prompts that offer precise object targets and allow the 3D detector to focus on lifting them into 3D. Pro3D is adaptable for use with a wide range of 2D and 3D detectors with minimal modifications. Comprehensive experiments demonstrate that our Pro3D significantly enhances existing methods, achieving state-of-the-art results on two contemporary benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSP-GNN: Learning to Track via Bilevel Optimization</title>
<link>https://arxiv.org/abs/2407.04308</link>
<guid>https://arxiv.org/abs/2407.04308</guid>
<content:encoded><![CDATA[
arXiv:2407.04308v3 Announce Type: replace 
Abstract: We propose a graph-based tracking formulation for multi-object tracking (MOT) where target detections contain kinematic information and re-identification features (attributes). Our method applies a successive shortest paths (SSP) algorithm to a tracking graph defined over a batch of frames. The edge costs in this tracking graph are computed via a message-passing network, a graph neural network (GNN) variant. The parameters of the GNN, and hence, the tracker, are learned end-to-end on a training set of example ground-truth tracks and detections. Specifically, learning takes the form of bilevel optimization guided by our novel loss function. We evaluate our algorithm on simulated scenarios to understand its sensitivity to scenario aspects and model hyperparameters. Across varied scenario complexities, our method compares favorably to a strong baseline.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JambaTalk: Speech-Driven 3D Talking Head Generation Based on Hybrid Transformer-Mamba Model</title>
<link>https://arxiv.org/abs/2408.01627</link>
<guid>https://arxiv.org/abs/2408.01627</guid>
<content:encoded><![CDATA[
arXiv:2408.01627v3 Announce Type: replace 
Abstract: In recent years, the talking head generation has become a focal point for researchers. Considerable effort is being made to refine lip-sync motion, capture expressive facial expressions, generate natural head poses, and achieve high-quality video. However, no single model has yet achieved equivalence across all quantitative and qualitative metrics. We introduce Jamba, a hybrid Transformer-Mamba model, to animate a 3D face. Mamba, a pioneering Structured State Space Model (SSM) architecture, was developed to overcome the limitations of conventional Transformer architectures, particularly in handling long sequences. This challenge has constrained traditional models. Jamba combines the advantages of both the Transformer and Mamba approaches, offering a comprehensive solution. Based on the foundational Jamba block, we present JambaTalk to enhance motion variety and lip sync through multimodal integration. Extensive experiments reveal that our method achieves performance comparable or superior to state-of-the-art models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Test Time Adaptation with Few-shot Guidance</title>
<link>https://arxiv.org/abs/2409.01341</link>
<guid>https://arxiv.org/abs/2409.01341</guid>
<content:encoded><![CDATA[
arXiv:2409.01341v4 Announce Type: replace 
Abstract: Deep neural networks often encounter significant performance drops while facing with domain shifts between training (source) and test (target) data. To address this issue, Test Time Adaptation (TTA) methods have been proposed to adapt pre-trained source model to handle out-of-distribution streaming target data. Although these methods offer some relief, they lack a reliable mechanism for domain shift correction, which can often be erratic in real-world applications. In response, we develop Few-Shot Test Time Adaptation (FS-TTA), a novel and practical setting that utilizes a few-shot support set on top of TTA. Adhering to the principle of few inputs, big gains, FS-TTA reduces blind exploration in unseen target domains. Furthermore, we propose a two-stage framework to tackle FS-TTA, including (i) fine-tuning the pre-trained source model with few-shot support set, along with using feature diversity augmentation module to avoid overfitting, (ii) implementing test time adaptation based on prototype memory bank guidance to produce high quality pseudo-label for model adaptation. Through extensive experiments on three cross-domain classification benchmarks, we demonstrate the superior performance and reliability of our FS-TTA and framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event-Customized Image Generation</title>
<link>https://arxiv.org/abs/2410.02483</link>
<guid>https://arxiv.org/abs/2410.02483</guid>
<content:encoded><![CDATA[
arXiv:2410.02483v2 Announce Type: replace 
Abstract: Customized Image Generation, generating customized images with user-specified concepts, has raised significant attention due to its creativity and novelty. With impressive progress achieved in subject customization, some pioneer works further explored the customization of action and interaction beyond entity (i.e., human, animal, and object) appearance. However, these approaches only focus on basic actions and interactions between two entities, and their effects are limited by insufficient ''exactly same'' reference images. To extend customized image generation to more complex scenes for general real-world applications, we propose a new task: event-customized image generation. Given a single reference image, we define the ''event'' as all specific actions, poses, relations, or interactions between different entities in the scene. This task aims at accurately capturing the complex event and generating customized images with various target entities. To solve this task, we proposed a novel training-free event customization method: FreeEvent. Specifically, FreeEvent introduces two extra paths alongside the general diffusion denoising process: 1) Entity switching path: it applies cross-attention guidance and regulation for target entity generation. 2) Event transferring path: it injects the spatial feature and self-attention maps from the reference image to the target image for event generation. To further facilitate this new task, we collected two evaluation benchmarks: SWiG-Event and Real-Event. Extensive experiments and ablations have demonstrated the effectiveness of FreeEvent.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RepLDM: Reprogramming Pretrained Latent Diffusion Models for High-Quality, High-Efficiency, High-Resolution Image Generation</title>
<link>https://arxiv.org/abs/2410.06055</link>
<guid>https://arxiv.org/abs/2410.06055</guid>
<content:encoded><![CDATA[
arXiv:2410.06055v2 Announce Type: replace 
Abstract: While latent diffusion models (LDMs), such as Stable Diffusion, are designed for high-resolution (HR) image generation, they often struggle with significant structural distortions when generating images at resolutions higher than their training one. Instead of relying on extensive retraining, a more resource-efficient approach is to reprogram the pretrained model for HR image generation; however, existing methods often result in poor image quality and long inference time. We introduce RepLDM, a novel reprogramming framework for pretrained LDMs that enables high-quality, high-efficiency, high-resolution image generation; see Fig. 1. RepLDM consists of two stages: (i) an attention guidance stage, which generates a latent representation of a higher-quality training-resolution image using a novel training-free self-attention mechanism to enhance the structural consistency; and (ii) a progressive upsampling stage, which progressively performs upsampling in pixel space to mitigate the severe artifacts caused by latent space upsampling. The effective initialization from the first stage allows for denoising at higher resolutions with significantly fewer steps, improving the efficiency. Extensive experimental results demonstrate that RepLDM significantly outperforms state-of-the-art methods in both quality and efficiency for HR image generation, underscoring its advantages for real-world applications. Codes: https://github.com/kmittle/RepLDM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moyun: A Diffusion-Based Model for Style-Specific Chinese Calligraphy Generation</title>
<link>https://arxiv.org/abs/2410.07618</link>
<guid>https://arxiv.org/abs/2410.07618</guid>
<content:encoded><![CDATA[
arXiv:2410.07618v2 Announce Type: replace 
Abstract: Although Chinese calligraphy generation has achieved style transfer, generating calligraphy by specifying the calligrapher, font, and character style remains challenging. To address this, we propose a new Chinese calligraphy generation model 'Moyun' , which replaces the Unet in the Diffusion model with Vision Mamba and introduces the TripleLabel control mechanism to achieve controllable calligraphy generation. The model was tested on our large-scale dataset 'Mobao' of over 1.9 million images, and the results demonstrate that 'Moyun' can effectively control the generation process and produce calligraphy in the specified style. Even for calligraphy the calligrapher has not written, 'Moyun' can generate calligraphy that matches the style of the calligrapher.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenizing Motion: A Generative Approach for Scene Dynamics Compression</title>
<link>https://arxiv.org/abs/2410.09768</link>
<guid>https://arxiv.org/abs/2410.09768</guid>
<content:encoded><![CDATA[
arXiv:2410.09768v3 Announce Type: replace 
Abstract: This paper proposes a novel generative video compression framework that leverages motion pattern priors, derived from subtle dynamics in common scenes (e.g., swaying flowers or a boat drifting on water), rather than relying on video content priors (e.g., talking faces or human bodies). These compact motion priors enable a new approach to ultra-low bitrate communication while achieving high-quality reconstruction across diverse scene contents. At the encoder side, motion priors can be streamlined into compact representations via a dense-to-sparse transformation. At the decoder side, these priors facilitate the reconstruction of scene dynamics using an advanced flow-driven diffusion model. Experimental results illustrate that the proposed method can achieve superior rate-distortion-performance and outperform the state-of-the-art conventional-video codec Enhanced Compression Model (ECM) on-scene dynamics sequences. The project page can be found at-https://github.com/xyzysz/GNVDC.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Normalization Strategies and Convolutional Kernels for Multimodal Image Fusion</title>
<link>https://arxiv.org/abs/2411.10036</link>
<guid>https://arxiv.org/abs/2411.10036</guid>
<content:encoded><![CDATA[
arXiv:2411.10036v2 Announce Type: replace 
Abstract: Multimodal image fusion (MMIF) integrates information from different modalities to obtain a comprehensive image, aiding downstream tasks. However, existing research focuses on complementary information fusion and training strategies, overlooking the critical role of underlying architectural components like normalization and convolution kernels. We reevaluate the UNet architecture for end-to-end MMIF, identifying that widely used batch normalization limits performance by smoothing crucial sparse features. To address this, we propose a hybrid of instance and group normalization to maintain sample independence and reinforce intrinsic feature correlations. Crucially, this strategy facilitates richer feature maps, enabling large kernel convolution to fully leverage its receptive field, enhancing detail preservation. Furthermore, the proposed multi-path adaptive fusion module dynamically calibrates features from varying scales and receptive fields, ensuring effective information transfer. Our method achieves SOTA objective performance on MSRS, M$^3$FD, TNO, and Harvard datasets, producing visually clearer salient objects and lesion areas. Notably, it improves MSRS segmentation mIoU by 8.1\% over the infrared image. This performance stems from a synergistic design of normalization and convolution kernels, which preserves critical sparse features. The code is available at https://github.com/HeDan-11/LKC-FUNet.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyViM: Frequency Decoupling for Tiny Hybrid Vision Mamba</title>
<link>https://arxiv.org/abs/2411.17473</link>
<guid>https://arxiv.org/abs/2411.17473</guid>
<content:encoded><![CDATA[
arXiv:2411.17473v2 Announce Type: replace 
Abstract: Mamba has shown great potential for computer vision due to its linear complexity in modeling the global context with respect to the input length. However, existing lightweight Mamba-based backbones cannot demonstrate performance that matches Convolution or Transformer-based methods. By observing, we find that simply modifying the scanning path in the image domain is not conducive to fully exploiting the potential of vision Mamba. In this paper, we first perform comprehensive spectral and quantitative analyses, and verify that the Mamba block mainly models low-frequency information under Convolution-Mamba hybrid architecture. Based on the analyses, we introduce a novel Laplace mixer to decouple the features in terms of frequency and input only the low-frequency components into the Mamba block. In addition, considering the redundancy of the features and the different requirements for high-frequency details and low-frequency global information at different stages, we introduce a frequency ramp inception, i.e., gradually reduce the input dimensions of the high-frequency branches, so as to efficiently trade-off the high-frequency and low-frequency components at different layers. By integrating mobile-friendly convolution and efficient Laplace mixer, we build a series of tiny hybrid vision Mamba called TinyViM. The proposed TinyViM achieves impressive performance on several downstream tasks including image classification, semantic segmentation, object detection and instance segmentation. In particular, TinyViM outperforms Convolution, Transformer and Mamba-based models with similar scales, and the throughput is about 2-3 times higher than that of other Mamba-based models. Code is available at https://github.com/xwmaxwma/TinyViM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-ICE: An Inner Interpretable Framework for Image Classification via Bi-directional Interactions between Concept and Input Embeddings</title>
<link>https://arxiv.org/abs/2411.18645</link>
<guid>https://arxiv.org/abs/2411.18645</guid>
<content:encoded><![CDATA[
arXiv:2411.18645v2 Announce Type: replace 
Abstract: Inner interpretability is a promising field aiming to uncover the internal mechanisms of AI systems through scalable, automated methods. While significant research has been conducted on large language models, limited attention has been paid to applying inner interpretability to large-scale image tasks, focusing primarily on architectural and functional levels to visualize learned concepts. In this paper, we first present a conceptual framework that supports inner interpretability and multilevel analysis for large-scale image classification tasks. Specifically, we introduce the Bi-directional Interaction between Concept and Input Embeddings (Bi-ICE) module, which facilitates interpretability across the computational, algorithmic, and implementation levels. This module enhances transparency by generating predictions based on human-understandable concepts, quantifying their contributions, and localizing them within the inputs. Finally, we showcase enhanced transparency in image classification, measuring concept contributions, and pinpointing their locations within the inputs. Our approach highlights algorithmic interpretability by demonstrating the process of concept learning and its convergence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Twisted Convolutional Networks (TCNs): Enhancing Feature Interactions for Non-Spatial Data Classification</title>
<link>https://arxiv.org/abs/2412.00238</link>
<guid>https://arxiv.org/abs/2412.00238</guid>
<content:encoded><![CDATA[
arXiv:2412.00238v2 Announce Type: replace 
Abstract: Twisted Convolutional Networks (TCNs) are proposed as a novel deep learning architecture for classifying one-dimensional data with arbitrary feature order and minimal spatial relationships. Unlike conventional Convolutional Neural Networks (CNNs) that rely on structured feature sequences, TCNs explicitly combine subsets of input features through theoretically grounded multiplicative and pairwise interaction mechanisms to create enriched representations. This feature combination strategy, formalized through polynomial feature expansions, captures high-order feature interactions that traditional convolutional approaches miss. We provide a comprehensive mathematical framework for TCNs, demonstrating how the twisted convolution operation generalizes standard convolutions while maintaining computational tractability. Through extensive experiments on five benchmark datasets from diverse domains (medical diagnostics, political science, synthetic data, chemometrics, and healthcare), we show that TCNs achieve statistically significant improvements over CNNs, Residual Networks (ResNet), Graph Neural Networks (GNNs), DeepSets, and Support Vector Machine (SVM). The performance gains are validated through statistical testing. TCNs also exhibit superior training stability and generalization capabilities, highlighting their robustness for non-spatial data classification tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explaining Object Detectors via Collective Contribution of Pixels</title>
<link>https://arxiv.org/abs/2412.00666</link>
<guid>https://arxiv.org/abs/2412.00666</guid>
<content:encoded><![CDATA[
arXiv:2412.00666v2 Announce Type: replace 
Abstract: Visual explanations for object detectors are crucial for enhancing their reliability. Object detectors identify and localize instances by assessing multiple visual features collectively. When generating explanations, overlooking these collective influences in detections may lead to missing compositional cues or capturing spurious correlations. However, existing methods typically focus solely on individual pixel contributions, neglecting the collective contribution of multiple pixels. To address this limitation, we propose a game-theoretic method based on Shapley values and interactions to explicitly capture both individual and collective pixel contributions. Our method provides explanations for both bounding box localization and class determination, highlighting regions crucial for detection. Extensive experiments demonstrate that the proposed method identifies important regions more accurately than state-of-the-art methods. The code will be publicly available soon.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation</title>
<link>https://arxiv.org/abs/2412.01254</link>
<guid>https://arxiv.org/abs/2412.01254</guid>
<content:encoded><![CDATA[
arXiv:2412.01254v3 Announce Type: replace 
Abstract: This paper aims to bring fine-grained expression control while maintaining high-fidelity identity in portrait generation. This is challenging due to the mutual interference between expression and identity: (i) fine expression control signals inevitably introduce appearance-related semantics (e.g., facial contours, and ratio), which impact the identity of the generated portrait; (ii) even coarse-grained expression control can cause facial changes that compromise identity, since they all act on the face. These limitations remain unaddressed by previous generation methods, which primarily rely on coarse control signals or two-stage inference that integrates portrait animation. Here, we introduce EmojiDiff, the first end-to-end solution that enables simultaneous control of extremely detailed expression (RGB-level) and high-fidelity identity in portrait generation. To address the above challenges, EmojiDiff adopts a two-stage scheme involving decoupled training and fine-tuning. For decoupled training, we innovate ID-irrelevant Data Iteration (IDI) to synthesize cross-identity expression pairs by dividing and optimizing the processes of maintaining expression and altering identity, thereby ensuring stable and high-quality data generation. Training the model with this data, we effectively disentangle fine expression features in the expression template from other extraneous information (e.g., identity, skin). Subsequently, we present ID-enhanced Contrast Alignment (ICA) for further fine-tuning. ICA achieves rapid reconstruction and joint supervision of identity and expression information, thus aligning identity representations of images with and without expression control. Experimental results demonstrate that our method remarkably outperforms counterparts, achieves precise expression control with highly maintained identity, and generalizes well to various diffusion models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAMCL: Empowering SAM to Continually Learn from Dynamic Domains with Extreme Storage Efficiency</title>
<link>https://arxiv.org/abs/2412.05012</link>
<guid>https://arxiv.org/abs/2412.05012</guid>
<content:encoded><![CDATA[
arXiv:2412.05012v2 Announce Type: replace 
Abstract: Segment Anything Model (SAM) struggles in open-world scenarios with diverse domains. In such settings, naive fine-tuning with a well-designed learning module is inadequate and often causes catastrophic forgetting issue when learning incrementally. To address this issue, we propose a novel continual learning (CL) method for SAM, termed SAMCL. Rather than relying on a fixed learning module, our method decomposes incremental knowledge into separate modules and trains a selector to choose the appropriate one during inference. However, this intuitive design introduces two key challenges: ensuring effective module learning and selection, and managing storage as tasks accumulate. To tackle these, we introduce two components: AugModule and Module Selector. AugModule reduces the storage of the popular LoRA learning module by sharing parameters across layers while maintaining accuracy. It also employs heatmaps-generated from point prompts-to further enhance domain adaptation with minimal additional cost. Module Selector leverages the observation that SAM's embeddings can effectively distinguish domains, enabling high selection accuracy by training on low-consumed embeddings instead of raw images. Experiments show that SAMCL outperforms state-of-the-art methods, achieving only 0.19% forgetting and at least 2.5% gain on unseen domains. Each AugModule requires just 0.233 MB, reducing storage by at least 24.3% over other fine-tuning approaches. The buffer storage for Module Selector is further reduced by up to 256$\times$.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unsupervised Domain Bridging via Image Degradation in Semantic Segmentation</title>
<link>https://arxiv.org/abs/2412.10339</link>
<guid>https://arxiv.org/abs/2412.10339</guid>
<content:encoded><![CDATA[
arXiv:2412.10339v2 Announce Type: replace 
Abstract: Semantic segmentation suffers from significant performance degradation when the trained network is applied to a different domain. To address this issue, unsupervised domain adaptation (UDA) has been extensively studied. Despite the effectiveness of selftraining techniques in UDA, they still overlook the explicit modeling of domain-shared feature extraction. In this paper, we propose DiDA, an unsupervised domain bridging approach for semantic segmentation. DiDA consists of two key modules: (1) Degradation-based Intermediate Domain Construction, which creates continuous intermediate domains through simple image degradation operations to encourage learning domain-invariant features as domain differences gradually diminish; (2) Semantic Shift Compensation, which leverages a diffusion encoder to disentangle and compensate for semantic shift information with degraded timesteps, preserving discriminative representations in the intermediate domains. As a plug-and-play solution, DiDA supports various degradation operations and seamlessly integrates with existing UDA methods. Extensive experiments on multiple domain adaptive semantic segmentation benchmarks demonstrate that DiDA consistently achieves significant performance improvements across all settings. Code is available at https://github.com/Woof6/DiDA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Interpretability for Adversarial Robustness: A Hybrid Generative Classification Approach</title>
<link>https://arxiv.org/abs/2412.20025</link>
<guid>https://arxiv.org/abs/2412.20025</guid>
<content:encoded><![CDATA[
arXiv:2412.20025v2 Announce Type: replace 
Abstract: Deep learning-based discriminative classifiers, despite their remarkable success, remain vulnerable to adversarial examples that can mislead model predictions. While adversarial training can enhance robustness, it fails to address the intrinsic vulnerability stemming from the opaque nature of these black-box models. We present a deep ensemble model that combines discriminative features with generative models to achieve both high accuracy and adversarial robustness. Our approach integrates a bottom-level pre-trained discriminative network for feature extraction with a top-level generative classification network that models adversarial input distributions through a deep latent variable model. Using variational Bayes, our model achieves superior robustness against white-box adversarial attacks without adversarial training. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate our model's superior adversarial robustness. Through evaluations using counterfactual metrics and feature interaction-based metrics, we establish correlations between model interpretability and adversarial robustness. Additionally, preliminary results on Tiny-ImageNet validate our approach's scalability to more complex datasets, offering a practical solution for developing robust image classification models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering</title>
<link>https://arxiv.org/abs/2501.01371</link>
<guid>https://arxiv.org/abs/2501.01371</guid>
<content:encoded><![CDATA[
arXiv:2501.01371v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) demonstrate remarkable capabilities in visual understanding and reasoning, such as in Visual Question Answering (VQA), where the model is asked a question related to a visual input. Still, these models can make distinctly unnatural errors, for example, providing (wrong) answers to unanswerable VQA questions, such as questions asking about objects that do not appear in the image. To address this issue, we propose CLIP-UP: CLIP-based Unanswerable Problem detection, a novel lightweight method for equipping VLMs with the ability to withhold answers to unanswerable questions. CLIP-UP leverages CLIP-based similarity measures to extract question-image alignment information to detect unanswerability, requiring efficient training of only a few additional layers, while keeping the original VLMs' weights unchanged. Tested across several models, CLIP-UP achieves significant improvements on benchmarks assessing unanswerability in both multiple-choice and open-ended VQA, surpassing other methods, while preserving original performance on other tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expectation-Maximization as the Engine of Scalable Medical Intelligence</title>
<link>https://arxiv.org/abs/2501.03410</link>
<guid>https://arxiv.org/abs/2501.03410</guid>
<content:encoded><![CDATA[
arXiv:2501.03410v2 Announce Type: replace 
Abstract: Large, high-quality, annotated datasets are the foundation of medical AI research, but constructing even a small, moderate-quality, annotated dataset can take years of effort from multidisciplinary teams. Although active learning can prioritize what to annotate, scaling up still requires extensive manual efforts to revise the noisy annotations. We formulate this as a missing-data problem and develop ScaleMAI, a framework that unifies data annotation and model development co-evolution through an Expectation-Maximization (EM) process. In this iterative process, the AI model automatically identifies and corrects the mistakes in annotations (Expectation), while the refined annotated data retrain the model to improve accuracy (Maximization). In addition to the classical EM algorithm, ScaleMAI brings human experts into the loop to review annotations that cannot be adequately addressed by either Expectation or Maximization step (<5%). As a result, ScaleMAI progressively creates an annotated dataset of 47,315 CT scans (4.8x larger than the largest public dataset, PanTS) including 4,163,720 per-voxel annotations for benign/malignant tumors and 88 anatomical structures. ScaleMAI iteratively trains a model that exceeds human expert performance in tumor diagnosis (+7%), and outperforms models developed from smaller, moderate-quality datasets, with statistically significant gains in tumor detection (+10%) and segmentation (+14%) on two prestigious benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering</title>
<link>https://arxiv.org/abs/2501.17792</link>
<guid>https://arxiv.org/abs/2501.17792</guid>
<content:encoded><![CDATA[
arXiv:2501.17792v3 Announce Type: replace 
Abstract: We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting for real-time, high-quality crowd rendering. Our method utilizes 3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos. We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality. The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis. The framework is also optimized for GPU memory usage to enhance scalability. Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance. Through the.se experiments, we demonstrate that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Assisted Continual learning for Visual Question Answering in Self-Driving</title>
<link>https://arxiv.org/abs/2502.00843</link>
<guid>https://arxiv.org/abs/2502.00843</guid>
<content:encoded><![CDATA[
arXiv:2502.00843v2 Announce Type: replace 
Abstract: In this paper, we propose a novel approach for solving the Visual Question Answering (VQA) task in autonomous driving by integrating Vision-Language Models (VLMs) with continual learning. In autonomous driving, VQA plays a vital role in enabling the system to understand and reason about its surroundings. However, traditional models often struggle with catastrophic forgetting when sequentially exposed to new driving tasks, such as perception, prediction, and planning, each requiring different forms of knowledge. To address this challenge, we present a novel continual learning framework that combines VLMs with selective memory replay and knowledge distillation, reinforced by task-specific projection layer regularization. The knowledge distillation allows a previously trained model to act as a "teacher" to guide the model through subsequent tasks, minimizing forgetting. Meanwhile, task-specific projection layers calculate the loss based on the divergence of feature representations, ensuring continuity in learning and reducing the shift between tasks. Evaluated on the DriveLM dataset, our framework shows substantial performance improvements, with gains ranging from 20.11% to 35.16% across various metrics. These results highlight the effectiveness of combining continual learning with VLMs in enhancing the resilience and reliability of VQA systems in autonomous driving. We will release our source code.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPFFNet: Strip Perception and Feature Fusion Spatial Pyramid Pooling for Fabric Defect Detection</title>
<link>https://arxiv.org/abs/2502.01445</link>
<guid>https://arxiv.org/abs/2502.01445</guid>
<content:encoded><![CDATA[
arXiv:2502.01445v3 Announce Type: replace 
Abstract: Defect detection in fabrics is critical for quality control, yet existing methods often struggle with complex backgrounds and shape-specific defects. In this paper, we propose an improved fabric defect detection model based on YOLOv11. To enhance the detection of strip defects, we introduce a Strip Perception Module (SPM) that improves feature capture through multi-scale convolution. We further enhance the spatial pyramid pooling fast (SPPF) by integrating a squeeze-and-excitation mechanism, resulting in the SE-SPPF module, which better integrates spatial and channel information for more effective defect feature extraction. Additionally, we propose a novel focal enhanced complete intersection over union (FECIoU) metric with adaptive weights, addressing scale differences and class imbalance by adjusting the weights of hard-to-detect instances through focal loss. Experimental results demonstrate that our model achieves a 0.8-8.1% improvement in mean average precision (mAP) on the Tianchi dataset and a 1.6-13.2% improvement on our custom dataset, outperforming other state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation</title>
<link>https://arxiv.org/abs/2502.09274</link>
<guid>https://arxiv.org/abs/2502.09274</guid>
<content:encoded><![CDATA[
arXiv:2502.09274v2 Announce Type: replace 
Abstract: 3D scene understanding is a critical yet challenging task in autonomous driving due to the irregularity and sparsity of LiDAR data, as well as the computational demands of processing large-scale point clouds. Recent methods leverage range-view representations to enhance efficiency, but they often adopt higher azimuth resolutions to mitigate information loss during spherical projection, where only the closest point is retained for each 2D grid. However, processing wide panoramic range-view images remains inefficient and may introduce additional distortions. Our empirical analysis shows that training with multiple range images, obtained from splitting the full point cloud, improves both segmentation accuracy and computational efficiency. However, this approach also poses new challenges of exacerbated class imbalance and increase in projection artifacts. To address these, we introduce FLARES, a novel training paradigm that incorporates two tailored data augmentation techniques and a specialized post-processing method designed for multi-range settings. Extensive experiments demonstrate that FLARES is highly generalizable across different architectures, yielding 2.1%~7.9% mIoU improvements on SemanticKITTI and 1.8%~3.9% mIoU on nuScenes, while delivering over 40% speed-up in inference.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Potential of Encoder-free Architectures in 3D LMMs</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
arXiv:2502.09620v4 Announce Type: replace 
Abstract: Encoder-free architectures have been preliminarily explored in the 2D Large Multimodal Models (LMMs), yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D LMMs. These long-standing challenges include the failure to adapt to varying point cloud resolutions during inference and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the pre-trained encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the state-of-the-art model, PointLLM-PiSA-13B, achieving 57.91%, 61.0%, and 55.20% on the classification, captioning, and VQA tasks, respectively. Our results show that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thicker and Quicker: A Jumbo Token for Fast Plain Vision Transformers</title>
<link>https://arxiv.org/abs/2502.15021</link>
<guid>https://arxiv.org/abs/2502.15021</guid>
<content:encoded><![CDATA[
arXiv:2502.15021v3 Announce Type: replace 
Abstract: ViTs are general and accurate, and address many tasks, but ViTs are slow, and are not always practical when efficiency is key. Existing methods for faster ViTs design hybrid non-ViT architectures, losing generality, or shrink their tokens, sacrificing accuracy. While many non-ViT architectures are both fast and accurate, they cannot flexibly process other input shapes, pre-train by SOTA self-supervised learning, reduce computation by dropping tokens, and more like ViTs can. We make ViTs faster by reducing patch token width while increasing global token width by adding a new Jumbo token. Our wider Jumbo token is processed by its own wider FFN to increase model capacity. Yet our Jumbo FFN is efficient: it processes a single token, for speed, and its parameters are shared across all layers, for memory. Crucially, our Jumbo is attention-only and non-hierarchical, like a plain ViT, so it is simple, scalable, flexible, and compatible with ViT methods new and old. Jumbo improves over ViT baselines with Registers from Nano to Large scales while maintaining speed/throughput on ImageNet-1K (0.1-13%). Jumbo also improves MAE pre-training (4.9% linear probing on ImageNet-1K), test-time adaptation (5.2% on ImageNet-C), and time series modeling. Our Jumbo models even achieve better speed-accuracy trade-offs than specialized non-ViT compute-efficient models, while maintaining plain-ViT compatibility for practicality. Code and weights available: https://github.com/antofuller/jumbo
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Industrial Anomalies Synthesis</title>
<link>https://arxiv.org/abs/2502.16412</link>
<guid>https://arxiv.org/abs/2502.16412</guid>
<content:encoded><![CDATA[
arXiv:2502.16412v2 Announce Type: replace 
Abstract: This paper comprehensively reviews anomaly synthesis methodologies. Existing surveys focus on limited techniques, missing an overall field view and understanding method interconnections. In contrast, our study offers a unified review, covering about 40 representative methods across Hand-crafted, Distribution-hypothesis-based, Generative models (GM)-based, and Vision-language models (VLM)-based synthesis. We introduce the first industrial anomaly synthesis (IAS) taxonomy. Prior works lack formal classification or use simplistic taxonomies, hampering structured comparisons and trend identification. Our taxonomy provides a fine-grained framework reflecting methodological progress and practical implications, grounding future research. Furthermore, we explore cross-modality synthesis and large-scale VLM. Previous surveys overlooked multimodal data and VLM in anomaly synthesis, limiting insights into their advantages. Our survey analyzes their integration, benefits, challenges, and prospects, offering a roadmap to boost IAS with multimodal learning. More resources are available at https://github.com/M-3LAB/awesome-anomaly-synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Griffin: Aerial-Ground Cooperative Detection and Tracking Dataset and Benchmark</title>
<link>https://arxiv.org/abs/2503.06983</link>
<guid>https://arxiv.org/abs/2503.06983</guid>
<content:encoded><![CDATA[
arXiv:2503.06983v2 Announce Type: replace 
Abstract: While cooperative perception can overcome the limitations of single-vehicle systems, the practical implementation of vehicle-to-vehicle and vehicle-to-infrastructure systems is often impeded by significant economic barriers. Aerial-ground cooperation (AGC), which pairs ground vehicles with drones, presents a more economically viable and rapidly deployable alternative. However, this emerging field has been held back by a critical lack of high-quality public datasets and benchmarks. To bridge this gap, we present \textit{Griffin}, a comprehensive AGC 3D perception dataset, featuring over 250 dynamic scenes (37k+ frames). It incorporates varied drone altitudes (20-60m), diverse weather conditions, realistic drone dynamics via CARLA-AirSim co-simulation, and critical occlusion-aware 3D annotations. Accompanying the dataset is a unified benchmarking framework for cooperative detection and tracking, with protocols to evaluate communication efficiency, altitude adaptability, and robustness to communication latency, data loss and localization noise. By experiments through different cooperative paradigms, we demonstrate the effectiveness and limitations of current methods and provide crucial insights for future research. The dataset and codes are available at https://github.com/wang-jh18-SVM/Griffin.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIRAM: Masked Image Autoencoders Across Multiple Scales with Hybrid-Attention Mechanism for Breast Lesion Risk Prediction</title>
<link>https://arxiv.org/abs/2503.07157</link>
<guid>https://arxiv.org/abs/2503.07157</guid>
<content:encoded><![CDATA[
arXiv:2503.07157v3 Announce Type: replace 
Abstract: Self-supervised learning (SSL) has garnered substantial interest within the machine learning and computer vision communities. Two prominent approaches in SSL include contrastive-based learning and self-distillation utilizing cropping augmentation. Lately, masked image modeling (MIM) has emerged as a more potent SSL technique, employing image inpainting as a pretext task. MIM creates a strong inductive bias toward meaningful spatial and semantic understanding. This has opened up new opportunities for SSL to contribute not only to classification tasks but also to more complex applications like object detection and image segmentation. Building upon this progress, our research paper introduces a scalable and practical SSL approach centered around more challenging pretext tasks that facilitate the acquisition of robust features. Specifically, we leverage multi-scale image reconstruction from randomly masked input images as the foundation for feature learning. Our hypothesis posits that reconstructing high-resolution images enables the model to attend to finer spatial details, particularly beneficial for discerning subtle intricacies within medical images. The proposed SSL features help improve classification performance on the Curated Breast Imaging Subset of Digital Database for Screening Mammography (CBIS-DDSM) dataset. In pathology classification, our method demonstrates a 3\% increase in average precision (AP) and a 1\% increase in the area under the receiver operating characteristic curve (AUC) when compared to state-of-the-art (SOTA) algorithms. Moreover, in mass margins classification, our approach achieves a 4\% increase in AP and a 2\% increase in AUC.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEMPLE: Incentivizing Temporal Understanding of Video Large Language Models via Progressive Pre-SFT Alignment</title>
<link>https://arxiv.org/abs/2503.16929</link>
<guid>https://arxiv.org/abs/2503.16929</guid>
<content:encoded><![CDATA[
arXiv:2503.16929v4 Announce Type: replace 
Abstract: Video Large Language Models (Video LLMs) have achieved significant success by adopting the paradigm of large-scale pre-training followed by supervised fine-tuning (SFT). However, existing approaches struggle with temporal reasoning due to weak temporal correspondence in the data and over-reliance on the next-token prediction paradigm}, which collectively result in the absence temporal supervision. To address these limitations, we propose TEMPLE (TEMporal Preference LEarning), a systematic framework that enhances temporal reasoning capabilities through Direct Preference Optimization (DPO). To address temporal information scarcity in data, we introduce an automated pipeline for systematically constructing temporality-intensive preference pairs comprising three steps: selecting temporally rich videos, designing video-specific perturbation strategies, and evaluating model responses on clean and perturbed inputs. Complementing this data pipeline, we provide additional supervision signals via preference learning and propose a novel Progressive Pre-SFT Alignment strategy featuring two key innovations: a curriculum learning strategy which progressively increases perturbation difficulty to maximize data efficiency; and applying preference optimization before instruction tuning to incentivize fundamental temporal alignment. Extensive experiments demonstrate that our approach consistently improves Video LLM performance across multiple benchmarks with a relatively small set of self-generated DPO data. Our findings highlight TEMPLE as a scalable and efficient complement to SFT-based methods, paving the way for developing reliable Video LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</title>
<link>https://arxiv.org/abs/2503.19912</link>
<guid>https://arxiv.org/abs/2503.19912</guid>
<content:encoded><![CDATA[
arXiv:2503.19912v2 Announce Type: replace 
Abstract: LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Math Blind: Failures in Diagram Understanding Undermine Reasoning in MLLMs</title>
<link>https://arxiv.org/abs/2503.20745</link>
<guid>https://arxiv.org/abs/2503.20745</guid>
<content:encoded><![CDATA[
arXiv:2503.20745v2 Announce Type: replace 
Abstract: Diagrams represent a form of visual language that encodes abstract concepts and relationships through structured symbols and their spatial arrangements. Unlike natural images, they are inherently symbolic, and entirely artificial. They thus pose unique challenges for Multimodal Large Language Models (MLLMs) distinct from natural image processing. Recent studies have shown that MLLMs often exhibit flawed reasoning and hallucinations when handling diagram inputs. We investigate here whether these limitations stem from shortcomings in the models' ability to interpret diagrams themselves. To this end, we develop a diagnostic test suite that isolates perception from reasoning. Our systematic evaluation reveals that MLLMs perform poorly on basic perceptual tasks, e.g., shape classification, object counting, relationship identification, and object grounding, with near-zero accuracy on fine-grained grounding. Further analysis shows that weak diagram perception leads to "blind faith in text", where models rely on textual shortcuts rather than visual understanding (that is, they are Math Blind). We hypothesize that enabling models to capture the inherent structural properties of diagrams, represented as graphs of primitives and their interrelationships, is essential for improving diagram understanding. Experiments with 7B and 32B MLLMs validate this assumption, with models trained on such representations achieving a +79% gain on the grounding task. Crucially, these gains transfer to reasoning, achieving 3-4% cross-suite improvements on three public benchmarks even without additional chain-of-thought reasoning data. Our findings demonstrate that low-level perception supports faithful high-level reasoning in mathematical MLLMs. We provide both methodological frameworks and empirical evidence to guide future research in this direction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TranSplat: Instant Cross-Scene Object Relighting in Gaussian Splatting via Spherical Harmonic Transfer</title>
<link>https://arxiv.org/abs/2503.22676</link>
<guid>https://arxiv.org/abs/2503.22676</guid>
<content:encoded><![CDATA[
arXiv:2503.22676v3 Announce Type: replace 
Abstract: We present TranSplat, a method for fast and accurate object relighting for the 3D Gaussian Splatting (GS) framework when transferring a 3D object from a source GS scene to a target GS scene. TranSplat is based on a theoretical radiance transfer identity for cross-scene relighting of objects with radially symmetric BRDFs that involves only taking simple products of spherical harmonic appearance coefficients of the object, source, and target environment maps without any explicit computation of scene quantities (e.g., the BRDFs themselves). TranSplat is the first method to demonstrate how this theoretical identity may be used to perform relighting within the GS framework, and furthermore, by automatically inferring unknown source and target environment maps directly from the source and target scene GS representations. We evaluated TranSplat on several synthetic and real-world scenes and objects, demonstrating comparable 3D object relighting performance to recent conventional inverse rendering-based GS methods with a fraction of their runtime. While TranSplat is theoretically best-suited for radially symmetric BRDFs, results demonstrate that TranSplat still offers perceptually realistic renderings on real scenes and opens a valuable, lightweight path forward to relighting with the GS framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blurry-Edges: Photon-Limited Depth Estimation from Defocused Boundaries</title>
<link>https://arxiv.org/abs/2503.23606</link>
<guid>https://arxiv.org/abs/2503.23606</guid>
<content:encoded><![CDATA[
arXiv:2503.23606v2 Announce Type: replace 
Abstract: Extracting depth information from photon-limited, defocused images is challenging because depth from defocus (DfD) relies on accurate estimation of defocus blur, which is fundamentally sensitive to image noise. We present a novel approach to robustly measure object depths from photon-limited images along the defocused boundaries. It is based on a new image patch representation, Blurry-Edges, that explicitly stores and visualizes a rich set of low-level patch information, including boundaries, color, and smoothness. We develop a deep neural network architecture that predicts the Blurry-Edges representation from a pair of differently defocused images, from which depth can be calculated using a closed-form DfD relation we derive. The experimental results on synthetic and real data show that our method achieves the highest depth estimation accuracy on photon-limited images compared to a broad range of state-of-the-art DfD methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Suite-IN++: A FlexiWear BodyNet Integrating Global and Local Motion Features from Apple Suite for Robust Inertial Navigation</title>
<link>https://arxiv.org/abs/2504.00438</link>
<guid>https://arxiv.org/abs/2504.00438</guid>
<content:encoded><![CDATA[
arXiv:2504.00438v2 Announce Type: replace 
Abstract: The proliferation of wearable technology has established multi-device ecosystems comprising smartphones, smartwatches, and headphones as critical enablers for ubiquitous pedestrian localization. However, traditional pedestrian dead reckoning (PDR) struggles with diverse motion modes, while data-driven methods, despite improving accuracy, often lack robustness due to their reliance on a single-device setup. Therefore, a promising solution is to fully leverage existing wearable devices to form a flexiwear bodynet for robust and accurate pedestrian localization. This paper presents Suite-IN++, a deep learning framework for flexiwear bodynet-based pedestrian localization. Suite-IN++ integrates motion data from wearable devices on different body parts, using contrastive learning to separate global and local motion features. It fuses global features based on the data reliability of each device to capture overall motion trends and employs an attention mechanism to uncover cross-device correlations in local features, extracting motion details helpful for accurate localization. To evaluate our method, we construct a real-life flexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and AirPods) across diverse walking modes and device configurations. Experimental results demonstrate that Suite-IN++ achieves superior localization accuracy and robustness, significantly outperforming state-of-the-art models in real-life pedestrian tracking scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three Forensic Cues for JPEG AI Images</title>
<link>https://arxiv.org/abs/2504.03191</link>
<guid>https://arxiv.org/abs/2504.03191</guid>
<content:encoded><![CDATA[
arXiv:2504.03191v2 Announce Type: replace 
Abstract: The JPEG standard was vastly successful. Currently, the first AI-based compression method ``JPEG AI'' will be standardized. JPEG AI brings remarkable benefits. JPEG AI images exhibit impressive image quality at bitrates that are an order of magnitude lower than images compressed with traditional JPEG. However, forensic analysis of JPEG AI has to be completely re-thought: forensic tools for traditional JPEG do not transfer to JPEG AI, and artifacts from JPEG AI are easily confused with artifacts from artificially generated images (``DeepFakes''). This creates a need for novel forensic approaches to detection and distinction of JPEG AI images. In this work, we make a first step towards a forensic JPEG AI toolset. We propose three cues for forensic algorithms for JPEG AI. These algorithms address three forensic questions: first, we show that the JPEG AI preprocessing introduces correlations in the color channels that do not occur in uncompressed images. Second, we show that repeated compression of JPEG AI images leads to diminishing distortion differences. This can be used to detect recompression, in a spirit similar to some classic JPEG forensics methods. Third, we show that the quantization of JPEG AI images in the latent space can be used to distinguish real images with JPEG AI compression from synthetically generated images. The proposed methods are interpretable for a forensic analyst, and we hope that they inspire further research in the forensics of AI-compressed images.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2504.09261</link>
<guid>https://arxiv.org/abs/2504.09261</guid>
<content:encoded><![CDATA[
arXiv:2504.09261v2 Announce Type: replace 
Abstract: Visual Autoregressive (VAR) models adopt a next-scale prediction paradigm, offering high-quality content generation with substantially fewer decoding steps. However, existing VAR models suffer from significant attention complexity and severe memory overhead due to the accumulation of key-value (KV) caches across scales. In this paper, we tackle this challenge by introducing KV cache compression into the next-scale generation paradigm. We begin with a crucial observation: attention heads in VAR models can be divided into two functionally distinct categories: Contextual Heads focus on maintaining semantic consistency, while Structural Heads are responsible for preserving spatial coherence. This structural divergence causes existing one-size-fits-all compression methods to perform poorly on VAR models. To address this, we propose HACK, a training-free Head-Aware KV cache Compression frameworK. HACK utilizes an offline classification scheme to separate head types, enabling it to apply pattern-specific compression strategies with asymmetric cache budgets for each category. By doing so, HACK effectively constrains the average KV cache length within a fixed budget $B$, reducing the theoretical attention complexity from $\mathcal{O}(n^4)$ to $\mathcal{O}(Bn^2)$. Extensive experiments on multiple VAR models across text-to-image and class-conditional tasks validate the effectiveness and generalizability of HACK. It achieves up to 70% KV cache compression without degrading output quality, resulting in memory savings and faster inference. For example, HACK provides a $1.75\times$ memory reduction and a $1.57\times$ speedup on Infinity-8B.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration</title>
<link>https://arxiv.org/abs/2504.09973</link>
<guid>https://arxiv.org/abs/2504.09973</guid>
<content:encoded><![CDATA[
arXiv:2504.09973v2 Announce Type: replace 
Abstract: All-in-One Image Restoration (AiOIR), which addresses diverse degradation types with a unified model, presents significant challenges in designing task-aware prompts that effectively guide restoration across multiple degradation scenarios. While adaptive prompt learning enables end-to-end optimization, it often yields overlapping or redundant task representations. Conversely, explicit prompts derived from pretrained classifiers enhance discriminability but discard critical visual information needed for reconstruction. To address these limitations, we introduce Contrastive Prompt Learning (CPL), a framework that aims to improve prompt-task alignment through two complementary components: a Sparse Prompt Module (SPM) that efficiently captures degradation-aware representations while reducing redundancy, and a Contrastive Prompt Regularization (CPR) that explicitly strengthens task boundaries by incorporating negative prompt samples across different degradation types. Unlike previous approaches that focus primarily on degradation classification, CPL directly optimizes the interaction between prompts and the restoration model. Extensive experiments across five benchmarks show that CPL consistently boosts the performance of strong AiOIR baselines across diverse scenarios. Our approach achieves state-of-the-art average performance on these benchmarks, providing a general and robust solution for AiOIR. The code is available at https://github.com/Aitical/CPLIR
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.17432</link>
<guid>https://arxiv.org/abs/2504.17432</guid>
<content:encoded><![CDATA[
arXiv:2504.17432v4 Announce Type: replace 
Abstract: The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM\'s language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</title>
<link>https://arxiv.org/abs/2505.01267</link>
<guid>https://arxiv.org/abs/2505.01267</guid>
<content:encoded><![CDATA[
arXiv:2505.01267v4 Announce Type: replace 
Abstract: The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image. For the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. Empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction</title>
<link>https://arxiv.org/abs/2505.06905</link>
<guid>https://arxiv.org/abs/2505.06905</guid>
<content:encoded><![CDATA[
arXiv:2505.06905v3 Announce Type: replace 
Abstract: Monocular height estimation (MHE) from very-high-resolution (VHR) optical imagery remains challenging due to limited structural cues and the high cost and geographic constraints of conventional elevation data such as airborne LiDAR and multi-view stereo. Although recent MHE and monocular depth estimation (MDE) models show strong performance, their robustness under varied illumination and scene conditions is still limited. We introduce a fully automated correction pipeline that integrates sparse, imperfect global LiDAR measurements from ICESat-2 with deep learning predictions to enhance accuracy and stability. The workflow relies entirely on publicly available models and data and requires only a single georeferenced optical image to produce corrected height maps, enabling low-cost and globally scalable deployment. We also establish the first benchmark for this task, evaluating two random forest based approaches, four parameter efficient fine tuning methods, and full fine tuning. Experiments across six diverse regions at 0.5 m resolution (297 km2), covering the urban cores of Tokyo, Paris, and Sao Paulo as well as suburban and forested areas, show substantial gains. The best method reduces the MHE model's mean absolute error (MAE) by 30.9 percent and improves its F1HE score by 44.2 percent. For the MDE model, MAE improves by 24.1 percent and the F1HE score by 25.1 percent. These results validate the effectiveness of our correction pipeline and demonstrate how sparse global LiDAR can systematically strengthen both MHE and MDE models, enabling scalable and widely accessible 3D height mapping.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</title>
<link>https://arxiv.org/abs/2505.08013</link>
<guid>https://arxiv.org/abs/2505.08013</guid>
<content:encoded><![CDATA[
arXiv:2505.08013v5 Announce Type: replace 
Abstract: As a core step in structure-from-motion and SLAM, robust feature detection and description under challenging scenarios such as significant viewpoint changes remain unresolved despite their ubiquity. While recent works have identified the importance of local features in modeling geometric transformations, these methods fail to learn the visual cues present in long-range relationships. We present Robust Deformable Detector (RDD), a novel and robust keypoint detector/descriptor leveraging the deformable transformer, which captures global context and geometric invariance through deformable self-attention mechanisms. Specifically, we observed that deformable attention focuses on key locations, effectively reducing the search space complexity and modeling the geometric invariance. Furthermore, we collected an Air-to-Ground dataset for training in addition to the standard MegaDepth dataset. Our proposed method outperforms all state-of-the-art keypoint detection/description methods in sparse matching tasks and is also capable of semi-dense matching. To ensure comprehensive evaluation, we introduce two challenging benchmarks: one emphasizing large viewpoint and scale variations, and the other being an Air-to-Ground benchmark -- an evaluation setting that has recently gaining popularity for 3D reconstruction across different altitudes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision</title>
<link>https://arxiv.org/abs/2505.18051</link>
<guid>https://arxiv.org/abs/2505.18051</guid>
<content:encoded><![CDATA[
arXiv:2505.18051v2 Announce Type: replace 
Abstract: Vision transformers are ever larger, more accurate, and more expensive to compute. The expense is even more extreme at high resolution as the number of tokens grows quadratically with the image size. We turn to adaptive computation to cope with this cost by learning to predict where to compute. Our LookWhere method divides the computation between a low-resolution selector and a high-resolution extractor without ever processing the full high-resolution input. We jointly pretrain the selector and extractor without task supervision by distillation from a self-supervised teacher, in effect, learning where and what to compute simultaneously. Unlike prior token reduction methods, which pay to save by pruning already-computed tokens, and prior token selection methods, which require complex and expensive per-task optimization, LookWhere economically and accurately selects and extracts transferrable representations of images. We show that LookWhere excels at sparse recognition on high-resolution inputs (Traffic Signs), maintaining accuracy while reducing FLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks that are global (ImageNet classification) or local (ADE20K segmentation), improving accuracy while reducing time by 1.36x. See https://github.com/antofuller/lookwhere for the code and weights.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Initialization for Vision Transformers</title>
<link>https://arxiv.org/abs/2505.19985</link>
<guid>https://arxiv.org/abs/2505.19985</guid>
<content:encoded><![CDATA[
arXiv:2505.19985v2 Announce Type: replace 
Abstract: Convolutional Neural Networks (CNNs) inherently encode strong inductive biases, enabling effective generalization on small-scale datasets. In this paper, we propose integrating this inductive bias into ViTs, not through an architectural intervention but solely through initialization. The motivation here is to have a ViT that can enjoy strong CNN-like performance when data assets are small, but can still scale to ViT-like performance as the data expands. Our approach is motivated by our empirical results that random impulse filters can achieve commensurate performance to learned filters within a CNN. We improve upon current ViT initialization strategies, which typically rely on empirical heuristics such as using attention weights from pretrained models or focusing on the distribution of attention weights without enforcing structures. Empirical results demonstrate that our method significantly outperforms standard ViT initialization across numerous small and medium-scale benchmarks, including Food-101, CIFAR-10, CIFAR-100, STL-10, Flowers, and Pets, while maintaining comparative performance on large-scale datasets such as ImageNet-1K. Moreover, our initialization strategy can be easily integrated into various transformer-based architectures such as Swin Transformer and MLP-Mixer with consistent improvements in performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DRS: MLLMs Need 3D-Aware Representation Supervision for Scene Understanding</title>
<link>https://arxiv.org/abs/2506.01946</link>
<guid>https://arxiv.org/abs/2506.01946</guid>
<content:encoded><![CDATA[
arXiv:2506.01946v2 Announce Type: replace 
Abstract: Recent advances in scene understanding have leveraged multimodal large language models (MLLMs) for 3D reasoning by capitalizing on their strong 2D pretraining. However, the lack of explicit 3D data during MLLM pretraining limits 3D representation capability. In this paper, we investigate the 3D-awareness of MLLMs by evaluating multi-view correspondence and reveal a strong positive correlation between the quality of 3D-aware representation and downstream task performance. Motivated by this, we propose 3DRS, a framework that enhances MLLM 3D representation learning by introducing supervision from pretrained 3D foundation models. Our approach aligns MLLM visual features with rich 3D knowledge distilled from 3D models, effectively improving scene understanding. Extensive experiments across multiple benchmarks and MLLMs -- including visual grounding, captioning, and question answering -- demonstrate consistent performance gains. Project page: https://visual-ai.github.io/3drs
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Normalize Filters! Classical Wisdom for Deep Vision</title>
<link>https://arxiv.org/abs/2506.04401</link>
<guid>https://arxiv.org/abs/2506.04401</guid>
<content:encoded><![CDATA[
arXiv:2506.04401v3 Announce Type: replace 
Abstract: Classical image filters, such as those for averaging or differencing, are carefully normalized to ensure consistency, interpretability, and to avoid artifacts like intensity shifts, halos, or ringing. In contrast, convolutional filters learned end-to-end in deep networks lack such constraints. Although they may resemble wavelets and blob/edge detectors, they are not normalized in the same or any way. Consequently, when images undergo atmospheric transfer, their responses become distorted, leading to incorrect outcomes. We address this limitation by proposing filter normalization, followed by learnable scaling and shifting, akin to batch normalization. This simple yet effective modification ensures that the filters are atmosphere-equivariant, enabling co-domain symmetry. By integrating classical filtering principles into deep learning (applicable to both convolutional neural networks and convolution-dependent vision transformers), our method achieves significant improvements on artificial and natural intensity variation benchmarks. Our ResNet34 could even outperform CLIP by a large margin. Our analysis reveals that unnormalized filters degrade performance, whereas filter normalization regularizes learning, promotes diversity, and improves robustness and generalization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images</title>
<link>https://arxiv.org/abs/2506.06389</link>
<guid>https://arxiv.org/abs/2506.06389</guid>
<content:encoded><![CDATA[
arXiv:2506.06389v2 Announce Type: replace 
Abstract: Deep learning models have shown remarkable success in dermatological image analysis, offering potential for automated skin disease diagnosis. Previously, convolutional neural network(CNN) based architectures have achieved immense popularity and success in computer vision (CV) based task like skin image recognition, generation and video analysis. But with the emergence of transformer based models, CV tasks are now are nowadays carrying out using these models. Vision Transformers (ViTs) is such a transformer-based models that have shown success in computer vision. It uses self-attention mechanisms to achieve state-of-the-art performance across various tasks. However, their reliance on global attention mechanisms makes them susceptible to adversarial perturbations. This paper aims to investigate the susceptibility of ViTs for medical images to adversarial watermarking-a method that adds so-called imperceptible perturbations in order to fool models. By generating adversarial watermarks through Projected Gradient Descent (PGD), we examine the transferability of such attacks to CNNs and analyze the performance defense mechanism -- adversarial training. Results indicate that while performance is not compromised for clean images, ViTs certainly become much more vulnerable to adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless, adversarial training raises it up to 90.0%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlayerOne: Egocentric World Simulator</title>
<link>https://arxiv.org/abs/2506.09995</link>
<guid>https://arxiv.org/abs/2506.09995</guid>
<content:encoded><![CDATA[
arXiv:2506.09995v2 Announce Type: replace 
Abstract: We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and worldconsistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Medical Visual Representation Learning with Pathological-level Cross-Modal Alignment and Correlation Exploration</title>
<link>https://arxiv.org/abs/2506.10573</link>
<guid>https://arxiv.org/abs/2506.10573</guid>
<content:encoded><![CDATA[
arXiv:2506.10573v2 Announce Type: replace 
Abstract: Learning medical visual representations from image-report pairs through joint learning has garnered increasing research attention due to its potential to alleviate the data scarcity problem in the medical domain. The primary challenges stem from the lengthy reports that feature complex discourse relations and semantic pathologies. Previous works have predominantly focused on instance-wise or token-wise cross-modal alignment, often neglecting the importance of pathological-level consistency. This paper presents a novel framework PLACE that promotes the Pathological-Level Alignment and enriches the fine-grained details via Correlation Exploration without additional human annotations. Specifically, we propose a novel pathological-level cross-modal alignment (PCMA) approach to maximize the consistency of pathology observations from both images and reports. To facilitate this, a Visual Pathology Observation Extractor is introduced to extract visual pathological observation representations from localized tokens. The PCMA module operates independently of any external disease annotations, enhancing the generalizability and robustness of our methods. Furthermore, we design a proxy task that enforces the model to identify correlations among image patches, thereby enriching the fine-grained details crucial for various downstream tasks. Experimental results demonstrate that our proposed framework achieves new state-of-the-art performance on multiple downstream tasks, including classification, image-to-text retrieval, semantic segmentation, object detection and report generation. Code is available at https://github.com/Markin-Wang/PLACE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability</title>
<link>https://arxiv.org/abs/2506.13558</link>
<guid>https://arxiv.org/abs/2506.13558</guid>
<content:encoded><![CDATA[
arXiv:2506.13558v3 Announce Type: replace 
Abstract: Diffusion models are advancing autonomous driving by enabling realistic data synthesis, predictive end-to-end planning, and closed-loop simulation, with a primary focus on temporally consistent generation. However, large-scale 3D scene generation requiring spatial coherence remains underexplored. In this paper, we present X-Scene, a novel framework for large-scale driving scene generation that achieves geometric intricacy, appearance fidelity, and flexible controllability. Specifically, X-Scene supports multi-granular control, including low-level layout conditioning driven by user input or text for detailed scene composition, and high-level semantic guidance informed by user intent and LLM-enriched prompts for efficient customization. To enhance geometric and visual fidelity, we introduce a unified pipeline that sequentially generates 3D semantic occupancy and corresponding multi-view images and videos, ensuring alignment and temporal consistency across modalities. We further extend local regions into large-scale scenes via consistency-aware outpainting, which extrapolates occupancy and images from previously generated areas to maintain spatial and visual coherence. The resulting scenes are lifted into high-quality 3DGS representations, supporting diverse applications such as simulation and scene exploration. Extensive experiments demonstrate that X-Scene substantially advances controllability and fidelity in large-scale scene generation, empowering data generation and simulation for autonomous driving.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation</title>
<link>https://arxiv.org/abs/2506.19852</link>
<guid>https://arxiv.org/abs/2506.19852</guid>
<content:encoded><![CDATA[
arXiv:2506.19852v2 Announce Type: replace 
Abstract: Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $\mathcal{O}(n \log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $\mathcal{O}(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\times$ longer while reducing training costs by up to 4.4$\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\times$ compared to dense attention inference. Code is released at \href{https://github.com/mit-han-lab/radial-attention}{https://github.com/mit-han-lab/radial-attention}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Explainable Bilingual Multimodal Misinformation Detection and Localization</title>
<link>https://arxiv.org/abs/2506.22930</link>
<guid>https://arxiv.org/abs/2506.22930</guid>
<content:encoded><![CDATA[
arXiv:2506.22930v2 Announce Type: replace 
Abstract: The increasing realism of multimodal content has made misinformation more subtle and harder to detect, especially in news media where images are frequently paired with bilingual (e.g., Chinese-English) subtitles. Such content often includes localized image edits and cross-lingual inconsistencies that jointly distort meaning while remaining superficially plausible. We introduce BiMi, a bilingual multimodal framework that jointly performs region-level localization, cross-modal and cross-lingual consistency detection, and natural language explanation for misinformation analysis. To support generalization, BiMi integrates an online retrieval module that supplements model reasoning with up-to-date external context. We further release BiMiBench, a large-scale and comprehensive benchmark constructed by systematically editing real news images and subtitles, comprising 104,000 samples with realistic manipulations across visual and linguistic modalities. To enhance interpretability, we apply Group Relative Policy Optimization (GRPO) to improve explanation quality, marking the first use of GRPO in this domain. Extensive experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore, advancing state-of-the-art performance in realistic, multilingual misinformation detection. Code, models, and datasets will be released.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions</title>
<link>https://arxiv.org/abs/2507.14555</link>
<guid>https://arxiv.org/abs/2507.14555</guid>
<content:encoded><![CDATA[
arXiv:2507.14555v2 Announce Type: replace 
Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires reasoning about the spatial and semantic relationships between them. Current 3D scene-language models often struggle with this relational understanding, particularly when visual embeddings alone do not adequately convey the roles and interactions of objects. In this paper, we introduce Descrip3D, a novel and powerful framework that explicitly encodes the relationships between objects using natural language. Unlike previous methods that rely only on 2D and 3D embeddings, Descrip3D enhances each object with a textual description that captures both its intrinsic attributes and contextual relationships. These relational cues are incorporated into the model through a dual-level integration: embedding fusion and prompt-level injection. This allows for unified reasoning across various tasks such as grounding, captioning, and question answering, all without the need for task-specific heads or additional supervision. When evaluated on five benchmark datasets, including ScanRefer, Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms strong baseline models, demonstrating the effectiveness of language-guided relational representation for understanding complex indoor scenes. Our code and data are publicly available at https://github.com/jintangxue/Descrip3D.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images</title>
<link>https://arxiv.org/abs/2507.14670</link>
<guid>https://arxiv.org/abs/2507.14670</guid>
<content:encoded><![CDATA[
arXiv:2507.14670v3 Announce Type: replace 
Abstract: Accurately predicting gene expression from histopathology images offers a scalable and non-invasive approach to molecular profiling, with significant implications for precision medicine and computational pathology. However, existing methods often underutilize the cross-modal representation alignment between histopathology images and gene expression profiles across multiple representational levels, thereby limiting their prediction performance. To address this, we propose Gene-DML, a unified framework that structures latent space through Dual-pathway Multi-Level discrimination to enhance correspondence between morphological and transcriptional modalities. The multi-scale instance-level discrimination pathway aligns hierarchical histopathology representations extracted at local, neighbor, and global levels with gene expression profiles, capturing scale-aware morphological-transcriptional relationships. In parallel, the cross-level instance-group discrimination pathway enforces structural consistency between individual (image/gene) instances and modality-crossed (gene/image, respectively) groups, strengthening the alignment across modalities. By jointly modeling fine-grained and structural-level discrimination, Gene-DML is able to learn robust cross-modal representations, enhancing both predictive accuracy and generalization across diverse biological contexts. Extensive experiments on public spatial transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art performance in gene expression prediction. The code and processed datasets are available at https://github.com/YXSong000/Gene-DML.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows</title>
<link>https://arxiv.org/abs/2507.18405</link>
<guid>https://arxiv.org/abs/2507.18405</guid>
<content:encoded><![CDATA[
arXiv:2507.18405v2 Announce Type: replace 
Abstract: We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRWKV: Focusing on Object Edges for Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2507.18594</link>
<guid>https://arxiv.org/abs/2507.18594</guid>
<content:encoded><![CDATA[
arXiv:2507.18594v3 Announce Type: replace 
Abstract: Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VGS-ATD: Robust Distributed Learning for Multi-Label Medical Image Classification Under Heterogeneous and Imbalanced Conditions</title>
<link>https://arxiv.org/abs/2507.18657</link>
<guid>https://arxiv.org/abs/2507.18657</guid>
<content:encoded><![CDATA[
arXiv:2507.18657v3 Announce Type: replace 
Abstract: In recent years, advanced deep learning architectures have shown strong performance in medical imaging tasks. However, the traditional centralized learning paradigm poses serious privacy risks as all data is collected and trained on a single server. To mitigate this challenge, decentralized approaches such as federated learning and swarm learning have emerged, allowing model training on local nodes while sharing only model weights. While these methods enhance privacy, they struggle with heterogeneous and imbalanced data and suffer from inefficiencies due to frequent communication and the aggregation of weights. More critically, the dynamic and complex nature of clinical environments demands scalable AI systems capable of continuously learning from diverse modalities and multilabels. Yet, both centralized and decentralized models are prone to catastrophic forgetting during system expansion, often requiring full model retraining to incorporate new data. To address these limitations, we propose VGS-ATD, a novel distributed learning framework. To validate VGS-ATD, we evaluate it in experiments spanning 30 datasets and 80 independent labels across distributed nodes, VGS-ATD achieved an overall accuracy of 92.7%, outperforming centralized learning (84.9%) and swarm learning (72.99%), while federated learning failed under these conditions due to high requirements on computational resources. VGS-ATD also demonstrated strong scalability, with only a 1% drop in accuracy on existing nodes after expansion, compared to a 20% drop in centralized learning, highlighting its resilience to catastrophic forgetting. Additionally, it reduced computational costs by up to 50% relative to both centralized and swarm learning, confirming its superior efficiency and scalability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pic2Diagnosis: A Method for Diagnosis of Cardiovascular Diseases from the Printed ECG Pictures</title>
<link>https://arxiv.org/abs/2507.19961</link>
<guid>https://arxiv.org/abs/2507.19961</guid>
<content:encoded><![CDATA[
arXiv:2507.19961v2 Announce Type: replace 
Abstract: The electrocardiogram (ECG) is a vital tool for diagnosing heart diseases. However, many disease patterns are derived from outdated datasets and traditional stepwise algorithms with limited accuracy. This study presents a method for direct cardiovascular disease (CVD) diagnosis from ECG images, eliminating the need for digitization. The proposed approach utilizes a two-step curriculum learning framework, beginning with the pre-training of a classification model on segmentation masks, followed by fine-tuning on grayscale, inverted ECG images. Robustness is further enhanced through an ensemble of three models with averaged outputs, achieving an AUC of 0.9534 and an F1 score of 0.7801 on the BHF ECG Challenge dataset, outperforming individual models. By effectively handling real-world artifacts and simplifying the diagnostic process, this method offers a reliable solution for automated CVD diagnosis, particularly in resource-limited settings where printed or scanned ECG images are commonly used. Such an automated procedure enables rapid and accurate diagnosis, which is critical for timely intervention in CVD cases that often demand urgent care.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations</title>
<link>https://arxiv.org/abs/2508.03209</link>
<guid>https://arxiv.org/abs/2508.03209</guid>
<content:encoded><![CDATA[
arXiv:2508.03209v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) such as GPT-4o now demonstrate a remarkable ability to infer users' locations from public shared images, posing a substantial risk to geoprivacy. Although adversarial perturbations offer a potential defense, current methods are ill-suited for this scenario: they often perform poorly on high-resolution images and low perturbation budgets, and may introduce irrelevant semantic content. To address these limitations, we propose GeoShield, a novel adversarial framework designed for robust geoprivacy protection in real-world scenarios. GeoShield comprises three key modules: a feature disentanglement module that separates geographical and non-geographical information, an exposure element identification module that pinpoints geo-revealing regions within an image, and a scale-adaptive enhancement module that jointly optimizes perturbations at both global and local levels to ensure effectiveness across resolutions. Extensive experiments on challenging benchmarks show that GeoShield consistently surpasses prior methods in black-box settings, achieving strong privacy protection with minimal impact on visual or semantic quality. To our knowledge, this work is the first to explore adversarial perturbations for defending against geolocation inference by advanced VLMs, providing a practical and effective solution to escalating privacy concerns.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCoAR: Deep Concept Injection into Unified Autoregressive Models for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2508.07341</link>
<guid>https://arxiv.org/abs/2508.07341</guid>
<content:encoded><![CDATA[
arXiv:2508.07341v2 Announce Type: replace 
Abstract: The unified autoregressive (AR) model excels at multimodal understanding and generation. However, its full potential in the domain of customized image generation has yet to be fully realized. Existing customization approaches for unified AR models face a fundamental dilemma: adaptation-based methods suffer from overfitting and scalability bottlenecks, while concept-injection paradigms are constrained by a shallow injection strategy that leads to poor visual fidelity and impaired re-contextualization. To address this, we propose DCoAR, a novel deep concept injection framework that maintains a completely frozen pre-trained model. DCoAR deeply integrates new concepts through a Layer-wise Multimodal Context Learning (LMCL) strategy, which is stabilized by a multi-faceted regularization scheme: a Dual Prior Preservation (DPP) loss to mitigate semantic drift and a Context-Aware Self-Regularization (CASR) loss to enhance re-contextualization. The framework also enables training-free subject customization in user-provided styles. Experiments demonstrate that DCoAR significantly outperforms previous injection-based methods and achieves performance competitive with adaptation-based approaches while requiring substantially fewer trainable parameters. Code: https://github.com/KZF-kzf/CoAR
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVCtrl: Efficient Control Adapter for Visual Generation</title>
<link>https://arxiv.org/abs/2508.10963</link>
<guid>https://arxiv.org/abs/2508.10963</guid>
<content:encoded><![CDATA[
arXiv:2508.10963v2 Announce Type: replace 
Abstract: Visual generation includes both image and video generation, training probabilistic models to create coherent, diverse, and semantically faithful content from scratch. While early research focused on unconditional sampling, practitioners now demand controllable generation that allows precise specification of layout, pose, motion, or style. While ControlNet grants precise spatial-temporal control, its auxiliary branch markedly increases latency and introduces redundant computation in both uncontrolled regions and denoising steps, especially for video. To address this problem, we introduce EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead without retraining the model. Specifically, we propose a spatio-temporal dual caching strategy for sparse control information. For spatial redundancy, we first profile how each layer of DiT-ControlNet responds to fine-grained control, then partition the network into global and local functional zones. A locality-aware cache focuses computation on the local zones that truly need the control signal, skipping the bulk of redundant computation in global regions. For temporal redundancy, we selectively omit unnecessary denoising steps to improve efficiency. Extensive experiments on CogVideo-Controlnet, Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image and video control generation without the need for training. For example, it achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and Wan2.1-Controlnet, respectively, with almost no degradation in generation quality.Codes are available in the supplementary materials.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLAIR: Frequency- and Locality-Aware Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2508.13544</link>
<guid>https://arxiv.org/abs/2508.13544</guid>
<content:encoded><![CDATA[
arXiv:2508.13544v4 Announce Type: replace 
Abstract: Implicit Neural Representations (INRs) leverage neural networks to map coordinates to corresponding signals, enabling continuous and compact representations. This paradigm has driven significant advances in various vision tasks. However, existing INRs lack frequency selectivity and spatial localization, leading to an over-reliance on redundant signal components. Consequently, they exhibit spectral bias, tending to learn low-frequency components early while struggling to capture fine high-frequency details. To address these issues, we propose FLAIR (Frequency- and Locality-Aware Implicit Neural Representations), which incorporates two key innovations. The first is Band-Localized Activation (BLA), a novel activation designed for joint frequency selection and spatial localization under the constraints of the time-frequency uncertainty principle (TFUP). Through structured frequency control and spatially localized responses, BLA effectively mitigates spectral bias and enhances training stability. The second is Wavelet-Energy-Guided Encoding (WEGE), which leverages the discrete wavelet transform to compute energy scores and explicitly guide frequency information to the network, enabling precise frequency selection and adaptive band control. Our method consistently outperforms existing INRs in 2D image representation, as well as 3D shape reconstruction and novel view synthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance</title>
<link>https://arxiv.org/abs/2508.16644</link>
<guid>https://arxiv.org/abs/2508.16644</guid>
<content:encoded><![CDATA[
arXiv:2508.16644v2 Announce Type: replace 
Abstract: Diffusion models have shown remarkable progress in photorealistic image synthesis, yet they remain unreliable for generating scenes with a precise number of object instances, particularly in complex and high-density settings. We present CountLoop, a training-free framework that provides diffusion models with accurate instance control through iterative structured feedback. The approach alternates between image generation and multimodal agent evaluation, where a language-guided planner and critic assess object counts, spatial arrangements, and attribute consistency. This feedback is then used to refine layouts and guide subsequent generations. To further improve separation between objects, especially in occluded scenes, we introduce instance-driven attention masking and compositional generation techniques. Experiments on COCO Count, T2I CompBench, and two new high-instance benchmarks show that CountLoop achieves counting accuracy of up to 98% while maintaining spatial fidelity and visual quality, outperforming layout-based and gradient-guided baselines with a score of 0.97.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning</title>
<link>https://arxiv.org/abs/2508.21451</link>
<guid>https://arxiv.org/abs/2508.21451</guid>
<content:encoded><![CDATA[
arXiv:2508.21451v3 Announce Type: replace 
Abstract: Systems such as video chatbots and navigation robots often depend on streaming image captioning to interpret visual inputs. Existing approaches typically employ large multimodal language models (MLLMs) for this purpose, but their substantial computational cost hinders practical application. This limitation motivates our development of a lightweight captioning model. Our investigation begins by replacing the large-scale language component in MLLMs with a compact 125M-parameter model. Surprisingly, this compact model, despite a 93x reduction in size, achieves comparable performance to MLLMs, suggesting that factual image captioning does not significantly require the complex reasoning abilities of LLMs. Despite this promising result, our lightweight model still lacks reliability. To address this, we draw inspiration from the human visual process: perceiving a global and coarse understanding of the scene before attending to finer details. Accordingly, we propose a multimodal self-refinement framework that guides the model to utilize features from salient regions, identified by referencing the previous coarse caption, and to produce a refined description. Experimental results demonstrate the superiority of our model in both single-sentence and detailed captioning, extending even to long-range video QA tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching</title>
<link>https://arxiv.org/abs/2509.05952</link>
<guid>https://arxiv.org/abs/2509.05952</guid>
<content:encoded><![CDATA[
arXiv:2509.05952v4 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has recently emerged as a powerful technique for improving image and video generation in Diffusion and Flow Matching models, specifically for enhancing output quality and alignment with prompts. A critical step for applying online RL methods on Flow Matching is the introduction of stochasticity into the deterministic framework, commonly realized by Stochastic Differential Equation (SDE). Our investigation reveals a significant drawback to this approach: SDE-based sampling introduces pronounced noise artifacts in the generated images, which we found to be detrimental to the reward learning process. A rigorous theoretical analysis traces the origin of this noise to an excess of stochasticity injected during inference. To address this, we draw inspiration from Denoising Diffusion Implicit Models (DDIM) to reformulate the sampling process. Our proposed method, Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This leads to more accurate reward modeling, ultimately enabling faster and more stable convergence for reinforcement learning-based optimizers like Flow-GRPO and Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT</title>
<link>https://arxiv.org/abs/2509.12069</link>
<guid>https://arxiv.org/abs/2509.12069</guid>
<content:encoded><![CDATA[
arXiv:2509.12069v3 Announce Type: replace 
Abstract: Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in dentistry, providing volumetric information about the anatomical structures of jaws and teeth. Accurate segmentation of these anatomies is critical for clinical applications such as diagnosis and surgical planning, but remains time-consuming and challenging. In this paper, we present U-Mamba2, a new neural network architecture designed for multi-anatomy CBCT segmentation in the context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state space models into the U-Net architecture, enforcing stronger structural constraints for higher efficiency without compromising performance. In addition, we integrate interactive click prompts with cross-attention blocks, pre-train U-Mamba2 using self-supervised learning, and incorporate dental domain knowledge into the model design to address key challenges of dental anatomy segmentation in CBCT. Extensive experiments, including independent tests, demonstrate that U-Mamba2 is both effective and efficient, securing first place in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2 achieved a mean Dice of 0.84, HD95 of 38.17 with the held-out test data, with an average inference time of 40.58s. In Task 2, U-Mamba2 achieved the mean Dice of 0.87 and HD95 of 2.15 with the held-out test data. The code is publicly available at https://github.com/zhiqin1998/UMamba2.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A PCA Based Model for Surface Reconstruction from Incomplete Point Clouds</title>
<link>https://arxiv.org/abs/2509.15675</link>
<guid>https://arxiv.org/abs/2509.15675</guid>
<content:encoded><![CDATA[
arXiv:2509.15675v2 Announce Type: replace 
Abstract: Point cloud data represents a crucial category of information for mathematical modeling, and surface reconstruction from such data is an important task across various disciplines. However, during the scanning process, the collected point cloud data may fail to cover the entire surface due to factors such as high light-absorption rate and occlusions, resulting in incomplete datasets. Inferring surface structures in data-missing regions and successfully reconstructing the surface poses a challenge. In this paper, we present a Principal Component Analysis (PCA) based model for surface reconstruction from incomplete point cloud data. Initially, we employ PCA to estimate the normal information of the underlying surface from the available point cloud data. This estimated normal information serves as a regularizer in our model, guiding the reconstruction of the surface, particularly in areas with missing data. Additionally, we introduce an operator-splitting method to effectively solve the proposed model. Through systematic experimentation, we demonstrate that our model successfully infers surface structures in data-missing regions and well reconstructs the underlying surfaces, outperforming existing methodologies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARSS: Taming Decoder-only Autoregressive Visual Generation for View Synthesis From Single View</title>
<link>https://arxiv.org/abs/2509.23008</link>
<guid>https://arxiv.org/abs/2509.23008</guid>
<content:encoded><![CDATA[
arXiv:2509.23008v2 Announce Type: replace 
Abstract: Diffusion models have achieved impressive results in world modeling tasks, including novel view generation from sparse inputs. However, most existing diffusion-based NVS methods generate target views jointly via an iterative denoising process, which makes it less straightforward to impose a strictly causal structure along a camera trajectory. In contrast, autoregressive (AR) models operate in a causal fashion, generating each token based on all previously generated tokens. In this work, we introduce ARSS, a novel framework that leverages a GPT-style decoder-only AR model to generate novel views from a single image, conditioned on a predefined camera trajectory. We employ an off-the-shelf video tokenizer to map continuous image sequences into discrete tokens and propose a camera encoder that converts camera trajectories into 3D positional guidance. Then to enhance generation quality while preserving the autoregressive structure, we propose an autoregressive transformer module that randomly permutes the spatial order of tokens while maintaining their temporal order. Qualitative and quantitative experiments on public datasets demonstrate that our method achieves overall performance comparable to state-of-the-art view synthesis approaches based on diffusion models. Project page: https://wbteng9526.github.io/arss/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D</title>
<link>https://arxiv.org/abs/2509.24528</link>
<guid>https://arxiv.org/abs/2509.24528</guid>
<content:encoded><![CDATA[
arXiv:2509.24528v3 Announce Type: replace 
Abstract: 3D scene understanding is fundamental for embodied AI and robotics, supporting reliable perception for interaction and navigation. Recent approaches achieve zero-shot, open-vocabulary 3D semantic mapping by assigning embedding vectors to 2D class-agnostic masks generated via vision-language models (VLMs) and projecting these into 3D. However, these methods often produce fragmented masks and inaccurate semantic assignments due to the direct use of raw masks, limiting their effectiveness in complex environments. To address this, we leverage SemanticSAM with progressive granularity refinement to generate more accurate and numerous object-level masks, mitigating the over-segmentation commonly observed in mask generation models such as vanilla SAM, and improving downstream 3D semantic segmentation. To further enhance semantic context, we employ a context-aware CLIP encoding strategy that integrates multiple contextual views of each mask using empirically determined weighting, providing much richer visual context. We evaluate our approach on multiple 3D scene understanding tasks, including 3D semantic segmentation and object retrieval from language queries, across several benchmark datasets. Experimental results demonstrate significant improvements over existing methods, highlighting the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaChest: Generalized few-shot learning of pathologies from chest X-rays</title>
<link>https://arxiv.org/abs/2509.25590</link>
<guid>https://arxiv.org/abs/2509.25590</guid>
<content:encoded><![CDATA[
arXiv:2509.25590v2 Announce Type: replace 
Abstract: The limited availability of annotated data presents a major challenge for applying deep learning methods to medical image analysis. Few-shot learning methods aim to recognize new classes from only a small number of labeled examples. These methods are typically studied under the standard few-shot learning setting, where all classes in a task are new. However, medical applications such as pathology classification from chest X-rays often require learning new classes while simultaneously leveraging knowledge of previously known ones, a scenario more closely aligned with generalized few-shot classification. Despite its practical relevance, few-shot learning has been scarcely studied in this context. In this work, we present MetaChest, a large-scale dataset of 479,215 chest X-rays collected from four public databases. MetaChest includes a meta-set partition specifically designed for standard few-shot classification, as well as an algorithm for generating multi-label episodes. We conduct extensive experiments evaluating both a standard transfer learning approach and an extension of ProtoNet across a wide range of few-shot multi-label classification tasks. Our results demonstrate that increasing the number of classes per episode and the number of training examples per class improves classification performance. Notably, the transfer learning approach consistently outperforms the ProtoNet extension, despite not being tailored for few-shot learning. We also show that higher-resolution images improve accuracy at the cost of additional computation, while efficient model architectures achieve comparable performance to larger models with significantly reduced resource requirements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</title>
<link>https://arxiv.org/abs/2509.25774</link>
<guid>https://arxiv.org/abs/2509.25774</guid>
<content:encoded><![CDATA[
arXiv:2509.25774v2 Announce Type: replace 
Abstract: While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeScope: Towards Task-Oriented Temporal Grounding In Long Videos</title>
<link>https://arxiv.org/abs/2509.26360</link>
<guid>https://arxiv.org/abs/2509.26360</guid>
<content:encoded><![CDATA[
arXiv:2509.26360v3 Announce Type: replace 
Abstract: Identifying key temporal intervals within long videos, known as temporal grounding (TG), is important to video understanding and reasoning tasks. In this paper, we introduce a new form of the temporal grounding problem, \textbf{Task-oriented Temporal Grounding} (\textbf{ToTG}), which is driven by the requirements of downstream tasks rather than explicit time-interval descriptions. For example, a ToTG input may be "explain why the man in the video is sent to the hospital," whereas traditional TG would take an explicit temporal description such as "the moments when the man is tripped by a stone and falls to the ground." This new ToTG formulation presents significant challenges for existing TG methods, as it requires jointly performing deep task comprehension and fine-grained temporal localization within long videos. To address these challenges, we conduct a systematic set of studies. First, we construct \textbf{a new benchmark ToTG-Bench}, which comprehensively evaluates ToTG performance across diverse settings. Second, we introduce \textbf{a new temporal-ground method TimeScope}, which performs coarse-to-fine localization through a progressive reasoning process. Leveraging extensive supervised fine-tuning with carefully curated chain-of-thought (CoT) data from a variety of scenarios, TimeScope generalizes effectively across tasks and domains. Our evaluation demonstrates \textbf{TimeScope's empirical advantages} over existing baselines from three perspectives: (1) substantial improvements in grounding precision, (2) significant benefits to downstream tasks, and (3) strong generalizability across different scenarios. All models, datasets, and source code will be fully open-sourced to support future research in this area.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extreme Amodal Face Detection</title>
<link>https://arxiv.org/abs/2510.06791</link>
<guid>https://arxiv.org/abs/2510.06791</guid>
<content:encoded><![CDATA[
arXiv:2510.06791v2 Announce Type: replace 
Abstract: Extreme amodal detection is the task of inferring the 2D location of objects that are not fully visible in the input image but are visible within an expanded field-of-view. This differs from amodal detection, where the object is partially visible within the input image, but is occluded. In this paper, we consider the sub-problem of face detection, since this class provides motivating applications involving safety and privacy, but do not tailor our method specifically to this class. Existing approaches rely on image sequences so that missing detections may be interpolated from surrounding frames or make use of generative models to sample possible completions. In contrast, we consider the single-image task and propose a more efficient, sample-free approach that makes use of the contextual cues from the image to infer the presence of unseen faces. We design a heatmap-based extreme amodal object detector that addresses the problem of efficiently predicting a lot (the out-of-frame region) from a little (the image) with a selective coarse-to-fine decoder. Our method establishes strong results for this new task, even outperforming less efficient generative approaches. Code, data, and models are available at https://charliesong1999.github.io/exaft_web/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReSplat: Learning Recurrent Gaussian Splats</title>
<link>https://arxiv.org/abs/2510.08575</link>
<guid>https://arxiv.org/abs/2510.08575</guid>
<content:encoded><![CDATA[
arXiv:2510.08575v2 Announce Type: replace 
Abstract: While feed-forward Gaussian splatting models offer computational efficiency and can generalize to sparse input settings, their performance is fundamentally constrained by relying on a single forward pass for inference. We propose ReSplat, a feed-forward recurrent Gaussian splatting model that iteratively refines 3D Gaussians without explicitly computing gradients. Our key insight is that the Gaussian splatting rendering error serves as a rich feedback signal, guiding the recurrent network to learn effective Gaussian updates. This feedback signal naturally adapts to unseen data distributions at test time, enabling robust generalization across datasets, view counts and image resolutions. To initialize the recurrent process, we introduce a compact reconstruction model that operates in a $16 \times$ subsampled space, producing $16 \times$ fewer Gaussians than previous per-pixel Gaussian models. This substantially reduces computational overhead and allows for efficient Gaussian updates. Extensive experiments across varying of input views (2, 8, 16, 32), resolutions ($256 \times 256$ to $540 \times 960$), and datasets (DL3DV, RealEstate10K and ACID) demonstrate that our method achieves state-of-the-art performance while significantly reducing the number of Gaussians and improving the rendering speed. Our project page is at https://haofeixu.github.io/resplat/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning</title>
<link>https://arxiv.org/abs/2510.13515</link>
<guid>https://arxiv.org/abs/2510.13515</guid>
<content:encoded><![CDATA[
arXiv:2510.13515v3 Announce Type: replace 
Abstract: Universal multimodal embedding models are foundational to various tasks. Existing approaches typically employ in-batch negative mining by measuring the similarity of query-candidate pairs. However, these methods often struggle to capture subtle semantic differences among candidates and lack diversity in negative samples. Moreover, the embeddings exhibit limited discriminative ability in distinguishing false and hard negatives. In this paper, we leverage the advanced understanding capabilities of MLLMs to enhance representation learning and present a novel Universal Multimodal Embedding (UniME-V2) model. Our approach first constructs a potential hard negative set through global retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes MLLMs to assess the semantic alignment of query-candidate pairs and generate soft semantic matching scores. These scores serve as a foundation for hard negative mining, mitigating the impact of false negatives and enabling the identification of diverse, high-quality hard negatives. Furthermore, the semantic matching scores are used as soft labels to mitigate the rigid one-to-one mapping constraint. By aligning the similarity matrix with the soft semantic matching score matrix, the model learns semantic distinctions among candidates, significantly enhancing its discriminative capacity. To further improve performance, we propose UniME-V2-Reranker, a reranking model trained on our mined hard negatives through a joint pairwise and listwise optimization approach. We conduct comprehensive experiments on the MMEB benchmark and multiple retrieval tasks, demonstrating that our method achieves state-of-the-art performance on average across all tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</title>
<link>https://arxiv.org/abs/2510.19060</link>
<guid>https://arxiv.org/abs/2510.19060</guid>
<content:encoded><![CDATA[
arXiv:2510.19060v2 Announce Type: replace 
Abstract: While vision-language models (VLMs) have advanced into detailed image description, evaluation remains a challenge. Standard metrics (e.g. CIDEr, SPICE) were designed for short texts and tuned to recognize errors that are now uncommon, such as object misidentification. In contrast, long texts require sensitivity to attribute and relation attachments and scores that localize errors to particular text spans. In this work, we introduce PoSh, a metric for detailed image description that uses scene graphs as structured rubrics to guide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grained errors (e.g. mistakes in compositional understanding). PoSh is replicable, interpretable and a better proxy for human raters than existing metrics (including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging new dataset, DOCENT. This novel benchmark contains artwork, paired with expert-written references, and model-generated descriptions, augmented with granular and coarse judgments of their quality from art history students. Thus, DOCENT enables evaluating both detailed image description metrics and detailed image description itself in a challenging new domain. We show that PoSh achieves stronger correlations (+0.05 Spearman $\rho$) with the human judgments in DOCENT than the best open-weight alternatives, is robust to image type (using CapArena, an existing dataset of web imagery) and is a capable reward function, outperforming standard supervised fine-tuning. Then, using PoSh, we characterize the performance of open and closed models in describing the paintings, sketches and statues in DOCENT and find that foundation models struggle to achieve full, error-free coverage of images with rich scene dynamics, establishing a demanding new task to gauge VLM progress. Through both PoSh and DOCENT, we hope to enable advances in important areas such as assistive text generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FootFormer: Estimating Stability from Visual Input</title>
<link>https://arxiv.org/abs/2510.19170</link>
<guid>https://arxiv.org/abs/2510.19170</guid>
<content:encoded><![CDATA[
arXiv:2510.19170v2 Announce Type: replace 
Abstract: We propose FootFormer, a cross-modality approach for jointly predicting human motion dynamics directly from visual input. On multiple datasets, FootFormer achieves statistically significantly better or equivalent estimates of foot pressure distributions, foot contact maps, and center of mass (CoM), as compared with existing methods that generate one or two of those measures. Furthermore, FootFormer achieves SOTA performance in estimating stability-predictive components (CoP, CoM, BoS) used in classic kinesiology metrics. Code and data are available at https://github.com/keatonkraiger/Vision-to-Stability.git.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRELLISWorld: Training-Free World Generation from Object Generators</title>
<link>https://arxiv.org/abs/2510.23880</link>
<guid>https://arxiv.org/abs/2510.23880</guid>
<content:encoded><![CDATA[
arXiv:2510.23880v2 Announce Type: replace 
Abstract: Text-driven 3D scene generation holds promise for a wide range of applications, from virtual prototyping to AR/VR and simulation. However, existing methods are often constrained to single-object generation, require domain-specific training, or lack support for full 360-degree viewability. In this work, we present a training-free approach to 3D scene synthesis by repurposing general-purpose text-to-3D object diffusion models as modular tile generators. We reformulate scene generation as a multi-tile denoising problem, where overlapping 3D regions are independently generated and seamlessly blended via weighted averaging. This enables scalable synthesis of large, coherent scenes while preserving local semantic control. Our method eliminates the need for scene-level datasets or retraining, relies on minimal heuristics, and inherits the generalization capabilities of object-level priors. We demonstrate that our approach supports diverse scene layouts, efficient generation, and flexible editing, establishing a simple yet powerful foundation for general-purpose, language-driven 3D scene construction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionStream: Real-Time Video Generation with Interactive Motion Controls</title>
<link>https://arxiv.org/abs/2511.01266</link>
<guid>https://arxiv.org/abs/2511.01266</guid>
<content:encoded><![CDATA[
arXiv:2511.01266v2 Announce Type: replace 
Abstract: Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons -- (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastGS: Training 3D Gaussian Splatting in 100 Seconds</title>
<link>https://arxiv.org/abs/2511.04283</link>
<guid>https://arxiv.org/abs/2511.04283</guid>
<content:encoded><![CDATA[
arXiv:2511.04283v3 Announce Type: replace 
Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at https://fastgs.github.io/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-ANC: Adaptive Neural Collapse for Robust 3D Point Cloud Recognition</title>
<link>https://arxiv.org/abs/2511.07040</link>
<guid>https://arxiv.org/abs/2511.07040</guid>
<content:encoded><![CDATA[
arXiv:2511.07040v2 Announce Type: replace 
Abstract: Deep neural networks have recently achieved notable progress in 3D point cloud recognition, yet their vulnerability to adversarial perturbations poses critical security challenges in practical deployments. Conventional defense mechanisms struggle to address the evolving landscape of multifaceted attack patterns. Through systematic analysis of existing defenses, we identify that their unsatisfactory performance primarily originates from an entangled feature space, where adversarial attacks can be performed easily. To this end, we present 3D-ANC, a novel approach that capitalizes on the Neural Collapse (NC) mechanism to orchestrate discriminative feature learning. In particular, NC depicts where last-layer features and classifier weights jointly evolve into a simplex equiangular tight frame (ETF) arrangement, establishing maximally separable class prototypes. However, leveraging this advantage in 3D recognition confronts two substantial challenges: (1) prevalent class imbalance in point cloud datasets, and (2) complex geometric similarities between object categories. To tackle these obstacles, our solution combines an ETF-aligned classification module with an adaptive training framework consisting of representation-balanced learning (RBL) and dynamic feature direction loss (FDL). 3D-ANC seamlessly empowers existing models to develop disentangled feature spaces despite the complexity in 3D data distribution. Comprehensive evaluations state that 3D-ANC significantly improves the robustness of models with various structures on two datasets. For instance, DGCNN's classification accuracy is elevated from 27.2% to 80.9% on ModelNet40 -- a 53.7% absolute gain that surpasses leading baselines by 34.0%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types</title>
<link>https://arxiv.org/abs/2511.11030</link>
<guid>https://arxiv.org/abs/2511.11030</guid>
<content:encoded><![CDATA[
arXiv:2511.11030v4 Announce Type: replace 
Abstract: Artificial intelligence is revealing what medicine never intended to encode. Deep vision models, trained on chest X-rays, can now detect not only disease but also invisible traces of social inequality. In this study, we show that state-of-the-art architectures (DenseNet121, SwinV2-B, MedMamba) can predict a patient's health insurance type, a strong proxy for socioeconomic status, from normal chest X-rays with significant accuracy (AUC around 0.70 on MIMIC-CXR-JPG, 0.68 on CheXpert). The signal was unlikely contributed by demographic features by our machine learning study combining age, race, and sex labels to predict health insurance types; it also remains detectable when the model is trained exclusively on a single racial group. Patch-based occlusion reveals that the signal is diffuse rather than localized, embedded in the upper and mid-thoracic regions. This suggests that deep networks may be internalizing subtle traces of clinical environments, equipment differences, or care pathways; learning socioeconomic segregation itself. These findings challenge the assumption that medical images are neutral biological data. By uncovering how models perceive and exploit these hidden social signatures, this work reframes fairness in medical AI: the goal is no longer only to balance datasets or adjust thresholds, but to interrogate and disentangle the social fingerprints embedded in clinical data itself.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STONE: Pioneering the One-to-N Universal Backdoor Threat in 3D Point Cloud</title>
<link>https://arxiv.org/abs/2511.11210</link>
<guid>https://arxiv.org/abs/2511.11210</guid>
<content:encoded><![CDATA[
arXiv:2511.11210v3 Announce Type: replace 
Abstract: Backdoor attacks pose a critical threat to deep learning, especially in safety-sensitive 3D domains such as autonomous driving and robotics. While potent, existing attacks on 3D point clouds are predominantly limited to one-to-one paradigms. The more flexible and universal one-to-N multi-target backdoor threat remains largely unexplored, lacking both theoretical and practical foundations. To bridge this gap, we propose STONE (Spherical Trigger One-to-N universal backdoor Enabling), the first method to instantiate this threat via a configurable spherical trigger design. Its parameterized spatial properties establish a dynamic key space, enabling a single trigger to map to multiple target labels. Theoretically, we ground STONE in a Neural Tangent Kernel (NTK) analysis, providing the first formal basis for one-to-N mappings in 3D models. Empirically, extensive evaluations demonstrate high attack success rates (up to 100\%) without compromising clean-data accuracy. This work establishes a foundational benchmark for multi-target backdoor threats under dirty-label and black-box settings in 3D vision -- a crucial step toward securing future intelligent systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data</title>
<link>https://arxiv.org/abs/2511.13036</link>
<guid>https://arxiv.org/abs/2511.13036</guid>
<content:encoded><![CDATA[
arXiv:2511.13036v2 Announce Type: replace 
Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language</title>
<link>https://arxiv.org/abs/2511.13127</link>
<guid>https://arxiv.org/abs/2511.13127</guid>
<content:encoded><![CDATA[
arXiv:2511.13127v2 Announce Type: replace 
Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models. Our demos and codes can be found at https://github.com/NY1024/VEIL.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution Matching Distillation Meets Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.13649</link>
<guid>https://arxiv.org/abs/2511.13649</guid>
<content:encoded><![CDATA[
arXiv:2511.13649v3 Announce Type: replace 
Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</title>
<link>https://arxiv.org/abs/2511.15464</link>
<guid>https://arxiv.org/abs/2511.15464</guid>
<content:encoded><![CDATA[
arXiv:2511.15464v2 Announce Type: replace 
Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment</title>
<link>https://arxiv.org/abs/2511.17397</link>
<guid>https://arxiv.org/abs/2511.17397</guid>
<content:encoded><![CDATA[
arXiv:2511.17397v2 Announce Type: replace 
Abstract: Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Words and Pixels: A Benchmark for Implicit World Knowledge Reasoning in Generative Models</title>
<link>https://arxiv.org/abs/2511.18271</link>
<guid>https://arxiv.org/abs/2511.18271</guid>
<content:encoded><![CDATA[
arXiv:2511.18271v2 Announce Type: replace 
Abstract: Text-to-image (T2I) models today are capable of producing photorealistic, instruction-following images, yet they still frequently fail on prompts that require implicit world knowledge. Existing evaluation protocols either emphasize compositional alignment or rely on single-round VQA-based scoring, leaving critical dimensions such as knowledge grounding, multi-physics interactions, and auditable evidence-substantially undertested. To address these limitations, we introduce PicWorld, the first comprehensive benchmark that assesses the grasp of implicit world knowledge and physical causal reasoning of T2I models. This benchmark consists of 1,100 prompts across three core categories. To facilitate fine-grained evaluation, we propose PW-Agent, an evidence-grounded multi-agent evaluator to hierarchically assess images on their physical realism and logical consistency by decomposing prompts into verifiable visual evidence. We conduct a thorough analysis of 17 mainstream T2I models on PicWorld, illustrating that they universally exhibit a fundamental limitation in their capacity for implicit world knowledge and physical causal reasoning to varying degrees. The findings highlight the need for reasoning-aware, knowledge-integrative architectures in future T2I systems. The code is available at https://github.com/D4-Lab/PicWorld}{https://github.com/D4-Lab/PicWorld.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</title>
<link>https://arxiv.org/abs/2511.20415</link>
<guid>https://arxiv.org/abs/2511.20415</guid>
<content:encoded><![CDATA[
arXiv:2511.20415v2 Announce Type: replace 
Abstract: Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our project page: https://longhz140516.github.io/MajutsuCity/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion</title>
<link>https://arxiv.org/abs/2511.20821</link>
<guid>https://arxiv.org/abs/2511.20821</guid>
<content:encoded><![CDATA[
arXiv:2511.20821v2 Announce Type: replace 
Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22019</link>
<guid>https://arxiv.org/abs/2511.22019</guid>
<content:encoded><![CDATA[
arXiv:2511.22019v2 Announce Type: replace 
Abstract: Vision-language models (VLMs), such as CLIP, have gained popularity for their strong open vocabulary classification performance, but they are prone to assigning high confidence scores to misclassifications, limiting their reliability in safety-critical applications. We introduce a training-free, post-hoc uncertainty estimation method for contrastive VLMs that can be used to detect erroneous predictions. The key to our approach is to measure visual feature consistency within a class, using feature projection combined with multivariate Gaussians to create class-specific probabilistic embeddings. Our method is VLM-agnostic, requires no fine-tuning, demonstrates robustness to distribution shift, and works effectively with as few as 10 training images per class. Extensive experiments on ImageNet, Flowers102, Food101, EuroSAT and DTD show state-of-the-art error detection performance, significantly outperforming both deterministic and probabilistic VLM baselines. Code is available at https://github.com/zhenxianglin/ICPE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</title>
<link>https://arxiv.org/abs/2511.22699</link>
<guid>https://arxiv.org/abs/2511.22699</guid>
<content:encoded><![CDATA[
arXiv:2511.22699v3 Announce Type: replace 
Abstract: The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the "scale-at-all-costs" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.23292</link>
<guid>https://arxiv.org/abs/2511.23292</guid>
<content:encoded><![CDATA[
arXiv:2511.23292v2 Announce Type: replace 
Abstract: Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering. Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness. However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity. This leads to inefficient texture space utilization, where high-frequency regions are under-sampled and smooth regions waste capacity, causing blurred appearance and loss of fine structural detail. We introduce FACT-GS, a Frequency-Aligned Complexity-aware Texture Gaussian Splatting framework that allocates texture sampling density according to local visual frequency. Grounded in adaptive sampling theory, FACT-GS reformulates texture parameterization as a differentiable sampling-density allocation problem, replacing the uniform textures with a learnable frequency-aware allocation strategy implemented via a deformation field whose Jacobian modulates local sampling density. Built on 2D Gaussian Splatting, FACT-GS performs non-uniform sampling on fixed-resolution texture grids, preserving real-time performance while recovering sharper high-frequency details under the same parameter budget.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</title>
<link>https://arxiv.org/abs/2511.23478</link>
<guid>https://arxiv.org/abs/2511.23478</guid>
<content:encoded><![CDATA[
arXiv:2511.23478v2 Announce Type: replace 
Abstract: Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Code: https://github.com/mbzuai-oryx/Video-R2
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Perspective for Loss-Oriented Imbalanced Learning via Localization</title>
<link>https://arxiv.org/abs/2310.04752</link>
<guid>https://arxiv.org/abs/2310.04752</guid>
<content:encoded><![CDATA[
arXiv:2310.04752v2 Announce Type: replace-cross 
Abstract: Due to the inherent imbalance in real-world datasets, na\"ive Empirical Risk Minimization (ERM) tends to bias the learning process towards the majority classes, hindering generalization to minority classes. To rebalance the learning process, one straightforward yet effective approach is to modify the loss function via class-dependent terms, such as re-weighting and logit-adjustment. However, existing analysis of these loss-oriented methods remains coarse-grained and fragmented, failing to explain some empirical results. After reviewing prior work, we find that the properties used through their analysis are typically global, i.e., defined over the whole dataset. Hence, these properties fail to effectively capture how class-dependent terms influence the learning process. To bridge this gap, we turn to explore the localized versions of such properties i.e., defined within each class. Specifically, we employ localized calibration to provide consistency validation across a broader range of losses and localized Lipschitz continuity to provide a fine-grained generalization bound. In this way, we reach a unified perspective for improving and adjusting loss-oriented methods. Finally, a principled learning algorithm is developed based on these insights. Empirical results on both traditional ResNets and foundation models validate our theoretical analyses and demonstrate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Bioplausible Neuron for SNN for Event Vision</title>
<link>https://arxiv.org/abs/2311.11853</link>
<guid>https://arxiv.org/abs/2311.11853</guid>
<content:encoded><![CDATA[
arXiv:2311.11853v3 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) offer a biologically inspired approach to computer vision that can lead to more efficient processing of visual data with reduced energy consumption. However, maintaining homeostasis within these networks is challenging, as it requires continuous adjustment of neural responses to preserve equilibrium and optimal processing efficiency amidst diverse and often unpredictable input signals. In response to these challenges, we propose the Asynchronous Bioplausible Neuron (ABN), a dynamic spike firing mechanism to auto-adjust the variations in the input signal. Comprehensive evaluation across various datasets demonstrates ABN's enhanced performance in image classification and segmentation, maintenance of neural equilibrium, and energy efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic Images: Leveraging Contextual Attention and Residual Learning</title>
<link>https://arxiv.org/abs/2401.13147</link>
<guid>https://arxiv.org/abs/2401.13147</guid>
<content:encoded><![CDATA[
arXiv:2401.13147v3 Announce Type: replace-cross 
Abstract: This study presents a deep convolutional autoencoder network for filtering reverberation clutter from transthoracic echocardiographic (TTE) image sequences. Given the spatiotemporal nature of this type of clutter, the filtering network employs 3D convolutional layers to suppress it throughout the cardiac cycle. The design of the network incorporates two key features that contribute to the effectiveness of the filter: 1) an attention mechanism for focusing on cluttered regions and leveraging contextual information, and 2) residual learning for preserving fine image structures. To train the network, a diverse set of artifact patterns was simulated and superimposed onto ultra-realistic synthetic TTE sequences from six ultrasound vendors, generating input for the filtering network. The artifact-free sequences served as ground-truth. Performance of the filtering network was evaluated using unseen synthetic and in vivo artifactual sequences. Results from the in vivo dataset confirmed the network's strong generalization capabilities, despite being trained solely on synthetic data and simulated artifacts. The suitability of the filtered sequences for downstream processing was assessed by computing segmental strain curves. A significant reduction in the discrepancy between strain profiles computed from cluttered and clutter-free segments was observed after filtering the cluttered sequences with the proposed network. The trained network processes a TTE sequence in a fraction of a second, enabling real-time clutter filtering and potentially improving the precision of clinically relevant indices derived from TTE sequences. The source code of the proposed method and example video files of the filtering results are available at: https://github.com/MahdiTabassian/Deep-ClutterFiltering/tree/main.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tyche: Stochastic In-Context Learning for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2401.13650</link>
<guid>https://arxiv.org/abs/2401.13650</guid>
<content:encoded><![CDATA[
arXiv:2401.13650v2 Announce Type: replace-cross 
Abstract: Existing learning-based solutions to medical image segmentation have two important shortcomings. First, for most new segmentation task, a new model has to be trained or fine-tuned. This requires extensive resources and machine learning expertise, and is therefore often infeasible for medical researchers and clinicians. Second, most existing segmentation methods produce a single deterministic segmentation mask for a given image. In practice however, there is often considerable uncertainty about what constitutes the correct segmentation, and different expert annotators will often segment the same image differently. We tackle both of these problems with Tyche, a model that uses a context set to generate stochastic predictions for previously unseen tasks without the need to retrain. Tyche differs from other in-context segmentation methods in two important ways. (1) We introduce a novel convolution block architecture that enables interactions among predictions. (2) We introduce in-context test-time augmentation, a new mechanism to provide prediction stochasticity. When combined with appropriate model design and loss functions, Tyche can predict a set of plausible diverse segmentation candidates for new or unseen medical images and segmentation tasks without the need to retrain.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-supervised Learning-based Reconstruction of High-resolution 4D Light Fields</title>
<link>https://arxiv.org/abs/2402.19020</link>
<guid>https://arxiv.org/abs/2402.19020</guid>
<content:encoded><![CDATA[
arXiv:2402.19020v2 Announce Type: replace-cross 
Abstract: Hand-held light field (LF) cameras often exhibit low spatial resolution due to the inherent trade-off between spatial and angular dimensions. Existing supervised learning-based LF spatial super-resolution (SR) methods, which rely on pre-defined image degradation models, struggle to overcome the domain gap between the training phase -- where LFs with natural resolution are used as ground truth -- and the inference phase, which aims to reconstruct higher-resolution LFs, especially when applied to real-world data.To address this challenge, this paper introduces a novel self-supervised learning-based method for LF spatial SR, which can produce higher spatial resolution LF images than originally captured ones without pre-defined image degradation models. The self-supervised method incorporates a hybrid LF imaging prototype, a real-world hybrid LF dataset, and a self-supervised LF spatial SR framework. The prototype makes reference image pairs between low-resolution central-view sub-aperture images and high-resolution (HR) images. The self-supervised framework consists of a well-designed LF spatial SR network with hybrid input, a central-view synthesis network with an HR-aware loss that enables side-view sub-aperture images to learn high-frequency information from the only HR central view reference image, and a backward degradation network with an epipolar-plane image gradient loss to preserve LF parallax structures. Extensive experiments on both simulated and real-world datasets demonstrate the significant superiority of our approach over state-of-the-art ones in reconstructing higher spatial resolution LF images without pre-defined degradation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node Segmentation</title>
<link>https://arxiv.org/abs/2403.17770</link>
<guid>https://arxiv.org/abs/2403.17770</guid>
<content:encoded><![CDATA[
arXiv:2403.17770v2 Announce Type: replace-cross 
Abstract: Despite the significant success achieved by deep learning methods in medical image segmentation, researchers still struggle in the computer-aided diagnosis of abdominal lymph nodes due to the complex abdominal environment, small and indistinguishable lesions, and limited annotated data. To address these problems, we present a pipeline that integrates the conditional diffusion model for lymph node generation and the nnU-Net model for lymph node segmentation to improve the segmentation performance of abdominal lymph nodes through synthesizing a diversity of realistic abdominal lymph node data. We propose LN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph node (LN) generation. LN-DDPM utilizes lymph node masks and anatomical structure masks as model conditions. These conditions work in two conditioning mechanisms: global structure conditioning and local detail conditioning, to distinguish between lymph nodes and their surroundings and better capture lymph node characteristics. The obtained paired abdominal lymph node images and masks are used for the downstream segmentation task. Experimental results on the abdominal lymph node datasets demonstrate that LN-DDPM outperforms other generative methods in the abdominal lymph node image synthesis and better assists the downstream abdominal lymph node segmentation task.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T1-PILOT: Optimized Trajectories for T1 Mapping Acceleration</title>
<link>https://arxiv.org/abs/2502.20333</link>
<guid>https://arxiv.org/abs/2502.20333</guid>
<content:encoded><![CDATA[
arXiv:2502.20333v2 Announce Type: replace-cross 
Abstract: Cardiac T1 mapping provides critical quantitative insights into myocardial tissue composition, enabling the assessment of pathologies such as fibrosis, inflammation, and edema. However, the inherently dynamic nature of the heart imposes strict limits on acquisition times, making high-resolution T1 mapping a persistent challenge. Compressed sensing (CS) approaches have reduced scan durations by undersampling k-space and reconstructing images from partial data, and recent studies show that jointly optimizing the undersampling patterns with the reconstruction network can substantially improve performance. Still, most current T1 mapping pipelines rely on static, hand-crafted masks that do not exploit the full acceleration and accuracy potential. In this work, we introduce T1-PILOT: an end-to-end method that explicitly incorporates the T1 signal relaxation model into the sampling-reconstruction framework to guide the learning of non-Cartesian trajectories, crossframe alignment, and T1 decay estimation. Through extensive experiments on the CMRxRecon dataset, T1-PILOT significantly outperforms several baseline strategies (including learned single-mask and fixed radial or golden-angle sampling schemes), achieving higher T1 map fidelity at greater acceleration factors. In particular, we observe consistent gains in PSNR and VIF relative to existing methods, along with marked improvements in delineating finer myocardial structures. Our results highlight that optimizing sampling trajectories in tandem with the physical relaxation model leads to both enhanced quantitative accuracy and reduced acquisition times.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echo-E$^3$Net: Efficient Endocardial Spatio-Temporal Network for Ejection Fraction Estimation</title>
<link>https://arxiv.org/abs/2503.17543</link>
<guid>https://arxiv.org/abs/2503.17543</guid>
<content:encoded><![CDATA[
arXiv:2503.17543v2 Announce Type: replace-cross 
Abstract: Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and is routinely used to diagnose heart failure and guide treatment decisions. Although deep learning has advanced automated LVEF estimation, many existing approaches are computationally demanding and underutilize the joint structure of spatial and temporal information in echocardiography videos, limiting their suitability for real-time clinical deployment. We propose Echo-E$^3$Net, an efficient endocardial spatio-temporal network specifically designed for LVEF estimation from echocardiography videos. Echo-E$^3$Net comprises two complementary modules: (1) a dual-phase Endocardial Border Detector (E$^2$CBD), which uses phase-specific cross-attention to predict ED/ES endocardial landmarks (EBs) and learn phase-aware landmark embeddings (LEs), and (2) an Endocardial Feature Aggregator (E$^2$FA), which fuses these embeddings with global statistical descriptors (mean, maximum, variance) of deep feature maps to refine EF regression. A multi-component loss function, inspired by Simpson's biplane method, jointly supervises EF, volumes, and landmark geometry, thereby aligning optimization with the clinical definition of LVEF and promoting robust spatio-temporal representation learning. Evaluated on the EchoNet-Dynamic dataset, Echo-E$^3$Net achieves an RMSE of 5.20 and an $R^2$ score of 0.82, while using only 1.54M parameters and 8.05 GFLOPs. The model operates without external pre-training, heavy data augmentation, or test-time ensembling, making it highly suitable for real-time point-of-care ultrasound (POCUS) applications. Code is available at https://github.com/UltrAi-lab/Echo-E3Net.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAVERIX: Multimodal Audio-Visual Evaluation and Recognition IndeX</title>
<link>https://arxiv.org/abs/2503.21699</link>
<guid>https://arxiv.org/abs/2503.21699</guid>
<content:encoded><![CDATA[
arXiv:2503.21699v2 Announce Type: replace-cross 
Abstract: We introduce MAVERIX (Multimodal audiovisual Evaluation and Recognition IndeX), a unified benchmark to probe the video understanding in multimodal LLMs, encompassing video, audio, text inputs with human performance baselines. Although recent advancements in models with vision and audio understanding capabilities have shown substantial progress, the field lacks a standardized evaluation framework to thoroughly assess their cross-modality comprehension performance. MAVERIX curates 2,556 questions from 700 videos, in the form of both multiple-choice and open-ended formats, explicitly designed to evaluate multimodal models through questions that necessitate tight integration of video and audio information, spanning a broad spectrum of agentic scenarios. MAVERIX uniquely provides models with audiovisual questions, closely mimicking the multimodal perceptual experiences available to humans during inference and decision-making processes. To our knowledge, MAVERIX is the first benchmark aimed explicitly at assessing comprehensive audiovisual integration in such granularity. Experiments with state-of-the-art models, including Qwen 2.5 Omni and Gemini 2.5 Flash-Lite, show performance around 64% accuracy, while human experts reach near-ceiling performance of 92.8%, exposing a substantial gap to human-level comprehension. With standardized evaluation protocols, a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a challenging testbed for advancing audiovisual multimodal intelligence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPLE: Encoding Dexterous Robotic Manipulation Priors Learned From Egocentric Videos</title>
<link>https://arxiv.org/abs/2504.06084</link>
<guid>https://arxiv.org/abs/2504.06084</guid>
<content:encoded><![CDATA[
arXiv:2504.06084v2 Announce Type: replace-cross 
Abstract: Large-scale egocentric video datasets capture diverse human activities across a wide range of scenarios, offering rich and detailed insights into how humans interact with objects, especially those that require fine-grained dexterous control. Such complex, dexterous skills with precise controls are crucial for many robotic manipulation tasks, yet are often insufficiently addressed by traditional data-driven approaches to robotic manipulation. To address this gap, we leverage manipulation priors learned from large-scale egocentric video datasets to improve policy learning for dexterous robotic manipulation tasks. We present MAPLE, a novel method for dexterous robotic manipulation that learns features to predict object contact points and detailed hand poses at the moment of contact from egocentric images. We then use the learned features to train policies for downstream manipulation tasks. Experimental results demonstrate the effectiveness of MAPLE across 4 existing simulation benchmarks, as well as a newly designed set of 4 challenging simulation tasks requiring fine-grained object control and complex dexterous skills. The benefits of MAPLE are further highlighted in real-world experiments using a 17 DoF dexterous robotic hand, whereas the simultaneous evaluation across both simulation and real-world experiments has remained underexplored in prior work. We additionally showcase the efficacy of our model on an egocentric contact point prediction task, validating its usefulness beyond dexterous manipulation policy learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust brain age estimation from structural MRI with contrastive learning</title>
<link>https://arxiv.org/abs/2507.01794</link>
<guid>https://arxiv.org/abs/2507.01794</guid>
<content:encoded><![CDATA[
arXiv:2507.01794v2 Announce Type: replace-cross 
Abstract: Estimating brain age from structural MRI has emerged as a powerful tool for characterizing normative and pathological aging. In this work, we explore contrastive learning as a scalable and robust alternative to L1-supervised approaches for brain age estimation. We introduce a novel contrastive loss function, $\mathcal{L}^{exp}$, and evaluate it across multiple public neuroimaging datasets comprising over 20,000 scans. Our experiments reveal four key findings. First, scaling pre-training on diverse, multi-site data consistently improves generalization performance, cutting external mean absolute error (MAE) nearly in half. Second, $\mathcal{L}^{exp}$ is robust to site-related confounds, maintaining low scanner-predictability as training size increases. Third, contrastive models reliably capture accelerated aging in patients with cognitive impairment and Alzheimer's disease, as shown through brain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike L1-supervised baselines, $\mathcal{L}^{exp}$ maintains a strong correlation between brain age accuracy and downstream diagnostic performance, supporting its potential as a foundation model for neuroimaging. These results position contrastive learning as a promising direction for building generalizable and clinically meaningful brain representations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning</title>
<link>https://arxiv.org/abs/2507.16302</link>
<guid>https://arxiv.org/abs/2507.16302</guid>
<content:encoded><![CDATA[
arXiv:2507.16302v2 Announce Type: replace-cross 
Abstract: Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are found to be fragile to downstream fine-tuning, as we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety, while effectively preserving benign generation capability. Our code and pretrained models are publicly available at https://github.com/AntigoneRandy/ResAlign.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DREAM: Scalable Red Teaming for Text-to-Image Generative Systems via Distribution Modeling</title>
<link>https://arxiv.org/abs/2507.16329</link>
<guid>https://arxiv.org/abs/2507.16329</guid>
<content:encoded><![CDATA[
arXiv:2507.16329v2 Announce Type: replace-cross 
Abstract: Despite the integration of safety alignment and external filters, text-to-image (T2I) generative systems are still susceptible to producing harmful content, such as sexual or violent imagery. This raises serious concerns about unintended exposure and potential misuse. Red teaming, which aims to proactively identify diverse prompts that can elicit unsafe outputs from the T2I system, is increasingly recognized as an essential method for assessing and improving safety before real-world deployment. However, existing automated red teaming approaches often treat prompt discovery as an isolated, prompt-level optimization task, which limits their scalability, diversity, and overall effectiveness. To bridge this gap, in this paper, we propose DREAM, a scalable red teaming framework to automatically uncover diverse problematic prompts from a given T2I system. Unlike prior work that optimizes prompts individually, DREAM directly models the probabilistic distribution of the target system's problematic prompts, which enables explicit optimization over both effectiveness and diversity, and allows efficient large-scale sampling after training. To achieve this without direct access to representative training samples, we draw inspiration from energy-based models and reformulate the objective into a simple and tractable form. We further introduce GC-SPSA, an efficient optimization algorithm that provides stable gradient estimates through the long and potentially non-differentiable T2I pipeline. During inference, we also propose a diversity-aware sampling strategy to enhance prompt variety. The effectiveness of DREAM is validated through extensive experiments, demonstrating state-of-the-art performance across a wide range of T2I models and safety filters in terms of both prompt success rate and diversity. Our code is available at https://github.com/AntigoneRandy/DREAM
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists</title>
<link>https://arxiv.org/abs/2508.13157</link>
<guid>https://arxiv.org/abs/2508.13157</guid>
<content:encoded><![CDATA[
arXiv:2508.13157v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) exhibits great potential in designing of analog integrated circuits (IC) because of its excellence in abstraction and generalization for knowledge. However, further development of LLM-based analog ICs heavily relies on textual description of analog ICs, while existing analog ICs are mostly illustrated in image-based circuit diagrams rather than text-based netlists. Converting circuit diagrams to netlists help LLMs to enrich the knowledge of analog IC. Nevertheless, previously proposed conversion frameworks face challenges in further application because of limited support of image styles and circuit elements. Up to now, it still remains a challenging task to effectively convert complex circuit diagrams into netlists. To this end, this paper constructs and opensources a new dataset with rich styles of circuit diagrams as well as balanced distribution of simple and complex analog ICs. And a hybrid framework, named Image2Net, is proposed for practical conversion from circuit diagrams to netlists. The netlist edit distance (NED) is also introduced to precisely assess the difference between the converted netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77% successful rate, which is 34.62%-45.19% higher than previous works. Specifically, the proposed work shows 0.116 averaged NED, which is 62.1%-69.6% lower than state-of-the-arts. Our datasets and benchmark are available at https://github.com/LAD021/ci2n_datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices</title>
<link>https://arxiv.org/abs/2509.01839</link>
<guid>https://arxiv.org/abs/2509.01839</guid>
<content:encoded><![CDATA[
arXiv:2509.01839v5 Announce Type: replace-cross 
Abstract: Currently, prominent Transformer architectures applied on graphs and meshes for shape analysis tasks employ traditional attention layers that heavily utilize spectral features requiring costly eigenvalue decomposition-based methods. To encode the mesh structure, these methods derive positional embeddings, that heavily rely on eigenvalue decomposition based operations, e.g. on the Laplacian matrix, or on heat-kernel signatures, which are then concatenated to the input features. This paper proposes a novel approach inspired by the explicit construction of the Hodge Laplacian operator in Discrete Exterior Calculus as a product of discrete Hodge operators and exterior derivatives, i.e. $(L := \star_0^{-1} d_0^T \star_1 d_0)$. We adjust the Transformer architecture in a novel deep learning layer that utilizes the multi-head attention mechanism to approximate Hodge matrices $\star_0$, $\star_1$ and $\star_2$ and learn families of discrete operators $L$ that act on mesh vertices, edges and faces. Our approach results in a computationally-efficient architecture that achieves comparable performance in mesh segmentation and classification tasks, through a direct learning framework, while eliminating the need for costly eigenvalue decomposition operations or complex preprocessing operations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
arXiv:2509.22601v4 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent's own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL, where a replay buffer stores good experience for off-policy update, by gradually steering the policy entropy across stages. Specifically, the proposed curriculum scheduling harmonizes intrinsic reward shaping and self-imitation to 1) expedite exploration via frequent tool interactions at the beginning, and 2) strengthen exploitation of successful tactics upon convergence towards familiarity with the environment. We also combine bag-of-tricks of industrial RL optimizations for a strong baseline Dr.BoT to demonstrate our effectiveness. In ALFWorld and WebShop, SPEAR increases the success rates of GRPO/GiGPO/Dr.BoT by up to 16.1%/5.1%/8.6% and 20.7%/11.8%/13.9%, respectively. In AIME24 and AIME25, SPEAR boosts Dr.BoT by up to 3.8% and 6.1%, respectively. Such gains incur only 10%-25% extra theoretical complexity and negligible runtime overhead in practice, demonstrating the plug-and-play scalability of SPEAR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saliency Guided Longitudinal Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2509.25374</link>
<guid>https://arxiv.org/abs/2509.25374</guid>
<content:encoded><![CDATA[
arXiv:2509.25374v2 Announce Type: replace-cross 
Abstract: Longitudinal medical visual question answering (Diff-VQA) requires comparing paired studies from different time points and answering questions about clinically meaningful changes. In this setting, the difference signal and the consistency of visual focus across time are more informative than absolute single-image findings. We propose a saliency-guided encoder-decoder for chest X-ray Diff-VQA that turns post-hoc saliency into actionable supervision. The model first performs a lightweight near-identity affine pre-alignment to reduce nuisance motion between visits. It then executes a within-epoch two-step loop: step 1 extracts a medically relevant keyword from the answer and generates keyword-conditioned Grad-CAM on both images to obtain disease-focused saliency; step 2 applies the shared saliency mask to both time points and generates the final answer. This closes the language-vision loop so that the terms that matter also guide where the model looks, enforcing spatially consistent attention on corresponding anatomy. On Medical-Diff-VQA, the approach attains competitive performance on BLEU, ROUGE-L, CIDEr, and METEOR while providing intrinsic interpretability. Notably, the backbone and decoder are general-domain pretrained without radiology-specific pretraining, highlighting practicality and transferability. These results support saliency-conditioned generation with mild pre-alignment as a principled framework for longitudinal reasoning in medical VQA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bionetta: Efficient Client-Side Zero-Knowledge Machine Learning Proving</title>
<link>https://arxiv.org/abs/2510.06784</link>
<guid>https://arxiv.org/abs/2510.06784</guid>
<content:encoded><![CDATA[
arXiv:2510.06784v2 Announce Type: replace-cross 
Abstract: In this report, we compare the performance of our UltraGroth-based zero-knowledge machine learning framework Bionetta to other tools of similar purpose such as EZKL, Lagrange's deep-prove, or zkml. The results show a significant boost in the proving time for custom-crafted neural networks: they can be proven even on mobile devices, enabling numerous client-side proving applications. While our scheme increases the cost of one-time preprocessing steps, such as circuit compilation and generating trusted setup, our approach is, to the best of our knowledge, the only one that is deployable on the native EVM smart contracts without overwhelming proof size and verification overheads.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI Synthesis</title>
<link>https://arxiv.org/abs/2510.09365</link>
<guid>https://arxiv.org/abs/2510.09365</guid>
<content:encoded><![CDATA[
arXiv:2510.09365v2 Announce Type: replace-cross 
Abstract: Magnetic resonance imaging (MRI) inpainting supports numerous clinical and research applications. We introduce the first generative model that conditions on voxel-level, continuous tumor concentrations to synthesize high-fidelity brain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this architecture to the complementary task of healthy tissue restoration by setting the tumor concentrations to zero. Our latent diffusion model conditioned on both tissue segmentations and the tumor concentrations generates 3D spatially coherent and anatomically consistent images for both tumor synthesis and healthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5, and for tumor inpainting, we achieve 17.4. Our code is available at: https://github.com/valentin-biller/ldm.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dejavu: Towards Experience Feedback Learning for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2510.10181</link>
<guid>https://arxiv.org/abs/2510.10181</guid>
<content:encoded><![CDATA[
arXiv:2510.10181v2 Announce Type: replace-cross 
Abstract: Embodied agents face a fundamental limitation: once deployed in real-world environments to perform specific tasks, they are unable to acquire additional knowledge to enhance task performance. In this paper, we propose a general post-deployment learning framework Dejavu, which employs an Experience Feedback Network (EFN) and augments the frozen Vision-Language-Action (VLA) policy with retrieved execution memories. EFN identifies contextually prior action experiences and conditions action prediction on this retrieved guidance. We adopt reinforcement learning with semantic similarity rewards to train EFN, ensuring that the predicted actions align with past behaviors under current observations. During deployment, EFN continually enriches its memory with new trajectories, enabling the agent to exhibit "learning from experience". Experiments across diverse embodied tasks show that EFN improves adaptability, robustness, and success rates over frozen baselines. We provide code and demo in our supplementary material.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualHash: A Stochastic Primal-Dual Algorithm with Theoretical Guarantee for Deep Hashing</title>
<link>https://arxiv.org/abs/2510.18218</link>
<guid>https://arxiv.org/abs/2510.18218</guid>
<content:encoded><![CDATA[
arXiv:2510.18218v2 Announce Type: replace-cross 
Abstract: Deep hashing converts high-dimensional feature vectors into compact binary codes, enabling efficient large-scale retrieval. A fundamental challenge in deep hashing stems from the discrete nature of quantization in generating the codes. W-type regularizations, such as $||z|-1|$, have been proven effective as they encourage variables toward binary values. However, existing methods often directly optimize these regularizations without convergence guarantees. While proximal gradient methods offer a promising solution, the coupling between W-type regularizers and neural network outputs results in composite forms that generally lack closed-form proximal solutions. In this paper, we present a stochastic primal-dual hashing algorithm, referred to as DualHash, that provides rigorous complexity bounds. Using Fenchel duality, we partially transform the nonconvex W-type regularization optimization into the dual space, which results in a proximal operator that admits closed-form solutions. We derive two algorithm instances: a momentum-accelerated version with $\mathcal{O}(\varepsilon^{-4})$ complexity and an improved $\mathcal{O}(\varepsilon^{-3})$ version using variance reduction. Experiments on three image retrieval databases demonstrate the superior performance of DualHash.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game</title>
<link>https://arxiv.org/abs/2511.17925</link>
<guid>https://arxiv.org/abs/2511.17925</guid>
<content:encoded><![CDATA[
arXiv:2511.17925v2 Announce Type: replace-cross 
Abstract: Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion</title>
<link>https://arxiv.org/abs/2511.22505</link>
<guid>https://arxiv.org/abs/2511.22505</guid>
<content:encoded><![CDATA[
arXiv:2511.22505v2 Announce Type: replace-cross 
Abstract: Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AREA3D: Active Reconstruction Agent with Unified Feed-Forward 3D Perception and Vision-Language Guidance</title>
<link>https://arxiv.org/abs/2512.05131</link>
<guid>https://arxiv.org/abs/2512.05131</guid>
<content:encoded><![CDATA[
<div> Active 3D reconstruction, view selection, uncertainty modeling, vision-language guidance, sparse-view reconstruction

<br /><br />Summary:  
1. The paper presents AREA3D, an active 3D reconstruction agent designed to autonomously select viewpoints to efficiently capture accurate and complete 3D scene geometry.  
2. Unlike existing methods that depend on hand-crafted geometric heuristics, AREA3D leverages feed-forward 3D reconstruction models combined with vision-language guidance to improve viewpoint selection.  
3. The framework decouples view-uncertainty modeling from the feed-forward reconstructor, allowing precise uncertainty estimation without the need for costly online optimization procedures.  
4. Integrating a vision-language model provides high-level semantic cues, encouraging the agent to choose informative and diverse viewpoints that go beyond purely geometric considerations.  
5. Extensive experiments conducted on both scene-level and object-level benchmarks demonstrate that AREA3D achieves state-of-the-art reconstruction accuracy, especially when limited to sparse-view input scenarios.  
6. The proposed approach reduces redundant observations and enhances reconstruction quality effectively, particularly in settings where viewpoint data is limited.  
7. The authors plan to make their code publicly available, facilitating further research and development in active 3D reconstruction. <div>
arXiv:2512.05131v1 Announce Type: new 
Abstract: Active 3D reconstruction enables an agent to autonomously select viewpoints to efficiently obtain accurate and complete scene geometry, rather than passively reconstructing scenes from pre-collected images. However, existing active reconstruction methods often rely on hand-crafted geometric heuristics, which can lead to redundant observations without substantially improving reconstruction quality. To address this limitation, we propose AREA3D, an active reconstruction agent that leverages feed-forward 3D reconstruction models and vision-language guidance. Our framework decouples view-uncertainty modeling from the underlying feed-forward reconstructor, enabling precise uncertainty estimation without expensive online optimization. In addition, an integrated vision-language model provides high-level semantic guidance, encouraging informative and diverse viewpoints beyond purely geometric cues. Extensive experiments on both scene-level and object-level benchmarks demonstrate that AREA3D achieves state-of-the-art reconstruction accuracy, particularly in the sparse-view regime. Code will be made available at: https://github.com/TianlingXu/AREA3D .
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training</title>
<link>https://arxiv.org/abs/2512.05132</link>
<guid>https://arxiv.org/abs/2512.05132</guid>
<content:encoded><![CDATA[
<div> Zero-Shot Super-Resolution, Spatiotemporal Forecasting, Scale Anchoring, Frequency Representation Learning, Nyquist Frequency  

<br /><br />Summary:  
This paper addresses the challenge of zero-shot super-resolution spatiotemporal forecasting, where models trained on low-resolution data are expected to perform inference on higher-resolution data. Existing methods evaluate success by maintaining similar error levels across resolutions, but the authors argue that true generalization should reduce errors as resolution increases, reflecting physical laws more accurately. They identify a fundamental problem termed "Scale Anchoring," caused by the Nyquist frequency limit of low-resolution data that restricts the representation of higher-frequency signals during inference at higher resolutions. This causes error rates to be anchored at the low-resolution level and misinterpreted as successful generalization. To overcome this, the authors propose Frequency Representation Learning (FRL), an architecture-agnostic framework that aligns frequency representations with resolution and employs spectral consistency training. FRL leads to more stable frequency responses in high-frequency bands on grids with higher Nyquist frequencies. Empirically, FRL-enhanced models demonstrate decreasing error with increasing resolution, significantly outperforming baseline methods within the studied task and resolution ranges. This improvement comes at only a modest computational cost, making FRL a promising solution for enhanced multi-resolution spatiotemporal forecasting. <div>
arXiv:2512.05132v1 Announce Type: new 
Abstract: Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider maintaining similar error across different resolutions as indicative of successful multi-resolution generalization. However, deep learning models serving as alternatives to numerical solvers should reduce error as resolution increases. The fundamental limitation is, the upper bound of physical law frequencies that low-resolution data can represent is constrained by its Nyquist frequency, making it difficult for models to process signals containing unseen frequency components during high-resolution inference. This results in errors being anchored at low resolution, incorrectly interpreted as successful generalization. We define this fundamental phenomenon as a new problem distinct from existing issues: Scale Anchoring. Therefore, we propose architecture-agnostic Frequency Representation Learning. It alleviates Scale Anchoring through resolution-aligned frequency representations and spectral consistency training: on grids with higher Nyquist frequencies, the frequency response in high-frequency bands of FRL-enhanced variants is more stable. This allows errors to decrease with resolution and significantly outperform baselines within our task and resolution range, while incurring only modest computational overhead.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05134</link>
<guid>https://arxiv.org/abs/2512.05134</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, acceleration, feature invariance, caching, deterministic sampling  

<br /><br />Summary:  
1. Diffusion models generate high-quality synthetic data but suffer from slow inference due to iterative sampling processes.  
2. The paper identifies feature invariance present in deterministic sampling that can be exploited to accelerate inference.  
3. The authors propose InvarDiff, a training-free acceleration technique leveraging relative temporal invariance across both timestep and layer scales.  
4. InvarDiff analyzes outputs from a few deterministic runs to create a binary cache plan matrix, indicating which modules at specific timesteps or layers can reuse cached results instead of recomputing.  
5. A re-sampling correction is introduced to prevent quality degradation (drift) when consecutive caches are utilized.  
6. The caching strategy operates at two levels: cross-timestep caching, where entire steps can reuse previous computations, and layer-wise caching within steps.  
7. The approach is applied to diffusion models like DiT and FLUX, demonstrating a 2 to 3 times speed-up in end-to-end inference without requiring retraining.  
8. Experimental results show minimal impact on standard quality metrics and almost no visual quality loss compared to full computation.  
9. InvarDiff thus effectively reduces redundant computations while maintaining synthesis fidelity, offering a practical method for faster diffusion model inference. <div>
arXiv:2512.05134v1 Announce Type: new 
Abstract: Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes</title>
<link>https://arxiv.org/abs/2512.05136</link>
<guid>https://arxiv.org/abs/2512.05136</guid>
<content:encoded><![CDATA[
<div> Keywords: Coronary artery disease, AI-ECG, stenosis prediction, coronary CT angiography, interpretability

<br /><br />Summary:  
Coronary artery disease (CAD) is a significant global health challenge, requiring accurate identification of culprit vessels and stenosis severity for personalized treatment. While coronary CT angiography (CCTA) is the preferred non-invasive diagnostic tool, its reliance on advanced equipment, radiation exposure, and patient cooperation limits widespread application. This study developed an interpretable artificial intelligence model using electrocardiography (AI-ECG) to predict severe or complete stenosis in the four major coronary arteries (RCA, LM, LAD, LCX) based on CCTA findings. The model achieved strong predictive performance, with internal validation AUCs of 0.794 (RCA), 0.818 (LM), 0.744 (LAD), and 0.755 (LCX), and external validation AUCs of 0.749, 0.971, 0.667, and 0.727 respectively. The AI-ECG model showed robustness even in patients with clinically normal ECGs, indicating its ability to detect subtler ischemic changes. Stability across different demographic groups and ECG acquisition times was confirmed through subgroup analyses. Risk stratification using vessel-specific incidence thresholds provided clear separations in calibration and event outcome curves. Interpretability analyses identified distinct ECG waveform differences between high- and low-risk groups, pinpointing key electrophysiological regions influencing model decisions and advancing understanding of ECG correlates of coronary stenosis. <div>
arXiv:2512.05136v1 Announce Type: new 
Abstract: Coronary artery disease (CAD) remains a major global health burden. Accurate identification of the culprit vessel and assessment of stenosis severity are essential for guiding individualized therapy. Although coronary CT angiography (CCTA) is the first-line non-invasive modality for CAD diagnosis, its dependence on high-end equipment, radiation exposure, and strict patient cooperation limits large-scale use. With advances in artificial intelligence (AI) and the widespread availability of electrocardiography (ECG), AI-ECG offers a promising alternative for CAD screening. In this study, we developed an interpretable AI-ECG model to predict severe or complete stenosis of the four major coronary arteries on CCTA. On the internal validation set, the model's AUCs for the right coronary artery (RCA), left main coronary artery (LM), left anterior descending artery (LAD), and left circumflex artery (LCX) were 0.794, 0.818, 0.744, and 0.755, respectively; on the external validation set, the AUCs reached 0.749, 0.971, 0.667, and 0.727, respectively. Performance remained stable in a clinically normal-ECG subset, indicating robustness beyond overt ECG abnormalities. Subgroup analyses across demographic and acquisition-time strata further confirmed model stability. Risk stratification based on vessel-specific incidence thresholds showed consistent separation on calibration and cumulative event curves. Interpretability analyses revealed distinct waveform differences between high- and low-risk groups, highlighting key electrophysiological regions contributing to model decisions and offering new insights into the ECG correlates of coronary stenosis.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images</title>
<link>https://arxiv.org/abs/2512.05137</link>
<guid>https://arxiv.org/abs/2512.05137</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, ChromouVQA, figure-ground segregation, chromatic camouflage, multimodal understanding<br /><br />Summary:  
1. This paper addresses a persistent challenge in Vision-Language Models (VLMs), specifically their difficulty in recognizing targets embedded within cluttered, camouflaged backgrounds that require figure-ground segregation.  
2. The authors introduce ChromouVQA, a novel large-scale, multi-task benchmark designed using Ishihara-style chromatic camouflage images to evaluate and improve multimodal vision-question-answering.  
3. ChromouVQA extends traditional Ishihara dot plates by incorporating multiple fill geometries and varying parameters such as chromatic separation, density, size, occlusion, and rotation while recording comprehensive metadata for reproducibility.  
4. The benchmark encompasses nine different vision-question-answering tasks including recognition, counting, comparison, and spatial reasoning, providing a diverse and challenging evaluation platform.  
5. Evaluations of both human subjects and state-of-the-art VLMs revealed significant performance gaps, particularly under conditions with subtle chromatic contrast or disruptive geometric fills, highlighting current model limitations.  
6. To improve shape recovery and figure-ground segregation, the authors propose a model-agnostic contrastive learning approach that aligns object silhouettes with their camouflaged renderings, enhancing global shape recognition.  
7. ChromouVQA serves as a compact, controlled, and reproducible benchmark that facilitates consistent evaluation, benchmarking, and future research extensions, with code and dataset publicly available on GitHub. <div>
arXiv:2512.05137v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have advanced multimodal understanding, yet still struggle when targets are embedded in cluttered backgrounds requiring figure-ground segregation. To address this, we introduce ChromouVQA, a large-scale, multi-task benchmark based on Ishihara-style chromatic camouflaged images. We extend classic dot plates with multiple fill geometries and vary chromatic separation, density, size, occlusion, and rotation, recording full metadata for reproducibility. The benchmark covers nine vision-question-answering tasks, including recognition, counting, comparison, and spatial reasoning. Evaluations of humans and VLMs reveal large gaps, especially under subtle chromatic contrast or disruptive geometric fills. We also propose a model-agnostic contrastive recipe aligning silhouettes with their camouflaged renderings, improving recovery of global shapes. ChromouVQA provides a compact, controlled benchmark for reproducible evaluation and extension. Code and dataset are available at https://github.com/Chromou-VQA-Benchmark/Chromou-VQA.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models</title>
<link>https://arxiv.org/abs/2512.05139</link>
<guid>https://arxiv.org/abs/2512.05139</guid>
<content:encoded><![CDATA[
<div> Keywords: transfer learning, generative downscaling, satellite imagery, diffusion models, spatiotemporal representation<br /><br />Summary:<br /><br />1. The article introduces a transfer-learning generative framework aimed at reconstructing high-resolution satellite images from coarse resolution inputs by combining a lightweight pretrained U-Net encoder with a diffusion-based generative model.<br /><br />2. The U-Net encoder is initially pretrained on extensive coarse resolution data to capture spatiotemporal features, then frozen and integrated into a larger downscaling diffusion model to serve as physically meaningful latent features.<br /><br />3. The approach uses NASA's MERRA-2 reanalysis data at 50 km resolution as the low-resolution source and the GEOS-5 Nature Run (G5NR) dataset at 7 km resolution as the high-resolution target.<br /><br />4. The study focuses on a large Asian region, divided into two subregions and four seasons to manage computational complexity.<br /><br />5. Domain similarity between the datasets was validated through Wasserstein distances, confirming minimal distributional shift and justifying frozen parameter transfer.<br /><br />6. The proposed model delivered strong performance across seasons and regions (R² between 0.65 and 0.94), outperforming deterministic U-Nets, variational autoencoders, and existing transfer learning baselines.<br /><br />7. Out-of-sample evaluations using semivariograms, autocorrelation functions, and lag-based error metrics demonstrated the spatial and temporal consistency of the downscaled images, supporting stable autoregressive forecasting beyond the original high-resolution record.<br /><br />8. This method offers a robust, physically coherent solution for downscaling long time-series satellite images with limited training data, with important applications in environmental exposure assessment and long-term environmental monitoring. <div>
arXiv:2512.05139v1 Announce Type: new 
Abstract: We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowEO: Generative Unsupervised Domain Adaptation for Earth Observation</title>
<link>https://arxiv.org/abs/2512.05140</link>
<guid>https://arxiv.org/abs/2512.05140</guid>
<content:encoded><![CDATA[
<div> Keywords: Earth observation, unsupervised domain adaptation, flow matching, remote sensing, image translation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of distribution shifts in Earth observation data, which arise due to heterogeneous sources including different sensors, regions, acquisition times, and atmospheric conditions, limiting the generalization of pretrained remote sensing models. <br />2. To overcome these challenges, the authors propose FlowEO, a novel framework that applies generative models and flow matching techniques to achieve image-space unsupervised domain adaptation (UDA) for Earth observation images.<br />3. FlowEO learns a semantically preserving mapping that transports images from the source domain distribution to the target domain, enabling effective alignment despite large domain shifts.<br />4. The framework is evaluated on four diverse datasets, covering complex domain adaptation scenarios such as synthetic aperture radar (SAR) to optical image translation, as well as temporal and semantic shifts caused by natural disasters.<br />5. Experimental results demonstrate that FlowEO outperforms existing image translation methods in domain adaptation tasks related to classification and semantic segmentation, achieving better or comparable perceptual image quality and showcasing the potential of flow-matching-based UDA in remote sensing applications. <div>
arXiv:2512.05140v1 Announce Type: new 
Abstract: The increasing availability of Earth observation data offers unprecedented opportunities for large-scale environmental monitoring and analysis. However, these datasets are inherently heterogeneous, stemming from diverse sensors, geographical regions, acquisition times, and atmospheric conditions. Distribution shifts between training and deployment domains severely limit the generalization of pretrained remote sensing models, making unsupervised domain adaptation (UDA) crucial for real-world applications. We introduce FlowEO, a novel framework that leverages generative models for image-space UDA in Earth observation. We leverage flow matching to learn a semantically preserving mapping that transports from the source to the target image distribution. This allows us to tackle challenging domain adaptation configurations for classification and semantic segmentation of Earth observation images. We conduct extensive experiments across four datasets covering adaptation scenarios such as SAR to optical translation and temporal and semantic shifts caused by natural disasters. Experimental results demonstrate that FlowEO outperforms existing image translation approaches for domain adaptation while achieving on-par or better perceptual image quality, highlighting the potential of flow-matching-based UDA for remote sensing.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Improving VLM Judges Without Human Annotations</title>
<link>https://arxiv.org/abs/2512.05145</link>
<guid>https://arxiv.org/abs/2512.05145</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, self-training, judge model, multimodal instruction-response, VL-RewardBench<br /><br />Summary:  
This paper addresses the challenge of developing effective judges for Vision-Language Models (VLMs), which are essential for evaluating and improving these models. Traditional approaches rely heavily on large-scale human preference annotations, which are costly and quickly outdated due to rapid model advancements. To overcome this, the authors propose a novel framework to self-train a VLM judge model without using any human preference data, instead leveraging only self-synthesized multimodal data. The method is iterative and includes three main stages: generating diverse multimodal instruction-response pairs at various quality levels, producing reasoning traces and judgments to filter out inconsistent data based on expected quality, and training the judge model on correct answers paired with their reasoning traces. Evaluation on Multimodal RewardBench and VL-RewardBench benchmarks demonstrates significant improvement in overall accuracy, raising a Llama-3.2-11B multimodal judge model's performance from 0.38 to 0.51. This approach often outperforms larger models like Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general accuracy, hallucination detection, and reasoning ability. The success of this human-annotation-free method indicates promising potential for future self-evolving judge models in tandem with advancing VLM capabilities. <div>
arXiv:2512.05145v1 Announce Type: new 
Abstract: Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows</title>
<link>https://arxiv.org/abs/2512.05150</link>
<guid>https://arxiv.org/abs/2512.05150</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal generative models, inference efficiency, distillation, 1-step generative models, TwinFlow<br /><br />Summary:<br /><br />This paper addresses the challenge of slow inference in large multi-modal generative models that typically rely on multi-step frameworks like diffusion and flow matching, requiring 40-100 Number of Function Evaluations (NFEs). Existing few-step acceleration methods face limitations, including iterative distillation requirements, performance degradation under very few steps, and complexity from adversarial training. The authors propose TwinFlow, a novel framework designed to train 1-step generative models without relying on fixed pretrained teacher models or adversarial networks, thus simplifying training and reducing computational overhead. TwinFlow excels specifically in text-to-image generation, achieving a high GenEval score of 0.83 at 1-NFE, outperforming strong existing methods such as SANA-Sprint (GAN-based) and RCGM (consistency-based). Demonstrating scalability, TwinFlow was fully trained on Qwen-Image-20B and efficiently adapted into a few-step generator. With only 1-NFE, the method matches the quality of original 100-NFE models on benchmarks including GenEval and DPG-Bench, thereby reducing computational cost by a factor of 100 with minimal image quality loss. The approach promises a significant step forward in building large-scale, efficient multi-modal generative models, with code and details available at the project page. <div>
arXiv:2512.05150v1 Announce Type: new 
Abstract: Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\times$ with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EFDiT: Efficient Fine-grained Image Generation Using Diffusion Transformer Models</title>
<link>https://arxiv.org/abs/2512.05152</link>
<guid>https://arxiv.org/abs/2512.05152</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, fine-grained image generation, tiered embedder, super-resolution, ProAttention

<br /><br />Summary: This paper addresses challenges in large-scale fine-grained image generation using diffusion models, which are known for their controllability and diverse image outputs but struggle with semantic entanglement and insufficient image detail. The authors introduce a novel concept called the tiered embedder that incorporates semantic information from both superclasses and child classes, enhancing the model's ability to manage and utilize semantic relationships effectively. To improve the quality and detail of generated fine-grained images, the study integrates super-resolution techniques at the perceptual information generation stage, employing both enhancement and degradation models to highlight finer image features. Additionally, a new ProAttention mechanism is proposed to optimize the diffusion model's performance, offering an efficient attention method suitable for the generation process. The approach was rigorously evaluated on public benchmark datasets and demonstrated superior results compared to existing state-of-the-art fine-tuning methods. This work contributes to advancing fine-grained image synthesis by combining hierarchical semantic embedding, attention improvements, and super-resolution enhancements within diffusion models. <div>
arXiv:2512.05152v1 Announce Type: new 
Abstract: Diffusion models are highly regarded for their controllability and the diversity of images they generate. However, class-conditional generation methods based on diffusion models often focus on more common categories. In large-scale fine-grained image generation, issues of semantic information entanglement and insufficient detail in the generated images still persist. This paper attempts to introduce a concept of a tiered embedder in fine-grained image generation, which integrates semantic information from both super and child classes, allowing the diffusion model to better incorporate semantic information and address the issue of semantic entanglement. To address the issue of insufficient detail in fine-grained images, we introduce the concept of super-resolution during the perceptual information generation stage, enhancing the detailed features of fine-grained images through enhancement and degradation models. Furthermore, we propose an efficient ProAttention mechanism that can be effectively implemented in the diffusion model. We evaluate our method through extensive experiments on public benchmarks, demonstrating that our approach outperforms other state-of-the-art fine-tuning methods in terms of performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05172</link>
<guid>https://arxiv.org/abs/2512.05172</guid>
<content:encoded><![CDATA[
<div> Keywords: Enhanced Semantic Motion Representations, Vision-Language Models, Reinforcement Learning, Dual-path backbone, Text-image alignment<br /><br />Summary:<br /><br />This paper addresses limitations in current Large Language Model (LLM)-based reinforcement learning (RL) methods, particularly the restricted representational capacity of backbone networks used for control policy guidance. To overcome this, the authors propose Semore, a novel framework that leverages Vision-Language Models (VLM) to provide enriched semantic and motion representations from RGB flow inputs through a dual-path backbone architecture. Semore incorporates common-sense knowledge from VLMs to extract critical information directly from the visual observations while employing pre-trained CLIP for effective text-image alignment, which helps ground-truth representations to be embedded in the backbone network. To optimize the fusion of semantic and motion features for decision-making, the framework uses a separate supervision strategy enabling concurrent guidance for both representation types while allowing natural interaction between them. Extensive experimental results demonstrate that Semore significantly improves adaptive and efficient performance in visual RL tasks compared to state-of-the-art approaches. The framework's design enables better interpretability and robustness by integrating semantic understanding with dynamic motion cues. The authors have also released all code to facilitate reproducibility and further research. <div>
arXiv:2512.05172v1 Announce Type: new 
Abstract: The growing exploration of Large Language Models (LLM) and Vision-Language Models (VLM) has opened avenues for enhancing the effectiveness of reinforcement learning (RL). However, existing LLM-based RL methods often focus on the guidance of control policy and encounter the challenge of limited representations of the backbone networks. To tackle this problem, we introduce Enhanced Semantic Motion Representations (Semore), a new VLM-based framework for visual RL, which can simultaneously extract semantic and motion representations through a dual-path backbone from the RGB flows. Semore utilizes VLM with common-sense knowledge to retrieve key information from observations, while using the pre-trained clip to achieve the text-image alignment, thereby embedding the ground-truth representations into the backbone. To efficiently fuse semantic and motion representations for decision-making, our method adopts a separately supervised approach to simultaneously guide the extraction of semantics and motion, while allowing them to interact spontaneously. Extensive experiments demonstrate that, under the guidance of VLM at the feature level, our method exhibits efficient and adaptive ability compared to state-of-art methods. All codes are released.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05198</link>
<guid>https://arxiv.org/abs/2512.05198</guid>
<content:encoded><![CDATA[
<div> Keywords: latent inpainting, Pixel-Equivalent Latent Compositing, DecFormer, diffusion models, mask interpolation  

<br /><br />Summary:  
This paper addresses the limitations of latent inpainting in diffusion models, which commonly rely on linear interpolation of VAE latents under downsampled masks, leading to artifacts and color inconsistencies at mask boundaries. The authors propose a new principle called Pixel-Equivalent Latent Compositing (PELC), which ensures that compositing latents produces the same result as pixel-space alpha compositing, enabling full-resolution mask control and smooth soft-edge blending despite the spatial compression of VAEs. To achieve this, they introduce DecFormer, a lightweight 7.7M-parameter transformer model that predicts per-channel blend weights and an off-manifold residual correction to perform mask-consistent latent fusion. DecFormer is designed to be plug-compatible with existing diffusion pipelines without needing backbone finetuning, adding minimal computational and parameter overhead. Experiments on the FLUX.1 diffusion model family demonstrate that DecFormer significantly improves global color consistency, mask sharpness, and soft-mask support, reducing edge errors by up to 53% compared to linear interpolation. When used as an inpainting prior, a lightweight LoRA trained with DecFormer achieves fidelity comparable to fully finetuned inpainting models. Beyond inpainting, PELC is presented as a versatile framework for pixel-equivalent latent editing, with demonstrated success on complex color-correction tasks. <div>
arXiv:2512.05198v1 Announce Type: new 
Abstract: Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask. We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC). An equivalent latent compositor should be the same as compositing in pixel space. This principle enables full-resolution mask control and true soft-edge alpha compositing, even though VAEs compress images 8x spatially. Modern VAEs capture global context beyond patch-aligned local structure, so linear latent blending cannot be pixel-equivalent: it produces large artifacts at mask seams and global degradation and color shifts. We introduce DecFormer, a 7.7M-parameter transformer that predicts per-channel blend weights and an off-manifold residual correction to realize mask-consistent latent fusion. DecFormer is trained so that decoding after fusion matches pixel-space alpha compositing, is plug-compatible with existing diffusion pipelines, requires no backbone finetuning and adds only 0.07% of FLUX.1-Dev's parameters and 3.5% FLOP overhead. On the FLUX.1 family, DecFormer restores global color consistency, soft-mask support, sharp boundaries, and high-fidelity masking, reducing error metrics around edges by up to 53% over standard mask interpolation. Used as an inpainting prior, a lightweight LoRA on FLUX.1-Dev with DecFormer achieves fidelity comparable to FLUX.1-Fill, a fully finetuned inpainting model. While we focus on inpainting, PELC is a general recipe for pixel-equivalent latent editing, as we demonstrate on a complex color-correction task.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEAR: Dataset for Evaluating the Aesthetics of RenderingDEAR: Dataset for Evaluating the Aesthetics of Rendering</title>
<link>https://arxiv.org/abs/2512.05209</link>
<guid>https://arxiv.org/abs/2512.05209</guid>
<content:encoded><![CDATA[
<div> Keywords: Image Quality Assessment, Aesthetic Evaluation, Rendering Styles, Crowdsourcing, DEAR Dataset  

<br /><br />Summary:  
1. Traditional Image Quality Assessment (IQA) primarily targets technical degradations such as noise, blur, and compression artifacts, using objective full-reference and no-reference metrics.  
2. Evaluating rendering aesthetics—important in photographic editing, content creation, and AI-generated imagery—remains underexplored due to the subjective and style-based nature of quality assessment.  
3. This work introduces DEAR (Dataset for Evaluating the Aesthetics of Rendering), a novel benchmark dataset built upon the MIT-Adobe FiveK dataset to model human aesthetic judgments of image rendering styles.  
4. DEAR is created through large-scale crowdsourcing, collecting pairwise human preference scores with 25 distinct evaluators per image pair, totaling 13,648 participants, capturing nuanced and context-sensitive aesthetic preferences.  
5. The dataset enables new tasks focused on Evaluation of Aesthetics of Rendering (EAR), going beyond distortion-based IQA, and supports use cases such as style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling.  
6. The data collection pipeline and human voting patterns are analyzed in detail.  
7. DEAR is reported to be the first dataset addressing image aesthetics of rendering assessment grounded on subjective human preferences, with a subset of 100 marked-up images published on HuggingFace, facilitating further research. <div>
arXiv:2512.05209v1 Announce Type: new 
Abstract: Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction</title>
<link>https://arxiv.org/abs/2512.05240</link>
<guid>https://arxiv.org/abs/2512.05240</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, RGB video reconstruction, diffusion model, autoregressive model, hybrid capture paradigm  

<br /><br />Summary:  
The paper addresses the challenge of continuous video monitoring in energy-constrained applications such as surveillance and wearable systems, where conventional RGB cameras consume significant power due to fixed-rate capture. To reduce energy consumption, the authors propose a hybrid capture paradigm that combines sparse RGB keyframes with continuous asynchronous event camera streams, enabling offline reconstruction of full RGB video sequences. They define a new task called Image and Event to Video (IE2Video), which involves generating RGB video from a single initial RGB keyframe and subsequent event data. Two architectural strategies are explored: an autoregressive model named HyperE2VID adapted for RGB video generation, and a diffusion-based model (LTX) that integrates event representations into a pretrained text-to-video diffusion framework using learned encoders and low-rank adaptation techniques. Experimental results reveal that the diffusion-based method outperforms the autoregressive baseline by 33% in perceptual quality, measured by the LPIPS metric (0.283 vs. 0.422). The approach is validated on three event camera datasets (BS-ERGB, HS-ERGB far, and HS-ERGB close) across various sequence lengths (32 to 128 frames) and shows strong cross-dataset generalization and robustness to unseen capture configurations, indicating its practical applicability for energy-efficient video reconstruction from event and RGB data. <div>
arXiv:2512.05240v1 Announce Type: new 
Abstract: Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Age-Inclusive 3D Human Mesh Recovery for Action-Preserving Data Anonymization</title>
<link>https://arxiv.org/abs/2512.05259</link>
<guid>https://arxiv.org/abs/2512.05259</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D shape estimation, children and infants, SMPL-A body model, transformer-based model, privacy-preserving 3D reconstruction  

<br /><br />Summary:  
This paper addresses the challenge of 3D shape and pose estimation, which typically underperforms on children and infants despite strong results on adults. The authors introduce AionHMR, a framework that incorporates the SMPL-A body model to enable simultaneous and accurate 3D modeling across adults, children, and infants. They propose an optimization-based extension to state-of-the-art models, generating pseudo-ground-truth annotations for existing child and infant image datasets. Using these annotations, they trained a transformer-based deep learning model capable of real-time, age-inclusive 3D human reconstruction. Extensive experiments reveal that their approach significantly improves pose and shape estimation accuracy for younger populations without degrading adult estimation performance. Additionally, the reconstructed 3D meshes act as privacy-preserving alternatives to raw images, capturing key action, pose, and geometry data while allowing anonymized dataset sharing. To demonstrate practical use, the authors introduce the 3D-BabyRobot dataset, featuring 3D reconstructions of children interacting with robots that retain action fidelity. Overall, this work bridges a critical gap in inclusive 3D human modeling by enabling accurate, privacy-aware reconstructions across all ages and fostering potential applications that require age-diverse human data. <div>
arXiv:2512.05259v1 Announce Type: new 
Abstract: While three-dimensional (3D) shape and pose estimation is a highly researched area that has yielded significant advances, the resulting methods, despite performing well for the adult population, generally fail to generalize effectively to children and infants. This paper addresses this challenge by introducing AionHMR, a comprehensive framework designed to bridge this domain gap. We propose an optimization-based method that extends a top-performing model by incorporating the SMPL-A body model, enabling the concurrent and accurate modeling of adults, children, and infants. Leveraging this approach, we generated pseudo-ground-truth annotations for publicly available child and infant image databases. Using these new training data, we then developed and trained a specialized transformer-based deep learning model capable of real-time 3D age-inclusive human reconstruction. Extensive experiments demonstrate that our methods significantly improve shape and pose estimation for children and infants without compromising accuracy on adults. Importantly, our reconstructed meshes serve as privacy-preserving substitutes for raw images, retaining essential action, pose, and geometry information while enabling anonymized datasets release. As a demonstration, we introduce the 3D-BabyRobot dataset, a collection of action-preserving 3D reconstructions of children interacting with robots. This work bridges a crucial domain gap and establishes a foundation for inclusive, privacy-aware, and age-diverse 3D human modeling.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARD: Correlation Aware Restoration with Diffusion</title>
<link>https://arxiv.org/abs/2512.05268</link>
<guid>https://arxiv.org/abs/2512.05268</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, correlated noise, image restoration, denoising, CARD<br /><br />Summary:  
This paper addresses the limitation of traditional denoising diffusion models that assume i.i.d. Gaussian noise, which is often not true for real-world sensor noise due to spatial correlations caused by sensor readout mechanisms. The authors propose a novel method called Correlation Aware Restoration with Diffusion (CARD), which is a training-free extension of DDRM, designed to explicitly handle correlated Gaussian noise. CARD operates by first whitening the noisy image, transforming the correlated noise into an i.i.d. form suitable for diffusion modeling. Subsequently, the standard diffusion restoration steps are replaced with noise-whitened updates that maintain DDRM’s efficient closed-form sampling while effectively addressing correlated noise. To support evaluation of such methods, the authors introduce CIN-D, a dataset containing real rolling-shutter sensor noise captured under various illumination conditions, filling an important gap in realistic correlated noise benchmarks. Empirical results demonstrate that CARD consistently outperforms existing techniques on synthetic correlated noise benchmarks as well as the real CIN-D dataset. The improvements are shown across multiple image restoration tasks including denoising, deblurring, and super-resolution, highlighting the effectiveness of explicitly modeling noise correlations in diffusion-based restoration frameworks. <div>
arXiv:2512.05268v1 Announce Type: new 
Abstract: Denoising diffusion models have achieved state-of-the-art performance in image restoration by modeling the process as sequential denoising steps. However, most approaches assume independent and identically distributed (i.i.d.) Gaussian noise, while real-world sensors often exhibit spatially correlated noise due to readout mechanisms, limiting their practical effectiveness. We introduce Correlation Aware Restoration with Diffusion (CARD), a training-free extension of DDRM that explicitly handles correlated Gaussian noise. CARD first whitens the noisy observation, which converts the noise into an i.i.d. form. Then, the diffusion restoration steps are replaced with noise-whitened updates, which inherits DDRM's closed-form sampling efficiency while now being able to handle correlated noise. To emphasize the importance of addressing correlated noise, we contribute CIN-D, a novel correlated noise dataset captured across diverse illumination conditions to evaluate restoration methods on real rolling-shutter sensor noise. This dataset fills a critical gap in the literature for experimental evaluation with real-world correlated noise. Experiments on standard benchmarks with synthetic correlated noise and on CIN-D demonstrate that CARD consistently outperforms existing methods across denoising, deblurring, and super-resolution tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring Compositional 4D Scenes without Ever Seeing One</title>
<link>https://arxiv.org/abs/2512.05272</link>
<guid>https://arxiv.org/abs/2512.05272</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D reconstruction, compositional modeling, spatio-temporal attention, monocular video, dynamic objects  

<br /><br />Summary:  
1. This paper addresses the challenge of capturing 4D structures, compositions, and spatio-temporal configurations of scenes consisting of multiple static and dynamic objects in-the-wild.  
2. Existing methods typically focus on either single objects or rely on parametric shape models limited to specific object categories, resulting in inconsistent scene reconstructions and limited generalizability.  
3. The authors propose COM4D, a novel method that jointly and consistently predicts the 4D structure and dynamics of multiple objects using only static multi-object images or videos of single dynamic objects as supervision.  
4. COM4D’s training disentangles spatial and temporal attention learning from different data sources—object compositions for spatial modeling and single object videos for temporal dynamics—thus avoiding the need for explicit 4D compositional training data.  
5. During inference, an attention mixing mechanism combines the separately learned spatial and temporal attentions, enabling reconstruction of complete, persistent 4D scenes with multiple interacting objects from monocular video input.  
6. The approach alternates between spatial and temporal reasoning to capture both structure and motion effectively.  
7. COM4D achieves state-of-the-art performance in separate tasks involving 4D object reconstruction and compositional 3D reconstruction, despite being purely data-driven and not relying on category-specific models. <div>
arXiv:2512.05272v1 Announce Type: new 
Abstract: Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.05277</link>
<guid>https://arxiv.org/abs/2512.05277</guid>
<content:encoded><![CDATA[
<div> Temporal understanding, autonomous driving, vision-language models, temporal reasoning, ego-centric footage<br /><br />Summary:<br /><br />1. Temporal understanding remains a critical challenge in autonomous driving (AD), particularly for state-of-the-art vision-language models (VLMs).<br />2. Existing temporal reasoning datasets and benchmarks focus on diverse video categories such as sports, cooking, and movies, but none concentrate exclusively on ego-centric AD video content.<br />3. To address this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is introduced, consisting of nearly 6,000 question-answer pairs across seven human-designed tasks aimed at assessing VLMs' temporal reasoning capabilities specific to AD.<br />4. An extensive evaluation of nine generalist and specialist SoTA AD models on TAD revealed subpar performance, mainly due to limited fine-grained motion understanding.<br />5. Two novel, training-free methods named Scene-CoT (leveraging Chain-of-Thought reasoning) and TCogMap (an ego-centric temporal cognitive map) are proposed to enhance motion understanding and overall model accuracy.<br />6. Integration of these approaches with existing VLMs led to accuracy improvements of up to 17.72% on the TAD benchmark.<br />7. The introduction of TAD, comprehensive benchmarking, and effective model enhancements are intended to drive further research on temporal understanding in autonomous driving.<br />8. The TAD dataset and evaluation code are publicly accessible via Hugging Face and GitHub repositories. <div>
arXiv:2512.05277v1 Announce Type: new 
Abstract: Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \href{https://github.com/vbdi/tad_bench}{Github}, respectively.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling</title>
<link>https://arxiv.org/abs/2512.05343</link>
<guid>https://arxiv.org/abs/2512.05343</guid>
<content:encoded><![CDATA[
<div> 3D generation, spatial control, generative models, geometric fidelity, interactive editing<br /><br />Summary:<br /><br />The paper introduces SpaceControl, a novel method aimed at providing explicit spatial control over 3D asset generation without the need for additional training. Unlike existing approaches that depend heavily on text or image prompts—often ambiguous or difficult to edit—SpaceControl can utilize various geometric inputs ranging from simple primitives to complex meshes. It integrates seamlessly with existing pre-trained generative models, functioning entirely at test time, which eliminates the overhead of retraining. A key feature of SpaceControl is a controllable parameter that allows users to balance geometric accuracy against visual realism, addressing the common trade-off found in generative 3D methods. Extensive evaluations, including quantitative metrics and user studies, indicate that SpaceControl surpasses both training-based and optimization-based baselines in maintaining geometric faithfulness without compromising image quality. The authors also contribute an interactive user interface that facilitates real-time editing of superquadrics, enabling straightforward conversion to textured 3D assets. This tool supports practical use in creative workflows and speeds up asset creation. Overall, SpaceControl represents a significant step toward more intuitive, precise, and flexible 3D asset generation that can be incorporated into a variety of content creation pipelines. <div>
arXiv:2512.05343v1 Announce Type: new 
Abstract: Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training</title>
<link>https://arxiv.org/abs/2512.05354</link>
<guid>https://arxiv.org/abs/2512.05354</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, interactive editing, Test-Time Training, photorealistic 3D, feedforward model<br /><br />Summary:  
The paper addresses the challenge of interactive refinement and editing of 3D Gaussian Splatting assets, which has been difficult due to limitations in existing diffusion and optimization-based methods. These previous approaches tend to be slow, can alter the original asset's identity, and lack precision for fine-grained control. To overcome these issues, the authors propose a new method called \ourmethod, a state-aware feedforward model designed for continuous editing of 3D Gaussian assets using user-provided 2D views. This method predicts attribute updates directly on a compact and feature-rich Gaussian representation, enabling efficient modifications. An important aspect of the approach is Test-Time Training, which facilitates a state-aware iterative editing workflow, allowing the model to adapt dynamically during editing sessions. The versatility of \ourmethod is demonstrated by its ability to perform multiple tasks with a single architecture, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring. Importantly, these operations are executed at interactive speeds, significantly improving user experience. Overall, this work paves the way for more fluid, intuitive, and precise 3D content creation and authoring, addressing a critical gap in the practical usability of 3D Gaussian Splatting technology. <div>
arXiv:2512.05354v1 Announce Type: new 
Abstract: The rise of 3D Gaussian Splatting has revolutionized photorealistic 3D asset creation, yet a critical gap remains for their interactive refinement and editing. Existing approaches based on diffusion or optimization are ill-suited for this task, as they are often prohibitively slow, destructive to the original asset's identity, or lack the precision for fine-grained control. To address this, we introduce \ourmethod, a state-aware feedforward model that enables continuous editing of 3D Gaussian assets from user-provided 2D view(s). Our method directly predicts updates to the attributes of a compact, feature-rich Gaussian representation and leverages Test-Time Training to create a state-aware, iterative workflow. The versatility of our approach allows a single architecture to perform diverse tasks, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring, all at interactive speeds, paving the way for fluid and intuitive 3D content authoring.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group Orthogonal Low-Rank Adaptation for RGB-T Tracking</title>
<link>https://arxiv.org/abs/2512.05359</link>
<guid>https://arxiv.org/abs/2512.05359</guid>
<content:encoded><![CDATA[
<div> Keywords: Parameter-efficient fine-tuning, RGB-T tracking, low-rank adaptation, group orthogonal constraint, singular value decomposition<br /><br />Summary:<br /><br />The paper addresses redundancy in low-rank adaptation methods for RGB-T tracking, which limits the model's ability to learn diverse and robust features. It proposes the Group Orthogonal Low-Rank Adaptation (GOLA) framework to enhance parameter efficiency and expressiveness by better leveraging the rank space. The approach involves decomposing the rank space using singular value decomposition (SVD) to identify and freeze crucial ranks, preserving pretrained knowledge while grouping redundant ranks. An inter-group orthogonal constraint is introduced to enforce orthogonality between these groups, encouraging complementary feature learning that addresses various tracking challenges. This structured parameter learning reduces information overlap and enables the model to acquire more diverse knowledge. Experimental evaluation on four benchmark RGB-T tracking datasets shows that GOLA significantly reduces parameter redundancy and improves feature representation capability. The method achieves superior performance compared to state-of-the-art approaches, validating both its theoretical motivation and practical effectiveness in adapting pretrained models to RGB-T tracking tasks while maintaining parameter efficiency. <div>
arXiv:2512.05359v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning has emerged as a promising paradigm in RGB-T tracking, enabling downstream task adaptation by freezing pretrained parameters and fine-tuning only a small set of parameters. This set forms a rank space made up of multiple individual ranks, whose expressiveness directly shapes the model's adaptability. However, quantitative analysis reveals low-rank adaptation exhibits significant redundancy in the rank space, with many ranks contributing almost no practical information. This hinders the model's ability to learn more diverse knowledge to address the various challenges in RGB-T tracking. To address this issue, we propose the Group Orthogonal Low-Rank Adaptation (GOLA) framework for RGB-T tracking, which effectively leverages the rank space through structured parameter learning. Specifically, we adopt a rank decomposition partitioning strategy utilizing singular value decomposition to quantify rank importance, freeze crucial ranks to preserve the pretrained priors, and cluster the redundant ranks into groups to prepare for subsequent orthogonal constraints. We further design an inter-group orthogonal constraint strategy. This constraint enforces orthogonality between rank groups, compelling them to learn complementary features that target diverse challenges, thereby alleviating information redundancy. Experimental results demonstrate that GOLA effectively reduces parameter redundancy and enhances feature representation capabilities, significantly outperforming state-of-the-art methods across four benchmark datasets and validating its effectiveness in RGB-T tracking tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoolNet: Deep Learning for 2D to 3D Video Process Validation</title>
<link>https://arxiv.org/abs/2512.05362</link>
<guid>https://arxiv.org/abs/2512.05362</guid>
<content:encoded><![CDATA[
<div> Structure-from-Motion, deep learning, PoolNet, camera pose, data validation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of extracting Structure-from-Motion (SfM) information from both sequential and non-sequential image datasets, highlighting the process as being time-consuming and computationally demanding. 2. It identifies a key problem that much of the publicly available image data is unsuitable for SfM due to limited variation in camera poses, presence of obscured scene elements, and noisy inputs, which negatively affect reconstruction quality. 3. To overcome these issues, the authors propose PoolNet, a deep learning framework designed to validate image data at both the frame and scene levels, focusing specifically on in-the-wild data scenarios. 4. PoolNet is demonstrated to effectively distinguish between scenes that are suitable for SfM processing and those that are not, thereby acting as a pre-processing filter. 5. An important advantage shown by the model is its ability to reduce the processing time considerably compared to current state-of-the-art SfM algorithms, making the data preparation workflow more efficient and reliable. <div>
arXiv:2512.05362v1 Announce Type: new 
Abstract: Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShaRP: SHAllow-LayeR Pruning for Video Large Language Models Acceleration</title>
<link>https://arxiv.org/abs/2512.05385</link>
<guid>https://arxiv.org/abs/2512.05385</guid>
<content:encoded><![CDATA[
<div> Video Large Language Models, attention-based pruning, positional debiasing, segment-aware causal masking, token deduplication<br /><br />Summary:<br /><br />This paper addresses the computational challenges faced by Video Large Language Models (VLLMs), particularly during the pre-filling stage where a large quantity of visual tokens is processed. Existing attention-based pruning techniques are commonly used to speed up inference but often lead to significant performance drops, especially when applied to early decoder layers at high compression rates. The authors identify that the limitations of these methods in shallow layers stem from positional encoding bias and insufficient interaction among token information. To overcome these issues, the paper proposes ShaRP, an enhanced attention-based pruning framework that incorporates segment-aware causal masking to better preserve temporal relationships, positional debiasing to reduce bias from positional encoding, and token deduplication to eliminate redundant tokens. ShaRP enables effective pruning even in early decoder layers without the need for retraining and maintains stable performance under high compression rates. Extensive experiments conducted on multiple video understanding benchmarks demonstrate that ShaRP achieves competitive performance while significantly accelerating VLLM inference. This work introduces a new paradigm for efficient video language model processing by balancing token reduction and model accuracy effectively. <div>
arXiv:2512.05385v1 Announce Type: new 
Abstract: Video Large Language Models (VLLMs) face the challenge of high computational load during the pre-filling stage due to the processing of an enormous number of visual tokens. Although attention-based pruning methods are widely used to accelerate inference, trials at early decoder layers often result in significant performance degradation, especially under high compression rates. We argue that while attention-based pruning inherently holds the potential to identify the most relevant visual tokens, its effectiveness in shallow decoder layers is limited by factors such as positional encoding bias and insufficient information interaction. In this paper, we propose an improved attention-based pruning framework, termed ShaRP, that integrates segment-aware causal masking, positional debiasing, and token deduplication for enhanced token selection. It enables effective pruning at shallow layers while maintaining stable performance under high compression rates without retraining. Extensive experiments demonstrate that ShaRP achieves competitive performance across multiple video understanding benchmarks, establishing a new paradigm for accelerating VLLM inference.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.05391</link>
<guid>https://arxiv.org/abs/2512.05391</guid>
<content:encoded><![CDATA[
<div> Keywords: Whole Slide Image, Multimodal Large Language Models, Redundancy Reduction, Sparse Token Merger, Cross-Attention Routing Adapter<br /><br />Summary:  
This work addresses the challenge of understanding Whole Slide Images (WSIs), which are extremely large and contain sparse diagnostically relevant regions. Traditional slide-level multimodal large language models (MLLMs) use computationally expensive slide-level encoders that process thousands of patch features in a brute-force manner, leading to high computation and memory costs. The authors observe that many tile-level features in WSIs exhibit strong global and local redundancy, and only a small subset of these tiles are truly relevant for the diagnostic task. To tackle this, they propose LoC-Path, an efficient MLLM framework designed to reduce redundancy and computational load. LoC-Path introduces two key modules: the Sparse Token Merger (STM) combined with an MAE-pretrained resampler to compress and remove redundancy in tile tokens, and the Cross-Attention Routing Adapter (CARA) along with a Token Importance Scorer (TIS) to integrate the compressed visual data into the language model efficiently. Extensive experiments show that LoC-Path achieves performance on par with current state-of-the-art whole-slide MLLMs but with significantly reduced computation and memory requirements, making it a promising approach for practical WSI-language modeling tasks. <div>
arXiv:2512.05391v1 Announce Type: new 
Abstract: Whole Slide Image (WSI) understanding is fundamentally challenging due to its gigapixel scale and the extreme sparsity of diagnostically relevant regions. Unlike human experts who primarily rely on key areas to arrive at a diagnosis, existing slide-level multimodal large language models (MLLMs) for pathology rely on heavy slide-level encoders that process thousands of patch features in a brute-force manner, resulting in excessive computational cost. In this work, we revisit the WSI-language modeling paradigm and show that tile-level features exhibit strong global and local redundancy, whereas only a small subset of tiles are truly task-relevant. Motivated by this observation, we introduce an efficient MLLM framework, called LoC-Path, that replaces the expensive slide-level encoder with redundancy-reducing modules. We first design a Sparse Token Merger (STM) and an MAE-pretrained resampler to remove local redundancy and compress globally redundant tile tokens into a compact slide-level representation set. We then propose a Cross-Attention Routing Adapter (CARA) and a Token Importance Scorer (TIS) to integrate the compressed visual representation with the language model in a computation-efficient manner. Extensive experiments demonstrate that our approach achieves performance comparable to existing state-of-the-art whole-slide MLLMs, while requiring significantly lower computation and memory.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability</title>
<link>https://arxiv.org/abs/2512.05394</link>
<guid>https://arxiv.org/abs/2512.05394</guid>
<content:encoded><![CDATA[
<div> Keywords: latent diffusion models, video VAE, spectral properties, regularizers, text-to-video generation<br /><br />Summary:<br /><br />This paper addresses the impact of latent space structure in video Variational Autoencoders (VAEs) on the efficiency of latent diffusion model training. The authors point out that while current video VAEs focus mainly on reconstruction fidelity, they neglect the importance of latent space properties for diffusion backbones. Through a statistical analysis, two crucial spectral characteristics of video VAE latents are identified: a spatio-temporal frequency spectrum that is biased toward low frequencies, and a channel-wise eigenspectrum dominated by only a few modes. To incorporate these desirable spectral traits into the latent representations, the paper introduces two novel, lightweight, and backbone-agnostic regularization techniques named Local Correlation Regularization and Latent Masked Reconstruction. These are designed to shape the latent space more effectively without heavy modifications to existing architectures. Experimental results demonstrate that the proposed Spectral-Structured VAE (SSVAE) provides a threefold acceleration in convergence speed during text-to-video generation, while also delivering a 10% improvement in video reward metrics compared to strong open-source VAE baselines. The contribution highlights the importance of latent space design for diffusion-based video generation and offers practical tools to enhance training efficiency and outcome quality. The authors have made their implementation publicly available for community use at the linked repository. <div>
arXiv:2512.05394v1 Announce Type: new 
Abstract: Latent diffusion models pair VAEs with diffusion backbones, and the structure of VAE latents strongly influences the difficulty of diffusion training. However, existing video VAEs typically focus on reconstruction fidelity, overlooking latent structure. We present a statistical analysis of video VAE latent spaces and identify two spectral properties essential for diffusion training: a spatio-temporal frequency spectrum biased toward low frequencies, and a channel-wise eigenspectrum dominated by a few modes. To induce these properties, we propose two lightweight, backbone-agnostic regularizers: Local Correlation Regularization and Latent Masked Reconstruction. Experiments show that our Spectral-Structured VAE (SSVAE) achieves a $3\times$ speedup in text-to-video generation convergence and a 10\% gain in video reward, outperforming strong open-source VAEs. The code is available at https://github.com/zai-org/SSVAE.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Dynamic Prior: Understanding 3D Structures for Casual Dynamic Videos</title>
<link>https://arxiv.org/abs/2512.05398</link>
<guid>https://arxiv.org/abs/2512.05398</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic Prior, Vision-Language Models, motion segmentation, camera pose optimization, 3D scene understanding<br /><br />Summary:<br /><br />This paper addresses the challenge of accurately estimating camera poses, 3D scene geometry, and object motion in videos containing dynamic objects, a problem that traditional structure-from-motion techniques struggle to solve. The authors identify that previous learning-based methods depend heavily on large-scale motion segmentation datasets, which limits their ability to generalize and results in inaccurate segmentation of dynamic objects. To overcome this limitation, they propose a novel approach called the Dynamic Prior (\ourmodel), which leverages Vision-Language Models (VLMs) for their semantic reasoning capabilities and Segment Anything Model 2 (SAM2) for precise spatial segmentation. \ourmodel operates without requiring task-specific training, enabling it to robustly detect dynamic objects in diverse video settings. This method can be integrated into existing pipelines for camera pose optimization, depth reconstruction, and 4D trajectory estimation, enhancing their performance. Extensive experimental evaluation on both synthetic and real-world video datasets demonstrates that \ourmodel achieves state-of-the-art results in motion segmentation. Moreover, incorporating \ourmodel substantially improves the accuracy and robustness of structural 3D understanding tasks, paving the way for more reliable dynamic scene analysis in-the-wild. <div>
arXiv:2512.05398v1 Announce Type: new 
Abstract: Estimating accurate camera poses, 3D scene geometry, and object motion from in-the-wild videos is a long-standing challenge for classical structure from motion pipelines due to the presence of dynamic objects. Recent learning-based methods attempt to overcome this challenge by training motion estimators to filter dynamic objects and focus on the static background. However, their performance is largely limited by the availability of large-scale motion segmentation datasets, resulting in inaccurate segmentation and, therefore, inferior structural 3D understanding. In this work, we introduce the Dynamic Prior (\ourmodel) to robustly identify dynamic objects without task-specific training, leveraging the powerful reasoning capabilities of Vision-Language Models (VLMs) and the fine-grained spatial segmentation capacity of SAM2. \ourmodel can be seamlessly integrated into state-of-the-art pipelines for camera pose optimization, depth reconstruction, and 4D trajectory estimation. Extensive experiments on both synthetic and real-world videos demonstrate that \ourmodel not only achieves state-of-the-art performance on motion segmentation, but also significantly improves accuracy and robustness for structural 3D understanding.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images</title>
<link>https://arxiv.org/abs/2512.05410</link>
<guid>https://arxiv.org/abs/2512.05410</guid>
<content:encoded><![CDATA[
<div> Keywords: Genetic Algorithm, Stereo Matching, UAV, Parameter Optimization, Forestry Imaging  

<br /><br />Summary:  
This paper addresses the challenge of parameter tuning in traditional stereo matching algorithms, specifically Semi-Global Block Matching (SGBM) combined with Weighted Least Squares (WLS) filtering, used for UAV-based distance measurement to tree branches. The authors propose a novel Genetic Algorithm (GA)-based parameter optimization framework that automates the search for optimal SGBM and WLS parameters, removing the need for manual tuning. Their contributions include developing this framework, establishing a thorough evaluation methodology using multiple image quality metrics, and delivering a practical solution tailored to the constraints of UAV systems in forestry environments. Experimental results reveal that the GA-optimized parameters significantly enhance disparity map quality: Mean Squared Error is reduced by 42.86%, Peak Signal-to-Noise Ratio improves by 8.47%, and Structural Similarity increases by 28.52% compared to baseline setups. Additionally, the method demonstrates strong generalization across diverse imaging conditions, which is critical for robust real-world applications in forestry. The approach retains the computational efficiency advantage of traditional methods, generating disparity maps in approximately 0.5 seconds per frame, making it suitable for resource-limited UAV platforms. Overall, the study presents an effective and efficient solution for enhancing stereo vision accuracy in UAV forestry monitoring via intelligent parameter optimization. <div>
arXiv:2512.05410v1 Announce Type: new 
Abstract: Traditional stereo matching algorithms like Semi-Global Block Matching (SGBM) with Weighted Least Squares (WLS) filtering offer speed advantages over neural networks for UAV applications, generating disparity maps in approximately 0.5 seconds per frame. However, these algorithms require meticulous parameter tuning. We propose a Genetic Algorithm (GA) based parameter optimization framework that systematically searches for optimal parameter configurations for SGBM and WLS, enabling UAVs to measure distances to tree branches with enhanced precision while maintaining processing efficiency. Our contributions include: (1) a novel GA-based parameter optimization framework that eliminates manual tuning; (2) a comprehensive evaluation methodology using multiple image quality metrics; and (3) a practical solution for resource-constrained UAV systems. Experimental results demonstrate that our GA-optimized approach reduces Mean Squared Error by 42.86% while increasing Peak Signal-to-Noise Ratio and Structural Similarity by 8.47% and 28.52%, respectively, compared with baseline configurations. Furthermore, our approach demonstrates superior generalization performance across varied imaging conditions, which is critcal for real-world forestry applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications</title>
<link>https://arxiv.org/abs/2512.05412</link>
<guid>https://arxiv.org/abs/2512.05412</guid>
<content:encoded><![CDATA[
<div> Keywords: Radiata pine, autonomous pruning, YOLO, stereo vision, branch detection  

<br /><br />Summary:  
This paper addresses the safety issues inherent in manual pruning of radiata pine trees, which involves working at dangerous heights and difficult terrain. To mitigate these risks, the authors propose an autonomous pruning system that leverages computer vision techniques. The framework integrates the YOLO object detection model with Semi-Global Block Matching (SGBM) stereo vision, enabling precise branch detection and accurate depth estimation. A key advantage of this approach is the reliance solely on stereo camera input, which removes the need for costly LiDAR sensors traditionally used in such applications. Experimental results demonstrate that YOLO outperforms Mask R-CNN in branch segmentation, achieving an impressive 82.0% mAPmask50-95 metric. The system can localize branches reliably within a 2-meter operational range and operates efficiently with processing times under one second per frame. These findings validate the system's potential as a cost-effective solution to improve worker safety and operational efficiency in commercial forestry by automating pruning tasks using drones equipped with stereo vision and advanced object detection. <div>
arXiv:2512.05412v1 Announce Type: new 
Abstract: Manual pruning of radiata pine trees poses significant safety risks due to extreme working heights and challenging terrain. This paper presents a computer vision framework that integrates YOLO object detection with Semi-Global Block Matching (SGBM) stereo vision for autonomous drone-based pruning operations. Our system achieves precise branch detection and depth estimation using only stereo camera input, eliminating the need for expensive LiDAR sensors. Experimental evaluation demonstrates YOLO's superior performance over Mask R-CNN, achieving 82.0% mAPmask50-95 for branch segmentation. The integrated system accurately localizes branches within a 2 m operational range, with processing times under one second per frame. These results establish the feasibility of cost-effective autonomous pruning systems that enhance worker safety and operational efficiency in commercial forestry.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moving object detection from multi-depth images with an attention-enhanced CNN</title>
<link>https://arxiv.org/abs/2512.05415</link>
<guid>https://arxiv.org/abs/2512.05415</guid>
<content:encoded><![CDATA[
<div> Keywords: moving object detection, convolutional neural network, attention module, multi-input architecture, solar system survey  

<br /><br />Summary:  
This paper addresses the challenge of distinguishing true moving objects in the solar system from noise in wide-field survey data, which traditionally depends heavily on labor-intensive human verification. The authors propose a novel multi-input convolutional neural network (CNN) integrated with a convolutional block attention module (CBAM) to improve detection efficiency and accuracy. The model processes multiple stacked images simultaneously through its multi-input architecture, allowing it to learn more effectively from temporal or spatial image data. The CBAM enhances the model's focus on important features across spatial and channel dimensions, bolstering its ability to identify genuine moving objects amidst noise. Evaluated on a dataset of roughly 2,000 observational images, the method achieved nearly 99% accuracy with an Area Under the Curve (AUC) above 0.99, demonstrating excellent classification performance. Importantly, by adjusting the detection threshold, the new system is able to reduce human verification workload by over 99%, significantly decreasing manual labor and associated costs. This advancement paves the way for more automated, robust, and scalable detection of moving objects in solar system surveys. <div>
arXiv:2512.05415v1 Announce Type: new 
Abstract: One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Evaluation of Deep Learning for Tree Branch Segmentation in Autonomous Forestry Systems</title>
<link>https://arxiv.org/abs/2512.05418</link>
<guid>https://arxiv.org/abs/2512.05418</guid>
<content:encoded><![CDATA[
<div> Tree Branch Segmentation, UAV Forestry, Deep Learning, Multi-Resolution Analysis, Performance Benchmarking<br /><br />Summary:<br /><br />1. This paper addresses the challenge of rapid and precise tree branch segmentation for UAV-based autonomous forestry operations, crucial for safe navigation and automated pruning.  
2. The study evaluates various deep learning methods on three input resolutions—256x256, 512x512, and 1024x1024 pixels—using the Urban Street Tree Dataset.  
3. Standard evaluation metrics such as Intersection over Union (IoU) and Dice coefficient are employed, alongside specialized metrics like Thin Structure IoU (TS-IoU) and Connectivity Preservation Rate (CPR), to assess model performance comprehensively.  
4. Among 22 tested configurations, U-Net with a MiT-B4 backbone performs best at 256x256 resolution, MiT-B4 leads at 512x512 in multiple metrics including IoU and Boundary-F1, while at 1024x1024 resolution, U-Net with MiT-B3 achieves top IoU/Dice scores and precision, with U-Net++ excelling in boundary quality.  
5. PSPNet is identified as the most computationally efficient model, requiring significantly fewer GFLOPs but sacrificing some accuracy compared to top performers.  
6. These results establish new multi-resolution benchmarks that balance accuracy and computational efficiency for deployment in embedded forestry systems.  
7. The implementation code is publicly available at the provided GitHub repository, facilitating further research and practical applications in autonomous forestry. <div>
arXiv:2512.05418v1 Announce Type: new 
Abstract: UAV-based autonomous forestry operations require rapid and precise tree branch segmentation for safe navigation and automated pruning across varying pixel resolutions and operational conditions. We evaluate different deep learning methods at three resolutions (256x256, 512x512, 1024x1024) using the Urban Street Tree Dataset, employing standard metrics (IoU, Dice) and specialized measures including Thin Structure IoU (TS-IoU) and Connectivity Preservation Rate (CPR). Among 22 configurations tested, U-Net with MiT-B4 backbone achieves strong performance at 256x256. At 512x512, MiT-B4 leads in IoU, Dice, TS-IoU, and Boundary-F1. At 1024x1024, U-Net+MiT-B3 shows the best validation performance for IoU/Dice and precision, while U-Net++ excels in boundary quality. PSPNet provides the most efficient option (2.36/9.43/37.74 GFLOPs) with 25.7/19.6/11.8 percentage point IoU reductions compared to top performers at respective resolutions. These results establish multi-resolution benchmarks for accuracy-efficiency trade-offs in embedded forestry systems. Implementation is available at https://github.com/BennyLinntu/PerformanceTreeBranchSegmentation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction</title>
<link>https://arxiv.org/abs/2512.05422</link>
<guid>https://arxiv.org/abs/2512.05422</guid>
<content:encoded><![CDATA[
<div> Keywords: Unified multimodal models, vision-language models, diffusion models, Layer Integration Module, Reinforcement Learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of improving visual generation by combining Vision-Language Models (VLMs) with diffusion models in a unified multimodal framework. Existing methods face difficulties balancing sufficient interaction across modalities and maintaining a flexible implementation due to the large differences in representations. <br /><br />2. The authors propose ParaUni, a novel approach that extracts features from multiple layers of various VLMs in parallel, leveraging the hierarchical nature of VLM layers that capture both low-level details and high-level semantics for comprehensive information interaction. <br /><br />3. ParaUni includes a Layer Integration Module (LIM) that fuses visual features from all VLM layers efficiently, producing a combined representation used as conditioning input to the diffusion model to enhance image generation quality. <br /><br />4. The paper introduces a Layer-wise Dynamic Adjustment Mechanism (LDAM) that leverages Reinforcement Learning to adjust the multi-layer features with respect to different rewards, aligning the hierarchical properties of the layers for better performance during training. <br /><br />5. Extensive experiments demonstrate that ParaUni significantly improves generation quality by effectively utilizing complementary features from multiple VLM layers and shows strong capability for multiple reward improvements during the RL stage. The code has been made publicly available. <div>
arXiv:2512.05422v1 Announce Type: new 
Abstract: Unified multimodal models significantly improve visual generation by combining vision-language models (VLMs) with diffusion models. However, existing methods struggle to fully balance sufficient interaction and flexible implementation due to vast representation difference. Considering abundant and hierarchical information in VLM's layers from low-level details to high-level semantics, we propose \textbf{ParaUni}. It extracts features from variants VLM's layers in a \textbf{Para}llel way for comprehensive information interaction and retains a flexible separation architecture to enhance generation in \textbf{Uni}fied multimodal model. Concretely, visual features from all VLM's layers are fed in parallel into a Layer Integration Module (LIM), which efficiently integrates fine-grained details and semantic abstractions and provides the fused representation as a condition to the diffusion model. To further enhance performance, we reveal that these hierarchical layers respond unequally to different rewards in Reinforcement Learning (RL). Crucially, we design a Layer-wise Dynamic Adjustment Mechanism (LDAM) to facilitate multiple reward improvements that aligns the hierarchical properties of these layers using RL. Extensive experiments show ParaUni leverages complementary multi-layer features to substantially improve generation quality and shows strong potential for multiple reward advances during RL stages. Code is available at https://github.com/JosephTiTan/ParaUni.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression</title>
<link>https://arxiv.org/abs/2512.05446</link>
<guid>https://arxiv.org/abs/2512.05446</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, dynamic scene representation, deformation scheme, rate-distortion compression, implicit neural representation  

<br /><br />Summary:  
This paper presents TED-4DGS, a novel method for dynamic 3D scene representation using 4D Gaussian Splatting (4DGS). The authors address limitations in prior work, which either over-specify short-lived Gaussian primitives in space-time 4DGS or use canonical 3DGS with insufficient temporal control for deformation. TED-4DGS introduces a temporally activated and embedding-based deformation scheme that combines the advantages of both approaches. It is based on a sparse anchor 3DGS representation where each anchor has learnable temporal-activation parameters to control its appearance and disappearance over time. Additionally, a lightweight per-anchor temporal embedding queries a shared deformation bank to generate anchor-specific deformations, enabling explicit temporal control. For compression, TED-4DGS integrates an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions and employs a channel-wise autoregressive model to capture intra-anchor correlations. These elements contribute to a rate-distortion-optimized compression framework for dynamic 3DGS representations. Experimental results demonstrate that TED-4DGS achieves state-of-the-art rate-distortion performance on real-world datasets. To the authors’ knowledge, this work is among the first to focus on a rate-distortion-optimized compression strategy specifically for dynamic 3D Gaussian Splatting. <div>
arXiv:2512.05446v1 Announce Type: new 
Abstract: Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>University Building Recognition Dataset in Thailand for the mission-oriented IoT sensor system</title>
<link>https://arxiv.org/abs/2512.05468</link>
<guid>https://arxiv.org/abs/2512.05468</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Federated Learning, Vision Transformer, Edge Devices, Building Recognition<br /><br />Summary:  
This paper addresses the emerging trend of performing machine learning training directly on edge devices, enabled by advances in semiconductor technology. It focuses on Wireless Ad Hoc Federated Learning (WAFL), an approach that leverages device-to-device communication among edge devices for collaborative model training without relying on centralized data. The study specifically examines the integration of WAFL with Vision Transformer models (WAFL-ViT) in the context of image recognition tasks, previously tested on the UTokyo Building Recognition Dataset (UTBR). Recognizing the mission-specific nature of WAFL-ViT sensor systems, the authors introduce a new dataset, the Chulalongkorn University Building Recognition Dataset (CUBR), tailored as a case study for Chulalongkorn University in Thailand. Their experiments demonstrate that training models under WAFL scenarios yields superior accuracy compared to self-training on individual devices, highlighting the benefits of collaborative learning frameworks in edge environments. Additionally, they provide open access to the CUBR dataset to promote further research, available at their GitHub repository. This work contributes to advancing federated learning strategies and deployment of vision models on resource-constrained edge devices for mission-oriented applications. <div>
arXiv:2512.05468v1 Announce Type: new 
Abstract: Many industrial sectors have been using of machine learning at inference mode on edge devices. Future directions show that training on edge devices is promising due to improvements in semiconductor performance. Wireless Ad Hoc Federated Learning (WAFL) has been proposed as a promising approach for collaborative learning with device-to-device communication among edges. In particular, WAFL with Vision Transformer (WAFL-ViT) has been tested on image recognition tasks with the UTokyo Building Recognition Dataset (UTBR). Since WAFL-ViT is a mission-oriented sensor system, it is essential to construct specific datasets by each mission. In our work, we have developed the Chulalongkorn University Building Recognition Dataset (CUBR), which is specialized for Chulalongkorn University as a case study in Thailand. Additionally, our results also demonstrate that training on WAFL scenarios achieves better accuracy than self-training scenarios. Dataset is available in https://github.com/jo2lxq/wafl/.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoStyle: Emotion-Driven Image Stylization</title>
<link>https://arxiv.org/abs/2512.05478</link>
<guid>https://arxiv.org/abs/2512.05478</guid>
<content:encoded><![CDATA[
<div> Affective Image Stylization, EmoStyle, Emotion-Content Reasoner, Style Quantizer, Emotion-aware stylization  

<br /><br />Summary:  
This paper introduces Affective Image Stylization (AIS), a new task aimed at applying artistic styles to images that evoke specific emotions while preserving the original content. To support this, the authors create EmoStyleSet, a unique dataset of content-emotion-stylized image triplets derived from ArtEmis, addressing the lack of training data in this domain. The EmoStyle framework features an Emotion-Content Reasoner, which adaptively integrates emotional cues with image content to generate coherent style guidance. Recognizing the discrete characteristics of artistic styles, the authors design a Style Quantizer that transforms continuous style representations into emotion-related codebook entries. Extensive evaluations, both qualitative and quantitative including user studies, show that EmoStyle improves the emotional expressiveness of stylized images without compromising content fidelity. Furthermore, the emotion-aware style dictionary learned during training is versatile and can be leveraged for other generative tasks. Overall, this work lays foundational ground for emotion-driven image stylization, enhancing the expressive and creative abilities of AI-generated art by incorporating emotional dimensions into style transfer. <div>
arXiv:2512.05478v1 Announce Type: new 
Abstract: Art has long been a profound medium for expressing emotions. While existing image stylization methods effectively transform visual appearance, they often overlook the emotional impact carried by styles. To bridge this gap, we introduce Affective Image Stylization (AIS), a task that applies artistic styles to evoke specific emotions while preserving content. We present EmoStyle, a framework designed to address key challenges in AIS, including the lack of training data and the emotion-style mapping. First, we construct EmoStyleSet, a content-emotion-stylized image triplet dataset derived from ArtEmis to support AIS. We then propose an Emotion-Content Reasoner that adaptively integrates emotional cues with content to learn coherent style queries. Given the discrete nature of artistic styles, we further develop a Style Quantizer that converts continuous style features into emotion-related codebook entries. Extensive qualitative and quantitative evaluations, including user studies, demonstrate that EmoStyle enhances emotional expressiveness while maintaining content consistency. Moreover, the learned emotion-aware style dictionary is adaptable to other generative tasks, highlighting its potential for broader applications. Our work establishes a foundation for emotion-driven image stylization, expanding the creative potential of AI-generated art.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniFS: Unified Multi-Contrast MRI Reconstruction via Frequency-Spatial Fusion</title>
<link>https://arxiv.org/abs/2512.05481</link>
<guid>https://arxiv.org/abs/2512.05481</guid>
<content:encoded><![CDATA[
<div> keywords: Multi-Contrast MR Reconstruction, k-space undersampling, frequency-spatial fusion, adaptive prompt learning, cross-modal fusion  

<br /><br />Summary:  
This paper addresses the challenge of generalization across different k-space undersampling patterns in Multi-Contrast MR Reconstruction (MCMR), where current methods require retraining for each pattern, limiting their practicality. The authors propose UniFS, a Unified Frequency-Spatial Fusion model capable of handling multiple undersampling patterns without retraining. UniFS incorporates three key modules: a Cross-Modal Frequency Fusion module to leverage complementary frequency information from multiple contrasts, an Adaptive Mask-Based Prompt Learning module to dynamically adapt to variations in undersampling masks, and a Dual-Branch Complementary Refinement module to extract domain-invariant features effectively. Unlike existing methods that focus mainly on spatial information or extract only shallow frequency features, UniFS introduces an adaptive prompt-guided frequency fusion approach designed for robust k-space learning and enhanced generalizability. The model was evaluated on the BraTS and HCP datasets with various undersampling patterns and acceleration factors, including patterns not seen during training. Results show that UniFS achieves state-of-the-art performance across multiple scenarios, demonstrating strong generalization capability. The authors also provide their implementation publicly for reproducibility and further research at the given GitHub repository. <div>
arXiv:2512.05481v1 Announce Type: new 
Abstract: Recently, Multi-Contrast MR Reconstruction (MCMR) has emerged as a hot research topic that leverages high-quality auxiliary modalities to reconstruct undersampled target modalities of interest. However, existing methods often struggle to generalize across different k-space undersampling patterns, requiring the training of a separate model for each specific pattern, which limits their practical applicability. To address this challenge, we propose UniFS, a Unified Frequency-Spatial Fusion model designed to handle multiple k-space undersampling patterns for MCMR tasks without any need for retraining. UniFS integrates three key modules: a Cross-Modal Frequency Fusion module, an Adaptive Mask-Based Prompt Learning module, and a Dual-Branch Complementary Refinement module. These modules work together to extract domain-invariant features from diverse k-space undersampling patterns while dynamically adapt to their own variations. Another limitation of existing MCMR methods is their tendency to focus solely on spatial information while neglect frequency characteristics, or extract only shallow frequency features, thus failing to fully leverage complementary cross-modal frequency information. To relieve this issue, UniFS introduces an adaptive prompt-guided frequency fusion module for k-space learning, significantly enhancing the model's generalization performance. We evaluate our model on the BraTS and HCP datasets with various k-space undersampling patterns and acceleration factors, including previously unseen patterns, to comprehensively assess UniFS's generalizability. Experimental results across multiple scenarios demonstrate that UniFS achieves state-of-the-art performance. Our code is available at https://github.com/LIKP0/UniFS.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept-based Explainable Data Mining with VLM for 3D Detection</title>
<link>https://arxiv.org/abs/2512.05482</link>
<guid>https://arxiv.org/abs/2512.05482</guid>
<content:encoded><![CDATA[
<div> Rare-object detection, autonomous driving, Vision-Language Models, outlier detection, 3D object detection<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting rare objects in autonomous driving systems using point cloud data. It introduces a novel cross-modal framework that leverages 2D Vision-Language Models (VLMs) to mine rare objects from driving scenes, thus improving 3D object detection. The approach integrates techniques including object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into an explainable pipeline. Specifically, the method combines Isolation Forest and t-SNE-based outlier detection with concept-based filtering to effectively identify semantically meaningful rare objects such as construction vehicles, motorcycles, and barriers. This targeted mining reduces the annotation effort by focusing only on valuable training samples. Experimental results on the nuScenes dataset demonstrate that this concept-guided strategy enhances 3D detection performance while using only a fraction of the training data. The improvements are particularly significant for challenging categories like trailers and bicycles compared to using the same amount of randomly selected data. This work has important implications for the efficient curation of datasets in safety-critical systems like autonomous driving, ensuring better identification of rare but critical objects. <div>
arXiv:2512.05482v1 Announce Type: new 
Abstract: Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaterWave: Bridging Underwater Image Enhancement into Video Streams via Wavelet-based Temporal Consistency Field</title>
<link>https://arxiv.org/abs/2512.05492</link>
<guid>https://arxiv.org/abs/2512.05492</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater video enhancement, temporal consistency, wavelet-based temporal consistency field, underwater flow correction, video tracking  

<br /><br />Summary:  
The paper addresses the challenge of enhancing underwater videos, which are difficult to obtain and suffer from temporal inconsistency when processed frame-by-frame using single-image enhancement techniques. To tackle this, the authors investigate the temporal manifold of natural videos and identify a temporal consistency prior from a local temporal frequency perspective in dynamic scenes. Leveraging this prior and working under a no paired-data condition, they propose an implicit representation method called WaterWave, which operates in a wavelet-based temporal consistency field. WaterWave progressively filters out inconsistent components while preserving motion details and scene integrity, resulting in smooth, natural-flowing underwater videos. The authors also introduce an underwater flow correction module to accurately represent temporal frequency bands by rectifying flow estimations to better reflect underwater transmission characteristics. Extensive experiments demonstrate that WaterWave considerably improves the quality of videos enhanced by existing single-image underwater enhancement models. Additionally, the method shows strong potential for downstream underwater tracking tasks such as UOSTrack and MAT, achieving significant precision improvements of 19.7% and 9.7%, respectively, compared to the original video inputs. <div>
arXiv:2512.05492v1 Announce Type: new 
Abstract: Underwater video pairs are fairly difficult to obtain due to the complex underwater imaging. In this case, most existing video underwater enhancement methods are performed by directly applying the single-image enhancement model frame by frame, but a natural issue is lacking temporal consistency. To relieve the problem, we rethink the temporal manifold inherent in natural videos and observe a temporal consistency prior in dynamic scenes from the local temporal frequency perspective. Building upon the specific prior and no paired-data condition, we propose an implicit representation manner for enhanced video signals, which is conducted in the wavelet-based temporal consistency field, WaterWave. Specifically, under the constraints of the prior, we progressively filter and attenuate the inconsistent components while preserving motion details and scenes, achieving a natural-flowing video. Furthermore, to represent temporal frequency bands more accurately, an underwater flow correction module is designed to rectify estimated flows considering the transmission in underwater scenes. Extensive experiments demonstrate that WaterWave significantly enhances the quality of videos generated using single-image underwater enhancements. Additionally, our method demonstrates high potential in downstream underwater tracking tasks, such as UOSTrack and MAT, outperforming the original video by a large margin, i.e., 19.7% and 9.7% on precise respectively.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding with Structured Awareness: Integrating Directional, Frequency-Spatial, and Structural Attention for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.05494</link>
<guid>https://arxiv.org/abs/2512.05494</guid>
<content:encoded><![CDATA[
<div> Adaptive Cross-Fusion Attention, Triple Feature Fusion Attention, Structural-aware Multi-scale Masking, Medical Image Segmentation, Frequency-Spatial Representation  

<br /><br />Summary:  
This paper introduces a novel decoder framework tailored for medical image segmentation to overcome the limitations of traditional Transformer decoders in capturing fine edge details, local textures, and spatial continuity. The framework consists of three main modules: first, the Adaptive Cross-Fusion Attention (ACFA) module, which integrates channel enhancement with spatial attention and incorporates learnable guidance along planar, horizontal, and vertical directions, improving focus on key regions and structural orientations. Second, the Triple Feature Fusion Attention (TFFA) module combines features from Spatial, Fourier, and Wavelet domains, enabling joint frequency-spatial representation that enhances global dependency and structural modeling, while preserving local edge and texture information, particularly useful in scenarios with complex or blurred boundaries. Third, the Structural-aware Multi-scale Masking Module (SMMM) refines skip connections between encoder and decoder by utilizing multi-scale context and structural saliency filtering, reducing feature redundancy and boosting semantic interaction quality. These components work synergistically to address decoder shortcomings and significantly improve segmentation performance in high-precision tasks such as tumor segmentation and organ boundary extraction. Experimental results confirm that this framework delivers both high accuracy and strong generalization ability, making it an efficient, practical solution for medical image segmentation challenges. <div>
arXiv:2512.05494v1 Announce Type: new 
Abstract: To address the limitations of Transformer decoders in capturing edge details, recognizing local textures and modeling spatial continuity, this paper proposes a novel decoder framework specifically designed for medical image segmentation, comprising three core modules. First, the Adaptive Cross-Fusion Attention (ACFA) module integrates channel feature enhancement with spatial attention mechanisms and introduces learnable guidance in three directions (planar, horizontal, and vertical) to enhance responsiveness to key regions and structural orientations. Second, the Triple Feature Fusion Attention (TFFA) module fuses features from Spatial, Fourier and Wavelet domains, achieving joint frequency-spatial representation that strengthens global dependency and structural modeling while preserving local information such as edges and textures, making it particularly effective in complex and blurred boundary scenarios. Finally, the Structural-aware Multi-scale Masking Module (SMMM) optimizes the skip connections between encoder and decoder by leveraging multi-scale context and structural saliency filtering, effectively reducing feature redundancy and improving semantic interaction quality. Working synergistically, these modules not only address the shortcomings of traditional decoders but also significantly enhance performance in high-precision tasks such as tumor segmentation and organ boundary extraction, improving both segmentation accuracy and model generalization. Experimental results demonstrate that this framework provides an efficient and practical solution for medical image segmentation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Infrared Small Target Detection: A Foundation-Driven Efficient Paradigm</title>
<link>https://arxiv.org/abs/2512.05511</link>
<guid>https://arxiv.org/abs/2512.05511</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Foundation Models, Infrared Small Target Detection, Semantic Alignment, Self-Distillation, Evaluation Metric<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting single-frame infrared small targets (SIRST), leveraging large-scale Visual Foundation Models (VFMs) which have not been previously explored in this domain. The authors propose a Foundation-Driven Efficient Paradigm (FDEP) that integrates frozen representations from VFMs into existing encoder-decoder architectures, improving detection accuracy without increasing inference time. A key component, the Semantic Alignment Modulation Fusion (SAMF) module, dynamically aligns and deeply fuses global semantic priors from VFMs with task-specific features for better performance. To mitigate inference overhead, the Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy is introduced; this involves parameter sharing and synchronized backpropagation between a main and lightweight branch to enable implicit semantic transfer. Additionally, the authors identify fragmentation in current evaluation methods and propose a unified Holistic SIRST Evaluation (HSE) metric, which performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness for a more stable and comprehensive comparison. Extensive experiments demonstrate that integrating FDEP into existing SIRST networks achieves state-of-the-art results across multiple public datasets. The authors also provide their codebase publicly for reproducibility and further research. <div>
arXiv:2512.05511v1 Announce Type: new 
Abstract: While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know-Show: Benchmarking Video-Language Models on Spatio-Temporal Grounded Reasoning</title>
<link>https://arxiv.org/abs/2512.05513</link>
<guid>https://arxiv.org/abs/2512.05513</guid>
<content:encoded><![CDATA[
<div> Keywords: Video-Language Models, spatio-temporal reasoning, Know-Show benchmark, GRAM plug-in, fine-grained grounding  

<br /><br />Summary:  
This paper introduces Know-Show, a novel benchmark designed to evaluate spatio-temporal grounded reasoning in video-language models (Video-LMs). Know-Show assesses a model’s ability to reason about actions and their semantics while grounding inferences in both visual and temporal evidence. The benchmark integrates reasoning and localization in a unified framework consisting of five scenarios across spatial (person, object, person-object, hand-object) and temporal dimensions. It is built using datasets Charades, Action Genome, and Ego4D, and includes 2.5K human-authored questions to thoroughly test models. Experimental results highlight significant performance gaps between current state-of-the-art Video-LMs (such as Qwen, VideoLLaVA, GPT-4o, and Gemini) and human reasoning capacities, particularly in handling fine-grained hand-object interactions. To address these issues, the authors propose GRAM, a training-free plug-in that enhances Video-LMs by applying attention-based video token selection and explicit timestamp encoding to achieve fine-grained grounding without retraining the models. Through extensive experiments, Know-Show reveals that existing models struggle to concurrently "show what they know" by connecting reasoning with clear evidence in video data. The benchmark thus sets a unified standard for evaluating grounded reasoning, aiming to guide development towards more interpretable and reliable multimodal reasoning systems. The project’s codebase will be publicly released to foster further research. <div>
arXiv:2512.05513v1 Announce Type: new 
Abstract: Large Video-Language Models (Video-LMs) have achieved impressive progress in multimodal understanding, yet their reasoning remains weakly grounded in space and time. We present Know-Show, a new benchmark designed to evaluate spatio-temporal grounded reasoning, the ability of a model to reason about actions and their semantics while simultaneously grounding its inferences in visual and temporal evidence. Know-Show unifies reasoning and localization within a single evaluation framework consisting of five complementary scenarios across spatial (person, object, person-object, and hand-object) and temporal dimensions. Built from Charades, Action Genome, and Ego4D with 2.5K human-authored questions, the benchmark exposes significant gaps between current Video-LMs and human reasoning. To bridge this gap, we propose GRAM, a training-free plug-in that augments Video-LMs with fine-grained grounding through attention-based video token selection and explicit timestamp encoding. Extensive experiments across open and closed Video-LMs (Qwen, VideoLLaVA, GPT-4o, and Gemini, etc.) reveal that existing models struggle to "show what they know" and vice versa, especially in fine-grained hand-object interactions. Know-Show establishes a unified standard for assessing grounded reasoning in video-language understanding and provides insights toward developing interpretable and reliable multimodal reasoning systems. We will release the code at https://github.com/LUNAProject22/Know-Show.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.05515</link>
<guid>https://arxiv.org/abs/2512.05515</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Sentiment Analysis, Alignment, Fusion, Contrastive Learning, Hierarchical Bottleneck Fusion  

<br /><br />Summary:  
Multimodal sentiment analysis (MSA) aims to combine different data modalities such as text, image, and audio to better understand sentiment. A major challenge in MSA is effectively aligning and fusing multimodal features. Alignment involves synchronizing temporal sequences and semantic content across different modalities, while fusion integrates these aligned features into a unified representation. Existing approaches typically handle alignment or fusion separately, which limits their overall effectiveness and efficiency. To address this, the paper proposes DashFusion, a novel framework consisting of two main components. First, the dual-stream alignment module performs temporal alignment using cross-modal attention to match frame-level information and semantic alignment through contrastive learning to maintain feature-space consistency. Second, supervised contrastive learning exploits label information to further enhance modality features. For fusion, DashFusion introduces hierarchical bottleneck fusion, which progressively compresses and integrates multimodal features using bottleneck tokens, balancing accuracy and computational load. The framework is evaluated on three popular datasets—CMU-MOSI, CMU-MOSEI, and CH-SIMS—demonstrating state-of-the-art results across multiple metrics. Ablation studies validate the effectiveness of the proposed alignment and fusion strategies. The authors have open-sourced their code for reproducibility and further research at the provided GitHub repository. <div>
arXiv:2512.05515v1 Announce Type: new 
Abstract: Multimodal sentiment analysis (MSA) integrates various modalities, such as text, image, and audio, to provide a more comprehensive understanding of sentiment. However, effective MSA is challenged by alignment and fusion issues. Alignment requires synchronizing both temporal and semantic information across modalities, while fusion involves integrating these aligned features into a unified representation. Existing methods often address alignment or fusion in isolation, leading to limitations in performance and efficiency. To tackle these issues, we propose a novel framework called Dual-stream Alignment with Hierarchical Bottleneck Fusion (DashFusion). Firstly, dual-stream alignment module synchronizes multimodal features through temporal and semantic alignment. Temporal alignment employs cross-modal attention to establish frame-level correspondences among multimodal sequences. Semantic alignment ensures consistency across the feature space through contrastive learning. Secondly, supervised contrastive learning leverages label information to refine the modality features. Finally, hierarchical bottleneck fusion progressively integrates multimodal information through compressed bottleneck tokens, which achieves a balance between performance and computational efficiency. We evaluate DashFusion on three datasets: CMU-MOSI, CMU-MOSEI, and CH-SIMS. Experimental results demonstrate that DashFusion achieves state-of-the-art performance across various metrics, and ablation studies confirm the effectiveness of our alignment and fusion techniques. The codes for our experiments are available at https://github.com/ultramarineX/DashFusion.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation</title>
<link>https://arxiv.org/abs/2512.05524</link>
<guid>https://arxiv.org/abs/2512.05524</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatio-temporal scene graph generation, vision-language models, dual-source query initialization, multi-modal feature bank, Action Genome dataset<br /><br />Summary:<br /><br />1. Spatio-temporal scene graph generation (ST-SGG) aims to represent objects and their dynamic relationships across video frames to facilitate tasks like video captioning and visual question answering.  
2. Current DETR-style single-stage ST-SGG models face challenges, particularly due to semantically uninformed and instance-agnostic learnable queries used for attention mechanisms.  
3. These models also depend solely on unimodal visual features for predicate classification, limiting their reasoning capabilities.  
4. The proposed VOST-SGG framework integrates vision-language models (VLMs), leveraging their common sense reasoning to improve ST-SGG by introducing a dual-source query initialization that separates semantic content ("what to attend to") from spatial location ("where to attend").  
5. VOST-SGG incorporates a multi-modal feature bank that combines visual, textual, and spatial information from VLMs for enhanced predicate classification.  
6. Extensive experiments on the Action Genome dataset confirm the approach achieves state-of-the-art performance, demonstrating the effectiveness of integrating VLM-aided semantic priors and multi-modal features into the ST-SGG pipeline.  
7. The authors plan to release the code to promote further research and application development at https://github.com/LUNAProject22/VOST. <div>
arXiv:2512.05524v1 Announce Type: new 
Abstract: Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at https://github.com/LUNAProject22/VOST.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors</title>
<link>https://arxiv.org/abs/2512.05529</link>
<guid>https://arxiv.org/abs/2512.05529</guid>
<content:encoded><![CDATA[
<div> Depth estimation, surgical scene segmentation, monocular depth, vision foundation models, template matching<br /><br />Summary:<br /><br />1. The article addresses the challenge of pixel-wise segmentation in laparoscopic surgical scenes, which is crucial for computer-assisted surgery but hampered by the high cost of creating dense annotations.<br />2. The authors propose a novel framework named Depth-guided Surgical Scene Segmentation (DepSeg), which operates without the need for training by leveraging monocular depth information as a geometric prior in combination with pretrained vision foundation models.<br />3. DepSeg works by first estimating a relative depth map using a pretrained monocular depth estimation network, which helps generate depth-guided point prompts.<br />4. These prompts are input into SAM2 (Segment Anything Model version 2) to produce class-agnostic segmentation masks.<br />5. Each mask is then described using pooled pretrained visual features and classified by matching against a template bank created from annotated frames.<br />6. Experimental results on the CholecSeg8k dataset demonstrate that DepSeg significantly outperforms a baseline SAM2 auto segmentation method, achieving 35.9% mean Intersection over Union (mIoU) compared to 14.7%.<br />7. Moreover, DepSeg maintains competitive segmentation performance even when using only 10–20% of the available object templates, illustrating its annotation efficiency.<br />8. The study concludes that depth-guided prompting together with template-based classification offers a promising and scalable approach for surgical scene segmentation without heavy annotation requirements. <div>
arXiv:2512.05529v1 Announce Type: new 
Abstract: Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ideal Observer for Segmentation of Dead Leaves Images</title>
<link>https://arxiv.org/abs/2512.05539</link>
<guid>https://arxiv.org/abs/2512.05539</guid>
<content:encoded><![CDATA[
<div> Keywords: dead leaves model, Bayesian ideal observer, image segmentation, occlusion, generative model

<br /><br />Summary:  
1. The human visual environment consists of numerous surfaces arranged spatially, where visibility depends on occlusions created by overlapping objects.  
2. Dead leaves models simulate these occlusions by layering randomly sampled objects ("leaves") from distributions of position, shape, color, and texture to generate images.  
3. Image generation in dead leaves models continues until the image is fully covered or a preset number of leaves is sampled.  
4. The paper develops a theoretical Bayesian ideal observer framework to infer the partitioning of a set of pixels based on independent distributions within the dead leaves model.  
5. Detailed step-by-step computations for the posterior probability are provided, alongside an analysis of the practical feasibility of implementing this ideal observer approach.  
6. The model and its associated ideal observer serve as an upper bound on segmentation performance for a limited set of pixels, offering a benchmark for comparing human vision and computer vision algorithms in segmentation tasks. <div>
arXiv:2512.05539v1 Announce Type: new 
Abstract: The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider "dead leaves" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects ("leaves") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.05546</link>
<guid>https://arxiv.org/abs/2512.05546</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Text Inertia, Cognitive Demand Sensor, Attention Reorientation, Game-Theoretic Interpretability<br /><br />Summary:  
This paper addresses the problem of text inertia in large Vision-Language Models (VLMs), where model attention drifts away from visual inputs towards linguistic priors, causing object hallucinations. Existing methods either intervene only at the output logits, which cannot correct internal reasoning drifts, or rely on heuristic controls lacking theoretical foundation. The authors propose Conscious Gaze (CG-VLM), a novel framework that operates during inference without retraining the model. CG-VLM leverages game-theoretic interpretability via a Cognitive Demand Sensor that uses Harsanyi interactions to estimate real-time vision-text synergy and detect when the model needs stronger visual grounding. When necessary, a Focused Consensus Induction module selectively redirects mid-layer attention back to visual tokens, preventing collapse into text prior biases. Experimental results on benchmarks including POPE and CHAIR demonstrate that CG-VLM improves performance across multiple VLM architectures such as InstructBLIP, LLaVA, Qwen-VL, and mPLUG. Crucially, this approach preserves general model capabilities while enabling precise, context-aware attention intervention at the token level. The study shows that integrating interpretable, token-level sensing mechanisms can effectively mitigate hallucinations and enhance visual grounding in large multimodal models without compromising foundational knowledge. <div>
arXiv:2512.05546v1 Announce Type: new 
Abstract: Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency</title>
<link>https://arxiv.org/abs/2512.05557</link>
<guid>https://arxiv.org/abs/2512.05557</guid>
<content:encoded><![CDATA[
<div> Keywords: sequential identity consistency, transient attribute control, stylized narrative dataset, Human-in-the-Loop pipeline, decoupled control scheme<br /><br />Summary:<br />1. The paper addresses the challenge of maintaining sequential identity consistency in visual storytelling, focusing on precise control over transient attributes such as pose, expression, and scene composition. <br />2. Current datasets fall short in providing high-fidelity data that disentangles stable character identities from varying transient attributes, limiting controllable and reliable visual narrative synthesis.<br />3. To overcome these limitations, the authors introduce "2K-Characters-10K-Stories," a large-scale multi-modal dataset featuring 2,000 uniquely stylized characters appearing across 10,000 illustration stories, explicitly pairing stable identities with decoupled control signals.<br />4. A novel Human-in-the-Loop (HiL) pipeline is proposed, combining expert-verified character templates with large language model (LLM)-guided narrative planning to generate structured and highly aligned data.<br />5. The dataset employs a decoupled control scheme separating persistent identity features from transient attributes and integrates a Quality-Gated loop that combines multimodal language model (MMLM) evaluation, Auto-Prompt Tuning, and Local Image Editing to ensure pixel-level consistency.<br />6. Experimental results show that models fine-tuned on this dataset achieve comparable performance to closed-source systems in generating coherent and visually consistent storytelling sequences. <div>
arXiv:2512.05557v1 Announce Type: new 
Abstract: Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \textbf{2{,}000} uniquely stylized characters appearing across \textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProPhy: Progressive Physical Alignment for Dynamic World Simulation</title>
<link>https://arxiv.org/abs/2512.05564</link>
<guid>https://arxiv.org/abs/2512.05564</guid>
<content:encoded><![CDATA[
<div> Physics-aware video generation, Mixture-of-Physics-Experts, anisotropic generation, physical alignment, vision-language models<br /><br />Summary:<br /><br />This paper addresses the challenge of generating videos that are physically consistent, especially in cases involving large-scale or complex physical dynamics where current models fall short due to isotropic responses to physical prompts and lack of localized physical cue alignment. The authors propose ProPhy, a Progressive Physical Alignment Framework designed to incorporate explicit physics-aware conditioning and enable anisotropic video generation. ProPhy introduces a novel two-stage Mixture-of-Physics-Experts (MoPE) mechanism, consisting of Semantic Experts that infer high-level physical principles from textual inputs and Refinement Experts that capture fine-grained, token-level physical dynamics. This dual-expert approach facilitates learning of detailed, physics-aligned video representations that adhere more closely to physical laws. Additionally, the framework includes a physical alignment strategy to transfer physical reasoning abilities from vision-language models (VLMs) into the Refinement Experts, enhancing the model’s capacity to accurately represent dynamic physical phenomena. Extensive experimental evaluation on established physics-aware video generation benchmarks demonstrates that ProPhy outperforms current state-of-the-art methods by producing videos that are more realistic, dynamic, and physically coherent. The results indicate significant progress towards creating advanced world simulators based on video generation. <div>
arXiv:2512.05564v1 Announce Type: new 
Abstract: Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging</title>
<link>https://arxiv.org/abs/2512.05571</link>
<guid>https://arxiv.org/abs/2512.05571</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image registration, diffusion models, voxel descriptors, spatial correspondence, lung CT

<br /><br />Summary: Accurate spatial correspondence between medical images is critical for applications such as longitudinal analysis, lesion tracking, and image-guided interventions. Traditional medical image registration methods rely on local intensity-based similarity measures, which often fail to capture global semantic structures and struggle in low-contrast or anatomically variable regions. This study introduces MedDIFT, a novel 3D correspondence framework that is training-free and leverages multi-scale features extracted from a pretrained latent medical diffusion model as voxel-wise descriptors. MedDIFT integrates diffusion activations to form rich descriptors which are matched via cosine similarity, and optionally refined using a local-search prior. Evaluation on a publicly available lung CT dataset demonstrates that MedDIFT achieves correspondence accuracy on par with the state-of-the-art learning-based UniGradICON model and outperforms traditional B-spline-based registration methods, all without requiring any task-specific model training. Ablation studies reveal that fusing multi-level diffusion features and applying a modest diffusion noise level positively impact performance, confirming the method's robustness and effectiveness. This work highlights the potential of leveraging pretrained diffusion model features for improved medical image registration without the need for retraining or supervision. <div>
arXiv:2512.05571v1 Announce Type: new 
Abstract: Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions. Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions. Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information. We present MedDIFT, a training-free 3D correspondence framework that leverages multi-scale features from a pretrained latent medical diffusion model as voxel descriptors. MedDIFT fuses diffusion activations into rich voxel-wise descriptors and matches them via cosine similarity, with an optional local-search prior. On a publicly available lung CT dataset, MedDIFT achieves correspondence accuracy comparable to the state-of-the-art learning-based UniGradICON model and surpasses conventional B-spline-based registration, without requiring any task-specific model training. Ablation experiments confirm that multi-level feature fusion and modest diffusion noise improve performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning High-Fidelity Cloth Animation via Skinning-Free Image Transfer</title>
<link>https://arxiv.org/abs/2512.05593</link>
<guid>https://arxiv.org/abs/2512.05593</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D garment deformation, skinning-free, vertex position and normal, high-frequency wrinkles, image-based texture encoding<br /><br />Summary:  
The paper introduces a novel skinning-free method for generating 3D garment deformations from given body poses, crucial for applications such as virtual try-on and extended reality. Traditional methods rely on linear blend skinning to capture low-frequency garment shapes and regress high-frequency wrinkles, but they suffer from misalignment due to lack of explicit skinning supervision. To overcome this, the authors independently estimate vertex positions for low-frequency shapes and vertex normals for high-frequency wrinkle details, enabling effective decoupling and direct supervision via garment geometry. To enhance animation quality, both vertex attributes are encoded as rendered texture images, facilitating 3D deformation through 2D image transfer. This approach leverages powerful pretrained image models to recover fine-grained wrinkle details while supporting multiple garment topologies without manual UV mapping. Additionally, a multimodal fusion strategy integrates constraints from both frequency modalities for robust garment deformation recovery from transferred images. Experimental results demonstrate that the method significantly improves animation realism across various garment types and outperforms state-of-the-art approaches by recovering finer wrinkles and more accurate garment shapes. <div>
arXiv:2512.05593v1 Announce Type: new 
Abstract: We present a novel method for generating 3D garment deformations from given body poses, which is key to a wide range of applications, including virtual try-on and extended reality. To simplify the cloth dynamics, existing methods mostly rely on linear blend skinning to obtain low-frequency posed garment shape and only regress high-frequency wrinkles. However, due to the lack of explicit skinning supervision, such skinning-based approach often produces misaligned shapes when posing the garment, consequently corrupts the high-frequency signals and fails to recover high-fidelity wrinkles. To tackle this issue, we propose a skinning-free approach by independently estimating posed (i) vertex position for low-frequency posed garment shape, and (ii) vertex normal for high-frequency local wrinkle details. In this way, each frequency modality can be effectively decoupled and directly supervised by the geometry of the deformed garment. To further improve the visual quality of animation, we propose to encode both vertex attributes as rendered texture images, so that 3D garment deformation can be equivalently achieved via 2D image transfer. This enables us to leverage powerful pretrained image models to recover fine-grained visual details in wrinkles, while maintaining superior scalability for garments of diverse topologies without relying on manual UV partition. Finally, we propose a multimodal fusion to incorporate constraints from both frequency modalities and robustly recover deformed 3D garments from transferred images. Extensive experiments show that our method significantly improves animation quality on various garment types and recovers finer wrinkles than state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction</title>
<link>https://arxiv.org/abs/2512.05597</link>
<guid>https://arxiv.org/abs/2512.05597</guid>
<content:encoded><![CDATA[
<div> Keywords: Fast SceneScript, multi-token prediction, 3D scene layout estimation, self-speculative decoding, confidence-guided decoding  

<br /><br />Summary:  
This paper introduces Fast SceneScript, a structured language model designed for efficient and accurate 3D scene layout estimation. Traditional perception-generalist language models rely on autoregressive next-token prediction, which is inherently slow due to sequential token generation. To address this, Fast SceneScript employs multi-token prediction (MTP), enabling the model to generate multiple tokens per decoding step and thereby significantly speeding up inference. However, generating multiple tokens simultaneously can introduce unreliable predictions, potentially affecting accuracy. To mitigate this, the authors adapt self-speculative decoding (SSD) tailored for structured language models and propose confidence-guided decoding (CGD), which uses an improved scoring mechanism to assess and filter token reliability effectively. Additionally, the method includes a parameter-efficient design to minimize the overhead introduced by MTP, adding only about 7.5% more parameters. Extensive experiments conducted on the ASE and Structured3D benchmarks validate the model's ability to produce up to nine tokens per decoder step with no loss in accuracy, demonstrating a good balance between speed and performance. This approach presents a valuable advancement in accelerating 3D scene layout tasks while maintaining state-of-the-art accuracy. <div>
arXiv:2512.05597v1 Announce Type: new 
Abstract: Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\sim7.5\%$ additional parameters.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NormalView: sensor-agnostic tree species classification from backpack and aerial lidar data using geometric projections</title>
<link>https://arxiv.org/abs/2512.05610</link>
<guid>https://arxiv.org/abs/2512.05610</guid>
<content:encoded><![CDATA[
arXiv:2512.05610v1 Announce Type: new 
Abstract: Laser scanning has proven to be an invaluable tool in assessing the decomposition of forest environments. Mobile laser scanning (MLS) has shown to be highly promising for extremely accurate, tree level inventory. In this study, we present NormalView, a sensor-agnostic projection-based deep learning method for classifying tree species from point cloud data. NormalView embeds local geometric information into two-dimensional projections, in the form of normal vector estimates, and uses the projections as inputs to an image classification network, YOLOv11. In addition, we inspected the effect of multispectral radiometric intensity information on classification performance. We trained and tested our model on high-density MLS data (7 species, ~5000 pts/m^2), as well as high-density airborne laser scanning (ALS) data (9 species, >1000 pts/m^2). On the MLS data, NormalView achieves an overall accuracy (macro-average accuracy) of 95.5 % (94.8 %), and 91.8 % (79.1 %) on the ALS data. We found that having intensity information from multiple scanners provides benefits in tree species classification, and the best model on the multispectral ALS dataset was a model using intensity information from all three channels of the multispectral ALS. This study demonstrates that projection-based methods, when enhanced with geometric information and coupled with state-of-the-art image classification backbones, can achieve exceptional results. Crucially, these methods are sensor-agnostic, relying only on geometric information. Additionally, we publically release the MLS dataset used in the study.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model</title>
<link>https://arxiv.org/abs/2512.05613</link>
<guid>https://arxiv.org/abs/2512.05613</guid>
<content:encoded><![CDATA[
arXiv:2512.05613v1 Announce Type: new 
Abstract: Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples. This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time. To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model's parameters through a teacher--student distillation process. By internalizing few-shot reasoning into a dedicated layer within the student network, DistillFSS eliminates the need for support images at test time, enabling fast, lightweight inference, while allowing efficient extension to novel classes in unseen domains through rapid teacher-driven specialization. Combined with fine-tuning, the approach scales efficiently to large support sets and significantly reduces computational overhead. To evaluate the framework under realistic conditions, we introduce a new CD-FSS benchmark spanning medical imaging, industrial inspection, and remote sensing, with disjoint label spaces and variable support sizes. Experiments show that DistillFSS matches or surpasses state-of-the-art baselines, particularly in multi-class and multi-shot scenarios, while offering substantial efficiency gains. The code is available at https://github.com/pasqualedem/DistillFSS.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Experts-Guided Unbalanced Optimal Transport for ISP Learning from Unpaired and/or Paired Data</title>
<link>https://arxiv.org/abs/2512.05635</link>
<guid>https://arxiv.org/abs/2512.05635</guid>
<content:encoded><![CDATA[
arXiv:2512.05635v1 Announce Type: new 
Abstract: Learned Image Signal Processing (ISP) pipelines offer powerful end-to-end performance but are critically dependent on large-scale paired raw-to-sRGB datasets. This reliance on costly-to-acquire paired data remains a significant bottleneck. To address this challenge, we introduce a novel, unsupervised training framework based on Optimal Transport capable of training arbitrary ISP architectures in both unpaired and paired modes. We are the first to successfully apply Unbalanced Optimal Transport (UOT) for this complex, cross-domain translation task. Our UOT-based framework provides robustness to outliers in the target sRGB data, allowing it to discount atypical samples that would be prohibitively costly to map. A key component of our framework is a novel ``committee of expert discriminators,'' a hybrid adversarial regularizer. This committee guides the optimal transport mapping by providing specialized, targeted gradients to correct specific ISP failure modes, including color fidelity, structural artifacts, and frequency-domain realism. To demonstrate the superiority of our approach, we retrained existing state-of-the-art ISP architectures using our paired and unpaired setups. Our experiments show that while our framework, when trained in paired mode, exceeds the performance of the original paired methods across all metrics, our unpaired mode concurrently achieves quantitative and qualitative performance that rivals, and in some cases surpasses, the original paired-trained counterparts. The code and pre-trained models are available at: https://github.com/gosha20777/EGUOT-ISP.git.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective</title>
<link>https://arxiv.org/abs/2512.05651</link>
<guid>https://arxiv.org/abs/2512.05651</guid>
<content:encoded><![CDATA[
arXiv:2512.05651v1 Announce Type: new 
Abstract: The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata -- specifically exchangeable image file format (EXIF) tags -- to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (\eg, camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (\eg, focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection</title>
<link>https://arxiv.org/abs/2512.05663</link>
<guid>https://arxiv.org/abs/2512.05663</guid>
<content:encoded><![CDATA[
arXiv:2512.05663v1 Announce Type: new 
Abstract: Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-Based Real-Time Sequential Facial Expression Analysis Using Geometric Features</title>
<link>https://arxiv.org/abs/2512.05669</link>
<guid>https://arxiv.org/abs/2512.05669</guid>
<content:encoded><![CDATA[
arXiv:2512.05669v1 Announce Type: new 
Abstract: Facial expression recognition is a crucial component in enhancing human-computer interaction and developing emotion-aware systems. Real-time detection and interpretation of facial expressions have become increasingly important for various applications, from user experience personalization to intelligent surveillance systems. This study presents a novel approach to real-time sequential facial expression recognition using deep learning and geometric features. The proposed method utilizes MediaPipe FaceMesh for rapid and accurate facial landmark detection. Geometric features, including Euclidean distances and angles, are extracted from these landmarks. Temporal dynamics are incorporated by analyzing feature differences between consecutive frames, enabling the detection of onset, apex, and offset phases of expressions. For classification, a ConvLSTM1D network followed by multilayer perceptron blocks is employed. The method's performance was evaluated on multiple publicly available datasets, including CK+, Oulu-CASIA (VIS and NIR), and MMI. Accuracies of 93%, 79%, 77%, and 68% were achieved respectively. Experiments with composite datasets were also conducted to assess the model's generalization capabilities. The approach demonstrated real-time applicability, processing approximately 165 frames per second on consumer-grade hardware. This research contributes to the field of facial expression analysis by providing a fast, accurate, and adaptable solution. The findings highlight the potential for further advancements in emotion-aware technologies and personalized user experiences, paving the way for more sophisticated human-computer interaction systems. To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub: https://github.com/miralab-ai/facial-expression-analysis.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem</title>
<link>https://arxiv.org/abs/2512.05672</link>
<guid>https://arxiv.org/abs/2512.05672</guid>
<content:encoded><![CDATA[
arXiv:2512.05672v1 Announce Type: new 
Abstract: Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperspectral Unmixing with 3D Convolutional Sparse Coding and Projected Simplex Volume Maximization</title>
<link>https://arxiv.org/abs/2512.05674</link>
<guid>https://arxiv.org/abs/2512.05674</guid>
<content:encoded><![CDATA[
arXiv:2512.05674v1 Announce Type: new 
Abstract: Hyperspectral unmixing (HSU) aims to separate each pixel into its constituent endmembers and estimate their corresponding abundance fractions. This work presents an algorithm-unrolling-based network for the HSU task, named the 3D Convolutional Sparse Coding Network (3D-CSCNet), built upon a 3D CSC model. Unlike existing unrolling-based networks, our 3D-CSCNet is designed within the powerful autoencoder (AE) framework. Specifically, to solve the 3D CSC problem, we propose a 3D CSC block (3D-CSCB) derived through deep algorithm unrolling. Given a hyperspectral image (HSI), 3D-CSCNet employs the 3D-CSCB to estimate the abundance matrix. The use of 3D CSC enables joint learning of spectral and spatial relationships in the 3D HSI data cube. The estimated abundance matrix is then passed to the AE decoder to reconstruct the HSI, and the decoder weights are extracted as the endmember matrix. Additionally, we propose a projected simplex volume maximization (PSVM) algorithm for endmember estimation, and the resulting endmembers are used to initialize the decoder weights of 3D-CSCNet. Extensive experiments on three real datasets and one simulated dataset with three different signal-to-noise ratio (SNR) levels demonstrate that our 3D-CSCNet outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Graph Neural Network with Frequency-Aware Learning for Optical Aberration Correction</title>
<link>https://arxiv.org/abs/2512.05683</link>
<guid>https://arxiv.org/abs/2512.05683</guid>
<content:encoded><![CDATA[
arXiv:2512.05683v1 Announce Type: new 
Abstract: Optical aberrations significantly degrade image quality in microscopy, particularly when imaging deeper into samples. These aberrations arise from distortions in the optical wavefront and can be mathematically represented using Zernike polynomials. Existing methods often address only mild aberrations on limited sample types and modalities, typically treating the problem as a black-box mapping without leveraging the underlying optical physics of wavefront distortions. We propose ZRNet, a physics-informed framework that jointly performs Zernike coefficient prediction and optical image Restoration. We contribute a Zernike Graph module that explicitly models physical relationships between Zernike polynomials based on their azimuthal degrees-ensuring that learned corrections align with fundamental optical principles. To further enforce physical consistency between image restoration and Zernike prediction, we introduce a Frequency-Aware Alignment (FAA) loss, which better aligns Zernike coefficient prediction and image features in the Fourier domain. Extensive experiments on CytoImageNet demonstrates that our approach achieves state-of-the-art performance in both image restoration and Zernike coefficient prediction across diverse microscopy modalities and biological samples with complex, large-amplitude aberrations. Code is available at https://github.com/janetkok/ZRNet.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning</title>
<link>https://arxiv.org/abs/2512.05698</link>
<guid>https://arxiv.org/abs/2512.05698</guid>
<content:encoded><![CDATA[
arXiv:2512.05698v1 Announce Type: new 
Abstract: Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning</title>
<link>https://arxiv.org/abs/2512.05710</link>
<guid>https://arxiv.org/abs/2512.05710</guid>
<content:encoded><![CDATA[
arXiv:2512.05710v1 Announce Type: new 
Abstract: Point cloud completion seeks to recover geometrically consistent shapes from partial or sparse 3D observations. Although recent methods have achieved reasonable global shape reconstruction, they often rely on Euclidean proximity and overlook the intrinsic nonlinear geometric structure of point clouds, resulting in suboptimal geometric consistency and semantic ambiguity. In this paper, we present a manifold-aware point cloud completion framework that explicitly incorporates nonlinear geometry information throughout the feature learning pipeline. Our approach introduces two key modules: a Geodesic Distance Approximator (GDA), which estimates geodesic distances between points to capture the latent manifold topology, and a Manifold-Aware Feature Extractor (MAFE), which utilizes geodesic-based $k$-NN groupings and a geodesic-relational attention mechanism to guide the hierarchical feature extraction process. By integrating geodesic-aware relational attention, our method promotes semantic coherence and structural fidelity in the reconstructed point clouds. Extensive experiments on benchmark datasets demonstrate that our approach consistently outperforms state-of-the-art methods in reconstruction quality.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision</title>
<link>https://arxiv.org/abs/2512.05740</link>
<guid>https://arxiv.org/abs/2512.05740</guid>
<content:encoded><![CDATA[
arXiv:2512.05740v1 Announce Type: new 
Abstract: Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05746</link>
<guid>https://arxiv.org/abs/2512.05746</guid>
<content:encoded><![CDATA[
arXiv:2512.05746v1 Announce Type: new 
Abstract: Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>USV: Unified Sparsification for Accelerating Video Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05754</link>
<guid>https://arxiv.org/abs/2512.05754</guid>
<content:encoded><![CDATA[
arXiv:2512.05754v1 Announce Type: new 
Abstract: The scalability of high-fidelity video diffusion models (VDMs) is constrained by two key sources of redundancy: the quadratic complexity of global spatio-temporal attention and the computational overhead of long iterative denoising trajectories. Existing accelerators -- such as sparse attention and step-distilled samplers -- typically target a single dimension in isolation and quickly encounter diminishing returns, as the remaining bottlenecks become dominant. In this work, we introduce USV (Unified Sparsification for Video diffusion models), an end-to-end trainable framework that overcomes this limitation by jointly orchestrating sparsification across both the model's internal computation and its sampling process. USV learns a dynamic, data- and timestep-dependent sparsification policy that prunes redundant attention connections, adaptively merges semantically similar tokens, and reduces denoising steps, treating them not as independent tricks but as coordinated actions within a single optimization objective. This multi-dimensional co-design enables strong mutual reinforcement among previously disjoint acceleration strategies. Extensive experiments on large-scale video generation benchmarks demonstrate that USV achieves up to 83.3% speedup in the denoising process and 22.7% end-to-end acceleration, while maintaining high visual fidelity. Our results highlight unified, dynamic sparsification as a practical path toward efficient, high-quality video generation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-Efficient Point Cloud Segmentation with Active Learning</title>
<link>https://arxiv.org/abs/2512.05759</link>
<guid>https://arxiv.org/abs/2512.05759</guid>
<content:encoded><![CDATA[
arXiv:2512.05759v1 Announce Type: new 
Abstract: Semantic segmentation of 3D point cloud data often comes with high annotation costs. Active learning automates the process of selecting which data to annotate, reducing the total amount of annotation needed to achieve satisfactory performance. Recent approaches to active learning for 3D point clouds are often based on sophisticated heuristics for both, splitting point clouds into annotatable regions and selecting the most beneficial for further neural network training. In this work, we propose a novel and easy-to-implement strategy to separate the point cloud into annotatable regions. In our approach, we utilize a 2D grid to subdivide the point cloud into columns. To identify the next data to be annotated, we employ a network ensemble to estimate the uncertainty in the network output. We evaluate our method on the S3DIS dataset, the Toronto-3D dataset, and a large-scale urban 3D point cloud of the city of Freiburg, which we labeled in parts manually. The extensive evaluation shows that our method yields performance on par with, or even better than, complex state-of-the-art methods on all datasets. Furthermore, we provide results suggesting that in the context of point clouds the annotated area can be a more meaningful measure for active learning algorithms than the number of annotated points.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FNOPT: Resolution-Agnostic, Self-Supervised Cloth Simulation using Meta-Optimization with Fourier Neural Operators</title>
<link>https://arxiv.org/abs/2512.05762</link>
<guid>https://arxiv.org/abs/2512.05762</guid>
<content:encoded><![CDATA[
arXiv:2512.05762v1 Announce Type: new 
Abstract: We present FNOpt, a self-supervised cloth simulation framework that formulates time integration as an optimization problem and trains a resolution-agnostic neural optimizer parameterized by a Fourier neural operator (FNO). Prior neural simulators often rely on extensive ground truth data or sacrifice fine-scale detail, and generalize poorly across resolutions and motion patterns. In contrast, FNOpt learns to simulate physically plausible cloth dynamics and achieves stable and accurate rollouts across diverse mesh resolutions and motion patterns without retraining. Trained only on a coarse grid with physics-based losses, FNOpt generalizes to finer resolutions, capturing fine-scale wrinkles and preserving rollout stability. Extensive evaluations on a benchmark cloth simulation dataset demonstrate that FNOpt outperforms prior learning-based approaches in out-of-distribution settings in both accuracy and robustness. These results position FNO-based meta-optimization as a compelling alternative to previous neural simulators for cloth, thus reducing the need for curated data and improving cross-resolution reliability.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.05774</link>
<guid>https://arxiv.org/abs/2512.05774</guid>
<content:encoded><![CDATA[
arXiv:2512.05774v1 Announce Type: new 
Abstract: Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth</title>
<link>https://arxiv.org/abs/2512.05783</link>
<guid>https://arxiv.org/abs/2512.05783</guid>
<content:encoded><![CDATA[
arXiv:2512.05783v1 Announce Type: new 
Abstract: When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bring Your Dreams to Life: Continual Text-to-Video Customization</title>
<link>https://arxiv.org/abs/2512.05802</link>
<guid>https://arxiv.org/abs/2512.05802</guid>
<content:encoded><![CDATA[
arXiv:2512.05802v1 Announce Type: new 
Abstract: Customized text-to-video generation (CTVG) has recently witnessed great progress in generating tailored videos from user-specific text. However, most CTVG methods assume that personalized concepts remain static and do not expand incrementally over time. Additionally, they struggle with forgetting and concept neglect when continuously learning new concepts, including subjects and motions. To resolve the above challenges, we develop a novel Continual Customized Video Diffusion (CCVD) model, which can continuously learn new concepts to generate videos across various text-to-video generation tasks by tackling forgetting and concept neglect. To address catastrophic forgetting, we introduce a concept-specific attribute retention module and a task-aware concept aggregation strategy. They can capture the unique characteristics and identities of old concepts during training, while combining all subject and motion adapters of old concepts based on their relevance during testing. Besides, to tackle concept neglect, we develop a controllable conditional synthesis to enhance regional features and align video contexts with user conditions, by incorporating layer-specific region attention-guided noise estimation. Extensive experimental comparisons demonstrate that our CCVD outperforms existing CTVG models. The code is available at https://github.com/JiahuaDong/CCVD.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling</title>
<link>https://arxiv.org/abs/2512.05809</link>
<guid>https://arxiv.org/abs/2512.05809</guid>
<content:encoded><![CDATA[
arXiv:2512.05809v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2512.05814</link>
<guid>https://arxiv.org/abs/2512.05814</guid>
<content:encoded><![CDATA[
arXiv:2512.05814v1 Announce Type: new 
Abstract: Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning</title>
<link>https://arxiv.org/abs/2512.05830</link>
<guid>https://arxiv.org/abs/2512.05830</guid>
<content:encoded><![CDATA[
arXiv:2512.05830v1 Announce Type: new 
Abstract: This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: https://github.com/miralab-ai/Phase-OTDR-event-detection.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack</title>
<link>https://arxiv.org/abs/2512.05853</link>
<guid>https://arxiv.org/abs/2512.05853</guid>
<content:encoded><![CDATA[
arXiv:2512.05853v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) are widely used in various fields due to their powerful cross-modal comprehension and generation capabilities. However, more modalities bring more vulnerabilities to being utilized for jailbreak attacks, which induces MLLMs to output harmful content. Due to the strong reasoning ability of MLLMs, previous jailbreak attacks try to explore reasoning safety risk in text modal, while similar threats have been largely overlooked in the visual modal. To fully evaluate potential safety risks in the visual reasoning task, we propose Visual Reasoning Sequential Attack (VRSA), which induces MLLMs to gradually externalize and aggregate complete harmful intent by decomposing the original harmful text into several sequentially related sub-images. In particular, to enhance the rationality of the scene in the image sequence, we propose Adaptive Scene Refinement to optimize the scene most relevant to the original harmful query. To ensure the semantic continuity of the generated image, we propose Semantic Coherent Completion to iteratively rewrite each sub-text combined with contextual information in this scene. In addition, we propose Text-Image Consistency Alignment to keep the semantical consistency. A series of experiments demonstrates that the VRSA can achieve a higher attack success rate compared with the state-of-the-art jailbreak attack methods on both the open-source and closed-source MLLMs such as GPT-4o and Claude-4.5-Sonnet.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edit-aware RAW Reconstruction</title>
<link>https://arxiv.org/abs/2512.05859</link>
<guid>https://arxiv.org/abs/2512.05859</guid>
<content:encoded><![CDATA[
arXiv:2512.05859v1 Announce Type: new 
Abstract: Users frequently edit camera images post-capture to achieve their preferred photofinishing style. While editing in the RAW domain provides greater accuracy and flexibility, most edits are performed on the camera's display-referred output (e.g., 8-bit sRGB JPEG) since RAW images are rarely stored. Existing RAW reconstruction methods can recover RAW data from sRGB images, but these approaches are typically optimized for pixel-wise RAW reconstruction fidelity and tend to degrade under diverse rendering styles and editing operations. We introduce a plug-and-play, edit-aware loss function that can be integrated into any existing RAW reconstruction framework to make the recovered RAWs more robust to different rendering styles and edits. Our loss formulation incorporates a modular, differentiable image signal processor (ISP) that simulates realistic photofinishing pipelines with tunable parameters. During training, parameters for each ISP module are randomly sampled from carefully designed distributions that model practical variations in real camera processing. The loss is then computed in sRGB space between ground-truth and reconstructed RAWs rendered through this differentiable ISP. Incorporating our loss improves sRGB reconstruction quality by up to 1.5-2 dB PSNR across various editing conditions. Moreover, when applied to metadata-assisted RAW reconstruction methods, our approach enables fine-tuning for target edits, yielding further gains. Since photographic editing is the primary motivation for RAW reconstruction in consumer imaging, our simple yet effective loss function provides a general mechanism for enhancing edit fidelity and rendering flexibility across existing methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Underwater Image Reconstruction Using a Swin Transformer-Based Generator and PatchGAN Discriminator</title>
<link>https://arxiv.org/abs/2512.05866</link>
<guid>https://arxiv.org/abs/2512.05866</guid>
<content:encoded><![CDATA[
arXiv:2512.05866v1 Announce Type: new 
Abstract: Underwater imaging is essential for marine exploration, environmental monitoring, and infrastructure inspection. However, water causes severe image degradation through wavelength-dependent absorption and scattering, resulting in color distortion, low contrast, and haze effects. Traditional reconstruction methods and convolutional neural network-based approaches often fail to adequately address these challenges due to limited receptive fields and inability to model global dependencies. This paper presented a novel deep learning framework that integrated a Swin Transformer architecture within a generative adversarial network (GAN) for underwater image reconstruction. Our generator employed a U-Net structure with Swin Transformer blocks to capture both local features and long-range dependencies crucial for color correction across entire images. A PatchGAN discriminator provided adversarial training to ensure high-frequency detail preservation. We trained and evaluated our model on the EUVP dataset, which contains paired underwater images of varying quality. Quantitative results demonstrate stateof-the-art performance with PSNR of 24.76 dB and SSIM of 0.89, representing significant improvements over existing methods. Visual results showed effective color balance restoration, contrast improvement, and haze reduction. An ablation study confirms the superiority of our Swin Transformer designed over convolutional alternatives. The proposed method offers robust underwater image reconstruction suitable for various marine applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations</title>
<link>https://arxiv.org/abs/2512.05905</link>
<guid>https://arxiv.org/abs/2512.05905</guid>
<content:encoded><![CDATA[
arXiv:2512.05905v1 Announce Type: new 
Abstract: Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \textbf{SCAIL} (\textbf{S}tudio-grade \textbf{C}haracter \textbf{A}nimation via \textbf{I}n-context \textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction</title>
<link>https://arxiv.org/abs/2512.05920</link>
<guid>https://arxiv.org/abs/2512.05920</guid>
<content:encoded><![CDATA[
arXiv:2512.05920v1 Announce Type: new 
Abstract: Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation</title>
<link>https://arxiv.org/abs/2512.05922</link>
<guid>https://arxiv.org/abs/2512.05922</guid>
<content:encoded><![CDATA[
arXiv:2512.05922v1 Announce Type: new 
Abstract: Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty</title>
<link>https://arxiv.org/abs/2512.05927</link>
<guid>https://arxiv.org/abs/2512.05927</guid>
<content:encoded><![CDATA[
arXiv:2512.05927v1 Announce Type: new 
Abstract: Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition</title>
<link>https://arxiv.org/abs/2512.05928</link>
<guid>https://arxiv.org/abs/2512.05928</guid>
<content:encoded><![CDATA[
arXiv:2512.05928v1 Announce Type: new 
Abstract: Facial recognition has become a widely used method for authentication and identification, with applications for secure access and locating missing persons. Its success is largely attributed to deep learning, which leverages large datasets and effective loss functions to learn discriminative features. Despite these advances, facial recognition still faces challenges in explainability, demographic bias, privacy, and robustness to aging, pose variations, lighting changes, occlusions, and facial expressions. Privacy regulations have also led to the degradation of several datasets, raising legal, ethical, and privacy concerns. Synthetic facial data generation has been proposed as a promising solution. It mitigates privacy issues, enables experimentation with controlled facial attributes, alleviates demographic bias, and provides supplementary data to improve models trained on real data. This study compares the effectiveness of synthetic facial datasets generated using different techniques in facial recognition tasks. We evaluate accuracy, rank-1, rank-5, and the true positive rate at a false positive rate of 0.01% on eight leading datasets, offering a comparative analysis not extensively explored in the literature. Results demonstrate the ability of synthetic data to capture realistic variations while emphasizing the need for further research to close the performance gap with real data. Techniques such as diffusion models, GANs, and 3D models show substantial progress; however, challenges remain.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition</title>
<link>https://arxiv.org/abs/2512.05936</link>
<guid>https://arxiv.org/abs/2512.05936</guid>
<content:encoded><![CDATA[
arXiv:2512.05936v1 Announce Type: new 
Abstract: In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization. In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments. Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs. In addition to a mask and a segmentation image, we also provide extensive metadata including the stochastically selected environment and imaging effect parameters for each image. We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception</title>
<link>https://arxiv.org/abs/2512.05937</link>
<guid>https://arxiv.org/abs/2512.05937</guid>
<content:encoded><![CDATA[
arXiv:2512.05937v1 Announce Type: new 
Abstract: Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...].
  Download: synset.de/datasets/synset-signset-ger/background-effect
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding</title>
<link>https://arxiv.org/abs/2512.05941</link>
<guid>https://arxiv.org/abs/2512.05941</guid>
<content:encoded><![CDATA[
arXiv:2512.05941v1 Announce Type: new 
Abstract: Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement</title>
<link>https://arxiv.org/abs/2512.05960</link>
<guid>https://arxiv.org/abs/2512.05960</guid>
<content:encoded><![CDATA[
arXiv:2512.05960v1 Announce Type: new 
Abstract: Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EditThinker: Unlocking Iterative Reasoning for Any Image Editor</title>
<link>https://arxiv.org/abs/2512.05965</link>
<guid>https://arxiv.org/abs/2512.05965</guid>
<content:encoded><![CDATA[
arXiv:2512.05965v1 Announce Type: new 
Abstract: Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyncVoice: Towards Video Dubbing with Vision-Augmented Pretrained TTS Model</title>
<link>https://arxiv.org/abs/2512.05126</link>
<guid>https://arxiv.org/abs/2512.05126</guid>
<content:encoded><![CDATA[
arXiv:2512.05126v1 Announce Type: cross 
Abstract: Video dubbing aims to generate high-fidelity speech that is precisely temporally aligned with the visual content. Existing methods still suffer from limitations in speech naturalness and audio-visual synchronization, and are limited to monolingual settings. To address these challenges, we propose SyncVoice, a vision-augmented video dubbing framework built upon a pretrained text-to-speech (TTS) model. By fine-tuning the TTS model on audio-visual data, we achieve strong audiovisual consistency. We propose a Dual Speaker Encoder to effectively mitigate inter-language interference in cross-lingual speech synthesis and explore the application of video dubbing in video translation scenarios. Experimental results show that SyncVoice achieves high-fidelity speech generation with strong synchronization performance, demonstrating its potential in video dubbing tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARCAS: An Augmented Reality Collision Avoidance System with SLAM-Based Tracking for Enhancing VRU Safety</title>
<link>https://arxiv.org/abs/2512.05299</link>
<guid>https://arxiv.org/abs/2512.05299</guid>
<content:encoded><![CDATA[
arXiv:2512.05299v1 Announce Type: cross 
Abstract: Vulnerable road users (VRUs) face high collision risks in mixed traffic, yet most existing safety systems prioritize driver or vehicle assistance over direct VRU support. This paper presents ARCAS, a real-time augmented reality collision avoidance system that provides personalized spatial alerts to VRUs via wearable AR headsets. By fusing roadside 360-degree 3D LiDAR with SLAM-based headset tracking and an automatic 3D calibration procedure, ARCAS accurately overlays world-locked 3D bounding boxes and directional arrows onto approaching hazards in the user's passthrough view. The system also enables multi-headset coordination through shared world anchoring. Evaluated in real-world pedestrian interactions with e-scooters and vehicles (180 trials), ARCAS nearly doubled pedestrians' time-to-collision and increased counterparts' reaction margins by up to 4x compared to unaided-eye conditions. Results validate the feasibility and effectiveness of LiDAR-driven AR guidance and highlight the potential of wearable AR as a promising next-generation safety tool for urban mobility.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EXR: An Interactive Immersive EHR Visualization in Extended Reality</title>
<link>https://arxiv.org/abs/2512.05438</link>
<guid>https://arxiv.org/abs/2512.05438</guid>
<content:encoded><![CDATA[
arXiv:2512.05438v1 Announce Type: cross 
Abstract: This paper presents the design and implementation of an Extended Reality (XR) platform for immersive, interactive visualization of Electronic Health Records (EHRs). The system extends beyond conventional 2D interfaces by visualizing both structured and unstructured patient data into a shared 3D environment, enabling intuitive exploration and real-time collaboration. The modular infrastructure integrates FHIR-based EHR data with volumetric medical imaging and AI-generated segmentation, ensuring interoperability with modern healthcare systems. The platform's capabilities are demonstrated using synthetic EHR datasets and computed tomography (CT)-derived spine models processed through an AI-powered segmentation pipeline. This work suggests that such integrated XR solutions could form the foundation for next-generation clinical decision-support tools, where advanced data infrastructures are directly accessible in an interactive and spatially rich environment.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interleaved Latent Visual Reasoning with Selective Perceptual Modeling</title>
<link>https://arxiv.org/abs/2512.05665</link>
<guid>https://arxiv.org/abs/2512.05665</guid>
<content:encoded><![CDATA[
arXiv:2512.05665v1 Announce Type: cross 
Abstract: Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation</title>
<link>https://arxiv.org/abs/2512.05812</link>
<guid>https://arxiv.org/abs/2512.05812</guid>
<content:encoded><![CDATA[
arXiv:2512.05812v1 Announce Type: cross 
Abstract: Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma</title>
<link>https://arxiv.org/abs/2512.05824</link>
<guid>https://arxiv.org/abs/2512.05824</guid>
<content:encoded><![CDATA[
arXiv:2512.05824v1 Announce Type: cross 
Abstract: Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically-Based Simulation of Automotive LiDAR</title>
<link>https://arxiv.org/abs/2512.05932</link>
<guid>https://arxiv.org/abs/2512.05932</guid>
<content:encoded><![CDATA[
arXiv:2512.05932v1 Announce Type: cross 
Abstract: We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter.
  Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties.
  Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01{\deg} resolution, which marks the best available resolution for measuring the beam pattern.
  The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.05955</link>
<guid>https://arxiv.org/abs/2512.05955</guid>
<content:encoded><![CDATA[
arXiv:2512.05955v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG</title>
<link>https://arxiv.org/abs/2512.05959</link>
<guid>https://arxiv.org/abs/2512.05959</guid>
<content:encoded><![CDATA[
arXiv:2512.05959v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving</title>
<link>https://arxiv.org/abs/2405.05258</link>
<guid>https://arxiv.org/abs/2405.05258</guid>
<content:encoded><![CDATA[
arXiv:2405.05258v3 Announce Type: replace 
Abstract: Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning. Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models. The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution. Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets. Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines. This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Direction-Aware Network for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2406.02037</link>
<guid>https://arxiv.org/abs/2406.02037</guid>
<content:encoded><![CDATA[
arXiv:2406.02037v3 Announce Type: replace 
Abstract: Infrared small target detection faces the problem that it is difficult to effectively separate the background and the target. Existing deep learning-based methods focus on edge and shape features, but ignore the richer structural differences and detailed information embedded in high-frequency components from different directions, thereby failing to fully exploit the value of high-frequency directional features in target perception. To address this limitation, we propose a multi-scale direction-aware network (MSDA-Net), which is the first attempt to integrate the high-frequency directional features of infrared small targets as domain prior knowledge into neural networks. Specifically, to fully mine the high-frequency directional features, on the one hand, a high-frequency direction injection (HFDI) module without trainable parameters is constructed to inject the high-frequency directional information of the original image into the network. On the other hand, a multi-scale direction-aware (MSDA) module is constructed, which promotes the full extraction of local relations at different scales and the full perception of key features in different directions. In addition, considering the characteristics of infrared small targets, we construct a feature aggregation (FA) structure to address target disappearance in high-level feature maps, and a feature calibration fusion (FCF) module to alleviate feature bias during cross-layer feature fusion. Extensive experimental results show that our MSDA-Net achieves state-of-the-art (SOTA) results on multiple public datasets. The code can be available at https://github.com/YuChuang1205/MSDA-Net
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iMotion-LLM: Instruction-Conditioned Trajectory Generation</title>
<link>https://arxiv.org/abs/2406.06211</link>
<guid>https://arxiv.org/abs/2406.06211</guid>
<content:encoded><![CDATA[
arXiv:2406.06211v3 Announce Type: replace 
Abstract: We introduce iMotion-LLM, a large language model (LLM) integrated with trajectory prediction modules for interactive motion generation. Unlike conventional approaches, it generates feasible, safety-aligned trajectories based on textual instructions, enabling adaptable and context-aware driving behavior. It combines an encoder-decoder multimodal trajectory prediction model with a pre-trained LLM fine-tuned using LoRA, projecting scene features into the LLM input space and mapping special tokens to a trajectory decoder for text-based interaction and interpretable driving. To support this framework, we introduce two datasets: 1) InstructWaymo, an extension of the Waymo Open Motion Dataset with direction-based motion instructions, and 2) Open-Vocabulary InstructNuPlan, which features safety-aligned instruction-caption pairs and corresponding safe trajectory scenarios. Our experiments validate that instruction conditioning enables trajectory generation that follows the intended condition. iMotion-LLM demonstrates strong contextual comprehension, achieving 84% average accuracy in direction feasibility detection and 96% average accuracy in safety evaluation of open-vocabulary instructions. This work lays the foundation for text-guided motion generation in autonomous driving, supporting simulated data generation, model interpretability, and robust safety alignment testing for trajectory generation models. Our code, pre-trained model, and datasets are available at: https://vision-cair.github.io/iMotion-LLM/.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Scene-aware Models Adaptation Scheme for Cross-scene Online Inference on Mobile Devices</title>
<link>https://arxiv.org/abs/2407.03331</link>
<guid>https://arxiv.org/abs/2407.03331</guid>
<content:encoded><![CDATA[
arXiv:2407.03331v2 Announce Type: replace 
Abstract: Emerging Artificial Intelligence of Things (AIoT) applications desire online prediction using deep neural network (DNN) models on mobile devices. However, due to the movement of devices, unfamiliar test samples constantly appear, significantly affecting the prediction accuracy of a pre-trained DNN. In addition, unstable network connection calls for local model inference. In this paper, we propose a light-weight scheme, called Anole, to cope with the local DNN model inference on mobile devices. The core idea of Anole is to first establish an army of compact DNN models, and then adaptively select the model fitting the current test sample best for online inference. The key is to automatically identify model-friendly scenes for training scene-specific DNN models. To this end, we design a weakly-supervised scene representation learning algorithm by combining both human heuristics and feature similarity in separating scenes. Moreover, we further train a model classifier to predict the best-fit scene-specific DNN model for each test sample. We implement Anole on different types of mobile devices and conduct extensive trace-driven and real-world experiments based on unmanned aerial vehicles (UAVs). The results demonstrate that Anole outwits the method of using a versatile large DNN in terms of prediction accuracy (4.5% higher), response time (33.1% faster) and power consumption (45.1% lower).
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2407.16344</link>
<guid>https://arxiv.org/abs/2407.16344</guid>
<content:encoded><![CDATA[
arXiv:2407.16344v5 Announce Type: replace 
Abstract: High frame-rate (HFR) videos of action recognition improve fine-grained expression while reducing the spatio-temporal relation and motion information density. Thus, large amounts of video samples are continuously required for traditional data-driven training. However, samples are not always sufficient in real-world scenarios, promoting few-shot action recognition (FSAR) research. We observe that most recent FSAR works build spatio-temporal relation of video samples via temporal alignment after spatial feature extraction, cutting apart spatial and temporal features within samples. They also capture motion information via narrow perspectives between adjacent frames without considering density, leading to insufficient motion information capturing. Therefore, we propose a novel plug-and-play architecture for FSAR called Spatio-tempOral frAme tuPle enhancer (SOAP) in this paper. The model we designed with such architecture refers to SOAP-Net. Temporal connections between different feature channels and spatio-temporal relation of features are considered instead of simple feature extraction. Comprehensive motion information is also captured, using frame tuples with multiple frames containing more motion information than adjacent frames. Combining frame tuples of diverse frame counts further provides a broader perspective. SOAP-Net achieves new state-of-the-art performance across well-known benchmarks such as SthSthV2, Kinetics, UCF101, and HMDB51. Extensive empirical evaluations underscore the competitiveness, pluggability, generalization, and robustness of SOAP. The code is released at https://github.com/wenbohuang1002/SOAP.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PLANesT-3D: A new annotated dataset for segmentation of 3D plant point clouds</title>
<link>https://arxiv.org/abs/2407.21150</link>
<guid>https://arxiv.org/abs/2407.21150</guid>
<content:encoded><![CDATA[
arXiv:2407.21150v2 Announce Type: replace 
Abstract: Creation of new annotated public datasets is crucial in helping advances in 3D computer vision and machine learning meet their full potential for automatic interpretation of 3D plant models. Despite the proliferation of deep neural network architectures for segmentation and phenotyping of 3D plant models in the last decade, the amount of data, and diversity in terms of species and data acquisition modalities are far from sufficient for evaluation of such tools for their generalization ability. To contribute to closing this gap, we introduce PLANesT-3D; a new annotated dataset of 3D color point clouds of plants. PLANesT-3D is composed of 34 point cloud models representing 34 real plants from three different plant species: \textit{Capsicum annuum}, \textit{Rosa kordana}, and \textit{Ribes rubrum}. Both semantic labels in terms of "leaf" and "stem", and organ instance labels were manually annotated for the full point clouds. PLANesT-3D introduces diversity to existing datasets by adding point clouds of two new species and providing 3D data acquired with the low-cost SfM/MVS technique as opposed to laser scanning or expensive setups. Point clouds reconstructed with SfM/MVS modality exhibit challenges such as missing data, variable density, and illumination variations. As an additional contribution, SP-LSCnet, a novel semantic segmentation method that is a combination of unsupervised superpoint extraction and a 3D point-based deep learning approach is introduced and evaluated on the new dataset. The advantages of SP-LSCnet over other deep learning methods are its modular structure and increased interpretability. Two existing deep neural network architectures, PointNet++ and RoseSegNet, were also tested on the point clouds of PLANesT-3D for semantic segmentation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLDiTalker: Speech-Driven 3D Facial Animation with Graph Latent Diffusion Transformer</title>
<link>https://arxiv.org/abs/2408.01826</link>
<guid>https://arxiv.org/abs/2408.01826</guid>
<content:encoded><![CDATA[
arXiv:2408.01826v5 Announce Type: replace 
Abstract: Speech-driven talking head generation is a critical yet challenging task with applications in augmented reality and virtual human modeling. While recent approaches using autoregressive and diffusion-based models have achieved notable progress, they often suffer from modality inconsistencies, particularly misalignment between audio and mesh, leading to reduced motion diversity and lip-sync accuracy. To address this, we propose GLDiTalker, a novel speech-driven 3D facial animation model based on a Graph Latent Diffusion Transformer. GLDiTalker resolves modality misalignment by diffusing signals within a quantized spatiotemporal latent space. It employs a two-stage training pipeline: the Graph-Enhanced Quantized Space Learning Stage ensures lip-sync accuracy, while the Space-Time Powered Latent Diffusion Stage enhances motion diversity. Together, these stages enable GLDiTalker to generate realistic, temporally stable 3D facial animations. Extensive evaluations on standard benchmarks demonstrate that GLDiTalker outperforms existing methods, achieving superior results in both lip-sync accuracy and motion diversity.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Correlation to Causation: Max-Pooling-Based Multi-Instance Learning Leads to More Robust Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2408.09449</link>
<guid>https://arxiv.org/abs/2408.09449</guid>
<content:encoded><![CDATA[
arXiv:2408.09449v3 Announce Type: replace 
Abstract: In whole slide images (WSIs) analysis, attention-based multi-instance learning (MIL) models are susceptible to spurious correlations and degrade under domain shift. These methods may assign high attention weights to non-tumor regions, such as staining biases or artifacts, leading to unreliable tumor region localization. In this paper, we revisit max-pooling-based MIL methods from a causal perspective. Under mild assumptions, our theoretical results demonstrate that max-pooling encourages the model to focus on causal factors while ignoring bias-related factors. Furthermore, we discover that existing max-pooling-based methods may overfit the training set through rote memorization of instance features and fail to learn meaningful patterns. To address these issues, we propose FocusMIL, which couples max-pooling with an instance-level variational information bottleneck (VIB) to learn compact, predictive latent representations, and employs a multi-bag mini-batch scheme to stabilize optimization. We conduct comprehensive experiments on three real-world datasets and one semi-synthetic dataset. The results show that, by capturing causal factors, FocusMIL exhibits significant advantages in out-of-distribution scenarios and instance-level tumor region localization tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image-Guided Semantic Pseudo-LiDAR Point Generation for 3D Object Detection</title>
<link>https://arxiv.org/abs/2409.14985</link>
<guid>https://arxiv.org/abs/2409.14985</guid>
<content:encoded><![CDATA[
arXiv:2409.14985v3 Announce Type: replace 
Abstract: In autonomous driving scenarios, accurate perception is becoming an even more critical task for safe navigation. While LiDAR provides precise spatial data, its inherent sparsity makes it difficult to detect small or distant objects. Existing methods try to address this by generating additional points within a Region of Interest (RoI), but relying on LiDAR alone often leads to false positives and a failure to recover meaningful structures. To address these limitations, we propose Image-Guided Semantic Pseudo-LiDAR Point Generation model, called ImagePG, a novel framework that leverages rich RGB image features to generate dense and semantically meaningful 3D points. Our framework includes an Image-Guided RoI Points Generation (IG-RPG) module, which creates pseudo-points guided by image features, and an Image-Aware Occupancy Prediction Network (I-OPN), which provides spatial priors to guide point placement. A multi-stage refinement (MR) module further enhances point quality and detection robustness. To the best of our knowledge, ImagePG is the first method to directly leverage image features for point generation. Extensive experiments on the KITTI and Waymo datasets demonstrate that ImagePG significantly improves the detection of small and distant objects like pedestrians and cyclists, reducing false positives by nearly 50%. On the KITTI benchmark, our framework improves mAP by +1.38%p (car), +7.91%p (pedestrian), and +5.21%p (cyclist) on the test set over the baseline, achieving state-of-the-art cyclist performance on the KITTI leaderboard. The code is available at: https://github.com/MS-LIMA/ImagePG
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point-PNG: Conditional Pseudo-Negatives Generation for Point Cloud Pre-Training</title>
<link>https://arxiv.org/abs/2409.15832</link>
<guid>https://arxiv.org/abs/2409.15832</guid>
<content:encoded><![CDATA[
arXiv:2409.15832v3 Announce Type: replace 
Abstract: We propose Point-PNG, a novel self-supervised learning framework that generates conditional pseudo-negatives in the latent space to learn point cloud representations that are both discriminative and transformation-sensitive. Conventional self-supervised learning methods focus on achieving invariance, discarding transformation-specific information. Recent approaches incorporate transformation sensitivity by explicitly modeling relationships between original and transformed inputs. However, they often suffer from an invariant-collapse phenomenon, where the predictor degenerates into identity mappings, resulting in latent representations with limited variation across transformations. To address this, we propose Point-PNG that explicitly penalizes invariant collapse through pseudo-negatives generation, enabling the network to capture richer transformation cues while preserving discriminative representations. To this end, we introduce a parametric network, COnditional Pseudo-Negatives Embedding (COPE), which learns localized displacements induced by transformations within the latent space. A key challenge arises when jointly training COPE with the MAE, as it tends to converge to trivial identity mappings. To overcome this, we design a loss function based on pseudo-negatives conditioned on the transformation, which penalizes such trivial invariant solutions and enforces meaningful representation learning. We validate Point-PNG on shape classification and relative pose estimation tasks, showing competitive performance on ModelNet40 and ScanObjectNN under challenging evaluation protocols, and achieving superior accuracy in relative pose estimation compared to supervised baselines.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Eulerian Scene Flow Fields</title>
<link>https://arxiv.org/abs/2410.02031</link>
<guid>https://arxiv.org/abs/2410.02031</guid>
<content:encoded><![CDATA[
arXiv:2410.02031v3 Announce Type: replace 
Abstract: We reframe scene flow as the task of estimating a continuous space-time ODE that describes motion for an entire observation sequence, represented with a neural prior. Our method, EulerFlow, optimizes this neural prior estimate against several multi-observation reconstruction objectives, enabling high quality scene flow estimation via pure self-supervision on real-world data. EulerFlow works out-of-the-box without tuning across multiple domains, including large-scale autonomous driving scenes and dynamic tabletop settings. Remarkably, EulerFlow produces high quality flow estimates on small, fast moving objects like birds and tennis balls, and exhibits emergent 3D point tracking behavior by solving its estimated ODE over long-time horizons. On the Argoverse 2 2024 Scene Flow Challenge, EulerFlow outperforms all prior art, surpassing the next-best unsupervised method by more than 2.5x, and even exceeding the next-best supervised method by over 10%.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedDiff-FM: A Diffusion-based Foundation Model for Versatile Medical Image Applications</title>
<link>https://arxiv.org/abs/2410.15432</link>
<guid>https://arxiv.org/abs/2410.15432</guid>
<content:encoded><![CDATA[
arXiv:2410.15432v3 Announce Type: replace 
Abstract: Diffusion models have achieved significant success in both natural image and medical image domains, encompassing a wide range of applications. Previous investigations in medical images have often been constrained to specific anatomical regions, particular applications, and limited datasets, resulting in isolated diffusion models. This paper introduces a diffusion-based foundation model to address a diverse range of medical image tasks, namely MedDiff-FM. MedDiff-FM leverages 3D CT images from multiple publicly available datasets, covering anatomical regions from head to abdomen, to pre-train a diffusion foundation model, and explores the capabilities of the diffusion foundation model across a variety of application scenarios. The diffusion foundation model handles multi-level integrated image processing both at the image-level and patch-level, utilizes position embedding to establish multi-level spatial relationships, and leverages region classes and anatomical structures to capture certain anatomical regions. MedDiff-FM manages several downstream tasks seamlessly, including image denoising, anomaly detection, and image synthesis. MedDiff-FM is also capable of performing super-resolution, lesion generation, and lesion inpainting by rapidly fine-tuning the diffusion foundation model using ControlNet with task-specific conditions. The experimental results demonstrate the effectiveness of MedDiff-FM in addressing diverse downstream medical image tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dressing the Imagination: A Dataset for AI-Powered Translation of Text into Fashion Outfits and A Novel KAN Adapter for Enhanced Feature Adaptation</title>
<link>https://arxiv.org/abs/2411.13901</link>
<guid>https://arxiv.org/abs/2411.13901</guid>
<content:encoded><![CDATA[
arXiv:2411.13901v4 Announce Type: replace 
Abstract: Specialized datasets that capture the fashion industry's rich language and styling elements can boost progress in AI-driven fashion design. We present FLORA, (Fashion Language Outfit Representation for Apparel Generation), the first comprehensive dataset containing 4,330 curated pairs of fashion outfits and corresponding textual descriptions. Each description utilizes industry-specific terminology and jargon commonly used by professional fashion designers, providing precise and detailed insights into the outfits. Hence, the dataset captures the delicate features and subtle stylistic elements necessary to create high-fidelity fashion designs.
  We demonstrate that fine-tuning generative models on the FLORA dataset significantly enhances their capability to generate accurate and stylistically rich images from textual descriptions of fashion sketches. FLORA will catalyze the creation of advanced AI models capable of comprehending and producing subtle, stylistically rich fashion designs. It will also help fashion designers and end-users to bring their ideas to life.
  As a second orthogonal contribution, we introduce NeRA (Nonlinear low-rank Expressive Representation Adapter), a novel adapter architecture based on Kolmogorov-Arnold Networks (KAN). Unlike traditional PEFT techniques such as LoRA, LoKR, DoRA, and LoHA that use MLP adapters, NeRA uses learnable spline-based nonlinear transformations, enabling superior modeling of complex semantic relationships, achieving strong fidelity, faster convergence and semantic alignment. Extensive experiments on our proposed FLORA and LAION-5B datasets validate the superiority of NeRA over existing adapters.
  We will open-source both the FLORA dataset and our implementation code.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models</title>
<link>https://arxiv.org/abs/2412.07755</link>
<guid>https://arxiv.org/abs/2412.07755</guid>
<content:encoded><![CDATA[
arXiv:2412.07755v3 Announce Type: replace 
Abstract: Reasoning about motion and space is a fundamental cognitive capability that is required by multiple real-world applications. While many studies highlight that large multimodal language models (MLMs) struggle to reason about space, they only focus on static spatial relationships, and not dynamic awareness of motion and space, i.e., reasoning about the effect of egocentric and object motions on spatial relationships. Manually annotating such object and camera movements is expensive. Hence, we introduce SAT, a simulated spatial aptitude training dataset utilizing 3D simulators, comprising both static and dynamic spatial reasoning across 175K question-answer (QA) pairs and 20K scenes. Complementing this, we also construct a small (150 image-QAs) yet challenging dynamic spatial test set using real-world images. Leveraging our SAT datasets and 6 existing static spatial benchmarks, we systematically investigate what improves both static and dynamic spatial awareness. Our results reveal that simulations are surprisingly effective at imparting spatial aptitude to MLMs that translate to real images. We show that perfect annotations in simulation are more effective than existing approaches of pseudo-annotating real images. For instance, SAT training improves a LLaVA-13B model by an average 11% and a LLaVA-Video-7B model by an average 8% on multiple spatial benchmarks, including our real-image dynamic test set and spatial reasoning on long videos -- even outperforming some large proprietary models. While reasoning over static relationships improves with synthetic training data, there is still considerable room for improvement for dynamic reasoning questions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM</title>
<link>https://arxiv.org/abs/2503.04504</link>
<guid>https://arxiv.org/abs/2503.04504</guid>
<content:encoded><![CDATA[
arXiv:2503.04504v4 Announce Type: replace 
Abstract: Video anomaly detection (VAD) is crucial for video analysis and surveillance in computer vision. However, existing VAD models rely on learned normal patterns, which makes them difficult to apply to diverse environments. Consequently, users should retrain models or develop separate AI models for new environments, which requires expertise in machine learning, high-performance hardware, and extensive data collection, limiting the practical usability of VAD. To address these challenges, this study proposes customizable video anomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers user-defined text as an abnormal event and detects frames containing a specified event in a video. We effectively implemented AnyAnomaly using a context-aware visual question answering without fine-tuning the large vision language model. To validate the effectiveness of the proposed model, we constructed C-VAD datasets and demonstrated the superiority of AnyAnomaly. Furthermore, our approach showed competitive results on VAD benchmarks, achieving state-of-the-art performance on UBnormal and UCF-Crime and surpassing other methods in generalization across all datasets. Our code is available online at github.com/SkiddieAhn/Paper-AnyAnomaly.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time 3D Occupancy Prediction</title>
<link>https://arxiv.org/abs/2503.08485</link>
<guid>https://arxiv.org/abs/2503.08485</guid>
<content:encoded><![CDATA[
arXiv:2503.08485v3 Announce Type: replace 
Abstract: Self-supervised 3D occupancy prediction offers a promising solution for understanding complex driving scenes without requiring costly 3D annotations. However, training dense occupancy decoders to capture fine-grained geometry and semantics can demand hundreds of GPU hours, and once trained, such models struggle to adapt to varying voxel resolutions or novel object categories without extensive retraining. To overcome these limitations, we propose a practical and flexible test-time occupancy prediction framework termed TT-Occ. Our method incrementally constructs, optimizes and voxelizes time-aware 3D Gaussians from raw sensor streams by integrating vision foundation models (VFMs) at runtime. The flexible nature of 3D Gaussians allows voxelization at arbitrary user-specified resolutions, while the generalization ability of VFMs enables accurate perception and open-vocabulary recognition, without any network training or fine-tuning. Specifically, TT-Occ operates in a lift-track-voxelize symphony: We first lift the geometry and semantics of surrounding-view extracted from VFMs to instantiate Gaussians at 3D space; Next, we track dynamic Gaussians while accumulating static ones to complete the scene and enforce temporal consistency; Finally, we voxelize the optimized Gaussians to generate occupancy prediction. Optionally, inherent noise in VFM predictions and tracking is mitigated by periodically smoothing neighboring Gaussians during optimization. To validate the generality and effectiveness of our framework, we offer two variants: one LiDAR-based and one vision-centric, and conduct extensive experiments on Occ3D and nuCraft benchmarks with varying voxel resolutions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Self-Supervised Video Alignment and Action Segmentation</title>
<link>https://arxiv.org/abs/2503.16832</link>
<guid>https://arxiv.org/abs/2503.16832</guid>
<content:encoded><![CDATA[
arXiv:2503.16832v3 Announce Type: replace 
Abstract: We introduce a novel approach for simultaneous self-supervised video alignment and action segmentation based on a unified optimal transport framework. In particular, we first tackle self-supervised video alignment by developing a fused Gromov-Wasserstein optimal transport formulation with a structural prior, which trains efficiently on GPUs and needs only a few iterations for solving the optimal transport problem. Our single-task method achieves the state-of-the-art performance on multiple video alignment benchmarks and outperforms VAVA, which relies on a traditional Kantorovich optimal transport formulation with an optimality prior. Furthermore, we extend our approach by proposing a unified optimal transport framework for joint self-supervised video alignment and action segmentation, which requires training and storing a single model and saves both time and memory consumption as compared to two different single-task models. Extensive evaluations on several video alignment and action segmentation datasets demonstrate that our multi-task method achieves comparable video alignment yet superior action segmentation results over previous methods in video alignment and action segmentation respectively. Finally, to the best of our knowledge, this is the first work to unify video alignment and action segmentation into a single model. Our code is available on our research website: https://retrocausal.ai/research/.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Ordinal Bias in Action Recognition for Instructional Videos</title>
<link>https://arxiv.org/abs/2504.06580</link>
<guid>https://arxiv.org/abs/2504.06580</guid>
<content:encoded><![CDATA[
arXiv:2504.06580v2 Announce Type: replace 
Abstract: Action recognition models have achieved promising results in understanding instructional videos. However, they often rely on dominant, dataset-specific action sequences rather than true video comprehension, a problem that we define as ordinal bias. To address this issue, we propose two effective video manipulation methods: Action Masking, which masks frames of frequently co-occurring actions, and Sequence Shuffling, which randomizes the order of action segments. Through comprehensive experiments, we demonstrate that current models exhibit significant performance drops when confronted with nonstandard action sequences, underscoring their vulnerability to ordinal bias. Our findings emphasize the importance of rethinking evaluation strategies and developing models capable of generalizing beyond fixed action patterns in diverse instructional videos.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChartQA-X: Generating Explanations for Visual Chart Reasoning</title>
<link>https://arxiv.org/abs/2504.13275</link>
<guid>https://arxiv.org/abs/2504.13275</guid>
<content:encoded><![CDATA[
arXiv:2504.13275v4 Announce Type: replace 
Abstract: The ability to explain complex information from chart images is vital for effective data-driven decision-making. In this work, we address the challenge of generating detailed explanations alongside answering questions about charts. We present ChartQA-X, a comprehensive dataset comprising 30,799 chart samples across four chart types, each paired with contextually relevant questions, answers, and explanations. Explanations are generated and selected based on metrics such as faithfulness, informativeness, coherence, and perplexity. Our human evaluation with 245 participants shows that model-generated explanations in ChartQA-X surpass human-written explanations in accuracy and logic and are comparable in terms of clarity and overall quality. Moreover, models fine-tuned on ChartQA-X show substantial improvements across various metrics, including absolute gains of up to 24.57 points in explanation quality, 18.96 percentage points in question-answering accuracy, and 14.75 percentage points on unseen benchmarks for the same task. By integrating explanatory narratives with answers, our approach enables agents to convey complex visual information more effectively, improving comprehension and greater trust in the generated responses.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3DHMR: Monocular 3D Hand Mesh Recovery</title>
<link>https://arxiv.org/abs/2505.20058</link>
<guid>https://arxiv.org/abs/2505.20058</guid>
<content:encoded><![CDATA[
arXiv:2505.20058v2 Announce Type: replace 
Abstract: Monocular 3D hand mesh recovery is challenging due to high degrees of freedom of hands, 2D-to-3D ambiguity and self-occlusion. Most existing methods are either inefficient or less straightforward for predicting the position of 3D mesh vertices. Thus, we propose a new pipeline called Monocular 3D Hand Mesh Recovery (M3DHMR) to directly estimate the positions of hand mesh vertices. M3DHMR provides 2D cues for 3D tasks from a single image and uses a new spiral decoder consist of several Dynamic Spiral Convolution (DSC) Layers and a Region of Interest (ROI) Layer. On the one hand, DSC Layers adaptively adjust the weights based on the vertex positions and extract the vertex features in both spatial and channel dimensions. On the other hand, ROI Layer utilizes the physical information and refines mesh vertices in each predefined hand region separately. Extensive experiments on popular dataset FreiHAND demonstrate that M3DHMR significantly outperforms state-of-the-art real-time methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Question Answering via only 2D Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.22143</link>
<guid>https://arxiv.org/abs/2505.22143</guid>
<content:encoded><![CDATA[
arXiv:2505.22143v2 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) have significantly advanced numerous fields. In this work, we explore how to harness their potential to address 3D scene understanding tasks, using 3D question answering (3D-QA) as a representative example. Due to the limited training data in 3D, we do not train LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a 3D point cloud and feed them into 2D models to answer a given question. When the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters the most. We propose cdViews, a novel approach to automatically selecting critical and diverse Views for 3D-QA. cdViews consists of two key components: viewSelector prioritizing critical views based on their potential to provide answer-specific information, and viewNMS enhancing diversity by removing redundant views based on spatial overlap. We evaluate cdViews on the widely-used ScanQA and SQA benchmarks, demonstrating that it achieves state-of-the-art performance in 3D-QA while relying solely on 2D models without fine-tuning. These findings support our belief that 2D LVLMs are currently the most effective alternative (of the resource-intensive 3D LVLMs) for addressing 3D tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning</title>
<link>https://arxiv.org/abs/2506.02738</link>
<guid>https://arxiv.org/abs/2506.02738</guid>
<content:encoded><![CDATA[
arXiv:2506.02738v3 Announce Type: replace 
Abstract: In biomedical vision-language modeling, datasets are typically mined from scientific literature, pairing compound figures with captions that are short, context-dependent, and oftern partially informative. Prior work on subfigure extraction has been limited in both dataset size and generalizability. In addition, no existing effort has incorporated rich medical context in image-text pairs. We revisit data curation as a foundational component of effective biomedical representation learning. Our data curation process integrates transformer-based subfigure detection, subcaption extraction, and contextual text enrichment derived from inline references. Our subfigure extraction model, trained on a corpus of 500,000 compound figures, achieves state-of-the-art performance on real and synthetic benchmarks. Using this process, we curate and release Open-PMC-18M, a large-scale high-fidelity biomedical dataset comprising 18 million image-text pairs, spanning radiology, microscopy, and visible light photography. We train vision-language models on our dataset and perform extensive evaluation on 6 retrieval and 19 zero-shot classification tasks across three major modalities. The models trained on our dataset set a new state-of-the-art results in medical representation learning. We release our dataset, models, and code to support reproducible benchmarks and further study into biomedical vision-language modeling and representation learning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Validation for Robust Few-Shot Recognition</title>
<link>https://arxiv.org/abs/2506.04713</link>
<guid>https://arxiv.org/abs/2506.04713</guid>
<content:encoded><![CDATA[
arXiv:2506.04713v2 Announce Type: replace 
Abstract: Few-Shot Recognition (FSR) tackles classification tasks by training with minimal task-specific labeled data. Prevailing methods adapt or finetune a pretrained Vision-Language Model (VLM) and augment the scarce training data by retrieving task-relevant but noisy samples from open data sources. The finetuned VLM generalizes decently well to the task-specific in-distribution (ID) test data but struggles with out-of-distribution (OOD) test data. This motivates our study of robust FSR with VLM finetuning. The core challenge of FSR is data scarcity, extending beyond limited training data to a complete lack of validation data. We identify a key paradox as a potential solution: repurposing the retrieved open data for validation. As such retrieved data are inherently OOD compared with the task-specific ID training data, finetuned VLMs yield degraded performance on the retrieved data. This causes the validation logic to favor the pretrained model without any finetuning, hindering improvements w.r.t generalization. To resolve this dilemma, we introduce a novel validation strategy that harmonizes performance gain and degradation on the few-shot ID data and the retrieved data, respectively. Our validation enables parameter selection for partial finetuning and checkpoint selection, mitigating overfitting and improving test-data generalization. We unify this strategy with robust learning into a cohesive framework: Validation-Enabled Stage-wise Tuning (VEST). Extensive experiments on the established ImageNet OOD benchmarks show that VEST significantly outperforms existing VLM adaptation methods, achieving state-of-the-art FSR performance on both ID and OOD data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion</title>
<link>https://arxiv.org/abs/2506.15747</link>
<guid>https://arxiv.org/abs/2506.15747</guid>
<content:encoded><![CDATA[
arXiv:2506.15747v2 Announce Type: replace 
Abstract: The single-view image guided point cloud completion (SVIPC) task aims to reconstruct a complete point cloud from a partial input with the help of a single-view image. While previous works have demonstrated the effectiveness of this multimodal approach, the fundamental necessity of image guidance remains largely unexamined. To explore this, we propose a strong baseline approach for SVIPC based on an attention-based multi-branch encoder-decoder network that only takes partial point clouds as input, view-free. Our hierarchical self-fusion mechanism, driven by cross-attention and self-attention layers, effectively integrates information across multiple streams, enriching feature representations and strengthening the networks ability to capture geometric structures. Extensive experiments and ablation studies on the ShapeNet-ViPC dataset demonstrate that our view-free framework performs superiorly to state-of-the-art SVIPC methods. We hope our findings provide new insights into the development of multimodal learning in SVIPC. Our demo code will be available at https://github.com/Zhang-VISLab.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Martian World Model: Controllable Video Synthesis with Physically Accurate 3D Reconstructions</title>
<link>https://arxiv.org/abs/2507.07978</link>
<guid>https://arxiv.org/abs/2507.07978</guid>
<content:encoded><![CDATA[
arXiv:2507.07978v2 Announce Type: replace 
Abstract: Synthesizing realistic Martian landscape videos is crucial for mission rehearsal and robotic simulation. However, this task poses unique challenges due to the scarcity of high-quality Martian data and the significant domain gap between Martian and terrestrial imagery. To address these challenges, we propose a holistic solution composed of two key components: 1) A data curation pipeline Multimodal Mars Synthesis (M3arsSynth), which reconstructs 3D Martian environments from real stereo navigation images, sourced from NASA's Planetary Data System (PDS), and renders high-fidelity multiview 3D video sequences. 2) A Martian terrain video generator, MarsGen, which synthesizes novel videos visually realistic and geometrically consistent with the 3D structure encoded in the data. Our M3arsSynth engine spans a wide range of Martian terrains and acquisition dates, enabling the generation of physically accurate 3D surface models at metric-scale resolution. MarsGen, fine-tuned on M3arsSynth data, synthesizes videos conditioned on an initial image frame and, optionally, camera trajectories or textual prompts, allowing for video generation in novel environments. Experimental results show that our approach outperforms video synthesis models trained on terrestrial datasets, achieving superior visual fidelity and 3D structural consistency.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains</title>
<link>https://arxiv.org/abs/2507.13326</link>
<guid>https://arxiv.org/abs/2507.13326</guid>
<content:encoded><![CDATA[
arXiv:2507.13326v2 Announce Type: replace 
Abstract: Hand-object interaction detection remains an open challenge in real-time applications, where intuitive user experiences depend on fast and accurate detection of interactions with surrounding objects. We propose an efficient approach for detecting hand-objects interactions from streaming egocentric vision that operates in real time. Our approach consists of an action recognition module and an object detection module for identifying active objects upon confirmed interaction. Our Mamba model with EfficientNetV2 as backbone for action recognition achieves 38.52% p-AP on the ENIGMA-51 benchmark at 30fps, while our fine-tuned YOLOWorld reaches 85.13% AP for hand and object. We implement our models in a cascaded architecture where the action recognition and object detection modules operate sequentially. When the action recognition predicts a contact state, it activates the object detection module, which in turn performs inference on the relevant frame to detect and classify the active object.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression</title>
<link>https://arxiv.org/abs/2507.14997</link>
<guid>https://arxiv.org/abs/2507.14997</guid>
<content:encoded><![CDATA[
arXiv:2507.14997v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based regression tasks, but current approaches face key limitations. Recent methods fine-tune MLLMs using preset output vocabularies and generic task-level prompts (e.g., "How would you rate this image?"), assuming this mimics human rating behavior. \textbf{Our analysis reveals that these approaches provide no benefit over image-only training}. Models using preset vocabularies and generic prompts perform equivalently to image-only models, failing to leverage semantic understanding from textual input. We propose \textbf{Regression via Transformer-Based Classification} (RvTC), which replaces vocabulary-constrained classification with a flexible bin-based approach. Unlike approaches that address discretization errors through complex distributional modeling, RvTC eliminates manual vocabulary crafting through straightforward bin increase, achieving state-of-the-art performance on four image assessment datasets using only images. \textbf{More importantly, we demonstrate that data-specific prompts dramatically improve performance}. Unlike generic task descriptions, prompts containing semantic information about specific images enable MLLMs to leverage cross-modal understanding. On the AVA dataset, adding challenge titles to prompts substantially improves our already state-of-the-art image-only baseline. We demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that MLLMs benefit from semantic prompt information, surpassing mere statistical biases. We validate RvTC across two different MLLM architectures, demonstrating consistent improvements and method generalizability.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perspective-Invariant 3D Object Detection</title>
<link>https://arxiv.org/abs/2507.17665</link>
<guid>https://arxiv.org/abs/2507.17665</guid>
<content:encoded><![CDATA[
arXiv:2507.17665v2 Announce Type: replace 
Abstract: With the rise of robotics, LiDAR-based 3D object detection has garnered significant attention in both academia and industry. However, existing datasets and methods predominantly focus on vehicle-mounted platforms, leaving other autonomous platforms underexplored. To bridge this gap, we introduce Pi3DET, the first benchmark featuring LiDAR data and 3D bounding box annotations collected from multiple platforms: vehicle, quadruped, and drone, thereby facilitating research in 3D object detection for non-vehicle platforms as well as cross-platform 3D detection. Based on Pi3DET, we propose a novel cross-platform adaptation framework that transfers knowledge from the well-studied vehicle platform to other platforms. This framework achieves perspective-invariant 3D detection through robust alignment at both geometric and feature levels. Additionally, we establish a benchmark to evaluate the resilience and robustness of current 3D detectors in cross-platform scenarios, providing valuable insights for developing adaptive 3D perception systems. Extensive experiments validate the effectiveness of our approach on challenging cross-platform tasks, demonstrating substantial gains over existing adaptation methods. We hope this work paves the way for generalizable and unified 3D perception systems across diverse and complex environments. Our Pi3DET dataset, cross-platform benchmark suite, and annotation toolkit have been made publicly available.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment</title>
<link>https://arxiv.org/abs/2509.01183</link>
<guid>https://arxiv.org/abs/2509.01183</guid>
<content:encoded><![CDATA[
arXiv:2509.01183v2 Announce Type: replace 
Abstract: High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks. The code is available at https://github.com/Yangbn97/SegAssess.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Instructed Reasoning for Group Activity Detection via Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2509.16054</link>
<guid>https://arxiv.org/abs/2509.16054</guid>
<content:encoded><![CDATA[
arXiv:2509.16054v2 Announce Type: replace 
Abstract: Group activity detection (GAD) aims to simultaneously identify group members and categorize their collective activities within video sequences. Existing deep learning-based methods develop specialized architectures (e.g., transformer networks) to model the dynamics of individual roles and semantic dependencies between individuals and groups. However, they rely solely on implicit pattern recognition from visual features and struggle with contextual reasoning and explainability. In this work, we propose LIR-GAD, a novel framework of language-instructed reasoning for GAD via Multimodal Large Language Model (MLLM). Our approach expand the original vocabulary of MLLM by introducing an activity-level  token and multiple cluster-specific  tokens. We process video frames alongside two specially designed tokens and language instructions, which are then integrated into the MLLM. The pretrained commonsense knowledge embedded in the MLLM enables the  token and  tokens to effectively capture the semantic information of collective activities and learn distinct representational features of different groups, respectively. Also, we introduce a multi-label classification loss to further enhance the  token's ability to learn discriminative semantic representations. Then, we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates MLLM's hidden embeddings corresponding to the designed tokens with visual features, significantly enhancing the performance of GAD. Both quantitative and qualitative experiments demonstrate the superior performance of our proposed method in GAD taks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-CECE: Visual Counterfactual Explanations via Conceptual Edits</title>
<link>https://arxiv.org/abs/2509.16567</link>
<guid>https://arxiv.org/abs/2509.16567</guid>
<content:encoded><![CDATA[
arXiv:2509.16567v2 Announce Type: replace 
Abstract: Recent black-box counterfactual generation frameworks fail to take into account the semantic content of the proposed edits, while relying heavily on training to guide the generation process. We propose a novel, plug-and-play black-box counterfactual generation framework, which suggests step-by-step edits based on theoretical guarantees of optimal edits to produce human-level counterfactual explanations with zero training. Our framework utilizes a pre-trained image editing diffusion model, and operates without access to the internals of the classifier, leading to an explainable counterfactual generation process. Throughout our experimentation, we showcase the explanatory gap between human reasoning and neural model behavior by utilizing both Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision Language Model (LVLM) classifiers, substantiated through a comprehensive human evaluation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning</title>
<link>https://arxiv.org/abs/2509.19552</link>
<guid>https://arxiv.org/abs/2509.19552</guid>
<content:encoded><![CDATA[
arXiv:2509.19552v3 Announce Type: replace 
Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc dash-cam driving video analysis is challenging due to their general-purpose training and lack of structured inductive biases. As vision is often the sole modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing video-based vision-language models (V-VLMs) struggle with spatial reasoning, causal inference, and explainability of events in the input video. To this end, we introduce iFinder, a structured semantic grounding framework that decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure for LLMs. iFinder operates as a modular, training-free pipeline that employs pretrained vision models to extract critical cues -- object pose, lane positions, and object trajectories -- which are hierarchically organized into frame- and video-level structures. Combined with a three-block prompting strategy, it enables step-wise, grounded reasoning for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning. Evaluations on four public dash-cam video benchmarks show that iFinder's proposed grounding with domain-specific cues, especially object orientation and global context, significantly outperforms end-to-end V-VLMs on four zero-shot driving benchmarks, with up to 39% gains in accident reasoning accuracy. By grounding LLMs with driving domain-specific representations, iFinder offers a zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for post-hoc driving video understanding.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Grounding IDs: How External Cues Shape Multimodal Binding</title>
<link>https://arxiv.org/abs/2509.24072</link>
<guid>https://arxiv.org/abs/2509.24072</guid>
<content:encoded><![CDATA[
arXiv:2509.24072v3 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) show strong performance across multimodal benchmarks but remain limited in structured reasoning and precise grounding. Recent work has demonstrated that adding simple visual structures, such as partitions and annotations, improves accuracy, yet the internal mechanisms underlying these gains remain unclear. We investigate this phenomenon and propose the concept of Grounding IDs, latent identifiers induced by external cues that bind objects to their designated partitions across modalities. Through representation analysis, we find that these identifiers emerge as consistent within-partition alignment in embedding space and reduce the modality gap between image and text. Causal interventions further confirm that these identifiers mediate binding between objects and symbolic cues. We show that Grounding IDs strengthen attention between related components, which in turn improves cross-modal grounding and reduces hallucinations. Taken together, our results identify Grounding IDs as a key symbolic mechanism that explains how external cues enhance multimodal binding and offer both interpretability and practical improvements.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZQBA: Zero Query Black-box Adversarial Attack</title>
<link>https://arxiv.org/abs/2510.00769</link>
<guid>https://arxiv.org/abs/2510.00769</guid>
<content:encoded><![CDATA[
arXiv:2510.00769v2 Announce Type: replace 
Abstract: Current black-box adversarial attacks either require multiple queries or diffusion models to produce adversarial samples that can impair the target model performance. However, these methods require training a surrogate loss or diffusion models to produce adversarial samples, which limits their applicability in real-world settings. Thus, we propose a Zero Query Black-box Adversarial (ZQBA) attack that exploits the representations of Deep Neural Networks (DNNs) to fool other networks. Instead of requiring thousands of queries to produce deceiving adversarial samples, we use the feature maps obtained from a DNN and add them to clean images to impair the classification of a target model. The results suggest that ZQBA can transfer the adversarial samples to different models and across various datasets, namely CIFAR and Tiny ImageNet. The experiments also show that ZQBA is more effective than state-of-the-art black-box attacks with a single query, while maintaining the imperceptibility of perturbations, evaluated both quantitatively (SSIM) and qualitatively, emphasizing the vulnerabilities of employing DNNs in real-world contexts. All the source code is available at https://github.com/Joana-Cabral/ZQBA.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging</title>
<link>https://arxiv.org/abs/2510.01498</link>
<guid>https://arxiv.org/abs/2510.01498</guid>
<content:encoded><![CDATA[
arXiv:2510.01498v2 Announce Type: replace 
Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming</title>
<link>https://arxiv.org/abs/2510.01660</link>
<guid>https://arxiv.org/abs/2510.01660</guid>
<content:encoded><![CDATA[
arXiv:2510.01660v4 Announce Type: replace 
Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for every new source-and-target pair, resulting in the number of training parameters and storage memory growing linearly with each new pair, and also preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases, we propose making use of domain-specific textural bias for domain adaptation via visual reprogramming, namely VirDA. Instead of fine-tuning the full backbone, VirDA prepends a domain-specific visual reprogramming layer to the backbone. This layer produces visual prompts that act as an added textural bias to the input image, adapting its "style" to a target domain. To optimize these visual reprogramming layers, we use multiple objective functions that optimize the intra- and inter-domain distribution differences when domain-adapting visual prompts are applied. This process does not require modifying the backbone parameters, allowing the same backbone to be reused across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M trainable parameters. VirDA surpasses PDA, the state-of-the-art parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8% of their trainable parameters. Relative to the strongest current methods (PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only 2.2% and 1.1% accuracy, respectively.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempoControl: Temporal Attention Guidance for Text-to-Video Models</title>
<link>https://arxiv.org/abs/2510.02226</link>
<guid>https://arxiv.org/abs/2510.02226</guid>
<content:encoded><![CDATA[
arXiv:2510.02226v2 Announce Type: replace 
Abstract: Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal pattern with a control signal (correlation), adjusting its strength where visibility is required (magnitude), and preserving semantic consistency (entropy). TempoControl provides precise temporal control while maintaining high video quality and diversity. We demonstrate its effectiveness across various applications, including temporal reordering of single and multiple objects, action timing, and audio-aligned video generation. Please see our project page for more details: https://shira-schiber.github.io/TempoControl/.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TeleEgo: Benchmarking Egocentric AI Assistants in the Wild</title>
<link>https://arxiv.org/abs/2510.23981</link>
<guid>https://arxiv.org/abs/2510.23981</guid>
<content:encoded><![CDATA[
arXiv:2510.23981v3 Announce Type: replace 
Abstract: Egocentric AI assistants in real-world settings must process multi-modal inputs (video, audio, text), respond in real time, and retain evolving long-term memory. However, existing benchmarks typically evaluate these abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming, omni-modal benchmark for evaluating egocentric AI assistants in realistic daily contexts. The dataset features over 14 hours per participant of synchronized egocentric video, audio, and text across four domains: work \& study, lifestyle \& routines, social activities, and outings \& culture. All data is aligned on a unified global timeline and includes high-quality visual narrations and speech transcripts, curated through human refinement.TeleEgo defines 12 diagnostic subtasks across three core capabilities: Memory (recalling past events), Understanding (interpreting the current moment), and Cross-Memory Reasoning (linking distant events). It contains 3,291 human-verified QA items spanning multiple question formats (single-choice, binary, multi-choice, and open-ended), evaluated strictly in a streaming setting. We propose Real-Time Accuracy (RTA) to jointly capture correctness and responsiveness under tight decision windows, and Memory Persistence Time (MPT) as a forward-looking metric for long-term retention in continuous streams. In this work, we report RTA results for current models and release TeleEgo, together with an MPT evaluation framework, as a realistic and extensible benchmark for future egocentric assistants with stronger streaming memory, enabling systematic study of both real-time behavior and long-horizon memory.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Saliency-Guided Contrastive Embeddings</title>
<link>https://arxiv.org/abs/2511.12744</link>
<guid>https://arxiv.org/abs/2511.12744</guid>
<content:encoded><![CDATA[
arXiv:2511.12744v2 Announce Type: replace 
Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views</title>
<link>https://arxiv.org/abs/2511.12878</link>
<guid>https://arxiv.org/abs/2511.12878</guid>
<content:encoded><![CDATA[
arXiv:2511.12878v3 Announce Type: replace 
Abstract: Forecasting how human hands move in egocentric views is critical for applications like augmented reality and human-robot policy transfer. Recently, several hand trajectory prediction (HTP) methods have been developed to generate future possible hand waypoints, which still suffer from insufficient prediction targets, inherent modality gaps, entangled hand-head motion, and limited validation in downstream tasks. To address these limitations, we present a universal hand motion forecasting framework considering multi-modal input, multi-dimensional and multi-target prediction patterns, and multi-task affordances for downstream applications. We harmonize multiple modalities by vision-language fusion, global context incorporation, and task-aware text embedding injection, to forecast hand waypoints in both 2D and 3D spaces. A novel dual-branch diffusion is proposed to concurrently predict human head and hand movements, capturing their motion synergy in egocentric vision. By introducing target indicators, the prediction model can forecast the specific joint waypoints of the wrist or the fingers, besides the widely studied hand center points. In addition, we enable Uni-Hand to additionally predict hand-object interaction states (contact/separation) to facilitate downstream tasks better. As the first work to incorporate downstream task evaluation in the literature, we build novel benchmarks to assess the real-world applicability of hand motion forecasting algorithms. The experimental results on multiple publicly available datasets and our newly proposed benchmarks demonstrate that Uni-Hand achieves the state-of-the-art performance in multi-dimensional and multi-target hand motion forecasting. Extensive validation in multiple downstream tasks also presents its impressive human-robot policy transfer to enable robotic manipulation, and effective feature enhancement for action anticipation/recognition.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2511.13026</link>
<guid>https://arxiv.org/abs/2511.13026</guid>
<content:encoded><![CDATA[
arXiv:2511.13026v2 Announce Type: replace 
Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiniBench: Infinite Benchmarking for Visual Spatial Reasoning with Customizable Scene Complexity</title>
<link>https://arxiv.org/abs/2511.18200</link>
<guid>https://arxiv.org/abs/2511.18200</guid>
<content:encoded><![CDATA[
arXiv:2511.18200v2 Announce Type: replace 
Abstract: Modern vision-language models (VLMs) are expected to have abilities of spatial reasoning with diverse scene complexities, but evaluating such abilities is difficult due to the lack of benchmarks that are not only diverse and scalable but also fully customizable. Existing benchmarks offer limited customizability over the scene complexity and are incapable of isolating and analyzing specific VLM failure modes under distinct spatial conditions. To address this gap, instead of individually presenting benchmarks for different scene complexities, in this paper we present InfiniBench, a fully automated, customizable and user-friendly benchmark generator that can synthesize a theoretically infinite variety of 3D scenes with parameterized control on scene complexity. InfiniBench uniquely translates scene descriptions in natural language into photo-realistic videos with complex and physically plausible 3D layouts. This is achieved through three key innovations: 1) a LLM-based agentic framework that iteratively refines procedural scene constraints from scene descriptions; 2) a flexible cluster-based layout optimizer that generates dense and cluttered scenes previously intractable for procedural methods; and 3) a task-aware camera trajectory optimization method that renders scenes into videos with full object coverage as VLM input. Experiments demonstrate that InfiniBench outperforms state-of-the-art procedural and LLM-based 3D generation methods in prompt fidelity and physical plausibility, especially in high-complexity scenarios. We further showcased the usefulness of InfiniBench, by generating benchmarks for representative spatial reasoning tasks including measurement, perspective-taking and spatiotemporal tracking.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MHB: Multimodal Handshape-aware Boundary Detection for Continuous Sign Language Recognition</title>
<link>https://arxiv.org/abs/2511.19907</link>
<guid>https://arxiv.org/abs/2511.19907</guid>
<content:encoded><![CDATA[
arXiv:2511.19907v2 Announce Type: replace 
Abstract: This paper employs a multimodal approach for continuous sign recognition by first using ML for detecting the start and end frames of signs in videos of American Sign Language (ASL) sentences, and then by recognizing the segmented signs. For improved robustness we use 3D skeletal features extracted from sign language videos to take into account the convergence of sign properties and their dynamics that tend to cluster at sign boundaries. Another focus of this paper is the incorporation of information from 3D handshape for boundary detection. To detect handshapes normally expected at the beginning and end of signs, we pretrain a handshape classifier for detection of 87 linguistically defined canonical handshape categories using a dataset that we created by integrating and normalizing several existing datasets. A multimodal fusion module is then used to unify the pretrained sign video segmentation framework and handshape classification models. Finally, the estimated boundaries are used for sign recognition, where the recognition model is trained on a large database containing both citation-form isolated signs and signs pre-segmented (based on manual annotations) from continuous signing-as such signs often differ a bit in certain respects. We evaluate our method on the ASLLRP corpus and demonstrate significant improvements over previous work.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure is Supervision: Multiview Masked Autoencoders for Radiology</title>
<link>https://arxiv.org/abs/2511.22294</link>
<guid>https://arxiv.org/abs/2511.22294</guid>
<content:encoded><![CDATA[
arXiv:2511.22294v3 Announce Type: replace 
Abstract: Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-Only Universal Adversarial Attacks in Distributed Learning</title>
<link>https://arxiv.org/abs/2411.10500</link>
<guid>https://arxiv.org/abs/2411.10500</guid>
<content:encoded><![CDATA[
arXiv:2411.10500v2 Announce Type: replace-cross 
Abstract: Distributed learning frameworks, which partition neural network models across multiple computing nodes, enhance efficiency in collaborative edge-cloud systems, but may also introduce new vulnerabilities to evasion attacks, often in the form of adversarial perturbations. In this work, we present a new threat model that explores the feasibility of generating universal adversarial perturbations (UAPs) when the attacker has access only to the edge portion of the model, consisting of its initial network layers. Unlike traditional attacks that require full model knowledge, our approach shows that adversaries can induce effective mispredictions in the unknown cloud component by manipulating key feature representations at the edge. Following the proposed threat model, we introduce both edge-only untargeted and targeted formulations of UAPs designed to control intermediate features before the split point. Our results on ImageNet demonstrate strong attack transferability to the unknown cloud part, and we compare the proposed method with classical white-box and black-box techniques, highlighting its effectiveness. Additionally, we analyze the capability of an attacker to achieve targeted adversarial effects with edge-only knowledge, revealing intriguing behaviors across multiple networks. By introducing the first adversarial attacks with edge-only knowledge in split inference, this work underscores the importance of addressing partial model access in adversarial robustness, encouraging further research in this area.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-centric Token Compression in Large Language Model</title>
<link>https://arxiv.org/abs/2502.00791</link>
<guid>https://arxiv.org/abs/2502.00791</guid>
<content:encoded><![CDATA[
arXiv:2502.00791v4 Announce Type: replace-cross 
Abstract: Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The project is at https://github.com/CSU-JPG/VIST.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSM: Hierarchical Scene Motifs for Multi-Scale Indoor Scene Generation</title>
<link>https://arxiv.org/abs/2503.16848</link>
<guid>https://arxiv.org/abs/2503.16848</guid>
<content:encoded><![CDATA[
arXiv:2503.16848v3 Announce Type: replace-cross 
Abstract: Despite advances in indoor 3D scene layout generation, synthesizing scenes with dense object arrangements remains challenging. Existing methods focus on large furniture while neglecting smaller objects, resulting in unrealistically empty scenes. Those that place small objects typically do not honor arrangement specifications, resulting in largely random placement not following the text description. We present Hierarchical Scene Motifs (HSM): a hierarchical framework for indoor scene generation with dense object arrangements across spatial scales. Indoor scenes are inherently hierarchical, with surfaces supporting objects at different scales, from large furniture on floors to smaller objects on tables and shelves. HSM embraces this hierarchy and exploits recurring cross-scale spatial patterns to generate complex and realistic scenes in a unified manner. Our experiments show that HSM outperforms existing methods by generating scenes that better conform to user input across room types and spatial configurations. Project website is available at https://3dlg-hcvc.github.io/hsm .
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight</title>
<link>https://arxiv.org/abs/2504.20454</link>
<guid>https://arxiv.org/abs/2504.20454</guid>
<content:encoded><![CDATA[
arXiv:2504.20454v2 Announce Type: replace-cross 
Abstract: This study integrates PET metabolic information with CT anatomical structures to establish a 3D multimodal segmentation dataset for lymphoma based on whole-body FDG PET/CT examinations, which bridges the gap of the lack of standardised multimodal segmentation datasets in the field of haematological malignancies. We retrospectively collected 483 examination datasets acquired between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were rigorously de-identified. Complete 3D structural information was preserved during data acquisition, preprocessing and annotation, and a high-quality dataset was constructed based on the nnUNet format. By systematic technical validation and evaluation of the preprocessing process, annotation quality and automatic segmentation algorithm, the deep learning model trained based on this dataset is verified to achieve accurate segmentation of lymphoma lesions in PET/CT images with high accuracy, good robustness and reproducibility, which proves the applicability and stability of this dataset in accurate segmentation and quantitative analysis. The deep fusion of PET/CT images achieved with this dataset not only significantly improves the accurate portrayal of the morphology, location and metabolic features of tumour lesions, but also provides solid data support for early diagnosis, clinical staging and personalized treatment, and promotes the development of automated image segmentation and precision medicine based on deep learning. The dataset and related resources are available at https://github.com/SuperD0122/LymphAtlas-.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.07413</link>
<guid>https://arxiv.org/abs/2506.07413</guid>
<content:encoded><![CDATA[
arXiv:2506.07413v3 Announce Type: replace-cross 
Abstract: Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies. Our code is available at https://github.com/ziwenwang28/VarContrast.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting</title>
<link>https://arxiv.org/abs/2506.13348</link>
<guid>https://arxiv.org/abs/2506.13348</guid>
<content:encoded><![CDATA[
arXiv:2506.13348v2 Announce Type: replace-cross 
Abstract: Gaussian Splatting have demonstrated remarkable novel view synthesis performance at high rendering frame rates. Optimization-based inverse rendering within complex capture scenarios remains however a challenging problem. A particular case is modelling complex surface light interactions for highly reflective scenes, which results in intricate high frequency specular radiance components. We hypothesize that such challenging settings can benefit from increased representation power. We hence propose a method that tackles this issue through a geometrically and physically grounded Gaussian Splatting borne radiance field, where normals and material properties are spatially variable in the primitive's local space. Using per-primitive texture maps for this purpose, we also propose to harness the GPU hardware to accelerate rendering at test time via unified material texture atlas. Code will be available at https://github.com/maeyounes/TextureSplat
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title>
<link>https://arxiv.org/abs/2506.16402</link>
<guid>https://arxiv.org/abs/2506.16402</guid>
<content:encoded><![CDATA[
arXiv:2506.16402v3 Announce Type: replace-cross 
Abstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems. Code and data are released under https://github.com/AI45Lab/IS-Bench.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation</title>
<link>https://arxiv.org/abs/2507.18262</link>
<guid>https://arxiv.org/abs/2507.18262</guid>
<content:encoded><![CDATA[
arXiv:2507.18262v3 Announce Type: replace-cross 
Abstract: Semantics-driven 3D spatial constraints align highlevel semantic representations with low-level action spaces, facilitating the unification of task understanding and execution in robotic manipulation. The synergistic reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation Models (VFMs) enables cross-modal 3D spatial constraint construction. Nevertheless, existing methods have three key limitations: (1) coarse semantic granularity in constraint modeling, (2) lack of real-time closed-loop planning, (3) compromised robustness in semantically diverse environments. To address these challenges, we propose ReSem3D, a unified manipulation framework for semantically diverse environments, leveraging the synergy between VFMs and MLLMs to achieve fine-grained visual grounding and dynamically constructs hierarchical 3D spatial constraints for real-time manipulation. Specifically, the framework is driven by hierarchical recursive reasoning in MLLMs, which interact with VFMs to automatically construct 3D spatial constraints from natural language instructions and RGB-D observations in two stages: part-level extraction and region-level refinement. Subsequently, these constraints are encoded as real-time optimization objectives in joint space, enabling reactive behavior to dynamic disturbances. Extensive simulation and real-world experiments are conducted in semantically rich household and sparse chemical lab environments. The results demonstrate that ReSem3D performs diverse manipulation tasks under zero-shot conditions, exhibiting strong adaptability and generalization. Code and videos are available at https://github.com/scy-v/ReSem3D and https://resem3d.github.io.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments</title>
<link>https://arxiv.org/abs/2510.23928</link>
<guid>https://arxiv.org/abs/2510.23928</guid>
<content:encoded><![CDATA[
arXiv:2510.23928v2 Announce Type: replace-cross 
Abstract: In this paper, we propose an adaptive keyframe selection method for improved 3D scene reconstruction in dynamic environments. The proposed method integrates two complementary modules: an error-based selection module utilizing photometric and structural similarity (SSIM) errors, and a momentum-based update module that dynamically adjusts keyframe selection thresholds according to scene motion dynamics. By dynamically curating the most informative frames, our approach addresses a key data bottleneck in real-time perception. This allows for the creation of high-quality 3D world representations from a compressed data stream, a critical step towards scalable robot learning and deployment in complex, dynamic environments. Experimental results demonstrate significant improvements over traditional static keyframe selection strategies, such as fixed temporal intervals or uniform frame skipping. These findings highlight a meaningful advancement toward adaptive perception systems that can dynamically respond to complex and evolving visual scenes. We evaluate our proposed adaptive keyframe selection module on two recent state-of-the-art 3D reconstruction networks, Spann3r and CUT3R, and observe consistent improvements in reconstruction quality across both frameworks. Furthermore, an extensive ablation study confirms the effectiveness of each individual component in our method, underlining their contribution to the overall performance gains.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</title>
<link>https://arxiv.org/abs/2511.04555</link>
<guid>https://arxiv.org/abs/2511.04555</guid>
<content:encoded><![CDATA[
arXiv:2511.04555v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control</title>
<link>https://arxiv.org/abs/2511.07820</link>
<guid>https://arxiv.org/abs/2511.07820</guid>
<content:encoded><![CDATA[
arXiv:2511.07820v2 Announce Type: replace-cross 
Abstract: Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited set of behaviors, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leveraging dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video</title>
<link>https://arxiv.org/abs/2511.18322</link>
<guid>https://arxiv.org/abs/2511.18322</guid>
<content:encoded><![CDATA[
arXiv:2511.18322v2 Announce Type: replace-cross 
Abstract: Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fractional Variational Approach to Spectral Filtering Using the Fourier Transform</title>
<link>https://arxiv.org/abs/2511.20675</link>
<guid>https://arxiv.org/abs/2511.20675</guid>
<content:encoded><![CDATA[
arXiv:2511.20675v2 Announce Type: replace-cross 
Abstract: The interference of fluorescence signals and noise remains a significant challenge in Raman spectrum analysis, often obscuring subtle spectral features that are critical for accurate analysis. Inspired by variational methods similar to those used in image denoising, our approach minimizes a functional involving fractional derivatives to balance noise suppression with the preservation of essential chemical features of the signal, such as peak position, intensity, and area. The original problem is reformulated in the frequency domain through the Fourier transform, making the implementation simple and fast. In this work, we discuss the theoretical framework, practical implementation, and the advantages and limitations of this method in the context of {simulated} Raman data, as well as in image processing. The main contribution of this article is the combination of a variational approach in the frequency domain, the use of fractional derivatives, and the optimization of the {regularization parameter and} derivative order through the concept of Shannon entropy. This work explores how the fractional order, combined with the regularization parameter, affects noise removal and preserves the essential features of the spectrum {and image}. Finally, the study shows that the combination of the proposed strategies produces an efficient, robust, and easily implementable filter.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SO-Bench: A Structural Output Evaluation of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.21750</link>
<guid>https://arxiv.org/abs/2511.21750</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, schema-grounded extraction, visual inputs, structured reasoning, SO-Bench  

<br /><br />Summary:  
1. Multimodal large language models (MLLMs) are increasingly used in real-world scenarios requiring outputs that are not only accurate but also comply with predefined data schemas.  
2. Despite advances in structured output generation for textual data, there is no existing benchmark to evaluate schema-grounded information extraction and reasoning specifically over visual inputs.  
3. The authors introduce SO-Bench, a comprehensive benchmark covering four visual domains: UI screens, natural images, documents, and charts. SO-Bench is built from over 6,500 diverse JSON schemas and 1,800 curated image-schema pairs validated by humans.  
4. Benchmarking experiments on both open-source and proprietary models indicate significant gaps in the ability of MLLMs to produce accurate and schema-compliant outputs, exposing the need for improved multimodal structured reasoning capabilities.  
5. Beyond evaluation, the authors conduct training experiments that substantially enhance a model’s ability to generate structured outputs in visual contexts. They plan to release SO-Bench to benefit the research community and accelerate progress in this area. <div>
arXiv:2511.21750v2 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment</title>
<link>https://arxiv.org/abs/2511.22345</link>
<guid>https://arxiv.org/abs/2511.22345</guid>
<content:encoded><![CDATA[
<div> Keywords: Normalizing Flows, generative models, invertibility, feature alignment, test-time optimization<br /><br />Summary:<br /><br />This paper addresses limitations in the generative quality of Normalizing Flows (NFs), which are generative models featuring an invertible architecture. The forward pass of NFs transforms data into latent space for density estimation, while the reverse pass generates new data samples, linking representation learning and data generation. Traditional log-likelihood optimization leads to suboptimal semantic representations, reducing generative performance. To overcome this, the authors propose a novel alignment strategy that uniquely exploits NF invertibility by aligning intermediate features from the generative (reverse) pass with embeddings from powerful vision foundation models, rather than regularizing the forward pass. This method achieves better results than naive alignment techniques. Additionally, the paper introduces a novel, training-free, test-time optimization algorithm for classification, offering an intrinsic measure of the semantic knowledge embedded in NFs. Experimental results show that the approach accelerates NF training over 3.3 times and simultaneously enhances generative quality and classification accuracy. Notably, the approach establishes new state-of-the-art performance for NFs on ImageNet datasets at resolutions 64×64 and 256×256. The authors provide public access to their implementation via a GitHub repository. <div>
arXiv:2511.22345v2 Announce Type: replace 
Abstract: Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3$\times$, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64$\times$64 and 256$\times$256. Our code is available at https://github.com/MCG-NJU/FlowBack.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JarvisEvo: Towards a Self-Evolving Photo Editing Agent with Synergistic Editor-Evaluator Optimization</title>
<link>https://arxiv.org/abs/2511.23002</link>
<guid>https://arxiv.org/abs/2511.23002</guid>
<content:encoded><![CDATA[
<div> Agent-based editing, instruction hallucination, reward hacking, multimodal chain-of-thought, policy optimization<br /><br />Summary:<br /><br />This paper introduces JarvisEvo, an advanced image editing agent designed to overcome two major challenges in interactive editing models: instruction hallucination and reward hacking. First, JarvisEvo addresses instruction hallucination by employing an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism, which improves the agent’s ability to follow instructions accurately and enhances the overall editing quality. Second, to tackle reward hacking, the authors propose a synergistic editor-evaluator policy optimization (SEPO) framework. This framework allows the agent to self-improve iteratively without relying on external reward functions, effectively preventing exploitation of reward model flaws. Third, JarvisEvo supports both global and local fine-grained image editing by seamlessly integrating with Adobe Lightroom, providing versatile control over the editing process. The system emulates an expert human designer, iteratively editing, selecting suitable tools, evaluating results, and reflecting on decisions to refine the output continuously. Evaluations on the ArtEdit-Bench dataset show that JarvisEvo significantly outperforms the prior state-of-the-art model Nano-Banana, with an average improvement of 18.95% on preservative editing metrics and a remarkable 44.96% gain in pixel-level content fidelity. The project demonstrates a promising direction for creating more reliable and effective autonomous image editing agents. <div>
arXiv:2511.23002v2 Announce Type: replace 
Abstract: Agent-based editing models have substantially advanced interactive experiences, processing quality, and creative flexibility. However, two critical challenges persist: (1) instruction hallucination, text-only chain-of-thought (CoT) reasoning cannot fully prevent factual errors due to inherent information bottlenecks; (2) reward hacking, dynamic policy optimization against static reward models allows agents to exploit flaws in reward functions. To address these issues, we propose JarvisEvo, a unified image editing agent that emulates an expert human designer by iteratively editing, selecting appropriate tools, evaluating results, and reflecting on its own decisions to refine outcomes. JarvisEvo offers three key advantages: (1) an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism that enhances instruction following and editing quality; (2) a synergistic editor-evaluator policy optimization (SEPO) framework that enables self-improvement without external rewards, effectively mitigating reward hacking; and (3) support for both global and local fine-grained editing through seamless integration of Adobe Lightroom. On ArtEdit-Bench, JarvisEvo outperforms Nano-Banana by an average of 18.95% on preservative editing metrics, including a substantial 44.96% improvement in pixel-level content fidelity. Project page: https://jarvisevo.vercel.app/
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.23075</link>
<guid>https://arxiv.org/abs/2511.23075</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, spatial reasoning, camera-guided fusion, dual-encoder architecture, 3D understanding<br /><br />Summary:<br /><br />Large vision-language models (VLMs) demonstrate strong multimodal understanding but face challenges in 3D spatial reasoning tasks such as distance estimation, size comparison, and maintaining cross-view consistency. Existing approaches either rely on additional 3D data or incorporate geometry encoders with RGB-only models via shallow feature fusion, limiting effectiveness. SpaceMind is proposed as a new multimodal large language model designed to perform spatial reasoning using only RGB inputs. It employs a dual-encoder architecture combining VGGT for spatial understanding and InternViT for 2D visual encoding. A key innovation is treating camera representation as an active guiding modality, rather than passive metadata, through a novel Camera-Guided Modality Fusion module. This module introduces camera-conditioned bias to spatial tokens, assigns query-independent weights based on geometric importance, and gates the fused representation using the camera embedding before processing by the language model. Empirical evaluation shows SpaceMind achieves state-of-the-art results on VSI-Bench, SQA3D, and SPBench benchmarks, outperforming both open-source and proprietary systems by wide margins. These findings highlight camera-guided modality fusion as an effective inductive bias, enabling VLMs to acquire genuinely spatially grounded intelligence. The authors plan to release code and model checkpoints to facilitate future research. <div>
arXiv:2511.23075v2 Announce Type: replace 
Abstract: Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Flicker: Detecting Kinematic Inconsistencies for Generalizable Deepfake Video Detection</title>
<link>https://arxiv.org/abs/2512.04175</link>
<guid>https://arxiv.org/abs/2512.04175</guid>
<content:encoded><![CDATA[
<div> Deepfake detection, video manipulation, facial landmarks, motion inconsistencies, autoencoder<br /><br />Summary:<br /><br />This paper addresses the challenge of generalizing deepfake detection to unseen manipulations, specifically focusing on videos rather than static images. Existing methods primarily detect temporal artifacts as frame-to-frame instabilities but neglect the natural motion dependencies that exist between different facial regions. To overcome this limitation, the authors propose a novel synthetic video generation method that introduces subtle kinematic inconsistencies by disrupting natural correlations in facial movements. They achieve this by training an autoencoder to decompose facial landmark configurations into motion bases, which they then manipulate selectively. These manipulated motion bases enable the creation of training videos with biomechanical flaws via face morphing techniques. Networks trained on this synthetic data gain the ability to detect sophisticated biomechanical inconsistencies that are hard to spot with conventional methods. Experimental results demonstrate that the proposed approach achieves state-of-the-art generalization performance on multiple popular deepfake detection benchmarks. This work advances the robustness of video deepfake detectors by exploiting nuanced biomechanical motion cues, offering a promising direction for more reliable detection of unseen facial video manipulations. <div>
arXiv:2512.04175v1 Announce Type: new 
Abstract: Generalizing deepfake detection to unseen manipulations remains a key challenge. A recent approach to tackle this issue is to train a network with pristine face images that have been manipulated with hand-crafted artifacts to extract more generalizable clues. While effective for static images, extending this to the video domain is an open issue. Existing methods model temporal artifacts as frame-to-frame instabilities, overlooking a key vulnerability: the violation of natural motion dependencies between different facial regions. In this paper, we propose a synthetic video generation method that creates training data with subtle kinematic inconsistencies. We train an autoencoder to decompose facial landmark configurations into motion bases. By manipulating these bases, we selectively break the natural correlations in facial movements and introduce these artifacts into pristine videos via face morphing. A network trained on our data learns to spot these sophisticated biomechanical flaws, achieving state-of-the-art generalization results on several popular benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology</title>
<link>https://arxiv.org/abs/2512.04187</link>
<guid>https://arxiv.org/abs/2512.04187</guid>
<content:encoded><![CDATA[
<div> Keywords: OnSight Pathology, AI histological analysis, digital pathology, real-time inference, multi-modal chat assistant<br /><br />Summary:  
The article introduces OnSight Pathology, a platform-agnostic computer vision software designed to provide real-time artificial intelligence (AI) inferences during digital slide image review, addressing challenges in the deployment of proprietary digital pathology solutions. The software operates locally on consumer-grade personal computers via a single executable file, avoiding complex software integrations and enabling cost-effective, secure use in research and clinical workflows. The utility of OnSight Pathology is demonstrated using over 2,500 publicly available whole slide images across various viewers and clinical cases, showcasing its robustness in key histopathological tasks such as brain tumor classification, mitosis detection, and immunohistochemical stain quantification. A novel built-in multi-modal chat assistant enhances quality control by delivering verifiable, flexible image descriptions without rigid class labels. Additionally, the software supports live microscope camera feeds, including those from smartphones, highlighting its potential application in analog, inter-operative, and telepathology settings. Overall, OnSight Pathology facilitates broad adoption of AI tools in histopathology by overcoming barriers to real-world deployment and streamlining AI integration into diverse pathology workflows. <div>
arXiv:2512.04187v1 Announce Type: new 
Abstract: The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers</title>
<link>https://arxiv.org/abs/2512.04213</link>
<guid>https://arxiv.org/abs/2512.04213</guid>
<content:encoded><![CDATA[
<div> multi-camera tracking, transformer, attention mechanism, 3D point representation, temporal consistency<br /><br />Summary:<br /><br />1. The paper introduces LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture designed for multi-camera point tracking that integrates appearance-based matching with geometric constraints. <br />2. Unlike traditional pipelines that separate detection, association, and tracking—leading to error propagation and temporal inconsistency—LAPA jointly reasons across views and time using attention mechanisms to improve robustness in challenging scenarios. <br />3. A key innovation is the cross-view attention mechanism enhanced by geometric priors, which establishes soft correspondences without relying on classical triangulation, instead constructing 3D point representations through attention-weighted aggregation. This approach inherently handles uncertainty and partial observations. <br />4. Temporal consistency is ensured by a transformer decoder that models long-range dependencies, enabling identity preservation through extended occlusions. <br />5. LAPA’s performance is validated on challenging datasets, including newly created multi-camera versions of TAPVid-3D panoptic and PointOdyssey, where it achieves substantial improvements—37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC—especially excelling in complex motion and occlusion scenarios. The codebase is publicly available for further research and application. <div>
arXiv:2512.04213v1 Announce Type: new 
Abstract: This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning</title>
<link>https://arxiv.org/abs/2512.04219</link>
<guid>https://arxiv.org/abs/2512.04219</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical event segmentation, predictive learning, streaming video, temporal abstraction, unsupervised learning  

<br /><br />Summary:  
1. The paper introduces PARSE, a novel unsupervised framework designed to segment video into a hierarchy of events at multiple temporal scales, reflecting the natural human perception of nested actions within coarser routines.  
2. PARSE employs a hierarchy of recurrent predictors where lower layers focus on short-term dynamics and higher layers capture longer-term context through attention-based feedback, enabling temporal granularity in processing.  
3. Event boundaries are identified as transient peaks in the prediction error of the model, which naturally lead to the emergence of nested event structures that respect containment relations similar to human event perception.  
4. The framework is evaluated on three established benchmarks—Breakfast Actions, 50 Salads, and Assembly 101—where it achieves state-of-the-art performance among streaming methods and competes closely with offline methods in temporal alignment and structural consistency metrics (H-GEBD, TED, hF1).  
5. These results underline the effectiveness of predictive learning under uncertainty for scalable, hierarchical temporal abstraction and compositional event understanding, pushing forward the field of computer vision toward human-like event segmentation and anticipation. <div>
arXiv:2512.04219v1 Announce Type: new 
Abstract: Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis</title>
<link>https://arxiv.org/abs/2512.04221</link>
<guid>https://arxiv.org/abs/2512.04221</guid>
<content:encoded><![CDATA[
<div> Newtonian motion, text-to-video generation, physics simulation, motion coherence, evaluation benchmark<br /><br />Summary:<br /><br />This paper addresses the challenge of generating text-to-video (T2V) content that is not only photorealistic but also physically accurate and intent-aligned with Newtonian motion principles. The authors propose MoReGen, a novel framework that combines large language models (LLMs), physics simulators, and rendering engines to produce reproducible videos grounded in physical laws from code-based text prompts. To evaluate physical validity, they introduce a new metric called object-trajectory correspondence, which directly measures how well generated videos adhere to expected object motions. Furthermore, they present MoReSet, a comprehensive benchmark dataset containing 1,275 human-annotated videos across nine categories of Newtonian phenomena, including detailed scene descriptions, spatiotemporal relations, and ground-truth trajectories. Through extensive experiments using MoReSet, the study evaluates existing T2V models, revealing that state-of-the-art approaches often fail to maintain physical correctness. In contrast, MoReGen demonstrates superior performance in generating physically coherent videos. This work highlights the importance of integrating physics knowledge into T2V systems and provides valuable tools and data to foster progress toward physically faithful video synthesis. <div>
arXiv:2512.04221v1 Announce Type: new 
Abstract: While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasonX: MLLM-Guided Intrinsic Image Decomposition</title>
<link>https://arxiv.org/abs/2512.04222</link>
<guid>https://arxiv.org/abs/2512.04222</guid>
<content:encoded><![CDATA[
<div> Intrinsic image decomposition, multimodal large language model, comparative supervision, GRPO rewards, intrinsic predictors<br /><br />Summary:  
Intrinsic image decomposition aims to separate an image into its fundamental physical components such as albedo, depth, surface normals, and illumination. Existing diffusion- and transformer-based models trained on synthetic paired data struggle to generalize well to diverse, real-world images. The paper introduces ReasonX, a novel framework that leverages a multimodal large language model (MLLM) to act as a perceptual judge by providing relative intrinsic comparisons between image components. These comparisons are then used as GRPO (Gradient-based Reinforcement Policy Optimization) rewards to fine-tune intrinsic decomposition models using unlabeled, in-the-wild images. Unlike traditional reinforcement learning approaches for generative models, ReasonX aligns the conditional intrinsic predictors by encouraging agreement between the MLLM’s relational assessments and analytically derived relationships from the model’s outputs. This approach is model-agnostic and can be applied across different intrinsic decomposition architectures and modalities. Experiments demonstrate that ReasonX achieves substantial improvements, including a 9-25% reduction in Weighted Human Disagreement Rate (WHDR) on IIW albedo prediction and up to 46% gains in depth accuracy on the ETH3D dataset. These results highlight the effectiveness of MLLM-guided comparative supervision in bridging the gap between low-level image decomposition and high-level vision reasoning tasks. <div>
arXiv:2512.04222v1 Announce Type: new 
Abstract: Intrinsic image decomposition aims to separate images into physical components such as albedo, depth, normals, and illumination. While recent diffusion- and transformer-based models benefit from paired supervision from synthetic datasets, their generalization to diverse, real-world scenarios remains challenging. We propose ReasonX, a novel framework that leverages a multimodal large language model (MLLM) as a perceptual judge providing relative intrinsic comparisons, and uses these comparisons as GRPO rewards for fine-tuning intrinsic decomposition models on unlabeled, in-the-wild images. Unlike RL methods for generative models, our framework aligns conditional intrinsic predictors by rewarding agreement between the judge's relational assessments and analytically derived relations from the model's outputs. ReasonX is model-agnostic and can be applied to different intrinsic predictors. Across multiple base architectures and modalities, ReasonX yields significant improvements, including 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D, highlighting the promise of MLLM-guided comparative supervision to bridge low- and high-level vision reasoning.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.04238</link>
<guid>https://arxiv.org/abs/2512.04238</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, rare anatomical variants, AdversarialAnatomyBench, anatomical bias, medical AI

<br /><br />Summary:  
This paper introduces AdversarialAnatomyBench, the first benchmark designed to evaluate vision-language models (VLMs) on naturally occurring rare anatomical variants across various imaging modalities and body regions, addressing an important gap in current model evaluation. The study defines these rare variants as "natural adversarial anatomy," highlighting how they deviate from common anatomical patterns learned by models. When testing 22 state-of-the-art VLMs on basic medical perception tasks, average accuracy significantly dropped from 74% on typical anatomy to 29% on atypical presentations, with top models like GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick experiencing performance declines between 41% and 51%. Analysis revealed that model errors strongly aligned with inherent anatomical biases. Attempts to mitigate these shortcomings through model scaling, bias-aware prompting, and test-time reasoning interventions failed to substantially improve generalization to rare anatomical variants. These results unearth a critical and previously unquantified limitation in current vision-language medical AI systems: poor adaptability to rare anatomical presentations. By providing AdversarialAnatomyBench, the authors establish a systematic platform for measuring and ultimately mitigating anatomical bias in multimodal medical AI, fostering the development of more robust clinical diagnostic tools. <div>
arXiv:2512.04238v1 Announce Type: new 
Abstract: Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about "typical" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models</title>
<link>https://arxiv.org/abs/2512.04248</link>
<guid>https://arxiv.org/abs/2512.04248</guid>
<content:encoded><![CDATA[
<div> MVRoom, novel view synthesis, multi-view diffusion, 3D indoor scenes, epipolar attention<br /><br />Summary:<br /><br />This article introduces MVRoom, a novel pipeline designed for controllable novel view synthesis (NVS) of 3D indoor scenes by leveraging multi-view diffusion conditioned on a coarse 3D layout. The approach utilizes a two-stage design: the first stage creates novel representations that effectively connect the 3D layout with consistent image-based conditioning signals to enable reliable multi-view generation. The second stage applies an image-conditioned multi-view generation process, which integrates a layout-aware epipolar attention mechanism to boost multi-view consistency throughout the diffusion process. Furthermore, the authors propose an iterative framework that generates 3D scenes with varying complexity and object counts by recursively applying multi-view generation, thereby enabling text-to-scene generation. Experimental evaluations demonstrate that MVRoom achieves high-fidelity and controllable 3D scene synthesis, outperforming state-of-the-art baseline methods in both quantitative metrics and qualitative results. Ablation studies confirm the critical roles and effectiveness of the key components in the generation pipeline, validating the design choices and mechanisms introduced for improved multi-view consistency and scene controllability. This work advances the state-of-the-art in controllable 3D indoor scene synthesis using diffusion-based multi-view methods. <div>
arXiv:2512.04248v1 Announce Type: new 
Abstract: We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniLight: A Unified Representation for Lighting</title>
<link>https://arxiv.org/abs/2512.04267</link>
<guid>https://arxiv.org/abs/2512.04267</guid>
<content:encoded><![CDATA[
<div> lighting, multimodal representation, environment maps, spherical harmonics, image synthesis<br /><br />Summary:<br /><br />1. The article addresses the challenge of representing and understanding lighting in images, which is known to significantly affect visual appearance but remains difficult to model effectively.<br />2. Existing lighting representations like environment maps, irradiance, spherical harmonics, and textual descriptions are largely incompatible with each other, limiting their combined utility and cross-modal applications.<br />3. To overcome this, the authors propose UniLight, a unified latent space that represents lighting across multiple modalities within a shared embedding.<br />4. UniLight employs modality-specific encoders for text, images, irradiance, and environment maps trained with contrastive learning to align their embeddings, complemented by an auxiliary spherical harmonics prediction task that enhances directional lighting understanding.<br />5. The system is trained and evaluated on large-scale, multi-modal data for three key tasks: lighting-based retrieval, environment-map generation, and controlling lighting in diffusion-based image synthesis.<br /><br />Results demonstrate that UniLight successfully captures consistent, transferable lighting features, enabling flexible and effective manipulation of lighting information across different modalities, thus advancing lighting representation for various computer vision and graphics applications. <div>
arXiv:2512.04267v1 Announce Type: new 
Abstract: Lighting has a strong influence on visual appearance, yet understanding and representing lighting in images remains notoriously difficult. Various lighting representations exist, such as environment maps, irradiance, spherical harmonics, or text, but they are incompatible, which limits cross-modal transfer. We thus propose UniLight, a joint latent space as lighting representation, that unifies multiple modalities within a shared embedding. Modality-specific encoders for text, images, irradiance, and environment maps are trained contrastively to align their representations, with an auxiliary spherical-harmonics prediction task reinforcing directional understanding. Our multi-modal data pipeline enables large-scale training and evaluation across three tasks: lighting-based retrieval, environment-map generation, and lighting control in diffusion-based image synthesis. Experiments show that our representation captures consistent and transferable lighting features, enabling flexible manipulation across modalities.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer</title>
<link>https://arxiv.org/abs/2512.04282</link>
<guid>https://arxiv.org/abs/2512.04282</guid>
<content:encoded><![CDATA[
<div> Keywords: video motion transfer, normalizing flows, stochastic sampling, GRU, multimodal forecasting  

<br /><br />Summary:  
The paper addresses the challenge of generating diverse and accurate future predictions for real-time video motion transfer, which is critical in applications like immersive gaming and vision-based anomaly detection. It presents a novel inference-time refinement technique that enhances the diversity of sequential forecasts by combining Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. Although GRU-NF integrates normalizing flows within a temporal forecasting model to capture multimodal distributions, its deterministic transformations can limit the model's expressive capacity. To overcome this, the authors incorporate Markov Chain Monte Carlo (MCMC) steps inspired by Stochastic Normalizing Flows (SNF) during inference, enabling exploration of a richer output space without retraining. The method, named Gated Recurrent Unit-Stochastic Normalizing Flows (GRU-SNF), is validated on a keypoint-based video motion transfer pipeline where temporal coherence and perceptual diversity are essential. Experimental results demonstrate that GRU-SNF produces more diverse outputs than GRU-NF while maintaining accuracy, even over longer prediction horizons. By injecting stochasticity during inference, the approach better captures multimodal behaviors in future trajectories, showcasing the potential benefits of integrating stochastic dynamics into flow-based sequence models for generative time series forecasting. <div>
arXiv:2512.04282v1 Announce Type: new 
Abstract: Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint</title>
<link>https://arxiv.org/abs/2512.04283</link>
<guid>https://arxiv.org/abs/2512.04283</guid>
<content:encoded><![CDATA[
<div> Flow matching, plug-and-play, stochastic differential equation, image restoration, acceleration  

<br /><br />Summary:  
This paper addresses the gap between the empirical success and theoretical understanding of plug-and-play flow matching (PnP-Flow) models in image restoration. First, the authors derive a continuous limit of PnP-Flow, formulating it as a stochastic differential equation (SDE) surrogate model. This SDE model provides a rigorous framework to analyze PnP-Flow and offers two key insights for enhancements. One, it allows quantification of the image restoration error, which guides improvements in step scheduling and the regularization of the Lipschitz constant in the neural network-parameterized vector field, ultimately reducing error. Two, the SDE model inspires an acceleration strategy by extrapolating off-the-shelf PnP-Flow models, leading to a rescaled version of the SDE for faster convergence. The authors validate their theoretical findings on benchmark image restoration tasks including denoising, deblurring, super-resolution, and inpainting. Experimental results demonstrate that the SDE-informed improvements outperform baseline PnP-Flow and other state-of-the-art methods, achieving superior performance across multiple evaluation metrics. This work thus bridges theoretical and practical aspects, offering both a deeper understanding and practical advancements for PnP-Flow in image restoration. <div>
arXiv:2512.04283v1 Announce Type: new 
Abstract: Flow matching-based generative models have been integrated into the plug-and-play image restoration framework, and the resulting plug-and-play flow matching (PnP-Flow) model has achieved some remarkable empirical success for image restoration. However, the theoretical understanding of PnP-Flow lags its empirical success. In this paper, we derive a continuous limit for PnP-Flow, resulting in a stochastic differential equation (SDE) surrogate model of PnP-Flow. The SDE model provides two particular insights to improve PnP-Flow for image restoration: (1) It enables us to quantify the error for image restoration, informing us to improve step scheduling and regularize the Lipschitz constant of the neural network-parameterized vector field for error reduction. (2) It informs us to accelerate off-the-shelf PnP-Flow models via extrapolation, resulting in a rescaled version of the proposed SDE model. We validate the efficacy of the SDE-informed improved PnP-Flow using several benchmark tasks, including image denoising, deblurring, super-resolution, and inpainting. Numerical results show that our method significantly outperforms the baseline PnP-Flow and other state-of-the-art approaches, achieving superior performance across evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Single-Image Super-Resolution in the JPEG Compressed Domain</title>
<link>https://arxiv.org/abs/2512.04284</link>
<guid>https://arxiv.org/abs/2512.04284</guid>
<content:encoded><![CDATA[
<div> JPEG features, deep learning, super-resolution, data loading, discrete cosine transform

<br /><br />Summary:  
Deep learning models have increased in complexity alongside the growth in input data size, causing data loading to remain a significant bottleneck impacting training and inference speeds despite advances in specialized hardware. This work addresses this challenge by proposing a method to train models directly on encoded JPEG features, which avoids the full JPEG decoding process and thus reduces computational overhead and improves data loading efficiency. Unlike previous studies that primarily focused on recognition tasks, this research explores the viability of the approach for single-image super-resolution (SISR), a restoration task. The authors develop a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain rather than in the pixel domain. Experimentally, this pipeline achieves a 2.6x speedup in data loading time and a 2.5x speedup in overall training time. Despite these efficiency gains, the model maintains visual quality that is comparable to traditional SISR methods, demonstrating that training directly on compressed features is an effective approach for accelerating training without sacrificing output quality. <div>
arXiv:2512.04284v1 Announce Type: new 
Abstract: Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications</title>
<link>https://arxiv.org/abs/2512.04303</link>
<guid>https://arxiv.org/abs/2512.04303</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular depth estimation, road surface geometry, planar parallax, self-supervised learning, metric depth recovery<br /><br />Summary:<br /><br />1. The paper introduces Gamma-from-Mono (GfM), a novel lightweight method for monocular geometry estimation designed to improve the accuracy of 3D perception of vehicle surroundings, particularly focusing on fine-scale road surface details such as bumps and slopes. <br />2. GfM addresses the projective ambiguity inherent in single-camera reconstruction by decoupling global and local structure, predicting a dominant road surface plane along with residual local variations expressed as gamma—a dimensionless ratio representing vertical deviation relative to depth from the camera. <br />3. This gamma parameter is grounded in planar parallax geometry and, combined with the known camera height above the ground, allows deterministic recovery of metric depth via a closed-form solution, bypassing the need for full extrinsic calibration.<br />4. The approach naturally prioritizes near-road details critical for vehicle control, uses a physically interpretable representation well-suited for self-supervised learning, and does not require large annotated datasets.<br />5. Experimental evaluation on the KITTI and Road Surface Reconstruction Dataset (RSRD) demonstrates that GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation, maintains competitive global depth performance, and exhibits robust adaptability across diverse camera setups with a lightweight 8.88 million parameter model. <div>
arXiv:2512.04303v1 Announce Type: new 
Abstract: Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How (Mis)calibrated is Your Federated CLIP and What To Do About It?</title>
<link>https://arxiv.org/abs/2512.04305</link>
<guid>https://arxiv.org/abs/2512.04305</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP calibration, federated learning, Textual Prompt Tuning, LoRA, FL2oRA<br /><br />Summary:<br />1. The paper investigates the calibration of vision-language models, specifically CLIP, under federated learning (FL) conditions, a topic previously unexplored despite CLIP's widespread study.<br />2. It identifies that Textual Prompt Tuning methods, when applied in FL, tend to degrade calibration performance, highlighting challenges specific to the distributed setup.<br />3. The study evaluates existing in-training calibration methods combined with various global aggregation strategies and finds these provide only limited improvements in calibration.<br />4. A key insight is that the choice of model components selected for fine-tuning significantly impacts calibration, beyond the aggregation or calibration techniques themselves.<br />5. To address this, the authors propose FL²oRA, a novel approach based on LoRA (Low-Rank Adaptation), which enhances calibration naturally within FL without requiring explicit calibration steps.<br />6. Extensive experiments across multiple benchmarks demonstrate FL²oRA consistently yields better-calibrated models, simplifying reliable deployment of CLIP models in distributed learning environments.<br />7. The authors also provide a detailed analysis of factors contributing to FL²oRA's effectiveness and release the codebase publicly for further research and application. <div>
arXiv:2512.04305v1 Announce Type: new 
Abstract: While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\text{FL}^2\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\text{FL}^2\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction</title>
<link>https://arxiv.org/abs/2512.04309</link>
<guid>https://arxiv.org/abs/2512.04309</guid>
<content:encoded><![CDATA[
<div> Keywords: image captioning, text-only training, modality gap reduction, CLIP, retrieval-augmentation<br /><br />Summary:<br /><br />This paper addresses the challenge of generating image captions without relying on human-annotated image-text pairs, focusing on reducing dependency on curated datasets. The authors propose TOMCap, a novel method that leverages a pre-trained language model decoder prompted by information extracted from CLIP representations, which undergo a specific process to minimize the modality gap between visual and textual domains. A key innovation of TOMCap is the combined use of retrieved caption examples and latent vector representations, which together guide the caption generation process more effectively. Extensive experiments demonstrate that TOMCap outperforms existing training-free and text-only image captioning methods, showcasing its potential as a strong alternative to fully supervised approaches. Furthermore, the paper includes an in-depth analysis of how different configurations of retrieval-augmentation and modality gap reduction influence the overall performance, providing insights into optimal design choices. The method highlights the feasibility of training image captioning models through text-only data by effectively bridging the visual-textual modality gap, opening new avenues for caption generation under limited supervision. <div>
arXiv:2512.04309v1 Announce Type: new 
Abstract: Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time Cricket Sorting By Sex</title>
<link>https://arxiv.org/abs/2512.04311</link>
<guid>https://arxiv.org/abs/2512.04311</guid>
<content:encoded><![CDATA[
<div> Keywords: Acheta domesticus, automated sex sorting, computer vision, YOLOv8, sustainable protein<br /><br />Summary:<br /><br />1. The rising global demand for sustainable protein sources has highlighted edible insects, with Acheta domesticus (house cricket) recognized as a prime candidate for industrial farming due to its nutritional value and production potential.<br /><br />2. Existing cricket farming typically involves mixed-sex populations without sex-based sorting, missing out on advantages such as optimized breeding ratios, selective breeding improvements, and tailored nutritional outcomes.<br /><br />3. This study introduces a low-cost, real-time automated system specifically designed for sex-based sorting of Acheta domesticus, integrating computer vision techniques with physical actuation.<br /><br />4. The system employs a Raspberry Pi 5 paired with the official Raspberry AI Camera alongside a custom-trained YOLOv8 nano object detection model to identify and classify crickets by sex.<br /><br />5. Achieving a high mean Average Precision (mAP@0.5) of 0.977 in testing and an 86.8% sorting accuracy in real-world trials, the results demonstrate the capability of lightweight deep learning models to operate effectively on resource-limited devices.<br /><br />6. This technology offers a practical and scalable solution to enhance efficiency, sustainability, and precision in cricket farming operations, potentially transforming practices in edible insect production. <div>
arXiv:2512.04311v1 Announce Type: new 
Abstract: The global demand for sustainable protein sources is driving increasing interest in edible insects, with Acheta domesticus (house cricket) identified as one of the most suitable species for industrial production. Current farming practices typically rear crickets in mixed-sex populations without automated sex sorting, despite potential benefits such as selective breeding, optimized reproduction ratios, and nutritional differentiation. This work presents a low-cost, real-time system for automated sex-based sorting of Acheta domesticus, combining computer vision and physical actuation. The device integrates a Raspberry Pi 5 with the official Raspberry AI Camera and a custom YOLOv8 nano object detection model, together with a servo-actuated sorting arm. The model reached a mean Average Precision at IoU 0.5 (mAP@0.5) of 0.977 during testing, and real-world experiments with groups of crickets achieved an overall sorting accuracy of 86.8%. These results demonstrate the feasibility of deploying lightweight deep learning models on resource-constrained devices for insect farming applications, offering a practical solution to improve efficiency and sustainability in cricket production.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding</title>
<link>https://arxiv.org/abs/2512.04313</link>
<guid>https://arxiv.org/abs/2512.04313</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG decoding, facial expression synthesis, CNN-Transformer, 3D Gaussian Splatting, emotion recognition<br /><br />Summary:  
1. This study introduces Mind-to-Face, a novel framework that decodes non-invasive EEG signals directly into high-fidelity, dynamic facial expressions.  
2. The authors constructed a dual-modality recording setup that synchronizes EEG data with multi-view facial videos under emotion-eliciting stimuli, enabling precise supervision for training the neural-to-visual mapping.  
3. Their model employs a CNN-Transformer encoder architecture to convert EEG inputs into dense 3D position maps with over 65,000 vertices, capturing fine facial geometry and subtle emotional dynamics.  
4. The generated 3D facial data is rendered using a modified 3D Gaussian Splatting pipeline, producing photorealistic and view-consistent avatar outputs.  
5. Extensive evaluations demonstrate that EEG signals alone can reliably and subject-specifically predict dynamic facial expressions, including nuanced emotional states, revealing richer affective and geometric information in neural signals than previously recognized.  
6. Mind-to-Face represents a new paradigm in neural-driven avatars, unlocking personalized, emotion-aware telepresence and enhanced cognitive interaction for immersive virtual environments. <div>
arXiv:2512.04313v1 Announce Type: new 
Abstract: Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision</title>
<link>https://arxiv.org/abs/2512.04314</link>
<guid>https://arxiv.org/abs/2512.04314</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, DisentangleFormer, hyperspectral imaging, spatial-channel decoupling, multi-scale FFN<br /><br />Summary:<br /><br />1. Vision Transformers (ViTs) typically use self-attention that jointly processes spatial and channel dimensions, leading to entangled representations that hinder the separate modeling of structural and semantic information. This challenge is particularly critical in hyperspectral imaging where each channel carries distinct biophysical or biochemical information.<br /><br />2. The paper introduces DisentangleFormer, a novel architecture designed to achieve robust multi-channel vision representation by explicitly decoupling spatial and channel information, inspired by information-theoretic principles favoring decorrelated representations.<br /><br />3. DisentangleFormer features three key components: (a) Parallel Disentanglement, which independently processes spatial-token and channel-token streams to ensure decorrelated features across spatial and spectral domains; (b) Squeezed Token Enhancer, an adaptive calibration module that dynamically fuses spatial and channel streams to optimize information flow; (c) Multi-Scale Feed-Forward Network (FFN), which supplements global attention with multi-scale local context to capture intricate structural and semantic dependencies.<br /><br />4. Extensive experiments on multiple hyperspectral benchmarks—including Indian Pine, Pavia University, Houston datasets, the large-scale BigEarthNet remote sensing dataset, and an infrared pathology dataset—show that DisentangleFormer consistently outperforms state-of-the-art models.<br /><br />5. Additionally, DisentangleFormer maintains competitive accuracy on the natural image classification benchmark ImageNet while reducing computational cost by 17.8% in FLOPs, demonstrating efficiency alongside superior performance. The code will be publicly released upon paper acceptance. <div>
arXiv:2512.04314v1 Announce Type: new 
Abstract: Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.04315</link>
<guid>https://arxiv.org/abs/2512.04315</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D Gaussian Splatting, multi-video synchronization, dynamic 3D scenes, Fused Gromov-Wasserstein, temporal alignment  

<br /><br />Summary:  
This paper addresses the challenge of modeling dynamic 3D scenes from multiple unsynchronized videos by introducing a novel 4D Gaussian Splatting (4DGS) method named SyncTrack4D. The approach starts by computing dense 4D feature tracks for each video and establishes cross-video correspondences using a Fused Gromov-Wasserstein optimal transport method. It then performs a global frame-level temporal alignment to maximize the overlap of matched dynamic scene motions across videos. To refine synchronization, SyncTrack4D applies sub-frame synchronization leveraging a motion-spline scaffold representation within the multi-video 4D Gaussian splatting framework. The output is a fully synchronized 4DGS model with explicit 3D trajectories and precise temporal offsets for every input video. The method is evaluated on datasets such as Panoptic Studio and SyncNeRF Blender, demonstrating state-of-the-art sub-frame synchronization accuracy with average temporal errors below 0.26 frames. Additionally, high-fidelity 4D reconstructions were achieved, reaching a peak signal-to-noise ratio (PSNR) of 26.3 on the Panoptic Studio dataset. Notably, this approach does not require predefined scene objects or prior models, marking the first general 4D Gaussian Splatting framework for handling unsynchronized multi-video dynamic scene reconstruction in real-world scenarios. <div>
arXiv:2512.04315v1 Announce Type: new 
Abstract: Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayes-DIC Net: Estimating Digital Image Correlation Uncertainty with Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2512.04323</link>
<guid>https://arxiv.org/abs/2512.04323</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital Image Correlation, non-uniform B-spline, displacement fields, Bayes-DIC Net, Bayesian neural network<br /><br />Summary:<br /><br />This paper presents a novel method to generate high-quality Digital Image Correlation (DIC) datasets utilizing non-uniform B-spline surfaces. The approach randomly generates control point coordinates to construct diverse displacement fields that mimic realistic scenarios, which are then used to create speckle pattern datasets. This enables the creation of large-scale datasets that better represent real-world displacement conditions, thereby improving the training and generalization ability of deep learning-based DIC algorithms. The paper also introduces a new network architecture named Bayes-DIC Net, which captures information at multiple levels during down-sampling and aggregates multi-level features using a single skip connection in the up-sampling phase. Bayes-DIC Net employs lightweight convolutional blocks to expand the receptive field and capture rich contextual information while keeping computational costs low. Incorporating dropout modules activated during inference transforms Bayes-DIC Net into a Bayesian neural network, allowing it to produce both predictions and uncertainty estimates on unlabeled real datasets. This capability significantly enhances the network’s reliability and practicality for real-world displacement field predictions. Overall, the paper contributes innovative dataset generation techniques and algorithmic improvements in DIC analysis. <div>
arXiv:2512.04323v1 Announce Type: new 
Abstract: This paper introduces a novel method for generating high-quality Digital Image Correlation (DIC) dataset based on non-uniform B-spline surfaces. By randomly generating control point coordinates, we construct displacement fields that encompass a variety of realistic displacement scenarios, which are subsequently used to generate speckle pattern datasets. This approach enables the generation of a large-scale dataset that capture real-world displacement field situations, thereby enhancing the training and generalization capabilities of deep learning-based DIC algorithms. Additionally, we propose a novel network architecture, termed Bayes-DIC Net, which extracts information at multiple levels during the down-sampling phase and facilitates the aggregation of information across various levels through a single skip connection during the up-sampling phase. Bayes-DIC Net incorporates a series of lightweight convolutional blocks designed to expand the receptive field and capture rich contextual information while minimizing computational costs. Furthermore, by integrating appropriate dropout modules into Bayes-DIC Net and activating them during the network inference stage, Bayes-DIC Net is transformed into a Bayesian neural network. This transformation allows the network to provide not only predictive results but also confidence levels in these predictions when processing real unlabeled datasets. This feature significantly enhances the practicality and reliability of our network in real-world displacement field prediction tasks. Through these innovations, this paper offers new perspectives and methods for dataset generation and algorithm performance enhancement in the field of DIC.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks</title>
<link>https://arxiv.org/abs/2512.04329</link>
<guid>https://arxiv.org/abs/2512.04329</guid>
<content:encoded><![CDATA[
<div> Keywords: neural modules, PyTorch codebases, retrieval-augmented generation, code reuse, neural architectures<br /><br />Summary: NN-RAG is a novel retrieval-augmented generation system designed to enhance research efficiency by enabling the discovery, extraction, and validation of reusable neural network components from large, heterogeneous PyTorch codebases. Unlike traditional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, preserves imports, and utilizes validator-gated promotion to ensure that every retrieved neural block is scope-closed, compilable, and runnable. When applied to 19 major repositories, NN-RAG extracted 1,289 candidate blocks and validated 941 of them (73.0%), with over 80% being structurally unique. Multi-level de-duplication techniques (exact, lexical, structural) revealed that NN-RAG contributes approximately 72% of novel network architectures in the LEMUR dataset, underscoring its capacity to significantly expand the diversity of executable neural architectures. Beyond quantity, NN-RAG uniquely supports cross-repository migration of architectural patterns, allowing automatic identification and regeneration of dependency-complete reusable modules in new contexts. The framework’s neutral design permits optional integration with language models for synthesis or dataset registration while avoiding redistribution of third-party code. Overall, NN-RAG offers the first open-source solution that transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery at scale. <div>
arXiv:2512.04329v1 Announce Type: new 
Abstract: Reusing existing neural-network components is central to research efficiency, yet discovering, extracting, and validating such modules across thousands of open-source repositories remains difficult. We introduce NN-RAG, a retrieval-augmented generation system that converts large, heterogeneous PyTorch codebases into a searchable and executable library of validated neural modules. Unlike conventional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, import-preserving reconstruction, and validator-gated promotion -- ensuring that every retrieved block is scope-closed, compilable, and runnable. Applied to 19 major repositories, the pipeline extracted 1,289 candidate blocks, validated 941 (73.0%), and demonstrated that over 80% are structurally unique. Through multi-level de-duplication (exact, lexical, structural), we find that NN-RAG contributes the overwhelming majority of unique architectures to the LEMUR dataset, supplying approximately 72% of all novel network structures. Beyond quantity, NN-RAG uniquely enables cross-repository migration of architectural patterns, automatically identifying reusable modules in one project and regenerating them, dependency-complete, in another context. To our knowledge, no other open-source system provides this capability at scale. The framework's neutral specifications further allow optional integration with language models for synthesis or dataset registration without redistributing third-party code. Overall, NN-RAG transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery, offering a first open-source solution that both quantifies and expands the diversity of executable neural architectures across repositories.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Set Face Forgery Detection via Dual-Level Evidence Collection</title>
<link>https://arxiv.org/abs/2512.04331</link>
<guid>https://arxiv.org/abs/2512.04331</guid>
<content:encoded><![CDATA[
<div> Face forgery detection, open set recognition, uncertainty estimation, dual-level evidential learning, frequency-spatial fusion<br /><br />Summary:<br /><br />1. The paper addresses the problem of face forgery detection, focusing on the limitations of current methods which mainly perform binary Real-vs-Fake classification or identify only known types of forgeries.<br /><br />2. It introduces the Open Set Face Forgery Detection (OSFFD) problem, which requires detection models to recognize novel, previously unseen fake categories, reflecting real-world challenges as new forgery methods constantly emerge.<br /><br />3. The authors propose a novel Dual-Level Evidential face forgery Detection (DLED) approach that leverages uncertainty estimation to better handle these new fake types.<br /><br />4. DLED collects and integrates category-specific evidence from both spatial and frequency domains in the visual data, enhancing the model’s ability to estimate prediction uncertainty effectively.<br /><br />5. Extensive experiments demonstrate that DLED outperforms multiple baseline models by an average of 20% in detecting novel fake categories and maintains competitive performance in the traditional Real-versus-Fake detection task, establishing state-of-the-art results in this field. <div>
arXiv:2512.04331v1 Announce Type: new 
Abstract: The proliferation of face forgeries has increasingly undermined confidence in the authenticity of online content. Given the rapid development of face forgery generation algorithms, new fake categories are likely to keep appearing, posing a major challenge to existing face forgery detection methods. Despite recent advances in face forgery detection, existing methods are typically limited to binary Real-vs-Fake classification or the identification of known fake categories, and are incapable of detecting the emergence of novel types of forgeries. In this work, we study the Open Set Face Forgery Detection (OSFFD) problem, which demands that the detection model recognize novel fake categories. We reformulate the OSFFD problem and address it through uncertainty estimation, enhancing its applicability to real-world scenarios. Specifically, we propose the Dual-Level Evidential face forgery Detection (DLED) approach, which collects and fuses category-specific evidence on the spatial and frequency levels to estimate prediction uncertainty. Extensive evaluations conducted across diverse experimental settings demonstrate that the proposed DLED method achieves state-of-the-art performance, outperforming various baseline models by an average of 20% in detecting forgeries from novel fake categories. Moreover, on the traditional Real-versus-Fake face forgery detection task, our DLED method concurrently exhibits competitive performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment</title>
<link>https://arxiv.org/abs/2512.04356</link>
<guid>https://arxiv.org/abs/2512.04356</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal LLMs, hallucination, video captioning, contrastive alignment, spurious correlations<br /><br />Summary:<br /><br />1. The paper addresses the problem of hallucination in video captioning by multimodal large language models (MLLMs), which causes factual inaccuracies in generated video descriptions involving objects and actions.<br />2. Unlike previous works focused on static images, this research targets the combined challenge of mitigating both visual object and temporal action hallucinations in dynamic video content.<br />3. To resolve this, the authors propose the Self-Augmented Contrastive Alignment (SANTA) framework, designed to improve faithfulness by reducing spurious correlations and emphasizing accurate visual facts.<br />4. SANTA introduces a hallucinative self-augmentation approach that generates contrasted negative captions by identifying potential hallucinations in the model’s output, thereby guiding the system to distinguish between factual and spurious content.<br />5. Additionally, the framework includes a tracklet-phrase contrastive alignment mechanism that aligns tracked regional objects and relation-guided temporal actions with their corresponding visual and temporal phrases to enforce semantic consistency.<br />6. Extensive experiments on benchmarks focused on hallucination detection demonstrate that SANTA significantly outperforms existing methods in reducing both object and action hallucinations, improving the overall descriptive accuracy of MLLMs for video captioning.<br /><br />This study highlights an effective strategy to enhance the reliability of multimodal models in generating faithful video descriptions by jointly addressing spatial and temporal visual hallucinations. <div>
arXiv:2512.04356v1 Announce Type: new 
Abstract: Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAFNet:Multi-frequency Adaptive Fusion Network for Real-time Stereo Matching</title>
<link>https://arxiv.org/abs/2512.04358</link>
<guid>https://arxiv.org/abs/2512.04358</guid>
<content:encoded><![CDATA[
<div> Keywords: stereo matching, multi-frequency, adaptive fusion, disparity estimation, real-time performance<br /><br />Summary:<br /><br />The paper addresses limitations in existing stereo matching networks, which either rely on costly 3D convolution-based cost-volume aggregation or iterative deformation methods that fail to capture non-local contextual information. These constraints hinder deployment on resource-limited mobile devices, especially for real-time applications. To overcome this, the authors propose the Multi-frequency Adaptive Fusion Network (MAFNet), designed to generate high-quality disparity maps efficiently with only 2D convolutions. A key innovation is the adaptive frequency-domain filtering attention module that separates the full cost volume into high- and low-frequency components, enabling frequency-aware feature aggregation tailored to these distinct bands. Furthermore, the method incorporates a Linformer-based low-rank attention mechanism to effectively fuse the frequency-separated features in an adaptive manner, enhancing disparity estimation robustness. Experimental results on benchmarks like Scene Flow and KITTI 2015 demonstrate that MAFNet outperforms current real-time stereo matching methods, offering an improved trade-off between accuracy and computational efficiency. This approach shows promise for stereo vision applications requiring real-time processing on devices with limited resources. <div>
arXiv:2512.04358v1 Announce Type: new 
Abstract: Existing stereo matching networks typically rely on either cost-volume construction based on 3D convolutions or deformation methods based on iterative optimization. The former incurs significant computational overhead during cost aggregation, whereas the latter often lacks the ability to model non-local contextual information. These methods exhibit poor compatibility on resource-constrained mobile devices, limiting their deployment in real-time applications. To address this, we propose a Multi-frequency Adaptive Fusion Network (MAFNet), which can produce high-quality disparity maps using only efficient 2D convolutions. Specifically, we design an adaptive frequency-domain filtering attention module that decomposes the full cost volume into high-frequency and low-frequency volumes, performing frequency-aware feature aggregation separately. Subsequently, we introduce a Linformer-based low-rank attention mechanism to adaptively fuse high- and low-frequency information, yielding more robust disparity estimation. Extensive experiments demonstrate that the proposed MAFNet significantly outperforms existing real-time methods on public datasets such as Scene Flow and KITTI 2015, showing a favorable balance between accuracy and real-time performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring</title>
<link>https://arxiv.org/abs/2512.04390</link>
<guid>https://arxiv.org/abs/2512.04390</guid>
<content:encoded><![CDATA[
<div> Keywords: video restoration, dynamic exposure, super-resolution, deblurring, FMA-Net++

<br /><br />Summary:  
1. The paper addresses the challenge of real-world video restoration affected by complex degradations stemming from motion combined with dynamically varying exposure, a problem often overlooked in prior research and common in auto-exposure or low-light video capture.  
2. The authors propose FMA-Net++, a novel framework for joint video super-resolution and deblurring that explicitly models the coupled effects of motion and changing exposure conditions to better restore video quality.  
3. FMA-Net++ features a sequence-level architecture using Hierarchical Refinement with Bidirectional Propagation blocks, which facilitates parallel processing and long-range temporal modeling to capture temporal dependencies effectively.  
4. Within each block, an Exposure Time-aware Modulation layer adjusts features based on each frame’s exposure, feeding into an exposure-aware Flow-Guided Dynamic Filtering module that infers degradation kernels sensitive to both motion and exposure variations.  
5. The method decouples degradation learning from restoration by first predicting exposure- and motion-aware priors to guide the restoration process, enhancing both accuracy and computational efficiency.  
6. To evaluate performance under realistic capture conditions, the authors introduce two new benchmarks: REDS-ME (multi-exposure) and REDS-RE (random exposure).  
7. Trained exclusively on synthetic data, FMA-Net++ achieves state-of-the-art results in restoration quality and temporal consistency on these benchmarks and the GoPro dataset, outperforming recent methods in accuracy and inference speed.  
8. Moreover, the approach generalizes effectively to challenging real-world videos, demonstrating robustness and practical applicability. <div>
arXiv:2512.04390v1 Announce Type: new 
Abstract: Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.04395</link>
<guid>https://arxiv.org/abs/2512.04395</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Fourier Analysis, Representation Disentanglement, Cross-Attention, Few-Shot Learning<br /><br />Summary:<br />1. The paper addresses the limitation of current large-scale pre-trained Vision-Language Models (VLMs) that learn holistic image representations, where domain-invariant structures and domain-specific styles are entangled, which can hinder generalization.<br />2. It introduces a novel framework called Fourier-Attentive Representation Learning (FARL) that explicitly disentangles visual features into structural and stylistic components using Fourier analysis.<br />3. The core innovation involves a dual cross-attention mechanism where learnable tokens separately query the phase spectrum for structural features and the amplitude spectrum for stylistic features, producing enriched and disentangled representations.<br />4. These disentangled tokens are injected deeply into the VLM encoders via an asymmetric injection strategy, which enhances the robustness of vision-language alignment and model adaptation.<br />5. Extensive experiments conducted across 15 diverse datasets validate the effectiveness of FARL, showing improved few-shot learning capabilities due to the better disentanglement of structural and stylistic visual cues. <div>
arXiv:2512.04395v1 Announce Type: new 
Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Evaluation of Transfer Learning Based Medical Image Classification Techniques for Disease Detection</title>
<link>https://arxiv.org/abs/2512.04397</link>
<guid>https://arxiv.org/abs/2512.04397</guid>
<content:encoded><![CDATA[
<div> Keywords: transfer learning, medical image classification, deep convolutional neural networks, chest X-ray, InceptionV3  

<br /><br />Summary:  
The paper addresses the challenge of training large deep learning models from scratch for medical image classification tasks, specifically using chest X-rays. It evaluates the effectiveness of transfer learning (TL) by reusing six pre-trained convolutional neural network models: AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3. The study finds that InceptionV3 consistently achieves the highest performance across standard evaluation metrics, while the ResNet models demonstrate improved accuracy with increasing depth. AlexNet and VGG16 perform adequately but lag behind the deeper architectures in terms of accuracy. Additionally, the authors examine the robustness of these models through uncertainty analysis and compare their computational efficiency via runtime measurements. The results indicate that TL provides clear benefits, particularly when the available dataset is limited in size. However, the degree of improvement depends on various factors including the model architecture, the size of the dataset, and how similar the source and target domains are. The research also highlights that leveraging a well-trained feature extractor allows the use of a simple lightweight feedforward network for efficient predictions. Overall, the findings offer valuable insights into selecting suitable TL models for medical image classification based on specific requirements such as accuracy, robustness, and computational resources. <div>
arXiv:2512.04397v1 Announce Type: new 
Abstract: Medical image classification plays an increasingly vital role in identifying various diseases by classifying medical images, such as X-rays, MRIs and CT scans, into different categories based on their features. In recent years, deep learning techniques have attracted significant attention in medical image classification. However, it is usually infeasible to train an entire large deep learning model from scratch. To address this issue, one of the solutions is the transfer learning (TL) technique, where a pre-trained model is reused for a new task. In this paper, we present a comprehensive analysis of TL techniques for medical image classification using deep convolutional neural networks. We evaluate six pre-trained models (AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3) on a custom chest X-ray dataset for disease detection. The experimental results demonstrate that InceptionV3 consistently outperforms other models across all the standard metrics. The ResNet family shows progressively better performance with increasing depth, whereas VGG16 and AlexNet perform reasonably well but with lower accuracy. In addition, we also conduct uncertainty analysis and runtime comparison to assess the robustness and computational efficiency of these models. Our findings reveal that TL is beneficial in most cases, especially with limited data, but the extent of improvement depends on several factors such as model architecture, dataset size, and domain similarity between source and target tasks. Moreover, we demonstrate that with a well-trained feature extractor, only a lightweight feedforward model is enough to provide efficient prediction. As such, this study contributes to the understanding of TL in medical image classification, and provides insights for selecting appropriate models based on specific requirements.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection</title>
<link>https://arxiv.org/abs/2512.04413</link>
<guid>https://arxiv.org/abs/2512.04413</guid>
<content:encoded><![CDATA[
<div> Knowledge distillation, remote sensing object detection, spectral decomposition, wavelet transform, density-independent scale weight<br /><br />Summary:<br /><br />This paper addresses the challenges of knowledge distillation in remote sensing object detection, particularly issues arising from mixed features and subtle variations leading to knowledge confusion. The authors propose a novel, architecture-agnostic distillation method called Dual-Stream Spectral Decoupling Distillation (DS2D2) that integrates both explicit and implicit distillation strategies using spectral decomposition. First, the method applies a first-order wavelet transform to decompose spectral features, preserving critical spatial characteristics of remote sensing images, which is important for accurate object detection. Building on this spatial preservation, a Density-Independent Scale Weight (DISW) is introduced to specifically target the difficulties associated with dense and small objects common in remote sensing imagery. Second, implicit knowledge is extracted from subtle discrepancies between student and teacher model features, which significantly impact prediction accuracy when processed through detection heads. These discrepancies are captured via full-frequency and high-frequency amplifiers, translating feature differences into prediction deviations. The effectiveness of DS2D2 is validated through extensive experiments on benchmark datasets DIOR and DOTA, demonstrating improvements of 4.2% and 3.8% in AP50 for RetinaNet and Faster R-CNN respectively, outperforming existing distillation approaches. The authors also provide the source code for reproducibility and further research. <div>
arXiv:2512.04413v1 Announce Type: new 
Abstract: Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UTrice: Unifying Primitives in Differentiable Ray Tracing and Rasterization via Triangles for Particle-Based 3D Scenes</title>
<link>https://arxiv.org/abs/2512.04421</link>
<guid>https://arxiv.org/abs/2512.04421</guid>
<content:encoded><![CDATA[
<div> Gaussian Particles, Ray Tracing, Triangle Rendering, Novel-View Synthesis, Real-Time Performance  

<br /><br />Summary:  
This work addresses limitations in current 3D Gaussian particle ray tracing methods which depend on proxy geometry, such as intermediate mesh construction and expensive intersection computations. The authors propose a novel differentiable ray tracing pipeline that directly uses triangles as rendering primitives, eliminating the need for proxy geometry. By treating triangles as the fundamental unit for both ray tracing and rasterization, the method unifies rendering primitives used in novel-view synthesis. Experimental results demonstrate that this triangle-based approach achieves significantly better rendering quality compared to traditional Gaussian particle ray tracing techniques while still maintaining real-time rendering speeds. Additionally, the proposed pipeline supports direct rendering of triangles optimized via the rasterization-based technique known as Triangle Splatting. This enables seamless integration and improved flexibility for rendering realistic effects like depth of field and refraction in novel-view synthesis applications. The approach therefore overcomes inefficiencies and quality issues present in previous methods by combining the strengths of rasterization and ray tracing in a single, efficient framework. <div>
arXiv:2512.04421v1 Announce Type: new 
Abstract: Ray tracing 3D Gaussian particles enables realistic effects such as depth of field, refractions, and flexible camera modeling for novel-view synthesis. However, existing methods trace Gaussians through proxy geometry, which requires constructing complex intermediate meshes and performing costly intersection tests. This limitation arises because Gaussian-based particles are not well suited as unified primitives for both ray tracing and rasterization. In this work, we propose a differentiable triangle-based ray tracing pipeline that directly treats triangles as rendering primitives without relying on any proxy geometry. Our results show that the proposed method achieves significantly higher rendering quality than existing ray tracing approaches while maintaining real-time rendering performance. Moreover, our pipeline can directly render triangles optimized by the rasterization-based method Triangle Splatting, thus unifying the primitives used in novel-view synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models</title>
<link>https://arxiv.org/abs/2512.04425</link>
<guid>https://arxiv.org/abs/2512.04425</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson’s disease, gait analysis, multimodal fusion, explainability, RGB-D data<br /><br />Summary:  
Accurate and interpretable gait analysis is essential for the early detection of Parkinson’s disease (PD). Most existing methods are limited by reliance on single-modality inputs, low robustness, and lack of clinical transparency. This paper proposes an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic and challenging conditions such as low lighting and occlusion. The system uses dual YOLOv11-based encoders to perform modality-specific feature extraction. A novel Multi-Scale Local-Global Extraction (MLGE) module along with a Cross-Spatial Neck Fusion mechanism enhances spatial-temporal representation, capturing detailed limb movements (e.g., reduced arm swing) and overall gait dynamics (e.g., short strides, turning difficulty). For clinical interpretability, a frozen Large Language Model (LLM) translates the fused visual embeddings and structured metadata into meaningful textual explanations. Experimental results demonstrate that this RGB-D fusion framework outperforms single-input baselines in accuracy and robustness, while providing clear visual-linguistic reasoning. By combining multimodal feature learning with language-based explainability, the study bridges the gap between visual recognition and clinical understanding, presenting a novel vision-language paradigm for reliable, interpretable Parkinson’s disease gait analysis. The code for the system is publicly available at the provided GitHub link. <div>
arXiv:2512.04425v1 Announce Type: new 
Abstract: Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Paced and Self-Corrective Masked Prediction for Movie Trailer Generation</title>
<link>https://arxiv.org/abs/2512.04426</link>
<guid>https://arxiv.org/abs/2512.04426</guid>
<content:encoded><![CDATA[
<div> Movie trailer generation, Transformer encoder, self-paced learning, masked prediction, self-correction  

<br /><br />Summary:  
This paper addresses the task of automatic movie trailer generation, which involves selecting and reorganizing movie shots to create engaging trailers. It critiques the dominant "selection-then-ranking" approach that first selects key shots and then ranks them, noting this method suffers from error propagation that limits trailer quality. To overcome these limitations, the authors propose a novel approach called SSMP, a self-paced and self-corrective masked prediction method. SSMP employs a Transformer encoder that models bi-directional context by taking entire sequences of movie shots as input prompts and generates trailer shot sequences. Training involves reconstructing trailer shot sequences from randomly masked versions, with the mask ratio controlled in a self-paced manner to adapt task difficulty to the model's performance. During generation, SSMP progressively fills in shot positions with high confidence predictions and re-masks remaining positions for further refinement, implementing a self-correction mechanism inspired by human editing processes. Extensive quantitative evaluations and user studies verify that SSMP significantly outperforms existing automatic trailer generation techniques. A demonstration of the method is made available via a publicly accessible GitHub repository. <div>
arXiv:2512.04426v1 Announce Type: new 
Abstract: As a challenging video editing task, movie trailer generation involves selecting and reorganizing movie shots to create engaging trailers. Currently, most existing automatic trailer generation methods employ a "selection-then-ranking" paradigm (i.e., first selecting key shots and then ranking them), which suffers from inevitable error propagation and limits the quality of the generated trailers. Beyond this paradigm, we propose a new self-paced and self-corrective masked prediction method called SSMP, which achieves state-of-the-art results in automatic trailer generation via bi-directional contextual modeling and progressive self-correction. In particular, SSMP trains a Transformer encoder that takes the movie shot sequences as prompts and generates corresponding trailer shot sequences accordingly. The model is trained via masked prediction, reconstructing each trailer shot sequence from its randomly masked counterpart. The mask ratio is self-paced, allowing the task difficulty to adapt to the model and thereby improving model performance. When generating a movie trailer, the model fills the shot positions with high confidence at each step and re-masks the remaining positions for the next prediction, forming a progressive self-correction mechanism that is analogous to how human editors work. Both quantitative results and user studies demonstrate the superiority of SSMP in comparison to existing automatic movie trailer generation methods. Demo is available at: https://github.com/Dixin-Lab/SSMP.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.04441</link>
<guid>https://arxiv.org/abs/2512.04441</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, trajectory planning, context simulation, vision-language model, multi-objective evaluation<br /><br />Summary:<br /><br />This paper presents MindDrive, an innovative framework for End-to-End autonomous driving that harmonizes high-quality trajectory generation with comprehensive decision reasoning. The approach is structured into three key stages: context simulation, candidate trajectory generation, and multi-objective trade-off evaluation. The Future-aware Trajectory Generator (FaTG), leveraging a World Action Model, enables ego-conditioned "what-if" simulations to forecast potential future scenarios and produce foresighted trajectory candidates. Complementing this, the VLM-oriented Evaluator (VLoE) applies a large vision-language model's reasoning capabilities to evaluate these candidates on multiple objectives, including safety, comfort, and efficiency, facilitating decision-making aligned with human reasoning. Extensive testing on the NAVSIM-v1 and NAVSIM-v2 benchmarks confirms that MindDrive outperforms current methods by achieving superior performance in multi-dimensional driving metrics. The framework notably enhances safety, regulatory compliance, and generalization in autonomous driving tasks. Ultimately, this work marks a significant advancement toward interpretable and cognitively guided autonomous driving systems that are both robust and aligned with human-like decision processes. <div>
arXiv:2512.04441v1 Announce Type: new 
Abstract: End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of "context simulation - candidate generation - multi-objective trade-off". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned "what-if" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios</title>
<link>https://arxiv.org/abs/2512.04451</link>
<guid>https://arxiv.org/abs/2512.04451</guid>
<content:encoded><![CDATA[
<div> Embodied Intelligence, Streaming Video, Question Answering, Video-LLMs, Benchmark

<br /><br />Summary:  
This paper introduces StreamEQA, a novel benchmark designed for streaming video question answering specifically in embodied scenarios, addressing the need for continuous perception and reasoning over real-world streaming visual inputs. The benchmark evaluates multimedia large language models (MLLMs) along two key dimensions: Embodied and Streaming. The Embodied dimension categorizes questions into three levels—perception (fine-grained visual recognition), interaction (reasoning about agent-object interactions), and planning (high-level goal-directed reasoning). The Streaming dimension divides questions into backward, real-time, and forward reasoning, each requiring different temporal contexts to answer. StreamEQA is built on 156 independent long videos, encompassing 42 distinct tasks and approximately 21,000 question-answer pairs annotated with precise timestamps. The dataset was created using a hybrid pipeline that combines automated question generation with human refinement to ensure quality. Evaluations conducted on 13 state-of-the-art video-based large language models show that, despite their strong performance on conventional benchmarks, these models still face significant challenges in understanding streaming videos within embodied scenarios. The authors anticipate that StreamEQA will drive future research efforts toward improving streaming video comprehension in embodied intelligence applications. <div>
arXiv:2512.04451v1 Announce Type: new 
Abstract: As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis</title>
<link>https://arxiv.org/abs/2512.04456</link>
<guid>https://arxiv.org/abs/2512.04456</guid>
<content:encoded><![CDATA[
<div> Keywords: image denoising, diffusion model, noise synthesis, guidance, data augmentation<br /><br />Summary:<br /><br />This paper addresses the challenge of acquiring real-world noisy image data for training denoising models by proposing a novel noise synthesis method called GuidNoise. Unlike previous generative approaches requiring extensive camera metadata and large noisy-clean image datasets, GuidNoise operates using only a single noisy/clean image pair as guidance, which is easier to obtain. The method leverages a diffusion model enhanced with a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss designed to improve the backward diffusion process, enabling the generation of realistic and diverse noise patterns. GuidNoise functions without the need for additional metadata during both training and inference, providing flexible noise synthesis across varying noise environments. A key advantage of GuidNoise is its ability to efficiently generate synthetic noisy-clean image pairs on demand at inference time. These synthetic pairs serve as a powerful data augmentation tool that significantly boosts denoising performance, particularly when using lightweight models or limited training data in practical applications. The paper also provides implementation code, facilitating reproducibility and adoption of the method in related image denoising tasks. <div>
arXiv:2512.04456v1 Announce Type: new 
Abstract: Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning</title>
<link>https://arxiv.org/abs/2512.04459</link>
<guid>https://arxiv.org/abs/2512.04459</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, vision-language models, discrete diffusion, end-to-end driving, behavior-trajectory consistency<br /><br />Summary:<br /><br />The paper addresses the challenge of out-of-distribution (OOD) scenarios in autonomous driving by focusing on improving end-to-end (E2E) driving systems through the integration of vision-language models (VLMs). It critiques existing autoregressive (AR) VLMs for their limitations caused by causal attention and sequential token generation, which hinder consistency and controllability between high-level reasoning and low-level planning. The authors propose dVLM-AD, a novel diffusion-based vision-language model employing discrete diffusion and bidirectional attention mechanisms to enhance controllability and reliability via iterative denoising. This unified model integrates perception, structured reasoning, and low-level planning into an E2E driving framework. Evaluations on nuScenes and WOD-E2E datasets show that dVLM-AD achieves more consistent reasoning-action pairs compared to AR-based baselines. Despite using a modest backbone, it yields planning performance on par with existing VLM/VLA driving systems. Quantitatively, dVLM-AD improves behavior-trajectory consistency by 9% and success rates (RFS) by 6% in long-tail driving scenarios on the WOD-E2E benchmark. The results highlight discrete diffusion VLMs as a promising and controllable approach for scalable and reliable autonomous driving solutions. <div>
arXiv:2512.04459v1 Announce Type: new 
Abstract: The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniTS: Unified Time Series Generative Model for Remote Sensing</title>
<link>https://arxiv.org/abs/2512.04461</link>
<guid>https://arxiv.org/abs/2512.04461</guid>
<content:encoded><![CDATA[
<div> Keywords: Unified Time Series Generative Model, spatiotemporal modeling, satellite remote sensing, cloud removal, time series forecasting<br /><br />Summary: This paper addresses the challenge of modeling complex Earth environment dynamics from satellite remote sensing data, focusing on tasks such as reconstructing continuous cloud-free image time series, detecting land cover changes, and forecasting surface evolution. Existing approaches typically rely on specialized models for each task without a unified framework. To overcome this, the authors propose UniTS, a Unified Time Series Generative Model that applies a single framework to multiple time series tasks including reconstruction, cloud removal, semantic change detection, and forecasting. UniTS is built on a flow matching generative paradigm that deterministically evolves data from noise to target guided by task-specific conditions, enabling unified spatiotemporal feature modeling across tasks. Its architecture features a diffusion transformer with spatiotemporal blocks enhanced by an Adaptive Condition Injector (ACor) to better incorporate multimodal inputs and a Spatiotemporal-aware Modulator (STM) to capture complex dependencies. To support evaluation, the authors present two new multimodal datasets, TS-S12 and TS-S12CR, targeting cloud removal and forecasting tasks. Extensive experiments demonstrate that UniTS outperforms current methods, especially under difficult conditions such as heavy cloud contamination, missing modalities, and phenological changes, showing strong generation and cognition capabilities for both low- and high-level time series tasks. <div>
arXiv:2512.04461v1 Announce Type: new 
Abstract: One of the primary objectives of satellite remote sensing is to capture the complex dynamics of the Earth environment, which encompasses tasks such as reconstructing continuous cloud-free time series images, detecting land cover changes, and forecasting future surface evolution. However, existing methods typically require specialized models tailored to different tasks, lacking unified modeling of spatiotemporal features across multiple time series tasks. In this paper, we propose a Unified Time Series Generative Model (UniTS), a general framework applicable to various time series tasks, including time series reconstruction, time series cloud removal, time series semantic change detection, and time series forecasting. Based on the flow matching generative paradigm, UniTS constructs a deterministic evolution path from noise to targets under the guidance of task-specific conditions, achieving unified modeling of spatiotemporal representations for multiple tasks. The UniTS architecture consists of a diffusion transformer with spatio-temporal blocks, where we design an Adaptive Condition Injector (ACor) to enhance the model's conditional perception of multimodal inputs, enabling high-quality controllable generation. Additionally, we design a Spatiotemporal-aware Modulator (STM) to improve the ability of spatio-temporal blocks to capture complex spatiotemporal dependencies. Furthermore, we construct two high-quality multimodal time series datasets, TS-S12 and TS-S12CR, filling the gap of benchmark datasets for time series cloud removal and forecasting tasks. Extensive experiments demonstrate that UniTS exhibits exceptional generative and cognitive capabilities in both low-level and high-level time series tasks. It significantly outperforms existing methods, particularly when facing challenges such as severe cloud contamination, modality absence, and forecasting phenological variations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeRA: Decoupled Representation Alignment for Video Tokenization</title>
<link>https://arxiv.org/abs/2512.04483</link>
<guid>https://arxiv.org/abs/2512.04483</guid>
<content:encoded><![CDATA[
<div> Keywords: DeRA, video tokenizer, spatial-temporal representation, Symmetric Alignment-Conflict Projection, autoregressive video generation<br /><br />Summary: This paper introduces DeRA, a novel one-dimensional (1D) video tokenizer designed to improve both training efficiency and performance by decoupling spatial and temporal representation learning. DeRA maintains a compact 1D latent space while factorizing video encoding into two distinct streams: appearance and motion. These streams are aligned with pretrained vision foundation models to effectively capture spatial semantics and temporal dynamics separately. To overcome the gradient conflicts caused by the heterogeneous supervision of these two streams, the authors propose a Symmetric Alignment-Conflict Projection (SACP) module. The SACP module proactively reformulates gradients by suppressing components along conflicting directions, thus stabilizing and improving training. Extensive evaluations demonstrate that DeRA significantly outperforms LARP, the previous state-of-the-art video tokenizer, by 25% on the UCF-101 dataset based on relative Frechet Video Distance (rFVD). Furthermore, when applied to autoregressive video generation tasks, DeRA achieves new state-of-the-art results both in class-conditional video generation on UCF-101 and frame prediction on Kinetics-600 (K600). Overall, DeRA presents a significant advancement in efficient and effective video tokenization through its novel factorization approach and conflict-aware gradient adjustment mechanism. <div>
arXiv:2512.04483v1 Announce Type: new 
Abstract: This paper presents DeRA, a novel 1D video tokenizer that decouples the spatial-temporal representation learning in video tokenization to achieve better training efficiency and performance. Specifically, DeRA maintains a compact 1D latent space while factorizing video encoding into appearance and motion streams, which are aligned with pretrained vision foundation models to capture the spatial semantics and temporal dynamics in videos separately. To address the gradient conflicts introduced by the heterogeneous supervision, we further propose the Symmetric Alignment-Conflict Projection (SACP) module that proactively reformulates gradients by suppressing the components along conflicting directions. Extensive experiments demonstrate that DeRA outperforms LARP, the previous state-of-the-art video tokenizer by 25% on UCF-101 in terms of rFVD. Moreover, using DeRA for autoregressive video generation, we also achieve new state-of-the-art results on both UCF-101 class-conditional generation and K600 frame prediction.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Birds Look The Same: Identity-Preserving Generation For Birds</title>
<link>https://arxiv.org/abs/2512.04485</link>
<guid>https://arxiv.org/abs/2512.04485</guid>
<content:encoded><![CDATA[
<div> birds, identity-preserving generation, NABirds Look-Alikes, zero-shot models, fine-grained recognition<br /><br />Summary:<br /><br />Since the development of controllable image generation, models allowing zero-shot and identity-preserving manipulation, like Insert Anything and OmniControl, have enhanced user customization without requiring fine-tuning. However, these models struggle with non-rigid and fine-grained categories, which lack high-quality accessible data, especially multi-view images or videos, limiting evaluation and improvement. Birds represent a challenging but important domain due to their diversity, fine-grained identification cues, and varied poses. To address this gap, the authors introduce the NABirds Look-Alikes (NABLA) dataset, featuring 4,759 expert-curated image pairs alongside 1,073 pairs from multi-image iNaturalist observations and a small set of videos, forming a benchmark for identity-preserving bird image generation. Evaluations show that existing state-of-the-art baselines perform poorly in maintaining bird identity on this dataset. The study further demonstrates that training methods grouping images by species, age, and sex—which act as proxies for identity—result in significant performance improvements for both species seen during training and novel unseen species. This work highlights the importance of specialized datasets and training strategies to improve fine-grained, identity-aware image generation for complex, non-rigid categories like birds. <div>
arXiv:2512.04485v1 Announce Type: new 
Abstract: Since the advent of controllable image generation, increasingly rich modes of control have enabled greater customization and accessibility for everyday users. Zero-shot, identity-preserving models such as Insert Anything and OminiControl now support applications like virtual try-on without requiring additional fine-tuning. While these models may be fitting for humans and rigid everyday objects, they still have limitations for non-rigid or fine-grained categories. These domains often lack accessible, high-quality data -- especially videos or multi-view observations of the same subject -- making them difficult both to evaluate and to improve upon. Yet, such domains are essential for moving beyond content creation toward applications that demand accuracy and fine detail. Birds are an excellent domain for this task: they exhibit high diversity, require fine-grained cues for identification, and come in a wide variety of poses. We introduce the NABirds Look-Alikes (NABLA) dataset, consisting of 4,759 expert-curated image pairs. Together with 1,073 pairs collected from multi-image observations on iNaturalist and a small set of videos, this forms a benchmark for evaluating identity-preserving generation of birds. We show that state-of-the-art baselines fail to maintain identity on this dataset, and we demonstrate that training on images grouped by species, age, and sex -- used as a proxy for identity -- substantially improves performance on both seen and unseen species.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable Long-term Motion Generation with Extended Joint Targets</title>
<link>https://arxiv.org/abs/2512.04487</link>
<guid>https://arxiv.org/abs/2512.04487</guid>
<content:encoded><![CDATA[
<div> COMET, character motion, Transformer, real-time, style transfer<br /><br />Summary:<br /><br />1. The paper addresses the challenge of generating stable and controllable character motion in real-time, a critical issue in computer animation. 2. It introduces COMET, an autoregressive framework built on an efficient Transformer-based conditional Variational Autoencoder (VAE), which enables versatile and precise control of character joints for various animation tasks such as goal-reaching and in-betweening. 3. Unlike existing methods that struggle with fine-grained control or experience motion degradation over long sequences, COMET maintains high-quality motion synthesis during extended durations through a novel reference-guided feedback mechanism, which prevents error accumulation. 4. This feedback mechanism also functions as a plug-and-play stylization module, allowing real-time style transfer to be integrated seamlessly into the motion generation process. 5. Extensive evaluations demonstrate COMET’s superiority over current state-of-the-art methods, producing robust, high-quality motion at real-time speeds, making it suitable for demanding interactive applications in computer animation and character control. <div>
arXiv:2512.04487v1 Announce Type: new 
Abstract: Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shift-Window Meets Dual Attention: A Multi-Model Architecture for Specular Highlight Removal</title>
<link>https://arxiv.org/abs/2512.04496</link>
<guid>https://arxiv.org/abs/2512.04496</guid>
<content:encoded><![CDATA[
<div> Keywords: specular highlight removal, multi-model architecture, convolutional neural networks, attention mechanism, Omni-Directional Attention Integration Block<br /><br />Summary:<br /><br />1. The presence of specular highlights in practical environments adversely affects visual quality and task performance, motivating the need for effective removal methods.  
2. Existing methods either focus on local details using convolutional neural networks or on global context using transformer models, but single-type approaches face a challenge balancing fine-grained local features and long-range dependencies across varying highlight scales.  
3. The proposed Multi-Model Specular Highlight Removal (MM-SHR) architecture integrates convolutional operations in shallow layers for local detail extraction, while employing attention mechanisms in deeper layers to capture global features efficiently and accurately.  
4. To handle long-range dependencies without excessive computational overhead, MM-SHR introduces two key modules: the Omni-Directional Attention Integration Block (OAIBlock) and the Adaptive Region-Aware Hybrid-Domain Dual Attention Convolutional Network (HDDAConv), which utilize omni-directional pixel-shifting and window-dividing operations on raw features.  
5. Extensive evaluations across three benchmarks and six surface material types show that MM-SHR surpasses state-of-the-art methods in both accuracy and efficiency for specular highlight removal. The authors will release their implementation publicly on GitHub. <div>
arXiv:2512.04496v1 Announce Type: new 
Abstract: Inevitable specular highlights in practical environments severely impair the visual performance, thus degrading the task effectiveness and efficiency. Although there exist considerable methods that focus on local information from convolutional neural network models or global information from transformer models, the single-type model falls into a modeling dilemma between local fine-grained details and global long-range dependencies, thus deteriorating for specular highlights with different scales. Therefore, to accommodate specular highlights of all scales, we propose a multi-model architecture for specular highlight removal (MM-SHR) that effectively captures fine-grained features in highlight regions and models long-range dependencies between highlight and highlight-free areas. Specifically, we employ convolution operations to extract local details in the shallow layers of MM-SHR, and utilize the attention mechanism to capture global features in the deep layers, ensuring both operation efficiency and removal accuracy. To model long-range dependencies without compromising computational complexity, we utilize a coarse-to-fine manner and propose Omni-Directional Attention Integration Block(OAIBlock) and Adaptive Region-Aware Hybrid-Domain Dual Attention Convolutional Network(HDDAConv) , which leverage omni-directiona pixel-shifting and window-dividing operations at the raw features to achieve specular highlight removal. Extensive experimental results on three benchmark tasks and six types of surface materials demonstrate that MM-SHR outperforms state-of-the-art methods in both accuracy and efficiency for specular highlight removal. The implementation will be made publicly available at https://github.com/Htcicv/MM-SHR.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to Basics: Motion Representation Matters for Human Motion Generation Using Diffusion Model</title>
<link>https://arxiv.org/abs/2512.04499</link>
<guid>https://arxiv.org/abs/2512.04499</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, motion synthesis, motion representations, loss functions, training efficiency  

<br /><br />Summary:  
This paper explores fundamental aspects of diffusion models in human motion synthesis, focusing on task-oriented applications such as action-to-motion, text-to-motion, and audio-to-motion. The study uses a proxy motion diffusion model (MDM) and introduces the v loss objective, defined as a weighted sum of motion data and noise, to better understand latent data distributions. First, the authors evaluate six common motion representations from existing literature, comparing their performance using quality and diversity metrics across diverse datasets, revealing significant differences in effectiveness. Second, the paper investigates various training configurations, analyzing their impact on training time to offer insights into accelerating the training process of motion diffusion models. Third, extensive evaluation on a large motion dataset provides robust empirical evidence supporting the earlier findings. The results highlight how choices related to motion representation and training setup critically influence model performance and efficiency. Ultimately, the paper provides a comprehensive controlled study that advances the foundational understanding of conditional motion diffusion models, potentially guiding future research and development in the field for enhanced generative motion synthesis systems. <div>
arXiv:2512.04499v1 Announce Type: new 
Abstract: Diffusion models have emerged as a widely utilized and successful methodology in human motion synthesis. Task-oriented diffusion models have significantly advanced action-to-motion, text-to-motion, and audio-to-motion applications. In this paper, we investigate fundamental questions regarding motion representations and loss functions in a controlled study, and we enumerate the impacts of various decisions in the workflow of the generative motion diffusion model. To answer these questions, we conduct empirical studies based on a proxy motion diffusion model (MDM). We apply v loss as the prediction objective on MDM (vMDM), where v is the weighted sum of motion data and noise. We aim to enhance the understanding of latent data distributions and provide a foundation for improving the state of conditional motion diffusion models. First, we evaluate the six common motion representations in the literature and compare their performance in terms of quality and diversity metrics. Second, we compare the training time under various configurations to shed light on how to speed up the training process of motion diffusion models. Finally, we also conduct evaluation analysis on a large motion dataset. The results of our experiments indicate clear performance differences across motion representations in diverse datasets. Our results also demonstrate the impacts of distinct configurations on model training and suggest the importance and effectiveness of these decisions on the outcomes of motion diffusion models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.04504</link>
<guid>https://arxiv.org/abs/2512.04504</guid>
<content:encoded><![CDATA[
<div> Keywords: image diffusion transformers, frequency analysis, positional embeddings, adaptive attention, ultra-high-resolution generation<br /><br />Summary: UltraImage addresses key challenges in image diffusion transformers related to large-scale image generation, particularly content repetition and quality degradation. By performing frequency-wise analysis of positional embeddings, the authors identify that content repetition is caused by the periodicity of a dominant frequency that corresponds to the training image resolution. To mitigate this, they propose a recursive dominant frequency correction method that confines the frequency within a single period during extrapolation. Furthermore, the research reveals that degraded image quality results from diluted attention in the transformer network. To counteract this, UltraImage employs entropy-guided adaptive attention concentration which adjusts the attention focus dynamically—sharpening local attention for fine details while maintaining lower attention on global structures to preserve overall consistency. Experimental results on Qwen-Image and Flux datasets (around 4K resolution) demonstrate that UltraImage reduces repetition and enhances visual fidelity compared to existing methods. Impressively, UltraImage can extrapolate beyond training resolution (1328p) to generate images up to 6K by 6K without relying on low-resolution guidance, underscoring its strong extrapolation capability. The project and additional resources can be accessed via their webpage. <div>
arXiv:2512.04504v1 Announce Type: new 
Abstract: Recent image diffusion transformers achieve high-fidelity generation, but struggle to generate images beyond these scales, suffering from content repetition and quality degradation. In this work, we present UltraImage, a principled framework that addresses both issues. Through frequency-wise analysis of positional embeddings, we identify that repetition arises from the periodicity of the dominant frequency, whose period aligns with the training resolution. We introduce a recursive dominant frequency correction to constrain it within a single period after extrapolation. Furthermore, we find that quality degradation stems from diluted attention and thus propose entropy-guided adaptive attention concentration, which assigns higher focus factors to sharpen local attention for fine detail and lower ones to global attention patterns to preserve structural consistency. Experiments show that UltraImage consistently outperforms prior methods on Qwen-Image and Flux (around 4K) across three generation scenarios, reducing repetition and improving visual fidelity. Moreover, UltraImage can generate images up to 6K*6K without low-resolution guidance from a training resolution of 1328p, demonstrating its extreme extrapolation capability. Project page is available at \href{https://thu-ml.github.io/ultraimage.github.io/}{https://thu-ml.github.io/ultraimage.github.io/}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DuGI-MAE: Improving Infrared Mask Autoencoders via Dual-Domain Guidance</title>
<link>https://arxiv.org/abs/2512.04511</link>
<guid>https://arxiv.org/abs/2512.04511</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared imaging, foundation model, masked autoencoder, dual-domain guidance, large-scale dataset

<br /><br />Summary: Infrared imaging is essential for effective vision in low-light and adverse weather conditions but existing foundation models like Masked Autoencoder (MAE), trained primarily on visible spectrum data, do not perform optimally for infrared images. To address this issue, the authors developed an infrared-specific foundation model called InfMAE, pretrained on large infrared datasets. However, InfMAE has limitations such as missing informative tokens, weak modeling of global token relationships, and lack of noise handling for non-uniform background noise found in infrared imagery. To overcome these challenges, the paper proposes DuGI-MAE, a Dual-domain Guided Infrared MAE model. DuGI-MAE employs a deterministic masking strategy based on token entropy that retains only high-entropy tokens to improve informativeness in reconstruction. In addition, the model incorporates a Dual-Domain Guidance (DDG) module to simultaneously capture global token associations and adaptively filter out non-uniform background noise. To support extensive pretraining, the authors introduce Inf-590K, a comprehensive infrared image dataset with diverse scenes, target types, and spatial resolutions. The model pretrained on Inf-590K demonstrates strong generalization across various downstream tasks including infrared object detection, semantic segmentation, and small target detection. Experimental results show that DuGI-MAE outperforms both supervised and self-supervised baselines, confirming its effectiveness. The code is provided as supplementary material. <div>
arXiv:2512.04511v1 Announce Type: new 
Abstract: Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoLCD: Egocentric Video Generation with Long Context Diffusion</title>
<link>https://arxiv.org/abs/2512.04515</link>
<guid>https://arxiv.org/abs/2512.04515</guid>
<content:encoded><![CDATA[
<div> Keywords: egocentric video generation, long-term memory, content drift, memory management, temporal consistency<br /><br />Summary:<br /><br />1. Generating long and coherent egocentric videos is challenging due to the complexity of hand-object interactions and procedural tasks that require reliable long-term memory retention.<br />2. Existing autoregressive video generation models tend to suffer from content drift, where the identity of objects and overall scene semantics degrade as the video progresses.<br />3. The proposed framework, EgoLCD, addresses these limitations by framing long video synthesis as a problem of efficient and stable memory management, combining both long-term and short-term memory components.<br />4. EgoLCD integrates a Long-Term Sparse Key-Value (KV) Cache to maintain stable global context and an attention-based short-term memory module extended via Low-Rank Adaptation (LoRA) for local content adaptation.<br />5. A novel Memory Regulation Loss is introduced to enforce consistent memory usage throughout the sequence, while Structured Narrative Prompting provides explicit temporal guidance to improve coherence.<br />6. Experimental results on the EgoVid-5M benchmark show that EgoLCD outperforms existing methods in perceptual quality and temporal consistency, effectively reducing generative forgetting.<br />7. The work represents an important step toward scalable world models for embodied AI, supported by publicly available code and a project website for further research and development. <div>
arXiv:2512.04515v1 Announce Type: new 
Abstract: Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory</title>
<link>https://arxiv.org/abs/2512.04519</link>
<guid>https://arxiv.org/abs/2512.04519</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive diffusion, long-video generation, state-space model, temporal consistency, interactive control<br /><br />Summary:<br /><br />This paper addresses the challenge of generating coherent long videos through autoregressive (AR) diffusion, which produces frames sequentially but faces issues like accumulated errors, motion drift, and repetitive content over minute-scale durations. To overcome these problems, the authors propose VideoSSM, a novel Long Video Model that combines autoregressive diffusion with a hybrid state-space memory system. The state-space model (SSM) acts as a global memory that evolves with the scene dynamics throughout the entire video sequence, while a localized context window captures finer motion details and local nuances. This hybrid memory design helps maintain global consistency in the video content, preventing frozen or repetitive patterns typically seen in long-video generation. Additionally, VideoSSM supports prompt-adaptive interaction, allowing dynamic user control during generation, and efficiently scales linearly with the length of the video sequence. Experimental results on both short- and long-range video benchmarks show that VideoSSM achieves state-of-the-art temporal consistency and motion stability for autoregressive video generation, especially over minute-long horizons. Overall, the work establishes a scalable framework that integrates memory-aware mechanisms for producing diverse, interactive, and coherent long-duration video content. <div>
arXiv:2512.04519v1 Announce Type: new 
Abstract: Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.04520</link>
<guid>https://arxiv.org/abs/2512.04520</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot segmentation, test-time adaptation, medical image segmentation, foundation models, boundary-aware attention<br /><br />Summary: This paper addresses the limitations of conventional tuning methods in medical image segmentation, which struggle due to scarce annotated data and high computational costs. It highlights the promise of zero-shot segmentation using foundation models like SAM (Segment Anything Model), while noting SAM’s reduced effectiveness on medical datasets caused by domain shifts. To tackle these issues, the authors propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that enhances SAM's zero-shot segmentation performance without requiring any source-domain training data. The framework incorporates two key innovations: (1) encoder-level Gaussian prompt injection, which embeds Gaussian-based prompts into the image encoder for improved initial representation learning, and (2) cross-layer boundary-aware attention alignment, which leverages hierarchical interactions within the ViT backbone to align deep semantic features with shallow boundary cues. Experimental results across four public medical datasets (ISIC, Kvasir, BUSI, and REFUGE) demonstrate an average DICE score improvement of 12.4% over SAM’s zero-shot baseline. The proposed method consistently outperforms state-of-the-art models in medical image segmentation, significantly boosting SAM’s generalization capability without additional training on source domain data. The code is publicly available for further research and implementation. <div>
arXiv:2512.04520v1 Announce Type: new 
Abstract: Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM's zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WiFi-based Cross-Domain Gesture Recognition Using Attention Mechanism</title>
<link>https://arxiv.org/abs/2512.04521</link>
<guid>https://arxiv.org/abs/2512.04521</guid>
<content:encoded><![CDATA[
<div> Keywords: WiFi sensing, gesture recognition, cross-domain, Doppler spectrum, attention mechanism<br /><br />Summary:<br /><br />This paper addresses the challenge of cross-domain gesture recognition using WiFi signals, which are advantageous due to their widespread availability, low cost, and robustness against environmental factors. The authors propose a novel approach that extracts Doppler spectra from channel state information (CSI) collected by multiple receivers. These spectra are concatenated along the time axis to form fused multi-angle images, enriching the input features for gesture recognition. A new neural network architecture is introduced, inspired by the convolutional block attention module (CBAM), which combines a multi-semantic spatial attention mechanism with a self-attention-based channel mechanism. This design enables the network to generate attention maps that highlight important spatiotemporal gesture features, crucial for capturing domain-independent characteristics. The well-known ResNet18 model is integrated as the backbone to extract deeper-level features effectively. The authors validate their network on the public Widar3 dataset, demonstrating that their approach achieves near-perfect accuracy in the trained domain (99.72%) and maintains high accuracy when tested across different domains (97.61%). This performance significantly surpasses existing state-of-the-art methods, indicating that their multi-angle Doppler fusion and attention-based network effectively enhance cross-domain recognition capabilities in WiFi-based gesture sensing. <div>
arXiv:2512.04521v1 Announce Type: new 
Abstract: While fulfilling communication tasks, wireless signals can also be used to sense the environment. Among various types of sensing media, WiFi signals offer advantages such as widespread availability, low hardware cost, and strong robustness to environmental conditions like light, temperature, and humidity. By analyzing Wi-Fi signals in the environment, it is possible to capture dynamic changes of the human body and accomplish sensing applications such as gesture recognition. Although many existing gesture sensing solutions perform well in-domain but lack cross-domain capabilities (i.e., recognition performance in untrained environments). To address this, we extract Doppler spectra from the channel state information (CSI) received by all receivers and concatenate each Doppler spectrum along the same time axis to generate fused images with multi-angle information as input features. Furthermore, inspired by the convolutional block attention module (CBAM), we propose a gesture recognition network that integrates a multi-semantic spatial attention mechanism with a self-attention-based channel mechanism. This network constructs attention maps to quantify the spatiotemporal features of gestures in images, enabling the extraction of key domain-independent features. Additionally, ResNet18 is employed as the backbone network to further capture deep-level features. To validate the network performance, we evaluate the proposed network on the public Widar3 dataset, and the results show that it not only maintains high in-domain accuracy of 99.72%, but also achieves high performance in cross-domain recognition of 97.61%, significantly outperforming existing best solutions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identity Clue Refinement and Enhancement for Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2512.04522</link>
<guid>https://arxiv.org/abs/2512.04522</guid>
<content:encoded><![CDATA[
<div> Visible-Infrared Person Re-Identification, cross-modal matching, modality-specific attributes, semantic distillation, identity-aware features  

<br /><br />Summary:  
This paper addresses the challenges of Visible-Infrared Person Re-Identification (VI-ReID), a cross-modal matching task complicated by significant differences between visual and infrared data modalities. Unlike existing approaches that primarily learn modality-invariant features by focusing only on shared discriminative semantics, this work emphasizes the importance of modality-specific identity-aware information for improved feature discrimination. The authors introduce the Identity Clue Refinement and Enhancement (ICRE) network, which incorporates several novel components. First, the Multi-Perception Feature Refinement (MPFR) module aggregates shallow features from shared network branches to better capture overlooked modality-specific attributes. Next, the Semantic Distillation Cascade Enhancement (SDCE) module distills identity-relevant knowledge from these shallow aggregated features to enhance the learning of modality-invariant features. Additionally, an Identity Clues Guided (ICG) Loss is proposed to mitigate modality discrepancies in the enhanced features and encourage a more diverse representation space. Extensive experiments validate the effectiveness of the ICRE framework, demonstrating its superior performance over state-of-the-art methods across multiple publicly available datasets. This work highlights the value of integrating modality-specific identity clues alongside invariant features for robust VI-ReID. <div>
arXiv:2512.04522v1 Announce Type: new 
Abstract: Visible-Infrared Person Re-Identification (VI-ReID) is a challenging cross-modal matching task due to significant modality discrepancies. While current methods mainly focus on learning modality-invariant features through unified embedding spaces, they often focus solely on the common discriminative semantics across modalities while disregarding the critical role of modality-specific identity-aware knowledge in discriminative feature learning. To bridge this gap, we propose a novel Identity Clue Refinement and Enhancement (ICRE) network to mine and utilize the implicit discriminative knowledge inherent in modality-specific attributes. Initially, we design a Multi-Perception Feature Refinement (MPFR) module that aggregates shallow features from shared branches, aiming to capture modality-specific attributes that are easily overlooked. Then, we propose a Semantic Distillation Cascade Enhancement (SDCE) module, which distills identity-aware knowledge from the aggregated shallow features and guide the learning of modality-invariant features. Finally, an Identity Clues Guided (ICG) Loss is proposed to alleviate the modality discrepancies within the enhanced features and promote the learning of a diverse representation space. Extensive experiments across multiple public datasets clearly show that our proposed ICRE outperforms existing SOTA methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto3R: Automated 3D Reconstruction and Scanning via Data-driven Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2512.04528</link>
<guid>https://arxiv.org/abs/2512.04528</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scanning, uncertainty quantification, autonomous reconstruction, non-lambertian materials, robotic digitization  

<br /><br />Summary:  
This article presents Auto3R, a novel data-driven uncertainty quantification model designed to automate the 3D scanning and reconstruction process for both scenes and objects. The motivation stems from the traditionally labor-intensive nature of high-quality 3D scanning, which requires human planning, and the emerging need for fully autonomous systems using drones and robots. Auto3R specifically addresses challenges posed by objects with complex surface properties such as non-lambertian and specular materials, which are difficult to scan accurately. The model operates in an iterative loop, predicting the uncertainty distribution of potential scanning viewpoints without prior knowledge of the ground truth geometry or appearance, enabling efficient and precise viewpoint selection. Extensive experiments demonstrate that Auto3R significantly outperforms existing state-of-the-art methods in accuracy and efficiency. Moreover, the approach is validated in practical deployment by integrating Auto3R onto a robotic arm equipped with a camera, successfully digitizing real-world objects to create photorealistic digital assets ready for use. The contribution facilitates fully automated, high-quality 3D digitization, advancing the capabilities of embodied systems in autonomous perception and reconstruction tasks. Additional resources and code are available on the project’s homepage for further exploration and application. <div>
arXiv:2512.04528v1 Announce Type: new 
Abstract: Traditional high-quality 3D scanning and reconstruction typically relies on human labor to plan the scanning procedure. With the rapid development of embodied systems such as drones and robots, there is a growing demand of performing accurate 3D scanning and reconstruction in an fully automated manner. We introduce Auto3R, a data-driven uncertainty quantification model that is designed to automate the 3D scanning and reconstruction of scenes and objects, including objects with non-lambertian and specular materials. Specifically, in a process of iterative 3D reconstruction and scanning, Auto3R can make efficient and accurate prediction of uncertainty distribution over potential scanning viewpoints, without knowing the ground truth geometry and appearance. Through extensive experiments, Auto3R achieves superior performance that outperforms the state-of-the-art methods by a large margin. We also deploy Auto3R on a robot arm equipped with a camera and demonstrate that Auto3R can be used to effectively digitize real-world 3D objects and delivers ready-to-use and photorealistic digital assets. Our homepage: https://tomatoma00.github.io/auto3r.github.io .
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement</title>
<link>https://arxiv.org/abs/2512.04532</link>
<guid>https://arxiv.org/abs/2512.04532</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Language Models, physical dynamics, Neural ODE, motion modeling, self-supervised learning<br /><br />Summary:<br /><br />Video Large Language Models (Video LLMs) excel at video-language tasks but struggle with deep physical dynamics understanding due to their reliance on appearance-based matching. To overcome this, the authors propose PhyVLLM, a framework designed to explicitly incorporate physical motion into Video LLMs. PhyVLLM addresses three main challenges: disentangling motion signals from appearance variations, modeling continuous-time physical dynamics, and avoiding the need for costly physical annotations. The approach utilizes a dual-branch encoder to separate visual appearance from object motion effectively. A Neural Ordinary Differential Equation (Neural ODE) module is integrated to model the temporal evolution of physical dynamics, producing differentiable motion-aware representations. These representations are then projected into the token space of a pretrained Large Language Model, allowing physics reasoning without sacrificing general multimodal capabilities. Importantly, PhyVLLM uses self-supervised learning to capture continuous motion evolution, thereby eliminating the requirement for explicit physical labels. Experimental results show that PhyVLLM significantly outperforms state-of-the-art Video LLMs in physical reasoning and overall video understanding, validating the benefits of embedding explicit physical modeling into video-language frameworks. <div>
arXiv:2512.04532v1 Announce Type: new 
Abstract: Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refa\c{c}ade: Editing Object with Given Reference Texture</title>
<link>https://arxiv.org/abs/2512.04534</link>
<guid>https://arxiv.org/abs/2512.04534</guid>
<content:encoded><![CDATA[
arXiv:2512.04534v1 Announce Type: new 
Abstract: Recent advances in diffusion models have brought remarkable progress in image and video editing, yet some tasks remain underexplored. In this paper, we introduce a new task, Object Retexture, which transfers local textures from a reference object to a target object in images or videos. To perform this task, a straightforward solution is to use ControlNet conditioned on the source structure and the reference texture. However, this approach suffers from limited controllability for two reasons: conditioning on the raw reference image introduces unwanted structural information, and it fails to disentangle the visual texture and structure information of the source. To address this problem, we propose Refa\c{c}ade, a method that consists of two key designs to achieve precise and controllable texture transfer in both images and videos. First, we employ a texture remover trained on paired textured/untextured 3D mesh renderings to remove appearance information while preserving the geometry and motion of source videos. Second, we disrupt the reference global layout using a jigsaw permutation, encouraging the model to focus on local texture statistics rather than the global layout of the object. Extensive experiments demonstrate superior visual quality, precise editing, and controllability, outperforming strong baselines in both quantitative and human evaluations. Code is available at https://github.com/fishZe233/Refacade.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model</title>
<link>https://arxiv.org/abs/2512.04536</link>
<guid>https://arxiv.org/abs/2512.04536</guid>
<content:encoded><![CDATA[
arXiv:2512.04536v1 Announce Type: new 
Abstract: Alcohol consumption is a significant public health concern and a major cause of accidents and fatalities worldwide. This study introduces a novel video-based facial sequence analysis approach dedicated to the detection of alcohol intoxication. The method integrates facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features extracted using a 3D ResNet. These features are dynamically fused with adaptive prioritization to enhance classification performance. Additionally, we introduce a curated dataset comprising 3,542 video segments derived from 202 individuals to support training and evaluation. Our model is compared against two baselines: a custom 3D-CNN and a VGGFace+LSTM architecture. Experimental results show that our approach achieves 95.82% accuracy, 0.977 precision, and 0.97 recall, outperforming prior methods. The findings demonstrate the model's potential for practical deployment in public safety systems for non-invasive, reliable alcohol intoxication detection.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</title>
<link>https://arxiv.org/abs/2512.04537</link>
<guid>https://arxiv.org/abs/2512.04537</guid>
<content:encoded><![CDATA[
arXiv:2512.04537v1 Announce Type: new 
Abstract: The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to "robotize" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly "overlay" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million "robotized" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management</title>
<link>https://arxiv.org/abs/2512.04540</link>
<guid>https://arxiv.org/abs/2512.04540</guid>
<content:encoded><![CDATA[
arXiv:2512.04540v1 Announce Type: new 
Abstract: Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understanding as a sequential generation task via adaptive memory management. Specifically, VideoMem dynamically updates a global memory buffer, which adaptively retains critical information while discarding redundant content across the video timeline. To efficiently train VLMs for such long-term tasks, VideoMem integrates the Progressive Grouped Relative Policy Optimization (PRPO) algorithm, equipped with two core modules: Progressive State Propagation (PSP) adaptively retains valid current states, propagates them to the next rollout step, and gradually narrows the model exploration space. Temporal Cascading Reward (TCR) further alleviates reward sparsity, improving sample utilization and accelerating convergence. Extensive experiments demonstrate that VideoMem significantly outperforms existing open-source models across diverse benchmarks for ultra-long video understanding tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization</title>
<link>https://arxiv.org/abs/2512.04542</link>
<guid>https://arxiv.org/abs/2512.04542</guid>
<content:encoded><![CDATA[
arXiv:2512.04542v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency. \replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment. Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\&amp;T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360. Notably, superior Chamfer Distance (0.64) on DTU and F1 score (0.44) on T\&amp;T are obtained, alongside the best SSIM (0.855) and LPIPS (0.136) among baselines on Mip-NeRF 360, validating the framework's ability to enhance surface reconstruction accuracy without compromising photometric fidelity.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfeit Answers: Adversarial Forgery against OCR-Free Document Visual Question Answering</title>
<link>https://arxiv.org/abs/2512.04554</link>
<guid>https://arxiv.org/abs/2512.04554</guid>
<content:encoded><![CDATA[
arXiv:2512.04554v1 Announce Type: new 
Abstract: Document Visual Question Answering (DocVQA) enables end-to-end reasoning grounded on information present in a document input. While recent models have shown impressive capabilities, they remain vulnerable to adversarial attacks. In this work, we introduce a novel attack scenario that aims to forge document content in a visually imperceptible yet semantically targeted manner, allowing an adversary to induce specific or generally incorrect answers from a DocVQA model. We develop specialized attack algorithms that can produce adversarially forged documents tailored to different attackers' goals, ranging from targeted misinformation to systematic model failure scenarios. We demonstrate the effectiveness of our approach against two end-to-end state-of-the-art models: Pix2Struct, a vision-language transformer that jointly processes image and text through sequence-to-sequence modeling, and Donut, a transformer-based model that directly extracts text and answers questions from document images. Our findings highlight critical vulnerabilities in current DocVQA systems and call for the development of more robust defenses.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence</title>
<link>https://arxiv.org/abs/2512.04563</link>
<guid>https://arxiv.org/abs/2512.04563</guid>
<content:encoded><![CDATA[
arXiv:2512.04563v1 Announce Type: new 
Abstract: Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset creation for supervised deep learning-based analysis of microscopic images - review of important considerations and recommendations</title>
<link>https://arxiv.org/abs/2512.04564</link>
<guid>https://arxiv.org/abs/2512.04564</guid>
<content:encoded><![CDATA[
arXiv:2512.04564v1 Announce Type: new 
Abstract: Supervised deep learning (DL) receives great interest for automated analysis of microscopic images with an increasing body of literature supporting its potential. The development and validation of those DL models relies heavily on the availability of high-quality, large-scale datasets. However, creating such datasets is a complex and resource-intensive process, often hindered by challenges such as time constraints, domain variability, and risks of bias in image collection and label creation. This review provides a comprehensive guide to the critical steps in dataset creation, including: 1) image acquisition, 2) selection of annotation software, and 3) annotation creation. In addition to ensuring a sufficiently large number of images, it is crucial to address sources of image variability (domain shifts) - such as those related to slide preparation and digitization - that could lead to algorithmic errors if not adequately represented in the training data. Key quality criteria for annotations are the three "C"s: correctness, completeness, and consistency. This review explores methods to enhance annotation quality through the use of advanced techniques that mitigate the limitations of single annotators. To support dataset creators, a standard operating procedure (SOP) is provided as supplemental material, outlining best practices for dataset development. Furthermore, the article underscores the importance of open datasets in driving innovation and enhancing reproducibility of DL research. By addressing the challenges and offering practical recommendations, this review aims to advance the creation of and availability to high-quality, large-scale datasets, ultimately contributing to the development of generalizable and robust DL models for pathology applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt2Craft: Generating Functional Craft Assemblies with LLMs</title>
<link>https://arxiv.org/abs/2512.04568</link>
<guid>https://arxiv.org/abs/2512.04568</guid>
<content:encoded><![CDATA[
arXiv:2512.04568v1 Announce Type: new 
Abstract: Inspired by traditional handmade crafts, where a person improvises assemblies based on the available objects, we formally introduce the Craft Assembly Task. It is a robotic assembly task that involves building an accurate representation of a given target object using the available objects, which do not directly correspond to its parts. In this work, we focus on selecting the subset of available objects for the final craft, when the given input is an RGB image of the target in the wild. We use a mask segmentation neural network to identify visible parts, followed by retrieving labeled template meshes. These meshes undergo pose optimization to determine the most suitable template. Then, we propose to simplify the parts of the transformed template mesh to primitive shapes like cuboids or cylinders. Finally, we design a search algorithm to find correspondences in the scene based on local and global proportions. We develop baselines for comparison that consider all possible combinations, and choose the highest scoring combination for common metrics used in foreground maps and mask accuracy. Our approach achieves comparable results to the baselines for two different scenes, and we show qualitative results for an implementation in a real-world scenario.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARDis: Time Attenuated Representation Disentanglement for Incomplete Multi-Modal Tumor Segmentation and Classification</title>
<link>https://arxiv.org/abs/2512.04576</link>
<guid>https://arxiv.org/abs/2512.04576</guid>
<content:encoded><![CDATA[
arXiv:2512.04576v1 Announce Type: new 
Abstract: Tumor segmentation and diagnosis in contrast-enhanced Computed Tomography (CT) rely heavily on the physiological dynamics of contrast agents. However, obtaining a complete multi-phase series is often clinically unfeasible due to radiation concerns or scanning limitations, leading to the "missing modality" problem. Existing deep learning approaches typically treat missing phases as absent independent channels, ignoring the inherent temporal continuity of hemodynamics. In this work, we propose Time Attenuated Representation Disentanglement (TARDis), a novel physics-aware framework that redefines missing modalities as missing sample points on a continuous Time-Attenuation Curve. TARDis explicitly disentangles the latent feature space into a time-invariant static component (anatomy) and a time-dependent dynamic component (perfusion). We achieve this via a dual-path architecture: a quantization-based path using a learnable embedding dictionary to extract consistent anatomical structures, and a probabilistic path using a Conditional Variational Autoencoder to model dynamic enhancement conditioned on the estimated scan time. This design allows the network to hallucinate missing hemodynamic features by sampling from the learned latent distribution. Extensive experiments on a large-scale private abdominal CT dataset (2,282 cases) and two public datasets demonstrate that TARDis significantly outperforms state-of-the-art incomplete modality frameworks. Notably, our method maintains robust diagnostic performance even in extreme data-sparsity scenarios, highlighting its potential for reducing radiation exposure while maintaining diagnostic precision.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.04581</link>
<guid>https://arxiv.org/abs/2512.04581</guid>
<content:encoded><![CDATA[
arXiv:2512.04581v1 Announce Type: new 
Abstract: Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM3-I: Segment Anything with Instructions</title>
<link>https://arxiv.org/abs/2512.04585</link>
<guid>https://arxiv.org/abs/2512.04585</guid>
<content:encoded><![CDATA[
arXiv:2512.04585v1 Announce Type: new 
Abstract: Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering</title>
<link>https://arxiv.org/abs/2512.04597</link>
<guid>https://arxiv.org/abs/2512.04597</guid>
<content:encoded><![CDATA[
arXiv:2512.04597v1 Announce Type: new 
Abstract: Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot</title>
<link>https://arxiv.org/abs/2512.04599</link>
<guid>https://arxiv.org/abs/2512.04599</guid>
<content:encoded><![CDATA[
arXiv:2512.04599v1 Announce Type: new 
Abstract: Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence</title>
<link>https://arxiv.org/abs/2512.04619</link>
<guid>https://arxiv.org/abs/2512.04619</guid>
<content:encoded><![CDATA[
arXiv:2512.04619v1 Announce Type: new 
Abstract: In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding</title>
<link>https://arxiv.org/abs/2512.04643</link>
<guid>https://arxiv.org/abs/2512.04643</guid>
<content:encoded><![CDATA[
arXiv:2512.04643v1 Announce Type: new 
Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I2I-Bench: A Comprehensive Benchmark Suite for Image-to-Image Editing Models</title>
<link>https://arxiv.org/abs/2512.04660</link>
<guid>https://arxiv.org/abs/2512.04660</guid>
<content:encoded><![CDATA[
arXiv:2512.04660v1 Announce Type: new 
Abstract: Image editing models are advancing rapidly, yet comprehensive evaluation remains a significant challenge. Existing image editing benchmarks generally suffer from limited task scopes, insufficient evaluation dimensions, and heavy reliance on manual annotations, which significantly constrain their scalability and practical applicability. To address this, we propose \textbf{I2I-Bench}, a comprehensive benchmark for image-to-image editing models, which features (i) diverse tasks, encompassing 10 task categories across both single-image and multi-image editing tasks, (ii) comprehensive evaluation dimensions, including 30 decoupled and fine-grained evaluation dimensions with automated hybrid evaluation methods that integrate specialized tools and large multimodal models (LMMs), and (iii) rigorous alignment validation, justifying the consistency between our benchmark evaluations and human preferences. Using I2I-Bench, we benchmark numerous mainstream image editing models, investigating the gaps and trade-offs between editing models across various dimensions. We will open-source all components of I2I-Bench to facilitate future research.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
<link>https://arxiv.org/abs/2512.04677</link>
<guid>https://arxiv.org/abs/2512.04677</guid>
<content:encoded><![CDATA[
arXiv:2512.04677v1 Announce Type: new 
Abstract: Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation</title>
<link>https://arxiv.org/abs/2512.04678</link>
<guid>https://arxiv.org/abs/2512.04678</guid>
<content:encoded><![CDATA[
arXiv:2512.04678v1 Announce Type: new 
Abstract: Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Cross-View Point Correspondence in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.04686</link>
<guid>https://arxiv.org/abs/2512.04686</guid>
<content:encoded><![CDATA[
arXiv:2512.04686v1 Announce Type: new 
Abstract: Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of "perceive", "reason", and "correspond". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.04699</link>
<guid>https://arxiv.org/abs/2512.04699</guid>
<content:encoded><![CDATA[
arXiv:2512.04699v1 Announce Type: new 
Abstract: Arbitrary-scale super-resolution (ASSR) overcomes the limitation of traditional super-resolution (SR) methods that operate only at fixed scales (e.g., 4x), enabling a single model to handle arbitrary magnification. Most existing ASSR approaches rely on implicit neural representation (INR), but its regression-driven feature extraction and aggregation intrinsically limit the ability to synthesize fine details, leading to low realism. Recent diffusion-based realistic image super-resolution (Real-ISR) models leverage powerful pre-trained diffusion priors and show impressive results at the 4x setting. We observe that they can also achieve ASSR because the diffusion prior implicitly adapts to scale by encouraging high-realism generation. However, without explicit scale control, the diffusion process cannot be properly adjusted for different magnification levels, resulting in excessive hallucination or blurry outputs, especially under ultra-high scales. To address these issues, we propose OmniScaleSR, a diffusion-based realistic arbitrary-scale SR framework designed to achieve both high fidelity and high realism. We introduce explicit, diffusion-native scale control mechanisms that work synergistically with implicit scale adaptation, enabling scale-aware and content-aware modulation of the diffusion process. In addition, we incorporate multi-domain fidelity enhancement designs to further improve reconstruction accuracy. Extensive experiments on bicubic degradation benchmarks and real-world datasets show that OmniScaleSR surpasses state-of-the-art methods in both fidelity and perceptual realism, with particularly strong performance at large magnification factors. Code will be released at https://github.com/chaixinning/OmniScaleSR.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild</title>
<link>https://arxiv.org/abs/2512.04728</link>
<guid>https://arxiv.org/abs/2512.04728</guid>
<content:encoded><![CDATA[
arXiv:2512.04728v1 Announce Type: new 
Abstract: Generative psychological analysis of in-the-wild conversations faces two fundamental challenges: (1) existing Vision-Language Models (VLMs) fail to resolve Articulatory-Affective Ambiguity, where visual patterns of speech mimic emotional expressions; and (2) progress is stifled by a lack of verifiable evaluation metrics capable of assessing visual grounding and reasoning depth. We propose a complete ecosystem to address these twin challenges. First, we introduce Multilevel Insight Network for Disentanglement(MIND), a novel hierarchical visual encoder that introduces a Status Judgment module to algorithmically suppress ambiguous lip features based on their temporal feature variance, achieving explicit visual disentanglement. Second, we construct ConvoInsight-DB, a new large-scale dataset with expert annotations for micro-expressions and deep psychological inference. Third, Third, we designed the Mental Reasoning Insight Rating Metric (PRISM), an automated dimensional framework that uses expert-guided LLM to measure the multidimensional performance of large mental vision models. On our PRISM benchmark, MIND significantly outperforms all baselines, achieving a +86.95% gain in micro-expression detection over prior SOTA. Ablation studies confirm that our Status Judgment disentanglement module is the most critical component for this performance leap. Our code has been opened.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.04733</link>
<guid>https://arxiv.org/abs/2512.04733</guid>
<content:encoded><![CDATA[
arXiv:2512.04733v1 Announce Type: new 
Abstract: End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MT-Depth: Multi-task Instance feature analysis for the Depth Completion</title>
<link>https://arxiv.org/abs/2512.04734</link>
<guid>https://arxiv.org/abs/2512.04734</guid>
<content:encoded><![CDATA[
arXiv:2512.04734v1 Announce Type: new 
Abstract: Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order Matters: 3D Shape Generation from Sequential VR Sketches</title>
<link>https://arxiv.org/abs/2512.04761</link>
<guid>https://arxiv.org/abs/2512.04761</guid>
<content:encoded><![CDATA[
arXiv:2512.04761v1 Announce Type: new 
Abstract: VR sketching lets users explore and iterate on ideas directly in 3D, offering a faster and more intuitive alternative to conventional CAD tools. However, existing sketch-to-shape models ignore the temporal ordering of strokes, discarding crucial cues about structure and design intent. We introduce VRSketch2Shape, the first framework and multi-category dataset for generating 3D shapes from sequential VR sketches. Our contributions are threefold: (i) an automated pipeline that generates sequential VR sketches from arbitrary shapes, (ii) a dataset of over 20k synthetic and 900 hand-drawn sketch-shape pairs across four categories, and (iii) an order-aware sketch encoder coupled with a diffusion-based 3D generator. Our approach yields higher geometric fidelity than prior work, generalizes effectively from synthetic to real sketches with minimal supervision, and performs well even on partial sketches. All data and models will be released open-source at https://chenyizi086.github.io/VRSketch2Shape_website.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling</title>
<link>https://arxiv.org/abs/2512.04784</link>
<guid>https://arxiv.org/abs/2512.04784</guid>
<content:encoded><![CDATA[
arXiv:2512.04784v1 Announce Type: new 
Abstract: Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaFiTe: A Generative Latent Field for 3D Native Texturing</title>
<link>https://arxiv.org/abs/2512.04786</link>
<guid>https://arxiv.org/abs/2512.04786</guid>
<content:encoded><![CDATA[
arXiv:2512.04786v1 Announce Type: new 
Abstract: Generating high-fidelity, seamless textures directly on 3D surfaces, what we term 3D-native texturing, remains a fundamental open challenge, with the potential to overcome long-standing limitations of UV-based and multi-view projection methods. However, existing native approaches are constrained by the absence of a powerful and versatile latent representation, which severely limits the fidelity and generality of their generated textures. We identify this representation gap as the principal barrier to further progress. We introduce LaFiTe, a framework that addresses this challenge by learning to generate textures as a 3D generative sparse latent color field. At its core, LaFiTe employs a variational autoencoder (VAE) to encode complex surface appearance into a sparse, structured latent space, which is subsequently decoded into a continuous color field. This representation achieves unprecedented fidelity, exceeding state-of-the-art methods by >10 dB PSNR in reconstruction, by effectively disentangling texture appearance from mesh topology and UV parameterization. Building upon this strong representation, a conditional rectified-flow model synthesizes high-quality, coherent textures across diverse styles and geometries. Extensive experiments demonstrate that LaFiTe not only sets a new benchmark for 3D-native texturing but also enables flexible downstream applications such as material synthesis and texture super-resolution, paving the way for the next generation of 3D content creation workflows.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</title>
<link>https://arxiv.org/abs/2512.04810</link>
<guid>https://arxiv.org/abs/2512.04810</guid>
<content:encoded><![CDATA[
arXiv:2512.04810v1 Announce Type: new 
Abstract: We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS</title>
<link>https://arxiv.org/abs/2512.04815</link>
<guid>https://arxiv.org/abs/2512.04815</guid>
<content:encoded><![CDATA[
arXiv:2512.04815v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.04821</link>
<guid>https://arxiv.org/abs/2512.04821</guid>
<content:encoded><![CDATA[
arXiv:2512.04821v1 Announce Type: new 
Abstract: Generative models have achieved remarkable progress with the emergence of flow matching (FM). It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities. Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation. To model the data distribution, we first design two variational autoencoders (VAEs) to encode both medical images and their corresponding masks into a lower-dimensional latent space. We then estimate a conditional velocity field that guides the flow based on the input image. By sampling multiple latent representations, our method synthesizes diverse segmentation outputs whose pixel-wise variance reliably captures the underlying data distribution, enabling both highly accurate and uncertainty-aware predictions. Furthermore, we generate confidence maps that quantify the model certainty, providing clinicians with richer information for deeper analysis. We conduct experiments on two datasets, ISIC-2018 and CVC-Clinic, and compare our method with several prior baselines, including both deterministic and generative approach models. Through comprehensive evaluations, both qualitative and quantitative results show that our approach achieves superior segmentation accuracy while remaining highly efficient in the latent space.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis</title>
<link>https://arxiv.org/abs/2512.04830</link>
<guid>https://arxiv.org/abs/2512.04830</guid>
<content:encoded><![CDATA[
arXiv:2512.04830v1 Announce Type: new 
Abstract: Closed-loop simulation and scalable pre-training for autonomous driving require synthesizing free-viewpoint driving scenes. However, existing datasets and generative pipelines rarely provide consistent off-trajectory observations, limiting large-scale evaluation and training. While recent generative models demonstrate strong visual realism, they struggle to jointly achieve interpolation consistency and extrapolation realism without per-scene optimization. To address this, we propose FreeGen, a feed-forward reconstruction-generation co-training framework for free-viewpoint driving scene synthesis. The reconstruction model provides stable geometric representations to ensure interpolation consistency, while the generation model performs geometry-aware enhancement to improve realism at unseen viewpoints. Through co-training, generative priors are distilled into the reconstruction model to improve off-trajectory rendering, and the refined geometry in turn offers stronger structural guidance for generation. Experiments demonstrate that FreeGen achieves state-of-the-art performance for free-viewpoint driving scene synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenizing Buildings: A Transformer for Layout Synthesis</title>
<link>https://arxiv.org/abs/2512.04832</link>
<guid>https://arxiv.org/abs/2512.04832</guid>
<content:encoded><![CDATA[
arXiv:2512.04832v1 Announce Type: new 
Abstract: We introduce Small Building Model (SBM), a Transformer-based architecture for layout synthesis in Building Information Modeling (BIM) scenes. We address the question of how to tokenize buildings by unifying heterogeneous feature sets of architectural elements into sequences while preserving compositional structure. Such feature sets are represented as a sparse attribute-feature matrix that captures room properties. We then design a unified embedding module that learns joint representations of categorical and possibly correlated continuous feature groups. Lastly, we train a single Transformer backbone in two modes: an encoder-only pathway that yields high-fidelity room embeddings, and an encoder-decoder pipeline for autoregressive prediction of room entities, referred to as Data-Driven Entity Prediction (DDEP). Experiments across retrieval and generative layout synthesis show that SBM learns compact room embeddings that reliably cluster by type and topology, enabling strong semantic retrieval. In DDEP mode, SBM produces functionally sound layouts, with fewer collisions and boundary violations and improved navigability.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Sanity Check for Multi-In-Domain Face Forgery Detection in the Real World</title>
<link>https://arxiv.org/abs/2512.04837</link>
<guid>https://arxiv.org/abs/2512.04837</guid>
<content:encoded><![CDATA[
arXiv:2512.04837v1 Announce Type: new 
Abstract: Existing methods for deepfake detection aim to develop generalizable detectors. Although "generalizable" is the ultimate target once and for all, with limited training forgeries and domains, it appears idealistic to expect generalization that covers entirely unseen variations, especially given the diversity of real-world deepfakes. Therefore, introducing large-scale multi-domain data for training can be feasible and important for real-world applications. However, within such a multi-domain scenario, the differences between multiple domains, rather than the subtle real/fake distinctions, dominate the feature space. As a result, despite detectors being able to relatively separate real and fake within each domain (i.e., high AUC), they struggle with single-image real/fake judgments in domain-unspecified conditions (i.e., low ACC). In this paper, we first define a new research paradigm named Multi-In-Domain Face Forgery Detection (MID-FFD), which includes sufficient volumes of real-fake domains for training. Then, the detector should provide definitive real-fake judgments to the domain-unspecified inputs, which simulate the frame-by-frame independent detection scenario in the real world. Meanwhile, to address the domain-dominant issue, we propose a model-agnostic framework termed DevDet (Developer for Detector) to amplify real/fake differences and make them dominant in the feature space. DevDet consists of a Face Forgery Developer (FFDev) and a Dose-Adaptive detector Fine-Tuning strategy (DAFT). Experiments demonstrate our superiority in predicting real-fake under the MID-FFD scenario while maintaining original generalization ability to unseen data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens</title>
<link>https://arxiv.org/abs/2512.04857</link>
<guid>https://arxiv.org/abs/2512.04857</guid>
<content:encoded><![CDATA[
arXiv:2512.04857v1 Announce Type: new 
Abstract: Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing</title>
<link>https://arxiv.org/abs/2512.04862</link>
<guid>https://arxiv.org/abs/2512.04862</guid>
<content:encoded><![CDATA[
arXiv:2512.04862v1 Announce Type: new 
Abstract: Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection</title>
<link>https://arxiv.org/abs/2512.04875</link>
<guid>https://arxiv.org/abs/2512.04875</guid>
<content:encoded><![CDATA[
arXiv:2512.04875v1 Announce Type: new 
Abstract: Automated lesion detection in chest X-rays has demonstrated significant potential for improving clinical diagnosis by precisely localizing pathological abnormalities. While recent promptable detection frameworks have achieved remarkable accuracy in target localization, existing methods typically rely on manual annotations as prompts, which are labor-intensive and impractical for clinical applications. To address this limitation, we propose SP-Det, a novel self-prompted detection framework that automatically generates rich textual context to guide multi-label lesion detection without requiring expert annotations. Specifically, we introduce an expert-free dual-text prompt generator (DTPG) that leverages two complementary textual modalities: semantic context prompts that capture global pathological patterns and disease beacon prompts that focus on disease-specific manifestations. Moreover, we devise a bidirectional feature enhancer (BFE) that synergistically integrates comprehensive diagnostic context with disease-specific embeddings to significantly improve feature representation and detection accuracy. Extensive experiments on two chest X-ray datasets with diverse thoracic disease categories demonstrate that our SP-Det framework outperforms state-of-the-art detection methods while completely eliminating the dependency on expert-annotated prompts compared to existing promptable architectures.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDG-Track: A Heterogeneous Observer-Follower Framework for High-Resolution UAV Tracking on Embedded Platforms</title>
<link>https://arxiv.org/abs/2512.04883</link>
<guid>https://arxiv.org/abs/2512.04883</guid>
<content:encoded><![CDATA[
arXiv:2512.04883v1 Announce Type: new 
Abstract: Real-time tracking of small unmanned aerial vehicles (UAVs) on edge devices faces a fundamental resolution-speed conflict. Downsampling high-resolution imagery to standard detector input sizes causes small target features to collapse below detectable thresholds. Yet processing native 1080p frames on resource-constrained platforms yields insufficient throughput for smooth gimbal control. We propose SDG-Track, a Sparse Detection-Guided Tracker that adopts an Observer-Follower architecture to reconcile this conflict. The Observer stream runs a high-capacity detector at low frequency on the GPU to provide accurate position anchors from 1920x1080 frames. The Follower stream performs high-frequency trajectory interpolation via ROI-constrained sparse optical flow on the CPU. To handle tracking failures from occlusion or model drift caused by spectrally similar distractors, we introduce Dual-Space Recovery, a training-free re-acquisition mechanism combining color histogram matching with geometric consistency constraints. Experiments on a ground-to-air tracking station demonstrate that SDG-Track achieves 35.1 FPS system throughput while retaining 97.2\% of the frame-by-frame detection precision. The system successfully tracks agile FPV drones under real-world operational conditions on an NVIDIA Jetson Orin Nano. Our paper code is publicly available at https://github.com/Jeffry-wen/SDG-Track
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Only Train Once (YOTO): A Retraining-Free Object Detection Framework</title>
<link>https://arxiv.org/abs/2512.04888</link>
<guid>https://arxiv.org/abs/2512.04888</guid>
<content:encoded><![CDATA[
arXiv:2512.04888v1 Announce Type: new 
Abstract: Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI</title>
<link>https://arxiv.org/abs/2512.04890</link>
<guid>https://arxiv.org/abs/2512.04890</guid>
<content:encoded><![CDATA[
arXiv:2512.04890v1 Announce Type: new 
Abstract: We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching</title>
<link>https://arxiv.org/abs/2512.04904</link>
<guid>https://arxiv.org/abs/2512.04904</guid>
<content:encoded><![CDATA[
arXiv:2512.04904v1 Announce Type: new 
Abstract: Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion</title>
<link>https://arxiv.org/abs/2512.04926</link>
<guid>https://arxiv.org/abs/2512.04926</guid>
<content:encoded><![CDATA[
arXiv:2512.04926v1 Announce Type: new 
Abstract: Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtually Unrolling the Herculaneum Papyri by Diffeomorphic Spiral Fitting</title>
<link>https://arxiv.org/abs/2512.04927</link>
<guid>https://arxiv.org/abs/2512.04927</guid>
<content:encoded><![CDATA[
arXiv:2512.04927v1 Announce Type: new 
Abstract: The Herculaneum Papyri are a collection of rolled papyrus documents that were charred and buried by the famous eruption of Mount Vesuvius. They promise to contain a wealth of previously unseen Greek and Latin texts, but are extremely fragile and thus most cannot be unrolled physically. A solution to access these texts is virtual unrolling, where the papyrus surface is digitally traced out in a CT scan of the scroll, to create a flattened representation. This tracing is very laborious to do manually in gigavoxel-sized scans, so automated approaches are desirable. We present the first top-down method that automatically fits a surface model to a CT scan of a severely damaged scroll. We take a novel approach that globally fits an explicit parametric model of the deformed scroll to existing neural network predictions of where the rolled papyrus likely passes. Our method guarantees the resulting surface is a single continuous 2D sheet, even passing through regions where the surface is not detectable in the CT scan. We conduct comprehensive experiments on high-resolution CT scans of two scrolls, showing that our approach successfully unrolls large regions, and exceeds the performance of the only existing automated unrolling method suitable for this data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging</title>
<link>https://arxiv.org/abs/2512.04939</link>
<guid>https://arxiv.org/abs/2512.04939</guid>
<content:encoded><![CDATA[
arXiv:2512.04939v1 Announce Type: new 
Abstract: 3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition</title>
<link>https://arxiv.org/abs/2512.04943</link>
<guid>https://arxiv.org/abs/2512.04943</guid>
<content:encoded><![CDATA[
arXiv:2512.04943v1 Announce Type: new 
Abstract: This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization</title>
<link>https://arxiv.org/abs/2512.04952</link>
<guid>https://arxiv.org/abs/2512.04952</guid>
<content:encoded><![CDATA[
arXiv:2512.04952v1 Announce Type: new 
Abstract: Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoPE:A Unified Geometric Positional Embedding for Structured Tensors</title>
<link>https://arxiv.org/abs/2512.04963</link>
<guid>https://arxiv.org/abs/2512.04963</guid>
<content:encoded><![CDATA[
arXiv:2512.04963v1 Announce Type: new 
Abstract: Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis</title>
<link>https://arxiv.org/abs/2512.04967</link>
<guid>https://arxiv.org/abs/2512.04967</guid>
<content:encoded><![CDATA[
arXiv:2512.04967v1 Announce Type: new 
Abstract: Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Use of Vision Transformers for AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2512.04969</link>
<guid>https://arxiv.org/abs/2512.04969</guid>
<content:encoded><![CDATA[
arXiv:2512.04969v1 Announce Type: new 
Abstract: Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks</title>
<link>https://arxiv.org/abs/2512.04970</link>
<guid>https://arxiv.org/abs/2512.04970</guid>
<content:encoded><![CDATA[
arXiv:2512.04970v1 Announce Type: new 
Abstract: We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.04981</link>
<guid>https://arxiv.org/abs/2512.04981</guid>
<content:encoded><![CDATA[
arXiv:2512.04981v1 Announce Type: new 
Abstract: Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs</title>
<link>https://arxiv.org/abs/2512.04996</link>
<guid>https://arxiv.org/abs/2512.04996</guid>
<content:encoded><![CDATA[
arXiv:2512.04996v1 Announce Type: new 
Abstract: This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflection Removal through Efficient Adaptation of Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.05000</link>
<guid>https://arxiv.org/abs/2512.05000</guid>
<content:encoded><![CDATA[
arXiv:2512.05000v1 Announce Type: new 
Abstract: We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning for Transparent Object Depth Completion Using Depth from Non-Transparent Objects</title>
<link>https://arxiv.org/abs/2512.05006</link>
<guid>https://arxiv.org/abs/2512.05006</guid>
<content:encoded><![CDATA[
arXiv:2512.05006v1 Announce Type: new 
Abstract: The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Neural Video Compression via Video Diffusion Prior</title>
<link>https://arxiv.org/abs/2512.05016</link>
<guid>https://arxiv.org/abs/2512.05016</guid>
<content:encoded><![CDATA[
arXiv:2512.05016v1 Announce Type: new 
Abstract: We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition</title>
<link>https://arxiv.org/abs/2512.05021</link>
<guid>https://arxiv.org/abs/2512.05021</guid>
<content:encoded><![CDATA[
arXiv:2512.05021v1 Announce Type: new 
Abstract: Handwritten Text Recognition remains challenging due to the limited data, high writing style variance, and scripts with complex diacritics. Existing approaches, though partially address these issues, often struggle to generalize without massive synthetic data. To address these challenges, we propose HTR-ConvText, a model designed to capture fine-grained, stroke-level local features while preserving global contextual dependencies. In the feature extraction stage, we integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details. We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency. Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification. Evaluations on IAM, READ2016, LAM and HANDS-VNOnDB demonstrate that our approach achieves improved performance and better generalization compared to existing methods, especially in scenarios with limited training samples and high handwriting diversity.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation</title>
<link>https://arxiv.org/abs/2512.05025</link>
<guid>https://arxiv.org/abs/2512.05025</guid>
<content:encoded><![CDATA[
arXiv:2512.05025v1 Announce Type: new 
Abstract: Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding</title>
<link>https://arxiv.org/abs/2512.05039</link>
<guid>https://arxiv.org/abs/2512.05039</guid>
<content:encoded><![CDATA[
arXiv:2512.05039v1 Announce Type: new 
Abstract: Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration. Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors. In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis. Our approach starts with a method that organizes and synthesizes information based on meaning, followed by refining the texture. This process gives clear insights into the facial structure before we move on to creating detailed images. In the first stage, we blend two techniques: one that focuses on local features with CNNs and global features with Vision Transformers. This helped us create clear and detailed semantic layouts. In the second stage, we use a Multi-Modal Texture Generator to refine these layouts by pulling in information from different scales, ensuring everything looks cohesive and consistent. The architecture naturally handles arbitrary mask configurations through dynamic attention without maskspecific training. Experiment on two datasets CelebA-HQ and FFHQ shows that our model outperforms other state-of-the-art methods, showing improvements in metrics like LPIPS, PSNR, and SSIM. It produces visually striking results with better semantic preservation, in challenging large-area inpainting situations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image</title>
<link>https://arxiv.org/abs/2512.05044</link>
<guid>https://arxiv.org/abs/2512.05044</guid>
<content:encoded><![CDATA[
arXiv:2512.05044v1 Announce Type: new 
Abstract: Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer</title>
<link>https://arxiv.org/abs/2512.05060</link>
<guid>https://arxiv.org/abs/2512.05060</guid>
<content:encoded><![CDATA[
arXiv:2512.05060v1 Announce Type: new 
Abstract: Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BulletTime: Decoupled Control of Time and Camera Pose for Video Generation</title>
<link>https://arxiv.org/abs/2512.05076</link>
<guid>https://arxiv.org/abs/2512.05076</guid>
<content:encoded><![CDATA[
arXiv:2512.05076v1 Announce Type: new 
Abstract: Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints</title>
<link>https://arxiv.org/abs/2512.05079</link>
<guid>https://arxiv.org/abs/2512.05079</guid>
<content:encoded><![CDATA[
arXiv:2512.05079v1 Announce Type: new 
Abstract: Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression</title>
<link>https://arxiv.org/abs/2512.05081</link>
<guid>https://arxiv.org/abs/2512.05081</guid>
<content:encoded><![CDATA[
arXiv:2512.05081v1 Announce Type: new 
Abstract: Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2512.05091</link>
<guid>https://arxiv.org/abs/2512.05091</guid>
<content:encoded><![CDATA[
arXiv:2512.05091v1 Announce Type: new 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards</title>
<link>https://arxiv.org/abs/2512.05098</link>
<guid>https://arxiv.org/abs/2512.05098</guid>
<content:encoded><![CDATA[
arXiv:2512.05098v1 Announce Type: new 
Abstract: In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoIR: Towards All-in-One Image Restoration via Evolutionary Frequency Modulation</title>
<link>https://arxiv.org/abs/2512.05104</link>
<guid>https://arxiv.org/abs/2512.05104</guid>
<content:encoded><![CDATA[
arXiv:2512.05104v1 Announce Type: new 
Abstract: All-in-One Image Restoration (AiOIR) tasks often involve diverse degradation that require robust and versatile strategies. However, most existing approaches typically lack explicit frequency modeling and rely on fixed or heuristic optimization schedules, which limit the generalization across heterogeneous degradation. To address these limitations, we propose EvoIR, an AiOIR-specific framework that introduces evolutionary frequency modulation for dynamic and adaptive image restoration. Specifically, EvoIR employs the Frequency-Modulated Module (FMM) that decomposes features into high- and low-frequency branches in an explicit manner and adaptively modulates them to enhance both structural fidelity and fine-grained details. Central to EvoIR, an Evolutionary Optimization Strategy (EOS) iteratively adjusts frequency-aware objectives through a population-based evolutionary process, dynamically balancing structural accuracy and perceptual fidelity. Its evolutionary guidance further mitigates gradient conflicts across degradation and accelerates convergence. By synergizing FMM and EOS, EvoIR yields greater improvements than using either component alone, underscoring their complementary roles. Extensive experiments on multiple benchmarks demonstrate that EvoIR outperforms state-of-the-art AiOIR methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</title>
<link>https://arxiv.org/abs/2512.05106</link>
<guid>https://arxiv.org/abs/2512.05106</guid>
<content:encoded><![CDATA[
arXiv:2512.05106v1 Announce Type: new 
Abstract: Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion {\phi}-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. {\phi}-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, {\phi}-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, {\phi}-PD improves CARLA-to-Waymo planner performance by 50\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShadowDraw: From Any Object to Shadow-Drawing Compositional Art</title>
<link>https://arxiv.org/abs/2512.05110</link>
<guid>https://arxiv.org/abs/2512.05110</guid>
<content:encoded><![CDATA[
arXiv:2512.05110v1 Announce Type: new 
Abstract: We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</title>
<link>https://arxiv.org/abs/2512.05111</link>
<guid>https://arxiv.org/abs/2512.05111</guid>
<content:encoded><![CDATA[
arXiv:2512.05111v1 Announce Type: new 
Abstract: Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</title>
<link>https://arxiv.org/abs/2512.05112</link>
<guid>https://arxiv.org/abs/2512.05112</guid>
<content:encoded><![CDATA[
arXiv:2512.05112v1 Announce Type: new 
Abstract: Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</title>
<link>https://arxiv.org/abs/2512.05113</link>
<guid>https://arxiv.org/abs/2512.05113</guid>
<content:encoded><![CDATA[
arXiv:2512.05113v1 Announce Type: new 
Abstract: Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light-X: Generative 4D Video Rendering with Camera and Illumination Control</title>
<link>https://arxiv.org/abs/2512.05115</link>
<guid>https://arxiv.org/abs/2512.05115</guid>
<content:encoded><![CDATA[
arXiv:2512.05115v1 Announce Type: new 
Abstract: Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Centred Evaluation of Text-to-Image Generation Models for Self-expression of Mental Distress: A Dataset Based on GPT-4o</title>
<link>https://arxiv.org/abs/2512.04087</link>
<guid>https://arxiv.org/abs/2512.04087</guid>
<content:encoded><![CDATA[
arXiv:2512.04087v1 Announce Type: cross 
Abstract: Effective communication is central to achieving positive healthcare outcomes in mental health contexts, yet international students often face linguistic and cultural barriers that hinder their communication of mental distress. In this study, we evaluate the effectiveness of AI-generated images in supporting self-expression of mental distress. To achieve this, twenty Chinese international students studying at UK universities were invited to describe their personal experiences of mental distress. These descriptions were elaborated using GPT-4o with four persona-based prompt templates rooted in contemporary counselling practice to generate corresponding images. Participants then evaluated the helpfulness of generated images in facilitating the expression of their feelings based on their original descriptions. The resulting dataset comprises 100 textual descriptions of mental distress, 400 generated images, and corresponding human evaluation scores. Findings indicate that prompt design substantially affects perceived helpfulness, with the illustrator persona achieving the highest ratings. This work introduces the first publicly available text-to-image evaluation dataset with human judgment scores in the mental health domain, offering valuable resources for image evaluation, reinforcement learning with human feedback, and multi-modal research on mental health communication.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The changing surface of the world's roads</title>
<link>https://arxiv.org/abs/2512.04092</link>
<guid>https://arxiv.org/abs/2512.04092</guid>
<content:encoded><![CDATA[
arXiv:2512.04092v1 Announce Type: cross 
Abstract: Resilient road infrastructure is a cornerstone of the UN Sustainable Development Goals. Yet a primary indicator of network functionality and resilience is critically lacking: a comprehensive global baseline of road surface information. Here, we overcome this gap by applying a deep learning framework to a global mosaic of Planetscope satellite imagery from 2020 and 2024. The result is the first global multi-temporal dataset of road pavedness and width for 9.2 million km of critical arterial roads, achieving 95.5% coverage where nearly half the network was previously unclassified. This dataset reveals a powerful multi-scale geography of human development. At the planetary scale, we show that the rate of change in pavedness is a robust proxy for a country's development trajectory (correlation with HDI = 0.65). At the national scale, we quantify how unpaved roads constitute a fragile backbone for economic connectivity. We further synthesize our data into a global Humanitarian Passability Matrix with direct implications for humanitarian logistics. At the local scale, case studies demonstrate the framework's versatility: in Ghana, road quality disparities expose the spatial outcomes of governance; in Pakistan, the data identifies infrastructure vulnerabilities to inform climate resilience planning. Together, this work delivers both a foundational dataset and a multi-scale analytical framework for monitoring global infrastructure, from the dynamics of national development to the realities of local governance, climate adaptation, and equity. Unlike traditional proxies such as nighttime lights, which reflect economic activity, road surface data directly measures the physical infrastructure that underpins prosperity and resilience - at higher spatial resolution.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness</title>
<link>https://arxiv.org/abs/2512.04264</link>
<guid>https://arxiv.org/abs/2512.04264</guid>
<content:encoded><![CDATA[
arXiv:2512.04264v1 Announce Type: cross 
Abstract: Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we study the ML model robustness using ten different activation functions through adversarial training in centralized environments and explore the ML model robustness in federal learning environments. In the centralized environment, we first propose an advanced adversarial training approach to improving the ML model robustness by incorporating model architecture change, soft labeling, simplified data augmentation, and varying learning rates. Then, we conduct extensive experiments on ten well-known activation functions in addition to ReLU to better understand how they impact the ML model robustness. Furthermore, we extend the proposed adversarial training approach to the federal learning environment, where both independent and identically distributed (IID) and non-IID data settings are considered. Our proposed centralized adversarial training approach achieves a natural and robust accuracy of 77.08% and 67.96%, respectively on CIFAR-10 against the fast gradient sign attacks. Experiments on ten activation functions reveal ReLU usually performs best. In the federated learning environment, however, the robust accuracy decreases significantly, especially on non-IID data. To address the significant performance drop in the non-IID data case, we introduce data sharing and achieve the natural and robust accuracy of 70.09% and 54.79%, respectively, surpassing the CalFAT algorithm, when 40% data sharing is used. That is, a proper percentage of data sharing can significantly improve the ML model robustness, which is useful to some real-world applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting</title>
<link>https://arxiv.org/abs/2512.04385</link>
<guid>https://arxiv.org/abs/2512.04385</guid>
<content:encoded><![CDATA[
arXiv:2512.04385v1 Announce Type: cross 
Abstract: Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Engineering vs. Deep Learning for Automated Coin Grading: A Comparative Study on Saint-Gaudens Double Eagles</title>
<link>https://arxiv.org/abs/2512.04464</link>
<guid>https://arxiv.org/abs/2512.04464</guid>
<content:encoded><![CDATA[
arXiv:2512.04464v1 Announce Type: cross 
Abstract: We challenge the common belief that deep learning always trumps older techniques, using the example of grading Saint-Gaudens Double Eagle gold coins automatically. In our work, we put a feature-based Artificial Neural Network built around 192 custom features pulled from Sobel edge detection and HSV color analysis up against a hybrid Convolutional Neural Network that blends in EfficientNetV2, plus a straightforward Support Vector Machine as the control. Testing 1,785 coins graded by experts, the ANN nailed 86% exact matches and hit 98% when allowing a 3-grade leeway. On the flip side, CNN and SVM mostly just guessed the most common grade, scraping by with 31% and 30% exact hits. Sure, the CNN looked good on broader tolerance metrics, but that is because of some averaging trick in regression that hides how it totally flops at picking out specific grades. All told, when you are stuck with under 2,000 examples and lopsided classes, baking in real coin-expert knowledge through feature design beats out those inscrutable, all-in-one deep learning setups. This rings true for other niche quality checks where data's thin and know-how matters more than raw compute.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Spatially-Variant Convolution via Differentiable Sparse Kernel Complex</title>
<link>https://arxiv.org/abs/2512.04556</link>
<guid>https://arxiv.org/abs/2512.04556</guid>
<content:encoded><![CDATA[
arXiv:2512.04556v1 Announce Type: cross 
Abstract: Image convolution with complex kernels is a fundamental operation in photography, scientific imaging, and animation effects, yet direct dense convolution is computationally prohibitive on resource-limited devices. Existing approximations, such as simulated annealing or low-rank decompositions, either lack efficiency or fail to capture non-convex kernels. We introduce a differentiable kernel decomposition framework that represents a target spatially-variant, dense, complex kernel using a set of sparse kernel samples. Our approach features (i) a decomposition that enables differentiable optimization of sparse kernels, (ii) a dedicated initialization strategy for non-convex shapes to avoid poor local minima, and (iii) a kernel-space interpolation scheme that extends single-kernel filtering to spatially varying filtering without retraining and additional runtime overhead. Experiments on Gaussian and non-convex kernels show that our method achieves higher fidelity than simulated annealing and significantly lower cost than low-rank decompositions. Our approach provides a practical solution for mobile imaging and real-time rendering, while remaining fully differentiable for integration into broader learning pipelines.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective</title>
<link>https://arxiv.org/abs/2512.04625</link>
<guid>https://arxiv.org/abs/2512.04625</guid>
<content:encoded><![CDATA[
arXiv:2512.04625v1 Announce Type: cross 
Abstract: In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware-aware Neural Architecture Search of Early Exiting Networks on Edge Accelerators</title>
<link>https://arxiv.org/abs/2512.04705</link>
<guid>https://arxiv.org/abs/2512.04705</guid>
<content:encoded><![CDATA[
arXiv:2512.04705v1 Announce Type: cross 
Abstract: Advancements in high-performance computing and cloud technologies have enabled the development of increasingly sophisticated Deep Learning (DL) models. However, the growing demand for embedded intelligence at the edge imposes stringent computational and energy constraints, challenging the deployment of these large-scale models. Early Exiting Neural Networks (EENN) have emerged as a promising solution, allowing dynamic termination of inference based on input complexity to enhance efficiency. Despite their potential, EENN performance is highly influenced by the heterogeneity of edge accelerators and the constraints imposed by quantization, affecting accuracy, energy efficiency, and latency. Yet, research on the automatic optimization of EENN design for edge hardware remains limited. To bridge this gap, we propose a hardware-aware Neural Architecture Search (NAS) framework that systematically integrates the effects of quantization and hardware resource allocation to optimize the placement of early exit points within a network backbone. Experimental results on the CIFAR-10 dataset demonstrate that our NAS framework can discover architectures that achieve over a 50\% reduction in computational costs compared to conventional static networks, making them more suitable for deployment in resource-constrained edge environments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemLoRA: Distilling Expert Adapters for On-Device Memory Systems</title>
<link>https://arxiv.org/abs/2512.04763</link>
<guid>https://arxiv.org/abs/2512.04763</guid>
<content:encoded><![CDATA[
arXiv:2512.04763v1 Announce Type: cross 
Abstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared Multi-modal Embedding Space for Face-Voice Association</title>
<link>https://arxiv.org/abs/2512.04814</link>
<guid>https://arxiv.org/abs/2512.04814</guid>
<content:encoded><![CDATA[
arXiv:2512.04814v1 Announce Type: cross 
Abstract: The FAME 2026 challenge comprises two demanding tasks: training face-voice associations combined with a multilingual setting that includes testing on languages on which the model was not trained. Our approach consists of separate uni-modal processing pipelines with general face and voice feature extraction, complemented by additional age-gender feature extraction to support prediction. The resulting single-modal features are projected into a shared embedding space and trained with an Adaptive Angular Margin (AAM) loss. Our approach achieved first place in the FAME 2026 challenge, with an average Equal-Error Rate (EER) of 23.99%.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Generated Human Videos to Physically Plausible Robot Trajectories</title>
<link>https://arxiv.org/abs/2512.05094</link>
<guid>https://arxiv.org/abs/2512.05094</guid>
<content:encoded><![CDATA[
arXiv:2512.05094v1 Announce Type: cross 
Abstract: Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TV2TV: A Unified Framework for Interleaved Language and Video Generation</title>
<link>https://arxiv.org/abs/2512.05103</link>
<guid>https://arxiv.org/abs/2512.05103</guid>
<content:encoded><![CDATA[
arXiv:2512.05103v1 Announce Type: cross 
Abstract: Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep infant brain segmentation from multi-contrast MRI</title>
<link>https://arxiv.org/abs/2512.05114</link>
<guid>https://arxiv.org/abs/2512.05114</guid>
<content:encoded><![CDATA[
arXiv:2512.05114v1 Announce Type: cross 
Abstract: Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value Gradient Guidance for Flow Matching Alignment</title>
<link>https://arxiv.org/abs/2512.05116</link>
<guid>https://arxiv.org/abs/2512.05116</guid>
<content:encoded><![CDATA[
arXiv:2512.05116v1 Announce Type: cross 
Abstract: While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Universal Weight Subspace Hypothesis</title>
<link>https://arxiv.org/abs/2512.05117</link>
<guid>https://arxiv.org/abs/2512.05117</guid>
<content:encoded><![CDATA[
arXiv:2512.05117v1 Announce Type: cross 
Abstract: We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient stereo matching on embedded GPUs with zero-means cross correlation</title>
<link>https://arxiv.org/abs/2212.00476</link>
<guid>https://arxiv.org/abs/2212.00476</guid>
<content:encoded><![CDATA[
arXiv:2212.00476v2 Announce Type: replace 
Abstract: Mobile stereo-matching systems have become an important part of many applications, such as automated-driving vehicles and autonomous robots. Accurate stereo-matching methods usually lead to high computational complexity; however, mobile platforms have only limited hardware resources to keep their power consumption low; this makes it difficult to maintain both an acceptable processing speed and accuracy on mobile platforms. To resolve this trade-off, we herein propose a novel acceleration approach for the well-known zero-means normalized cross correlation (ZNCC) matching cost calculation algorithm on a Jetson Tx2 embedded GPU. In our method for accelerating ZNCC, target images are scanned in a zigzag fashion to efficiently reuse one pixel's computation for its neighboring pixels; this reduces the amount of data transmission and increases the utilization of on-chip registers, thus increasing the processing speed. As a result, our method is 2X faster than the traditional image scanning method, and 26% faster than the latest NCC method. By combining this technique with the domain transformation (DT) algorithm, our system show real-time processing speed of 32 fps, on a Jetson Tx2 GPU for 1,280x384 pixel images with a maximum disparity of 128. Additionally, the evaluation results on the KITTI 2015 benchmark show that our combined system is more accurate than the same algorithm combined with census by 7.26%, while maintaining almost the same processing speed. Source Code: https://github.com/changqiong/Z2ZNCC.git
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polygon Intersection-over-Union Loss for Viewpoint-Agnostic Monocular 3D Vehicle Detection</title>
<link>https://arxiv.org/abs/2309.07104</link>
<guid>https://arxiv.org/abs/2309.07104</guid>
<content:encoded><![CDATA[
arXiv:2309.07104v2 Announce Type: replace 
Abstract: Monocular 3D object detection is a challenging task because depth information is difficult to obtain from 2D images. A subset of viewpoint-agnostic monocular 3D detection methods also do not explicitly leverage scene homography or geometry during training, meaning that a model trained thusly can detect objects in images from arbitrary viewpoints. Such works predict the projections of the 3D bounding boxes on the image plane to estimate the location of the 3D boxes, but these projections are not rectangular so the calculation of IoU between these projected polygons is not straightforward. This work proposes an efficient, fully differentiable algorithm for the calculation of IoU between two convex polygons, which can be utilized to compute the IoU between two 3D bounding box footprints viewed from an arbitrary angle. We test the performance of the proposed polygon IoU loss (PIoU loss) on three state-of-the-art viewpoint-agnostic 3D detection models. Experiments demonstrate that the proposed PIoU loss converges faster than L1 loss and that in 3D detection models, a combination of PIoU loss and L1 loss gives better results than L1 loss alone (+1.64% AP70 for MonoCon on cars, +0.18% AP70 for RTM3D on cars, and +0.83%/+2.46% AP50/AP25 for MonoRCNN on cyclists).
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surface-Based Visibility-Guided Uncertainty for Continuous Active 3D Neural Reconstruction</title>
<link>https://arxiv.org/abs/2405.02568</link>
<guid>https://arxiv.org/abs/2405.02568</guid>
<content:encoded><![CDATA[
arXiv:2405.02568v3 Announce Type: replace 
Abstract: View selection is critical in active 3D neural reconstruction as it impacts the contents of training set and resulting final output quality. Recent view selection strategies emphasize the visibility when evaluating model uncertainty in active 3D reconstruction. However, existing approaches estimate visibility only after the model fully converges, which has confined their application primarily to non-continuous active learning settings. This paper proposes Surface-Based Visibility field (SBV) that successfully estimates the visibility-guided uncertainty in continuous active 3D neural reconstruction. During learning neural implicit surfaces, our model learns rendering uncertainties and infers surface confidence values derived from signed distance functions. It then updates surface confidences using a voxel grid, robustly deducing the surface-based visibility for uncertainties. This approach captures uncertainties across all regions, whether well-defined surfaces or ambiguous areas, ensuring accurate visibility measurement in continuous active learning. Experiments on benchmark datasets-Tanks and Temples, BlendedMVS, Blender, DTU-and the newly proposed imbalanced viewpoint dataset (ImBView) show that view selection based on SBV-guided uncertainty improves performance by up to 11.6% over existing methods, highlighting its effectiveness in challenging reconstruction scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many Relationships</title>
<link>https://arxiv.org/abs/2405.18770</link>
<guid>https://arxiv.org/abs/2405.18770</guid>
<content:encoded><![CDATA[
arXiv:2405.18770v5 Announce Type: replace 
Abstract: Pre-trained vision-language (VL) models are highly vulnerable to adversarial attacks. However, existing defense methods primarily focus on image classification, overlooking two key aspects of VL tasks: multimodal attacks, where both image and text can be perturbed, and the one-to-many relationship of images and texts, where a single image can correspond to multiple textual descriptions and vice versa (1:N and N:1). This work is the first to explore defense strategies against multimodal attacks in VL tasks, whereas prior VL defense methods focus on vision robustness. We propose multimodal adversarial training (MAT), which incorporates adversarial perturbations in both image and text modalities during training, significantly outperforming existing unimodal defenses. Furthermore, we discover that MAT is limited by deterministic one-to-one (1:1) image-text pairs in VL training data. To address this, we conduct a comprehensive study on leveraging one-to-many relationships to enhance robustness, investigating diverse augmentation techniques. Our analysis shows that, for a more effective defense, augmented image-text pairs should be well-aligned, diverse, yet avoid distribution shift -- conditions overlooked by prior research. This work pioneers defense strategies against multimodal attacks, providing insights for building robust VLMs from both optimization and data perspectives. Our code is publicly available at https://github.com/CyberAgentAI/multimodal-adversarial-training.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation</title>
<link>https://arxiv.org/abs/2408.16547</link>
<guid>https://arxiv.org/abs/2408.16547</guid>
<content:encoded><![CDATA[
arXiv:2408.16547v2 Announce Type: replace 
Abstract: Category-level articulated object pose estimation focuses on the pose estimation of unknown articulated objects within known categories. Despite its significance, this task remains challenging due to the varying shapes and poses of objects, expensive dataset annotation costs, and complex real-world environments. In this paper, we propose a novel self-supervised approach that leverages a single-frame point cloud to solve this task. Our model consistently generates reconstruction with a canonical pose and joint state for the entire input object, and it estimates object-level poses that reduce overall pose variance and part-level poses that align each part of the input with its corresponding part of the reconstruction. Experimental results demonstrate that our approach significantly outperforms previous self-supervised methods and is comparable to the state-of-the-art supervised methods. To assess the performance of our model in real-world scenarios, we also introduce a new real-world articulated object benchmark dataset.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Markup Document Models for Graphic Design Completion</title>
<link>https://arxiv.org/abs/2409.19051</link>
<guid>https://arxiv.org/abs/2409.19051</guid>
<content:encoded><![CDATA[
arXiv:2409.19051v2 Announce Type: replace 
Abstract: We introduce MarkupDM, a multimodal markup document model that represents graphic design as an interleaved multimodal document consisting of both markup language and images. Unlike existing holistic approaches that rely on an element-by-attribute grid representation, our representation accommodates variable-length elements, type-dependent attributes, and text content. Inspired by fill-in-the-middle training in code generation, we train the model to complete the missing part of a design document from its surrounding context, allowing it to treat various design tasks in a unified manner. Our model also supports image generation by predicting discrete image tokens through a specialized tokenizer with support for image transparency. We evaluate MarkupDM on three tasks, attribute value, image, and text completion, and demonstrate that it can produce plausible designs consistent with the given context. To further illustrate the flexibility of our approach, we evaluate our approach on a new instruction-guided design completion task where our instruction-tuned MarkupDM compares favorably to state-of-the-art image editing models, especially in textual completion. These findings suggest that multimodal language models with our document representation can serve as a versatile foundation for broad design automation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Geodesics of Geometric Shape Deformations From Images</title>
<link>https://arxiv.org/abs/2410.18797</link>
<guid>https://arxiv.org/abs/2410.18797</guid>
<content:encoded><![CDATA[
arXiv:2410.18797v2 Announce Type: replace 
Abstract: This paper presents a novel method, named geodesic deformable networks (GDN), that for the first time enables the learning of geodesic flows of deformation fields derived from images. In particular, the capability of our proposed GDN being able to predict geodesics is important for quantifying and comparing deformable shape presented in images. The geodesic deformations, also known as optimal transformations that align pairwise images, are often parameterized by a time sequence of smooth vector fields governed by nonlinear differential equations. A bountiful literature has been focusing on learning the initial conditions (e.g., initial velocity fields) based on registration networks. However, the definition of geodesics central to deformation-based shape analysis is blind to the networks. To address this problem, we carefully develop an efficient neural operator to treat the geodesics as unknown mapping functions learned from the latent deformation spaces. A composition of integral operators and smooth activation functions is then formulated to effectively approximate such mappings. In contrast to previous works, our GDN jointly optimizes a newly defined geodesic loss, which adds additional benefits to promote the network regularizability and generalizability. We demonstrate the effectiveness of GDN on both 2D synthetic data and 3D real brain magnetic resonance imaging (MRI).
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields</title>
<link>https://arxiv.org/abs/2412.13547</link>
<guid>https://arxiv.org/abs/2412.13547</guid>
<content:encoded><![CDATA[
arXiv:2412.13547v2 Announce Type: replace 
Abstract: Novel-view synthesis is an important problem in computer vision with applications in 3D reconstruction, mixed reality, and robotics. Recent methods like 3D Gaussian Splatting (3DGS) have become the preferred method for this task, providing high-quality novel views in real time. However, the training time of a 3DGS model is slow, often taking 30 minutes for a scene with 200 views. In contrast, our goal is to reduce the optimization time by training for fewer steps while maintaining high rendering quality. Specifically, we combine the guidance from both the position error and the appearance error to achieve a more effective densification. To balance the rate between adding new Gaussians and fitting old Gaussians, we develop a convergence-aware budget control mechanism. Moreover, to make the densification process more reliable, we selectively add new Gaussians from mostly visited regions. With these designs, we reduce the Gaussian optimization steps to one-third of the previous approach while achieving a comparable or even better novel view rendering quality. To further facilitate the rapid fitting of 4K resolution images, we introduce a dilation-based rendering technique. Our method, Turbo-GS, speeds up optimization for typical scenes and scales well to high-resolution (4K) scenarios on standard datasets. Through extensive experiments, we show that our method is significantly faster in optimization than other methods while retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoLUT: Efficient Volumetric streaming enhanced by LUT-based super-resolution</title>
<link>https://arxiv.org/abs/2502.12151</link>
<guid>https://arxiv.org/abs/2502.12151</guid>
<content:encoded><![CDATA[
arXiv:2502.12151v2 Announce Type: replace 
Abstract: 3D volumetric video provides immersive experience and is gaining traction in digital media. Despite its rising popularity, the streaming of volumetric video content poses significant challenges due to the high data bandwidth requirement. A natural approach to mitigate the bandwidth issue is to reduce the volumetric video's data rate by downsampling the content prior to transmission. The video can then be upsampled at the receiver's end using a super-resolution (SR) algorithm to reconstruct the high-resolution details. While super-resolution techniques have been extensively explored and advanced for 2D video content, there is limited work on SR algorithms tailored for volumetric videos.
  To address this gap and the growing need for efficient volumetric video streaming, we have developed VoLUT with a new SR algorithm specifically designed for volumetric content. Our algorithm uniquely harnesses the power of lookup tables (LUTs) to facilitate the efficient and accurate upscaling of low-resolution volumetric data. The use of LUTs enables our algorithm to quickly reference precomputed high-resolution values, thereby significantly reducing the computational complexity and time required for upscaling. We further apply adaptive video bit rate algorithm (ABR) to dynamically determine the downsampling rate according to the network condition and stream the selected video rate to the receiver. Compared to related work, VoLUT is the first to enable high-quality 3D SR on commodity mobile devices at line-rate. Our evaluation shows VoLUT can reduce bandwidth usage by 70% , boost QoE by 36.7% for volumetric video streaming and achieve
  3D SR speed-up with no quality compromise.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SYNTHIA: Novel Concept Design with Affordance Composition</title>
<link>https://arxiv.org/abs/2502.17793</link>
<guid>https://arxiv.org/abs/2502.17793</guid>
<content:encoded><![CDATA[
arXiv:2502.17793v4 Announce Type: replace 
Abstract: Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence--the integration of multiple affordances into a single coherent concept--remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1% and 14.7% for novelty and functional coherence in human evaluation, respectively.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sliding-Window Merging for Compacting Patch-Redundant Layers in LLMs</title>
<link>https://arxiv.org/abs/2502.19159</link>
<guid>https://arxiv.org/abs/2502.19159</guid>
<content:encoded><![CDATA[
arXiv:2502.19159v4 Announce Type: replace 
Abstract: Depth-wise pruning accelerates LLM inference in resource-constrained scenarios but suffers from performance degradation due to direct removal of entire Transformer layers. This paper reveals ``Patch-like'' redundancy across layers via correlation analysis of the outputs of different layers in reproducing kernel Hilbert space, demonstrating consecutive layers exhibit high functional similarity. Building on this observation, this paper proposes Sliding-Window Merging (SWM) - a dynamic compression method that selects consecutive layers from top to bottom using a pre-defined similarity threshold, and compacts patch-redundant layers through a parameter consolidation, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35% pruning on the Vicuna-7B model, our method achieved a 1.654% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at https://github.com/920927/SLM-a-sliding-layer-merging-method.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVE: Towards End-to-End Video Subtitle Extraction with Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.04058</link>
<guid>https://arxiv.org/abs/2503.04058</guid>
<content:encoded><![CDATA[
arXiv:2503.04058v2 Announce Type: replace 
Abstract: Video subtitles play a crucial role in short videos and movies, as they not only help models better understand video content but also support applications such as video translation and content retrieval. Existing video subtitle extraction methods typically rely on multi-stage frameworks, where errors accumulate across stages and temporal dependencies are underutilized due to frame-wise processing. Moreover, although some Large Vision-Language Models (LVLMs) possess strong OCR capabilities, predicting accurate timestamps for subtitle texts remains challenging. To this end, we propose an End-to-end Video subtitle Extraction framework based on LVLMs, named EVE, which can output subtitles and their timestamps simultaneously. Specifically, we introduce a dual-branch Spatiotemporal Subtitle-Salient (S\textsuperscript{3}) Module that serves as an adapter for LVLMs, capable of representing subtitle-related content and considering inter-frame correlations using only a small number of tokens. Within this module, the Spatial Semantic Context Aggregate branch aggregates high-level global semantics to provide spatial visual contextual information, while the Temporal Subtitle Token Query branch explicitly queries subtitle-relevant tokens while considering temporal correlation across frames. The small number of tokens retained by the S\textsuperscript{3} module are fed to the language model, which then directly outputs the subtitle text along with its timestamps. Furthermore, we construct the first large-scale dataset dedicated to video subtitle extraction, ViSa, containing over 2.5M videos with timestamped and bilingual annotation, thereby providing the community with a well-organized training and evaluation benchmark.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CMMCoT: Enhancing Complex Multi-Image Comprehension via Multi-Modal Chain-of-Thought and Memory Augmentation</title>
<link>https://arxiv.org/abs/2503.05255</link>
<guid>https://arxiv.org/abs/2503.05255</guid>
<content:encoded><![CDATA[
arXiv:2503.05255v2 Announce Type: replace 
Abstract: While previous multimodal slow-thinking methods have demonstrated remarkable success in single-image understanding scenarios, their effectiveness becomes fundamentally constrained when extended to more complex multi-image comprehension tasks. This limitation stems from their predominant reliance on text-based intermediate reasoning processes. While for human, when engaging in sophisticated multi-image analysis, they typically perform two complementary cognitive operations: (1) continuous cross-image visual comparison through region-of-interest matching, and (2) dynamic memorization of critical visual concepts throughout the reasoning chain. Motivated by these observations, we propose the Complex Multi-Modal Chain-of-Thought (CMMCoT) framework, a multi-step reasoning framework that mimics human-like "slow thinking" for multi-image understanding. Our approach incorporates two key innovations: (1) The construction of interleaved multimodal multi-step reasoning chains, which utilize critical visual region tokens, extracted from intermediate reasoning steps, as supervisory signals. This mechanism not only facilitates comprehensive cross-modal understanding but also enhances model interpretability. (2) The introduction of a test-time memory augmentation module that expands the model's reasoning capacity during inference while preserving parameter efficiency. Furthermore, to facilitate research in this direction, we have curated a novel multi-image slow-thinking dataset. Extensive experiments demonstrate the effectiveness of our model. Code is available at https://github.com/zhangguanghao523/CMMCoT.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAVE: Diagnostic benchmark for Audio Visual Evaluation</title>
<link>https://arxiv.org/abs/2503.09321</link>
<guid>https://arxiv.org/abs/2503.09321</guid>
<content:encoded><![CDATA[
arXiv:2503.09321v2 Announce Type: replace 
Abstract: Audio-visual understanding is a rapidly evolving field that seeks to integrate and interpret information from both auditory and visual modalities. Despite recent advances in multi-modal learning, existing benchmarks often suffer from strong visual bias -- when answers can be inferred from visual data alone -- and provide only aggregate scores that conflate multiple sources of error. This makes it difficult to determine whether models struggle with visual understanding, audio interpretation, or audio-visual alignment. In this work, we introduce DAVE: Diagnostic Audio Visual Evaluation, a novel benchmark dataset designed to systematically evaluate audio-visual models across controlled settings. DAVE alleviates existing limitations by (i) ensuring both modalities are necessary to answer correctly and (ii) decoupling evaluation into atomic subcategories. Our detailed analysis of state-of-the-art models reveals specific failure modes and provides targeted insights for improvement. By offering this standardized diagnostic framework, we aim to facilitate more robust development of audio-visual models.
  Dataset: https://huggingface.co/datasets/gorjanradevski/dave
  Code: https://github.com/gorjanradevski/dave
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining</title>
<link>https://arxiv.org/abs/2503.15470</link>
<guid>https://arxiv.org/abs/2503.15470</guid>
<content:encoded><![CDATA[
arXiv:2503.15470v2 Announce Type: replace 
Abstract: Egocentric video-language pretraining has significantly advanced video representation learning. Humans perceive and interact with a fully 3D world, developing spatial awareness that extends beyond text-based understanding. However, most previous works learn from 1D text or 2D visual cues, such as bounding boxes, which inherently lack 3D understanding. To bridge this gap, we introduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained through large-scale 3D-aware video pretraining and video-text contrastive learning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently learn 3D-awareness from pseudo depth maps generated by depth estimation models. To further facilitate 3D-aware video pretraining, we enrich the original brief captions with hand-object visual cues by organically combining several foundation models. Extensive experiments demonstrate EgoDTM's superior performance across diverse downstream tasks, highlighting its superior 3D-aware visual understanding. Code: https://github.com/xuboshen/EgoDTM.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion</title>
<link>https://arxiv.org/abs/2503.22622</link>
<guid>https://arxiv.org/abs/2503.22622</guid>
<content:encoded><![CDATA[
arXiv:2503.22622v4 Announce Type: replace 
Abstract: Multi-view or 4D video generation has emerged as a significant research topic. Nonetheless, recent approaches to 4D generation still struggle with fundamental limitations, as they primarily rely on harnessing multiple video diffusion models with additional training or compute-intensive training of a full 4D diffusion model with limited real-world 4D data and large computational costs. To address these challenges, here we propose the first training-free 4D video generation method that leverages the off-the-shelf video diffusion models to generate multi-view videos from a single input video. Our approach consists of two key steps: (1) By designating the edge frames in the spatio-temporal sampling grid as key frames, we first synthesize them using a video diffusion model, leveraging a depth-based warping technique for guidance. This approach ensures structural consistency across the generated frames, preserving spatial and temporal coherence. (2) We then interpolate the remaining frames using a video diffusion model, constructing a fully populated and temporally coherent sampling grid while preserving spatial and temporal consistency. Through this approach, we extend a single video into a multi-view video along novel camera trajectories while maintaining spatio-temporal consistency. Our method is training-free and fully utilizes an off-the-shelf video diffusion model, offering a practical and effective solution for multi-view video generation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoCoIns: Consistent Subject Generation via Contrastive Instantiated Concepts</title>
<link>https://arxiv.org/abs/2503.24387</link>
<guid>https://arxiv.org/abs/2503.24387</guid>
<content:encoded><![CDATA[
arXiv:2503.24387v2 Announce Type: replace 
Abstract: While text-to-image generative models can synthesize diverse and faithful content, subject variation across multiple generations limits their application to long-form content generation. Existing approaches require time-consuming fine-tuning, reference images for all subjects, or access to previously generated content. We introduce Contrastive Concept Instantiation (CoCoIns), a framework that effectively synthesizes consistent subjects across multiple independent generations. The framework consists of a generative model and a mapping network that transforms input latent codes into pseudo-words associated with specific concept instances. Users can generate consistent subjects by reusing the same latent codes. To construct such associations, we propose a contrastive learning approach that trains the network to distinguish between different combinations of prompts and latent codes. Extensive evaluations on human faces with a single subject show that CoCoIns performs comparably to existing methods while maintaining greater flexibility. We also demonstrate the potential for extending CoCoIns to multiple subjects and other object categories.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution Can Improve Robust Scene Graph Generation</title>
<link>https://arxiv.org/abs/2504.12606</link>
<guid>https://arxiv.org/abs/2504.12606</guid>
<content:encoded><![CDATA[
arXiv:2504.12606v2 Announce Type: replace 
Abstract: In this paper, we propose Robo-SGG, a plug-and-play module for robust scene graph generation (SGG). Unlike standard SGG, the robust scene graph generation aims to perform inference on a diverse range of corrupted images, with the core challenge being the domain shift between the clean and corrupted images. Existing SGG methods suffer from degraded performance due to shifted visual features (e.g., corruption interference or occlusions). To obtain robust visual features, we leverage layout information, representing the global structure of an image, which is robust to domain shift, to enhance the robustness of SGG methods under corruption. Specifically, we employ Instance Normalization (IN) to alleviate the domain-specific variations and recover the robust structural features (i.e., the positional and semantic relationships among objects) by the proposed Layout-Oriented Restitution. Furthermore, under corrupted images, we introduce a Layout-Embedded Encoder (LEE) that adaptively fuses layout and visual features via a gating mechanism, enhancing the robustness of positional and semantic representations for objects and predicates. Note that our proposed Robo-SGG module is designed as a plug-and-play component, which can be easily integrated into any baseline SGG model. Extensive experiments demonstrate that by integrating the state-of-the-art method into our proposed Robo-SGG, we achieve relative improvements of 6.3%, 11.1%, and 8.0% in mR@50 for PredCls, SGCls, and SGDet tasks on the VG-C benchmark, respectively, and achieve new state-of-the-art performance in the corruption scene graph generation benchmark (VG-C and GQA-C). We will release our source code and model.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TongUI: Internet-Scale Trajectories from Multimodal Web Tutorials for Generalized GUI Agents</title>
<link>https://arxiv.org/abs/2504.12679</link>
<guid>https://arxiv.org/abs/2504.12679</guid>
<content:encoded><![CDATA[
arXiv:2504.12679v4 Announce Type: replace 
Abstract: Building Graphical User Interface (GUI) agents is a promising research direction, which simulates human interaction with computers or mobile phones to perform diverse GUI tasks. However, a major challenge in developing generalized GUI agents is the lack of sufficient trajectory data across various operating systems and applications, mainly due to the high cost of manual annotations. In this paper, we propose the TongUI framework that builds generalized GUI agents by learning from rich multimodal web tutorials. Concretely, we crawl and process online GUI tutorials (such as videos and articles) into GUI agent trajectory data, through which we produce the GUI-Net dataset containing 143K trajectory data across five operating systems and more than 200 applications. We develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net, which show remarkable performance improvements on commonly used grounding and navigation benchmarks, outperforming baseline agents about 10\% on multiple benchmarks, showing the effectiveness of the GUI-Net dataset and underscoring the significance of our TongUI framework. We will fully open-source the code, the GUI-Net dataset, and the trained models soon.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments</title>
<link>https://arxiv.org/abs/2505.04488</link>
<guid>https://arxiv.org/abs/2505.04488</guid>
<content:encoded><![CDATA[
arXiv:2505.04488v2 Announce Type: replace 
Abstract: The visually impaired population faces significant challenges in daily activities. While prior works employ vision language models for assistance, most focus on static content and cannot address real-time perception needs in complex environments. Recent VideoLLMs enable real-time vision and speech interaction, offering promising potential for assistive tasks. In this work, we conduct the first study evaluating their effectiveness in supporting daily life for visually impaired individuals. We first conducted a user survey with visually impaired participants to design the benchmark VisAssistDaily for daily life evaluation. Using VisAssistDaily, we evaluate popular VideoLLMs and find GPT-4o achieves the highest task success rate. We further conduct a user study to reveal concerns about hazard perception. To address this, we propose SafeVid, an environment-awareness dataset, and fine-tune VITA-1.5, improving risk recognition accuracy from 25.00% to 76.00%.We hope this work provides valuable insights and inspiration for future research in this field.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs</title>
<link>https://arxiv.org/abs/2505.15436</link>
<guid>https://arxiv.org/abs/2505.15436</guid>
<content:encoded><![CDATA[
arXiv:2505.15436v2 Announce Type: replace 
Abstract: Vision language models (VLMs) have achieved impressive performance across a variety of computer vision tasks. However, the multimodal reasoning capability has not been fully explored in existing models. In this paper, we propose a Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and zooming in on key image regions based on obtained visual cues and the given questions, achieving efficient multimodal reasoning. To enable this CoF capability, we present a two-stage training pipeline, including supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct the MM-CoF dataset, comprising 3K samples derived from a visual agent designed to adaptively identify key regions to solve visual tasks with different image resolutions and questions. We use MM-CoF to fine-tune the Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome accuracies and formats as rewards to update the Qwen2.5-VL model, enabling further refining the search and reasoning strategy of models without human priors. Our model achieves significant improvements on multiple benchmarks. On the V* benchmark that requires strong visual reasoning capability, our model outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to 4K, demonstrating the effectiveness of the proposed CoF method and facilitating the more efficient deployment of VLMs in practical applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Average Calibration Losses for Reliable Uncertainty in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2506.03942</link>
<guid>https://arxiv.org/abs/2506.03942</guid>
<content:encoded><![CDATA[
arXiv:2506.03942v3 Announce Type: replace 
Abstract: Deep neural networks for medical image segmentation are often overconfident, compromising both reliability and clinical utility. In this work, we propose differentiable formulations of marginal L1 Average Calibration Error (mL1-ACE) as an auxiliary loss that can be computed on a per-image basis. We compare both hard- and soft-binning approaches to directly improve pixel-wise calibration. Our experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that incorporating mL1-ACE significantly reduces calibration errors, particularly Average Calibration Error (ACE) and Maximum Calibration Error (MCE), while largely maintaining high Dice Similarity Coefficients (DSCs). We find that the soft-binned variant yields the greatest improvements in calibration, over the Dice plus cross-entropy loss baseline, but often compromises segmentation performance, with hard-binned mL1-ACE maintaining segmentation performance, albeit with weaker calibration improvement. To gain further insight into calibration performance and its variability across an imaging dataset, we introduce dataset reliability histograms, an aggregation of per-image reliability diagrams. The resulting analysis highlights improved alignment between predicted confidences and true accuracies. Overall, our approach not only enhances the trustworthiness of segmentation predictions but also shows potential for safer integration of deep learning methods into clinical workflows. We share our code here: https://github.com/cai4cai/Average-Calibration-Losses
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Focus Temporal Shifting for Precise Event Spotting in Sports Videos</title>
<link>https://arxiv.org/abs/2507.07381</link>
<guid>https://arxiv.org/abs/2507.07381</guid>
<content:encoded><![CDATA[
arXiv:2507.07381v3 Announce Type: replace 
Abstract: Precise Event Spotting (PES) in sports videos requires frame-level recognition of fine-grained actions from single-camera footage. Existing PES models typically incorporate lightweight temporal modules such as the Gate Shift Module (GSM) or the Gate Shift Fuse to enrich 2D CNN feature extractors with temporal context. However, these modules are limited in both temporal receptive field and spatial adaptability. We propose Multi-Focus Temporal Shifting Module (MFS) that enhances GSM with multi-scale temporal shifts and Group Focus Module, enabling efficient modeling of both short and long-term dependencies while focusing on salient regions. MFS is a lightweight, plug-and-play module that integrates seamlessly with diverse 2D backbones. To further advance the field, we introduce the Table Tennis Australia dataset, the first PES benchmark for table tennis containing over 4,800 precisely annotated events. Extensive experiments across five PES benchmarks demonstrate that MFS consistently improves performance with minimal overhead, achieving leading results among lightweight methods (+4.09 mAP, 45 GFLOPs).
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-branch Prompting for Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2507.17588</link>
<guid>https://arxiv.org/abs/2507.17588</guid>
<content:encoded><![CDATA[
arXiv:2507.17588v2 Announce Type: replace 
Abstract: Multimodal Machine Translation (MMT) typically enhances text-only translation by incorporating aligned visual features. Despite the remarkable progress, state-of-the-art MMT approaches often rely on paired image-text inputs at inference and are sensitive to irrelevant visual noise, which limits their robustness and practical applicability. To address these issues, we propose D2P-MMT, a diffusion-based dual-branch prompting framework for robust vision-guided translation. Specifically, D2P-MMT requires only the source text and a reconstructed image generated by a pre-trained diffusion model, which naturally filters out distracting visual details while preserving semantic cues. During training, the model jointly learns from both authentic and reconstructed images using a dual-branch prompting strategy, encouraging rich cross-modal interactions. To bridge the modality gap and mitigate training-inference discrepancies, we introduce a distributional alignment loss that enforces consistency between the output distributions of the two branches. Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves superior translation performance compared to existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Face Experts Fusion in Video Generation: Boosting Identity Consistency Across Large Face Poses</title>
<link>https://arxiv.org/abs/2508.09476</link>
<guid>https://arxiv.org/abs/2508.09476</guid>
<content:encoded><![CDATA[
arXiv:2508.09476v3 Announce Type: replace 
Abstract: Current video generation models struggle with identity preservation under large face poses, primarily facing two challenges: the difficulty in exploring an effective mechanism to integrate identity features into DiT architectures, and the lack of targeted coverage of large face poses in existing open-source video datasets. To address these, we present two key innovations. First, we propose Collaborative Face Experts Fusion (CoFE), which dynamically fuses complementary signals from three specialized experts within the DiT backbone: an identity expert that captures cross-pose invariant features, a semantic expert that encodes high-level visual context, and a detail expert that preserves pixel-level attributes such as skin texture and color gradients. Second, we introduce a data curation pipeline comprising three key components: Face Constraints to ensure diverse large-pose coverage, Identity Consistency to maintain stable identity across frames, and Speech Disambiguation to align textual captions with actual speaking behavior. This pipeline yields LaFID-180K, a large-scale dataset of pose-annotated video clips designed for identity-preserving video generation. Experimental results on several benchmarks demonstrate that our approach significantly outperforms state-of-the-art methods in face similarity, FID, and CLIP semantic alignment. \href{https://rain152.github.io/CoFE/}{Project page}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization</title>
<link>https://arxiv.org/abs/2508.09560</link>
<guid>https://arxiv.org/abs/2508.09560</guid>
<content:encoded><![CDATA[
arXiv:2508.09560v3 Announce Type: replace 
Abstract: Visual geo-localization for drones faces critical degradation under weather perturbations, \eg, rain and fog, where existing methods struggle with two inherent limitations: 1) Heavy reliance on limited weather categories that constrain generalization, and 2) Suboptimal disentanglement of entangled scene-weather features through pseudo weather categories. We present WeatherPrompt, a multi-modality learning paradigm that establishes weather-invariant representations through fusing the image embedding with the text context. Our framework introduces two key contributions: First, a Training-free Weather Reasoning mechanism that employs off-the-shelf large multi-modality models to synthesize multi-weather textual descriptions through human-like reasoning. It improves the scalability to unseen or complex weather, and could reflect different weather strength. Second, to better disentangle the scene and weather feature, we propose a multi-modality framework with the dynamic gating mechanism driven by the text embedding to adaptively reweight and fuse visual features across modalities. The framework is further optimized by the cross-modal objectives, including image-text contrastive learning and image-text matching, which maps the same scene with different weather conditions closer in the respresentation space. Extensive experiments validate that, under diverse weather conditions, our method achieves competitive recall rates compared to state-of-the-art drone geo-localization methods. Notably, it improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog and snow conditions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Downscaling climate projections to 1 km with single-image super resolution</title>
<link>https://arxiv.org/abs/2509.21399</link>
<guid>https://arxiv.org/abs/2509.21399</guid>
<content:encoded><![CDATA[
arXiv:2509.21399v2 Announce Type: replace 
Abstract: High-resolution climate projections are essential for local decision-making. However, available climate projections have low spatial resolution (e.g. 12.5 km), which limits their usability. We address this limitation by leveraging single-image super-resolution models to statistically downscale climate projections to 1-km resolution. Since high-resolution climate projections are unavailable, we train models on a high-resolution observational gridded data set and apply them to low-resolution climate projections. We cannot evaluate downscaled climate projections with common metrics (e.g. pixel-wise root-mean-square error) because we lack ground-truth high-resolution climate projections. Therefore, we evaluate climate indicators computed at weather station locations. Experiments on daily mean temperature demonstrate that single-image super-resolution models can downscale climate projections without increasing the error of climate indicators compared to low-resolution climate projections.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MORPH: PDE Foundation Models with Arbitrary Data Modality</title>
<link>https://arxiv.org/abs/2509.21670</link>
<guid>https://arxiv.org/abs/2509.21670</guid>
<content:encoded><![CDATA[
arXiv:2509.21670v3 Announce Type: replace 
Abstract: We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning</title>
<link>https://arxiv.org/abs/2509.22761</link>
<guid>https://arxiv.org/abs/2509.22761</guid>
<content:encoded><![CDATA[
arXiv:2509.22761v2 Announce Type: replace 
Abstract: Reasoning-augmented machine learning systems have shown improved performance in various domains, including image generation. However, existing reasoning-based methods for image generation either restrict reasoning to a single modality (image or text) or rely on high-quality reasoning data for fine-tuning. To tackle these limitations, we propose MILR, a test-time method that jointly reasons over image and text in a unified latent vector space. Reasoning in MILR is performed by searching through vector representations of discrete image and text tokens. Practically, this is implemented via the policy gradient method, guided by an image quality critic. We instantiate MILR within the unified multimodal understanding and generation (MUG) framework that natively supports language reasoning before image synthesis and thus facilitates cross-modal reasoning. The intermediate model outputs, which are to be optimized, serve as the unified latent space, enabling MILR to operate entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE, achieving state-of-the-art results on all benchmarks. Notably, on knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over the baseline by 80%. Our further analysis indicates that joint reasoning in the unified latent space is the key to its strong performance. Moreover, our qualitative studies reveal MILR's non-trivial ability in temporal and cultural reasoning, highlighting the efficacy of our reasoning method.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition</title>
<link>https://arxiv.org/abs/2509.25723</link>
<guid>https://arxiv.org/abs/2509.25723</guid>
<content:encoded><![CDATA[
arXiv:2509.25723v2 Announce Type: replace 
Abstract: Visual Place Recognition (VPR) requires robust retrieval of geotagged images despite large appearance, viewpoint, and environmental variation. Prior methods focus on descriptor fine-tuning or fixed sampling strategies yet neglect the dynamic interplay between spatial context and visual similarity during training. We present SAGE (Spatial-visual Adaptive Graph Exploration), a unified training pipeline that enhances granular spatial-visual discrimination by jointly improving local feature aggregation, organize samples during training, and hard sample mining. We introduce a lightweight Soft Probing module that learns residual weights from training data for patch descriptors before bilinear aggregation, boosting distinctive local cues. During training we reconstruct an online geo-visual graph that fuses geographic proximity and current visual similarity so that candidate neighborhoods reflect the evolving embedding landscape. To concentrate learning on the most informative place neighborhoods, we seed clusters from high-affinity anchors and iteratively expand them with a greedy weighted clique expansion sampler. Implemented with a frozen DINOv2 backbone and parameter-efficient fine-tuning, SAGE achieves SOTA across eight benchmarks. It attains 98.9%, 95.8%, 94.5%, and 96.0% Recall@1 on SPED, Pitts30k-test, MSLS-val, and Nordland, respectively. Notably, our method obtains 100% Recall@10 on SPED only using 4096D global descriptors. Code and models will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VRWKV-Editor: Reducing quadratic complexity in transformer-based video editing</title>
<link>https://arxiv.org/abs/2509.25998</link>
<guid>https://arxiv.org/abs/2509.25998</guid>
<content:encoded><![CDATA[
arXiv:2509.25998v3 Announce Type: replace 
Abstract: In light of recent progress in video editing, deep learning models focusing on both spatial and temporal dependencies have emerged as the primary method. However, these models suffer from the quadratic computational complexity of traditional attention mechanisms, making them difficult to adapt to long-duration and high-resolution videos. This limitation restricts their applicability in practical contexts such as real-time video processing. To tackle this challenge, we introduce a method to reduce both time and space complexity of these systems by proposing VRWKV-Editor, a novel video editing model that integrates a linear spatio-temporal aggregation module into video-based diffusion models. VRWKV-Editor leverages bidirectional weighted key-value recurrence mechanism of the RWKV transformer to capture global dependencies while preserving temporal coherence, achieving linear complexity without sacrificing quality. Extensive experiments demonstrate that the proposed method achieves up to 3.7x speedup and 60% lower memory usage compared to state-of-the-art diffusion-based video editing methods, while maintaining competitive performance in frame consistency and text alignment. Furthermore, a comparative analysis we conducted on videos with different sequence lengths confirms that the gap in editing speed between our approach and architectures with self-attention becomes more significant with long videos.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTRV: Test-Time Reinforcement Learning for Vision Language Models</title>
<link>https://arxiv.org/abs/2510.06783</link>
<guid>https://arxiv.org/abs/2510.06783</guid>
<content:encoded><![CDATA[
arXiv:2510.06783v2 Announce Type: replace 
Abstract: Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets. Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions</title>
<link>https://arxiv.org/abs/2510.07828</link>
<guid>https://arxiv.org/abs/2510.07828</guid>
<content:encoded><![CDATA[
arXiv:2510.07828v3 Announce Type: replace 
Abstract: Real-world scenes often feature multiple humans interacting with multiple objects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D human-object interaction (HOI) benchmarks consider only a fraction of these complex interactions. To close this gap, we present MMHOI -- a large-scale, Multi-human Multi-object Interaction dataset consisting of images from 12 everyday scenarios. MMHOI offers complete 3D shape and pose annotations for every person and object, along with labels for 78 action categories and 14 interaction-specific body parts, providing a comprehensive testbed for next-generation HOI research. Building on MMHOI, we present MMHOI-Net, an end-to-end transformer-based neural network for jointly estimating human-object 3D geometries, their interactions, and associated actions. A key innovation in our framework is a structured dual-patch representation for modeling objects and their interactions, combined with action recognition to enhance the interaction prediction. Experiments on MMHOI and the recently proposed CORE4D datasets demonstrate that our approach achieves state-of-the-art performance in multi-HOI modeling, excelling in both accuracy and reconstruction quality. The MMHOI dataset is publicly available at https://zenodo.org/records/17711786.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training</title>
<link>https://arxiv.org/abs/2510.12586</link>
<guid>https://arxiv.org/abs/2510.12586</guid>
<content:encoded><![CDATA[
arXiv:2510.12586v2 Announce Type: replace 
Abstract: Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our framework achieves state-of-the-art (SOTA) performance on ImageNet. Specifically, our diffusion model reaches an FID of 1.58 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE) surpassing prior pixel-space methods and VAE-based counterparts by a large margin in both generation quality and training efficiency. In a direct comparison, our model significantly outperforms DiT while using only around 30\% of its training compute.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning</title>
<link>https://arxiv.org/abs/2511.08003</link>
<guid>https://arxiv.org/abs/2511.08003</guid>
<content:encoded><![CDATA[
arXiv:2511.08003v2 Announce Type: replace 
Abstract: Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOP-ASK: Object-Interaction Reasoning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.16857</link>
<guid>https://arxiv.org/abs/2511.16857</guid>
<content:encoded><![CDATA[
arXiv:2511.16857v2 Announce Type: replace 
Abstract: Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A lightweight detector for real-time detection of remote sensing images</title>
<link>https://arxiv.org/abs/2511.17147</link>
<guid>https://arxiv.org/abs/2511.17147</guid>
<content:encoded><![CDATA[
arXiv:2511.17147v2 Announce Type: replace 
Abstract: Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DE-KAN: A Kolmogorov Arnold Network with Dual Encoder for accurate 2D Teeth Segmentation</title>
<link>https://arxiv.org/abs/2511.18533</link>
<guid>https://arxiv.org/abs/2511.18533</guid>
<content:encoded><![CDATA[
arXiv:2511.18533v2 Announce Type: replace 
Abstract: Accurate segmentation of individual teeth from panoramic radiographs remains a challenging task due to anatomical variations, irregular tooth shapes, and overlapping structures. These complexities often limit the performance of conventional deep learning models. To address this, we propose DE-KAN, a novel Dual Encoder Kolmogorov Arnold Network, which enhances feature representation and segmentation precision. The framework employs a ResNet-18 encoder for augmented inputs and a customized CNN encoder for original inputs, enabling the complementary extraction of global and local spatial features. These features are fused through KAN-based bottleneck layers, incorporating nonlinear learnable activation functions derived from the Kolmogorov Arnold representation theorem to improve learning capacity and interpretability. Extensive experiments on two benchmark dental X-ray datasets demonstrate that DE-KAN outperforms state-of-the-art segmentation models, achieving mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36%, representing up to +4.7% improvement in Dice compared to existing methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents</title>
<link>https://arxiv.org/abs/2511.18685</link>
<guid>https://arxiv.org/abs/2511.18685</guid>
<content:encoded><![CDATA[
arXiv:2511.18685v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents. Project page: \href{https://cfg-bench.github.io/}{https://cfg-bench.github.io/}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Changes in Gaza: DINOv3-Powered Multi-Class Change Detection for Damage Assessment in Conflict Zones</title>
<link>https://arxiv.org/abs/2511.19035</link>
<guid>https://arxiv.org/abs/2511.19035</guid>
<content:encoded><![CDATA[
arXiv:2511.19035v2 Announce Type: replace 
Abstract: Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. The multi-scale cross-attention mechanism allows for precise localization of subtle semantic changes, while the difference siamese structure enhances inter-class feature discrimination, enabling fine-grained semantic change detection. Furthermore, a simple yet powerful lightweight decoder is designed to generate clear detection maps while maintaining high efficiency. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. We evaluated our method on the Gaza-Change and two classical datasets: the SECOND and Landsat-SCD datasets. Experimental results demonstrate that our proposed approach effectively addresses the MCD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling</title>
<link>https://arxiv.org/abs/2511.20785</link>
<guid>https://arxiv.org/abs/2511.20785</guid>
<content:encoded><![CDATA[
arXiv:2511.20785v2 Announce Type: replace 
Abstract: Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables "Thinking with Long Videos" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?</title>
<link>https://arxiv.org/abs/2511.21523</link>
<guid>https://arxiv.org/abs/2511.21523</guid>
<content:encoded><![CDATA[
arXiv:2511.21523v2 Announce Type: replace 
Abstract: Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs. All codes and pretrained models are available at https://github.com/pierreadorni/EoS-FM.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImageNot: A contrast with ImageNet preserves model rankings</title>
<link>https://arxiv.org/abs/2404.02112</link>
<guid>https://arxiv.org/abs/2404.02112</guid>
<content:encoded><![CDATA[
arXiv:2404.02112v2 Announce Type: replace-cross 
Abstract: We introduce ImageNot, a dataset constructed explicitly to be drastically different than ImageNet while matching its scale. ImageNot is designed to test the external validity of deep learning progress on ImageNet. We show that key model architectures developed for ImageNet over the years rank identically to how they rank on ImageNet when trained from scratch and evaluated on ImageNot. Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets. Our work demonstrates a surprising degree of external validity in the relative performance of image classification models when trained and evaluated on an entirely different dataset. This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2407.11698</link>
<guid>https://arxiv.org/abs/2407.11698</guid>
<content:encoded><![CDATA[
arXiv:2407.11698v3 Announce Type: replace-cross 
Abstract: Quantization is a pivotal technique for managing the growing computational and memory demands of Deep Neural Networks (DNNs). By reducing the number of bits used to represent weights and activations (typically from 32-bit Floating-Point (FP) to 16-bit or 8-bit integers), quantization reduces memory footprint, energy consumption, and execution time of DNNs. However, most existing methods typically target DNN inference, while training still relies on FP operations, limiting applicability in environments where FP arithmetic is unavailable. To date, only one prior work has addressed integer-only training, and only for Multi-Layer Perceptron (MLP) architectures. This paper introduces NITRO-D, a novel framework for training deep integer-only Convolutional Neural Networks (CNNs) that operate entirely in the integer domain for both training and inference. NITRO-D enables training of integer CNNs without requiring a separate quantization scheme. Specifically, it introduces a novel architecture that integrates multiple local-loss blocks, which include the proposed NITRO-Scaling layer and NITRO-ReLU activation function. The proposed framework also features a novel learning algorithm that employs local error signals and leverages IntegerSGD, an optimizer specifically designed for integer computations. NITRO-D is implemented as an open-source Python library. Extensive evaluations on state-of-the-art image recognition datasets demonstrate its effectiveness. For integer-only MLPs, NITRO-D improves test accuracy by up to +5.96% over the state-of-the-art. It also successfully trains integer-only CNNs, reducing memory requirements and energy consumption by up to 76.14% and 32.42%, respectively, compared to the traditional FP backpropagation algorithm.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based Models</title>
<link>https://arxiv.org/abs/2501.07033</link>
<guid>https://arxiv.org/abs/2501.07033</guid>
<content:encoded><![CDATA[
arXiv:2501.07033v2 Announce Type: replace-cross 
Abstract: This study explores the use of Generative Adversarial Networks (GANs) to detect AI deepfakes and fraudulent activities in online payment systems. With the growing prevalence of deepfake technology, which can manipulate facial features in images and videos, the potential for fraud in online transactions has escalated. Traditional security systems struggle to identify these sophisticated forms of fraud. This research proposes a novel GAN-based model that enhances online payment security by identifying subtle manipulations in payment images. The model is trained on a dataset consisting of real-world online payment images and deepfake images generated using advanced GAN architectures, such as StyleGAN and DeepFake. The results demonstrate that the proposed model can accurately distinguish between legitimate transactions and deepfakes, achieving a high detection rate above 95%. This approach significantly improves the robustness of payment systems against AI-driven fraud. The paper contributes to the growing field of digital security, offering insights into the application of GANs for fraud detection in financial services. Keywords- Payment Security, Image Recognition, Generative Adversarial Networks, AI Deepfake, Fraudulent Activities
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ENTIRE: Learning-based Volume Rendering Time Prediction</title>
<link>https://arxiv.org/abs/2501.12119</link>
<guid>https://arxiv.org/abs/2501.12119</guid>
<content:encoded><![CDATA[
arXiv:2501.12119v2 Announce Type: replace-cross 
Abstract: We introduce ENTIRE, a novel deep learning-based approach for fast and accurate volume rendering time prediction. Predicting rendering time is inherently challenging due to its dependence on multiple factors, including volume data characteristics, image resolution, camera configuration, and transfer function settings. Our method addresses this by first extracting a feature vector that encodes structural volume properties relevant to rendering performance. This feature vector is then integrated with additional rendering parameters, such as image resolution, camera setup, and transfer function settings, to produce the final prediction. We evaluate ENTIRE across multiple rendering frameworks (CPU- and GPU-based) and configurations (with and without single-scattering) on diverse datasets. The results demonstrate that our model achieves high prediction accuracy with fast inference speed. Furthermore, we showcase ENTIRE's effectiveness in two case studies, where it enables dynamic parameter adaptation for stable frame rates and load balancing.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments</title>
<link>https://arxiv.org/abs/2505.19361</link>
<guid>https://arxiv.org/abs/2505.19361</guid>
<content:encoded><![CDATA[
arXiv:2505.19361v4 Announce Type: replace-cross 
Abstract: The deployment of pre-trained perception models in novel environments often leads to performance degradation due to distributional shifts. Although recent artificial intelligence approaches for metacognition use logical rules to characterize and filter model errors, improving precision often comes at the cost of reduced recall. This paper addresses the hypothesis that leveraging multiple pre-trained models can mitigate this recall reduction. We formulate the challenge of identifying and managing conflicting predictions from various models as a consistency-based abduction problem, building on the idea of abductive learning (ABL) but applying it to test-time instead of training. The input predictions and the learned error detection rules derived from each model are encoded in a logic program. We then seek an abductive explanation--a subset of model predictions--that maximizes prediction coverage while ensuring the rate of logical inconsistencies (derived from domain constraints) remains below a specified threshold. We propose two algorithms for this knowledge representation task: an exact method based on Integer Programming (IP) and an efficient Heuristic Search (HS). Through extensive experiments on a simulated aerial imagery dataset featuring controlled, complex distributional shifts, we demonstrate that our abduction-based framework outperforms individual models and standard ensemble baselines, achieving, for instance, average relative improvements of approximately 13.6\% in F1-score and 16.6\% in accuracy across 15 diverse test datasets when compared to the best individual model. Our results validate the use of consistency-based abduction as an effective mechanism to robustly integrate knowledge from multiple imperfect models in challenging, novel scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09532</link>
<guid>https://arxiv.org/abs/2506.09532</guid>
<content:encoded><![CDATA[
arXiv:2506.09532v4 Announce Type: replace-cross 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainPath: A Biologically-Informed AI Framework for Individualized Aging Brain Generation</title>
<link>https://arxiv.org/abs/2508.16667</link>
<guid>https://arxiv.org/abs/2508.16667</guid>
<content:encoded><![CDATA[
arXiv:2508.16667v3 Announce Type: replace-cross 
Abstract: The global population is aging rapidly, and aging is a major risk factor for various diseases. It is an important task to predict how each individual's brain will age, as the brain supports many human functions. This capability can greatly facilitate healthcare automation by enabling personalized, proactive intervention and efficient healthcare resource allocation. However, this task is extremely challenging because of the brain's complex 3D anatomy. While there have been successes in natural image generation and brain MRI synthesis, existing methods fall short in generating individualized, anatomically faithful aging brain trajectories. To address these gaps, we propose BrainPath, a novel AI model that, given a single structural MRI of an individual, generates synthetic longitudinal MRIs that represent that individual's expected brain anatomy as they age. BrainPath introduces three architectural innovations: an age-aware encoder with biologically grounded supervision, a differential age conditioned decoder for anatomically faithful MRI synthesis, and a swap-learning strategy that implicitly separates stable subject-specific anatomy from aging effects. We further design biologically informed loss functions, including an age calibration loss and an age and structural perceptual loss, to complement the conventional reconstruction loss. This enables the model to capture subtle, temporally meaningful anatomical changes associated with aging. We apply BrainPath to two of the largest public aging datasets and conduct a comprehensive, multifaceted evaluation. Our results demonstrate BrainPath's superior performance in generation accuracy, anatomical fidelity, and cross-dataset generalizability, outperforming competing methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignBind-LLM: Multi-Stage Modality Fusion for Sign Language Translation</title>
<link>https://arxiv.org/abs/2509.00030</link>
<guid>https://arxiv.org/abs/2509.00030</guid>
<content:encoded><![CDATA[
arXiv:2509.00030v3 Announce Type: replace-cross 
Abstract: Despite progress in gloss-free Sign Language Translation (SLT), traditional single modality end-to-end approaches consistently fail on two critical components of natural signing: the precise recognition of high-speed fingerspelling and the integration of asynchronous non-manual cues from the face. Recent progress in SLT with Large Language Models has side stepped this challenge, forcing a single network to learn these simultaneously resulting in poor performance when tasked with translating crucial information such as names, places, and technical terms. We introduce SignBind-LLM, a modular framework designed to overcome these limitations. Our approach employs separate, specialized predictors for continuous signing, fingerspelling, and lipreading. Each expert network first decodes its specific modality into a sequence of tokens. These parallel streams are then fused by a lightweight transformer that resolves temporal misalignments before passing the combined representation to a Large Language Model (LLM) for final sentence generation. Our method establishes a new state-of-the-art on the How2Sign, ChicagoFSWildPlus, and BOBSL datasets with a BLEU-4 score of 22.1, 73.2% letter accuracy and BLEU-4 score of 6.8 respectively. These results validate our core hypothesis: isolating and solving distinct recognition tasks before fusion provides a more powerful and effective pathway to robust, high-fidelity sign language translation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
<link>https://arxiv.org/abs/2509.04734</link>
<guid>https://arxiv.org/abs/2509.04734</guid>
<content:encoded><![CDATA[
arXiv:2509.04734v2 Announce Type: replace-cross 
Abstract: The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) supervised contrastive learning with Euclidean distance as the feature space metric is improved by replacing the standard loss function with Jenson-Shannon divergence (JSD); (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded $f$-divergence. Our results highlight the importance of considering divergence choices in representation learning optimization.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GigaBrain-0: A World Model-Powered Vision-Language-Action Model</title>
<link>https://arxiv.org/abs/2510.19430</link>
<guid>https://arxiv.org/abs/2510.19430</guid>
<content:encoded><![CDATA[
arXiv:2510.19430v3 Announce Type: replace-cross 
Abstract: Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceTrans: Translation and Spatial Tracing for Surgical Prediction</title>
<link>https://arxiv.org/abs/2510.22379</link>
<guid>https://arxiv.org/abs/2510.22379</guid>
<content:encoded><![CDATA[
arXiv:2510.22379v4 Announce Type: replace-cross 
Abstract: Image-to-image translation models have achieved notable success in converting images across visual domains and are increasingly used for medical tasks such as predicting post-operative outcomes and modeling disease progression. However, most existing methods primarily aim to match the target distribution and often neglect spatial correspondences between the source and translated images. This limitation can lead to structural inconsistencies and hallucinations, undermining the reliability and interpretability of the predictions. These challenges are accentuated in clinical applications by the stringent requirement for anatomical accuracy. In this work, we present TraceTrans, a novel deformable image translation model designed for post-operative prediction that generates images aligned with the target distribution while explicitly revealing spatial correspondences with the pre-operative input. The framework employs an encoder for feature extraction and dual decoders for predicting spatial deformations and synthesizing the translated image. The predicted deformation field imposes spatial constraints on the generated output, ensuring anatomical consistency with the source. Extensive experiments on medical cosmetology and brain MRI datasets demonstrate that TraceTrans delivers accurate and interpretable post-operative predictions, highlighting its potential for reliable clinical deployment.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing</title>
<link>https://arxiv.org/abs/2511.09568</link>
<guid>https://arxiv.org/abs/2511.09568</guid>
<content:encoded><![CDATA[
arXiv:2511.09568v2 Announce Type: replace-cross 
Abstract: Diffusion models show promise for 3D molecular generation, but face a fundamental trade-off between sampling efficiency and conformational accuracy. While flow-based models are fast, they often produce geometrically inaccurate structures, as they have difficulty capturing the multimodal distributions of molecular conformations. In contrast, denoising diffusion models are more accurate but suffer from slow sampling, a limitation attributed to sub-optimal integration between diffusion dynamics and SE(3)-equivariant architectures. To address this, we propose VEDA, a unified SE(3)-equivariant framework that combines variance-exploding diffusion with annealing to efficiently generate conformationally accurate 3D molecular structures. Specifically, our key technical contributions include: (1) a VE schedule that enables noise injection functionally analogous to simulated annealing, improving 3D accuracy and reducing relaxation energy; (2) a novel preconditioning scheme that reconciles the coordinate-predicting nature of SE(3)-equivariant networks with a residual-based diffusion objective, and (3) a new arcsin-based scheduler that concentrates sampling in critical intervals of the logarithmic signal-to-noise ratio. On the QM9 and GEOM-DRUGS datasets, VEDA matches the sampling efficiency of flow-based models, achieving state-of-the-art valency stability and validity with only 100 sampling steps. More importantly, VEDA's generated structures are remarkably stable, as measured by their relaxation energy during GFN2-xTB optimization. The median energy change is only 1.72 kcal/mol, significantly lower than the 32.3 kcal/mol from its architectural baseline, SemlaFlow. Our framework demonstrates that principled integration of VE diffusion with SE(3)-equivariant architectures can achieve both high chemical accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Process Reward Models are Symbolic Vision Learners</title>
<link>https://arxiv.org/abs/2512.03126</link>
<guid>https://arxiv.org/abs/2512.03126</guid>
<content:encoded><![CDATA[
<div> Keywords: symbolic computer vision, self-supervised auto-encoder, hierarchical process reward, neuro-symbolic system, diagram reconstruction  

<br /><br />Summary:  
1. The paper introduces a novel approach in symbolic computer vision that models diagrams using explicit logical rules and structured primitives such as points, lines, and shapes, contrasting with conventional pixel-based methods that rely on textures and colors.  
2. A self-supervised symbolic auto-encoder is proposed, which encodes diagrams into structured geometric primitives and their relationships and decodes them via an executable engine to faithfully reconstruct the original diagrams.  
3. Central to the architecture is the Symbolic Hierarchical Process Reward Modeling, a hierarchical, step-level reward system that enforces consistency rules like point-on-line, line-on-shape, and shape-on-relation during parsing and reconstruction.  
4. The authors address poor exploration challenges in reinforcement learning by integrating stabilization mechanisms that balance exploration and exploitation in the policy space for improved diagram reconstruction.  
5. Their approach extends to fine-tuning the symbolic encoder for downstream tasks, resulting in a neuro-symbolic system that merges neural network reasoning with symbolic interpretability using reasoning-grounded visual rewards.  
6. Experimental results demonstrate significant improvements: a 98.2% reduction in mean squared error for geometric diagram reconstruction, outperforming GPT-4o by 0.6% in chart reconstruction with a 7B model, and notable enhancements on perception and reasoning benchmarks like MathGlance (+13%), MathVerse (+3%), and GeoQA (+3%). <div>
arXiv:2512.03126v1 Announce Type: new 
Abstract: Symbolic computer vision represents diagrams through explicit logical rules and structured representations, enabling interpretable understanding in machine vision. This requires fundamentally different learning paradigms from pixel-based visual models. Symbolic visual learners parse diagrams into geometric primitives-points, lines, and shapes-whereas pixel-based learners operate on textures and colors. We propose a novel self-supervised symbolic auto-encoder that encodes diagrams into structured primitives and their interrelationships within the latent space, and decodes them through our executable engine to reconstruct the input diagrams. Central to this architecture is Symbolic Hierarchical Process Reward Modeling, which applies hierarchical step-level parsing rewards to enforce point-on-line, line-on-shape, and shape-on-relation consistency. Since vanilla reinforcement learning exhibits poor exploration in the policy space during diagram reconstruction; we thus introduce stabilization mechanisms to balance exploration and exploitation. We fine-tune our symbolic encoder on downstream tasks, developing a neuro-symbolic system that integrates the reasoning capabilities of neural networks with the interpretability of symbolic models through reasoning-grounded visual rewards. Evaluations across reconstruction, perception, and reasoning tasks demonstrate the effectiveness of our approach: achieving a 98.2% reduction in MSE for geometric diagram reconstruction, surpassing GPT-4o by 0.6% with a 7B model on chart reconstruction, and improving by +13% on the MathGlance perception benchmark, and by +3% on MathVerse and GeoQA reasoning benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drainage: A Unifying Framework for Addressing Class Uncertainty</title>
<link>https://arxiv.org/abs/2512.03182</link>
<guid>https://arxiv.org/abs/2512.03182</guid>
<content:encoded><![CDATA[
<div> Keywords: noisy labels, drainage node, instance-dependent noise, open-set recognition, semi-supervised cleaning

<br /><br />Summary:  
This paper addresses challenges in deep learning related to noisy labels, class ambiguity, and the rejection of out-of-distribution or corrupted samples. The authors introduce a novel concept called the "drainage node," an additional output node in neural networks designed to reallocate probability mass toward uncertainty. This facilitates better handling of ambiguous, anomalous, or noisy inputs while maintaining end-to-end differentiability and training efficiency. Extensive experiments on CIFAR-10 and CIFAR-100 datasets with varying levels of instance-dependent and asymmetric noise demonstrate up to a 9% accuracy improvement over state-of-the-art methods in high noise scenarios. Real-world validations on datasets like mini-WebVision, mini-ImageNet, and Clothing-1M show competitive or superior performance compared to existing approaches. The drainage node helps absorb corrupted, mislabeled, or outlier samples, which stabilizes decision boundaries and produces a denoising effect. Moreover, the framework extends beyond classification, offering practical benefits for large-scale semi-supervised dataset cleaning and open-set recognition tasks. This unified solution thus provides a robust method for improving resilience and accuracy in challenging noise conditions in deep learning settings. <div>
arXiv:2512.03182v1 Announce Type: new 
Abstract: Modern deep learning faces significant challenges with noisy labels, class ambiguity, as well as the need to robustly reject out-of-distribution or corrupted samples. In this work, we propose a unified framework based on the concept of a "drainage node'' which we add at the output of the network. The node serves to reallocate probability mass toward uncertainty, while preserving desirable properties such as end-to-end training and differentiability. This mechanism provides a natural escape route for highly ambiguous, anomalous, or noisy samples, particularly relevant for instance-dependent and asymmetric label noise. In systematic experiments involving the addition of varying proportions of instance-dependent noise or asymmetric noise to CIFAR-10/100 labels, our drainage formulation achieves an accuracy increase of up to 9\% over existing approaches in the high-noise regime. Our results on real-world datasets, such as mini-WebVision, mini-ImageNet and Clothing-1M, match or surpass existing state-of-the-art methods. Qualitative analysis reveals a denoising effect, where the drainage neuron consistently absorbs corrupt, mislabeled, or outlier data, leading to more stable decision boundaries. Furthermore, our drainage formulation enables applications well beyond classification, with immediate benefits for web-scale, semi-supervised dataset cleaning, and open-set applications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Head Pose Correction Improve Biometric Facial Recognition?</title>
<link>https://arxiv.org/abs/2512.03199</link>
<guid>https://arxiv.org/abs/2512.03199</guid>
<content:encoded><![CDATA[
<div> Keywords: facial recognition, head-pose correction, image restoration, CFR-GAN, CodeFormer  

<br /><br />Summary:  
This study addresses the challenge that biometric facial recognition systems face when dealing with real-world images that are often low quality, exhibit non-frontal poses, and include occlusions. The authors explore whether AI-driven techniques aimed at head-pose correction and image restoration can enhance recognition accuracy. They employ a large-scale, model-agnostic forensic-evaluation pipeline to rigorously test three specific restoration methods: 3D reconstruction using NextFace, 2D frontalization through CFR-GAN, and feature enhancement via CodeFormer. Their results reveal that the straightforward, indiscriminate application of any of these restoration techniques actually harms facial recognition performance. However, they identify that a carefully targeted combination of CFR-GAN for 2D frontalization and CodeFormer for feature enhancement can lead to meaningful improvements in recognition accuracy. Thus, this work highlights the importance of selective application and integration of restoration technologies rather than naive usage, contributing valuable insights for improving real-world biometric facial recognition systems. <div>
arXiv:2512.03199v1 Announce Type: new 
Abstract: Biometric facial recognition models often demonstrate significant decreases in accuracy when processing real-world images, often characterized by poor quality, non-frontal subject poses, and subject occlusions. We investigate whether targeted, AI-driven, head-pose correction and image restoration can improve recognition accuracy. Using a model-agnostic, large-scale, forensic-evaluation pipeline, we assess the impact of three restoration approaches: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer). We find that naive application of these techniques substantially degrades facial recognition accuracy. However, we also find that selective application of CFR-GAN combined with CodeFormer yields meaningful improvements.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flux4D: Flow-based Unsupervised 4D Reconstruction</title>
<link>https://arxiv.org/abs/2512.03210</link>
<guid>https://arxiv.org/abs/2512.03210</guid>
<content:encoded><![CDATA[
<div> Keywords: Flux4D, 4D reconstruction, 3D Gaussian Splatting, unsupervised learning, dynamic scenes  

<br /><br />Summary:  
This paper introduces Flux4D, a novel framework designed for scalable 4D reconstruction of large-scale dynamic scenes from visual input. Unlike previous differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting, which are limited by scalability and require annotations to separate actor motion, Flux4D operates in a fully unsupervised manner without reliance on pre-trained models or foundational priors. It directly predicts 3D Gaussians and their motion dynamics using only photometric losses combined with an "as static as possible" regularization. This enables the model to decompose dynamic elements automatically by training across multiple scenes, avoiding the need for per-scene optimization and heavy hyperparameter tuning common in existing self-supervised methods. Flux4D demonstrates high efficiency, reconstructing dynamic scenes within seconds while scaling well to large datasets. Moreover, it generalizes effectively to unseen environments, including rare and unknown objects, addressing challenges in outdoor driving scenarios. Experimental results show that Flux4D significantly outperforms state-of-the-art methods in terms of scalability, reconstruction quality, and generalization, highlighting its potential for applications in robotics and autonomous systems where dynamic scene understanding is essential. <div>
arXiv:2512.03210v1 Announce Type: new 
Abstract: Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an "as static as possible" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object Counting with GPT-4o and GPT-5: A Comparative Study</title>
<link>https://arxiv.org/abs/2512.03233</link>
<guid>https://arxiv.org/abs/2512.03233</guid>
<content:encoded><![CDATA[
<div> Zero-shot object counting, large language models, multi-modal LLMs, GPT-4o, GPT-5

<br /><br />Summary:  
This paper addresses the task of zero-shot object counting, which involves estimating the number of object instances in novel categories without prior training on those categories. Traditional methods rely heavily on large annotated datasets and visual exemplars to guide the counting process, which can be labor-intensive and limiting. The authors propose leveraging the visual and reasoning capabilities of cutting-edge multi-modal large language models, specifically GPT-4o and GPT-5, to perform object counting using only textual prompts and no supervision. They conduct evaluations on two established datasets, FSC-147 and CARPK, to benchmark these models’ performance against current zero-shot counting approaches. The results demonstrate that the multi-modal LLMs achieve comparable accuracy to state-of-the-art zero-shot methods on FSC-147 and, in some cases, even outperform them. This suggests the promising potential of incorporating advanced multi-modal LLMs in computer vision tasks like object counting, reducing dependency on large labeled datasets and visual exemplars. The study highlights the expanded capabilities of LLMs beyond natural language understanding, emphasizing their integrative reasoning and data comprehension power in zero-shot scenarios. <div>
arXiv:2512.03233v1 Announce Type: new 
Abstract: Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Guided Material Inference for 3D Point Clouds</title>
<link>https://arxiv.org/abs/2512.03237</link>
<guid>https://arxiv.org/abs/2512.03237</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D shape, material inference, large language model, zero-shot learning, semantic reasoning<br /><br />Summary:<br /><br />This paper addresses the limitation of current 3D shape datasets and models that predominantly focus on geometry while neglecting material properties crucial for object appearance. The authors propose a novel two-stage method utilizing large language models (LLMs) to infer material composition directly from 3D point clouds that include coarse segmentations. The key innovation lies in decoupling the reasoning process: the first stage uses an LLM to predict the object's semantic category, and the second stage assigns plausible materials to each geometric segment based on the semantics inferred earlier. Both stages function in a zero-shot fashion, eliminating the need for task-specific training data. To evaluate their approach, the authors use an LLM-based judge implemented within DeepEval, compensating for the lack of reliable material annotations in existing datasets. Empirical results on 1,000 shapes drawn from Fusion/ABS and ShapeNet demonstrate the method’s high performance regarding semantic correctness and material plausibility. Overall, the study showcases how language models can act as versatile priors that bridge the gap between geometric understanding and material reasoning in 3D data analysis. <div>
arXiv:2512.03237v1 Announce Type: new 
Abstract: Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>2-Shots in the Dark: Low-Light Denoising with Minimal Data Acquisition</title>
<link>https://arxiv.org/abs/2512.03245</link>
<guid>https://arxiv.org/abs/2512.03245</guid>
<content:encoded><![CDATA[
<div> keywords: noise synthesis, low-light denoising, Poisson distribution, Fourier-domain spectral sampling, sensor noise

<br /><br />Summary: This article addresses the challenge of denoising raw images taken under low-light conditions, where noise arises from low photon counts and sensor imperfections. It highlights the limitations of learning-based denoisers, which typically require large paired datasets of clean and noisy images that are difficult to obtain. To overcome this, the authors propose a novel noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. Their approach models signal-dependent noise using a Poisson distribution, while signal-independent noise is accurately captured through a Fourier-domain spectral sampling algorithm. This method generates diverse noise patterns that preserve the spatial and statistical characteristics of real sensor noise. Unlike existing approaches, it avoids reliance on simplified parametric noise models or extensive clean-noisy image pairs. The paper demonstrates that the proposed noise synthesis method is both practical and accurate, leading to state-of-the-art performance on several low-light denoising benchmarks. Overall, the contribution offers a scalable and effective alternative for noise modeling and synthesis in image denoising tasks, particularly under challenging low-light scenarios. <div>
arXiv:2512.03245v1 Announce Type: new 
Abstract: Raw images taken in low-light conditions are very noisy due to low photon count and sensor noise. Learning-based denoisers have the potential to reconstruct high-quality images. For training, however, these denoisers require large paired datasets of clean and noisy images, which are difficult to collect. Noise synthesis is an alternative to large-scale data acquisition: given a clean image, we can synthesize a realistic noisy counterpart. In this work, we propose a general and practical noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. We represent signal-dependent noise with a Poisson distribution and introduce a Fourier-domain spectral sampling algorithm to accurately model signal-independent noise. The latter generates diverse noise realizations that maintain the spatial and statistical properties of real sensor noise. As opposed to competing approaches, our method neither relies on simplified parametric models nor on large sets of clean-noisy image pairs. Our synthesis method is not only accurate and practical, it also leads to state-of-the-art performances on multiple low-light denoising benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement</title>
<link>https://arxiv.org/abs/2512.03247</link>
<guid>https://arxiv.org/abs/2512.03247</guid>
<content:encoded><![CDATA[
<div> Latent Diffusion Models, image inpainting, pixel-level refinement, artifact simulation, local editing<br /><br />Summary:<br /><br />Latent Diffusion Models (LDMs) have significantly improved image inpainting and local editing, but their latent compression often leads to pixel-level artifacts such as chromatic shifts, texture mismatches, and seam visibility at editing boundaries. Existing solutions, like background-conditioned latent decoding and pixel-space harmonization, fail to fully remove these artifacts and lack generalizability across different latent representations and editing tasks. To address this, the paper introduces PixPerfect, a novel pixel-level refinement framework designed to provide seamless and high-fidelity local edits compatible with various LDM architectures and tasks. PixPerfect operates through three main components: a differentiable discriminative pixel space that highlights subtle color and texture inconsistencies for effective correction, an artifact simulation pipeline that trains the system on realistic editing errors, and a direct pixel-space refinement method ensuring broad applicability. Extensive experiments conducted on benchmarks including inpainting, object removal, and insertion reveal that PixPerfect greatly improves perceptual quality and editing performance. The approach establishes a new baseline for robust, high-quality localized image editing across diverse settings, overcoming the limitations of previous methods and enhancing the visual coherence of edited images. <div>
arXiv:2512.03247v1 Announce Type: new 
Abstract: Latent Diffusion Models (LDMs) have markedly advanced the quality of image inpainting and local editing. However, the inherent latent compression often introduces pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams along editing boundaries. Existing remedies, including background-conditioned latent decoding and pixel-space harmonization, usually fail to fully eliminate these artifacts in practice and do not generalize well across different latent representations or tasks. We introduce PixPerfect, a pixel-level refinement framework that delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks. PixPerfect leverages (i) a differentiable discriminative pixel space that amplifies and suppresses subtle color and texture discrepancies, (ii) a comprehensive artifact simulation pipeline that exposes the refiner to realistic local editing artifacts during training, and (iii) a direct pixel-space refinement scheme that ensures broad applicability across diverse latent representations and tasks. Extensive experiments on inpainting, object removal, and insertion benchmarks demonstrate that PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2512.03257</link>
<guid>https://arxiv.org/abs/2512.03257</guid>
<content:encoded><![CDATA[
<div> wildfire detection, deep learning, onboard processing, fire radiative power, multi-class classification<br /><br />Summary:<br /><br />1. The study addresses the critical need for rapid and accurate wildfire detection in airborne and spaceborne platforms, emphasizing real-time distinction between no fire, active fire, and post-fire conditions alongside fire intensity estimation.<br /><br />2. It highlights challenges posed by multispectral and hyperspectral thermal imagers, which provide detailed spectral data but demand computationally efficient algorithms due to high dimensionality and limited onboard resources.<br /><br />3. Multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, are systematically evaluated for multi-class fire classification tasks.<br /><br />4. The authors propose PyroFocus, a novel two-stage pipeline that first classifies fire presence and then performs either fire radiative power regression or fire segmentation to optimize inference speed and reduce computational burden.<br /><br />5. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), they demonstrate that PyroFocus achieves an effective balance between accuracy and latency, confirming its suitability for real-time edge deployment in future wildfire monitoring missions. <div>
arXiv:2512.03257v1 Announce Type: new 
Abstract: Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.
  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.
  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2512.03284</link>
<guid>https://arxiv.org/abs/2512.03284</guid>
<content:encoded><![CDATA[
<div> Spatial reasoning, 3D visual question answering, multi-floor environments, active perception, reinforcement learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of spatial reasoning in large-scale 3D environments for vision-language models, which are commonly limited to room-scale settings.  
2. It introduces H$^2$U3D (Holistic House Understanding in 3D), a novel 3D visual question answering dataset featuring complex house-scale environments with up to three floors, 10-20 rooms, and spanning more than 300 m².  
3. The dataset is created using an automated annotation pipeline that generates hierarchical coarse-to-fine visual representations and diverse question-answer pairs enriched with chain-of-thought annotations.  
4. The authors propose SpatialReasoner, an active perception framework that autonomously leverages spatial tools to explore 3D scenes conditioned on textual queries.  
5. SpatialReasoner is trained in two stages: a supervised cold start phase followed by reinforcement learning guided by an adaptive exploration reward that encourages exploration efficiency and avoids redundant actions.  
6. Extensive experiments demonstrate that SpatialReasoner outperforms strong baselines such as GPT-4o and Gemini-2.5-Pro on the H$^2$U3D dataset.  
7. Notably, SpatialReasoner achieves superior results while using only 3-4 images on average, compared to baselines which require 16 or more images, underscoring the effectiveness of the proposed coarse-to-fine active exploration approach. <div>
arXiv:2512.03284v1 Announce Type: new 
Abstract: Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction</title>
<link>https://arxiv.org/abs/2512.03317</link>
<guid>https://arxiv.org/abs/2512.03317</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, map fusion, diffusion models, online map construction, sensor data<br /><br />Summary:<br /><br />This paper introduces NavMapFusion, a novel diffusion-based framework designed for online construction of accurate environmental maps crucial for autonomous driving. Traditional high-definition (HD) maps provide detailed static road infrastructure but may become outdated, necessitating online updates using on-board sensor data. NavMapFusion leverages standard-definition (SD) navigation maps, which are widely available but low-resolution, as coarse priors to guide the mapping process. The key innovation is modeling discrepancies between prior maps and real-time sensor data as noise within the diffusion model's iterative denoising process. This approach naturally suppresses outdated map regions while reinforcing consistent areas, enhancing robustness in map fusion. The paper addresses two core questions: how outdated coarse maps can effectively guide online map construction, and what benefits diffusion models bring to this fusion task. Experimental results on the nuScenes benchmark show a significant 21.4% relative improvement at 100 meters range when fusing OpenStreetMap road line priors with sensor data, with even higher gains at greater perception distances, all while preserving real-time processing capabilities. This fusion strategy produces accurate, up-to-date environmental representations, ultimately contributing to safer and more reliable autonomous navigation. The authors have made their code publicly available to facilitate further research and application. <div>
arXiv:2512.03317v1 Announce Type: new 
Abstract: Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-by-step Layered Design Generation</title>
<link>https://arxiv.org/abs/2512.03335</link>
<guid>https://arxiv.org/abs/2512.03335</guid>
<content:encoded><![CDATA[
<div> Keywords: design generation, step-by-step process, layered changes, multi-modal LLMs, benchmark dataset<br /><br />Summary:<br /><br />1. The paper identifies a mismatch between the real-world step-by-step nature of the design process and existing AI approaches that treat design generation as a single-step task. 2. To address this, the authors propose the new problem formulation called Step-by-Step Layered Design Generation, which requires a model to generate designs progressively based on a sequence of designer instructions. 3. They introduce SLEDGE (Step-by-step LayEred Design GEnerator), a method leveraging multi-modal large language models (LLMs) to produce incremental, atomic layered changes grounded on the given instructions, effectively modeling each design update as a modification over the previous state. 4. A new evaluation suite is created, comprising a specialized dataset and benchmark tailored to this stepwise layered design generation task, enabling rigorous assessment of model performance in this novel setting. 5. Extensive experiments demonstrate that SLEDGE outperforms existing state-of-the-art approaches adapted to the new problem, highlighting the importance and effectiveness of modeling design as a sequential, layered generation process. The work aims to stimulate further research in this practical and underexplored area of design synthesis. <div>
arXiv:2512.03335v1 Announce Type: new 
Abstract: Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography</title>
<link>https://arxiv.org/abs/2512.03339</link>
<guid>https://arxiv.org/abs/2512.03339</guid>
<content:encoded><![CDATA[
<div> Ejection fraction, prototype learning, video-based model, explainability, cardiac function<br /><br />Summary:<br /><br />1. Ejection fraction (EF) is a vital clinical metric used to evaluate cardiac function and diagnose heart failure, but traditional EF assessment involves manual tracing and expert judgment, which is time-consuming and variable across observers.<br />2. Most existing deep learning approaches for EF prediction operate as black-box models, lacking transparency and thus limiting clinical trust and adoption.<br />3. Post-hoc explainability methods attempt to interpret deep learning outcomes after prediction but fail to influence or improve the model’s internal reasoning, resulting in limited clinical reliability.<br />4. ProtoEFNet is introduced as a novel video-based prototype learning model designed for continuous EF regression, which learns dynamic spatiotemporal prototypes representing meaningful cardiac motion patterns.<br />5. A new Prototype Angular Separation (PAS) loss is proposed to enforce discriminative representations along the continuous EF scale, enhancing model performance.<br />6. Experiments on the EchonetDynamic dataset demonstrate that ProtoEFNet matches the accuracy of non-interpretable models while providing improved clinical insights.<br />7. An ablation study shows that adding the PAS loss increases the F1 score by approximately 2%, from 77.67±2.68 to 79.64±2.10.<br />8. The source code for ProtoEFNet is publicly available, promoting transparency and reproducibility in EF prediction research. <div>
arXiv:2512.03339v1 Announce Type: new 
Abstract: Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\pm$2.68 to 79.64$\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration</title>
<link>https://arxiv.org/abs/2512.03345</link>
<guid>https://arxiv.org/abs/2512.03345</guid>
<content:encoded><![CDATA[
<div> Keywords: hallucinations, image restoration, low-field MRI, diffusion models, hallucination detection<br /><br />Summary:  
Generative models used in image restoration often produce hallucinations—plausible but incorrect image structures absent from the ground truth—which pose significant risks in safety-critical areas like medical imaging, industrial inspection, and remote sensing. This challenge is particularly acute in low-field MRI enhancement, prevalent in resource-limited settings, where hallucinations can cause serious diagnostic errors. A key limitation in addressing hallucinations is the dependency on labeled data for evaluation, which is costly and subjective. To overcome this, the authors introduce HalluGen, a diffusion-based framework capable of synthesizing realistic hallucinations with controllable attributes such as type, location, and severity, resulting in perceptually realistic yet semantically incorrect images (demonstrated by segmentation IoU dropping from 0.86 to 0.36). Using HalluGen, they create the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MRI scans for low-field MRI enhancement. This dataset enables systematic evaluation of hallucination detection and mitigation methods. The work demonstrates two main applications: (1) benchmarking image quality metrics and proposing SHAFE, a feature-based metric with soft-attention pooling that improves hallucination sensitivity beyond traditional metrics, and (2) training reference-free hallucination detectors that generalize well to real-world restoration failures. Together, HalluGen and the dataset provide the first scalable foundation for evaluating hallucinations in safety-critical image restoration tasks. <div>
arXiv:2512.03345v1 Announce Type: new 
Abstract: Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus</title>
<link>https://arxiv.org/abs/2512.03346</link>
<guid>https://arxiv.org/abs/2512.03346</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical attention, volumetric medical imaging, subclinical keratoconus, deep learning architectures, anomaly detection<br /><br />Summary:  
This study addresses the challenge of detecting subtle, spatially distributed anomalies in volumetric medical imaging, focusing on early disease signals that are often missed due to suboptimal model architectures. It compares sixteen modern deep learning architectures, including 2D/3D CNNs, hybrid models, and volumetric transformers, specifically for subclinical keratoconus detection using 3D anterior segment OCT volumes. The research finds that hierarchical attention models deliver superior sensitivity and specificity (21-23% improvement) compared to traditional CNNs and Vision Transformers by better matching the spatial scale of sparse anomalies. This improvement is attributed to hierarchical windowing, which creates receptive fields aligned with the intermediate range of subclinical abnormalities, avoiding drawbacks of excessive locality in CNNs and the overly global attention in standard ViTs. Measurements of attention-distance reveal that spatial integration requirements vary with signal strength, necessitating longer integration for subclinical cases than for healthy or manifest disease states. Additional analyses, including representational similarity and auxiliary tasks such as age and sex prediction, affirm the broad applicability and robustness of these inductive biases. The study thus provides important design insights for future volumetric anomaly detection systems, highlighting hierarchical attention as an effective architectural strategy for early pathological change detection in 3D medical imaging. <div>
arXiv:2512.03346v1 Announce Type: new 
Abstract: The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge. The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention. This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved. This study presents a controlled comparison of sixteen modern deep learning architectures spanning 2D/3D convolutional, hybrid, and volumetric transformer families for subclinical keratoconus (SKC) detection from 3D anterior segment OCT volumes. We demonstrate that hierarchical attention models offer a superior and more parameter-efficient inductive bias, surpassing the performance of both 2D and 3D CNNs and ViTs. Our results show 21-23% higher sensitivity and specificity in the sparse anomaly (subclinical) regime. Mechanistic analyses reveal that this advantage stems from precise spatial scale alignment: hierarchical windowing produces effective receptive fields matched to the intermediate, multi-slice extent of subclinical abnormalities. This avoids excessive CNN locality and diffuse global attention. Attention-distance measurements confirm a key insight into architectural adaptation: the required spatial integration length shifts significantly based on the signal strength, with subclinical cases necessitating longer integration compared to both healthy and manifest disease states. Representational similarity and auxiliary age/sex prediction tasks further support the generalizability of these inductive principles. The findings provide design guidance for future volumetric anomaly detection systems, establishing hierarchical attention as a principled and effective approach for early pathological change analysis in 3D medical imaging.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation</title>
<link>https://arxiv.org/abs/2512.03350</link>
<guid>https://arxiv.org/abs/2512.03350</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D dynamics, visual generation, 2D to 4D reconstruction, continuous modeling, video editing<br /><br />Summary:  
The paper introduces SeeU, a novel method designed to improve visual understanding and generation by leveraging the continuous 4D dynamics of the world, incorporating 3D spatial information plus time rather than operating solely on 2D projections. Firstly, SeeU reconstructs the continuous 4D world from sparse monocular 2D frames, establishing a 2D→4D learning process. Secondly, it models the continuous temporal dynamics on a low-rank representation constrained by physical laws, transitioning from discrete 4D data to continuous 4D understanding. Thirdly, by rolling the learned 4D world forward in time and re-projecting it to 2D viewpoints, SeeU can generate unseen spatial and temporal visual content using spatial-temporal context awareness. This approach allows for physically consistent and continuous novel visual generation, overcoming limitations of typical 2D-based techniques. The authors demonstrate SeeU’s versatility and strong potential in tasks such as generating unseen temporal frames, filling in previously unobserved spatial regions, and video editing applications. Overall, SeeU advances the capability of modeling and synthesizing dynamic visual scenes by bridging 2D observations and continuous 4D world representations. <div>
arXiv:2512.03350v1 Announce Type: new 
Abstract: Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\to$4D$\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Deep Learning Framework with Explainable AI for Lung Cancer Classification with DenseNet169 and SVM</title>
<link>https://arxiv.org/abs/2512.03359</link>
<guid>https://arxiv.org/abs/2512.03359</guid>
<content:encoded><![CDATA[
<div> Keywords: Lung cancer, CT scans, Deep learning, DenseNet169, Model interpretability<br /><br />Summary:<br /><br />1. Lung cancer is a highly lethal disease globally, and early detection through computed tomography (CT) scans is critical for improving survival rates. Manual interpretation of CT scans is time-intensive and susceptible to human error.<br /><br />2. The study proposes an automatic lung cancer classification system based on deep learning to increase diagnostic accuracy and interpretability.<br /><br />3. The IQOTHNCCD dataset, a public CT scan dataset with cases labeled as Normal, Benign, and Malignant, was used for model training and evaluation.<br /><br />4. The primary model is DenseNet169 with integrated Squeeze-and-Excitation blocks, Focal Loss for managing class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion.<br /><br />5. Additionally, an SVM classifier was developed using features extracted by MobileNetV2, enhancing classification performance.<br /><br />6. To improve model transparency, Grad-CAM was applied to visualize important decision regions in CT images, and SHAP was used to explain feature importance in the SVM model.<br /><br />7. Both DenseNet169 and SVM models achieved high accuracy of 98%, indicating their effectiveness and robustness for real-world medical applications.<br /><br />8. The results highlight the potential of deep learning approaches to improve lung cancer diagnosis by providing high accuracy, greater interpretability, and reliable performance. <div>
arXiv:2512.03359v1 Announce Type: new 
Abstract: Lung cancer is a very deadly disease worldwide, and its early diagnosis is crucial for increasing patient survival rates. Computed tomography (CT) scans are widely used for lung cancer diagnosis as they can give detailed lung structures. However, manual interpretation is time-consuming and prone to human error. To surmount this challenge, the study proposes a deep learning-based automatic lung cancer classification system to enhance detection accuracy and interpretability. The IQOTHNCCD lung cancer dataset is utilized, which is a public CT scan dataset consisting of cases categorized into Normal, Benign, and Malignant and used DenseNet169, which includes Squeezeand-Excitation blocks for attention-based feature extraction, Focal Loss for handling class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion. In addition, an SVM model was developed using MobileNetV2 for feature extraction, improving its classification performance. For model interpretability enhancement, the study integrated Grad-CAM for the visualization of decision-making regions in CT scans and SHAP (Shapley Additive Explanations) for explanation of feature contributions within the SVM model. Intensive evaluation was performed, and it was found that both DenseNet169 and SVM models achieved 98% accuracy, suggesting their robustness for real-world medical practice. These results open up the potential for deep learning to improve the diagnosis of lung cancer by a higher level of accuracy, transparency, and robustness.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting</title>
<link>https://arxiv.org/abs/2512.03369</link>
<guid>https://arxiv.org/abs/2512.03369</guid>
<content:encoded><![CDATA[
<div> Wildfire prediction, multi-modal dataset, UAV, FiReDiff, generative models<br /><br />Summary:<br /><br />This paper addresses the challenge of fine-grained wildfire spread prediction, which is essential for improving emergency response and decision-making accuracy. Existing studies typically use coarse spatiotemporal scales and low-resolution satellite imagery, limiting precise modeling of localized fire dynamics. To overcome these limitations, the authors introduce FireSentry, a provincial-scale multi-modal wildfire dataset with sub-meter spatial and sub-second temporal resolution, captured using synchronized UAVs. FireSentry includes visible and infrared video streams, in-situ environmental data, and manually validated fire masks. Using this dataset, the authors build a benchmark incorporating physics-based, data-driven, and generative wildfire spread models, highlighting the shortcomings of current mask-only prediction methods. They propose FiReDiff, a novel dual-modality approach that first generates future infrared video sequences to capture fire dynamics, then accurately segments fire masks based on the generated data. FiReDiff demonstrates significant improvements over existing generative models, achieving substantial gains in video quality metrics (PSNR, SSIM, LPIPS, FVD) and mask accuracy metrics (AUPRC, F1 score, IoU, MSE). The FireSentry dataset and FiReDiff framework together represent a major advance in fine-grained wildfire forecasting and dynamic disaster simulation. The benchmark dataset is publicly accessible at the provided GitHub link. <div>
arXiv:2512.03369v1 Announce Type: new 
Abstract: Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2512.03370</link>
<guid>https://arxiv.org/abs/2512.03370</guid>
<content:encoded><![CDATA[
<div> Keywords: ShelfGaussian, Gaussian-based methods, multi-modal transformer, vision foundation models, zero-shot semantic occupancy prediction<br /><br />Summary:<br /><br />1. ShelfGaussian is a novel open-vocabulary multi-modal 3D scene understanding framework that leverages Gaussian-based representations supervised by off-the-shelf vision foundation models (VFMs).<br />2. Existing Gaussian-based methods are limited either by closed-set semantic Gaussians tied to annotated 3D labels without rendering capabilities or by purely 2D self-supervised open-set Gaussians that degrade geometric quality and depend solely on camera data.<br />3. The proposed Multi-Modal Gaussian Transformer enables Gaussians to query and integrate features from diverse sensor modalities, enhancing the expressive power of Gaussian representations.<br />4. Shelf-Supervised Learning Paradigm jointly optimizes Gaussians by utilizing VFM-derived features at both the 2D image and the 3D scene levels, improving learning efficiency and effectiveness.<br />5. Evaluations on Occ3D-nuScenes highlight ShelfGaussian’s state-of-the-art zero-shot semantic occupancy prediction capabilities, and additional tests on unmanned ground vehicles demonstrate its robust real-world performance in complex urban environments.<br />6. The framework shows promising results for perception and planning tasks, pushing forward the generalization and applicability of Gaussian-based 3D scene understanding methods across multiple sensor modalities.<br />7. Further details and resources are made available at the project website: https://lunarlab-gatech.github.io/ShelfGaussian/. <div>
arXiv:2512.03370v1 Announce Type: new 
Abstract: We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification</title>
<link>https://arxiv.org/abs/2512.03404</link>
<guid>https://arxiv.org/abs/2512.03404</guid>
<content:encoded><![CDATA[
<div> Cross-modal ship re-identification, Optical imagery, SAR imagery, Modality gap, Diffusion model<br /><br />Summary:<br /><br />This paper addresses the challenge of cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) images, which is a crucial yet underexplored problem in maritime surveillance due to the large modality gap. To overcome this, the authors propose MOS, a novel framework aimed at reducing this modality gap and enabling modality-consistent feature learning. MOS is composed of two main components: (1) Modality-Consistent Representation Learning (MCRL), which includes denoising SAR images and employing a class-wise modality alignment loss to better align feature distributions of the same ship identity across modalities; (2) Cross-modal Data Generation and Feature Fusion (CDGF), which uses a Brownian bridge diffusion model to synthesize cross-modal samples that are fused with original features during inference to improve feature alignment and discrimination. The approach is evaluated extensively on the HOSS ReID dataset, where MOS outperforms current state-of-the-art techniques significantly. Results show improvements of +3.0%, +6.2%, and +16.4% in Rank-1 accuracy under ALL to ALL, Optical to SAR, and SAR to Optical evaluation protocols, respectively. The authors also plan to release their code and trained models upon publication. <div>
arXiv:2512.03404v1 Announce Type: new 
Abstract: Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViDiC: Video Difference Captioning</title>
<link>https://arxiv.org/abs/2512.03405</link>
<guid>https://arxiv.org/abs/2512.03405</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Difference Captioning, Multimodal Large Language Models, ViDiC-1K dataset, comparative reasoning, video understanding<br /><br />Summary: Understanding visual differences in dynamic scenes necessitates analyzing compositional, spatial, and temporal changes, a challenge inadequately addressed by current vision-language systems. Past research on Image Difference Captioning (IDC) primarily focused on semantic changes between static images but failed to capture motion continuity, event progression, and editing consistency over time. To overcome this limitation, the paper introduces the Video Difference Captioning (ViDiC) task alongside the ViDiC-1K dataset, specifically designed to evaluate Multimodal Large Language Models' (MLLMs) capacity for fine-grained descriptions of similarities and differences between video pairs. The ViDiC-1K dataset comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items across seven categories: subject, style, background, cinematography, motion, location, and playback techniques. The authors propose a dual-checklist evaluation framework that separately measures the accuracy of similarity and difference descriptions, utilizing an LLM-as-a-Judge protocol for reliable performance assessment. Extensive experiments involving nineteen multimodal models reveal a substantial performance gap in these models’ ability to perceive differences and generate comparative descriptions accurately. This work aims to provide a challenging benchmark that advances video understanding, editing awareness, and comparative reasoning capabilities in multimodal AI systems. <div>
arXiv:2512.03405v1 Announce Type: new 
Abstract: Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOLOA: Real-Time Affordance Detection via LLM Adapter</title>
<link>https://arxiv.org/abs/2512.03418</link>
<guid>https://arxiv.org/abs/2512.03418</guid>
<content:encoded><![CDATA[
<div> affordance detection, embodied AI, YOLOA, large language model adapter, real-time performance<br /><br />Summary:  
This paper addresses the challenge of affordance detection in embodied AI, which requires understanding "what" an object is, "where" it is located, and "how" it can be used. Existing methods often focus only on the "how" aspect or treat object detection and affordance learning as independent tasks, limiting interaction and real-time feasibility. To overcome these issues, the authors propose YOLO Affordance (YOLOA), a novel real-time model that jointly performs object detection and affordance learning. YOLOA incorporates a lightweight detector with two branches refined through a large language model (LLM) adapter. The LLM adapter interacts with preliminary predictions during training, enhancing both branches by generating accurate class priors, bounding box offsets, and affordance gates. Experimental results on the relabeled ADG-Det and IIT-Heat benchmarks demonstrate YOLOA’s state-of-the-art accuracy, achieving 52.8 and 73.1 mean Average Precision (mAP) respectively. Additionally, YOLOA maintains real-time processing speeds of up to 89.77 frames per second (FPS), with a lightweight variant reaching up to 846.24 FPS. These results show that YOLOA successfully balances high accuracy with efficiency, making it suitable for real-time affordance detection applications in embodied AI. <div>
arXiv:2512.03418v1 Announce Type: new 
Abstract: Affordance detection aims to jointly address the fundamental "what-where-how" challenge in embodied AI by understanding "what" an object is, "where" the object is located, and "how" it can be used. However, most affordance learning methods focus solely on "how" objects can be used while neglecting the "what" and "where" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DM3D: Deformable Mamba via Offset-Guided Gaussian Sequencing for Point Cloud Understanding</title>
<link>https://arxiv.org/abs/2512.03424</link>
<guid>https://arxiv.org/abs/2512.03424</guid>
<content:encoded><![CDATA[
<div> Keywords: State Space Models, Point Clouds, Deformable Mamba, Gaussian Sequencing, Adaptive Serialization<br /><br />Summary: State Space Models (SSMs) have shown great potential for modeling long sequences but face challenges when applied to point clouds due to their dependence on input order, which conflicts with the irregular and unordered nature of point clouds. To address this, the paper proposes DM3D, a novel deformable Mamba architecture designed specifically for point cloud understanding. DM3D introduces an offset-guided Gaussian sequencing mechanism that integrates local resampling and global reordering into a single deformable scan process, enabling adaptive serialization of point clouds. The architecture includes two key components: Gaussian-based KNN Resampling (GKR), which adaptively reorganizes neighboring points to enhance structural awareness, and Gaussian-based Differentiable Reordering (GDR), which allows for end-to-end optimization of the serialization order. Additionally, the Tri-Path Frequency Fusion module improves feature complementarity and reduces aliasing effects. Together, these innovations enable DM3D to adaptively serialize point clouds based on their geometric structure, unlocking the potential of SSMs for this domain. Extensive experiments on benchmark datasets demonstrate that DM3D achieves state-of-the-art results in classification, few-shot learning, and part segmentation tasks, validating the effectiveness of adaptive serialization for point cloud understanding. <div>
arXiv:2512.03424v1 Announce Type: new 
Abstract: State Space Models (SSMs) demonstrate significant potential for long-sequence modeling, but their reliance on input order conflicts with the irregular nature of point clouds. Existing approaches often rely on predefined serialization strategies, which cannot adjust based on diverse geometric structures. To overcome this limitation, we propose \textbf{DM3D}, a deformable Mamba architecture for point cloud understanding. Specifically, DM3D introduces an offset-guided Gaussian sequencing mechanism that unifies local resampling and global reordering within a deformable scan. The Gaussian-based KNN Resampling (GKR) enhances structural awareness by adaptively reorganizing neighboring points, while the Gaussian-based Differentiable Reordering (GDR) enables end-to-end optimization of serialization order. Furthermore, a Tri-Path Frequency Fusion module enhances feature complementarity and reduces aliasing. Together, these components enable structure-adaptive serialization of point clouds. Extensive experiments on benchmark datasets show that DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications</title>
<link>https://arxiv.org/abs/2512.03427</link>
<guid>https://arxiv.org/abs/2512.03427</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV forestry, depth estimation, zero-shot evaluation, stereo methods, cross-domain generalization

<br /><br />Summary:  
This paper addresses the need for robust depth estimation methods tailored for autonomous UAV operations in dense vegetation environments, an area underrepresented in existing evaluations that focus mainly on urban and indoor scenes. The authors conduct the first systematic zero-shot evaluation of eight state-of-the-art stereo depth estimation techniques: RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM, and two baseline methods ACVNet and PSMNet, all trained exclusively on the Scene Flow dataset. The evaluation spans four standard benchmarks—ETH3D, KITTI 2012/2015, Middlebury—and introduces a new forestry dataset from Canterbury, consisting of 5,313 stereo pairs captured with a ZED Mini camera at 1920x1080 resolution. Results show distinct performance patterns depending on scene structure: foundation models excel in structured scenes with low pixel error, while iterative refinement-based methods demonstrate greater cross-domain robustness. A critical failure is noted for RAFT-Stereo on ETH3D caused by negative disparity predictions, although it performs well on KITTI. Qualitative analysis on the Canterbury forestry dataset highlights DEFOM as the preferred baseline for vegetation depth estimation due to superior smoothness, occlusion handling, and consistency across domains. IGEV++ provides finer detail preservation but lacks some of DEFOM’s robustness. <div>
arXiv:2512.03427v1 Announce Type: new 
Abstract: Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization. However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments. We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms. All methods are trained exclusively on Scene Flow and evaluated without fine-tuning on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury forestry dataset captured with ZED Mini camera (1920x1080). Performance reveals scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D, 0.83-1.07 px on KITTI; DEFOM: 0.35-4.65 px across benchmarks), while iterative methods maintain cross-domain robustness (IGEV++: 0.36-6.77 px; IGEV: 0.33-21.91 px). Critical finding: RAFT-Stereo exhibits catastrophic ETH3D failure (26.23 px EPE, 98 percent error rate) due to negative disparity predictions, while performing normally on KITTI (0.90-1.11 px). Qualitative evaluation on Canterbury forestry dataset identifies DEFOM as the optimal gold-standard baseline for vegetation depth estimation, exhibiting superior depth smoothness, occlusion handling, and cross-domain consistency compared to IGEV++, despite IGEV++'s finer detail preservation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-Efficient Hyperspectral Image Classification via Spectral FiLM Modulation of Low-Level Pretrained Diffusion Features</title>
<link>https://arxiv.org/abs/2512.03430</link>
<guid>https://arxiv.org/abs/2512.03430</guid>
<content:encoded><![CDATA[
<div> Hyperspectral Imaging, Diffusion Models, Label Efficiency, Multimodal Fusion, Remote Sensing<br /><br />Summary:<br /><br />1. The paper addresses challenges in hyperspectral imaging (HSI) such as low spatial resolution and sparse annotation labels, which hinder detailed land cover classification.<br /><br />2. It proposes a label-efficient framework leveraging spatial features extracted from a frozen diffusion model pretrained on natural images, which provides robust low-level representations relevant for HSI despite domain differences.<br /><br />3. Key to their approach is the use of high-resolution decoder layers at early denoising steps of the diffusion model, capturing low-texture spatial structures typical in hyperspectral data.<br /><br />4. To effectively integrate spectral and spatial modalities, the authors introduce a lightweight Feature-wise Linear Modulation (FiLM) based fusion module that adaptively modulates the frozen spatial features using spectral information.<br /><br />5. Experimental validation on two recent hyperspectral datasets shows the method outperforms state-of-the-art models under sparse supervision, and ablation studies confirm the importance of diffusion-derived features and their spectral-aware fusion design.<br /><br />6. The results indicate pretrained diffusion models as powerful, domain-agnostic tools for label-efficient representation learning in remote sensing and potentially other scientific imaging fields. <div>
arXiv:2512.03430v1 Announce Type: new 
Abstract: Hyperspectral imaging (HSI) enables detailed land cover classification, yet low spatial resolution and sparse annotations pose significant challenges. We present a label-efficient framework that leverages spatial features from a frozen diffusion model pretrained on natural images. Our approach extracts low-level representations from high-resolution decoder layers at early denoising timesteps, which transfer effectively to the low-texture structure of HSI. To integrate spectral and spatial information, we introduce a lightweight FiLM-based fusion module that adaptively modulates frozen spatial features using spectral cues, enabling robust multimodal learning under sparse supervision. Experiments on two recent hyperspectral datasets demonstrate that our method outperforms state-of-the-art approaches using only the provided sparse training labels. Ablation studies further highlight the benefits of diffusion-derived features and spectral-aware fusion. Overall, our results indicate that pretrained diffusion models can support domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation</title>
<link>https://arxiv.org/abs/2512.03445</link>
<guid>https://arxiv.org/abs/2512.03445</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language pretraining, Multi-Agent data generation, Ontology-based knowledge, Medical image analysis, Dermatology<br /><br />Summary:<br />1. This study addresses challenges in vision-language pretraining (VLP) for medical image analysis, particularly the noise in web-collected data and the complexity of long, unstructured medical texts.<br />2. The authors propose a novel VLP framework integrating two main components: a Multi-Agent data GENeration system (MAGEN) and an Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining method.<br />3. MAGEN improves data quality by generating knowledge-enriched descriptions using a foundation model-assisted captioning approach paired with a retrieval-based verification process.<br />4. O-MAKE tackles the difficulty of learning from lengthy, unstructured text by decomposing it into multiple knowledge aspects, enabling fine-grained alignment between images and text at both global and patch levels while explicitly modeling relationships between medical concepts using ontology-guided mechanisms.<br />5. The framework is validated in dermatology through extensive experiments, achieving state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight different datasets.<br />6. The authors will release their code and an augmented dataset named Derm1M-AgentAug, containing over 400,000 skin-image-text pairs, to support further research in this domain. <div>
arXiv:2512.03445v1 Announce Type: new 
Abstract: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis</title>
<link>https://arxiv.org/abs/2512.03449</link>
<guid>https://arxiv.org/abs/2512.03449</guid>
<content:encoded><![CDATA[
<div> Keywords: knee MRI, cartilage segmentation, radiomics, lateral/medial compartmentalisation, osteoarthritis  

<br /><br />Summary:  
This study introduces LM-CartSeg, a fully automatic pipeline designed for robust cartilage and subchondral bone segmentation in knee MRI, integrating geometric lateral/medial compartmentalisation and radiomics analysis. Two 3D nnU-Net models were trained on two datasets (SKM-TEA with 138 knees and OAIZIB-CM with 404 knees) and combined with geometric post-processing steps such as connected-component cleaning, construction of 10 mm subchondral bone bands, and a data-driven tibial lateral/medial split using PCA and k-means clustering. The method was evaluated on independent test datasets (OAIZIB-CM with 103 knees and SKI-10 with 100 knees), showing significant improvement in segmentation accuracy with a macro average symmetric surface distance (ASSD) reduction from 2.63 mm to 0.36 mm and Hausdorff distance (HD95) reduction from 25.2 mm to 3.35 mm, achieving a Dice similarity coefficient (DSC) of 0.91; zero-shot performance on SKI-10 had DSC of 0.80. The geometric lateral/medial compartmentalisation proved more stable and reliable than a direct nnU-Net classifier, which showed domain-dependent inaccuracies. Quality control used volume and thickness signatures to ensure ROI consistency. Radiomic analysis demonstrated that only a small portion (6–12%) of features correlated with size metrics, and radiomics models focusing on these size-linked features performed differently. Overall, LM-CartSeg provides automatic, quality-controlled ROIs and radiomic features that extend beyond simple morphometry, offering a practical tool for multi-centre knee osteoarthritis radiomics research. <div>
arXiv:2512.03449v1 Announce Type: new 
Abstract: Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03450</link>
<guid>https://arxiv.org/abs/2512.03450</guid>
<content:encoded><![CDATA[
<div> 3D keypoints, unsupervised learning, point cloud, Elucidated Diffusion Model, generative models<br /><br />Summary:<br /><br />This paper addresses the challenge of understanding and representing 3D object structures in an unsupervised manner, focusing on learning spatially structured 3D keypoints from point cloud data. Unlike most existing methods, which are not suited for unconditional generative setups, the proposed framework bridges this gap by integrating learned keypoints with an Elucidated Diffusion Model (EDM) for full shape reconstruction. The keypoints serve as a compact and interpretable representation that retains consistent spatial structure across different object instances, making them reliable and meaningful for 3D shape understanding. Moreover, the keypoints enable smooth interpolation within their latent space, highlighting their effectiveness in capturing geometric variations across shapes. Experimental results demonstrate strong performance across a range of diverse object categories, with the approach surpassing previous methods by a significant margin—achieving a 6 percentage-point increase in keypoint consistency. This advancement suggests that the framework not only improves unsupervised keypoint extraction but also enhances the integration of keypoints into modern generative 3D models, potentially benefiting various applications in computer vision and graphics. <div>
arXiv:2512.03450v1 Announce Type: new 
Abstract: Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.03451</link>
<guid>https://arxiv.org/abs/2512.03451</guid>
<content:encoded><![CDATA[
<div> Diffusion models, video generation, classifier-free guidance, GalaxyDiT, computational efficiency  

<br /><br />Summary:  
The paper addresses the computational inefficiency of diffusion models used in video generation, particularly those based on transformer architectures (DiTs) and utilizing classifier-free guidance (CFG), which requires extensive iterative computation and doubles compute needs. To mitigate this challenge, the authors propose GalaxyDiT, a training-free acceleration method that leverages guidance alignment and systematic proxy selection through rank-order correlation analysis. This approach identifies optimal proxies for different video models, accommodating various model families and parameter scales to maximize computational reuse. Experiments demonstrate significant speedups of 1.87× and 2.37× on Wan2.1-1.3B and Wan2.1-14B models respectively, with minimal performance degradation of only 0.97% and 0.72% on the VBench-2.0 benchmark. Furthermore, GalaxyDiT outperforms previous state-of-the-art methods by maintaining higher fidelity at elevated speedup rates, reflected in a 5 to 10 dB improvement in peak signal-to-noise ratio (PSNR). Overall, this work presents a practical and effective solution to accelerate video diffusion models without retraining, thereby facilitating wider applicability in downstream video generation tasks. <div>
arXiv:2512.03451v1 Announce Type: new 
Abstract: Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.
  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\times$ and $2.37\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoVideo: Introducing Geometric Regularization into Video Generation Model</title>
<link>https://arxiv.org/abs/2512.03453</link>
<guid>https://arxiv.org/abs/2512.03453</guid>
<content:encoded><![CDATA[
<div> Depth prediction, latent diffusion, video generation, geometric consistency, multi-view loss<br /><br />Summary:<br /><br />This paper addresses the challenge of 3D structural consistency in video generation, which most current 2D pixel-based diffusion transformer models lack. The authors introduce geometric regularization by incorporating per-frame depth prediction into latent diffusion models, leveraging advances in depth estimation and their synergy with image-based latent encoders. To maintain temporal structural coherence, a novel multi-view geometric loss is proposed, aligning predicted depth maps across frames within a unified 3D coordinate system. This approach tightly integrates appearance synthesis with explicit 3D modeling, improving spatio-temporal coherence and shape stability in generated videos. Experiments on several benchmark datasets demonstrate that this method significantly reduces temporal artifacts, implausible motions, and structural inconsistencies compared to existing video generation baselines. Overall, the work bridges the gap between 2D video synthesis and 3D geometry understanding, resulting in more physically plausible and visually stable video outputs. <div>
arXiv:2512.03453v1 Announce Type: new 
Abstract: Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.03454</link>
<guid>https://arxiv.org/abs/2512.03454</guid>
<content:encoded><![CDATA[
<div> Spatial-Aware World Model, Visual Grounding, Autonomous Driving, Hypergraph Decoder, Retrieval-Augmented Generation  

<br /><br />Summary:  
This paper addresses the challenge of interpreting natural-language commands to localize target objects for autonomous driving (AD), highlighting the limitations of current visual grounding (VG) methods that fail to reason about 3D spatial relations and future scene evolution. The authors introduce ThinkDeeper, a novel framework that anticipates future spatial states before making localization decisions. Central to this approach is the Spatial-Aware World Model (SA-WM), which distills the current scene into a command-aware latent representation and rolls out future latent states, enabling forward-looking disambiguation cues. To enhance reasoning over complex spatial dependencies, a hypergraph-guided decoder hierarchically fuses these future states with multimodal inputs. Additionally, the paper presents DrivePilot, a new multi-source VG dataset for autonomous vehicles, assembled with semantic annotations generated via a Retrieval-Augmented Generation (RAG) combined with Chain-of-Thought (CoT) prompted large language models (LLMs). Extensive evaluations on six benchmarks demonstrate ThinkDeeper's superior performance, ranking first on the Talk2Car leaderboard and outperforming state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g datasets. The framework also proves robust and efficient in challenging scenarios involving long commands, multiple agents, and ambiguous instructions, maintaining high accuracy even when trained with only half of the available data. <div>
arXiv:2512.03454v1 Announce Type: new 
Abstract: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.03463</link>
<guid>https://arxiv.org/abs/2512.03463</guid>
<content:encoded><![CDATA[
<div> Text-centric training, vision-language models, synthetic images, Text-Printed Image, data augmentation<br /><br />Summary:<br /><br />1. Vision-language models (LVLMs) typically require large image-text datasets for effective performance on visual question answering (VQA) tasks, which are costly and labor-intensive to obtain.<br />2. The authors propose text-centric training, a novel paradigm that relies solely on textual descriptions without the need for real images, making data collection more scalable, affordable, and less restricted by privacy or niche domain scarcity.<br />3. A key challenge is the modality gap between text and images that limits gains when training on text alone.<br />4. To bridge this gap, the paper introduces the Text-Printed Image (TPI) method, which creates synthetic images by rendering textual descriptions directly onto plain white canvases, preserving text semantics better than conventional text-to-image generation.<br />5. Extensive experiments across four different LVLMs and seven benchmarks show that TPI-enhanced text-centric training outperforms synthetic images generated by diffusion models.<br />6. Additionally, TPI serves as an effective, low-cost data augmentation technique.<br />7. Overall, the study highlights the promising potential of text-centric training and automated synthetic data generation to support scalable, cost-effective improvements in LVLM development. <div>
arXiv:2512.03463v1 Announce Type: new 
Abstract: Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Difference Decomposition Networks for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2512.03470</link>
<guid>https://arxiv.org/abs/2512.03470</guid>
<content:encoded><![CDATA[
<div> Keywords: Infrared small target detection, Basis Decomposition Module, Spatial Difference, Temporal Difference, U-shaped architecture<br /><br />Summary:<br /><br />The article addresses the challenges in Infrared Small Target Detection (ISTD), mainly the lack of clear target texture and significant background clutter that obscures targets. To overcome these challenges, the authors introduce the Basis Decomposition Module (BDM), a lightweight and extensible module that decomposes complex features into several basis features, enhancing useful information while reducing redundancy. Building upon BDM, several specialized modules are developed: the Spatial Difference Decomposition Module (SD²M), the Spatial Difference Decomposition Downsampling Module (SD³M), and the Temporal Difference Decomposition Module (TD²M). These modules are integrated into two networks: Spatial Difference Decomposition Network (SD²Net) for single-frame ISTD (SISTD) and Spatiotemporal Difference Decomposition Network (STD²Net) for multi-frame ISTD (MISTD). SD²Net uses SD²M and SD³M within a modified U-shaped architecture, and STD²Net incorporates TD²M to include motion information from multiple frames. Experimental results demonstrate state-of-the-art performance in both SISTD and MISTD tasks. Specifically, STD²Net significantly outperforms SD²Net on MISTD datasets, achieving a mean Intersection over Union (mIoU) of 87.68% versus 64.97%. The code for these networks is publicly available on GitHub. <div>
arXiv:2512.03470v1 Announce Type: new 
Abstract: Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Procedural Mistake Detection via Action Effect Modeling</title>
<link>https://arxiv.org/abs/2512.03474</link>
<guid>https://arxiv.org/abs/2512.03474</guid>
<content:encoded><![CDATA[
<div> Keywords: mistake detection, procedural tasks, action effect, effect-aware representations, one-class classification<br /><br />Summary:<br /><br />This paper addresses mistake detection in procedural tasks by focusing not only on how an action is performed but also on its resulting outcome, termed the action effect. The authors identify a limitation in existing methods that typically analyze execution alone, neglecting errors that appear only in the produced outcome such as unintended object states or spatial misarrangements. To overcome this, they propose Action Effect Modeling (AEM), a unified probabilistic framework that jointly captures both action execution and its outcomes. AEM selects the most informative effect frame for each action based on semantic relevance and visual quality, and integrates complementary visual and symbolic cues from visual grounding and scene graphs into a shared latent space for robust effect-aware representations. For mistake detection, a prompt-based detector is designed to incorporate task-specific prompts and align each action segment with its intended semantics. The approach achieves state-of-the-art results on EgoPER and CaptainCook4D benchmarks under the challenging one-class classification setting. The study demonstrates that modeling both execution and outcomes enhances mistake detection reliability and suggests that effect-aware representations have potential utility across various downstream applications. <div>
arXiv:2512.03474v1 Announce Type: new 
Abstract: Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis</title>
<link>https://arxiv.org/abs/2512.03477</link>
<guid>https://arxiv.org/abs/2512.03477</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, fairness, Low-Rank Adaptation, MaxAccGap loss, medical imaging<br /><br />Summary:<br />1. The paper addresses diagnostic accuracy disparities across demographic groups in vision-language models (VLMs) applied to medical imaging tasks.<br />2. It introduces a fairness-aware Low-Rank Adaptation (LoRA) approach for medical VLMs, which combines parameter efficiency with explicit fairness optimization.<br />3. The key contribution is the differentiable MaxAccGap loss function that enables end-to-end optimization focused on accuracy parity among different demographic groups.<br />4. Three methods are proposed: FR-LoRA (integrates MaxAccGap as a regularizer in training), GR-LoRA (uses inverse frequency weighting to balance gradients), and Hybrid-LoRA (combines both techniques).<br />5. The methods were evaluated on a dataset of 10,000 glaucoma fundus images, where GR-LoRA reduced accuracy disparities by 69% while maintaining an overall accuracy of 53.15%.<br />6. Ablation studies showed that stronger regularization improves fairness with minimal accuracy loss, and race-specific optimization achieved a 60% reduction in disparity.<br />7. The approach is parameter-efficient, requiring only 0.24% of trainable parameters, enabling deployment of fair medical AI in resource-limited healthcare settings. <div>
arXiv:2512.03477v1 Announce Type: new 
Abstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Object-centric Understanding for Instructional Videos</title>
<link>https://arxiv.org/abs/2512.03479</link>
<guid>https://arxiv.org/abs/2512.03479</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural activities, object-centric reasoning, instructional video benchmark, state transitions, vision-language models<br /><br />Summary:<br /><br />Understanding procedural activities is essential for developing advanced assistive AI capable of reasoning about complex real-world tasks. Traditional action-centric approaches fall short when task step orders vary depending on object states. This work introduces a shift towards an object-centric paradigm, viewing actions as drivers of state transitions in objects. To support this approach, the authors present Object-IVQA, a benchmark consisting of 107 long-form instructional videos with 514 open-ended question-answer pairs annotated with temporally grounded evidence. Object-IVQA evaluates four critical aspects of object-centric reasoning: state evolution, precondition verification, counterfactual reasoning, and mistake recognition. The paper further proposes an AI agent framework that integrates object-centric planning, perception, analysis, and generation tools. This framework facilitates explicit evidence retrieval and multi-hop reasoning across non-contiguous video segments. Experimental results demonstrate that existing large vision-language models struggle with object-level recognition and reasoning tasks in this domain. By contrast, the proposed framework substantially improves performance, highlighting the importance and effectiveness of object-centric approaches for procedural activity understanding and reasoning in instructional videos. <div>
arXiv:2512.03479v1 Announce Type: new 
Abstract: Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks. Existing action-centric methods struggle with the flexibility of real procedures, where step order varies depending on object states. In this work, we propose to shift the focus to an object-centric paradigm by regarding actions as mechanisms that drive state transitions. To advance this direction, we introduce Object-IVQA, a long-form instructional video benchmark with 107 videos and 514 open-ended question-answer pairs annotated with temporally grounded evidence. The benchmark evaluates four dimensions of object-centric reasoning, including state evolution, precondition verification, counterfactual reasoning and mistake recognition. We further propose an agent framework that orchestrates object-centric planning, perception, analysis and generation tools, enabling explicit evidence retrieval and multi-hop reasoning across disjoint segments. Experiments show that existing large vision-language models struggle in object-level recognition and reasoning, whereas our framework achieves substantially improvement.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation</title>
<link>https://arxiv.org/abs/2512.03499</link>
<guid>https://arxiv.org/abs/2512.03499</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, Low-Rank Adaptation, Neural Architecture Search, Parameter-Efficient Fine-Tuning, Vision Transformer  

<br /><br />Summary:  
The Segment Anything Model (SAM) is a robust visual foundation model for image segmentation but faces challenges when adapting to specialized fields like medical and agricultural imaging. To enhance SAM's adaptability, Low-Rank Adaptation (LoRA) and its variants are commonly used to fine-tune the model efficiently across different domains. However, a major limitation is that the Transformer encoder within SAM lacks spatial priors for image patches, which may restrict the learning of high-level semantic features necessary for domain-specific tasks. To overcome this, the paper introduces NAS-LoRA, a novel Parameter-Efficient Fine-Tuning (PEFT) approach that integrates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder in LoRA. This block dynamically optimizes the incorporation of prior knowledge into weight updates, addressing the semantic gap between SAM's pretrained features and the target domain. Additionally, a stage-wise optimization method is proposed to balance weight updating and architectural adjustments within the Vision Transformer encoder, promoting a gradual and effective acquisition of semantic understanding. Experimental results show that NAS-LoRA outperforms existing PEFT methods, achieving a 24.14% reduction in training cost without incurring additional inference overhead. This work demonstrates the promising role NAS can play in boosting PEFT techniques for visual foundation models. <div>
arXiv:2512.03499v1 Announce Type: new 
Abstract: The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEA: Exploration-Exploitation Agent for Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.03500</link>
<guid>https://arxiv.org/abs/2512.03500</guid>
<content:encoded><![CDATA[
<div> Keywords: long-form video understanding, exploration-exploitation balance, hierarchical tree search, semantic guidance, vision-language models  

<br /><br />Summary: This paper addresses the challenges in long-form video understanding, where efficiently navigating large volumes of visual data is critical to identify sparse but important content. Existing methods either incur high computational costs due to dense preprocessing or fail to effectively balance exploration and exploitation, leading to incomplete coverage and inefficiency. The authors propose EEA, a novel video agent framework that achieves a balance between exploration and exploitation through semantic guidance coupled with a hierarchical tree search process. EEA autonomously discovers and dynamically updates semantic queries relevant to the task, using these queries to collect video frames as semantic anchors. During the search, instead of uniformly expanding nodes, EEA prioritizes semantically relevant frames while maintaining adequate coverage in unexplored segments. Additionally, EEA incorporates intrinsic rewards derived from vision-language models combined with semantic priors and explicitly models uncertainty to ensure stable and accurate evaluation of video segments. Extensive experiments on multiple long-video benchmarks demonstrate that EEA outperforms existing approaches both in terms of performance and computational efficiency, validating the effectiveness of the proposed method. <div>
arXiv:2512.03500v1 Announce Type: new 
Abstract: Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.03508</link>
<guid>https://arxiv.org/abs/2512.03508</guid>
<content:encoded><![CDATA[
<div> Keywords: Domain Generalization, Semantic Segmentation, Vision-Language Models, Prompt Learning, Contrastive Learning<br /><br />Summary: This paper addresses the challenge of domain generalized semantic segmentation (DGSS) by tackling semantic misalignment between visual and textual contexts caused by fixed context prompts trained on a single source domain. The authors propose a new framework called Domain-aware Prompt-driven Masked Transformer (DPMFormer) to improve domain generalization. First, they introduce domain-aware prompt learning to better align visual features with textual cues, enhancing semantic understanding across domains. To handle domain-specific variations within a single source dataset, domain-aware contrastive learning is employed alongside texture perturbation techniques that simulate diverse observable domains. Finally, the framework incorporates domain-robust consistency learning to ensure stable predictions under different environmental conditions by minimizing discrepancies between original and augmented images. Experimental results demonstrate that DPMFormer outperforms existing methods on multiple DGSS benchmarks, setting a new state-of-the-art for robustness and accuracy. The paper also provides a publicly available implementation to facilitate further research and adoption. <div>
arXiv:2512.03508v1 Announce Type: new 
Abstract: Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model</title>
<link>https://arxiv.org/abs/2512.03509</link>
<guid>https://arxiv.org/abs/2512.03509</guid>
<content:encoded><![CDATA[
<div> Keywords: automated dance analysis, YOLOv8, Segment Anything Model, motion quantification, AfroBeats dance<br /><br />Summary:<br /><br />This paper introduces a preliminary investigation into automated analysis of dance movements leveraging modern computer vision techniques. A proof-of-concept framework is proposed that combines YOLOv8 and v11 models for dancer detection with the Segment Anything Model (SAM) for accurate segmentation, enabling markerless tracking and quantification of dance movements from video. The system identifies dancers in video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency throughout performances. The framework was tested on a single 49-second video of Ghanaian AfroBeats dance, achieving about 94% precision and 89% recall in dancer detection based on manual inspection. Pixel-level segmentation by SAM achieved approximately 83% intersection-over-union with visual verification, allowing for detailed motion quantification that surpasses bounding-box methods in capturing body configuration changes. Analysis revealed that the system-designated primary dancer performed 23% more steps, showed 37% higher motion intensity, and utilized 42% more performance space compared to secondary dancers. The study acknowledges significant limitations, including validation on only one video, absence of systematic ground truth data, and no comparison with established pose estimation approaches. This work aims to demonstrate technical feasibility, highlight promising quantitative dance metrics, and lay groundwork for future validation efforts. <div>
arXiv:2512.03509v1 Announce Type: new 
Abstract: This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSMapping: Scalable Crowdsourced Semantic Mapping and Topology Inference for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.03510</link>
<guid>https://arxiv.org/abs/2512.03510</guid>
<content:encoded><![CDATA[
<div> Keywords: Crowdsourcing, Semantic Mapping, Latent Diffusion Model, Topological Mapping, Autonomous Driving<br /><br />Summary:<br />1. The paper addresses the challenge of improving autonomous driving map quality using crowdsourced data that often suffers from noise due to low-cost sensors.<br />2. It introduces CSMapping, a novel system designed to generate accurate semantic maps and topological road centerlines, with map quality that improves as more crowdsourced data becomes available.<br />3. For semantic mapping, CSMapping leverages a latent diffusion model trained on high-definition (HD) maps, optionally conditioned on standard-definition (SD) maps, to learn a generative prior of real-world map structures without requiring paired crowdsourced and HD map data.<br />4. This generative prior is integrated via constrained maximum a posteriori (MAP) optimization in latent space, which enhances robustness against severe noise and allows plausible completion in areas with missing observations.<br />5. The initialization involves a robust vectorized mapping module followed by diffusion inversion, while optimization includes Gaussian-basis reparameterization, projected gradient descent with multi-start, and a latent-space factor graph to ensure global consistency.<br />6. For topological mapping, confidence-weighted k-medoids clustering and kinematic refinement are applied to trajectory data to produce smooth, human-like road centerlines that are robust to variations in crowdsourced trajectories.<br />7. Extensive experiments on the nuScenes, Argoverse 2, and a large proprietary dataset demonstrate state-of-the-art performance in both semantic and topological mapping, supplemented by thorough ablation and scalability analyses. <div>
arXiv:2512.03510v1 Announce Type: new 
Abstract: Crowdsourcing enables scalable autonomous driving map construction, but low-cost sensor noise hinders quality from improving with data volume. We propose CSMapping, a system that produces accurate semantic maps and topological road centerlines whose quality consistently increases with more crowdsourced data. For semantic mapping, we train a latent diffusion model on HD maps (optionally conditioned on SD maps) to learn a generative prior of real-world map structure, without requiring paired crowdsourced/HD-map supervision. This prior is incorporated via constrained MAP optimization in latent space, ensuring robustness to severe noise and plausible completion in unobserved areas. Initialization uses a robust vectorized mapping module followed by diffusion inversion; optimization employs efficient Gaussian-basis reparameterization, projected gradient descent zobracket multi-start, and latent-space factor-graph for global consistency. For topological mapping, we apply confidence-weighted k-medoids clustering and kinematic refinement to trajectories, yielding smooth, human-like centerlines robust to trajectory variation. Experiments on nuScenes, Argoverse 2, and a large proprietary dataset achieve state-of-the-art semantic and topological mapping performance, with thorough ablation and scalability studies.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation</title>
<link>https://arxiv.org/abs/2512.03520</link>
<guid>https://arxiv.org/abs/2512.03520</guid>
<content:encoded><![CDATA[
<div> Keywords: FloodDiffusion, human motion generation, diffusion forcing, streaming text-driven motion, time-varying control

<br /><br />Summary:  
The paper introduces FloodDiffusion, a novel framework designed for streaming human motion generation driven by time-varying text prompts. Unlike previous approaches that process motion in discrete chunks or use auto-regressive diffusion heads, FloodDiffusion employs a diffusion forcing framework tailored for continuous time-series generation under evolving control signals. The authors identify that the vanilla diffusion forcing method, previously used in video tasks, fails to capture the true distribution of human motion data when applied directly. To address this, three key modifications are proposed: (i) replacing causal attention with bi-directional attention during training for better temporal modeling, (ii) implementing a lower triangular time scheduler rather than a random scheduler to guide the diffusion process appropriately, and (iii) integrating text conditioning in a continuous, time-varying manner to align motions closely with the input prompts. These improvements enable FloodDiffusion to outmatch current state-of-the-art methods on the streaming motion generation task, achieving a superior Frechet Inception Distance (FID) score of 0.057 on the HumanML3D benchmark. The authors also provide access to models, code, and pretrained weights for broader use and reproducibility. <div>
arXiv:2512.03520v1 Announce Type: new 
Abstract: We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation</title>
<link>https://arxiv.org/abs/2512.03532</link>
<guid>https://arxiv.org/abs/2512.03532</guid>
<content:encoded><![CDATA[
<div> Open-vocabulary 3D instance segmentation, mesh-free environments, visual-spatial tracker, multi-modal large language model, compositional reasoning  

<br /><br />Summary:  
This paper addresses the challenge of generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments, which is important for applications in robotics and AR/VR. Existing methods fall short due to their reliance on dataset-specific proposal networks or mesh-based superpoints, limiting applicability and generalization in mesh-free scenarios. Additionally, CLIP-based classifiers have weak textual reasoning abilities and struggle with compositional and functional queries. To overcome these limitations, the authors propose OpenTrack3D, a novel and generalizable framework that constructs object proposals online using a visual-spatial tracker without relying on pre-generated proposals. Given an RGB-D input stream, the pipeline uses a 2D open-vocabulary segmenter to create masks, which are then lifted to 3D point clouds with depth information. Instance features guided by these masks are extracted using DINO feature maps, and the tracker fuses visual and spatial data to maintain consistent instances across views. The pipeline operates fully mesh-free but includes an optional superpoints refinement module if a scene mesh is available. Lastly, the method replaces CLIP with a multi-modal large language model (MLLM), significantly boosting compositional reasoning for complex user queries. Extensive experiments on ScanNet200, Replica, ScanNet++, and SceneFun3D datasets demonstrate state-of-the-art performance and strong generalization. <div>
arXiv:2512.03532v1 Announce Type: new 
Abstract: Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation</title>
<link>https://arxiv.org/abs/2512.03534</link>
<guid>https://arxiv.org/abs/2512.03534</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt redesign, inference-time scaling, text-to-visual generation, element-level factual correction, alignment feedback<br /><br />Summary:<br /><br />1. The paper addresses the challenge of achieving precise alignment between user prompts and generated visuals in text-to-visual generation tasks, where a single generation attempt often fails to meet the desired output.  
2. Prior approaches to improve generation quality mainly focus on scaling visual generation parameters such as sampling steps or random seeds but encounter a quality plateau because the guiding prompt remains fixed.  
3. To overcome this limitation, the authors propose PRIS (Prompt Redesign for Inference-time Scaling), a framework that adaptively revises prompts during inference based on analysis of the previously generated visuals.  
4. PRIS identifies recurring failure patterns across multiple visuals and redesigns the prompt accordingly before regenerating the visuals, enabling improved alignment between intent and output.  
5. A novel verifier, element-level factual correction, is introduced to provide fine-grained, interpretable feedback by evaluating alignment between specific prompt attributes and generated visuals, outperforming holistic evaluation methods.  
6. Extensive experiments on text-to-image and text-to-video benchmarks demonstrate the effectiveness of PRIS, showing up to a 15% improvement on the VBench 2.0 benchmark.  
7. The findings emphasize that jointly scaling both prompts and visual generation during inference is crucial for fully leveraging scaling laws, leading to better output quality and alignment. <div>
arXiv:2512.03534v1 Announce Type: new 
Abstract: Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation</title>
<link>https://arxiv.org/abs/2512.03540</link>
<guid>https://arxiv.org/abs/2512.03540</guid>
<content:encoded><![CDATA[
<div> Keywords: Cooking, Diffusion Models, Recipe Illustration, Step-wise Regional Control, Positional Encoding

<br /><br />Summary:  
The paper addresses the challenges of generating coherent and structured multi-step cooking illustrations using diffusion models. It highlights that existing text-to-image diffusion models struggle with multi-step, visually grounded tasks like recipe illustration and often generate a fixed number of images regardless of the recipe's length. To overcome these limitations, the authors propose CookAnything, a diffusion-based framework capable of producing semantically consistent image sequences aligned with each textual cooking instruction step, irrespective of recipe length. CookAnything introduces three novel components: Step-wise Regional Control (SRC) to spatially align individual cooking steps within a single denoising process; Flexible RoPE, a step-aware positional encoding method that improves both temporal coherence across steps and spatial diversity within images; and Cross-Step Consistency Control (CSCC) to ensure fine-grained ingredient consistency throughout the sequence. Experimental evaluations on recipe illustration benchmarks demonstrate that CookAnything outperforms existing methods in both training-based and training-free settings. This framework enables scalable, high-quality visual synthesis for complex procedural instructions and has significant potential applications in instructional media and procedural content creation domains. <div>
arXiv:2512.03540v1 Announce Type: new 
Abstract: Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention</title>
<link>https://arxiv.org/abs/2512.03542</link>
<guid>https://arxiv.org/abs/2512.03542</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, visual neglect, hallucinations, visual inference-time intervention, V-ITI  

<br /><br />Summary:  
Multimodal Large Language Models (MLLMs) perform well on vision-language tasks but often generate hallucinations—content that contradicts the input images—compromising their reliability in precise domains. This problem arises from visual neglect, where models insufficiently prioritize the input visuals during processing. Prior solutions mostly focus on "how to intervene" by adjusting attention scores or output logits but neglect the crucial aspect of "when to intervene," leading to over-intervention issues that cause new hallucinations and increase computational costs. Addressing this, the authors analyze the visual neglect mechanism and find it can be reliably detected through head-level activation patterns in MLLMs. Based on this insight, they propose V-ITI, a lightweight framework consisting of a Visual Neglect Detector and a Visual Recall Intervenor. The detector uses discriminative probes to identify visual neglect during inference, while the recall intervenor selectively modifies activations with stored visual information only when neglect is detected. Experiments across eight benchmarks and multiple MLLM families show that V-ITI effectively reduces vision-related hallucinations without sacrificing general task performance, demonstrating its efficiency and robustness in improving the reliability of vision-language models. <div>
arXiv:2512.03542v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on "how to intervene" but overlooking the prerequisite "when to intervene", which leads to the "over-intervention" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching</title>
<link>https://arxiv.org/abs/2512.03553</link>
<guid>https://arxiv.org/abs/2512.03553</guid>
<content:encoded><![CDATA[
<div> Content Moderation, Livestreaming, Multimodal Large Language Model, Hybrid Framework, Similarity Matching<br /><br />Summary:<br /><br />This paper addresses the critical challenge of content moderation on large-scale user-generated livestreaming platforms, emphasizing the need for timely, multimodal, and robust approaches. The authors propose a hybrid moderation framework that integrates supervised classification for detecting known violations with reference-based similarity matching designed to catch novel or subtle cases that traditional classifiers might miss. The system processes multimodal inputs, including text, audio, and visual data, via two pipelines enhanced by a multimodal large language model (MLLM), which distills knowledge to improve accuracy while keeping inference efficient. In production settings, the classification pipeline achieves a recall of 67% at 80% precision, while the similarity matching pipeline performs better with 76% recall at the same precision level. Large-scale A/B testing demonstrates the framework’s effectiveness by reducing user exposure to unwanted livestream content by 6-8%. Overall, the proposed solution offers a scalable, adaptable, and multimodal strategy for content governance capable of managing both explicit policy violations and emerging adversarial behaviors in dynamic livestream environments. <div>
arXiv:2512.03553v1 Announce Type: new 
Abstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding</title>
<link>https://arxiv.org/abs/2512.03558</link>
<guid>https://arxiv.org/abs/2512.03558</guid>
<content:encoded><![CDATA[
<div> cartographic maps, visual-language models, question answering, geospatial reasoning, OCR errors<br /><br />Summary:<br /><br />The paper introduces CartoMapQA, a novel benchmark created to assess Visual-Language Models’ (LVLMs) capabilities in understanding cartographic maps via question-answering tasks. The dataset comprises over 2,000 samples, each including a map, a question—either open-ended or multiple-choice—and a verified correct answer. Tasks cover a range of map interpretation skills, from basic symbol recognition to complex route-based reasoning, scale interpretation, and extraction of embedded information. Evaluations conducted on both open-source and proprietary LVLMs reveal persistent challenges, including difficulties in grasping map-specific semantics and limitations in geospatial reasoning abilities. Additionally, models frequently suffer from OCR-related errors when interpreting map text. By pinpointing these shortcomings, CartoMapQA serves as a valuable benchmark to inform future LVLM architectural improvements tailored for cartographic data. Ultimately, this work supports advances in creating more robust and reliable models capable of addressing real-world applications such as navigation, geographic information retrieval, and urban planning. The authors have made the source code and dataset publicly accessible, encouraging further research and development in this specialized domain. <div>
arXiv:2512.03558v1 Announce Type: new 
Abstract: The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAOT: Generating Articulated Objects Through Text-Guided Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03566</link>
<guid>https://arxiv.org/abs/2512.03566</guid>
<content:encoded><![CDATA[
<div> Articulated object generation, text conditioning, diffusion models, hypergraph learning, PartNet-Mobility<br /><br />Summary: This paper introduces GAOT, a novel three-phase framework designed to generate 3D articulated objects from text prompts. The method addresses the challenge of linking textual descriptions to detailed 3D object representations. In the first phase, a point cloud generation model is fine-tuned to create a coarse object structure based on the input text. The second phase employs a hypergraph-based learning approach, which refines the initial representation by modeling object parts as graph vertices, effectively capturing the intrinsic connection between articulated objects and graph structures. The final phase focuses on generating the joints of the articulated objects—represented as graph edges—using a diffusion model that operates on the refined graph. The approach was extensively evaluated on the PartNet-Mobility dataset, showing both qualitative and quantitative improvements over previous methods. Overall, GAOT successfully bridges the gap between textual prompts and complex articulated object generation by integrating diffusion techniques with graph-based learning, achieving superior object articulation and structural fidelity. <div>
arXiv:2512.03566v1 Announce Type: new 
Abstract: Articulated object generation has seen increasing advancements, yet existing models often lack the ability to be conditioned on text prompts. To address the significant gap between textual descriptions and 3D articulated object representations, we propose GAOT, a three-phase framework that generates articulated objects from text prompts, leveraging diffusion models and hypergraph learning in a three-step process. First, we fine-tune a point cloud generation model to produce a coarse representation of objects from text prompts. Given the inherent connection between articulated objects and graph structures, we design a hypergraph-based learning method to refine these coarse representations, representing object parts as graph vertices. Finally, leveraging a diffusion model, the joints of articulated objects-represented as graph edges-are generated based on the object parts. Extensive qualitative and quantitative experiments on the PartNet-Mobility dataset demonstrate the effectiveness of our approach, achieving superior performance over previous methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global-Local Aware Scene Text Editing</title>
<link>https://arxiv.org/abs/2512.03574</link>
<guid>https://arxiv.org/abs/2512.03574</guid>
<content:encoded><![CDATA[
<div> Keywords: Scene Text Editing, Global-Local Features, Text Style Consistency, Length Insensitivity, Affine Fusion<br /><br />Summary: Scene Text Editing (STE) aims to replace text within scene images while preserving the original text style and background texture. Existing STE methods face two critical problems: inconsistency between edited local areas and their surrounding context, and difficulty handling variations in text length during editing. To address these, the paper introduces GLASTE, an end-to-end framework that integrates both global contextual information and fine-grained local features. GLASTE employs a novel global-local combination structure alongside joint global and local loss functions to maintain style consistency within local patches and ensure harmony between edited regions and the broader image. The framework represents text style as a vector independent of image size, allowing style transfer across target text images of different dimensions. An affine fusion technique is used to insert the target text into the editing patch while preserving the aspect ratio. Extensive experiments on real-world datasets demonstrate that GLASTE surpasses previous approaches in quantitative evaluation metrics and qualitative visual fidelity. Moreover, it effectively tackles the challenges of context inconsistency and length insensitivity, making it a robust solution for scene text editing tasks. <div>
arXiv:2512.03574v1 Announce Type: new 
Abstract: Scene Text Editing (STE) involves replacing text in a scene image with new target text while preserving both the original text style and background texture. Existing methods suffer from two major challenges: inconsistency and length-insensitivity. They often fail to maintain coherence between the edited local patch and the surrounding area, and they struggle to handle significant differences in text length before and after editing. To tackle these challenges, we propose an end-to-end framework called Global-Local Aware Scene Text Editing (GLASTE), which simultaneously incorporates high-level global contextual information along with delicate local features. Specifically, we design a global-local combination structure, joint global and local losses, and enhance text image features to ensure consistency in text style within local patches while maintaining harmony between local and global areas. Additionally, we express the text style as a vector independent of the image size, which can be transferred to target text images of various sizes. We use an affine fusion to fill target text images into the editing patch while maintaining their aspect ratio unchanged. Extensive experiments on real-world datasets validate that our GLASTE model outperforms previous methods in both quantitative metrics and qualitative results and effectively mitigates the two challenges.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniComp: Rethinking Video Compression Through Informational Uniqueness</title>
<link>https://arxiv.org/abs/2512.03575</link>
<guid>https://arxiv.org/abs/2512.03575</guid>
<content:encoded><![CDATA[
arXiv:2512.03575v1 Announce Type: new 
Abstract: Distinct from attention-based compression methods, this paper presents an information uniqueness driven video compression framework, termed UniComp, which aims to maximize the information fidelity of video representations under constrained computational budgets. Starting from the information-theoretic perspective, we formulate the vision compression as an optimization problem that minimizes conditional entropy (reconstruction error) between retained and full tokens. To achieve this, we introduce the notion of information uniqueness to measure intrinsic redundancy among tokens to link with reconstruction error. Based on uniqueness, we design three modules-Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression-that progressively perform semantic frame grouping, adaptive resource allocation, and fine-grained spatial compression. Extensive experiments demonstrate that UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets, highlighting the pivotal role of information uniqueness in token compression efficacy.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning</title>
<link>https://arxiv.org/abs/2512.03577</link>
<guid>https://arxiv.org/abs/2512.03577</guid>
<content:encoded><![CDATA[
arXiv:2512.03577v1 Announce Type: new 
Abstract: Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&amp;E enriches H&amp;E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&amp;E, HER2, KI67, ER, PGR) to enable paired H&amp;E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&amp;E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&amp;E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes</title>
<link>https://arxiv.org/abs/2512.03580</link>
<guid>https://arxiv.org/abs/2512.03580</guid>
<content:encoded><![CDATA[
arXiv:2512.03580v1 Announce Type: new 
Abstract: We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Boundary Frames: Audio-Visual Semantic Guidance for Context-Aware Video Interpolation</title>
<link>https://arxiv.org/abs/2512.03590</link>
<guid>https://arxiv.org/abs/2512.03590</guid>
<content:encoded><![CDATA[
arXiv:2512.03590v1 Announce Type: new 
Abstract: Handling fast, complex, and highly non-linear motion patterns has long posed challenges for video frame interpolation. Although recent diffusion-based approaches improve upon traditional optical-flow-based methods, they still struggle to cover diverse application scenarios and often fail to produce sharp, temporally consistent frames in fine-grained motion tasks such as audio-visual synchronized interpolation. To address these limitations, we introduce BBF (Beyond Boundary Frames), a context-aware video frame interpolation framework, which could be guided by audio/visual semantics. First, we enhance the input design of the interpolation model so that it can flexibly handle multiple conditional modalities, including text, audio, images, and video. Second, we propose a decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. Finally, to maintain the generation abilities of the foundation model, we adopt a progressive multi-stage training paradigm, where the start-end frame difference embedding is used to dynamically adjust both the data sampling and the loss weighting. Extensive experimental results demonstrate that BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Hypergraphs in Geometric Deep Learning for 3D RNA Inverse Folding</title>
<link>https://arxiv.org/abs/2512.03592</link>
<guid>https://arxiv.org/abs/2512.03592</guid>
<content:encoded><![CDATA[
arXiv:2512.03592v1 Announce Type: new 
Abstract: The RNA inverse folding problem, a key challenge in RNA design, involves identifying nucleotide sequences that can fold into desired secondary structures, which are critical for ensuring molecular stability and function. The inherent complexity of this task stems from the intricate relationship between sequence and structure, making it particularly challenging. In this paper, we propose a framework, named HyperRNA, a generative model with an encoder-decoder architecture that leverages hypergraphs to design RNA sequences. Specifically, our HyperRNA model consists of three main components: preprocessing, encoding and decoding.
  In the preprocessing stage, graph structures are constructed by extracting the atom coordinates of RNA backbone based on 3-bead coarse-grained representation. The encoding stage processes these graphs, capturing higher order dependencies and complex biomolecular interactions using an attention embedding module and a hypergraph-based encoder. Finally, the decoding stage generates the RNA sequence in an autoregressive manner. We conducted quantitative and qualitative experiments on the PDBBind and RNAsolo datasets to evaluate the inverse folding task for RNA sequence generation and RNA-protein complex sequence generation. The experimental results demonstrate that HyperRNA not only outperforms existing RNA design methods but also highlights the potential of leveraging hypergraphs in RNA engineering.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CloseUpAvatar: High-Fidelity Animatable Full-Body Avatars with Mixture of Multi-Scale Textures</title>
<link>https://arxiv.org/abs/2512.03593</link>
<guid>https://arxiv.org/abs/2512.03593</guid>
<content:encoded><![CDATA[
arXiv:2512.03593v1 Announce Type: new 
Abstract: We present a CloseUpAvatar - a novel approach for articulated human avatar representation dealing with more general camera motions, while preserving rendering quality for close-up views. CloseUpAvatar represents an avatar as a set of textured planes with two sets of learnable textures for low and high-frequency detail. The method automatically switches to high-frequency textures only for cameras positioned close to the avatar's surface and gradually reduces their impact as the camera moves farther away. Such parametrization of the avatar enables CloseUpAvatar to adjust rendering quality based on camera distance ensuring realistic rendering across a wider range of camera orientations than previous approaches. We provide experiments using the ActorsHQ dataset with high-resolution input images. CloseUpAvatar demonstrates both qualitative and quantitative improvements over existing methods in rendering from novel wide range camera positions, while maintaining high FPS by limiting the number of required primitives.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HBFormer: A Hybrid-Bridge Transformer for Microtumor and Miniature Organ Segmentation</title>
<link>https://arxiv.org/abs/2512.03597</link>
<guid>https://arxiv.org/abs/2512.03597</guid>
<content:encoded><![CDATA[
arXiv:2512.03597v1 Announce Type: new 
Abstract: Medical image segmentation is a cornerstone of modern clinical diagnostics. While Vision Transformers that leverage shifted window-based self-attention have established new benchmarks in this field, they are often hampered by a critical limitation: their localized attention mechanism struggles to effectively fuse local details with global context. This deficiency is particularly detrimental to challenging tasks such as the segmentation of microtumors and miniature organs, where both fine-grained boundary definition and broad contextual understanding are paramount. To address this gap, we propose HBFormer, a novel Hybrid-Bridge Transformer architecture. The 'Hybrid' design of HBFormer synergizes a classic U-shaped encoder-decoder framework with a powerful Swin Transformer backbone for robust hierarchical feature extraction. The core innovation lies in its 'Bridge' mechanism, a sophisticated nexus for multi-scale feature integration. This bridge is architecturally embodied by our novel Multi-Scale Feature Fusion (MFF) decoder. Departing from conventional symmetric designs, the MFF decoder is engineered to fuse multi-scale features from the encoder with global contextual information. It achieves this through a synergistic combination of channel and spatial attention modules, which are constructed from a series of dilated and depth-wise convolutions. These components work in concert to create a powerful feature bridge that explicitly captures long-range dependencies and refines object boundaries with exceptional precision. Comprehensive experiments on challenging medical image segmentation datasets, including multi-organ, liver tumor, and bladder tumor benchmarks, demonstrate that HBFormer achieves state-of-the-art results, showcasing its outstanding capabilities in microtumor and miniature organ segmentation. Code and models are available at: https://github.com/lzeeorno/HBFormer.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Guided Point Cloud Completion for Dental Reconstruction</title>
<link>https://arxiv.org/abs/2512.03598</link>
<guid>https://arxiv.org/abs/2512.03598</guid>
<content:encoded><![CDATA[
arXiv:2512.03598v1 Announce Type: new 
Abstract: Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding</title>
<link>https://arxiv.org/abs/2512.03601</link>
<guid>https://arxiv.org/abs/2512.03601</guid>
<content:encoded><![CDATA[
arXiv:2512.03601v1 Announce Type: new 
Abstract: Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAMP: Language-Assisted Motion Planning for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2512.03619</link>
<guid>https://arxiv.org/abs/2512.03619</guid>
<content:encoded><![CDATA[
arXiv:2512.03619v1 Announce Type: new 
Abstract: Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation</title>
<link>https://arxiv.org/abs/2512.03621</link>
<guid>https://arxiv.org/abs/2512.03621</guid>
<content:encoded><![CDATA[
arXiv:2512.03621v1 Announce Type: new 
Abstract: We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FeatureLens: A Highly Generalizable and Interpretable Framework for Detecting Adversarial Examples Based on Image Features</title>
<link>https://arxiv.org/abs/2512.03625</link>
<guid>https://arxiv.org/abs/2512.03625</guid>
<content:encoded><![CDATA[
arXiv:2512.03625v1 Announce Type: new 
Abstract: Although the remarkable performance of deep neural networks (DNNs) in image classification, their vulnerability to adversarial attacks remains a critical challenge. Most existing detection methods rely on complex and poorly interpretable architectures, which compromise interpretability and generalization. To address this, we propose FeatureLens, a lightweight framework that acts as a lens to scrutinize anomalies in image features. Comprising an Image Feature Extractor (IFE) and shallow classifiers (e.g., SVM, MLP, or XGBoost) with model sizes ranging from 1,000 to 30,000 parameters, FeatureLens achieves high detection accuracy ranging from 97.8% to 99.75% in closed-set evaluation and 86.17% to 99.6% in generalization evaluation across FGSM, PGD, CW, and DAmageNet attacks, using only 51 dimensional features. By combining strong detection performance with excellent generalization, interpretability, and computational efficiency, FeatureLens offers a practical pathway toward transparent and effective adversarial defense.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms</title>
<link>https://arxiv.org/abs/2512.03640</link>
<guid>https://arxiv.org/abs/2512.03640</guid>
<content:encoded><![CDATA[
arXiv:2512.03640v1 Announce Type: new 
Abstract: Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optical Context Compression Is Just (Bad) Autoencoding</title>
<link>https://arxiv.org/abs/2512.03643</link>
<guid>https://arxiv.org/abs/2512.03643</guid>
<content:encoded><![CDATA[
arXiv:2512.03643v1 Announce Type: new 
Abstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Visual Prompting for Lightweight Small-Image Classification</title>
<link>https://arxiv.org/abs/2512.03663</link>
<guid>https://arxiv.org/abs/2512.03663</guid>
<content:encoded><![CDATA[
arXiv:2512.03663v1 Announce Type: new 
Abstract: Visual prompting has recently emerged as an efficient strategy to adapt vision models using lightweight, learnable parameters injected into the input space. However, prior work mainly targets large Vision Transformers and high-resolution datasets such as ImageNet. In contrast, small-image benchmarks like MNIST, Fashion-MNIST, and CIFAR-10 remain widely used in education, prototyping, and research, yet have received little attention in the context of prompting. In this paper, we introduce \textbf{Multi-Scale Visual Prompting (MSVP)}, a simple and generic module that learns a set of global, mid-scale, and local prompt maps fused with the input image via a lightweight $1 \times 1$ convolution. MSVP is backbone-agnostic, adds less than $0.02\%$ parameters, and significantly improves performance across CNN and Vision Transformer backbones.
  We provide a unified benchmark on MNIST, Fashion-MNIST, and CIFAR-10 using a simple CNN, ResNet-18, and a small Vision Transformer. Our method yields consistent improvements with negligible computational overhead. We further include ablations on prompt scales, fusion strategies, and backbone architectures, along with qualitative analyzes using prompt visualizations and Grad-CAM. Our results demonstrate that multi-scale prompting provides an effective inductive bias even on low-resolution images.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos</title>
<link>https://arxiv.org/abs/2512.03666</link>
<guid>https://arxiv.org/abs/2512.03666</guid>
<content:encoded><![CDATA[
arXiv:2512.03666v1 Announce Type: new 
Abstract: A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning</title>
<link>https://arxiv.org/abs/2512.03667</link>
<guid>https://arxiv.org/abs/2512.03667</guid>
<content:encoded><![CDATA[
arXiv:2512.03667v1 Announce Type: new 
Abstract: In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.03673</link>
<guid>https://arxiv.org/abs/2512.03673</guid>
<content:encoded><![CDATA[
arXiv:2512.03673v1 Announce Type: new 
Abstract: Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces</title>
<link>https://arxiv.org/abs/2512.03683</link>
<guid>https://arxiv.org/abs/2512.03683</guid>
<content:encoded><![CDATA[
arXiv:2512.03683v1 Announce Type: new 
Abstract: 3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Visual Perception: Opportunities and Challenges</title>
<link>https://arxiv.org/abs/2512.03687</link>
<guid>https://arxiv.org/abs/2512.03687</guid>
<content:encoded><![CDATA[
arXiv:2512.03687v1 Announce Type: new 
Abstract: Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic, Interpretable, Perceptual Metric Between Images</title>
<link>https://arxiv.org/abs/2512.03701</link>
<guid>https://arxiv.org/abs/2512.03701</guid>
<content:encoded><![CDATA[
arXiv:2512.03701v1 Announce Type: new 
Abstract: Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS, achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties.
  We introduce the Structured Uncertainty Similarity Score (SUSS); it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling.
  SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction</title>
<link>https://arxiv.org/abs/2512.03715</link>
<guid>https://arxiv.org/abs/2512.03715</guid>
<content:encoded><![CDATA[
arXiv:2512.03715v1 Announce Type: new 
Abstract: This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images. The
  method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and
  matching. DINO is employed to retrieve semantically relevant image pairs in large collections, while
  rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue. Experiments on the Kaggle Image Matching Challenge 2025 demonstrate consistent improve ments in mean Average Accuracy (mAA), achieving a Silver Award (47th of 943 teams). The results
  confirm that combining self-supervised global descriptors with rotation-enhanced local matching offers
  a robust and scalable solution for large-scale 3D reconstruction.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention</title>
<link>https://arxiv.org/abs/2512.03724</link>
<guid>https://arxiv.org/abs/2512.03724</guid>
<content:encoded><![CDATA[
arXiv:2512.03724v1 Announce Type: new 
Abstract: The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-the-box: Black-box Causal Attacks on Object Detectors</title>
<link>https://arxiv.org/abs/2512.03730</link>
<guid>https://arxiv.org/abs/2512.03730</guid>
<content:encoded><![CDATA[
arXiv:2512.03730v1 Announce Type: new 
Abstract: Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2512.03745</link>
<guid>https://arxiv.org/abs/2512.03745</guid>
<content:encoded><![CDATA[
arXiv:2512.03745v1 Announce Type: new 
Abstract: Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking with Programming Vision: Towards a Unified View for Thinking with Images</title>
<link>https://arxiv.org/abs/2512.03746</link>
<guid>https://arxiv.org/abs/2512.03746</guid>
<content:encoded><![CDATA[
arXiv:2512.03746v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03749</link>
<guid>https://arxiv.org/abs/2512.03749</guid>
<content:encoded><![CDATA[
arXiv:2512.03749v1 Announce Type: new 
Abstract: Text-to-image (T2I) diffusion models have achieved widespread success due to their ability to generate high-resolution, photorealistic images. These models are trained on large-scale datasets, like LAION-5B, often scraped from the internet. However, since this data contains numerous biases, the models inherently learn and reproduce them, resulting in stereotypical outputs. We introduce SelfDebias, a fully unsupervised test-time debiasing method applicable to any diffusion model that uses a UNet as its noise predictor. SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference, minimizing the KL divergence between the output distribution and the uniform distribution. Unlike supervised approaches, SelfDebias does not require human-annotated datasets or external classifiers trained for each generated concept. Instead, it is designed to automatically identify semantic modes. Extensive experiments show that SelfDebias generalizes across prompts and diffusion model architectures, including both conditional and unconditional models. It not only effectively debiases images along key demographic dimensions while maintaining the visual fidelity of the generated images, but also more abstract concepts for which identifying biases is also challenging.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research on Brain Tumor Classification Method Based on Improved ResNet34 Network</title>
<link>https://arxiv.org/abs/2512.03751</link>
<guid>https://arxiv.org/abs/2512.03751</guid>
<content:encoded><![CDATA[
arXiv:2512.03751v1 Announce Type: new 
Abstract: Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition</title>
<link>https://arxiv.org/abs/2512.03794</link>
<guid>https://arxiv.org/abs/2512.03794</guid>
<content:encoded><![CDATA[
arXiv:2512.03794v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2512.03796</link>
<guid>https://arxiv.org/abs/2512.03796</guid>
<content:encoded><![CDATA[
arXiv:2512.03796v1 Announce Type: new 
Abstract: Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English</title>
<link>https://arxiv.org/abs/2512.03817</link>
<guid>https://arxiv.org/abs/2512.03817</guid>
<content:encoded><![CDATA[
arXiv:2512.03817v1 Announce Type: new 
Abstract: Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Robust Camera-based Method for Breath Rate Measurement</title>
<link>https://arxiv.org/abs/2512.03827</link>
<guid>https://arxiv.org/abs/2512.03827</guid>
<content:encoded><![CDATA[
arXiv:2512.03827v1 Announce Type: new 
Abstract: Proliferation of cheap and accessible cameras makes it possible to measure a subject's breath rate from video footage alone. Recent works on this topic have proposed a variety of approaches for accurately measuring human breath rate, however they are either tested in near-ideal conditions, or produce results that are not sufficiently accurate. The present study proposes a more robust method to measure breath rate in humans with minimal hardware requirements using a combination of mathematical transforms with a relative deviation from the ground truth of less than 5%. The method was tested on videos taken from 14 volunteers with a total duration of over 2 hours 30 minutes. The obtained results were compared to reference data and the average mean absolute error was found to be at 0.57 respirations per minute, which is noticeably better than the results from previous works. The breath rate measurement method proposed in the present article is more resistant to distortions caused by subject movement and thus allows one to remotely measure the subject's breath rate without any significant limitations on the subject's behavior.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lean Unet: A Compact Model for Image Segmentation</title>
<link>https://arxiv.org/abs/2512.03834</link>
<guid>https://arxiv.org/abs/2512.03834</guid>
<content:encoded><![CDATA[
arXiv:2512.03834v1 Announce Type: new 
Abstract: Unet and its variations have been standard in semantic image segmentation, especially for computer assisted radiology. Current Unet architectures iteratively downsample spatial resolution while increasing channel dimensions to preserve information content. Such a structure demands a large memory footprint, limiting training batch sizes and increasing inference latency. Channel pruning compresses Unet architecture without accuracy loss, but requires lengthy optimization and may not generalize across tasks and datasets. By investigating Unet pruning, we hypothesize that the final structure is the crucial factor, not the channel selection strategy of pruning. Based on our observations, we propose a lean Unet architecture (LUnet) with a compact, flat hierarchy where channels are not doubled as resolution is halved. We evaluate on a public MRI dataset allowing comparable reporting, as well as on two internal CT datasets. We show that a state-of-the-art pruning solution (STAMP) mainly prunes from the layers with the highest number of channels. Comparatively, simply eliminating a random channel at the pruning-identified layer or at the largest layer achieves similar or better performance. Our proposed LUnet with fixed architectures and over 30 times fewer parameters achieves performance comparable to both conventional Unet counterparts and data-adaptively pruned networks. The proposed lean Unet with constant channel count across layers requires far fewer parameters while achieving performance superior to standard Unet for the same total number of parameters. Skip connections allow Unet bottleneck channels to be largely reduced, unlike standard encoder-decoder architectures requiring increased bottleneck channels for information propagation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heatmap Pooling Network for Action Recognition from RGB Videos</title>
<link>https://arxiv.org/abs/2512.03837</link>
<guid>https://arxiv.org/abs/2512.03837</guid>
<content:encoded><![CDATA[
arXiv:2512.03837v1 Announce Type: new 
Abstract: Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation</title>
<link>https://arxiv.org/abs/2512.03844</link>
<guid>https://arxiv.org/abs/2512.03844</guid>
<content:encoded><![CDATA[
arXiv:2512.03844v1 Announce Type: new 
Abstract: Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the "intrinsic core distribution" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation</title>
<link>https://arxiv.org/abs/2512.03848</link>
<guid>https://arxiv.org/abs/2512.03848</guid>
<content:encoded><![CDATA[
arXiv:2512.03848v1 Announce Type: new 
Abstract: Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Traffic Image Restoration under Adverse Weather via Frequency-Aware Mamba</title>
<link>https://arxiv.org/abs/2512.03852</link>
<guid>https://arxiv.org/abs/2512.03852</guid>
<content:encoded><![CDATA[
arXiv:2512.03852v1 Announce Type: new 
Abstract: Traffic image restoration under adverse weather conditions remains a critical challenge for intelligent transportation systems. Existing methods primarily focus on spatial-domain modeling but neglect frequency-domain priors. Although the emerging Mamba architecture excels at long-range dependency modeling through patch-wise correlation analysis, its potential for frequency-domain feature extraction remains unexplored. To address this, we propose Frequency-Aware Mamba (FAMamba), a novel framework that integrates frequency guidance with sequence modeling for efficient image restoration. Our architecture consists of two key components: (1) a Dual-Branch Feature Extraction Block (DFEB) that enhances local-global interaction via bidirectional 2D frequency-adaptive scanning, dynamically adjusting traversal paths based on sub-band texture distributions; and (2) a Prior-Guided Block (PGB) that refines texture details through wavelet-based high-frequency residual learning, enabling high-quality image reconstruction with precise details. Meanwhile, we design a novel Adaptive Frequency Scanning Mechanism (AFSM) for the Mamba architecture, which enables the Mamba to achieve frequency-domain scanning across distinct subgraphs, thereby fully leveraging the texture distribution characteristics inherent in subgraph structures. Extensive experiments demonstrate the efficiency and effectiveness of FAMamba.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population</title>
<link>https://arxiv.org/abs/2512.03854</link>
<guid>https://arxiv.org/abs/2512.03854</guid>
<content:encoded><![CDATA[
arXiv:2512.03854v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diminishing Returns in Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2512.03862</link>
<guid>https://arxiv.org/abs/2512.03862</guid>
<content:encoded><![CDATA[
arXiv:2512.03862v1 Announce Type: new 
Abstract: While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Automated Framework for Large-Scale Graph-Based Cerebrovascular Analysis</title>
<link>https://arxiv.org/abs/2512.03869</link>
<guid>https://arxiv.org/abs/2512.03869</guid>
<content:encoded><![CDATA[
arXiv:2512.03869v1 Announce Type: new 
Abstract: We present CaravelMetrics, a computational framework for automated cerebrovascular analysis that models vessel morphology through skeletonization-derived graph representations. The framework integrates atlas-based regional parcellation, centerline extraction, and graph construction to compute fifteen morphometric, topological, fractal, and geometric features. The features can be estimated globally from the complete vascular network or regionally within arterial territories, enabling multiscale characterization of cerebrovascular organization. Applied to 570 3D TOF-MRA scans from the IXI dataset (ages 20-86), CaravelMetrics yields reproducible vessel graphs capturing age- and sex-related variations and education-associated increases in vascular complexity, consistent with findings reported in the literature. The framework provides a scalable and fully automated approach for quantitative cerebrovascular feature extraction, supporting normative modeling and population-level studies of vascular health and aging.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy</title>
<link>https://arxiv.org/abs/2512.03883</link>
<guid>https://arxiv.org/abs/2512.03883</guid>
<content:encoded><![CDATA[
arXiv:2512.03883v1 Announce Type: new 
Abstract: Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images during WW are essential to manage care and prevent distant metastases. Hence, we developed a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to combine longitudinal endoscopic images at restaging and follow-up and distinguish cCR from LR. SSDCA leverages pretrained Swin transformers to extract domain agnostic features and enhance robustness to imaging variations. Dual cross attention is implemented to emphasize features from the two scans without requiring any spatial alignment of images to predict response. SSDCA as well as Swin-based baselines were trained using image pairs from 135 patients and evaluated on a held-out set of image pairs from 62 patients. SSDCA produced the best balanced accuracy (81.76\% $\pm$ 0.04), sensitivity (90.07\% $\pm$ 0.08), and specificity (72.86\% $\pm$ 0.05). Robustness analysis showed stable performance irrespective of artifacts including blood, stool, telangiectasia, and poor image quality. UMAP clustering of extracted features showed maximal inter-cluster separation (1.45 $\pm$ 0.18) and minimal intra-cluster dispersion (1.07 $\pm$ 0.19) with SSDCA, confirming discriminative representation learning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence</title>
<link>https://arxiv.org/abs/2512.03905</link>
<guid>https://arxiv.org/abs/2512.03905</guid>
<content:encoded><![CDATA[
arXiv:2512.03905v1 Announce Type: new 
Abstract: The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework</title>
<link>https://arxiv.org/abs/2512.03918</link>
<guid>https://arxiv.org/abs/2512.03918</guid>
<content:encoded><![CDATA[
arXiv:2512.03918v1 Announce Type: new 
Abstract: We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Ground Truth: Enhanced Supervision for Image Restoration</title>
<link>https://arxiv.org/abs/2512.03932</link>
<guid>https://arxiv.org/abs/2512.03932</guid>
<content:encoded><![CDATA[
arXiv:2512.03932v1 Announce Type: new 
Abstract: Deep learning-based image restoration has achieved significant success. However, when addressing real-world degradations, model performance is limited by the quality of ground-truth images in datasets due to practical constraints in data acquisition. To address this limitation, we propose a novel framework that enhances existing ground truth images to provide higher-quality supervision for real-world restoration. Our framework generates perceptually enhanced ground truth images using super-resolution by incorporating adaptive frequency masks, which are learned by a conditional frequency mask generator. These masks guide the optimal fusion of frequency components from the original ground truth and its super-resolved variants, yielding enhanced ground truth images. This frequency-domain mixup preserves the semantic consistency of the original content while selectively enriching perceptual details, preventing hallucinated artifacts that could compromise fidelity. The enhanced ground truth images are used to train a lightweight output refinement network that can be seamlessly integrated with existing restoration models. Extensive experiments demonstrate that our approach consistently improves the quality of restored images. We further validate the effectiveness of both supervision enhancement and output refinement through user studies. Code is available at https://github.com/dhryougit/Beyond-the-Ground-Truth.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUT3R: Motion-aware Updating Transformer for Dynamic 3D Reconstruction</title>
<link>https://arxiv.org/abs/2512.03939</link>
<guid>https://arxiv.org/abs/2512.03939</guid>
<content:encoded><![CDATA[
arXiv:2512.03939v1 Announce Type: new 
Abstract: Recent stateful recurrent neural networks have achieved remarkable progress on static 3D reconstruction but remain vulnerable to motion-induced artifacts, where non-rigid regions corrupt attention propagation between the spatial memory and image feature. By analyzing the internal behaviors of the state and image token updating mechanism, we find that aggregating self-attention maps across layers reveals a consistent pattern: dynamic regions are naturally down-weighted, exposing an implicit motion cue that the pretrained transformer already encodes but never explicitly uses. Motivated by this observation, we introduce MUT3R, a training-free framework that applies the attention-derived motion cue to suppress dynamic content in the early layers of the transformer during inference. Our attention-level gating module suppresses the influence of dynamic regions before their artifacts propagate through the feature hierarchy. Notably, we do not retrain or fine-tune the model; we let the pretrained transformer diagnose its own motion cues and correct itself. This early regulation stabilizes geometric reasoning in streaming scenarios and leads to improvements in temporal consistency and camera pose robustness across multiple dynamic benchmarks, offering a simple and training-free pathway toward motion-aware streaming reconstruction.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.03963</link>
<guid>https://arxiv.org/abs/2512.03963</guid>
<content:encoded><![CDATA[
arXiv:2512.03963v1 Announce Type: new 
Abstract: Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training for Identity, Inference for Controllability: A Unified Approach to Tuning-Free Face Personalization</title>
<link>https://arxiv.org/abs/2512.03964</link>
<guid>https://arxiv.org/abs/2512.03964</guid>
<content:encoded><![CDATA[
arXiv:2512.03964v1 Announce Type: new 
Abstract: Tuning-free face personalization methods have developed along two distinct paradigms: text embedding approaches that map facial features into the text embedding space, and adapter-based methods that inject features through auxiliary cross-attention layers. While both paradigms have shown promise, existing methods struggle to simultaneously achieve high identity fidelity and flexible text controllability. We introduce UniID, a unified tuning-free framework that synergistically integrates both paradigms. Our key insight is that when merging these approaches, they should mutually reinforce only identity-relevant information while preserving the original diffusion prior for non-identity attributes. We realize this through a principled training-inference strategy: during training, we employ an identity-focused learning scheme that guides both branches to capture identity features exclusively; at inference, we introduce a normalized rescaling mechanism that recovers the text controllability of the base diffusion model while enabling complementary identity signals to enhance each other. This principled design enables UniID to achieve high-fidelity face personalization with flexible text controllability. Extensive experiments against six state-of-the-art methods demonstrate that UniID achieves superior performance in both identity preservation and text controllability. Code will be available at https://github.com/lyuPang/UniID
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlurDM: A Blur Diffusion Model for Image Deblurring</title>
<link>https://arxiv.org/abs/2512.03979</link>
<guid>https://arxiv.org/abs/2512.03979</guid>
<content:encoded><![CDATA[
arXiv:2512.03979v1 Announce Type: new 
Abstract: Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment</title>
<link>https://arxiv.org/abs/2512.03981</link>
<guid>https://arxiv.org/abs/2512.03981</guid>
<content:encoded><![CDATA[
arXiv:2512.03981v1 Announce Type: new 
Abstract: Drag-based image editing using generative models provides intuitive control over image structures. However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision. Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts. To address these limitations, we propose DirectDrag, a novel mask- and prompt-free editing framework. DirectDrag enables precise and efficient manipulation with minimal user input while maintaining high image fidelity and accurate point alignment. DirectDrag introduces two key innovations. First, we design an Auto Soft Mask Generation module that intelligently infers editable regions from point displacement, automatically localizing deformation along movement paths while preserving contextual integrity through the generative model's inherent capacity. Second, we develop a Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits, substantially improving visual fidelity. Despite operating without manual mask or prompt, DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy. Extensive experiments on DragBench and real-world scenarios demonstrate the effectiveness and practicality of DirectDrag for high-quality, interactive image manipulation. Project Page: https://frakw.github.io/DirectDrag/. Code is available at: https://github.com/frakw/DirectDrag.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation</title>
<link>https://arxiv.org/abs/2512.03992</link>
<guid>https://arxiv.org/abs/2512.03992</guid>
<content:encoded><![CDATA[
arXiv:2512.03992v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation</title>
<link>https://arxiv.org/abs/2512.03996</link>
<guid>https://arxiv.org/abs/2512.03996</guid>
<content:encoded><![CDATA[
arXiv:2512.03996v1 Announce Type: new 
Abstract: Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2512.04000</link>
<guid>https://arxiv.org/abs/2512.04000</guid>
<content:encoded><![CDATA[
arXiv:2512.04000v1 Announce Type: new 
Abstract: The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Temporality for Sketch Representation Learning</title>
<link>https://arxiv.org/abs/2512.04007</link>
<guid>https://arxiv.org/abs/2512.04007</guid>
<content:encoded><![CDATA[
arXiv:2512.04007v1 Announce Type: new 
Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Outlier View Rejection in Visual Geometry Grounded Transformers</title>
<link>https://arxiv.org/abs/2512.04012</link>
<guid>https://arxiv.org/abs/2512.04012</guid>
<content:encoded><![CDATA[
arXiv:2512.04012v1 Announce Type: new 
Abstract: Reliable 3D reconstruction from in-the-wild image collections is often hindered by "noisy" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Group Actions In Disentangled Latent Image Representations</title>
<link>https://arxiv.org/abs/2512.04015</link>
<guid>https://arxiv.org/abs/2512.04015</guid>
<content:encoded><![CDATA[
arXiv:2512.04015v1 Announce Type: new 
Abstract: Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ultra-lightweight Neural Video Representation Compression</title>
<link>https://arxiv.org/abs/2512.04019</link>
<guid>https://arxiv.org/abs/2512.04019</guid>
<content:encoded><![CDATA[
arXiv:2512.04019v1 Announce Type: new 
Abstract: Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C3G: Learning Compact 3D Representations with 2K Gaussians</title>
<link>https://arxiv.org/abs/2512.04021</link>
<guid>https://arxiv.org/abs/2512.04021</guid>
<content:encoded><![CDATA[
arXiv:2512.04021v1 Announce Type: new 
Abstract: Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.04025</link>
<guid>https://arxiv.org/abs/2512.04025</guid>
<content:encoded><![CDATA[
arXiv:2512.04025v1 Announce Type: new 
Abstract: Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast &amp; Efficient Normalizing Flows and Applications of Image Generative Models</title>
<link>https://arxiv.org/abs/2512.04039</link>
<guid>https://arxiv.org/abs/2512.04039</guid>
<content:encoded><![CDATA[
arXiv:2512.04039v1 Announce Type: new 
Abstract: This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.
  The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RELIC: Interactive Video World Model with Long-Horizon Memory</title>
<link>https://arxiv.org/abs/2512.04040</link>
<guid>https://arxiv.org/abs/2512.04040</guid>
<content:encoded><![CDATA[
arXiv:2512.04040v1 Announce Type: new 
Abstract: A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable Signer: Hierarchical Sign Language Generative Model</title>
<link>https://arxiv.org/abs/2512.04048</link>
<guid>https://arxiv.org/abs/2512.04048</guid>
<content:encoded><![CDATA[
arXiv:2512.04048v1 Announce Type: new 
Abstract: Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</title>
<link>https://arxiv.org/abs/2512.04069</link>
<guid>https://arxiv.org/abs/2512.04069</guid>
<content:encoded><![CDATA[
arXiv:2512.04069v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design</title>
<link>https://arxiv.org/abs/2512.04082</link>
<guid>https://arxiv.org/abs/2512.04082</guid>
<content:encoded><![CDATA[
arXiv:2512.04082v1 Announce Type: new 
Abstract: Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows</title>
<link>https://arxiv.org/abs/2512.04084</link>
<guid>https://arxiv.org/abs/2512.04084</guid>
<content:encoded><![CDATA[
arXiv:2512.04084v1 Announce Type: new 
Abstract: Normalizing Flows (NFs) learn invertible mappings between the data and a Gaussian distribution. Prior works usually suffer from two limitations. First, they add random noise to training samples or VAE latents as data augmentation, introducing complex pipelines including extra noising and denoising steps. Second, they use a pretrained and frozen VAE encoder, resulting in suboptimal reconstruction and generation quality. In this paper, we find that the two issues can be solved in a very simple way: just fixing the variance (which would otherwise be predicted by the VAE encoder) to a constant (e.g., 0.5). On the one hand, this method allows the encoder to output a broader distribution of tokens and the decoder to learn to reconstruct clean images from the augmented token distribution, avoiding additional noise or denoising design. On the other hand, fixed variance simplifies the VAE evidence lower bound, making it stable to train an NF with a VAE jointly. On the ImageNet $256 \times 256$ generation task, our model SimFlow obtains a gFID score of 2.15, outperforming the state-of-the-art method STARFlow (gFID 2.40). Moreover, SimFlow can be seamlessly integrated with the end-to-end representation alignment (REPA-E) method and achieves an improved gFID of 1.91, setting a new state of the art among NFs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unique Lives, Shared World: Learning from Single-Life Videos</title>
<link>https://arxiv.org/abs/2512.04085</link>
<guid>https://arxiv.org/abs/2512.04085</guid>
<content:encoded><![CDATA[
arXiv:2512.04085v1 Announce Type: new 
Abstract: We introduce the "single-life" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LATTICE: Democratize High-Fidelity 3D Generation at Scale</title>
<link>https://arxiv.org/abs/2512.03052</link>
<guid>https://arxiv.org/abs/2512.03052</guid>
<content:encoded><![CDATA[
arXiv:2512.03052v1 Announce Type: cross 
Abstract: We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research</title>
<link>https://arxiv.org/abs/2512.03054</link>
<guid>https://arxiv.org/abs/2512.03054</guid>
<content:encoded><![CDATA[
arXiv:2512.03054v1 Announce Type: cross 
Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PanFoMa: A Lightweight Foundation Model and Benchmark for Pan-Cancer</title>
<link>https://arxiv.org/abs/2512.03111</link>
<guid>https://arxiv.org/abs/2512.03111</guid>
<content:encoded><![CDATA[
arXiv:2512.03111v1 Announce Type: cross 
Abstract: Single-cell RNA sequencing (scRNA-seq) is essential for decoding tumor heterogeneity. However, pan-cancer research still faces two key challenges: learning discriminative and efficient single-cell representations, and establishing a comprehensive evaluation benchmark. In this paper, we introduce PanFoMa, a lightweight hybrid neural network that combines the strengths of Transformers and state-space models to achieve a balance between performance and efficiency. PanFoMa consists of a front-end local-context encoder with shared self-attention layers to capture complex, order-independent gene interactions; and a back-end global sequential feature decoder that efficiently integrates global context using a linear-time state-space model. This modular design preserves the expressive power of Transformers while leveraging the scalability of Mamba to enable transcriptome modeling, effectively capturing both local and global regulatory signals. To enable robust evaluation, we also construct a large-scale pan-cancer single-cell benchmark, PanFoMaBench, containing over 3.5 million high-quality cells across 33 cancer subtypes, curated through a rigorous preprocessing pipeline. Experimental results show that PanFoMa outperforms state-of-the-art models on our pan-cancer benchmark (+4.0\%) and across multiple public tasks, including cell type annotation (+7.4\%), batch integration (+4.0\%) and multi-omics integration (+3.1\%). The code is available at https://github.com/Xiaoshui-Huang/PanFoMa.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments</title>
<link>https://arxiv.org/abs/2512.03166</link>
<guid>https://arxiv.org/abs/2512.03166</guid>
<content:encoded><![CDATA[
arXiv:2512.03166v1 Announce Type: cross 
Abstract: The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Culture Affordance Atlas: Reconciling Object Diversity Through Functional Mapping</title>
<link>https://arxiv.org/abs/2512.03173</link>
<guid>https://arxiv.org/abs/2512.03173</guid>
<content:encoded><![CDATA[
arXiv:2512.03173v1 Announce Type: cross 
Abstract: Culture shapes the objects people use and for what purposes, yet mainstream Vision-Language (VL) datasets frequently exhibit cultural biases, disproportionately favoring higher-income, Western contexts. This imbalance reduces model generalizability and perpetuates performance disparities, especially impacting lower-income and non-Western communities. To address these disparities, we propose a novel function-centric framework that categorizes objects by the functions they fulfill, across diverse cultural and economic contexts. We implement this framework by creating the Culture Affordance Atlas, a re-annotated and culturally grounded restructuring of the Dollar Street dataset spanning 46 functions and 288 objects publicly available at https://lit.eecs.umich.edu/CultureAffordance-Atlas/index.html. Through extensive empirical analyses using the CLIP model, we demonstrate that function-centric labels substantially reduce socioeconomic performance gaps between high- and low-income groups by a median of 6 pp (statistically significant), improving model effectiveness for lower-income contexts. Furthermore, our analyses reveals numerous culturally essential objects that are frequently overlooked in prominent VL datasets. Our contributions offer a scalable pathway toward building inclusive VL datasets and equitable AI systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kaleidoscopic Scintillation Event Imaging</title>
<link>https://arxiv.org/abs/2512.03216</link>
<guid>https://arxiv.org/abs/2512.03216</guid>
<content:encoded><![CDATA[
arXiv:2512.03216v1 Announce Type: cross 
Abstract: Scintillators are transparent materials that interact with high-energy particles and emit visible light as a result. They are used in state of the art methods of measuring high-energy particles and radiation sources. Most existing methods use fast single-pixel detectors to detect and time scintillation events. Cameras provide spatial resolution but can only capture an average over many events, making it difficult to image the events associated with an individual particle. Emerging single-photon avalanche diode cameras combine speed and spatial resolution to enable capturing images of individual events. This allows us to use machine vision techniques to analyze events, enabling new types of detectors. The main challenge is the very low brightness of the events. Techniques have to work with a very limited number of photons.
  We propose a kaleidoscopic scintillator to increase light collection in a single-photon camera while preserving the event's spatial information. The kaleidoscopic geometry creates mirror reflections of the event in known locations for a given event location that are captured by the camera. We introduce theory for imaging an event in a kaleidoscopic scintillator and an algorithm to estimate the event's 3D position. We find that the kaleidoscopic scintillator design provides sufficient light collection to perform high-resolution event measurements for advanced radiation imaging techniques using a commercial CMOS single-photon camera. Code and data are available at https://github.com/bocchs/kaleidoscopic_scintillator.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models</title>
<link>https://arxiv.org/abs/2512.03422</link>
<guid>https://arxiv.org/abs/2512.03422</guid>
<content:encoded><![CDATA[
arXiv:2512.03422v1 Announce Type: cross 
Abstract: In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3DR: Towards Universal Multilingual Multimodal Document Retrieval</title>
<link>https://arxiv.org/abs/2512.03514</link>
<guid>https://arxiv.org/abs/2512.03514</guid>
<content:encoded><![CDATA[
arXiv:2512.03514v1 Announce Type: cross 
Abstract: Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization</title>
<link>https://arxiv.org/abs/2512.03522</link>
<guid>https://arxiv.org/abs/2512.03522</guid>
<content:encoded><![CDATA[
arXiv:2512.03522v1 Announce Type: cross 
Abstract: Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL</title>
<link>https://arxiv.org/abs/2512.03556</link>
<guid>https://arxiv.org/abs/2512.03556</guid>
<content:encoded><![CDATA[
arXiv:2512.03556v1 Announce Type: cross 
Abstract: Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting</title>
<link>https://arxiv.org/abs/2512.03656</link>
<guid>https://arxiv.org/abs/2512.03656</guid>
<content:encoded><![CDATA[
arXiv:2512.03656v1 Announce Type: cross 
Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction</title>
<link>https://arxiv.org/abs/2512.03962</link>
<guid>https://arxiv.org/abs/2512.03962</guid>
<content:encoded><![CDATA[
arXiv:2512.03962v1 Announce Type: cross 
Abstract: Deep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Microsaccade Compensation: Stable Vision for an Ornithopter</title>
<link>https://arxiv.org/abs/2512.03995</link>
<guid>https://arxiv.org/abs/2512.03995</guid>
<content:encoded><![CDATA[
arXiv:2512.03995v1 Announce Type: cross 
Abstract: Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for "Artificial Microsaccade Compensation". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jina-VLM: Small Multilingual Vision Language Model</title>
<link>https://arxiv.org/abs/2512.04032</link>
<guid>https://arxiv.org/abs/2512.04032</guid>
<content:encoded><![CDATA[
arXiv:2512.04032v1 Announce Type: cross 
Abstract: We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radiance Meshes for Volumetric Reconstruction</title>
<link>https://arxiv.org/abs/2512.04076</link>
<guid>https://arxiv.org/abs/2512.04076</guid>
<content:encoded><![CDATA[
arXiv:2512.04076v1 Announce Type: cross 
Abstract: We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization. Unlike a Voronoi diagram, a Delaunay tetrahedralization yields simple triangles that are natively supported by existing hardware. As such, our model is able to perform exact and fast volume rendering using both rasterization and ray-tracing. We introduce a new rasterization method that achieves faster rendering speeds than all prior radiance field representations (assuming an equivalent number of primitives and resolution) across a variety of platforms. Optimizing the positions of Delaunay vertices introduces topological discontinuities (edge flips). To solve this, we use a Zip-NeRF-style backbone which allows us to express a smoothly varying field even when the topology changes. Our rendering method exactly evaluates the volume rendering equation and enables high quality, real-time view synthesis on standard consumer hardware. Our tetrahedral meshes also lend themselves to a variety of exciting applications including fisheye lens distortion, physics-based simulation, editing, and mesh extraction.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Learning Paradigm for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2209.15402</link>
<guid>https://arxiv.org/abs/2209.15402</guid>
<content:encoded><![CDATA[
arXiv:2209.15402v4 Announce Type: replace 
Abstract: Due to the subjective crowdsourcing annotations and the inherent inter-class similarity of facial expressions, the real-world Facial Expression Recognition (FER) datasets usually exhibit ambiguous annotation. To simplify the learning paradigm, most previous methods convert ambiguous annotation results into precise one-hot annotations and train FER models in an end-to-end supervised manner. In this paper, we rethink the existing training paradigm and propose that it is better to use weakly supervised strategies to train FER models with original ambiguous annotation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference</title>
<link>https://arxiv.org/abs/2405.14430</link>
<guid>https://arxiv.org/abs/2405.14430</guid>
<content:encoded><![CDATA[
arXiv:2405.14430v4 Announce Type: replace 
Abstract: This paper presents PipeFusion, an innovative parallel methodology to tackle the high latency issues associated with generating high-resolution images using diffusion transformers (DiTs) models. PipeFusion partitions images into patches and the model layers across multiple GPUs. It employs a patch-level pipeline parallel strategy to orchestrate communication and computation efficiently. By capitalizing on the high similarity between inputs from successive diffusion steps, PipeFusion reuses one-step stale feature maps to provide context for the current pipeline step. This approach notably reduces communication costs compared to existing DiTs inference parallelism, including tensor parallel, sequence parallel and DistriFusion. PipeFusion enhances memory efficiency through parameter distribution across devices, ideal for large DiTs like Flux.1. Experimental results demonstrate that PipeFusion achieves state-of-the-art performance on 8$\times$L40 PCIe GPUs for Pixart, Stable-Diffusion 3, and Flux.1 models. Our source code is available at https://github.com/xdit-project/xDiT.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Margin-aware Preference Optimization for Aligning Diffusion Models without Reference</title>
<link>https://arxiv.org/abs/2406.06424</link>
<guid>https://arxiv.org/abs/2406.06424</guid>
<content:encoded><![CDATA[
arXiv:2406.06424v2 Announce Type: replace 
Abstract: Modern preference alignment methods, such as DPO, rely on divergence regularization to a reference model for training stability-but this creates a fundamental problem we call "reference mismatch." In this paper, we investigate the negative impacts of reference mismatch in aligning text-to-image (T2I) diffusion models, showing that larger reference mismatch hinders effective adaptation given the same amount of data, e.g., as when learning new artistic styles, or personalizing to specific objects. We demonstrate this phenomenon across text-to-image (T2I) diffusion models and introduce margin-aware preference optimization (MaPO), a reference-agnostic approach that breaks free from this constraint. By directly optimizing the likelihood margin between preferred and dispreferred outputs under the Bradley-Terry model without anchoring to a reference, MaPO transforms diverse T2I tasks into unified pairwise preference optimization. We validate MaPO's versatility across five challenging domains: (1) safe generation, (2) style adaptation, (3) cultural representation, (4) personalization, and (5) general preference alignment. Our results reveal that MaPO's advantage grows dramatically with reference mismatch severity, outperforming both DPO and specialized methods like DreamBooth while reducing training time by 15%. MaPO thus emerges as a versatile and memory-efficient method for generic T2I adaptation tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NVRC: Neural Video Representation Compression</title>
<link>https://arxiv.org/abs/2409.07414</link>
<guid>https://arxiv.org/abs/2409.07414</guid>
<content:encoded><![CDATA[
arXiv:2409.07414v2 Announce Type: replace 
Abstract: Recent advances in implicit neural representation (INR)-based video coding have demonstrated its potential to compete with both conventional and other learning-based approaches. With INR methods, a neural network is trained to overfit a video sequence, with its parameters compressed to obtain a compact representation of the video content. However, although promising results have been achieved, the best INR-based methods are still out-performed by the latest standard codecs, such as VVC VTM, partially due to the simple model compression techniques employed. In this paper, rather than focusing on representation architectures as in many existing works, we propose a novel INR-based video compression framework, Neural Video Representation Compression (NVRC), targeting compression of the representation. Based on the novel entropy coding and quantization models proposed, NVRC, for the first time, is able to optimize an INR-based video codec in a fully end-to-end manner. To further minimize the additional bitrate overhead introduced by the entropy models, we have also proposed a new model compression framework for coding all the network, quantization and entropy model parameters hierarchically. Our experiments show that NVRC outperforms many conventional and learning-based benchmark codecs, with a 24% average coding gain over VVC VTM (Random Access) on the UVG dataset, measured in PSNR. As far as we are aware, this is the first time an INR-based video codec achieving such performance. The implementation of NVRC will be released.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Efficient Variants of Segment Anything Model: A Survey</title>
<link>https://arxiv.org/abs/2410.04960</link>
<guid>https://arxiv.org/abs/2410.04960</guid>
<content:encoded><![CDATA[
arXiv:2410.04960v4 Announce Type: replace 
Abstract: The Segment Anything Model (SAM) is a foundational model for image segmentation tasks, known for its strong generalization across diverse applications. However, its impressive performance comes with significant computational and resource demands, making it challenging to deploy in resource-limited environments such as edge devices. To address this, a variety of SAM variants have been proposed to enhance efficiency while keeping accuracy. This survey provides the first comprehensive review of these efficient SAM variants. We begin by exploring the motivations driving this research. We then present core techniques used in SAM and model acceleration. This is followed by a detailed exploration of SAM acceleration strategies, categorized by approach, and a discussion of several future research directions. Finally, we offer a unified and extensive evaluation of these methods across various hardware, assessing their efficiency and accuracy on representative benchmarks, and providing a clear comparison of their overall performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes</title>
<link>https://arxiv.org/abs/2410.18084</link>
<guid>https://arxiv.org/abs/2410.18084</guid>
<content:encoded><![CDATA[
arXiv:2410.18084v3 Announce Type: replace 
Abstract: Urban scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic 4D scenes with semantics. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D occupancy generation methods across multiple metrics. The code and models have been released to facilitate future research.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Prose: Advancing Multi-Modal Language Models for Remote Sensing</title>
<link>https://arxiv.org/abs/2411.05826</link>
<guid>https://arxiv.org/abs/2411.05826</guid>
<content:encoded><![CDATA[
arXiv:2411.05826v2 Announce Type: replace 
Abstract: Remote sensing has evolved from simple image acquisition to complex systems capable of integrating and processing visual and textual data. This review examines the development and application of multi-modal language models (MLLMs) in remote sensing, focusing on their ability to interpret and describe satellite imagery using natural language. We cover the technical underpinnings of MLLMs, including dual-encoder architectures, Transformer models, self-supervised and contrastive learning, and cross-modal integration. The unique challenges of remote sensing data--varying spatial resolutions, spectral richness, and temporal changes--are analyzed for their impact on MLLM performance. Key applications such as scene description, object detection, change detection, text-to-image retrieval, image-to-text generation, and visual question answering are discussed to demonstrate their relevance in environmental monitoring, urban planning, and disaster response. We review significant datasets and resources supporting the training and evaluation of these models. Challenges related to computational demands, scalability, data quality, and domain adaptation are highlighted. We conclude by proposing future research directions and technological advancements to further enhance MLLM utility in remote sensing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-time Correction: An Online 3D Detection System via Visual Prompting</title>
<link>https://arxiv.org/abs/2412.07768</link>
<guid>https://arxiv.org/abs/2412.07768</guid>
<content:encoded><![CDATA[
arXiv:2412.07768v3 Announce Type: replace 
Abstract: This paper introduces Test-time Correction (TTC), an online 3D detection system designed to rectify test-time errors using various auxiliary feedback, aiming to enhance the safety of deployed autonomous driving systems. Unlike conventional offline 3D detectors that remain fixed during inference, TTC enables immediate online error correction without retraining, allowing autonomous vehicles to adapt to new scenarios and reduce deployment risks. To achieve this, we equip existing 3D detectors with an Online Adapter (OA) module -- a prompt-driven query generator for real-time correction. At the core of OA module are visual prompts: image-based descriptions of objects of interest derived from auxiliary feedback such as mismatches with 2D detections, road descriptions, or user clicks. These visual prompts, collected from risky objects during inference, are maintained in a visual prompt buffer to enable continuous correction in future frames. By leveraging this mechanism, TTC consistently detects risky objects, achieving reliable, adaptive, and versatile driving autonomy. Extensive experiments show that TTC significantly improves instant error rectification over frozen 3D detectors, even under limited labels, zero-shot settings, and adverse conditions. We hope this work inspires future research on post-deployment online rectification systems for autonomous driving.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GT23D-Bench: A Comprehensive General Text-to-3D Generation Benchmark</title>
<link>https://arxiv.org/abs/2412.09997</link>
<guid>https://arxiv.org/abs/2412.09997</guid>
<content:encoded><![CDATA[
arXiv:2412.09997v2 Announce Type: replace 
Abstract: Text-to-3D (T23D) generation has emerged as a crucial visual generation task, aiming at synthesizing 3D content from textual descriptions. Studies of this task are currently shifting from per-scene T23D, which requires optimization of the model for every content generated, to General T23D (GT23D), which requires only one pre-trained model to generate different content without re-optimization, for more generalized and efficient 3D generation. Despite notable advancements, GT23D is severely bottlenecked by two interconnected challenges: the lack of high-quality, large-scale training data and the prevalence of evaluation metrics that overlook intrinsic 3D properties. Existing datasets often suffer from incomplete annotations, noisy organization, and inconsistent quality, while current evaluations rely heavily on 2D image-text similarity or scoring, failing to thoroughly assess 3D geometric integrity and semantic relevance. To address these fundamental gaps, we introduce GT23D-Bench, the first comprehensive benchmark specifically designed for GT23D training and evaluation. We first construct a high-quality dataset of 400K 3D assets, featuring diverse visual annotations (70M+ visual samples) and multi-granularity hierarchical captions (1M+ descriptions) to foster robust semantic learning. Second, we propose a comprehensive evaluation suite with 10 metrics assessing both text-3D alignment and 3D visual quality at multiple levels. Crucially, we demonstrate through rigorous experiments that our proposed metrics exhibit significantly higher correlation with human judgment compared to existing methods. Our in-depth analysis of eight leading GT23D models using this benchmark provides the community with critical insights into current model capabilities and their shared failure modes. GT23D-Bench will be publicly available to facilitate rigorous and reproducible research.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.04005</link>
<guid>https://arxiv.org/abs/2501.04005</guid>
<content:encoded><![CDATA[
arXiv:2501.04005v3 Announce Type: replace 
Abstract: Recent advancements in vision foundation models (VFMs) have revolutionized visual perception in 2D, yet their potential for 3D scene understanding, particularly in autonomous driving applications, remains underexplored. In this paper, we introduce LargeAD, a versatile and scalable framework designed for large-scale 3D pretraining across diverse real-world driving datasets. Our framework leverages VFMs to extract semantically rich superpixels from 2D images, which are aligned with LiDAR point clouds to generate high-quality contrastive samples. This alignment facilitates cross-modal representation learning, enhancing the semantic consistency between 2D and 3D data. We introduce several key innovations: (i) VFM-driven superpixel generation for detailed semantic representation, (ii) a VFM-assisted contrastive learning strategy to align multimodal features, (iii) superpoint temporal consistency to maintain stable representations across time, and (iv) multi-source data pretraining to generalize across various LiDAR configurations. Our approach achieves substantial gains over state-of-the-art methods in linear probing and fine-tuning for LiDAR-based segmentation and object detection. Extensive experiments on 11 large-scale multi-sensor datasets highlight our superior performance, demonstrating adaptability, efficiency, and robustness in real-world autonomous driving scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Point, I Learn: Online Adaptation of Interactive Segmentation Models for Handling Distribution Shifts in Medical Imaging</title>
<link>https://arxiv.org/abs/2503.06717</link>
<guid>https://arxiv.org/abs/2503.06717</guid>
<content:encoded><![CDATA[
arXiv:2503.06717v2 Announce Type: replace 
Abstract: Interactive segmentation uses real-time user inputs, such as mouse clicks, to iteratively refine model predictions. Although not originally designed to address distribution shifts, this paradigm naturally lends itself to such challenges. In medical imaging, where distribution shifts are common, interactive methods can use user inputs to guide models towards improved predictions. Moreover, once a model is deployed, user corrections can be used to adapt the network parameters to the new data distribution, mitigating distribution shift. Based on these insights, we aim to develop a practical, effective method for improving the adaptive capabilities of interactive segmentation models to new data distributions in medical imaging. Firstly, we found that strengthening the model's responsiveness to clicks is important for the initial training process. Moreover, we show that by treating the post-interaction user-refined model output as pseudo-ground-truth, we can design a lean, practical online adaptation method that enables a model to learn effectively across sequential test images. The framework includes two components: (i) a Post-Interaction adaptation process, updating the model after the user has completed interactive refinement of an image, and (ii) a Mid-Interaction adaptation process, updating incrementally after each click. Both processes include a Click-Centered Gaussian loss that strengthens the model's reaction to clicks and enhances focus on user-guided, clinically relevant regions. Experiments on 5 fundus and 4 brain-MRI databases show that our approach consistently outperforms existing methods under diverse distribution shifts, including unseen imaging modalities and pathologies. Code and pretrained models will be released upon publication.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.06859</link>
<guid>https://arxiv.org/abs/2503.06859</guid>
<content:encoded><![CDATA[
arXiv:2503.06859v2 Announce Type: replace 
Abstract: Gaussian splatting (GS) along with its extensions and variants provides outstanding performance in real-time scene rendering while meeting reduced storage demands and computational efficiency. While the selection of 2D images capturing the scene of interest is crucial for the proper initialization and training of GS, hence markedly affecting the rendering performance, prior works rely on passively and typically densely selected 2D images. In contrast, this paper proposes `ActiveInitSplat', a novel framework for active selection of training images for proper initialization and training of GS. ActiveInitSplat relies on density and occupancy criteria of the resultant 3D scene representation from the selected 2D images, to ensure that the latter are captured from diverse viewpoints leading to better scene coverage and that the initialized Gaussian functions are well aligned with the actual 3D structure. Numerical tests on well-known simulated and real environments demonstrate the merits of ActiveInitSplat resulting in significant GS rendering performance improvement over passive GS baselines in both dense- and sparse-view settings, in the widely adopted LPIPS, SSIM, and PSNR metrics.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Radiance and Gaze Fields for Visual Attention Modeling in 3D Environments</title>
<link>https://arxiv.org/abs/2503.07828</link>
<guid>https://arxiv.org/abs/2503.07828</guid>
<content:encoded><![CDATA[
arXiv:2503.07828v2 Announce Type: replace 
Abstract: We introduce Neural Radiance and Gaze Fields (NeRGs), a novel approach for representing visual attention in complex environments. Much like how Neural Radiance Fields (NeRFs) perform novel view synthesis, NeRGs reconstruct gaze patterns from arbitrary viewpoints, implicitly mapping visual attention to 3D surfaces. We achieve this by augmenting a standard NeRF with an additional network that models local egocentric gaze probability density, conditioned on scene geometry and observer position. The output of a NeRG is a rendered view of the scene alongside a pixel-wise salience map representing the conditional probability that a given observer fixates on visible surfaces. Unlike prior methods, our system is lightweight and enables visualization of gaze fields at interactive framerates. Moreover, NeRGs allow the observer perspective to be decoupled from the rendering camera and correctly account for gaze occlusion due to intervening geometry. We demonstrate the effectiveness of NeRGs using head pose from skeleton tracking as a proxy for gaze, employing our proposed gaze probes to aggregate noisy rays into robust probability density targets for supervision.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow to the Mode: Mode-Seeking Diffusion Autoencoders for State-of-the-Art Image Tokenization</title>
<link>https://arxiv.org/abs/2503.11056</link>
<guid>https://arxiv.org/abs/2503.11056</guid>
<content:encoded><![CDATA[
arXiv:2503.11056v3 Announce Type: replace 
Abstract: Since the advent of popular visual generation frameworks like VQGAN and latent diffusion models, state-of-the-art image generation systems have generally been two-stage systems that first tokenize or compress visual data into a lower-dimensional latent space before learning a generative model. Tokenizer training typically follows a standard recipe in which images are compressed and reconstructed subject to a combination of MSE, perceptual, and adversarial losses. Diffusion autoencoders have been proposed in prior work as a way to learn end-to-end perceptually-oriented image compression, but have not yet shown state-of-the-art performance on the competitive task of ImageNet-1K reconstruction. We propose FlowMo, a transformer-based diffusion autoencoder that achieves a new state-of-the-art for image tokenization at multiple compression rates without using convolutions, adversarial losses, spatially-aligned two-dimensional latent codes, or distilling from other tokenizers. Our key insight is that FlowMo training should be broken into a mode-matching pre-training stage and a mode-seeking post-training stage. In addition, we conduct extensive analyses and explore the training of generative models atop the FlowMo tokenizer. Our code and models will be available at http://kylesargent.github.io/flowmo .
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction</title>
<link>https://arxiv.org/abs/2503.13430</link>
<guid>https://arxiv.org/abs/2503.13430</guid>
<content:encoded><![CDATA[
arXiv:2503.13430v2 Announce Type: replace 
Abstract: Autonomous driving requires understanding infrastructure elements, such as lanes and crosswalks. To navigate safely, this understanding must be derived from sensor data in real-time and needs to be represented in vectorized form. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set of camera images from multiple views into one joint latent BEV grid. Traditionally, from this latent space, an intermediate raster map is predicted, providing dense spatial supervision but requiring post-processing into the desired vectorized form. More recent models directly derive infrastructure elements as polylines using vectorized map decoders, providing instance-level information. Our approach, Augmentation Map Network (AugMapNet), proposes latent BEV feature grid augmentation, a novel technique that significantly enhances the latent BEV representation. AugMapNet combines vector decoding and dense spatial supervision more effectively than existing architectures while remaining easy to integrate compared to other hybrid approaches. It additionally benefits from extra processing on its latent BEV features. Experiments on nuScenes and Argoverse2 datasets demonstrate significant improvements on vectorized map prediction of up to 13.3% over the StreamMapNet baseline on 60 m range and greater improvements on larger ranges. We confirm transferability by applying our method to another baseline, SQD-MapNet, and find similar improvements. A detailed analysis of the latent BEV grid confirms a more structured latent space of AugMapNet and shows the value of our novel concept beyond pure performance improvement. The code can be found at https://github.com/tmonnin/augmapnet
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables</title>
<link>https://arxiv.org/abs/2503.23793</link>
<guid>https://arxiv.org/abs/2503.23793</guid>
<content:encoded><![CDATA[
arXiv:2503.23793v2 Announce Type: replace 
Abstract: Recently, deep learning-based pan-sharpening algorithms have achieved notable advancements over traditional methods. However, deep learning-based methods incur substantial computational overhead during inference, especially with large images. This excessive computational demand limits the applicability of these methods in real-world scenarios, particularly in the absence of dedicated computing devices such as GPUs and TPUs. To address these challenges, we propose Pan-LUT, a novel learnable look-up table (LUT) framework for pan-sharpening that strikes a balance between performance and computational efficiency for large remote sensing images. Our method makes it possible to process 15K*15K remote sensing images on a 24GB GPU. To finely control the spectral transformation, we devise the PAN-guided look-up table (PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained spatial details, we introduce the spatial details look-up table (SDLUT). Furthermore, to adaptively aggregate channel information for generating high-resolution multispectral images, we design an adaptive output look-up table (AOLUT). Our model contains fewer than 700K parameters and processes a 9K*9K image in under 1 ms using one RTX 2080 Ti GPU, demonstrating significantly faster performance compared to other methods. Experiments reveal that Pan-LUT efficiently processes large remote sensing images in a lightweight manner, bridging the gap to real-world applications. Furthermore, our model surpasses SOTA methods in full-resolution scenes under real-world conditions, highlighting its effectiveness and efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video</title>
<link>https://arxiv.org/abs/2504.15122</link>
<guid>https://arxiv.org/abs/2504.15122</guid>
<content:encoded><![CDATA[
arXiv:2504.15122v4 Announce Type: replace 
Abstract: We present MoBGS, a novel motion deblurring 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method using a proposed Blur-adaptive Neural Ordinary Differential Equation (ODE) solver for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both a global camera and local object motions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent methods, achieving state-of-the-art performance for dynamic NVS under motion blur.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can VLMs Detect and Localize Fine-Grained AI-Edited Images?</title>
<link>https://arxiv.org/abs/2505.15644</link>
<guid>https://arxiv.org/abs/2505.15644</guid>
<content:encoded><![CDATA[
arXiv:2505.15644v2 Announce Type: replace 
Abstract: Fine-grained detection and localization of localized image edits is crucial for assessing content authenticity, especially as modern diffusion models and image editors can produce highly realistic manipulations. However, this problem faces three key challenges: (1) most AIGC detectors produce only a global real-or-fake label without indicating where edits occur; (2) traditional computer vision methods for edit localization typically rely on costly pixel-level annotations; and (3) there is no large-scale, modern benchmark specifically targeting edited-image detection. To address these gaps, we develop an automated data-generation pipeline and construct FragFake, a large-scale benchmark of AI-edited images spanning multiple source datasets, diverse editing models, and several common edit types. Building on FragFake, we are the first to systematically study vision language models (VLMs) for edited-image classification and edited-region localization. Our experiments show that pretrained VLMs, including GPT4o, perform poorly on this task, whereas fine-tuned models such as Qwen2.5-VL achieve high accuracy and substantially higher object precision across all settings. We further explore GRPO-based RLVR training, which yields modest metric gains while improving the interpretability of model outputs. Ablation and transfer analyses reveal how data balancing, training size, LoRA rank, and training domain affect performance, and highlight both the potential and the limitations of cross-editor and cross-dataset generalization. We anticipate that this work will establish a solid foundation to facilitate and inspire subsequent research endeavors in the domain of multimodal content authenticity.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATORI-R1: Incentivizing Multimodal Reasoning through Explicit Visual Anchoring</title>
<link>https://arxiv.org/abs/2505.19094</link>
<guid>https://arxiv.org/abs/2505.19094</guid>
<content:encoded><![CDATA[
arXiv:2505.19094v2 Announce Type: replace 
Abstract: DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI ($\textbf{S}patially$ $\textbf{A}nchored$ $\textbf{T}ask$ $\textbf{O}ptimization$ with $\textbf{R}e\textbf{I}nforcement$ Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to $15.7\%$ improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guard Me If You Know Me: Protecting Specific Face-Identity from Deepfakes</title>
<link>https://arxiv.org/abs/2505.19582</link>
<guid>https://arxiv.org/abs/2505.19582</guid>
<content:encoded><![CDATA[
arXiv:2505.19582v2 Announce Type: replace 
Abstract: Securing personal identity against deepfake attacks is increasingly critical in the digital age, especially for celebrities and political figures whose faces are easily accessible and frequently targeted. Most existing deepfake detection methods focus on general-purpose scenarios and often ignore the valuable prior knowledge of known facial identities, e.g., "VIP individuals" whose authentic facial data are already available. In this paper, we propose \textbf{VIPGuard}, a unified multimodal framework designed to capture fine-grained and comprehensive facial representations of a given identity, compare them against potentially fake or similar-looking faces, and reason over these comparisons to make accurate and explainable predictions. Specifically, our framework consists of three main stages. First, fine-tune a multimodal large language model (MLLM) to learn detailed and structural facial attributes. Second, we perform identity-level discriminative learning to enable the model to distinguish subtle differences between highly similar faces, including real and fake variations. Finally, we introduce user-specific customization, where we model the unique characteristics of the target face identity and perform semantic reasoning via MLLM to enable personalized and explainable deepfake detection. Our framework shows clear advantages over previous detection works, where traditional detectors mainly rely on low-level visual cues and provide no human-understandable explanations, while other MLLM-based models often lack a detailed understanding of specific face identities. To facilitate the evaluation of our method, we built a comprehensive identity-aware benchmark called \textbf{VIPBench} for personalized deepfake detection, involving the latest 7 face-swapping and 7 entire face synthesis techniques for generation. The code is available at https://github.com/KQL11/VIPGuard .
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query</title>
<link>https://arxiv.org/abs/2506.03144</link>
<guid>https://arxiv.org/abs/2506.03144</guid>
<content:encoded><![CDATA[
arXiv:2506.03144v3 Announce Type: replace 
Abstract: Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GS4: Generalizable Sparse Splatting Semantic SLAM</title>
<link>https://arxiv.org/abs/2506.06517</link>
<guid>https://arxiv.org/abs/2506.06517</guid>
<content:encoded><![CDATA[
arXiv:2506.06517v3 Announce Type: replace 
Abstract: Traditional SLAM algorithms excel at camera tracking, but typically produce incomplete and low-resolution maps that are not tightly integrated with semantics prediction. Recent work integrates Gaussian Splatting (GS) into SLAM to enable dense, photorealistic 3D mapping, yet existing GS-based SLAM methods require per-scene optimization that is slow and consumes an excessive number of Gaussians. We present GS4, the first generalizable GS-based semantic SLAM system. Compared with prior approaches, GS4 runs 10x faster, uses 10x fewer Gaussians, and achieves state-of-the-art performance across color, depth, semantic mapping and camera tracking. From an RGB-D video stream, GS4 incrementally builds and updates a set of 3D Gaussians using a feed-forward network. First, the Gaussian Prediction Model estimates a sparse set of Gaussian parameters from input frame, which integrates both color and semantic prediction with the same backbone. Then, the Gaussian Refinement Network merges new Gaussians with the existing set while avoiding redundancy. Finally, when significant pose changes are detected, we perform only 1-5 iterations of joint Gaussian-pose optimization to correct drift, remove floaters, and further improve tracking accuracy. Experiments on the real-world ScanNet and ScanNet++ benchmarks demonstrate state-of-the-art semantic SLAM performance, with strong generalization capability shown through zero-shot transfer to the NYUv2 and TUM RGB-D datasets.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Top Activations: Efficient and Reliable Crowdsourced Evaluation of Automated Interpretability</title>
<link>https://arxiv.org/abs/2506.07985</link>
<guid>https://arxiv.org/abs/2506.07985</guid>
<content:encoded><![CDATA[
arXiv:2506.07985v2 Announce Type: replace 
Abstract: Interpreting individual neurons or directions in activation space is an important topic in mechanistic interpretability. Numerous automated interpretability methods have been proposed to generate such explanations, but it remains unclear how reliable these explanations are, and which methods produce the most accurate descriptions. While crowd-sourced evaluations are commonly used, existing pipelines are noisy, costly, and typically assess only the highest-activating inputs, leading to unreliable results. In this paper, we introduce two techniques to enable cost-effective and accurate crowdsourced evaluation of automated interpretability methods beyond top activating inputs. First, we propose Model-Guided Importance Sampling (MG-IS) to select the most informative inputs to show human raters. In our experiments, we show this reduces the number of inputs needed to reach the same evaluation accuracy by ~13x. Second, we address label noise in crowd-sourced ratings through Bayesian Rating Aggregation (BRAgg), which allows us to reduce the number of ratings per input required to overcome noise by ~3x. Together, these techniques reduce the evaluation cost by ~40x, making large-scale evaluation feasible. Finally, we use our methods to conduct a large scale crowd-sourced study comparing recent automated interpretability methods for vision networks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting</title>
<link>https://arxiv.org/abs/2506.08710</link>
<guid>https://arxiv.org/abs/2506.08710</guid>
<content:encoded><![CDATA[
arXiv:2506.08710v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets are public to accelerate research in generalizable 3DGS scene understanding.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GNSS-Inertial State Initialization Using Inter-Epoch Baseline Residuals</title>
<link>https://arxiv.org/abs/2506.11534</link>
<guid>https://arxiv.org/abs/2506.11534</guid>
<content:encoded><![CDATA[
arXiv:2506.11534v2 Announce Type: replace 
Abstract: Initializing the state of a sensorized platform can be challenging, as a limited set of measurements often provide low-informative constraints that are in addition highly non-linear. This may lead to poor initial estimates that may converge to local minima during subsequent non-linear optimization. We propose an adaptive GNSS-inertial initialization strategy that delays the incorporation of global GNSS constraints until they become sufficiently informative. In the initial stage, our method leverages inter-epoch baseline vector residuals between consecutive GNSS fixes to mitigate inertial drift. To determine when to activate global constraints, we introduce a general criterion based on the evolution of the Hessian matrix's singular values, effectively quantifying system observability. Experiments on EuRoC, GVINS and MARS-LVIG datasets show that our approach consistently outperforms the naive strategy of fusing all measurements from the outset, yielding more accurate and robust initializations.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A$^2$LC: Active and Automated Label Correction for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2506.11599</link>
<guid>https://arxiv.org/abs/2506.11599</guid>
<content:encoded><![CDATA[
arXiv:2506.11599v2 Announce Type: replace 
Abstract: Active Label Correction (ALC) has emerged as a promising solution to the high cost and error-prone nature of manual pixel-wise annotation in semantic segmentation, by actively identifying and correcting mislabeled data. Although recent work has improved correction efficiency by generating pseudo-labels using foundation models, substantial inefficiencies still remain. In this paper, we introduce A$^2$LC, an Active and Automated Label Correction framework for semantic segmentation, where manual and automatic correction stages operate in a cascaded manner. Specifically, the automatic correction stage leverages human feedback to extend label corrections beyond the queried samples, thereby maximizing cost efficiency. In addition, we introduce an adaptively balanced acquisition function that emphasizes underrepresented tail classes, working in strong synergy with the automatic correction stage. Extensive experiments on Cityscapes and PASCAL VOC 2012 demonstrate that A$^2$LC significantly outperforms previous state-of-the-art methods. Notably, A$^2$LC exhibits high efficiency by outperforming previous methods with only 20% of their budget, and shows strong effectiveness by achieving a 27.23% performance gain under the same budget on Cityscapes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BitMark: Watermarking Bitwise Autoregressive Image Generative Models</title>
<link>https://arxiv.org/abs/2506.21209</link>
<guid>https://arxiv.org/abs/2506.21209</guid>
<content:encoded><![CDATA[
arXiv:2506.21209v2 Announce Type: replace 
Abstract: State-of-the-art text-to-image models generate photorealistic images at an unprecedented speed. This work focuses on models that operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework. Our method embeds a watermark directly at the bit level of the token stream during the image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs. The code is available at https://github.com/sprintml/BitMark.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Labelling for Low-Light Pedestrian Detection</title>
<link>https://arxiv.org/abs/2507.02513</link>
<guid>https://arxiv.org/abs/2507.02513</guid>
<content:encoded><![CDATA[
arXiv:2507.02513v2 Announce Type: replace 
Abstract: Pedestrian detection in RGB images is a key task in pedestrian safety, as the most common sensor in autonomous vehicles and advanced driver assistance systems is the RGB camera. A challenge in RGB pedestrian detection, that does not appear to have large public datasets, is low-light conditions. As a solution, in this research, we propose an automated infrared-RGB labeling pipeline. The proposed pipeline consists of 1) Infrared detection, where a fine-tuned model for infrared pedestrian detection is used 2) Label transfer process from the infrared detections to their RGB counterparts 3) Training object detection models using the generated labels for low-light RGB pedestrian detection. The research was performed using the KAIST dataset. For the evaluation, object detection models were trained on the generated autolabels and ground truth labels. When compared on a previously unseen image sequence, the results showed that the models trained on generated labels outperformed the ones trained on ground-truth labels in 6 out of 9 cases for the mAP@50 and mAP@50-95 metrics. The source code for this research is available at https://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying</title>
<link>https://arxiv.org/abs/2508.03142</link>
<guid>https://arxiv.org/abs/2508.03142</guid>
<content:encoded><![CDATA[
arXiv:2508.03142v2 Announce Type: replace 
Abstract: While Unified Vision-Language Models promise to synergistically combine the high-level semantic understanding of vision-language models with the generative fidelity of diffusion models, current editing methodologies remain fundamentally decoupled and open loop performing static, pre-defined transformations without dynamic feedback between semantic interpretation and visual generation. A central limitation stems from the representation gap: understanding typically leverages high-level, language aligned encoders, whereas generation relies on low level, pixel-space autoencoders, resulting in misaligned feature spaces. To bridge this gap, Recent advances such as Representation Autoencoders and BLIP3-o advocate performing diffusion-based modeling directly in high level features from pretrained semantic encoders. We find editing in the semantic latent space modifies conceptual representations rather than pixels, ensuring intermediates that are both semantically coherent and visually plausible. Building on this insight, We propose UniEdit-I, the first training-free, closed-loop image editing framework that operates entirely within the semantic latent space of a unified VLM by introducing an Understanding-Editing-Verifying (UEV) loop, By transforming the VLM from a posthoc evaluator into an in-process conductor, UniEdit-I establishes the first semantics-driven, self-correcting closed-loop image editing pipeline. Evaluated on GEdit-Bench, UniEdit-I achieves state of the art performance without any fine tuning or architectural modifications, and even surpasses several largescale pretrained editors.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.08136</link>
<guid>https://arxiv.org/abs/2508.08136</guid>
<content:encoded><![CDATA[
arXiv:2508.08136v2 Announce Type: replace 
Abstract: The success of 3DGS in generative and editing applications has sparked growing interest in 3DGS-based style transfer. However, current methods still face two major challenges: (1) multi-view inconsistency often leads to style conflicts, resulting in appearance smoothing and distortion; and (2) heavy reliance on VGG features, which struggle to disentangle style and content from style images, often causing content leakage and excessive stylization. To tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style transfer framework, and the first to rely entirely on diffusion model distillation. It comprises two key components: (1) \textbf{Multi-View Frequency Consistency}. We enhance cross-view consistency by applying a 3D filter to multi-view noisy latent, selectively reducing low-frequency components to mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized Distillation}. To suppress content leakage from style images, we introduce negative guidance to exclude undesired content. In addition, we identify the limitations of Score Distillation Sampling and Delta Denoising Score in 3D style transfer and remove the reconstruction term accordingly. Building on these insights, we propose a controllable stylized distillation that leverages negative guidance to more effectively optimize the 3D Gaussians. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art approaches, achieving higher stylization quality and visual realism across various scenes and styles. The code is available at https://github.com/yangyt46/FantasyStyle.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2508.11323</link>
<guid>https://arxiv.org/abs/2508.11323</guid>
<content:encoded><![CDATA[
arXiv:2508.11323v2 Announce Type: replace 
Abstract: 3D multi-object tracking is a critical and challenging task in the field of autonomous driving. A common paradigm relies on modeling individual object motion, e.g., Kalman filters, to predict trajectories. While effective in simple scenarios, this approach often struggles in crowded environments or with inaccurate detections, as it overlooks the rich geometric relationships between objects. This highlights the need to leverage spatial cues. However, existing geometry-aware methods can be susceptible to interference from irrelevant objects, leading to ambiguous features and incorrect associations. To address this, we propose focusing on cue-consistency: identifying and matching stable spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency Tracker (DSC-Track) to implement this principle. Firstly, we design a unified spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative trajectory embeddings while suppressing interference. Secondly, our cue-consistency transformer module explicitly aligns consistent feature representations between historical tracks and current detections. Finally, a dynamic update mechanism preserves salient spatiotemporal information for stable online tracking. Extensive experiments on the nuScenes and Waymo Open Datasets validate the effectiveness and robustness of our approach. On the nuScenes benchmark, for instance, our method achieves state-of-the-art performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets, respectively.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing</title>
<link>https://arxiv.org/abs/2508.12409</link>
<guid>https://arxiv.org/abs/2508.12409</guid>
<content:encoded><![CDATA[
arXiv:2508.12409v3 Announce Type: replace 
Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS) analysis by leveraging unlabeled data through pseudo-labeling and consistency learning. However, existing S4 studies often rely on small-scale datasets and models, limiting their practical applicability. To address this, we propose S5, the first scalable framework for semi-supervised semantic segmentation in RS, which unlocks the potential of vast unlabeled Earth observation data typically underutilized due to costly pixel-level annotations. Built upon existing large-scale RS datasets, S5 introduces a data selection strategy that integrates entropy-based filtering and diversity expansion, resulting in the RS4P-1M dataset. Using this dataset, we systematically scale up S4 into a new pretraining paradigm, S4 pre-training (S4P), to pretrain RS foundation models (RSFMs) of varying sizes on this extensive corpus, significantly boosting their performance on land cover segmentation and object detection tasks. Furthermore, during fine-tuning, we incorporate a Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which enables efficient adaptation to multiple RS benchmarks with fewer parameters. This approach improves the generalization and versatility of RSFMs across diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance across all benchmarks, underscoring the viability of scaling semi-supervised learning for RS applications. All datasets, code, and models will be released at https://github.com/MiliLab/S5
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecGen: Neural Spectral BRDF Generation via Spectral-Spatial Tri-plane Aggregation</title>
<link>https://arxiv.org/abs/2508.17316</link>
<guid>https://arxiv.org/abs/2508.17316</guid>
<content:encoded><![CDATA[
arXiv:2508.17316v2 Announce Type: replace 
Abstract: Synthesizing spectral images across different wavelengths is essential for photorealistic rendering. Unlike conventional spectral uplifting methods that convert RGB images into spectral ones, we introduce SpecGen, a novel method that generates spectral bidirectional reflectance distribution functions (BRDFs) from a single RGB image of a sphere. This enables spectral image rendering under arbitrary illuminations and shapes covered by the corresponding material. A key challenge in spectral BRDF generation is the scarcity of measured spectral BRDF data. To address this, we propose the Spectral-Spatial Tri-plane Aggregation (SSTA) network, which models reflectance responses across wavelengths and incident-outgoing directions, allowing the training strategy to leverage abundant RGB BRDF data to enhance spectral BRDF generation. Experiments show that our method accurately reconstructs spectral BRDFs from limited spectral data and surpasses state-of-the-art methods in hyperspectral image reconstruction, achieving an improvement of 8 dB in PSNR. Codes and data will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery</title>
<link>https://arxiv.org/abs/2508.19499</link>
<guid>https://arxiv.org/abs/2508.19499</guid>
<content:encoded><![CDATA[
arXiv:2508.19499v2 Announce Type: replace 
Abstract: Origin-Destination (OD) flow matrices are critical for urban mobility analysis, supporting traffic forecasting, infrastructure planning, and policy design. Existing methods face two key limitations: (1) reliance on costly auxiliary features (e.g., Points of Interest, socioeconomic statistics) with limited spatial coverage, and (2) fragility to spatial topology changes, where reordering urban regions disrupts the structural coherence of generated flows. We propose Sat2Flow, a structure-aware diffusion framework that generates structurally coherent OD flows using only satellite imagery. Our approach employs a multi-kernel encoder to capture diverse regional interactions and a permutation-aware diffusion process that maintains consistency across regional orderings. Through joint contrastive training linking satellite features with OD patterns and equivariant diffusion training enforcing structural invariance, Sat2Flow ensures topological robustness under arbitrary regional reindexing. Experiments on real-world datasets show that Sat2Flow outperforms physics-based and data-driven baselines in accuracy while preserving flow distributions and spatial structures under index permutations. Sat2Flow offers a globally scalable solution for OD flow generation in data-scarce environments, eliminating region-specific auxiliary data dependencies while maintaining structural robustness for reliable mobility modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Driven Object-Oriented Two-Stage Method for Scene Graph Anticipation</title>
<link>https://arxiv.org/abs/2509.05661</link>
<guid>https://arxiv.org/abs/2509.05661</guid>
<content:encoded><![CDATA[
arXiv:2509.05661v2 Announce Type: replace 
Abstract: A scene graph is a structured representation of objects and their spatio-temporal relationships in dynamic scenes. Scene Graph Anticipation (SGA) involves predicting future scene graphs from video clips, enabling applications in intelligent surveillance and human-machine collaboration. While recent SGA approaches excel at leveraging visual evidence, long-horizon forecasting fundamentally depends on semantic priors and commonsense temporal regularities that are challenging to extract purely from visual features. To explicitly model these semantic dynamics, we propose Linguistic Scene Graph Anticipation (LSGA), a linguistic formulation of SGA that performs temporal relational reasoning over sequences of textualized scene graphs, with visual scene-graph detection handled by a modular front-end when operating on video. Building on this formulation, we introduce Object-Oriented Two-Stage Method (OOTSM), a language-based framework that anticipates object-set dynamics and forecasts object-centric relation trajectories with temporal consistency regularization, and we evaluate it on a dedicated benchmark constructed from Action Genome annotations. Extensive experiments show that compact fine-tuned language models with up to 3B parameters consistently outperform strong zero- and one-shot API baselines, including GPT-4o, GPT-4o-mini, and DeepSeek-V3, under matched textual inputs and context windows. When coupled with off-the-shelf visual scene-graph generators, the resulting multimodal system achieves substantial improvements on video-based SGA, boosting long-horizon mR@50 by up to 21.9\% over strong visual SGA baselines.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D and 4D World Modeling: A Survey</title>
<link>https://arxiv.org/abs/2509.07996</link>
<guid>https://arxiv.org/abs/2509.07996</guid>
<content:encoded><![CDATA[
arXiv:2509.07996v3 Announce Type: replace 
Abstract: World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/awesome-3d-4d-world-models
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception</title>
<link>https://arxiv.org/abs/2509.09828</link>
<guid>https://arxiv.org/abs/2509.09828</guid>
<content:encoded><![CDATA[
arXiv:2509.09828v2 Announce Type: replace 
Abstract: Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DeLiVER datasets. Code and models will be available at https://github.com/timbroed/DGFusion
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Data Challenges of Computational Pathology: A Pack-based Multiple Instance Learning Training Framework</title>
<link>https://arxiv.org/abs/2509.20923</link>
<guid>https://arxiv.org/abs/2509.20923</guid>
<content:encoded><![CDATA[
arXiv:2509.20923v2 Announce Type: replace 
Abstract: Computational pathology (CPath) digitizes pathology slides into whole slide images (WSIs), enabling analysis for critical healthcare tasks such as cancer diagnosis and prognosis. However, WSIs possess extremely long sequence lengths (up to 200K), significant length variations (from 200 to 200K), and limited supervision. These extreme variations in sequence length lead to high data heterogeneity and redundancy. Conventional methods often compromise on training efficiency and optimization to preserve such heterogeneity under limited supervision. To comprehensively address these challenges, we propose a pack-based MIL framework. It packs multiple sampled, variable-length feature sequences into fixed-length ones, enabling batched training while preserving data heterogeneity. Moreover, we introduce a residual branch that composes discarded features from multiple slides into a hyperslide which is trained with tailored labels. It offers multi-slide supervision while mitigating feature loss from sampling. Meanwhile, an attention-driven downsampler is introduced to compress features in both branches to reduce redundancy. By alleviating these challenges, our approach achieves an accuracy improvement of up to 8% while using only 12% of the training time in the PANDA(UNI). Extensive experiments demonstrate that focusing data challenges in CPath holds significant potential in the era of foundation models. The code is https://github.com/FangHeng/PackMIL
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Alignment of Popular CNNs to the Brain for Valence Appraisal</title>
<link>https://arxiv.org/abs/2509.21384</link>
<guid>https://arxiv.org/abs/2509.21384</guid>
<content:encoded><![CDATA[
arXiv:2509.21384v2 Announce Type: replace 
Abstract: Convolutional Neural Networks (CNNs) are a popular type of computer model that have proven their worth in many computer vision tasks. Moreover, they form an interesting study object for the field of psychology, with shown correspondences between the workings of CNNs and the human brain. However, these correspondences have so far mostly been studied in the context of general visual perception. In contrast, this paper explores to what extent this correspondence also holds for a more complex brain process, namely social cognition. To this end, we assess the alignment between popular CNN architectures and both human behavioral and fMRI data for image valence appraisal through a correlation analysis. We show that for this task CNNs struggle to go beyond simple visual processing, and do not seem to reflect higher-order brain processing. Furthermore, we present Object2Brain, a novel framework that combines GradCAM and object detection at the CNN-filter level with the aforementioned correlation analysis to study the influence of different object classes on the CNN-to-human correlations. Despite similar correlation trends, different CNN architectures are shown to display different object class sensitivities.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation</title>
<link>https://arxiv.org/abs/2509.24980</link>
<guid>https://arxiv.org/abs/2509.24980</guid>
<content:encoded><![CDATA[
arXiv:2509.24980v2 Announce Type: replace 
Abstract: Pre-trained diffusion models provide rich multi-scale latent features and are emerging as powerful vision backbones. While recent works such as Marigold and Lotus adapt diffusion priors for dense prediction with strong cross-domain generalization, their potential for structured outputs remains underexplored. In this paper, we propose SDPose, a fine-tuning framework built upon Stable Diffusion to fully exploit pre-trained diffusion priors for human pose estimation. First, rather than modifying cross-attention modules or introducing learnable embeddings, we directly predict keypoint heatmaps in the SD U-Net's image latent space to preserve the original generative priors. Second, we map these latent features into keypoint heatmaps through a lightweight convolutional pose head, which avoids disrupting the pre-trained backbone. Finally, to prevent overfitting and enhance out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction branch that preserves domain-transferable generative semantics. To evaluate robustness under domain shift, we further construct COCO-OOD, a style-transferred variant of COCO with preserved annotations. With just one-fifth of the training schedule used by Sapiens on COCO, SDPose attains parity with Sapiens-1B/2B on the COCO validation set and establishes a new state of the art on the cross-domain benchmarks HumanArt and COCO-OOD. Extensive ablations highlight the importance of diffusion priors, RGB reconstruction, and multi-scale SD U-Net features for cross-domain generalization, and t-SNE analyses further explain SD's domain-invariant latent structure. We also show that SDPose serves as an effective zero-shot pose annotator for controllable image and video generation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Score Distillation of Flow Matching Models</title>
<link>https://arxiv.org/abs/2509.25127</link>
<guid>https://arxiv.org/abs/2509.25127</guid>
<content:encoded><![CDATA[
arXiv:2509.25127v2 Announce Type: replace 
Abstract: Diffusion models achieve high-quality image generation but are limited by slow iterative sampling. Distillation methods alleviate this by enabling one- or few-step generation. Flow matching, originally introduced as a distinct framework, has since been shown to be theoretically equivalent to diffusion under Gaussian assumptions, raising the question of whether distillation techniques such as score distillation transfer directly. We provide a simple derivation -- based on Bayes' rule and conditional expectations -- that unifies Gaussian diffusion and flow matching without relying on ODE/SDE formulations. Building on this view, we extend Score identity Distillation (SiD) to pretrained text-to-image flow-matching models, including SANA, SD3-Medium, SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show that, with only modest flow-matching- and DiT-specific adjustments, SiD works out of the box across these models, in both data-free and data-aided settings, without requiring teacher finetuning or architectural changes. This provides the first systematic evidence that score distillation applies broadly to text-to-image flow matching models, resolving prior concerns about stability and soundness and unifying acceleration techniques across diffusion- and flow-based generators. A project page is available at https://yigu1008.github.io/SiD-DiT.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes</title>
<link>https://arxiv.org/abs/2510.03747</link>
<guid>https://arxiv.org/abs/2510.03747</guid>
<content:encoded><![CDATA[
arXiv:2510.03747v2 Announce Type: replace 
Abstract: Deepfakes pose significant societal risks, motivating the development of proactive defenses that embed adversarial perturbations in facial images to prevent manipulation. However, in this paper, we show that these preemptive defenses often lack robustness and reliability. We propose a novel approach, Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch into Deepfake generators to bypass state-of-the-art defenses. A learnable gating mechanism adaptively controls the effect of the LoRA patch and prevents gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature Alignment (MMFA) loss, encouraging the features of adversarial outputs to align with those of the desired outputs at the semantic level. Beyond bypassing, we present defensive LoRA patching, embedding visible warnings in the outputs as a complementary solution to mitigate this newly identified security vulnerability. With only 1,000 facial examples and a single epoch of fine-tuning, LoRA patching successfully defeats multiple proactive defenses. These results reveal a critical weakness in current paradigms and underscore the need for more robust Deepfake defense strategies. Our code is available at https://github.com/ZOMIN28/LoRA-Patching.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control</title>
<link>https://arxiv.org/abs/2510.13186</link>
<guid>https://arxiv.org/abs/2510.13186</guid>
<content:encoded><![CDATA[
arXiv:2510.13186v4 Announce Type: replace 
Abstract: Edge Gaussian splatting (EGS), which aggregates data from distributed clients (e.g., drones) and trains a global GS model at the edge (e.g., ground server), is an emerging paradigm for scene reconstruction in low-altitude economy. Unlike traditional edge resource management methods that emphasize communication throughput or general-purpose learning performance, EGS explicitly aims to maximize the GS qualities, rendering existing approaches inapplicable. To address this problem, this paper formulates a novel GS-oriented objective function that distinguishes the heterogeneous view contributions of different clients. However, evaluating this function in turn requires clients' images, leading to a causality dilemma. To this end, this paper further proposes a sample-then-transmit EGS (or STT-GS for short) strategy, which first samples a subset of images as pilot data from each client for loss prediction. Based on the first-stage evaluation, communication resources are then prioritized towards more valuable clients. To achieve efficient sampling, a feature-domain clustering (FDC) scheme is proposed to select the most representative data and pilot transmission time minimization (PTTM) is adopted to reduce the pilot overhead. Subsequently, we develop a joint client selection and power control (JCSPC) framework to maximize the GS-oriented function under communication resource constraints. Despite the nonconvexity of the problem, we propose a low-complexity efficient solution based on the penalty alternating majorization minimization (PAMM) algorithm. Experiments reveal that the proposed scheme significantly outperforms existing benchmarks on real-world datasets. The GS-oriented objective can be accurately predicted with low sampling ratios (e.g., 10%), and our method achieves an excellent tradeoff between view contributions and communication costs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</title>
<link>https://arxiv.org/abs/2510.13747</link>
<guid>https://arxiv.org/abs/2510.13747</guid>
<content:encoded><![CDATA[
arXiv:2510.13747v2 Announce Type: replace 
Abstract: We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch</title>
<link>https://arxiv.org/abs/2510.16088</link>
<guid>https://arxiv.org/abs/2510.16088</guid>
<content:encoded><![CDATA[
arXiv:2510.16088v4 Announce Type: replace 
Abstract: Quantization of neural networks provides benefits of inference in less compute and memory requirements. Previous work in quantization lack two important aspects which this work provides. First almost all previous work in quantization used a non-differentiable approach and for learning; the derivative is usually set manually in backpropogation which make the learning ability of algorithm questionable, our approach is not just differentiable, we also provide proof of convergence of our approach to the optimal neural network. Second previous work in shift/logrithmic quantization either have avoided activation quantization along with weight quantization or achieved less accuracy. Learning logrithmic quantize values of form $2^n$ requires the quantization function can scale to more than 1 bit quantization which is another benifit of our quantization that it provides $n$ bits quantization as well. Our approach when tested with image classification task using imagenet dataset, resnet18 and weight quantization only achieves less than 1 percent accuracy compared to full precision accuracy while taking only 15 epochs to train using shift bit quantization and achieves comparable to SOTA approaches accuracy in both weight and activation quantization using shift bit quantization in 15 training epochs with slightly higher(only higher cpu instructions) inference cost compared to 1 bit quantization(without logrithmic quantization) and not requiring any higher precision multiplication.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
<link>https://arxiv.org/abs/2510.18214</link>
<guid>https://arxiv.org/abs/2510.18214</guid>
<content:encoded><![CDATA[
arXiv:2510.18214v2 Announce Type: replace 
Abstract: Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Based Mistake Analysis in Procedural Activities: A Review of Advances and Challenges</title>
<link>https://arxiv.org/abs/2510.19292</link>
<guid>https://arxiv.org/abs/2510.19292</guid>
<content:encoded><![CDATA[
arXiv:2510.19292v2 Announce Type: replace 
Abstract: Mistake analysis in procedural activities is a critical area of research with applications spanning industrial automation, physical rehabilitation, education and human-robot collaboration. This paper reviews vision-based methods for detecting and predicting mistakes in structured tasks, focusing on procedural and executional errors. By leveraging advancements in computer vision, including action recognition, anticipation and activity understanding, vision-based systems can identify deviations in task execution, such as incorrect sequencing, use of improper techniques, or timing errors. We explore the challenges posed by intra-class variability, viewpoint differences and compositional activity structures, which complicate mistake detection. Additionally, we provide a comprehensive overview of existing datasets, evaluation metrics and state-of-the-art methods, categorizing approaches based on their use of procedural structure, supervision levels and learning strategies. Open challenges, such as distinguishing permissible variations from true mistakes and modeling error propagation are discussed alongside future directions, including neuro-symbolic reasoning and counterfactual state modeling. This work aims to establish a unified perspective on vision-based mistake analysis in procedural activities, highlighting its potential to enhance safety, efficiency and task performance across diverse domains.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2510.26641</link>
<guid>https://arxiv.org/abs/2510.26641</guid>
<content:encoded><![CDATA[
arXiv:2510.26641v2 Announce Type: replace 
Abstract: Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MagicView: Multi-View Consistent Identity Customization via Priors-Guided In-Context Learning</title>
<link>https://arxiv.org/abs/2511.00293</link>
<guid>https://arxiv.org/abs/2511.00293</guid>
<content:encoded><![CDATA[
arXiv:2511.00293v2 Announce Type: replace 
Abstract: Recent advances in personalized generative models have demonstrated impressive capabilities in producing identity-consistent images of the same individual across diverse scenes. However, most existing methods lack explicit viewpoint control and fail to ensure multi-view consistency of generated identities. To address this limitation, we present MagicView, a lightweight adaptation framework that equips existing generative models with multi-view generation capability through 3D priors-guided in-context learning. While prior studies have shown that in-context learning preserves identity consistency across grid samples, its effectiveness in multi-view settings remains unexplored. Building upon this insight, we conduct an in-depth analysis of the multi-view in-context learning ability, and design a conditioning architecture that leverages 3D priors to activate this capability for multi-view consistent identity customization. On the other hand, acquiring robust multi-view capability typically requires large-scale multi-dimensional datasets, which makes incorporating multi-view contextual learning under limited data regimes prone to textual controllability degradation. To address this issue, we introduce a novel Semantic Correspondence Alignment loss, which effectively preserves semantic alignment while maintaining multi-view consistency. Extensive experiments demonstrate that MagicView substantially outperforms recent baselines in multi-view consistency, text alignment, identity similarity, and visual quality, achieving strong results with only 100 multi-view training samples.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation</title>
<link>https://arxiv.org/abs/2511.12528</link>
<guid>https://arxiv.org/abs/2511.12528</guid>
<content:encoded><![CDATA[
arXiv:2511.12528v2 Announce Type: replace 
Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images</title>
<link>https://arxiv.org/abs/2511.16717</link>
<guid>https://arxiv.org/abs/2511.16717</guid>
<content:encoded><![CDATA[
arXiv:2511.16717v2 Announce Type: replace 
Abstract: Neutron imaging is essential for diagnosing and optimizing inertial confinement fusion implosions at the National Ignition Facility. Due to the required 10-micrometer resolution, however, neutron image require image reconstruction using iterative algorithms. For low-yield sources, the images may be degraded by various types of noise. Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring the edges where the source information is encoded. Traditional denoising techniques, such as filtering and thresholding, can inadvertently alter critical features or reshape the noise statistics, potentially impacting the ultimate fidelity of the iterative image reconstruction pipeline. However, recent advances in synthetic data production and machine learning have opened new opportunities to address these challenges. In this study, we present an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space, designed to suppress for mixed Gaussian-Poisson noise while preserving essential image features. The network successfully denoises neutron imaging data. Benchmarking against both simulated and experimental NIF datasets demonstrates that our approach achieves lower reconstruction error and superior edge preservation compared to conventional filtering methods such as Block-matching and 3D filtering (BM3D). By validating the effectiveness of unsupervised learning for denoising neutron images, this study establishes a critical first step towards fully AI-driven, end-to-end reconstruction frameworks for ICF diagnostics.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Transferable Optimal Transport via Min-Sliced Transport Plans</title>
<link>https://arxiv.org/abs/2511.19741</link>
<guid>https://arxiv.org/abs/2511.19741</guid>
<content:encoded><![CDATA[
arXiv:2511.19741v2 Announce Type: replace 
Abstract: Optimal Transport (OT) offers a powerful framework for finding correspondences between distributions and addressing matching and alignment problems in various areas of computer vision, including shape analysis, image generation, and multimodal tasks. The computation cost of OT, however, hinders its scalability. Slice-based transport plans have recently shown promise for reducing the computational cost by leveraging the closed-form solutions of 1D OT problems. These methods optimize a one-dimensional projection (slice) to obtain a conditional transport plan that minimizes the transport cost in the ambient space. While efficient, these methods leave open the question of whether learned optimal slicers can transfer to new distribution pairs under distributional shift. Understanding this transferability is crucial in settings with evolving data or repeated OT computations across closely related distributions. In this paper, we study the min-Sliced Transport Plan (min-STP) framework and investigate the transferability of optimized slicers: can a slicer trained on one distribution pair yield effective transport plans for new, unseen pairs? Theoretically, we show that optimized slicers remain close under slight perturbations of the data distributions, enabling efficient transfer across related tasks. To further improve scalability, we introduce a minibatch formulation of min-STP and provide statistical guarantees on its accuracy. Empirically, we demonstrate that the transferable min-STP achieves strong one-shot matching performance and facilitates amortized training for point cloud alignment and flow-based generative modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web</title>
<link>https://arxiv.org/abs/2409.18980</link>
<guid>https://arxiv.org/abs/2409.18980</guid>
<content:encoded><![CDATA[
arXiv:2409.18980v2 Announce Type: replace-cross 
Abstract: Recently advancements in large multimodal models have led to significant strides in image comprehension capabilities. Despite these advancements, there is a lack of the robust benchmark specifically for assessing the Image-to-Web conversion proficiency of these large models. Primarily, it is essential to ensure the integrity of the web elements generated. These elements comprise visible and invisible categories. Previous evaluation methods (e.g.,BLEU) are notably susceptible to significant alterations due to the presence of invisible elements in Web. Furthermore, it is crucial to measure the layout information of web pages, referring to the positional relationships between elements, which is overlooked by previous work. To address challenges, we have curated and aligned a benchmark of images and corresponding web codes (IW-BENCH). Specifically, we propose the Element Accuracy, which tests the completeness of the elements by parsing the Document Object Model (DOM) tree. Layout Accuracy is also proposed to analyze the positional relationships of elements by converting DOM tree into a common subsequence. Besides, we design a five-hop multimodal Chain-of-Thought Prompting for better performance, which contains five hop: 1) SoM prompt injection. 2) Inferring Elements. 3) Inferring Layout. 4) Inferring Web code. 5) Reflection. Our benchmark comprises 1200 pairs of images and web codes with varying levels of difficulty. We have conducted extensive experiments on existing large multimodal models, offering insights into their performance and areas for improvement in image-to-web domain.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tractable Two-Step Linear Mixing Model Solved with Second-Order Optimization for Spectral Unmixing under Variability</title>
<link>https://arxiv.org/abs/2502.17212</link>
<guid>https://arxiv.org/abs/2502.17212</guid>
<content:encoded><![CDATA[
arXiv:2502.17212v3 Announce Type: replace-cross 
Abstract: In this paper, we propose a Two-Step Linear Mixing Model (2LMM) that bridges the gap between model complexity and computational tractability. The model achieves this by introducing two distinct scaling steps: an endmember scaling step across the image, and another for pixel-wise scaling. We show that this model leads to only a mildly non-convex optimization problem, which we solve with an optimization algorithm that incorporates second-order information. To the authors' knowledge, this work represents the first application of second-order optimization techniques to solve a spectral unmixing problem that models endmember variability. Our method is highly robust, as it requires virtually no hyperparameter tuning and can therefore be used easily and quickly in a wide range of unmixing tasks. We show through extensive experiments on both simulated and real data that the new model is competitive and in some cases superior to the state of the art in unmixing. The model also performs very well in challenging scenarios, such as blind unmixing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PixCell: A generative foundation model for digital histopathology images</title>
<link>https://arxiv.org/abs/2506.05127</link>
<guid>https://arxiv.org/abs/2506.05127</guid>
<content:encoded><![CDATA[
arXiv:2506.05127v2 Announce Type: replace-cross 
Abstract: The digitization of histology slides has revolutionized pathology, providing massive datasets for cancer diagnosis and research. Self-supervised and vision-language models have been shown to effectively mine large pathology datasets to learn discriminative representations. On the other hand, there are unique problems in pathology, such as annotated data scarcity, privacy regulations in data sharing, and inherently generative tasks like virtual staining. Generative models, capable of synthesizing realistic and diverse images, present a compelling solution to address these problems through image synthesis. We introduce PixCell, the first generative foundation model for histopathology images. PixCell is a diffusion model trained on PanCan-30M, a large, diverse dataset derived from 69,184 H&amp;E-stained whole slide images of various cancer types. We employ a progressive training strategy and a self-supervision-based conditioning that allows us to scale up training without any human-annotated data. By conditioning on real slides, the synthetic images capture the properties of the real data and can be used as data augmentation for small-scale datasets to boost classification performance. We prove the foundational versatility of PixCell by applying it to two generative downstream tasks: privacy-preserving synthetic data generation and virtual IHC staining. PixCell's high-fidelity conditional generation enables institutions to use their private data to synthesize highly realistic, site-specific surrogate images that can be shared in place of raw patient data. Furthermore, using datasets of roughly paired H&amp;E-IHC tiles, we learn to translate PixCell's conditioning from H&amp;E to multiple IHC stains, allowing the generation of IHC images from H&amp;E inputs. Our trained models are publicly released to accelerate research in computational pathology.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism</title>
<link>https://arxiv.org/abs/2507.01513</link>
<guid>https://arxiv.org/abs/2507.01513</guid>
<content:encoded><![CDATA[
arXiv:2507.01513v2 Announce Type: replace-cross 
Abstract: By incorporating visual inputs, Multimodal Large Language Models (MLLMs) extend LLMs to support visual reasoning. However, this integration also introduces new vulnerabilities, making MLLMs susceptible to multimodal jailbreak attacks and hindering their safe deployment.Existing defense methods, including Image-to-Text Translation, Safe Prompting, and Multimodal Safety Tuning, attempt to address this by aligning multimodal inputs with LLMs' built-in safeguards.Yet, they fall short in uncovering root causes of multimodal vulnerabilities, particularly how harmful multimodal tokens trigger jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing heavy training overhead.To bridge this gap, we present an comprehensive analysis of where, how and which harmful multimodal tokens bypass safeguards in MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers are responsible for inducing unsafe behaviors, highlighting the potential of precisely removing a small subset of harmful tokens, without requiring safety tuning, can still effectively improve safety against jailbreaks. Motivated by this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense framework that selectively prunes harmful tokens at vulnerable layers while restoring benign features at subsequent layers.Without incurring additional computational overhead, SafePTR significantly enhances the safety of MLLMs while preserving efficiency. Extensive evaluations across three MLLMs and five benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating jailbreak risks without compromising utility.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation</title>
<link>https://arxiv.org/abs/2508.03758</link>
<guid>https://arxiv.org/abs/2508.03758</guid>
<content:encoded><![CDATA[
arXiv:2508.03758v3 Announce Type: replace-cross 
Abstract: Automated segmentation of diabetic foot ulcers (DFUs) plays a critical role in clinical diagnosis, therapeutic planning, and longitudinal wound monitoring. However, this task remains challenging due to the heterogeneous appearance, irregular morphology, and complex backgrounds associated with ulcer regions in clinical photographs. Traditional convolutional neural networks (CNNs), such as U-Net, provide strong localization capabilities but struggle to model long-range spatial dependencies due to their inherently limited receptive fields. To address this, we employ the TransUNet architecture, a hybrid framework that integrates the global attention mechanism of Vision Transformers (ViTs) into the U-Net structure. This combination allows the model to extract global contextual features while maintaining fine-grained spatial resolution. We trained the model on the public Foot Ulcer Segmentation Challenge (FUSeg) dataset using a robust augmentation pipeline and a hybrid loss function to mitigate class imbalance. On the validation set, the model achieved a Dice Similarity Coefficient (F1-score) of 0.8799 using an optimized threshold of 0.4389. To ensure clinical transparency, we integrated Grad-CAM visualizations to highlight model focus areas. Furthermore, a clinical utility analysis demonstrated a strong correlation (Pearson r = 0.9631) between predicted and ground-truth wound areas. These outcomes demonstrate that our approach effectively integrates global and local feature extraction, offering a reliable, effective, and explainable solution for automated foot ulcer assessment.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain Vessel Segmentation on Transcranial Color-coded Doppler</title>
<link>https://arxiv.org/abs/2508.13875</link>
<guid>https://arxiv.org/abs/2508.13875</guid>
<content:encoded><![CDATA[
arXiv:2508.13875v2 Announce Type: replace-cross 
Abstract: The Circle of Willis (CoW), vital for ensuring consistent blood flow to the brain, is closely linked to ischemic stroke. Accurate assessment of the CoW is important for identifying individuals at risk and guiding appropriate clinical management. Among existing imaging methods, Transcranial Color-coded Doppler (TCCD) offers unique advantages due to its radiation-free nature, affordability, and accessibility. However, reliable TCCD assessments depend heavily on operator expertise for identifying anatomical landmarks and performing accurate angle correction, which limits its widespread adoption. To address this challenge, we propose an AI-powered, real-time CoW auto-segmentation system capable of efficiently capturing cerebral arteries. No prior studies have explored AI-driven cerebrovascular segmentation using TCCD. In this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO) network tailored for TCCD data, designed to provide real-time guidance for brain vessel segmentation in the CoW. We prospectively collected TCCD data comprising 738 annotated frames and 3,419 labeled artery instances to establish a high-quality dataset for model training and evaluation. The proposed AAW-YOLO demonstrated strong performance in segmenting both ipsilateral and contralateral CoW vessels, achieving an average Dice score of 0.901, IoU of 0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame inference speed of 14.199 ms. This system offers a practical solution to reduce reliance on operator experience in TCCD-based cerebrovascular screening, with potential applications in routine clinical workflows and resource-constrained settings. Future research will explore bilateral modeling and larger-scale validation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail</title>
<link>https://arxiv.org/abs/2509.23762</link>
<guid>https://arxiv.org/abs/2509.23762</guid>
<content:encoded><![CDATA[
arXiv:2509.23762v3 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) have attracted growing interest in both computational neuroscience and artificial intelligence, primarily due to their inherent energy efficiency and compact memory footprint. However, achieving adversarial robustness in SNNs, (particularly for vision-related tasks) remains a nascent and underexplored challenge. Recent studies have proposed leveraging sparse gradients as a form of regularization to enhance robustness against adversarial perturbations. In this work, we present a surprising finding: under specific architectural configurations, SNNs exhibit natural gradient sparsity and can achieve state-of-the-art adversarial defense performance without the need for any explicit regularization. Further analysis reveals a trade-off between robustness and generalization: while sparse gradients contribute to improved adversarial resilience, they can impair the model's ability to generalize; conversely, denser gradients support better generalization but increase vulnerability to attacks. Our findings offer new insights into the dual role of gradient sparsity in SNN training.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MACS: Measurement-Aware Consistency Sampling for Inverse Problems</title>
<link>https://arxiv.org/abs/2510.02208</link>
<guid>https://arxiv.org/abs/2510.02208</guid>
<content:encoded><![CDATA[
arXiv:2510.02208v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative priors for solving inverse imaging problems. However, their practical deployment is hindered by the substantial computational cost of slow, multi-step sampling. Although Consistency Models (CMs) address this limitation by enabling high-quality generation in only one or a few steps, their direct application to inverse problems has remained largely unexplored. This paper introduces a modified consistency sampling framework specifically designed for inverse problems. The proposed approach regulates the sampler's stochasticity through a measurement-consistency mechanism that leverages the degradation operator, thereby enforcing fidelity to the observed data while preserving the computational efficiency of consistency-based generation. Comprehensive experiments on the Fashion-MNIST and LSUN Bedroom datasets demonstrate consistent improvements across both perceptual and pixel-level metrics, including the Fr\'echet Inception Distance (FID), Kernel Inception Distance (KID), peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM), compared with baseline consistency and diffusion-based sampling methods. The proposed method achieves competitive or superior reconstruction quality with only a small number of sampling steps.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Multi-Domain Translation via Diffusion Routers</title>
<link>https://arxiv.org/abs/2510.03252</link>
<guid>https://arxiv.org/abs/2510.03252</guid>
<content:encoded><![CDATA[
arXiv:2510.03252v2 Announce Type: replace-cross 
Abstract: Multi-domain translation (MDT) aims to learn translations between multiple domains, yet existing approaches either require fully aligned tuples or can only handle domain pairs seen in training, limiting their practicality and excluding many cross-domain mappings. We introduce universal MDT (UMDT), a generalization of MDT that seeks to translate between any pair of $K$ domains using only $K-1$ paired datasets with a central domain. To tackle this problem, we propose Diffusion Router (DR), a unified diffusion-based framework that models all central$\leftrightarrow$non-central translations with a single noise predictor conditioned on the source and target domain labels. DR enables indirect non-central translations by routing through the central domain. We further introduce a novel scalable learning strategy with a variational-bound objective and an efficient Tweedie refinement procedure to support direct non-central mappings. Through evaluation on three large-scale UMDT benchmarks, DR achieves state-of-the-art results for both indirect and direct translations, while lowering sampling cost and unlocking novel tasks such as sketch$\leftrightarrow$segmentation. These results establish DR as a scalable and versatile framework for universal translation across multiple domains.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Cascade Compression: Exploring the Upper Limits of Text Compression</title>
<link>https://arxiv.org/abs/2511.15244</link>
<guid>https://arxiv.org/abs/2511.15244</guid>
<content:encoded><![CDATA[
arXiv:2511.15244v2 Announce Type: replace-cross 
Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale</title>
<link>https://arxiv.org/abs/2512.02055</link>
<guid>https://arxiv.org/abs/2512.02055</guid>
<content:encoded><![CDATA[
<div> Keywords: Flood mapping, Geospatial Foundation Models, Sentinel-1, Sentinel-2, FloodsNet<br /><br />Summary:  
1. Floods are among the most damaging weather-related hazards, with extreme events affecting communities globally, especially in 2024, the warmest year on record.  
2. Earth observation satellites, like Sentinel-1 (SAR) and Sentinel-2 (optical), provide crucial data for flood inundation mapping, but operational accuracy relies heavily on quality labeled datasets and model generalization.  
3. Geospatial Foundation Models (GFMs), exemplified by ESA-IBM's TerraMind, improve generalization through large-scale self-supervised pretraining, yet their performance on diverse global flood events has been underexplored.  
4. This study fine-tuned TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset covering 85 global flood events, testing four configurations varying model sizes and whether backbones were frozen or unfrozen.  
5. The base model with unfrozen backbone balanced accuracy, precision, and recall with significantly lower computational cost. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed those trained on Sen1Floods11 in recall, with similar overall accuracy.  
6. A U-Net trained on both datasets achieved higher recall than all GFM configurations, but with slightly lower accuracy and precision.  
7. Results highlight that integrating multimodal optical and SAR data and fine-tuning a GFM enhances near-real-time flood mapping capabilities.  
8. This represents one of the first global-scale evaluations of a GFM for flood segmentation, outlining its promise and current limitations for climate adaptation and disaster resilience. <div>
arXiv:2512.02055v1 Announce Type: new 
Abstract: Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.
  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.
  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework</title>
<link>https://arxiv.org/abs/2512.02152</link>
<guid>https://arxiv.org/abs/2512.02152</guid>
<content:encoded><![CDATA[
<div> Keywords: Contrastive learning, contrastive loss, information distortion, augmented samples, image classification<br /><br />Summary:<br /><br />This paper addresses shortcomings in traditional contrastive learning where models often over-rely on label-based similarities and neglect positive pairs from the same source image, causing information distortion especially in large datasets. The authors propose a novel context-enriched contrastive loss function that incorporates two convergence targets: one sensitive to label contrasts to effectively distinguish features of identical versus distinct classes, and another that brings augmented samples from the same original image closer while distancing others. This dual-target approach aims to enhance both learning efficiency and reduce information distortion. The method was evaluated on eight large-scale image classification benchmark datasets including CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA. Experimental results showed the proposed approach outperformed 16 state-of-the-art contrastive learning methods in terms of generalization and convergence speed. Notably, the approach demonstrated a significant 22.9% improvement over original contrastive loss functions on the BiasedMNIST dataset, which involves systematic distortion tasks. These results suggest that the context-enriched contrastive loss enhances training efficiency and leads to more equitable downstream model performance across diverse tasks and datasets. <div>
arXiv:2512.02152v1 Announce Type: new 
Abstract: Contrastive learning has gained popularity and pushes state-of-the-art performance across numerous large-scale benchmarks. In contrastive learning, the contrastive loss function plays a pivotal role in discerning similarities between samples through techniques such as rotation or cropping. However, this learning mechanism can also introduce information distortion from the augmented samples. This is because the trained model may develop a significant overreliance on information from samples with identical labels, while concurrently neglecting positive pairs that originate from the same initial image, especially in expansive datasets. This paper proposes a context-enriched contrastive loss function that concurrently improves learning effectiveness and addresses the information distortion by encompassing two convergence targets. The first component, which is notably sensitive to label contrast, differentiates between features of identical and distinct classes which boosts the contrastive training efficiency. Meanwhile, the second component draws closer the augmented samples from the same source image and distances all other samples. We evaluate the proposed approach on image classification tasks, which are among the most widely accepted 8 recognition large-scale benchmark datasets: CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA datasets. The experimental results demonstrate that the proposed method achieves improvements over 16 state-of-the-art contrastive learning methods in terms of both generalization performance and learning convergence speed. Interestingly, our technique stands out in addressing systematic distortion tasks. It demonstrates a 22.9% improvement compared to original contrastive loss functions in the downstream BiasedMNIST dataset, highlighting its promise for more efficient and equitable downstream training.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges</title>
<link>https://arxiv.org/abs/2512.02161</link>
<guid>https://arxiv.org/abs/2512.02161</guid>
<content:encoded><![CDATA[
<div> Text-to-image generation, Vision language models, Failure modes, Attribute fidelity, Benchmarking<br /><br />Summary:  
This paper addresses the limitations of current text-to-image (T2I) models, which, despite generating visually impressive images, often fail to accurately depict specific attributes from user prompts, such as the correct number and color of objects. To better evaluate these models, the authors propose a hierarchical evaluation framework designed to compare how well different T2I models adhere to prompt details. Recognizing a gap in benchmarks for vision language models (VLMs), especially with complex scenes, the study introduces a structured methodology for jointly assessing T2I models and VLMs. This involves testing whether VLMs can detect 27 distinct failure modes present in images generated by T2I systems under challenging prompts. The authors contribute a novel dataset comprising prompts, images generated by five T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large), and annotations from three VLMs (Molmo, InternVL3, Pixtral), with labels curated by the Llama3 large language model to verify failure mode identifications. The analysis uncovers systematic errors related to attribute fidelity and object representation in generated images. The findings underscore the inadequacy of existing evaluation metrics to fully capture these nuanced errors and highlight the critical need for targeted benchmarks to enhance the reliability and interpretability of generative models. <div>
arXiv:2512.02161v1 Announce Type: new 
Abstract: Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping of Lesion Images to Somatic Mutations</title>
<link>https://arxiv.org/abs/2512.02162</link>
<guid>https://arxiv.org/abs/2512.02162</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, somatic mutations, deep latent variable model, variational autoencoders, cancer diagnosis<br /><br />Summary:  
The article presents a novel deep latent variable model designed to predict patients' somatic mutation profiles based on medical imaging data. Initially, the authors introduce a point cloud representation of lesion images to achieve invariance across different imaging modalities. The core of the approach is LLOST, a model that integrates dual variational autoencoders connected through a shared latent space, which combines features from lesion point clouds and counts of distinct somatic mutations. Three separate latent spaces are employed, each modeled with conditional normalizing flow priors to handle the diverse distribution characteristics of each domain effectively. The study conducts both qualitative and quantitative experiments using de-identified medical images from The Cancer Imaging Archive alongside somatic mutation data from the Pan Cancer dataset of The Cancer Genomic Archive. Results demonstrate that the model can predict specific mutation counts and the occurrence of mutations with notable accuracy. Moreover, it reveals shared patterns between imaging data and somatic mutation profiles that correspond to cancer types. The authors discuss potential improvements for the model and propose future research directions involving the inclusion of additional genetic information domains to enhance predictive capability and clinical relevance. <div>
arXiv:2512.02162v1 Announce Type: new 
Abstract: Medical imaging is a critical initial tool used by clinicians to determine a patient's cancer diagnosis, allowing for faster intervention and more reliable patient prognosis. At subsequent stages of patient diagnosis, genetic information is extracted to help select specific patient treatment options. As the efficacy of cancer treatment often relies on early diagnosis and treatment, we build a deep latent variable model to determine patients' somatic mutation profiles based on their corresponding medical images. We first introduce a point cloud representation of lesions images to allow for invariance to the imaging modality. We then propose, LLOST, a model with dual variational autoencoders coupled together by a separate shared latent space that unifies features from the lesion point clouds and counts of distinct somatic mutations. Therefore our model consists of three latent space, each of which is learned with a conditional normalizing flow prior to account for the diverse distributions of each domain. We conduct qualitative and quantitative experiments on de-identified medical images from The Cancer Imaging Archive and the corresponding somatic mutations from the Pan Cancer dataset of The Cancer Genomic Archive. We show the model's predictive performance on the counts of specific mutations as well as it's ability to accurately predict the occurrence of mutations. In particular, shared patterns between the imaging and somatic mutation domain that reflect cancer type. We conclude with a remark on how to improve the model and possible future avenues of research to include other genetic domains.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.02172</link>
<guid>https://arxiv.org/abs/2512.02172</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, Super-Resolution, Multi-View Consistency, Camera Pose, Novel View Synthesis

<br /><br />Summary:  
1. The paper addresses the challenge of generating higher-resolution renders from 3D Gaussian Splatting (3DGS) for novel view synthesis, which is limited by the resolution of training images.  
2. Applying super-resolution (SR) independently to each low-resolution (LR) input image results in multi-view inconsistencies, causing blurriness in rendered views.  
3. Existing methods mitigate inconsistencies by learned neural components, temporal video priors, or joint optimization, but these methods apply uniform SR across all views.  
4. The authors propose SplatSuRe, a method that selectively applies SR only in undersampled regions identified using camera pose relative to scene geometry, preserving high-frequency details where multiple views overlap.  
5. This selective approach leads to sharper and more consistent renders, outperforming baseline methods on benchmarks such as Tanks & Temples, Deep Blending, and Mip-NeRF 360.  
6. Improvements are especially notable in localized foreground regions where higher detail is most beneficial.  
7. Overall, SplatSuRe enhances fidelity and perceptual quality by intelligently incorporating SR information based on multi-view sampling density and scene geometry. <div>
arXiv:2512.02172v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustSurg: Tackling domain generalisation for out-of-distribution surgical scene segmentation</title>
<link>https://arxiv.org/abs/2512.02188</link>
<guid>https://arxiv.org/abs/2512.02188</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical scene segmentation, domain generalisation, instance normalisation, feature covariance mapping, multicentre dataset<br /><br />Summary:<br /><br />1. This study addresses the challenge of generalising deep learning models for surgical scene segmentation across different centres and imaging modalities, where existing methods often fail due to distribution shifts and modality gaps.<br /><br />2. Inspired by successful approaches in natural scene generalisation, the authors propose exploiting style and content information through instance normalisation and feature covariance mapping to reduce appearance variability caused by factors like blood and imaging artefacts.<br /><br />3. To prevent the loss of important feature information linked to objects of interest, a novel restitution module is integrated within a ResNet backbone to retain task-relevant features during feature learning.<br /><br />4. Due to the scarcity of multiclass, multicentre surgical datasets, the work also introduces a newly curated dataset aimed at improving generalisability research in this domain.<br /><br />5. The proposed method, named RobustSurg, demonstrates significant performance gains, including approximately 23% improvement over the DeepLabv3+ baseline and 10-32% improvement over state-of-the-art (SOTA) on an unseen centre dataset (HeiCholSeg), as well as nearly 22% and 11% improvements on the EndoUDA polyp dataset compared to baseline and recent SOTA methods, respectively. <div>
arXiv:2512.02188v1 Announce Type: new 
Abstract: While recent advances in deep learning for surgical scene segmentation have demonstrated promising results on single-centre and single-imaging modality data, these methods usually do not generalise to unseen distribution (i.e., from other centres) and unseen modalities. Current literature for tackling generalisation on out-of-distribution data and domain gaps due to modality changes has been widely researched but mostly for natural scene data. However, these methods cannot be directly applied to the surgical scenes due to limited visual cues and often extremely diverse scenarios compared to the natural scene data. Inspired by these works in natural scenes to push generalisability on OOD data, we hypothesise that exploiting the style and content information in the surgical scenes could minimise the appearances, making it less variable to sudden changes such as blood or imaging artefacts. This can be achieved by performing instance normalisation and feature covariance mapping techniques for robust and generalisable feature representations. Further, to eliminate the risk of removing salient feature representation associated with the objects of interest, we introduce a restitution module within the feature learning ResNet backbone that can enable the retention of useful task-relevant features. To tackle the lack of multiclass and multicentre data for surgical scene segmentation, we also provide a newly curated dataset that can be vital for addressing generalisability in this domain. Our proposed RobustSurg obtained nearly 23% improvement on the baseline DeepLabv3+ and from 10-32% improvement on the SOTA in terms of mean IoU score on an unseen centre HeiCholSeg dataset when trained on CholecSeg8K. Similarly, RobustSurg also obtained nearly 22% improvement over the baseline and nearly 11% improvement on a recent SOTA method for the target set of the EndoUDA polyp dataset.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multifractal Recalibration of Neural Networks for Medical Imaging Segmentation</title>
<link>https://arxiv.org/abs/2512.02198</link>
<guid>https://arxiv.org/abs/2512.02198</guid>
<content:encoded><![CDATA[
<div> Multifractal analysis, deep learning, channel-attention, semantic segmentation, medical imaging

<br /><br />Summary:  
This paper addresses the limited use of multifractal analysis in deep learning, especially for tasks like semantic segmentation where traditional multifractal methods are hampered by heavy pooling and feature-space decimation. The authors introduce two inductive priors, Monofractal and Multifractal Recalibration, which exploit the relationship between probability mass of exponents and the multifractal spectrum to statistically describe encoder embeddings. These recalibration methods are implemented as channel-attention mechanisms within convolutional neural networks. Using a U-Net-based architecture, they demonstrate that multifractal recalibration outperforms baseline models equipped with other channel-attention strategies that leverage higher-order statistics. The approach is validated on three public medical imaging datasets: ISIC18 for dermoscopy, Kvasir-SEG for endoscopy, and BUSI for ultrasound imaging, highlighting its applicability to pathological image analysis. Their empirical studies reveal that excitation responses in U-Net encoders do not progressively specialize with depth due to skip connections. Additionally, the effectiveness of the attention layers may correlate with the global statistics of instance variability. This work offers new insights into the integration of multifractal methods within modern deep learning frameworks, improving segmentation accuracy in medical imaging tasks. <div>
arXiv:2512.02198v1 Announce Type: new 
Abstract: Multifractal analysis has revealed regularities in many self-seeding phenomena, yet its use in modern deep learning remains limited. Existing end-to-end multifractal methods rely on heavy pooling or strong feature-space decimation, which constrain tasks such as semantic segmentation. Motivated by these limitations, we introduce two inductive priors: Monofractal and Multifractal Recalibration. These methods leverage relationships between the probability mass of the exponents and the multifractal spectrum to form statistical descriptions of encoder embeddings, implemented as channel-attention functions in convolutional networks.
  Using a U-Net-based framework, we show that multifractal recalibration yields substantial gains over a baseline equipped with other channel-attention mechanisms that also use higher-order statistics. Given the proven ability of multifractal analysis to capture pathological regularities, we validate our approach on three public medical-imaging datasets: ISIC18 (dermoscopy), Kvasir-SEG (endoscopy), and BUSI (ultrasound).
  Our empirical analysis also provides insights into the behavior of these attention layers. We find that excitation responses do not become increasingly specialized with encoder depth in U-Net architectures due to skip connections, and that their effectiveness may relate to global statistics of instance variability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Video Quality Assessment</title>
<link>https://arxiv.org/abs/2512.02224</link>
<guid>https://arxiv.org/abs/2512.02224</guid>
<content:encoded><![CDATA[
<div> Keywords: video quality assessment, Diagnostic Mixture-of-Experts, multi-proxy expert training, interpretable feedback, streaming artifacts<br /><br />Summary:<br /><br />1. Traditional video quality assessment (VQA) models often provide only a single quality score per video, lacking diagnostic and interpretable feedback to explain quality degradation.<br />2. Many existing VQA methods are format-specific and fail to generalize across different video formats and distortion types, limiting their practicality.<br />3. This paper introduces Unified-VQA, a novel unified quality assessment framework that treats VQA as a Diagnostic Mixture-of-Experts (MoE) problem, employing multiple perceptual experts specialized for distinct perceptual domains.<br />4. A new multi-proxy expert training strategy is proposed, which optimizes each expert using a ranking-inspired loss guided by the most appropriate proxy metric corresponding to its domain.<br />5. The framework features a diagnostic multi-task head that outputs both a global quality score and a multi-dimensional artifact vector, leveraging weakly-supervised learning based on known training data properties.<br />6. Without retraining or fine-tuning, Unified-VQA consistently outperforms more than 18 benchmark methods on generic VQA and diagnostic artifact detection tasks across 17 diverse databases including HD, UHD, HDR, and HFR streaming artifacts.<br />7. The approach marks a significant advancement towards practical, actionable, and interpretable video quality assessment applicable across multiple video formats and distortion types. <div>
arXiv:2512.02224v1 Announce Type: new 
Abstract: Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.02231</link>
<guid>https://arxiv.org/abs/2512.02231</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, audiovisual reasoning, speaker-centric, AV-SpeakerBench, Gemini models<br /><br />Summary:  
(1) Multimodal large language models (MLLMs) aim to jointly interpret vision, audio, and language, but current video benchmarks lack fine-grained evaluation of human speech understanding.  
(2) Many existing tasks can be solved primarily through visual information or only coarsely assess speech, limiting assessment of models’ ability to align the speaker identity, spoken content, and timing.  
(3) The authors introduce AV-SpeakerBench, a new benchmark consisting of 3,212 multiple-choice questions designed specifically for speaker-centric audiovisual reasoning in real-world videos.  
(4) AV-SpeakerBench emphasizes speakers as the central reasoning unit rather than scenes, embeds audiovisual dependencies directly into question semantics, and uses expert annotations to ensure temporal accuracy and cross-modal validity.  
(5) Evaluation results show that the Gemini model family consistently outperforms open-source alternatives, with Gemini 2.5 Pro achieving the best performance; Qwen3-Omni-30B is competitive with Gemini 2.0 Flash but remains behind Gemini 2.5 Pro mainly due to weaker audiovisual fusion capabilities rather than visual perception.  
(6) This benchmark establishes a rigorous foundation for future development of fine-grained audiovisual reasoning in multimodal AI systems. <div>
arXiv:2512.02231v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Potentials of Spiking Neural Networks for Image Deraining</title>
<link>https://arxiv.org/abs/2512.02258</link>
<guid>https://arxiv.org/abs/2512.02258</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, image deraining, Visual LIF neuron, energy efficiency, multi-scale representation<br /><br />Summary:<br /><br />This paper explores the application of Spiking Neural Networks (SNNs), which are biologically plausible and energy-efficient, in the domain of low-level vision tasks, focusing on image deraining. The study identifies that traditional spiking neurons inherently represent high-pass characteristics but lack adequate spatial contextual understanding needed for effective deraining. To overcome this, the authors propose a novel Visual LIF (VLIF) neuron that enhances spatial information processing. They also address the problem of frequency-domain saturation common in conventional spiking neurons by introducing the Spiking Decomposition and Enhancement Module alongside a lightweight Spiking Multi-scale Unit. These innovations enable hierarchical multi-scale representation learning, improving the network’s ability to restore images degraded by rain. The method was evaluated extensively on five benchmark deraining datasets, showing significant improvements over existing state-of-the-art SNN-based deraining methods. Moreover, the proposed framework achieves this high performance with only 13% of the energy consumption compared to prior works, demonstrating both efficiency and effectiveness. The results support the feasibility of deploying SNNs for high-performance, energy-conscious low-level vision applications. <div>
arXiv:2512.02258v1 Announce Type: new 
Abstract: Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatiotemporal Pyramid Flow Matching for Climate Emulation</title>
<link>https://arxiv.org/abs/2512.02268</link>
<guid>https://arxiv.org/abs/2512.02268</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative models, climate emulation, Spatiotemporal Pyramid Flows, flow matching, ClimateSuite<br /><br />Summary:<br /><br />This paper introduces Spatiotemporal Pyramid Flows (SPF), a novel generative modeling approach for emulating Earth's climate that improves upon previous weather-scale autoregressive methods by enabling faster and more stable long-term simulations. SPF employs a hierarchical spatiotemporal pyramid design that progressively increases spatial resolution while associating each stage with distinct temporal scales, allowing direct and efficient sampling across multiple timescales. Conditioning on prescribed physical forcings such as greenhouse gases and aerosols enhances the model's ability to generate realistic climate scenarios. When evaluated on ClimateBench, SPF outperforms state-of-the-art flow matching baselines and pre-trained models at both yearly and monthly scales, offering especially fast sampling at coarser temporal levels. To support scalability and further research, the authors present ClimateSuite, the largest Earth system simulation dataset to date, containing more than 33,000 simulation-years from ten different climate models, including novel climate intervention scenarios. The scaled SPF model exhibits strong generalization capabilities to unseen scenarios and climate models, demonstrating its robustness and applicability. The combination of SPF and ClimateSuite establishes a comprehensive framework for accurate, efficient, and probabilistic climate emulation that can aid in forecasting and understanding future climate changes. All associated data and code are publicly accessible at the provided GitHub repository. <div>
arXiv:2512.02268v1 Announce Type: new 
Abstract: Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Image Restoration via Text-Conditioned Video Generation</title>
<link>https://arxiv.org/abs/2512.02273</link>
<guid>https://arxiv.org/abs/2512.02273</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, image restoration, CogVideo, super-resolution, zero-shot<br /><br />Summary: Recent advances in text-to-video models have shown strong temporal generation capabilities, but their use for image restoration is less explored. This work repurposes the CogVideo model to handle progressive visual restoration tasks such as super-resolution, deblurring, and low-light enhancement by fine-tuning it to generate restoration trajectories instead of typical video motions. Synthetic datasets are created where each sample illustrates a gradual transition from degraded to clean frames, enabling the model to learn temporal progression linked to restoration quality. Two prompting strategies are evaluated: a uniform text prompt applied across all samples and a scene-specific prompt generated via the LLaVA multi-modal large language model and refined with ChatGPT. The fine-tuned CogVideo produces sequences that progressively improve key perceptual metrics, including PSNR, SSIM, and LPIPS, indicating enhanced spatial detail and illumination consistency. Experimental results confirm the model maintains temporal coherence throughout the restoration process. Notably, the model also demonstrates strong zero-shot robustness by generalizing effectively to real-world datasets like ReLoBlur without further training. This showcases its interpretability through temporal restoration and suggests promising directions for applying video generation models in image restoration tasks. <div>
arXiv:2512.02273v1 Announce Type: new 
Abstract: Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Cross Domain SAR Oil Spill Segmentation via Morphological Region Perturbation and Synthetic Label-to-SAR Generation</title>
<link>https://arxiv.org/abs/2512.02290</link>
<guid>https://arxiv.org/abs/2512.02290</guid>
<content:encoded><![CDATA[
<div> Keywords: SAR oil spill segmentation, domain adaptation, synthetic augmentation, Morphological Region Perturbation, Sentinel-1 dataset<br /><br />Summary:<br /><br />This paper addresses the challenge of poor generalization of deep learning models for oil spill segmentation in Synthetic Aperture Radar (SAR) images across different geographic regions, notably from the Mediterranean to the Peruvian coast where labeled data is limited. The authors introduce MORP--Synth, a two-stage synthetic augmentation framework designed to enhance domain transfer. The first stage applies Morphological Region Perturbation, a curvature-guided method that generates realistic geometric variations of oil spills and look-alike areas within label masks. The second stage uses a conditional generative INADE model to synthesize SAR-like textures from these modified masks, creating realistic augmented images. To validate their approach, the authors compile and harmonize a novel Peruvian dataset of 2112 labeled 512×512 patches from 40 Sentinel-1 scenes with the Mediterranean CleanSeaNet benchmark. They evaluate seven different segmentation architectures and find that models pretrained on Mediterranean data drop significantly in performance (from 67.8% to 51.8% mean Intersection over Union, mIoU) when applied directly to Peruvian data. Incorporating MORP--Synth synthetic augmentation improves the mIoU by up to 6 points overall and leads to substantial gains in minority-class IoUs, especially for oil (+10.8) and look-alike (+14.6) regions, demonstrating the efficacy of the method in enhancing generalization to new SAR domains. <div>
arXiv:2512.02290v1 Announce Type: new 
Abstract: Deep learning models for SAR oil spill segmentation often fail to generalize across regions due to differences in sea-state, backscatter statistics, and slick morphology, a limitation that is particularly severe along the Peruvian coast where labeled Sentinel-1 data remain scarce. To address this problem, we propose \textbf{MORP--Synth}, a two-stage synthetic augmentation framework designed to improve transfer from Mediterranean to Peruvian conditions. Stage~A applies Morphological Region Perturbation, a curvature guided label space method that generates realistic geometric variations of oil and look-alike regions. Stage~B renders SAR-like textures from the edited masks using a conditional generative INADE model. We compile a Peruvian dataset of 2112 labeled 512$\times$512 patches from 40 Sentinel-1 scenes (2014--2024), harmonized with the Mediterranean CleanSeaNet benchmark, and evaluate seven segmentation architectures. Models pretrained on Mediterranean data degrade from 67.8\% to 51.8\% mIoU on the Peruvian domain; MORP--Synth improves performance up to +6 mIoU and boosts minority-class IoU (+10.8 oil, +14.6 look-alike).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision</title>
<link>https://arxiv.org/abs/2512.02339</link>
<guid>https://arxiv.org/abs/2512.02339</guid>
<content:encoded><![CDATA[
<div> motion representations, self-supervised tracking, video diffusion models, visually similar objects, denoising process<br /><br />Summary:<br /><br />This paper addresses the challenge of distinguishing visually similar objects in video tracking, a critical problem in computer vision. It identifies limitations in current self-supervised trackers, which struggle with ambiguous visual cues and thus require extensive labeled data to generalize effectively. The authors discover that pre-trained video diffusion models naturally learn motion representations suitable for tracking tasks without additional task-specific training. This capability emerges from the denoising process within these models, where motion information is isolated during early, high-noise stages, separate from later stages focused on appearance refinement. Building on this insight, the proposed self-supervised tracker leverages diffusion-derived motion features to improve performance, especially in scenarios involving visually similar or even identical objects. Experiments demonstrate up to a 6-point improvement over recent self-supervised methods on standard benchmarks and a newly introduced test suite emphasizing similar object tracking challenges. Visualizations further validate that the learned representations enable robust tracking across difficult viewpoint changes and object deformations, highlighting the method’s robustness and generalization potential without reliance on large labeled datasets. This work opens new directions for exploiting video diffusion models in motion-centric vision tasks. <div>
arXiv:2512.02339v1 Announce Type: new 
Abstract: Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TALO: Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction</title>
<link>https://arxiv.org/abs/2512.02341</link>
<guid>https://arxiv.org/abs/2512.02341</guid>
<content:encoded><![CDATA[
<div> 3D vision, temporal consistency, Thin Plate Spline, submap registration, noisy geometry<br /><br />Summary:<br /><br />This paper addresses the challenge of maintaining temporal consistency in 3D vision foundation models when deployed in online or dynamic environments like driving scenarios, where predictions occur over temporal windows. The authors identify limitations in current alignment methods that rely on global transformations, specifically issues with assumption validity, limited local alignment scope, and vulnerability to noisy geometric inputs. To overcome these, they propose a novel alignment framework that employs Thin Plate Spline (TPS) transformations offering higher degrees of freedom (DOF) for spatially varying corrections. Their approach uses globally propagated control points to ensure long-term and flexible alignment of consecutive predictions. Additionally, the framework introduces a point-agnostic submap registration strategy, enhancing robustness against noisy and imperfect geometric data. Designed as a plug-and-play solution, it is compatible with various 3D foundation models and camera configurations, including monocular and surround-view setups. Extensive experiments across multiple datasets, backbone models, and camera types demonstrate that their method significantly improves the coherence of reconstructed 3D geometry and reduces trajectory errors. The code for the proposed framework is publicly released, promoting reproducibility and further research in temporal alignment for 3D vision applications. <div>
arXiv:2512.02341v1 Announce Type: new 
Abstract: 3D vision foundation models have shown strong generalization in reconstructing key 3D attributes from uncalibrated images through a single feed-forward pass. However, when deployed in online settings such as driving scenarios, predictions are made over temporal windows, making it non-trivial to maintain consistency across time. Recent strategies align consecutive predictions by solving global transformation, yet our analysis reveals their fundamental limitations in assumption validity, local alignment scope, and robustness under noisy geometry. In this work, we propose a higher-DOF and long-term alignment framework based on Thin Plate Spline, leveraging globally propagated control points to correct spatially varying inconsistencies. In addition, we adopt a point-agnostic submap registration design that is inherently robust to noisy geometry predictions. The proposed framework is fully plug-and-play, compatible with diverse 3D foundation models and camera configurations (e.g., monocular or surround-view). Extensive experiments demonstrate that our method consistently yields more coherent geometry and lower trajectory errors across multiple datasets, backbone models, and camera setups, highlighting its robustness and generality. Codes are publicly available at \href{https://github.com/Xian-Bei/TALO}{https://github.com/Xian-Bei/TALO}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multi-weight self-matching visual explanation for cnns on sar images</title>
<link>https://arxiv.org/abs/2512.02344</link>
<guid>https://arxiv.org/abs/2512.02344</guid>
<content:encoded><![CDATA[
<div> Keywords: convolutional neural networks, interpretability, multi-weight self-matching class activation mapping, synthetic aperture radar, weakly-supervised object localization  

<br /><br />Summary:  
This paper addresses the challenge of improving interpretability in convolutional neural networks (CNNs) applied to synthetic aperture radar (SAR) tasks. The authors propose a novel visual explanation method named multi-weight self-matching class activation mapping (MS-CAM), designed to enhance understanding of the decision-making process within CNNs. MS-CAM functions by matching SAR images with the CNN-extracted feature maps and their gradients, integrating both channel-wise and element-wise weights to visualize the model's learned decision basis effectively. The method is validated on a self-constructed SAR target classification dataset where it demonstrates superior ability to highlight important network regions of interest and capture detailed target features, thereby improving the overall interpretability of the network. Additionally, the paper explores the potential of MS-CAM in weakly-supervised object localization within SAR imagery, investigating key factors influencing localization accuracy, such as pixel threshold settings. The thorough analysis and experimental results underscore MS-CAM's value in advancing reliable CNN applications in SAR by making their internal mechanisms more transparent and interpretable, which is crucial for high-reliability requirements in this domain. <div>
arXiv:2512.02344v1 Announce Type: new 
Abstract: In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Harnessing Sparsity in Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2512.02351</link>
<guid>https://arxiv.org/abs/2512.02351</guid>
<content:encoded><![CDATA[
<div> Keywords: unified multimodal models, training-free pruning, Mixture-of-Experts, sparse activation, model compressibility  

<br /><br />Summary:  
This paper addresses the inefficiencies inherent in unified multimodal models, which combine diverse components for both understanding and generation tasks but often use more resources than necessary for specific tasks or samples. 1) The authors perform a systematic analysis leveraging training-free pruning techniques, including both depth pruning and width reduction, to evaluate model component compressibility. 2) Their findings indicate that the understanding components are quite compressible across tasks, especially for generation, whereas generation components are sensitive and degrade rapidly when compressed. 3) To overcome the sensitivity in generation modules, the work introduces a Mixture-of-Experts (MoE) Adaptation method, partitioning the generation module into multiple experts and selectively activating them based on the input. 4) Sparse activation through expert-frozen tuning effectively restores and maintains generation quality despite compression, with further performance improvements found via full training adaptation. 5) The resulting adapted BAGEL model achieves comparable performance to the original, using only approximately half of the parameters during inference. The authors also provide open-source code to facilitate further research and application. <div>
arXiv:2512.02351v1 Announce Type: new 
Abstract: Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting</title>
<link>https://arxiv.org/abs/2512.02359</link>
<guid>https://arxiv.org/abs/2512.02359</guid>
<content:encoded><![CDATA[
<div> Multi-view crowd counting, weakly supervised learning, calibration-free, self-supervised ranking loss, semantic information<br /><br />Summary:<br /><br />This paper addresses the challenges of multi-view crowd counting, particularly the occlusion problems in single-image counting. Existing multi-view methods rely on projecting images onto a common ground-plane density map space, which demands costly crowd annotations and camera calibrations. To overcome these limitations, the authors propose a novel weakly-supervised, calibration-free multi-view crowd counting method (WSCF-MVCC). Unlike previous methods that require expensive image-level density map annotations, WSCF-MVCC uses only crowd count as supervision, eliminating the need for detailed annotation. It incorporates a self-supervised ranking loss leveraging multi-scale priors, enhancing the single-view counting module’s perceptual ability without extra annotation costs. Additionally, the model employs semantic information to improve the accuracy of view matching, leading to more precise scene-level crowd count estimations. Extensive experiments on three widely used multi-view counting datasets show that WSCF-MVCC outperforms existing state-of-the-art approaches under weakly supervised settings. The results indicate that this method is not only effective but also more practical for real-world deployment compared to calibration-dependent methods. The authors release their code publicly, contributing to the community’s development of efficient, annotation-light multi-view crowd counting solutions. <div>
arXiv:2512.02359v1 Announce Type: new 
Abstract: Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VACoT: Rethinking Visual Data Augmentation with VLMs</title>
<link>https://arxiv.org/abs/2512.02361</link>
<guid>https://arxiv.org/abs/2512.02361</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Augmentation Chain-of-Thought, visual language models, image augmentations, OCR adversarial scenarios, reinforcement learning<br /><br />Summary: This paper addresses the challenge of improving robustness in visual language models (VLMs) by introducing a novel framework called Visual Augmentation Chain-of-Thought (VACoT). Unlike traditional data augmentation methods mostly used during training, VACoT dynamically applies image augmentations during model inference to enhance perception capabilities. The framework incorporates a diverse set of post-hoc visual transformations, including denoising, that go beyond simple cropping techniques, thereby broadening the range of input views processed by the model. VACoT leverages efficient agentic reinforcement learning with a conditional reward scheme, which balances the need for necessary augmentations while penalizing overly verbose outputs, promoting concise yet effective reasoning. This approach not only improves model robustness against out-of-distribution inputs but is particularly effective in optical character recognition (OCR) tasks subject to adversarial attacks. Extensive evaluation across 13 perception benchmarks validates VACoT’s superiority over existing methods. Additionally, the authors introduce a new adversarial OCR dataset, AdvOCR, to demonstrate the generalization advantages brought by their post-hoc augmentation approach in adversarial contexts. Overall, VACoT offers a promising direction for enhancing VLM performance with reduced training overhead and improved resilience to challenging real-world visual scenarios. <div>
arXiv:2512.02361v1 Announce Type: new 
Abstract: While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Tuberculosis: A Comparative Dive into Machine Learning for Tuberculosis Detection</title>
<link>https://arxiv.org/abs/2512.02364</link>
<guid>https://arxiv.org/abs/2512.02364</guid>
<content:encoded><![CDATA[
<div> Keywords: tuberculosis, machine learning, ResNet-50, SqueezeNet, chest X-ray

<br /><br />Summary:  
This study investigates the use of machine learning models, specifically a pretrained ResNet-50 and a general SqueezeNet, for diagnosing tuberculosis (TB) using chest X-ray images. TB remains a challenging infectious disease to diagnose, particularly in resource-poor settings where traditional diagnostic methods like sputum smear microscopy and culture are inefficient. The research utilized a Kaggle dataset consisting of 4,200 chest X-rays, applying preprocessing steps such as data splitting, augmentation, and resizing to optimize model training. Performance metrics including accuracy, precision, recall, and confusion matrix were employed to evaluate how well each model detected TB. Results indicated that SqueezeNet outperformed ResNet-50, achieving a loss of 32%, accuracy of 89%, precision of 98%, recall of 80%, and F1 score of 87%, whereas ResNet-50 recorded a loss of 54%, accuracy of 73%, precision of 88%, recall of 52%, and F1 score of 65%. The findings highlight the promise of machine learning methods for early TB detection and timely treatment initiation. Additionally, integrating such lightweight models into mobile devices could expand TB screening capabilities in underserved regions. Despite these encouraging outcomes, the study underscores the ongoing need to develop faster, smaller, and more accurate TB detection models to aid global TB control efforts. <div>
arXiv:2512.02364v1 Announce Type: new 
Abstract: This study explores the application of machine learning models, specifically a pretrained ResNet-50 model and a general SqueezeNet model, in diagnosing tuberculosis (TB) using chest X-ray images. TB, a persistent infectious disease affecting humanity for millennia, poses challenges in diagnosis, especially in resource-limited settings. Traditional methods, such as sputum smear microscopy and culture, are inefficient, prompting the exploration of advanced technologies like deep learning and computer vision. The study utilized a dataset from Kaggle, consisting of 4,200 chest X-rays, to develop and compare the performance of the two machine learning models. Preprocessing involved data splitting, augmentation, and resizing to enhance training efficiency. Evaluation metrics, including accuracy, precision, recall, and confusion matrix, were employed to assess model performance. Results showcase that the SqueezeNet achieved a loss of 32%, accuracy of 89%, precision of 98%, recall of 80%, and an F1 score of 87%. In contrast, the ResNet-50 model exhibited a loss of 54%, accuracy of 73%, precision of 88%, recall of 52%, and an F1 score of 65%. This study emphasizes the potential of machine learning in TB detection and possible implications for early identification and treatment initiation. The possibility of integrating such models into mobile devices expands their utility in areas lacking TB detection resources. However, despite promising results, the need for continued development of faster, smaller, and more accurate TB detection models remains crucial in contributing to the global efforts in combating TB.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention</title>
<link>https://arxiv.org/abs/2512.02368</link>
<guid>https://arxiv.org/abs/2512.02368</guid>
<content:encoded><![CDATA[
<div> Keywords: trajectory prediction, autonomous driving, Mixture of Experts, selective attention, multimodal decoder<br /><br />Summary:<br /><br />Trajectory prediction is essential for the safety and reliability of autonomous driving but remains challenging in complex interactive environments. The paper identifies that current methods often suffer from inefficiencies due to redundant data, which impacts both computational efficiency and prediction accuracy. To overcome these challenges, the authors introduce a novel map-free trajectory prediction algorithm that operates across temporal, spatial, and frequency domains. For temporal processing, a Mixture of Experts (MoE) mechanism is employed to adaptively select important frequency components, which are then extracted and enriched using multi-scale temporal features. To reduce redundant information, a selective attention module filters data in both temporal sequences and spatial interactions. The system incorporates a multimodal decoder trained under patch-level and point-level loss functions to enhance trajectory prediction quality. Experimental results on the Nuscenes dataset demonstrate the method's superiority, proving its effectiveness in handling complex scenarios involving multiple interacting agents without relying on map data. Overall, the approach advances trajectory prediction by blending frequency-domain analysis, adaptive information selection, and multimodal decoding, resulting in improved accuracy and efficiency in challenging autonomous driving conditions. <div>
arXiv:2512.02368v1 Announce Type: new 
Abstract: Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains</title>
<link>https://arxiv.org/abs/2512.02369</link>
<guid>https://arxiv.org/abs/2512.02369</guid>
<content:encoded><![CDATA[
<div> Keywords: domain generalization, semantic segmentation, style adaptive generalization, privacy constraints, visual prompts<br /><br />Summary:  
1. The paper addresses domain generalization for semantic segmentation, focusing on mitigating performance drops caused by domain shifts when model parameters and architectures remain inaccessible due to privacy and security concerns.  
2. Traditional fine-tuning or adaptation techniques cannot be applied under such constraints, creating a need for input-level strategies that improve generalization without modifying model weights.  
3. The authors propose SAGE (Style-Adaptive Generalization), a framework that enhances the generalization ability of frozen models by synthesizing visual prompts that implicitly align feature distributions across various styles.  
4. SAGE leverages style transfer to generate a diverse set of style representations from the source domain, capturing a wide spectrum of visual features to cover potential domain variations.  
5. The model dynamically fuses these style cues depending on the visual context of each input image, creating adaptive prompts that harmonize image appearance and bridge the gap between model invariance and the diversity of unseen domains.  
6. Experimental evaluation on five benchmark datasets shows that SAGE achieves competitive or superior results compared to state-of-the-art methods under privacy constraints and outperforms models with full fine-tuning across all tested scenarios. <div>
arXiv:2512.02369v1 Announce Type: new 
Abstract: Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \textbf{S}tyle-\textbf{A}daptive \textbf{GE}neralization framework (\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning</title>
<link>https://arxiv.org/abs/2512.02375</link>
<guid>https://arxiv.org/abs/2512.02375</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time UAV photogrammetry, Structure-from-Motion (SfM), online mesh quality assessment, predictive path planning, adaptive exploration

<br /><br />Summary:  
This work addresses the limitations of conventional offline UAV photogrammetry by proposing a real-time, adaptive framework called On-the-fly Feedback SfM for time-sensitive geospatial applications like disaster response and digital-twin maintenance. Unlike existing methods that process images without real-time quality evaluation or guidance, this approach integrates three main modules: (1) an online incremental coarse-mesh generator that dynamically expands a sparse 3D point cloud as the UAV explores; (2) an online mesh quality assessment module providing actionable indicators to evaluate reconstruction quality on the fly; and (3) a predictive path planning system that refines UAV trajectories in real time to improve coverage and reduce gaps. Through iterative exploration and exploitation of known and unknown areas, the system enhances 3D reconstruction accuracy and efficiency. Extensive experiments validate that the method supports near real-time in-situ reconstruction and quality evaluation, significantly reducing coverage gaps and the need for re-flight missions. By combining data collection, processing, 3D reconstruction, assessment, and feedback, the framework shifts traditional UAV photogrammetry from passive operation to an intelligent, adaptive exploration workflow. Open-source code for the project is available at the provided GitHub repository. <div>
arXiv:2512.02375v1 Announce Type: new 
Abstract: Compared with conventional offline UAV photogrammetry, real-time UAV photogrammetry is essential for time-critical geospatial applications such as disaster response and active digital-twin maintenance. However, most existing methods focus on processing captured images or sequential frames in real time, without explicitly evaluating the quality of the on-the-go 3D reconstruction or providing guided feedback to enhance image acquisition in the target area. This work presents On-the-fly Feedback SfM, an explore-and-exploit framework for real-time UAV photogrammetry, enabling iterative exploration of unseen regions and exploitation of already observed and reconstructed areas in near real time. Built upon SfM on-the-fly , the proposed method integrates three modules: (1) online incremental coarse-mesh generation for dynamically expanding sparse 3D point cloud; (2) online mesh quality assessment with actionable indicators; and (3) predictive path planning for on-the-fly trajectory refinement. Comprehensive experiments demonstrate that our method achieves in-situ reconstruction and evaluation in near real time while providing actionable feedback that markedly reduces coverage gaps and re-flight costs. Via the integration of data collection, processing, 3D reconstruction and assessment, and online feedback, our on the-fly feedback SfM could be an alternative for the transition from traditional passive working mode to a more intelligent and adaptive exploration workflow. Code is now available at https://github.com/IRIS-LAB-whu/OntheflySfMFeedback.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2512.02392</link>
<guid>https://arxiv.org/abs/2512.02392</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-object tracking, DETR, feature refinement, contrastive learning, temporal continuity  

<br /><br />Summary:  
This paper addresses the challenge of low association accuracy in end-to-end multi-object tracking (MOT) methods that unify detection and association in a single framework.  
The authors find that the shared DETR architecture produces object embeddings with high inter-object similarity because it focuses only on category-level discrimination within single frames, which is insufficient for tracking that requires instance-level distinction across frames.  
To improve embedding discriminativeness, the authors propose FDTA (From Detection to Association), a feature refinement framework enhancing object embeddings from three complementary aspects: spatial, temporal, and identity levels.  
Specifically, FDTA introduces a Spatial Adapter (SA) to incorporate depth-aware information for maintaining spatial continuity, a Temporal Adapter (TA) to aggregate historical features for capturing temporal dependencies, and an Identity Adapter (IA) that applies quality-aware contrastive learning to improve instance-level separability.  
Extensive experiments on challenging MOT benchmarks such as DanceTrack, SportsMOT, and BFT show that FDTA achieves state-of-the-art performance, validating the effectiveness of its discriminative embedding enhancement strategy. The authors also provide their code publicly to facilitate further research. <div>
arXiv:2512.02392v1 Announce Type: new 
Abstract: End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reproducing and Extending RaDelft 4D Radar with Camera-Assisted Labels</title>
<link>https://arxiv.org/abs/2512.02394</link>
<guid>https://arxiv.org/abs/2512.02394</guid>
<content:encoded><![CDATA[
<div> 4D radar, semantic segmentation, RaDelft dataset, camera-guided labeling, fog conditions<br /><br />Summary:<br /><br />This work addresses the challenge of semantic segmentation for 4D radar data, which is hindered by the lack of open-source datasets and annotated labels. The authors reproduce the results from the RaDelft dataset, identifying limitations such as its reliance on LiDAR annotations and the absence of publicly available code for radar label generation. To overcome this, they propose a novel camera-guided radar labeling pipeline that creates accurate labels for radar point clouds without human annotation. Their method involves projecting radar data into camera-based semantic segmentation outputs combined with spatial clustering to produce enhanced radar labels. This pipeline significantly improves the accuracy and reproducibility of radar label generation, making it accessible for broader research use. Furthermore, the study evaluates how varying fog levels impact the radar labeling performance, providing insights into the robustness of radar perception under adverse weather conditions. Overall, this work establishes a reproducible framework that enriches radar data labeling capabilities and facilitates future developments in radar-based environment perception. <div>
arXiv:2512.02394v1 Announce Type: new 
Abstract: Recent advances in 4D radar highlight its potential for robust environment perception under adverse conditions, yet progress in radar semantic segmentation remains constrained by the scarcity of open source datasets and labels. The RaDelft data set, although seminal, provides only LiDAR annotations and no public code to generate radar labels, limiting reproducibility and downstream research. In this work, we reproduce the numerical results of the RaDelft group and demonstrate that a camera-guided radar labeling pipeline can generate accurate labels for radar point clouds without relying on human annotations. By projecting radar point clouds into camera-based semantic segmentation and applying spatial clustering, we create labels that significantly enhance the accuracy of radar labels. These results establish a reproducible framework that allows the research community to train and evaluate the labeled 4D radar data. In addition, we study and quantify how different fog levels affect the radar labeling performance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch</title>
<link>https://arxiv.org/abs/2512.02395</link>
<guid>https://arxiv.org/abs/2512.02395</guid>
<content:encoded><![CDATA[
<div> Keywords: Skywork-R1V4, multimodal agentic model, image manipulation, supervised fine-tuning, multimodal search<br /><br />Summary: Skywork-R1V4 is a 30 billion parameter multimodal agentic model designed to unify image manipulation, web search, and multimodal planning into a single framework. Unlike previous systems that treat these capabilities separately and depend heavily on reinforcement learning, Skywork-R1V4 relies solely on supervised fine-tuning of fewer than 30,000 high-quality trajectories aligned with planning and execution consistency. The model dynamically interleaves reasoning between visual operations and external knowledge retrieval, supporting active image manipulation, deep multimodal search, and complex planning. It demonstrates emergent long-horizon reasoning by successfully orchestrating over 10 tool calls to address intricate multi-step tasks. Skywork-R1V4 achieves state-of-the-art performance, scoring 66.1 on the MMSearch benchmark and 67.2 on FVQA, outperforming prior systems like Gemini 2.5 Flash across 11 metrics. The approach highlights that sophisticated agentic multimodal intelligence can be developed without reinforcement learning, relying instead on carefully curated supervised datasets and stepwise consistency filtering for validation. Overall, the work showcases a novel pathway for building advanced multimodal AI agents capable of integrated reasoning, planning, and tool use within a unified model architecture. <div>
arXiv:2512.02395v1 Announce Type: new 
Abstract: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation ("thinking with images"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation</title>
<link>https://arxiv.org/abs/2512.02400</link>
<guid>https://arxiv.org/abs/2512.02400</guid>
<content:encoded><![CDATA[
<div> Keywords: object-goal navigation, open-vocabulary, Chain-of-Thought reasoning, Similarity-Aware Memory, unseen objects<br /><br />Summary:<br /><br />1. This paper addresses the challenge of object-goal navigation in environments where agents need to locate novel, unseen objects without prior category knowledge, a task complicated by opaque decision-making and low success rates in existing methods.<br />2. The authors introduce Nav-$R^2$, a novel framework that explicitly models two key relationships: target-environment modeling and environment-action planning, by leveraging structured Chain-of-Thought (CoT) reasoning.<br />3. They develop a Nav$R^2$-CoT dataset designed to train the model to perceive the environment effectively, identify target-related objects within its context, and plan future actions accordingly.<br />4. A critical component of Nav-$R^2$ is the Similarity-Aware Memory (SA-Mem), which compresses video frames and fuses historical observations to preserve the most relevant features both temporally and semantically, enhancing memory without adding extra parameters.<br />5. Experimental results demonstrate that Nav-$R^2$ achieves state-of-the-art performance in locating unseen objects, maintains real-time inference at 2Hz, avoids overfitting to known categories, and streamlines the navigation pipeline.<br />6. The resources and code for Nav-$R^2$ will be publicly available on GitHub to facilitate further research and development in this area. <div>
arXiv:2512.02400v1 Announce Type: new 
Abstract: Object-goal navigation in open-vocabulary settings requires agents to locate novel objects in unseen environments, yet existing approaches suffer from opaque decision-making processes and low success rate on locating unseen objects. To address these challenges, we propose Nav-$R^2$, a framework that explicitly models two critical types of relationships, target-environment modeling and environment-action planning, through structured Chain-of-Thought (CoT) reasoning coupled with a Similarity-Aware Memory. We construct a Nav$R^2$-CoT dataset that teaches the model to perceive the environment, focus on target-related objects in the surrounding context and finally make future action plans. Our SA-Mem preserves the most target-relevant and current observation-relevant features from both temporal and semantic perspectives by compressing video frames and fusing historical observations, while introducing no additional parameters. Compared to previous methods, Nav-R^2 achieves state-of-the-art performance in localizing unseen objects through a streamlined and efficient pipeline, avoiding overfitting to seen object categories while maintaining real-time inference at 2Hz. Resources will be made publicly available at \href{https://github.com/AMAP-EAI/Nav-R2}{github link}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2512.02405</link>
<guid>https://arxiv.org/abs/2512.02405</guid>
<content:encoded><![CDATA[
<div> Multi-agent debate, multimodal reasoning, Weighted Iterative Society-of-Experts, Dawid-Skene algorithm, vision-and-language tasks<br /><br />Summary:<br /><br />This paper addresses the challenge of applying multi-agent debate (MAD) frameworks to multimodal vision-and-language reasoning tasks, an area that has been less explored compared to language-only problems. The authors introduce Weighted Iterative Society-of-Experts (WISE), a modular and generalized MAD system that organizes agents into two roles: Solvers, who propose solutions, and Reflectors, who verify these solutions, assign weights, and offer natural language feedback. The framework supports heterogeneous agents with both single- and multi-modal capabilities, allowing a nuanced debate process. To effectively combine multiple agents’ solutions across debate rounds and account for variability in responses and feedback, the authors adapt the Dawid-Skene algorithm in a two-stage post-processing method tailored to their debate model. Experiments are conducted on several vision-and-language datasets, including SMART-840, VisualPuzzles, EvoChart-QA, and a novel SMART-840++ dataset designed with programmatically generated problems of varying difficulty. Results demonstrate that WISE consistently improves accuracy by 2 to 7% compared to existing state-of-the-art MAD approaches and aggregation strategies, highlighting its effectiveness for complex multimodal reasoning tasks and various large language model configurations. <div>
arXiv:2512.02405v1 Announce Type: new 
Abstract: Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MitUNet: Enhancing Floor Plan Recognition using a Hybrid Mix-Transformer and U-Net Architecture</title>
<link>https://arxiv.org/abs/2512.02413</link>
<guid>https://arxiv.org/abs/2512.02413</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, semantic segmentation, Mix-Transformer, U-Net, Tversky loss<br /><br />Summary:<br />1. The paper addresses the challenge of accurately segmenting walls from 2D floor plans to enable precise 3D reconstruction of indoor spaces.<br />2. Existing segmentation techniques often fail to detect thin structural elements and produce irregular mask boundaries, which negatively impacts vectorization.<br />3. The authors propose MitUNet, a hybrid neural network that combines a hierarchical Mix-Transformer encoder for capturing global context with a U-Net decoder enhanced by scSE attention blocks to improve boundary precision.<br />4. They introduce an optimization strategy using the Tversky loss function, which balances precision and recall by prioritizing the suppression of false positives along wall boundaries while still detecting thin structures.<br />5. Experimental results on the CubiCasa5k dataset and a proprietary regional dataset show that MitUNet outperforms standard single-task models, producing structurally correct masks with improved boundary accuracy.<br />6. MitUNet serves as a robust tool for preparing data in automated 3D reconstruction pipelines, enhancing the quality and reliability of 3D indoor models derived from 2D floor plans. <div>
arXiv:2512.02413v1 Announce Type: new 
Abstract: Automatic 3D reconstruction of indoor spaces from 2D floor plans requires high-precision semantic segmentation of structural elements, particularly walls. However, existing methods optimized for standard metrics often struggle to detect thin structural components and yield masks with irregular boundaries, lacking the geometric precision required for subsequent vectorization. To address this issue, we introduce MitUNet, a hybrid neural network architecture specifically designed for wall segmentation tasks in the context of 3D modeling. In MitUNet, we utilize a hierarchical Mix-Transformer encoder to capture global context and a U-Net decoder enhanced with scSE attention blocks for precise boundary recovery. Furthermore, we propose an optimization strategy based on the Tversky loss function to effectively balance precision and recall. By fine-tuning the hyperparameters of the loss function, we prioritize the suppression of false positive noise along wall boundaries while maintaining high sensitivity to thin structures. Our experiments on the public CubiCasa5k dataset and a proprietary regional dataset demonstrate that the proposed approach ensures the generation of structurally correct masks with high boundary accuracy, outperforming standard single-task models. MitUNet provides a robust tool for data preparation in automated 3D reconstruction pipelines.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Vision-Language Models with Dedicated Prompt Guidance</title>
<link>https://arxiv.org/abs/2512.02421</link>
<guid>https://arxiv.org/abs/2512.02421</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, domain generalization, prompt tuning, expert models, cross-modal attention<br /><br />Summary:<br /><br />This paper addresses the challenge of balancing domain specificity and domain generalization (DG) when fine-tuning large pretrained vision-language models (VLMs). The authors reveal through theoretical analysis that training multiple parameter-efficient expert models on partitioned source domains improves DG performance compared to fine-tuning a single universal model. Building on this insight, they propose a novel two-step framework called domain-expert-Guided DG (GuiDG). First, GuiDG uses prompt tuning to create specialized source domain experts. Next, it applies a Cross-Modal Attention module to adaptively integrate these experts during the fine-tuning of the vision encoder, enhancing model adaptation. To better evaluate few-shot domain generalization, the authors introduce the ImageNet-DG dataset, which comprises ImageNet and its variants. Experiments conducted on standard DG benchmarks and the newly constructed ImageNet-DG demonstrate that GuiDG outperforms current state-of-the-art fine-tuning methods. Additionally, it achieves this improvement while maintaining parameter efficiency, illustrating an effective trade-off between model specialization and generalization. Overall, GuiDG offers a theoretically grounded and practically effective solution for improving domain generalization in vision-language model fine-tuning. <div>
arXiv:2512.02421v1 Announce Type: new 
Abstract: Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02423</link>
<guid>https://arxiv.org/abs/2512.02423</guid>
<content:encoded><![CDATA[
<div> GUI, reinforcement learning, navigation, simulation environment, agent training<br /><br />Summary:<br /><br />With the advancement of Large Vision Language Models, GUI agent tasks have evolved from handling single-screen tasks to managing complex screen navigation challenges. Real-world GUI environments like PC software and mobile apps are often complex and proprietary, limiting access to comprehensive environment information necessary for agent training and evaluation. To overcome this, the authors present GUI Exploration Lab, a simulation environment engine that allows flexible creation and composition of screens, icons, and navigation graphs, providing full environment access for thorough training and benchmarking. Experiments show that supervised fine-tuning effectively helps the agent memorize fundamental knowledge, acting as a base for further learning. Building on this foundation, single-turn reinforcement learning improves the agent's ability to generalize to unseen scenarios. Multi-turn reinforcement learning further enhances performance by encouraging agents to develop exploration strategies through interactive trial and error. The proposed methods are validated on both static and interactive benchmarks, demonstrating that the approach generalizes well to real-world scenarios. Overall, the study highlights the benefits of reinforcement learning in GUI navigation and offers guidance for developing more capable and generalizable GUI agents. <div>
arXiv:2512.02423v1 Announce Type: new 
Abstract: With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning</title>
<link>https://arxiv.org/abs/2512.02425</link>
<guid>https://arxiv.org/abs/2512.02425</guid>
<content:encoded><![CDATA[
<div> Keywords: WorldMM, multimodal memory, long video reasoning, adaptive retrieval, video question answering<br /><br />Summary: Recent video large language models excel at understanding short clips but struggle with hours- or days-long videos due to limited context capacity and loss of vital visual details when abstracting information. Traditional memory-augmented approaches depend on textual summaries, which overlook critical visual evidence, especially in complex scenes, and their fixed temporal retrieval scales lack flexibility for events of varying duration. To overcome these issues, WorldMM is introduced as a novel multimodal memory agent that combines textual and visual representations across multiple complementary memory types. Specifically, WorldMM includes episodic memory for indexing factual events at multiple temporal scales, semantic memory for continual conceptual knowledge updates, and visual memory for retaining detailed scene information. The system features an adaptive retrieval agent that iteratively selects the most relevant memory source and temporal granularity according to the query until sufficient information is gathered. Experimental results show that WorldMM substantially surpasses existing methods on five long video question-answering benchmarks, achieving an average 8.4% performance improvement, highlighting its effectiveness in reasoning over long videos. <div>
arXiv:2512.02425v1 Announce Type: new 
Abstract: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightHCG: a Lightweight yet powerful HSIC Disentanglement based Causal Glaucoma Detection Model framework</title>
<link>https://arxiv.org/abs/2512.02437</link>
<guid>https://arxiv.org/abs/2512.02437</guid>
<content:encoded><![CDATA[
<div> Keywords: glaucoma detection, causal representation, LightHCG, convolutional VAE, AI intervention analysis

<br /><br />Summary:  
Glaucoma is a serious optic degenerative disease causing irreversible vision loss primarily due to optic nerve damage from high intraocular pressure or retinal neovascularization. Traditional diagnosis methods rely on perimetry tests, optic papilla inspections, and tonometer-based IOP measurements to identify optic nerve damage. Recent advances in AI utilizing computer vision models like VGG16 and Vision Transformers have enhanced glaucoma detection and optic cup segmentation from retinal fundus and OCT images, aiding diagnosis with improved accuracy. However, current AI approaches face challenges such as excessive parameters, potential spurious correlations, limited reliability, and inadequate applications in clinical intervention analysis. To overcome these, the study introduces LightHCG, a novel and extremely lightweight glaucoma detection model based on a convolutional variational autoencoder (VAE) designed to learn causal representations of glaucoma-related factors in the optic nerve region. LightHCG employs HSIC-based latent space disentanglement and unsupervised causal representation learning through a Graph Autoencoder, effectively capturing true causality. This model achieves glaucoma classification performance comparable to or better than advanced models like InceptionV3, MobileNetV2, and VGG16, while reducing model size by 93–99%. Additionally, LightHCG enhances the potential for AI-driven clinical intervention analysis and simulation, providing a promising direction for more reliable and interpretable glaucoma diagnostics using AI. <div>
arXiv:2512.02437v1 Announce Type: new 
Abstract: As a representative optic degenerative condition, glaucoma has been a threat to millions due to its irreversibility and severe impact on human vision fields. Mainly characterized by dimmed and blurred visions, or peripheral vision loss, glaucoma is well known to occur due to damages in the optic nerve from increased intraocular pressure (IOP) or neovascularization within the retina. Traditionally, most glaucoma related works and clinical diagnosis focused on detecting these damages in the optic nerve by using patient data from perimetry tests, optic papilla inspections and tonometer-based IOP measurements. Recently, with advancements in computer vision AI models, such as VGG16 or Vision Transformers (ViT), AI-automatized glaucoma detection and optic cup segmentation based on retinal fundus images or OCT recently exhibited significant performance in aiding conventional diagnosis with high performance. However, current AI-driven glaucoma detection approaches still have significant room for improvement in terms of reliability, excessive parameter usage, possibility of spurious correlation within detection, and limitations in applications to intervention analysis or clinical simulations. Thus, this research introduced a novel causal representation driven glaucoma detection model: LightHCG, an extremely lightweight Convolutional VAE-based latent glaucoma representation model that can consider the true causality among glaucoma-related physical factors within the optic nerve region. Using HSIC-based latent space disentanglement and Graph Autoencoder based unsupervised causal representation learning, LightHCG not only exhibits higher performance in classifying glaucoma with 93~99% less weights, but also enhances the possibility of AI-driven intervention analysis, compared to existing advanced vision models such as InceptionV3, MobileNetV2 or VGG16.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources</title>
<link>https://arxiv.org/abs/2512.02438</link>
<guid>https://arxiv.org/abs/2512.02438</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, contrastive learning, momentum self-distillation, few-shot learning, computational efficiency

<br /><br />Summary:  
This paper addresses challenges in training Vision-Language Models (VLMs) for medical healthcare, where obtaining detailed annotations is difficult and data is limited. The authors highlight the significance of contrastive learning (CL) for VLM training but note its reliance on large batch sizes, which demands significant computational resources often unavailable to many institutions. To overcome these constraints, the study proposes leveraging momentum-based self-distillation combined with gradient accumulation. First, momentum self-distillation is employed to enhance multimodal learning by extracting more knowledge during training. Second, the integration of momentum mechanisms with gradient accumulation effectively enlarges the batch size without increasing hardware or memory consumption. Empirical results demonstrate that the method achieves competitive zero-shot classification performance comparable to state-of-the-art approaches while substantially improving few-shot adaptation, reaching over 90% AUC-ROC and enhancing retrieval tasks by 2-3%. Importantly, the training process is efficient enough to run on a single GPU with reasonable training times, making it accessible for institutions with limited resources. Overall, the proposed approach advances efficient multimodal learning by balancing improved performance and reduced resource requirements. The implementation is publicly available at their GitHub repository. <div>
arXiv:2512.02438v1 Announce Type: new 
Abstract: In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2512.02441</link>
<guid>https://arxiv.org/abs/2512.02441</guid>
<content:encoded><![CDATA[
<div> Keywords: meta-learning, transfer learning, parameter-efficient fine-tuning, orthogonal basis, low-rank adaptation  

<br /><br />Summary:  
This paper addresses the challenge of adapting large pre-trained models to new, unseen tasks under constraints of limited data and computational resources. Existing meta-learning methods are effective but costly and unstable due to the need for extensive meta-training over multiple tasks. In contrast, the authors propose BOLT (Basis-Oriented Low-rank Transfer), which leverages a collection of already fine-tuned models by extracting an orthogonal spectral basis informed by tasks rather than merging weights. In an offline phase, dominant singular vectors from different task-specific models are collected and orthogonalized per layer to form reusable bases. During the online phase, these bases are frozen and only a small number of diagonal coefficients per layer are trained for the new task, yielding a rank-controlled and parameter-efficient adaptation. BOLT provides a strong, training-free initialization for unseen tasks by pooling source-task coefficients with a lightweight rescaling step, which leverages the shared orthogonal bases to facilitate transfer. Experimentally, BOLT achieves competitive performance compared to common parameter-efficient fine-tuning baselines and a meta-learned initialization, demonstrating that constraining adaptation within a task-informed orthogonal subspace offers an effective alternative for unseen-task transfer. <div>
arXiv:2512.02441v1 Announce Type: new 
Abstract: Adapting large pre-trained models to unseen tasks under tight data and compute budgets remains challenging. Meta-learning approaches explicitly learn good initializations, but they require an additional meta-training phase over many tasks, incur high training cost, and can be unstable. At the same time, the number of task-specific pre-trained models continues to grow, yet the question of how to transfer them to new tasks with minimal additional training remains relatively underexplored. We propose BOLT (Basis-Oriented Low-rank Transfer), a framework that reuses existing fine-tuned models not by merging weights, but instead by extracting an orthogonal, task-informed spectral basis and adapting within that subspace. In the offline phase, BOLT collects dominant singular directions from multiple task vectors and orthogonalizes them per layer to form reusable bases. In the online phase, we freeze these bases and train only a small set of diagonal coefficients per layer for the new task, yielding a rank-controlled update with very few trainable parameters. This design provides (i) a strong, training-free initialization for unseen tasks, obtained by pooling source-task coefficients, along with a lightweight rescaling step while leveraging the shared orthogonal bases, and (ii) a parameter-efficient fine-tuning (PEFT) path that, in our experiments, achieves robust performance compared to common PEFT baselines as well as a representative meta-learned initialization. Our results show that constraining adaptation to a task-informed orthogonal subspace provides an effective alternative for unseen-task transfer.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors</title>
<link>https://arxiv.org/abs/2512.02447</link>
<guid>https://arxiv.org/abs/2512.02447</guid>
<content:encoded><![CDATA[
<div> Spiking Neural Networks, Temporal Dynamics Enhancer, Spike-Driven Attention, Object Detection, Energy Efficiency  

<br /><br />Summary:  
Spiking Neural Networks (SNNs) are recognized for their brain-inspired temporal dynamics and energy-efficient spike-based computations, making them promising alternatives to traditional Artificial Neural Networks (ANNs). However, conventional SNNs typically process inputs either by direct replication or fixed-interval aggregation, resulting in neurons receiving repetitive stimuli over time and thereby limiting the network's ability to capture complex temporal features needed for tasks like object detection. To address this, the authors introduce the Temporal Dynamics Enhancer (TDE), which enhances temporal information modeling in SNNs. TDE incorporates two key components: a Spiking Encoder (SE) that produces varied input stimuli across time steps, and an Attention Gating Module (AGM) that leverages inter-temporal dependencies to regulate SE output. To combat the increased energy demands caused by AGM’s multiplication operations, a Spike-Driven Attention (SDA) mechanism is proposed, significantly lowering attention-related energy consumption. Experimental evaluations demonstrate that integrating TDE into existing SNN-based detectors consistently boosts performance, achieving mean Average Precision (mAP50-95) scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. Additionally, the SDA module operates with only 24% of the energy consumption typical of conventional attention modules, highlighting its efficiency. <div>
arXiv:2512.02447v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nuScenes Revisited: Progress and Challenges in Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.02448</link>
<guid>https://arxiv.org/abs/2512.02448</guid>
<content:encoded><![CDATA[
<div> Keywords: nuScenes, autonomous vehicles, dataset, multi-modal sensor fusion, deep learning<br /><br />Summary:<br /><br />1. The paper revisits the nuScenes dataset, a pioneering resource in autonomous vehicle (AV) research that includes radar data, diverse urban scenes from multiple continents, and data collected from fully autonomous vehicles on public roads.<br /><br />2. nuScenes supports multi-modal sensor fusion by integrating data from cameras, LiDAR, radar, and other sensors, enabling comprehensive AV perception and decision-making tasks.<br /><br />3. The authors reveal extensive technical details on the creation of nuScenes and its extensions—nuImages and Panoptic nuScenes—that had not been published before, providing valuable insights into dataset construction and annotation.<br /><br />4. The influence of nuScenes on the autonomous driving community is traced, showing how it set new standards and inspired many subsequent datasets and benchmarks used for tasks like perception, localization, mapping, prediction, and planning.<br /><br />5. The paper also surveys a wide array of research tasks, both official and unofficial, that leverage nuScenes, summarizing major methodological advances and offering a comprehensive overview of the literature centered around this influential dataset. <div>
arXiv:2512.02448v1 Announce Type: new 
Abstract: Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) have been revolutionized by Deep Learning. As a data-driven approach, Deep Learning relies on vast amounts of driving data, typically labeled in great detail. As a result, datasets, alongside hardware and algorithms, are foundational building blocks for the development of AVs. In this work we revisit one of the most widely used autonomous driving datasets: the nuScenes dataset. nuScenes exemplifies key trends in AV development, being the first dataset to include radar data, to feature diverse urban driving scenes from two continents, and to be collected using a fully autonomous vehicle operating on public roads, while also promoting multi-modal sensor fusion, standardized benchmarks, and a broad range of tasks including perception, localization \& mapping, prediction and planning. We provide an unprecedented look into the creation of nuScenes, as well as its extensions nuImages and Panoptic nuScenes, summarizing many technical details that have hitherto not been revealed in academic publications. Furthermore, we trace how the influence of nuScenes impacted a large number of other datasets that were released later and how it defined numerous standards that are used by the community to this day. Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild</title>
<link>https://arxiv.org/abs/2512.02450</link>
<guid>https://arxiv.org/abs/2512.02450</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D layout estimation, multi-floor buildings, HouseLayout3D, MultiFloor3D, benchmark dataset<br /><br />Summary:<br />1. Existing 3D layout estimation models typically train on synthetic datasets that feature simple environments such as single rooms or single floors, limiting their applicability to larger, more complex structures.<br />2. These models require scenes to be divided into individual floors prior to processing, which causes a loss of global spatial context necessary for understanding multi-floor connections like staircases.<br />3. To address these limitations, the authors introduce HouseLayout3D, a real-world benchmark dataset designed specifically for full building scale layout estimation including multiple floors and architecturally complex spaces.<br />4. The paper also presents MultiFloor3D, a training-free baseline method that builds upon recent advances in scene understanding and effectively outperforms current 3D layout estimation models on both the new HouseLayout3D benchmark and existing datasets.<br />5. The availability of data and code at https://houselayout3d.github.io aims to facilitate further research and development toward comprehensive multi-floor 3D layout estimation models that retain global spatial context. <div>
arXiv:2512.02450v1 Announce Type: new 
Abstract: Current 3D layout estimation models are primarily trained on synthetic datasets containing simple single room or single floor environments. As a consequence, they cannot natively handle large multi floor buildings and require scenes to be split into individual floors before processing, which removes global spatial context that is essential for reasoning about structures such as staircases that connect multiple levels. In this work, we introduce HouseLayout3D, a real world benchmark designed to support progress toward full building scale layout estimation, including multiple floors and architecturally intricate spaces. We also present MultiFloor3D, a simple training free baseline that leverages recent scene understanding methods and already outperforms existing 3D layout estimation models on both our benchmark and prior datasets, highlighting the need for further research in this direction. Data and code are available at: https://houselayout3d.github.io.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation</title>
<link>https://arxiv.org/abs/2512.02453</link>
<guid>https://arxiv.org/abs/2512.02453</guid>
<content:encoded><![CDATA[
<div> Stylized motion generation, intra-style diversity, clustering framework, style embedding, motion style transfer<br /><br />Summary:<br /><br />This paper addresses the challenge of capturing intra-style diversity in stylized motion generation, where motions of a single style exhibit diverse variations. Existing models typically extract an unstructured embedding for each style motion, but this approach limits diversity representation. To overcome this, the authors propose ClusterStyle, a clustering-based framework that uses multiple prototypes to represent diverse style patterns within the same style category. They identify two types of style diversity: global-level diversity, which exists among different style motions of the same category, and local-level diversity, which manifests within the temporal dynamics of motion sequences. ClusterStyle jointly structures two embedding spaces—global and local—through alignment with fixed prototype anchors, which are non-learnable. Additionally, the paper introduces the Stylistic Modulation Adapter (SMA), which integrates these enriched style features into a pretrained text-to-motion generation model, enhancing its style control capability. Extensive experiments demonstrate that the proposed method surpasses current state-of-the-art approaches in both stylized motion generation and motion style transfer, indicating its effectiveness in producing diverse and high-quality stylistic motions consistent with specified styles. <div>
arXiv:2512.02453v1 Announce Type: new 
Abstract: Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion. However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge. In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation. Instead of learning an unstructured embedding from each style motion, we leverage a set of prototypes to effectively model diverse style patterns across motions belonging to the same style category. We consider two types of style diversity: global-level diversity among style motions of the same category, and local-level diversity within the temporal dynamics of motion sequences. These components jointly shape two structured style embedding spaces, i.e., global and local, optimized via alignment with non-learnable prototype anchors. Furthermore, we augment the pretrained text-to-motion generation model with the Stylistic Modulation Adapter (SMA) to integrate the style features. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art models in stylized motion generation and motion style transfer.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See, Think, Learn: A Self-Taught Multimodal Reasoner</title>
<link>https://arxiv.org/abs/2512.02456</link>
<guid>https://arxiv.org/abs/2512.02456</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, multimodal reasoning, self-training, structured rationales, negative rationales

<br /><br />Summary: The paper presents See-Think-Learn (STL), a self-training framework designed to improve multimodal reasoning in Vision-Language Models (VLMs) by jointly enhancing their perception and reasoning capabilities. STL introduces a structured reasoning template that encourages the model to first "see" by extracting visual attributes into textual form before "thinking" or reasoning over these attributes. Unlike previous methods reliant on expensive human annotations or proprietary models for Chain-of-Thought reasoning data, STL enables the model to generate and learn from its own structured rationales in a self-training loop. Additionally, the framework incorporates negative rationales, explanations for why certain answer choices are incorrect, which help the model better distinguish between correct and misleading options, fostering more robust learning. Experimental results across various domains indicate that STL consistently outperforms baseline models trained solely on answers or self-generated reasoning. Qualitative analysis further confirms the high quality and effectiveness of the generated rationales. Ultimately, STL offers a cost-effective and scalable solution to enhance the multimodal reasoning ability of VLMs by combining improved visual perception with structured reasoning and discriminative learning through positive and negative explanations. <div>
arXiv:2512.02456v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation</title>
<link>https://arxiv.org/abs/2512.02457</link>
<guid>https://arxiv.org/abs/2512.02457</guid>
<content:encoded><![CDATA[
<div> audio-video generation, joint denoising, video quality, cross-modal co-training, AVFullDiT<br /><br />Summary:<br /><br />1. This paper investigates whether training audio and video modalities together through joint denoising improves video generation quality, even when only video output is of interest. 2. The authors introduce AVFullDiT, a parameter-efficient architecture that combines pre-trained text-to-video (T2V) and text-to-audio (T2A) models to perform audio-video joint denoising. 3. They conduct experiments comparing a T2AV model using AVFullDiT to a T2V-only baseline under identical conditions. 4. Results show that joint audio-video denoising enhances video generation beyond improving mere audio-video synchrony, particularly on challenging video subsets with large motions and object contact interactions. 5. The study hypothesizes that predicting audio serves as a privileged signal, helping the model learn causal links between visual events and their sounds (e.g., collisions producing impact noises), which improves temporal coherence and physical realism in videos. 6. The findings support cross-modal co-training as a promising direction for building stronger, more physically grounded generative world models. 7. The authors plan to publicly release their code and dataset to facilitate further research in this area. <div>
arXiv:2512.02457v1 Announce Type: new 
Abstract: Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration</title>
<link>https://arxiv.org/abs/2512.02458</link>
<guid>https://arxiv.org/abs/2512.02458</guid>
<content:encoded><![CDATA[
<div> Keywords: embodied AI, sequential tasks, spatial memory, multi-modal large language models, embodied exploration<br /><br />Summary:<br /><br />1. This work addresses the challenge of performing sequential indoor embodied tasks, where an agent must complete a sequence of sub-tasks, some of which may be infeasible, such as searching for objects that do not exist. 2. The key difficulty compared to single-task settings is how to effectively reuse spatial knowledge accumulated from previous explorations to support ongoing reasoning and exploration. 3. To study this practically important issue, the authors introduce SEER-Bench, a new benchmark that integrates two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN), designed for sequential task evaluation. 4. Building on SEER-Bench, they propose 3DSPMR, a 3D Spatial Memory Reasoning method that leverages relational, visual, and geometric information from explored environments to augment Multi-Modal Large Language Models (MLLMs) for improved reasoning and exploration in sequential embodied tasks. 5. This approach is pioneering in explicitly incorporating geometric information into MLLM-based spatial understanding, and experimental results demonstrate that 3DSPMR significantly improves performance on sequential EQA and EMN tasks. <div>
arXiv:2512.02458v1 Announce Type: new 
Abstract: Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution</title>
<link>https://arxiv.org/abs/2512.02469</link>
<guid>https://arxiv.org/abs/2512.02469</guid>
<content:encoded><![CDATA[
<div> dataset distillation, distribution matching, feature evolution, trajectory alignment, semantic diversity<br /><br />Summary:<br /><br />1. Dataset distillation aims to compress large datasets into smaller, synthetic ones to save storage and computational costs, with distribution matching (DM)-based methods being prominent for their efficiency.<br /><br />2. Existing DM methods often neglect the dynamic evolution of feature representations during model training, which reduces the quality and expressiveness of the distilled synthetic data and weakens performance on downstream tasks.<br /><br />3. The proposed Trajectory Guided Dataset Distillation (TGDD) addresses this limitation by reformulating distribution matching as a dynamic alignment process along the model’s training trajectory, capturing evolving semantic information at each training stage.<br /><br />4. TGDD incorporates a distribution constraint regularization to reduce class overlap, ensuring that synthetic data maintains both semantic diversity and representativeness.<br /><br />5. This approach achieves an improved trade-off between performance and efficiency without adding optimization overhead, demonstrating state-of-the-art results on ten datasets, including a notable 5.0% accuracy improvement on high-resolution benchmarks. <div>
arXiv:2512.02469v1 Announce Type: new 
Abstract: Dataset distillation compresses large datasets into compact synthetic ones to reduce storage and computational costs. Among various approaches, distribution matching (DM)-based methods have attracted attention for their high efficiency. However, they often overlook the evolution of feature representations during training, which limits the expressiveness of synthetic data and weakens downstream performance. To address this issue, we propose Trajectory Guided Dataset Distillation (TGDD), which reformulates distribution matching as a dynamic alignment process along the model's training trajectory. At each training stage, TGDD captures evolving semantics by aligning the feature distribution between the synthetic and original dataset. Meanwhile, it introduces a distribution constraint regularization to reduce class overlap. This design helps synthetic data preserve both semantic diversity and representativeness, improving performance in downstream tasks. Without additional optimization overhead, TGDD achieves a favorable balance between performance and efficiency. Experiments on ten datasets demonstrate that TGDD achieves state-of-the-art performance, notably a 5.0% accuracy gain on high-resolution benchmarks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling</title>
<link>https://arxiv.org/abs/2512.02473</link>
<guid>https://arxiv.org/abs/2512.02473</guid>
<content:encoded><![CDATA[
<div> Keywords: video world models, compressed memory, long-term generation, spatial consistency, LoopNav<br /><br />Summary:<br /><br />1. This paper addresses the challenge of achieving temporally and spatially consistent long-term video world modeling, a problem made difficult by the high computational costs associated with long-context inputs in current models. <br />2. The authors propose WorldPack, a novel video world model that employs an efficient compressed memory system to overcome these limitations by using significantly shorter context lengths without sacrificing generation quality. <br />3. WorldPack's compressed memory consists of two main components: trajectory packing, which enhances context efficiency by compressing past trajectories, and memory retrieval, which maintains spatial consistency and supports long-term generation involving spatial reasoning. <br />4. The model was evaluated on LoopNav, a specialized benchmark based on the Minecraft environment designed to test long-term consistency in video generation. <br />5. Experimental results demonstrate that WorldPack substantially outperforms existing state-of-the-art methods in terms of spatial consistency, visual fidelity, and overall quality for long-term video generation tasks. <div>
arXiv:2512.02473v1 Announce Type: new 
Abstract: Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline</title>
<link>https://arxiv.org/abs/2512.02482</link>
<guid>https://arxiv.org/abs/2512.02482</guid>
<content:encoded><![CDATA[
<div> Keywords: G-SHARP, real-time surgical reconstruction, Gaussian splatting, deformable tissue modeling, EndoNeRF benchmark<br /><br />Summary:<br /><br />1. The article introduces G-SHARP, a surgical scene reconstruction framework designed to provide fast and accurate 3D modeling of deformable tissue during minimally invasive procedures.<br />2. G-SHARP addresses limitations of prior Gaussian splatting methods by offering a commercially compatible, real-time pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer.<br />3. The framework supports principled deformation modeling and robust occlusion handling, which enhances the quality and reliability of reconstructions.<br />4. The authors demonstrate that G-SHARP achieves state-of-the-art reconstruction performance on the EndoNeRF pulling benchmark, balancing speed and accuracy for intra-operative suitability.<br />5. To facilitate practical deployment, the article presents a Holoscan SDK application that runs G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in clinical operating-room environments. <div>
arXiv:2512.02482v1 Announce Type: new 
Abstract: We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making</title>
<link>https://arxiv.org/abs/2512.02485</link>
<guid>https://arxiv.org/abs/2512.02485</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, medical diagnosis, reasoning detachment, multi-agent framework, visual evidence<br /><br />Summary:<br /><br />This paper addresses the challenge of reasoning detachment in Vision-Language Models (VLMs) used for medical diagnosis, where explanations may deviate from actual image evidence, reducing clinical trust. Existing multi-agent frameworks simulate team discussions to reduce bias but often increase textual noise and computational costs without sufficiently anchoring reasoning to visual data. The authors propose UCAgents, a hierarchical multi-agent framework inspired by clinical workflows that enforces unidirectional convergence and structured evidence auditing. UCAgents restricts agent interactions to focused evidence verification and prevents position changes, thereby suppressing rhetorical drift while enhancing the extraction of visual signals. A one-round inquiry discussion is introduced to identify potential misalignments between the visual content and textual explanations. This design jointly addresses the dual-noise bottleneck—visual ambiguity and textual noise—formalized through information theory. Experiments on four medical Visual Question Answering benchmarks demonstrate that UCAgents achieves superior accuracy (71.3% on PathVQA, a 6.0% improvement over state-of-the-art) with an 87.7% reduction in token cost. The results confirm that UCAgents balances thorough evidence discovery and minimizes confusing textual interference, exhibiting both diagnostic reliability and computational efficiency important for real-world clinical applications. The code is publicly available on GitHub. <div>
arXiv:2512.02485v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding</title>
<link>https://arxiv.org/abs/2512.02487</link>
<guid>https://arxiv.org/abs/2512.02487</guid>
<content:encoded><![CDATA[
<div> 3D scene-language understanding, Large Language Models, attention mask, spatial structure, multi-modal reasoning<br /><br />Summary:<br /><br />1. The paper addresses challenges in 3D scene-language understanding where Large Language Models (LLMs) are used for reasoning in 3D multi-modal contexts. 2. Existing approaches utilize standard language modeling decoders with causal attention masks, which introduce conflicts such as sequential bias among order-agnostic 3D objects and limited attention between object tokens and instruction context. 3. To resolve these issues, the authors propose 3D Spatial Language Instruction Mask (3D-SLIM), a novel masking strategy designed specifically for 3D scene spatial structures. 4. 3D-SLIM consists of two main components: a Geometry-adaptive Mask that defines attention based on spatial density instead of token order, and an Instruction-aware Mask that allows direct attention between object tokens and instruction context. 5. This approach requires no architectural changes or additional parameters but significantly improves performance on various 3D scene-language tasks, demonstrating the important role of decoder design in enhancing 3D multi-modal reasoning capabilities. <div>
arXiv:2512.02487v1 Announce Type: new 
Abstract: Recent advances in 3D scene-language understanding have leveraged Large Language Models (LLMs) for 3D reasoning by transferring their general reasoning ability to 3D multi-modal contexts. However, existing methods typically adopt standard decoders from language modeling, which rely on a causal attention mask. This design introduces two fundamental conflicts in 3D scene understanding: sequential bias among order-agnostic 3D objects and restricted object-instruction attention, hindering task-specific reasoning. To overcome these limitations, we propose 3D Spatial Language Instruction Mask (3D-SLIM), an effective masking strategy that replaces the causal mask with an adaptive attention mask tailored to the spatial structure of 3D scenes. Our 3D-SLIM introduces two key components: a Geometry-adaptive Mask that constrains attention based on spatial density rather than token order, and an Instruction-aware Mask that enables object tokens to directly access instruction context. This design allows the model to process objects based on their spatial relationships while being guided by the user's task. 3D-SLIM is simple, requires no architectural modifications, and adds no extra parameters, yet it yields substantial performance improvements across diverse 3D scene-language tasks. Extensive experiments across multiple benchmarks and LLM baselines validate its effectiveness and underscore the critical role of decoder design in 3D multi-modal reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YingVideo-MV: Music-Driven Multi-Stage Video Generation</title>
<link>https://arxiv.org/abs/2512.02492</link>
<guid>https://arxiv.org/abs/2512.02492</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion model, music-performance video, camera motion, temporal-aware diffusion Transformer, long-sequence consistency<br /><br />Summary: The paper presents YingVideo-MV, a novel cascaded framework designed for music-driven long-video generation, specifically targeting music-performance videos with dynamic camera motions. It addresses the challenges of producing long sequences that maintain natural audio-visual synchronization and preserve performer identity. The framework integrates multiple components: audio semantic analysis for extracting meaningful music features, an interpretable shot planning module called MV-Director to control video shots, temporal-aware diffusion Transformer architectures for effective video generation, and long-sequence consistency modeling to ensure coherent output over extended durations. To support the development and evaluation of their approach, the authors create a large-scale Music-in-the-Wild Dataset sourced from real web data, enabling diverse and high-quality video generation. Recognizing that existing long-video generation methods lack explicit control over camera movements, YingVideo-MV introduces a camera adapter module that embeds camera pose information into latent noise, allowing precise manipulation of camera motion. Additionally, a time-aware dynamic window range strategy is proposed to adaptively adjust denoising ranges during long-sequence inference, improving continuity between video clips based on audio embeddings. Benchmark results demonstrate that YingVideo-MV excels in generating coherent, expressive music videos with accurate synchronization of music, performer motion, and camera movement, advancing the state-of-the-art in music-driven video synthesis. <div>
arXiv:2512.02492v1 Announce Type: new 
Abstract: While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention-guided reference point shifting for Gaussian-mixture-based partial point set registration</title>
<link>https://arxiv.org/abs/2512.02496</link>
<guid>https://arxiv.org/abs/2512.02496</guid>
<content:encoded><![CDATA[
<div> Keywords: partial-to-partial point set registration, deep learning, Gaussian mixture models, attention-based reference point shifting, transformation-invariant features<br /><br />Summary:  
This study focuses on the challenges in partial-to-partial point set registration when input point sets undergo translation and rotation, especially in deep learning methods using Gaussian mixture models (GMMs). It highlights theoretical and practical limitations found in DeepGMR, a foundational approach in this domain, specifically its shortcomings for partial-to-partial registration scenarios. The authors aim to identify root causes for these limitations and propose an effective solution. They introduce an attention-based reference point shifting (ARPS) layer designed to robustly detect a common reference point between two partial point sets. This approach helps in obtaining features that are invariant to transformations like translation and rotation. Unlike previous methods that focus on finding overlapping regions, the ARPS layer employs an attention mechanism to identify a common reference point directly. Integrating the ARPS layer significantly improves the registration performance of DeepGMR and its variant UGMMReg. Furthermore, these enhanced models outperform other recent deep learning techniques that utilize attention blocks or Transformers aimed at overlap or reference point detection. The findings offer new insights and practical improvements for registration methods involving deep learning and GMMs. <div>
arXiv:2512.02496v1 Announce Type: new 
Abstract: This study investigates the impact of the invariance of feature vectors for partial-to-partial point set registration under translation and rotation of input point sets, particularly in the realm of techniques based on deep learning and Gaussian mixture models (GMMs). We reveal both theoretical and practical problems associated with such deep-learning-based registration methods using GMMs, with a particular focus on the limitations of DeepGMR, a pioneering study in this line, to the partial-to-partial point set registration. Our primary goal is to uncover the causes behind such methods and propose a comprehensible solution for that. To address this, we introduce an attention-based reference point shifting (ARPS) layer, which robustly identifies a common reference point of two partial point sets, thereby acquiring transformation-invariant features. The ARPS layer employs a well-studied attention module to find a common reference point rather than the overlap region. Owing to this, it significantly enhances the performance of DeepGMR and its recent variant, UGMMReg. Furthermore, these extension models outperform even prior deep learning methods using attention blocks and Transformer to extract the overlap region or common reference points. We believe these findings provide deeper insights into registration methods using deep learning and GMMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Large Scale Benchmark for Test Time Adaptation Methods in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.02497</link>
<guid>https://arxiv.org/abs/2512.02497</guid>
<content:encoded><![CDATA[
<div> Test Time Adaptation, Medical Image Segmentation, Domain Shift, Benchmark, Multimodality<br /><br />Summary:  
This paper introduces MedSeg-TTA, a comprehensive benchmark designed to evaluate test time adaptation (TTA) methods for medical image segmentation across seven imaging modalities: MRI, CT, ultrasound, pathology, dermoscopy, OCT, and chest X-ray. The benchmark standardizes data preprocessing, backbone configurations, and test time protocols to ensure methodological consistency. MedSeg-TTA covers twenty representative adaptation methods categorized into four paradigms: Input-level Transformation, Feature-level Alignment, Output-level Regularization, and Prior Estimation, facilitating the first systematic cross-modality comparison of their reliability and applicability. Key findings reveal that no single paradigm excels universally; input-level methods perform more stably under mild appearance shifts, feature-level and output-level methods improve boundary-related metrics, while prior-based methods are highly modality-dependent. Some methods notably degrade under large inter-center and inter-device variations, highlighting the critical need for careful method selection in clinical settings. The benchmark offers standardized datasets, validated implementations, and a public leaderboard, creating a rigorous platform for advancing robust, clinically reliable test time adaptation techniques in medical imaging. All source codes and datasets are openly available at the provided GitHub repository. <div>
arXiv:2512.02497v1 Announce Type: new 
Abstract: Test time Adaptation is a promising approach for mitigating domain shift in medical image segmentation; however, current evaluations remain limited in terms of modality coverage, task diversity, and methodological consistency. We present MedSeg-TTA, a comprehensive benchmark that examines twenty representative adaptation methods across seven imaging modalities, including MRI, CT, ultrasound, pathology, dermoscopy, OCT, and chest X-ray, under fully unified data preprocessing, backbone configuration, and test time protocols. The benchmark encompasses four significant adaptation paradigms: Input-level Transformation, Feature-level Alignment, Output-level Regularization, and Prior Estimation, enabling the first systematic cross-modality comparison of their reliability and applicability. The results show that no single paradigm performs best in all conditions. Input-level methods are more stable under mild appearance shifts. Feature-level and Output-level methods offer greater advantages in boundary-related metrics, whereas prior-based methods exhibit strong modality dependence. Several methods degrade significantly under large inter-center and inter-device shifts, which highlights the importance of principled method selection for clinical deployment. MedSeg-TTA provides standardized datasets, validated implementations, and a public leaderboard, establishing a rigorous foundation for future research on robust, clinically reliable test-time adaptation. All source codes and open-source datasets are available at https://github.com/wenjing-gg/MedSeg-TTA.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.02498</link>
<guid>https://arxiv.org/abs/2512.02498</guid>
<content:encoded><![CDATA[
arXiv:2512.02498v1 Announce Type: new 
Abstract: Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots.ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, dots.ocr establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding</title>
<link>https://arxiv.org/abs/2512.02505</link>
<guid>https://arxiv.org/abs/2512.02505</guid>
<content:encoded><![CDATA[
arXiv:2512.02505v1 Announce Type: new 
Abstract: Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-Stage Vision Transformer for Image Restoration: Colorization Pretraining + Residual Upsampling</title>
<link>https://arxiv.org/abs/2512.02512</link>
<guid>https://arxiv.org/abs/2512.02512</guid>
<content:encoded><![CDATA[
arXiv:2512.02512v1 Announce Type: new 
Abstract: In computer vision, Single Image Super-Resolution (SISR) is still a difficult problem. We present ViT-SR, a new technique to improve the performance of a Vision Transformer (ViT) employing a two-stage training strategy. In our method, the model learns rich, generalizable visual representations from the data itself through a self-supervised pretraining phase on a colourization task. The pre-trained model is then adjusted for 4x super-resolution. By predicting the addition of a high-frequency residual image to an initial bicubic interpolation, this design simplifies residual learning. ViT-SR, trained and evaluated on the DIV2K benchmark dataset, achieves an impressive SSIM of 0.712 and PSNR of 22.90 dB. These results demonstrate the efficacy of our two-stage approach and highlight the potential of self-supervised pre-training for complex image restoration tasks. Further improvements may be possible with larger ViT architectures or alternative pretext tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts</title>
<link>https://arxiv.org/abs/2512.02517</link>
<guid>https://arxiv.org/abs/2512.02517</guid>
<content:encoded><![CDATA[
arXiv:2512.02517v1 Announce Type: new 
Abstract: The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.02520</link>
<guid>https://arxiv.org/abs/2512.02520</guid>
<content:encoded><![CDATA[
arXiv:2512.02520v1 Announce Type: new 
Abstract: Zero-shot anomaly classification and segmentation (AC/AS) aim to detect anomalous samples and regions without any training data, a capability increasingly crucial in industrial inspection and medical imaging. This dissertation aims to investigate the core challenges of zero-shot AC/AS and presents principled solutions rooted in theory and algorithmic design.
  We first formalize the problem of consistent anomalies, a failure mode in which recurring similar anomalies systematically bias distance-based methods. By analyzing the statistical and geometric behavior of patch representations from pre-trained Vision Transformers, we identify two key phenomena - similarity scaling and neighbor-burnout - that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects.
  We then introduce CoDeGraph, a graph-based framework for filtering consistent anomalies built on the similarity scaling and neighbor-burnout phenomena. Through multi-stage graph construction, community detection, and structured refinement, CoDeGraph effectively suppresses the influence of consistent anomalies.
  Next, we extend this framework to 3D medical imaging by proposing a training-free, computationally efficient volumetric tokenization strategy for MRI data. This enables a genuinely zero-shot 3D anomaly detection pipeline and shows that volumetric anomaly segmentation is achievable without any 3D training samples.
  Finally, we bridge batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models. Together, this dissertation provides theoretical understanding and practical solutions for the zero-shot AC/AS problem.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens</title>
<link>https://arxiv.org/abs/2512.02536</link>
<guid>https://arxiv.org/abs/2512.02536</guid>
<content:encoded><![CDATA[
arXiv:2512.02536v1 Announce Type: new 
Abstract: Recent progress in multimodal large language models (MLLMs) has highlighted the challenge of efficiently bridging pre-trained Vision-Language Models (VLMs) with Diffusion Models. While methods using a fixed number of learnable query tokens offer computational efficiency, they suffer from task generalization collapse, failing to adapt to new tasks that are distant from their pre-training tasks. To overcome this, we propose Noisy Query Tokens, which learn a distributed representation space between the VLM and Diffusion Model via end-to-end optimization, enhancing continual learning. Additionally, we introduce a VAE branch with linear projection to recover fine-grained image details. Experimental results confirm our approach mitigates generalization collapse and enables stable continual learning across diverse tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVGGT: Rethinking Global Attention for Accelerating VGGT</title>
<link>https://arxiv.org/abs/2512.02541</link>
<guid>https://arxiv.org/abs/2512.02541</guid>
<content:encoded><![CDATA[
arXiv:2512.02541v1 Announce Type: new 
Abstract: Since DUSt3R, models such as VGGT and $\pi^3$ have shown strong multi-view 3D performance, but their heavy reliance on global self-attention results in high computational cost. Existing sparse-attention variants offer partial speedups, yet lack a systematic analysis of how global attention contributes to multi-view reasoning. In this paper, we first conduct an in-depth investigation of the global attention modules in VGGT and $\pi^3$ to better understand their roles. Our analysis reveals a clear division of roles in the alternating global-frame architecture: early global layers do not form meaningful correspondences, middle layers perform cross-view alignment, and last layers provide only minor refinements. Guided by these findings, we propose a training-free two-step acceleration scheme: (1) converting early global layers into frame attention, and (2) subsampling global attention by subsampling K/V over patch tokens with diagonal preservation and a mean-fill component. We instantiate this strategy on VGGT and $\pi^3$ and evaluate across standard pose and point-map benchmarks. Our method achieves up to $8$-$10\times$ speedup in inference time while matching or slightly improving the accuracy of the original models, and remains robust even in extremely dense multi-view settings where prior sparse-attention baselines fail.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniPerson: Unified Identity-Preserving Pedestrian Generation</title>
<link>https://arxiv.org/abs/2512.02554</link>
<guid>https://arxiv.org/abs/2512.02554</guid>
<content:encoded><![CDATA[
arXiv:2512.02554v1 Announce Type: new 
Abstract: Person re-identification (ReID) suffers from a lack of large-scale high-quality training data due to challenges in data privacy and annotation costs. While previous approaches have explored pedestrian generation for data augmentation, they often fail to ensure identity consistency and suffer from insufficient controllability, thereby limiting their effectiveness in dataset augmentation. To address this, We introduce OmniPerson, the first unified identity-preserving pedestrian generation pipeline for visible/infrared image/video ReID tasks. Our contributions are threefold: 1) We proposed OmniPerson, a unified generation model, offering holistic and fine-grained control over all key pedestrian attributes. Supporting RGB/IR modality image/video generation with any number of reference images, two kinds of person poses, and text. Also including RGB-to-IR transfer and image super-resolution abilities.2) We designed Multi-Refer Fuser for robust identity preservation with any number of reference images as input, making OmniPerson could distill a unified identity from a set of multi-view reference images, ensuring our generated pedestrians achieve high-fidelity pedestrian generation.3) We introduce PersonSyn, the first large-scale dataset for multi-reference, controllable pedestrian generation, and present its automated curation pipeline which transforms public, ID-only ReID benchmarks into a richly annotated resource with the dense, multi-modal supervision required for this task. Experimental results demonstrate that OmniPerson achieves SoTA in pedestrian generation, excelling in both visual fidelity and identity consistency. Furthermore, augmenting existing datasets with our generated data consistently improves the performance of ReID models. We will open-source the full codebase, pretrained model, and the PersonSyn dataset.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature</title>
<link>https://arxiv.org/abs/2512.02566</link>
<guid>https://arxiv.org/abs/2512.02566</guid>
<content:encoded><![CDATA[
arXiv:2512.02566v1 Announce Type: new 
Abstract: There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-speech Gesture Video Generation via Motion-Based Graph Retrieval</title>
<link>https://arxiv.org/abs/2512.02576</link>
<guid>https://arxiv.org/abs/2512.02576</guid>
<content:encoded><![CDATA[
arXiv:2512.02576v1 Announce Type: new 
Abstract: Synthesizing synchronized and natural co-speech gesture videos remains a formidable challenge. Recent approaches have leveraged motion graphs to harness the potential of existing video data. To retrieve an appropriate trajectory from the graph, previous methods either utilize the distance between features extracted from the input audio and those associated with the motions in the graph or embed both the input audio and motion into a shared feature space. However, these techniques may not be optimal due to the many-to-many mapping nature between audio and gestures, which cannot be adequately addressed by one-to-one mapping. To alleviate this limitation, we propose a novel framework that initially employs a diffusion model to generate gesture motions. The diffusion model implicitly learns the joint distribution of audio and motion, enabling the generation of contextually appropriate gestures from input audio sequences. Furthermore, our method extracts both low-level and high-level features from the input audio to enrich the training process of the diffusion model. Subsequently, a meticulously designed motion-based retrieval algorithm is applied to identify the most suitable path within the graph by assessing both global and local similarities in motion. Given that not all nodes in the retrieved path are sequentially continuous, the final step involves seamlessly stitching together these segments to produce a coherent video output. Experimental results substantiate the efficacy of our proposed method, demonstrating a significant improvement over prior approaches in terms of synchronization accuracy and naturalness of generated gestures.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content-Aware Texturing for Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.02621</link>
<guid>https://arxiv.org/abs/2512.02621</guid>
<content:encoded><![CDATA[
arXiv:2512.02621v1 Announce Type: new 
Abstract: Gaussian Splatting has become the method of choice for 3D reconstruction and real-time rendering of captured real scenes. However, fine appearance details need to be represented as a large number of small Gaussian primitives, which can be wasteful when geometry and appearance exhibit different frequency characteristics.
  Inspired by the long tradition of texture mapping, we propose to use texture to represent detailed appearance where possible. Our main focus is to incorporate per-primitive texture maps that adapt to the scene in a principled manner during Gaussian Splatting optimization. We do this by proposing a new appearance representation for 2D Gaussian primitives with textures where the size of a texel is bounded by the image sampling frequency and adapted to the content of the input images. We achieve this by adaptively upscaling or downscaling the texture resolution during optimization. In addition, our approach enables control of the number of primitives during optimization based on texture resolution. We show that our approach performs favorably in image quality and total number of parameters used compared to alternative solutions for textured Gaussian primitives. Project page: https://repo-sam.inria.fr/nerphys/gs-texturing/
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence</title>
<link>https://arxiv.org/abs/2512.02622</link>
<guid>https://arxiv.org/abs/2512.02622</guid>
<content:encoded><![CDATA[
arXiv:2512.02622v1 Announce Type: new 
Abstract: Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding</title>
<link>https://arxiv.org/abs/2512.02624</link>
<guid>https://arxiv.org/abs/2512.02624</guid>
<content:encoded><![CDATA[
arXiv:2512.02624v1 Announce Type: new 
Abstract: PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening</title>
<link>https://arxiv.org/abs/2512.02643</link>
<guid>https://arxiv.org/abs/2512.02643</guid>
<content:encoded><![CDATA[
arXiv:2512.02643v1 Announce Type: new 
Abstract: Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking</title>
<link>https://arxiv.org/abs/2512.02648</link>
<guid>https://arxiv.org/abs/2512.02648</guid>
<content:encoded><![CDATA[
arXiv:2512.02648v1 Announce Type: new 
Abstract: We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hear What Matters! Text-conditioned Selective Video-to-Audio Generation</title>
<link>https://arxiv.org/abs/2512.02650</link>
<guid>https://arxiv.org/abs/2512.02650</guid>
<content:encoded><![CDATA[
arXiv:2512.02650v1 Announce Type: new 
Abstract: This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation</title>
<link>https://arxiv.org/abs/2512.02660</link>
<guid>https://arxiv.org/abs/2512.02660</guid>
<content:encoded><![CDATA[
arXiv:2512.02660v1 Announce Type: new 
Abstract: Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes</title>
<link>https://arxiv.org/abs/2512.02664</link>
<guid>https://arxiv.org/abs/2512.02664</guid>
<content:encoded><![CDATA[
arXiv:2512.02664v1 Announce Type: new 
Abstract: Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions. However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence. We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation. This process achieves high-fidelity reflection separation and full-scene reconstruction without requiring environment maps or restrictive material assumptions. We demonstrate on public and self-collected datasets that PolarGuide-GSDR achieves state-of-the-art performance in specular reconstruction, normal estimation, and novel view synthesis, all while maintaining real-time rendering capabilities. To our knowledge, this is the first framework embedding polarization priors directly into 3DGS optimization, yielding superior interpretability and real-time performance for modeling complex reflective scenes.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking</title>
<link>https://arxiv.org/abs/2512.02668</link>
<guid>https://arxiv.org/abs/2512.02668</guid>
<content:encoded><![CDATA[
arXiv:2512.02668v1 Announce Type: new 
Abstract: Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.02681</link>
<guid>https://arxiv.org/abs/2512.02681</guid>
<content:encoded><![CDATA[
arXiv:2512.02681v1 Announce Type: new 
Abstract: Although diffusion-based models have achieved impressive results in image super-resolution, they often rely on large-scale backbones such as Stable Diffusion XL (SDXL) and Diffusion Transformers (DiT), which lead to excessive computational and memory costs during training and inference. To address this issue, we develop a lightweight diffusion method, PGP-DiffSR, by removing redundant information from diffusion models under the guidance of the phase information of inputs for efficient image super-resolution. We first identify the intra-block redundancy within the diffusion backbone and propose a progressive pruning approach that removes redundant blocks while reserving restoration capability. We note that the phase information of the restored images produced by the pruned diffusion model is not well estimated. To solve this problem, we propose a phase-exchange adapter module that explores the phase information of the inputs to guide the pruned diffusion model for better restoration performance. We formulate the progressive pruning approach and the phase-exchange adapter module into a unified model. Extensive experiments demonstrate that our method achieves competitive restoration quality while significantly reducing computational load and memory consumption. The code is available at https://github.com/yzb1997/PGP-DiffSR.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance</title>
<link>https://arxiv.org/abs/2512.02685</link>
<guid>https://arxiv.org/abs/2512.02685</guid>
<content:encoded><![CDATA[
arXiv:2512.02685v1 Announce Type: new 
Abstract: Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data</title>
<link>https://arxiv.org/abs/2512.02686</link>
<guid>https://arxiv.org/abs/2512.02686</guid>
<content:encoded><![CDATA[
arXiv:2512.02686v1 Announce Type: new 
Abstract: Anomaly segmentation seeks to detect and localize unknown or out-of-distribution (OoD) objects that fall outside predefined semantic classes a capability essential for safe autonomous driving. However, the scarcity and limited diversity of anomaly data severely constrain model generalization in open-world environments. Existing approaches mitigate this issue through synthetic data generation, either by copy-pasting external objects into driving scenes or by leveraging text-to-image diffusion models to inpaint anomalous regions. While these methods improve anomaly diversity, they often lack contextual coherence and physical realism, resulting in domain gaps between synthetic and real data. In this paper, we present ClimaDrive, a semantics-guided image-to-image framework for synthesizing semantically coherent, weather-diverse, and physically plausible OoD driving data. ClimaDrive unifies structure-guided multi-weather generation with prompt-driven anomaly inpainting, enabling the creation of visually realistic training data. Based on this framework, we construct ClimaOoD, a large-scale benchmark spanning six representative driving scenarios under both clear and adverse weather conditions. Extensive experiments on four state-of-the-art methods show that training with ClimaOoD leads to robust improvements in anomaly segmentation. Across all methods, AUROC, AP, and FPR95 show notable gains, with FPR95 dropping from 3.97 to 3.52 for RbA on Fishyscapes LAF. These results demonstrate that ClimaOoD enhances model robustness, offering valuable training data for better generalization in open-world anomaly detection.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection</title>
<link>https://arxiv.org/abs/2512.02696</link>
<guid>https://arxiv.org/abs/2512.02696</guid>
<content:encoded><![CDATA[
arXiv:2512.02696v1 Announce Type: new 
Abstract: Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</title>
<link>https://arxiv.org/abs/2512.02697</link>
<guid>https://arxiv.org/abs/2512.02697</guid>
<content:encoded><![CDATA[
arXiv:2512.02697v1 Announce Type: new 
Abstract: Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm</title>
<link>https://arxiv.org/abs/2512.02700</link>
<guid>https://arxiv.org/abs/2512.02700</guid>
<content:encoded><![CDATA[
arXiv:2512.02700v1 Announce Type: new 
Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tissue-mask supported inter-subject whole-body image registration in the UK Biobank - A method benchmarking study</title>
<link>https://arxiv.org/abs/2512.02702</link>
<guid>https://arxiv.org/abs/2512.02702</guid>
<content:encoded><![CDATA[
arXiv:2512.02702v1 Announce Type: new 
Abstract: The UK Biobank is a large-scale study collecting whole-body MR imaging and non-imaging health data. Robust and accurate inter-subject image registration of these whole-body MR images would enable their body-wide spatial standardization, and region-/voxel-wise correlation analysis of non-imaging data with image-derived parameters (e.g., tissue volume or fat content).
  We propose a sex-stratified inter-subject whole-body MR image registration approach that uses subcutaneous adipose tissue- and muscle-masks from the state-of-the-art VIBESegmentator method to augment intensity-based graph-cut registration. The proposed method was evaluated on a subset of 4000 subjects by comparing it to an intensity-only method as well as two previously published registration methods, uniGradICON and MIRTK. The evaluation comprised overlap measures applied to the 71 VIBESegmentator masks: 1) Dice scores, and 2) voxel-wise label error frequency. Additionally, voxel-wise correlation between age and each of fat content and tissue volume was studied to exemplify the usefulness for medical research.
  The proposed method exhibited a mean dice score of 0.77 / 0.75 across the cohort and the 71 masks for males/females, respectively. When compared to the intensity-only registration, the mean values were 6 percentage points (pp) higher for both sexes, and the label error frequency was decreased in most tissue regions. These differences were 9pp / 8pp against uniGradICON and 12pp / 13pp against MIRTK. Using the proposed method, the age-correlation maps were less noisy and showed higher anatomical alignment.
  In conclusion, the image registration method using two tissue masks improves whole-body registration of UK Biobank images.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding</title>
<link>https://arxiv.org/abs/2512.02715</link>
<guid>https://arxiv.org/abs/2512.02715</guid>
<content:encoded><![CDATA[
arXiv:2512.02715v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>