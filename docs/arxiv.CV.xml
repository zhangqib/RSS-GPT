<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>

<item>
<title>Beyond Flicker: Detecting Kinematic Inconsistencies for Generalizable Deepfake Video Detection</title>
<link>https://arxiv.org/abs/2512.04175</link>
<guid>https://arxiv.org/abs/2512.04175</guid>
<content:encoded><![CDATA[
<div> Deepfake detection, video manipulation, facial landmarks, motion inconsistencies, autoencoder<br><br>Summary:<br><br>This paper addresses the challenge of generalizing deepfake detection to unseen manipulations, specifically focusing on videos rather than static images. Existing methods primarily detect temporal artifacts as frame-to-frame instabilities but neglect the natural motion dependencies that exist between different facial regions. To overcome this limitation, the authors propose a novel synthetic video generation method that introduces subtle kinematic inconsistencies by disrupting natural correlations in facial movements. They achieve this by training an autoencoder to decompose facial landmark configurations into motion bases, which they then manipulate selectively. These manipulated motion bases enable the creation of training videos with biomechanical flaws via face morphing techniques. Networks trained on this synthetic data gain the ability to detect sophisticated biomechanical inconsistencies that are hard to spot with conventional methods. Experimental results demonstrate that the proposed approach achieves state-of-the-art generalization performance on multiple popular deepfake detection benchmarks. This work advances the robustness of video deepfake detectors by exploiting nuanced biomechanical motion cues, offering a promising direction for more reliable detection of unseen facial video manipulations. <div>
arXiv:2512.04175v1 Announce Type: new 
Abstract: Generalizing deepfake detection to unseen manipulations remains a key challenge. A recent approach to tackle this issue is to train a network with pristine face images that have been manipulated with hand-crafted artifacts to extract more generalizable clues. While effective for static images, extending this to the video domain is an open issue. Existing methods model temporal artifacts as frame-to-frame instabilities, overlooking a key vulnerability: the violation of natural motion dependencies between different facial regions. In this paper, we propose a synthetic video generation method that creates training data with subtle kinematic inconsistencies. We train an autoencoder to decompose facial landmark configurations into motion bases. By manipulating these bases, we selectively break the natural correlations in facial movements and introduce these artifacts into pristine videos via face morphing. A network trained on our data learns to spot these sophisticated biomechanical flaws, achieving state-of-the-art generalization results on several popular benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology</title>
<link>https://arxiv.org/abs/2512.04187</link>
<guid>https://arxiv.org/abs/2512.04187</guid>
<content:encoded><![CDATA[
<div> Keywords: OnSight Pathology, AI histological analysis, digital pathology, real-time inference, multi-modal chat assistant<br><br>Summary:  
The article introduces OnSight Pathology, a platform-agnostic computer vision software designed to provide real-time artificial intelligence (AI) inferences during digital slide image review, addressing challenges in the deployment of proprietary digital pathology solutions. The software operates locally on consumer-grade personal computers via a single executable file, avoiding complex software integrations and enabling cost-effective, secure use in research and clinical workflows. The utility of OnSight Pathology is demonstrated using over 2,500 publicly available whole slide images across various viewers and clinical cases, showcasing its robustness in key histopathological tasks such as brain tumor classification, mitosis detection, and immunohistochemical stain quantification. A novel built-in multi-modal chat assistant enhances quality control by delivering verifiable, flexible image descriptions without rigid class labels. Additionally, the software supports live microscope camera feeds, including those from smartphones, highlighting its potential application in analog, inter-operative, and telepathology settings. Overall, OnSight Pathology facilitates broad adoption of AI tools in histopathology by overcoming barriers to real-world deployment and streamlining AI integration into diverse pathology workflows. <div>
arXiv:2512.04187v1 Announce Type: new 
Abstract: The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers</title>
<link>https://arxiv.org/abs/2512.04213</link>
<guid>https://arxiv.org/abs/2512.04213</guid>
<content:encoded><![CDATA[
<div> multi-camera tracking, transformer, attention mechanism, 3D point representation, temporal consistency<br><br>Summary:<br><br>1. The paper introduces LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture designed for multi-camera point tracking that integrates appearance-based matching with geometric constraints. <br>2. Unlike traditional pipelines that separate detection, association, and tracking—leading to error propagation and temporal inconsistency—LAPA jointly reasons across views and time using attention mechanisms to improve robustness in challenging scenarios. <br>3. A key innovation is the cross-view attention mechanism enhanced by geometric priors, which establishes soft correspondences without relying on classical triangulation, instead constructing 3D point representations through attention-weighted aggregation. This approach inherently handles uncertainty and partial observations. <br>4. Temporal consistency is ensured by a transformer decoder that models long-range dependencies, enabling identity preservation through extended occlusions. <br>5. LAPA’s performance is validated on challenging datasets, including newly created multi-camera versions of TAPVid-3D panoptic and PointOdyssey, where it achieves substantial improvements—37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC—especially excelling in complex motion and occlusion scenarios. The codebase is publicly available for further research and application. <div>
arXiv:2512.04213v1 Announce Type: new 
Abstract: This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning</title>
<link>https://arxiv.org/abs/2512.04219</link>
<guid>https://arxiv.org/abs/2512.04219</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical event segmentation, predictive learning, streaming video, temporal abstraction, unsupervised learning  

<br><br>Summary:  
1. The paper introduces PARSE, a novel unsupervised framework designed to segment video into a hierarchy of events at multiple temporal scales, reflecting the natural human perception of nested actions within coarser routines.  
2. PARSE employs a hierarchy of recurrent predictors where lower layers focus on short-term dynamics and higher layers capture longer-term context through attention-based feedback, enabling temporal granularity in processing.  
3. Event boundaries are identified as transient peaks in the prediction error of the model, which naturally lead to the emergence of nested event structures that respect containment relations similar to human event perception.  
4. The framework is evaluated on three established benchmarks—Breakfast Actions, 50 Salads, and Assembly 101—where it achieves state-of-the-art performance among streaming methods and competes closely with offline methods in temporal alignment and structural consistency metrics (H-GEBD, TED, hF1).  
5. These results underline the effectiveness of predictive learning under uncertainty for scalable, hierarchical temporal abstraction and compositional event understanding, pushing forward the field of computer vision toward human-like event segmentation and anticipation. <div>
arXiv:2512.04219v1 Announce Type: new 
Abstract: Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis</title>
<link>https://arxiv.org/abs/2512.04221</link>
<guid>https://arxiv.org/abs/2512.04221</guid>
<content:encoded><![CDATA[
<div> Newtonian motion, text-to-video generation, physics simulation, motion coherence, evaluation benchmark<br><br>Summary:<br><br>This paper addresses the challenge of generating text-to-video (T2V) content that is not only photorealistic but also physically accurate and intent-aligned with Newtonian motion principles. The authors propose MoReGen, a novel framework that combines large language models (LLMs), physics simulators, and rendering engines to produce reproducible videos grounded in physical laws from code-based text prompts. To evaluate physical validity, they introduce a new metric called object-trajectory correspondence, which directly measures how well generated videos adhere to expected object motions. Furthermore, they present MoReSet, a comprehensive benchmark dataset containing 1,275 human-annotated videos across nine categories of Newtonian phenomena, including detailed scene descriptions, spatiotemporal relations, and ground-truth trajectories. Through extensive experiments using MoReSet, the study evaluates existing T2V models, revealing that state-of-the-art approaches often fail to maintain physical correctness. In contrast, MoReGen demonstrates superior performance in generating physically coherent videos. This work highlights the importance of integrating physics knowledge into T2V systems and provides valuable tools and data to foster progress toward physically faithful video synthesis. <div>
arXiv:2512.04221v1 Announce Type: new 
Abstract: While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasonX: MLLM-Guided Intrinsic Image Decomposition</title>
<link>https://arxiv.org/abs/2512.04222</link>
<guid>https://arxiv.org/abs/2512.04222</guid>
<content:encoded><![CDATA[
<div> Intrinsic image decomposition, multimodal large language model, comparative supervision, GRPO rewards, intrinsic predictors<br><br>Summary:  
Intrinsic image decomposition aims to separate an image into its fundamental physical components such as albedo, depth, surface normals, and illumination. Existing diffusion- and transformer-based models trained on synthetic paired data struggle to generalize well to diverse, real-world images. The paper introduces ReasonX, a novel framework that leverages a multimodal large language model (MLLM) to act as a perceptual judge by providing relative intrinsic comparisons between image components. These comparisons are then used as GRPO (Gradient-based Reinforcement Policy Optimization) rewards to fine-tune intrinsic decomposition models using unlabeled, in-the-wild images. Unlike traditional reinforcement learning approaches for generative models, ReasonX aligns the conditional intrinsic predictors by encouraging agreement between the MLLM’s relational assessments and analytically derived relationships from the model’s outputs. This approach is model-agnostic and can be applied across different intrinsic decomposition architectures and modalities. Experiments demonstrate that ReasonX achieves substantial improvements, including a 9-25% reduction in Weighted Human Disagreement Rate (WHDR) on IIW albedo prediction and up to 46% gains in depth accuracy on the ETH3D dataset. These results highlight the effectiveness of MLLM-guided comparative supervision in bridging the gap between low-level image decomposition and high-level vision reasoning tasks. <div>
arXiv:2512.04222v1 Announce Type: new 
Abstract: Intrinsic image decomposition aims to separate images into physical components such as albedo, depth, normals, and illumination. While recent diffusion- and transformer-based models benefit from paired supervision from synthetic datasets, their generalization to diverse, real-world scenarios remains challenging. We propose ReasonX, a novel framework that leverages a multimodal large language model (MLLM) as a perceptual judge providing relative intrinsic comparisons, and uses these comparisons as GRPO rewards for fine-tuning intrinsic decomposition models on unlabeled, in-the-wild images. Unlike RL methods for generative models, our framework aligns conditional intrinsic predictors by rewarding agreement between the judge's relational assessments and analytically derived relations from the model's outputs. ReasonX is model-agnostic and can be applied to different intrinsic predictors. Across multiple base architectures and modalities, ReasonX yields significant improvements, including 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D, highlighting the promise of MLLM-guided comparative supervision to bridge low- and high-level vision reasoning.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.04238</link>
<guid>https://arxiv.org/abs/2512.04238</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, rare anatomical variants, AdversarialAnatomyBench, anatomical bias, medical AI

<br><br>Summary:  
This paper introduces AdversarialAnatomyBench, the first benchmark designed to evaluate vision-language models (VLMs) on naturally occurring rare anatomical variants across various imaging modalities and body regions, addressing an important gap in current model evaluation. The study defines these rare variants as "natural adversarial anatomy," highlighting how they deviate from common anatomical patterns learned by models. When testing 22 state-of-the-art VLMs on basic medical perception tasks, average accuracy significantly dropped from 74% on typical anatomy to 29% on atypical presentations, with top models like GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick experiencing performance declines between 41% and 51%. Analysis revealed that model errors strongly aligned with inherent anatomical biases. Attempts to mitigate these shortcomings through model scaling, bias-aware prompting, and test-time reasoning interventions failed to substantially improve generalization to rare anatomical variants. These results unearth a critical and previously unquantified limitation in current vision-language medical AI systems: poor adaptability to rare anatomical presentations. By providing AdversarialAnatomyBench, the authors establish a systematic platform for measuring and ultimately mitigating anatomical bias in multimodal medical AI, fostering the development of more robust clinical diagnostic tools. <div>
arXiv:2512.04238v1 Announce Type: new 
Abstract: Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about "typical" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models</title>
<link>https://arxiv.org/abs/2512.04248</link>
<guid>https://arxiv.org/abs/2512.04248</guid>
<content:encoded><![CDATA[
<div> MVRoom, novel view synthesis, multi-view diffusion, 3D indoor scenes, epipolar attention<br><br>Summary:<br><br>This article introduces MVRoom, a novel pipeline designed for controllable novel view synthesis (NVS) of 3D indoor scenes by leveraging multi-view diffusion conditioned on a coarse 3D layout. The approach utilizes a two-stage design: the first stage creates novel representations that effectively connect the 3D layout with consistent image-based conditioning signals to enable reliable multi-view generation. The second stage applies an image-conditioned multi-view generation process, which integrates a layout-aware epipolar attention mechanism to boost multi-view consistency throughout the diffusion process. Furthermore, the authors propose an iterative framework that generates 3D scenes with varying complexity and object counts by recursively applying multi-view generation, thereby enabling text-to-scene generation. Experimental evaluations demonstrate that MVRoom achieves high-fidelity and controllable 3D scene synthesis, outperforming state-of-the-art baseline methods in both quantitative metrics and qualitative results. Ablation studies confirm the critical roles and effectiveness of the key components in the generation pipeline, validating the design choices and mechanisms introduced for improved multi-view consistency and scene controllability. This work advances the state-of-the-art in controllable 3D indoor scene synthesis using diffusion-based multi-view methods. <div>
arXiv:2512.04248v1 Announce Type: new 
Abstract: We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniLight: A Unified Representation for Lighting</title>
<link>https://arxiv.org/abs/2512.04267</link>
<guid>https://arxiv.org/abs/2512.04267</guid>
<content:encoded><![CDATA[
<div> lighting, multimodal representation, environment maps, spherical harmonics, image synthesis<br><br>Summary:<br><br>1. The article addresses the challenge of representing and understanding lighting in images, which is known to significantly affect visual appearance but remains difficult to model effectively.<br>2. Existing lighting representations like environment maps, irradiance, spherical harmonics, and textual descriptions are largely incompatible with each other, limiting their combined utility and cross-modal applications.<br>3. To overcome this, the authors propose UniLight, a unified latent space that represents lighting across multiple modalities within a shared embedding.<br>4. UniLight employs modality-specific encoders for text, images, irradiance, and environment maps trained with contrastive learning to align their embeddings, complemented by an auxiliary spherical harmonics prediction task that enhances directional lighting understanding.<br>5. The system is trained and evaluated on large-scale, multi-modal data for three key tasks: lighting-based retrieval, environment-map generation, and controlling lighting in diffusion-based image synthesis.<br><br>Results demonstrate that UniLight successfully captures consistent, transferable lighting features, enabling flexible and effective manipulation of lighting information across different modalities, thus advancing lighting representation for various computer vision and graphics applications. <div>
arXiv:2512.04267v1 Announce Type: new 
Abstract: Lighting has a strong influence on visual appearance, yet understanding and representing lighting in images remains notoriously difficult. Various lighting representations exist, such as environment maps, irradiance, spherical harmonics, or text, but they are incompatible, which limits cross-modal transfer. We thus propose UniLight, a joint latent space as lighting representation, that unifies multiple modalities within a shared embedding. Modality-specific encoders for text, images, irradiance, and environment maps are trained contrastively to align their representations, with an auxiliary spherical-harmonics prediction task reinforcing directional understanding. Our multi-modal data pipeline enables large-scale training and evaluation across three tasks: lighting-based retrieval, environment-map generation, and lighting control in diffusion-based image synthesis. Experiments show that our representation captures consistent and transferable lighting features, enabling flexible manipulation across modalities.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer</title>
<link>https://arxiv.org/abs/2512.04282</link>
<guid>https://arxiv.org/abs/2512.04282</guid>
<content:encoded><![CDATA[
<div> Keywords: video motion transfer, normalizing flows, stochastic sampling, GRU, multimodal forecasting  

<br><br>Summary:  
The paper addresses the challenge of generating diverse and accurate future predictions for real-time video motion transfer, which is critical in applications like immersive gaming and vision-based anomaly detection. It presents a novel inference-time refinement technique that enhances the diversity of sequential forecasts by combining Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. Although GRU-NF integrates normalizing flows within a temporal forecasting model to capture multimodal distributions, its deterministic transformations can limit the model's expressive capacity. To overcome this, the authors incorporate Markov Chain Monte Carlo (MCMC) steps inspired by Stochastic Normalizing Flows (SNF) during inference, enabling exploration of a richer output space without retraining. The method, named Gated Recurrent Unit-Stochastic Normalizing Flows (GRU-SNF), is validated on a keypoint-based video motion transfer pipeline where temporal coherence and perceptual diversity are essential. Experimental results demonstrate that GRU-SNF produces more diverse outputs than GRU-NF while maintaining accuracy, even over longer prediction horizons. By injecting stochasticity during inference, the approach better captures multimodal behaviors in future trajectories, showcasing the potential benefits of integrating stochastic dynamics into flow-based sequence models for generative time series forecasting. <div>
arXiv:2512.04282v1 Announce Type: new 
Abstract: Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint</title>
<link>https://arxiv.org/abs/2512.04283</link>
<guid>https://arxiv.org/abs/2512.04283</guid>
<content:encoded><![CDATA[
<div> Flow matching, plug-and-play, stochastic differential equation, image restoration, acceleration  

<br><br>Summary:  
This paper addresses the gap between the empirical success and theoretical understanding of plug-and-play flow matching (PnP-Flow) models in image restoration. First, the authors derive a continuous limit of PnP-Flow, formulating it as a stochastic differential equation (SDE) surrogate model. This SDE model provides a rigorous framework to analyze PnP-Flow and offers two key insights for enhancements. One, it allows quantification of the image restoration error, which guides improvements in step scheduling and the regularization of the Lipschitz constant in the neural network-parameterized vector field, ultimately reducing error. Two, the SDE model inspires an acceleration strategy by extrapolating off-the-shelf PnP-Flow models, leading to a rescaled version of the SDE for faster convergence. The authors validate their theoretical findings on benchmark image restoration tasks including denoising, deblurring, super-resolution, and inpainting. Experimental results demonstrate that the SDE-informed improvements outperform baseline PnP-Flow and other state-of-the-art methods, achieving superior performance across multiple evaluation metrics. This work thus bridges theoretical and practical aspects, offering both a deeper understanding and practical advancements for PnP-Flow in image restoration. <div>
arXiv:2512.04283v1 Announce Type: new 
Abstract: Flow matching-based generative models have been integrated into the plug-and-play image restoration framework, and the resulting plug-and-play flow matching (PnP-Flow) model has achieved some remarkable empirical success for image restoration. However, the theoretical understanding of PnP-Flow lags its empirical success. In this paper, we derive a continuous limit for PnP-Flow, resulting in a stochastic differential equation (SDE) surrogate model of PnP-Flow. The SDE model provides two particular insights to improve PnP-Flow for image restoration: (1) It enables us to quantify the error for image restoration, informing us to improve step scheduling and regularize the Lipschitz constant of the neural network-parameterized vector field for error reduction. (2) It informs us to accelerate off-the-shelf PnP-Flow models via extrapolation, resulting in a rescaled version of the proposed SDE model. We validate the efficacy of the SDE-informed improved PnP-Flow using several benchmark tasks, including image denoising, deblurring, super-resolution, and inpainting. Numerical results show that our method significantly outperforms the baseline PnP-Flow and other state-of-the-art approaches, achieving superior performance across evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Single-Image Super-Resolution in the JPEG Compressed Domain</title>
<link>https://arxiv.org/abs/2512.04284</link>
<guid>https://arxiv.org/abs/2512.04284</guid>
<content:encoded><![CDATA[
<div> JPEG features, deep learning, super-resolution, data loading, discrete cosine transform

<br><br>Summary:  
Deep learning models have increased in complexity alongside the growth in input data size, causing data loading to remain a significant bottleneck impacting training and inference speeds despite advances in specialized hardware. This work addresses this challenge by proposing a method to train models directly on encoded JPEG features, which avoids the full JPEG decoding process and thus reduces computational overhead and improves data loading efficiency. Unlike previous studies that primarily focused on recognition tasks, this research explores the viability of the approach for single-image super-resolution (SISR), a restoration task. The authors develop a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain rather than in the pixel domain. Experimentally, this pipeline achieves a 2.6x speedup in data loading time and a 2.5x speedup in overall training time. Despite these efficiency gains, the model maintains visual quality that is comparable to traditional SISR methods, demonstrating that training directly on compressed features is an effective approach for accelerating training without sacrificing output quality. <div>
arXiv:2512.04284v1 Announce Type: new 
Abstract: Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications</title>
<link>https://arxiv.org/abs/2512.04303</link>
<guid>https://arxiv.org/abs/2512.04303</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular depth estimation, road surface geometry, planar parallax, self-supervised learning, metric depth recovery<br><br>Summary:<br><br>1. The paper introduces Gamma-from-Mono (GfM), a novel lightweight method for monocular geometry estimation designed to improve the accuracy of 3D perception of vehicle surroundings, particularly focusing on fine-scale road surface details such as bumps and slopes. <br>2. GfM addresses the projective ambiguity inherent in single-camera reconstruction by decoupling global and local structure, predicting a dominant road surface plane along with residual local variations expressed as gamma—a dimensionless ratio representing vertical deviation relative to depth from the camera. <br>3. This gamma parameter is grounded in planar parallax geometry and, combined with the known camera height above the ground, allows deterministic recovery of metric depth via a closed-form solution, bypassing the need for full extrinsic calibration.<br>4. The approach naturally prioritizes near-road details critical for vehicle control, uses a physically interpretable representation well-suited for self-supervised learning, and does not require large annotated datasets.<br>5. Experimental evaluation on the KITTI and Road Surface Reconstruction Dataset (RSRD) demonstrates that GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation, maintains competitive global depth performance, and exhibits robust adaptability across diverse camera setups with a lightweight 8.88 million parameter model. <div>
arXiv:2512.04303v1 Announce Type: new 
Abstract: Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How (Mis)calibrated is Your Federated CLIP and What To Do About It?</title>
<link>https://arxiv.org/abs/2512.04305</link>
<guid>https://arxiv.org/abs/2512.04305</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP calibration, federated learning, Textual Prompt Tuning, LoRA, FL2oRA<br><br>Summary:<br>1. The paper investigates the calibration of vision-language models, specifically CLIP, under federated learning (FL) conditions, a topic previously unexplored despite CLIP's widespread study.<br>2. It identifies that Textual Prompt Tuning methods, when applied in FL, tend to degrade calibration performance, highlighting challenges specific to the distributed setup.<br>3. The study evaluates existing in-training calibration methods combined with various global aggregation strategies and finds these provide only limited improvements in calibration.<br>4. A key insight is that the choice of model components selected for fine-tuning significantly impacts calibration, beyond the aggregation or calibration techniques themselves.<br>5. To address this, the authors propose FL²oRA, a novel approach based on LoRA (Low-Rank Adaptation), which enhances calibration naturally within FL without requiring explicit calibration steps.<br>6. Extensive experiments across multiple benchmarks demonstrate FL²oRA consistently yields better-calibrated models, simplifying reliable deployment of CLIP models in distributed learning environments.<br>7. The authors also provide a detailed analysis of factors contributing to FL²oRA's effectiveness and release the codebase publicly for further research and application. <div>
arXiv:2512.04305v1 Announce Type: new 
Abstract: While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\text{FL}^2\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\text{FL}^2\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction</title>
<link>https://arxiv.org/abs/2512.04309</link>
<guid>https://arxiv.org/abs/2512.04309</guid>
<content:encoded><![CDATA[
<div> Keywords: image captioning, text-only training, modality gap reduction, CLIP, retrieval-augmentation<br><br>Summary:<br><br>This paper addresses the challenge of generating image captions without relying on human-annotated image-text pairs, focusing on reducing dependency on curated datasets. The authors propose TOMCap, a novel method that leverages a pre-trained language model decoder prompted by information extracted from CLIP representations, which undergo a specific process to minimize the modality gap between visual and textual domains. A key innovation of TOMCap is the combined use of retrieved caption examples and latent vector representations, which together guide the caption generation process more effectively. Extensive experiments demonstrate that TOMCap outperforms existing training-free and text-only image captioning methods, showcasing its potential as a strong alternative to fully supervised approaches. Furthermore, the paper includes an in-depth analysis of how different configurations of retrieval-augmentation and modality gap reduction influence the overall performance, providing insights into optimal design choices. The method highlights the feasibility of training image captioning models through text-only data by effectively bridging the visual-textual modality gap, opening new avenues for caption generation under limited supervision. <div>
arXiv:2512.04309v1 Announce Type: new 
Abstract: Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time Cricket Sorting By Sex</title>
<link>https://arxiv.org/abs/2512.04311</link>
<guid>https://arxiv.org/abs/2512.04311</guid>
<content:encoded><![CDATA[
<div> Keywords: Acheta domesticus, automated sex sorting, computer vision, YOLOv8, sustainable protein<br><br>Summary:<br><br>1. The rising global demand for sustainable protein sources has highlighted edible insects, with Acheta domesticus (house cricket) recognized as a prime candidate for industrial farming due to its nutritional value and production potential.<br><br>2. Existing cricket farming typically involves mixed-sex populations without sex-based sorting, missing out on advantages such as optimized breeding ratios, selective breeding improvements, and tailored nutritional outcomes.<br><br>3. This study introduces a low-cost, real-time automated system specifically designed for sex-based sorting of Acheta domesticus, integrating computer vision techniques with physical actuation.<br><br>4. The system employs a Raspberry Pi 5 paired with the official Raspberry AI Camera alongside a custom-trained YOLOv8 nano object detection model to identify and classify crickets by sex.<br><br>5. Achieving a high mean Average Precision (mAP@0.5) of 0.977 in testing and an 86.8% sorting accuracy in real-world trials, the results demonstrate the capability of lightweight deep learning models to operate effectively on resource-limited devices.<br><br>6. This technology offers a practical and scalable solution to enhance efficiency, sustainability, and precision in cricket farming operations, potentially transforming practices in edible insect production. <div>
arXiv:2512.04311v1 Announce Type: new 
Abstract: The global demand for sustainable protein sources is driving increasing interest in edible insects, with Acheta domesticus (house cricket) identified as one of the most suitable species for industrial production. Current farming practices typically rear crickets in mixed-sex populations without automated sex sorting, despite potential benefits such as selective breeding, optimized reproduction ratios, and nutritional differentiation. This work presents a low-cost, real-time system for automated sex-based sorting of Acheta domesticus, combining computer vision and physical actuation. The device integrates a Raspberry Pi 5 with the official Raspberry AI Camera and a custom YOLOv8 nano object detection model, together with a servo-actuated sorting arm. The model reached a mean Average Precision at IoU 0.5 (mAP@0.5) of 0.977 during testing, and real-world experiments with groups of crickets achieved an overall sorting accuracy of 86.8%. These results demonstrate the feasibility of deploying lightweight deep learning models on resource-constrained devices for insect farming applications, offering a practical solution to improve efficiency and sustainability in cricket production.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding</title>
<link>https://arxiv.org/abs/2512.04313</link>
<guid>https://arxiv.org/abs/2512.04313</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG decoding, facial expression synthesis, CNN-Transformer, 3D Gaussian Splatting, emotion recognition<br><br>Summary:  
1. This study introduces Mind-to-Face, a novel framework that decodes non-invasive EEG signals directly into high-fidelity, dynamic facial expressions.  
2. The authors constructed a dual-modality recording setup that synchronizes EEG data with multi-view facial videos under emotion-eliciting stimuli, enabling precise supervision for training the neural-to-visual mapping.  
3. Their model employs a CNN-Transformer encoder architecture to convert EEG inputs into dense 3D position maps with over 65,000 vertices, capturing fine facial geometry and subtle emotional dynamics.  
4. The generated 3D facial data is rendered using a modified 3D Gaussian Splatting pipeline, producing photorealistic and view-consistent avatar outputs.  
5. Extensive evaluations demonstrate that EEG signals alone can reliably and subject-specifically predict dynamic facial expressions, including nuanced emotional states, revealing richer affective and geometric information in neural signals than previously recognized.  
6. Mind-to-Face represents a new paradigm in neural-driven avatars, unlocking personalized, emotion-aware telepresence and enhanced cognitive interaction for immersive virtual environments. <div>
arXiv:2512.04313v1 Announce Type: new 
Abstract: Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision</title>
<link>https://arxiv.org/abs/2512.04314</link>
<guid>https://arxiv.org/abs/2512.04314</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, DisentangleFormer, hyperspectral imaging, spatial-channel decoupling, multi-scale FFN<br><br>Summary:<br><br>1. Vision Transformers (ViTs) typically use self-attention that jointly processes spatial and channel dimensions, leading to entangled representations that hinder the separate modeling of structural and semantic information. This challenge is particularly critical in hyperspectral imaging where each channel carries distinct biophysical or biochemical information.<br><br>2. The paper introduces DisentangleFormer, a novel architecture designed to achieve robust multi-channel vision representation by explicitly decoupling spatial and channel information, inspired by information-theoretic principles favoring decorrelated representations.<br><br>3. DisentangleFormer features three key components: (a) Parallel Disentanglement, which independently processes spatial-token and channel-token streams to ensure decorrelated features across spatial and spectral domains; (b) Squeezed Token Enhancer, an adaptive calibration module that dynamically fuses spatial and channel streams to optimize information flow; (c) Multi-Scale Feed-Forward Network (FFN), which supplements global attention with multi-scale local context to capture intricate structural and semantic dependencies.<br><br>4. Extensive experiments on multiple hyperspectral benchmarks—including Indian Pine, Pavia University, Houston datasets, the large-scale BigEarthNet remote sensing dataset, and an infrared pathology dataset—show that DisentangleFormer consistently outperforms state-of-the-art models.<br><br>5. Additionally, DisentangleFormer maintains competitive accuracy on the natural image classification benchmark ImageNet while reducing computational cost by 17.8% in FLOPs, demonstrating efficiency alongside superior performance. The code will be publicly released upon paper acceptance. <div>
arXiv:2512.04314v1 Announce Type: new 
Abstract: Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.04315</link>
<guid>https://arxiv.org/abs/2512.04315</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D Gaussian Splatting, multi-video synchronization, dynamic 3D scenes, Fused Gromov-Wasserstein, temporal alignment  

<br><br>Summary:  
This paper addresses the challenge of modeling dynamic 3D scenes from multiple unsynchronized videos by introducing a novel 4D Gaussian Splatting (4DGS) method named SyncTrack4D. The approach starts by computing dense 4D feature tracks for each video and establishes cross-video correspondences using a Fused Gromov-Wasserstein optimal transport method. It then performs a global frame-level temporal alignment to maximize the overlap of matched dynamic scene motions across videos. To refine synchronization, SyncTrack4D applies sub-frame synchronization leveraging a motion-spline scaffold representation within the multi-video 4D Gaussian splatting framework. The output is a fully synchronized 4DGS model with explicit 3D trajectories and precise temporal offsets for every input video. The method is evaluated on datasets such as Panoptic Studio and SyncNeRF Blender, demonstrating state-of-the-art sub-frame synchronization accuracy with average temporal errors below 0.26 frames. Additionally, high-fidelity 4D reconstructions were achieved, reaching a peak signal-to-noise ratio (PSNR) of 26.3 on the Panoptic Studio dataset. Notably, this approach does not require predefined scene objects or prior models, marking the first general 4D Gaussian Splatting framework for handling unsynchronized multi-video dynamic scene reconstruction in real-world scenarios. <div>
arXiv:2512.04315v1 Announce Type: new 
Abstract: Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayes-DIC Net: Estimating Digital Image Correlation Uncertainty with Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2512.04323</link>
<guid>https://arxiv.org/abs/2512.04323</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital Image Correlation, non-uniform B-spline, displacement fields, Bayes-DIC Net, Bayesian neural network<br><br>Summary:<br><br>This paper presents a novel method to generate high-quality Digital Image Correlation (DIC) datasets utilizing non-uniform B-spline surfaces. The approach randomly generates control point coordinates to construct diverse displacement fields that mimic realistic scenarios, which are then used to create speckle pattern datasets. This enables the creation of large-scale datasets that better represent real-world displacement conditions, thereby improving the training and generalization ability of deep learning-based DIC algorithms. The paper also introduces a new network architecture named Bayes-DIC Net, which captures information at multiple levels during down-sampling and aggregates multi-level features using a single skip connection in the up-sampling phase. Bayes-DIC Net employs lightweight convolutional blocks to expand the receptive field and capture rich contextual information while keeping computational costs low. Incorporating dropout modules activated during inference transforms Bayes-DIC Net into a Bayesian neural network, allowing it to produce both predictions and uncertainty estimates on unlabeled real datasets. This capability significantly enhances the network’s reliability and practicality for real-world displacement field predictions. Overall, the paper contributes innovative dataset generation techniques and algorithmic improvements in DIC analysis. <div>
arXiv:2512.04323v1 Announce Type: new 
Abstract: This paper introduces a novel method for generating high-quality Digital Image Correlation (DIC) dataset based on non-uniform B-spline surfaces. By randomly generating control point coordinates, we construct displacement fields that encompass a variety of realistic displacement scenarios, which are subsequently used to generate speckle pattern datasets. This approach enables the generation of a large-scale dataset that capture real-world displacement field situations, thereby enhancing the training and generalization capabilities of deep learning-based DIC algorithms. Additionally, we propose a novel network architecture, termed Bayes-DIC Net, which extracts information at multiple levels during the down-sampling phase and facilitates the aggregation of information across various levels through a single skip connection during the up-sampling phase. Bayes-DIC Net incorporates a series of lightweight convolutional blocks designed to expand the receptive field and capture rich contextual information while minimizing computational costs. Furthermore, by integrating appropriate dropout modules into Bayes-DIC Net and activating them during the network inference stage, Bayes-DIC Net is transformed into a Bayesian neural network. This transformation allows the network to provide not only predictive results but also confidence levels in these predictions when processing real unlabeled datasets. This feature significantly enhances the practicality and reliability of our network in real-world displacement field prediction tasks. Through these innovations, this paper offers new perspectives and methods for dataset generation and algorithm performance enhancement in the field of DIC.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks</title>
<link>https://arxiv.org/abs/2512.04329</link>
<guid>https://arxiv.org/abs/2512.04329</guid>
<content:encoded><![CDATA[
<div> Keywords: neural modules, PyTorch codebases, retrieval-augmented generation, code reuse, neural architectures<br><br>Summary: NN-RAG is a novel retrieval-augmented generation system designed to enhance research efficiency by enabling the discovery, extraction, and validation of reusable neural network components from large, heterogeneous PyTorch codebases. Unlike traditional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, preserves imports, and utilizes validator-gated promotion to ensure that every retrieved neural block is scope-closed, compilable, and runnable. When applied to 19 major repositories, NN-RAG extracted 1,289 candidate blocks and validated 941 of them (73.0%), with over 80% being structurally unique. Multi-level de-duplication techniques (exact, lexical, structural) revealed that NN-RAG contributes approximately 72% of novel network architectures in the LEMUR dataset, underscoring its capacity to significantly expand the diversity of executable neural architectures. Beyond quantity, NN-RAG uniquely supports cross-repository migration of architectural patterns, allowing automatic identification and regeneration of dependency-complete reusable modules in new contexts. The framework’s neutral design permits optional integration with language models for synthesis or dataset registration while avoiding redistribution of third-party code. Overall, NN-RAG offers the first open-source solution that transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery at scale. <div>
arXiv:2512.04329v1 Announce Type: new 
Abstract: Reusing existing neural-network components is central to research efficiency, yet discovering, extracting, and validating such modules across thousands of open-source repositories remains difficult. We introduce NN-RAG, a retrieval-augmented generation system that converts large, heterogeneous PyTorch codebases into a searchable and executable library of validated neural modules. Unlike conventional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, import-preserving reconstruction, and validator-gated promotion -- ensuring that every retrieved block is scope-closed, compilable, and runnable. Applied to 19 major repositories, the pipeline extracted 1,289 candidate blocks, validated 941 (73.0%), and demonstrated that over 80% are structurally unique. Through multi-level de-duplication (exact, lexical, structural), we find that NN-RAG contributes the overwhelming majority of unique architectures to the LEMUR dataset, supplying approximately 72% of all novel network structures. Beyond quantity, NN-RAG uniquely enables cross-repository migration of architectural patterns, automatically identifying reusable modules in one project and regenerating them, dependency-complete, in another context. To our knowledge, no other open-source system provides this capability at scale. The framework's neutral specifications further allow optional integration with language models for synthesis or dataset registration without redistributing third-party code. Overall, NN-RAG transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery, offering a first open-source solution that both quantifies and expands the diversity of executable neural architectures across repositories.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Set Face Forgery Detection via Dual-Level Evidence Collection</title>
<link>https://arxiv.org/abs/2512.04331</link>
<guid>https://arxiv.org/abs/2512.04331</guid>
<content:encoded><![CDATA[
<div> Face forgery detection, open set recognition, uncertainty estimation, dual-level evidential learning, frequency-spatial fusion<br><br>Summary:<br><br>1. The paper addresses the problem of face forgery detection, focusing on the limitations of current methods which mainly perform binary Real-vs-Fake classification or identify only known types of forgeries.<br><br>2. It introduces the Open Set Face Forgery Detection (OSFFD) problem, which requires detection models to recognize novel, previously unseen fake categories, reflecting real-world challenges as new forgery methods constantly emerge.<br><br>3. The authors propose a novel Dual-Level Evidential face forgery Detection (DLED) approach that leverages uncertainty estimation to better handle these new fake types.<br><br>4. DLED collects and integrates category-specific evidence from both spatial and frequency domains in the visual data, enhancing the model’s ability to estimate prediction uncertainty effectively.<br><br>5. Extensive experiments demonstrate that DLED outperforms multiple baseline models by an average of 20% in detecting novel fake categories and maintains competitive performance in the traditional Real-versus-Fake detection task, establishing state-of-the-art results in this field. <div>
arXiv:2512.04331v1 Announce Type: new 
Abstract: The proliferation of face forgeries has increasingly undermined confidence in the authenticity of online content. Given the rapid development of face forgery generation algorithms, new fake categories are likely to keep appearing, posing a major challenge to existing face forgery detection methods. Despite recent advances in face forgery detection, existing methods are typically limited to binary Real-vs-Fake classification or the identification of known fake categories, and are incapable of detecting the emergence of novel types of forgeries. In this work, we study the Open Set Face Forgery Detection (OSFFD) problem, which demands that the detection model recognize novel fake categories. We reformulate the OSFFD problem and address it through uncertainty estimation, enhancing its applicability to real-world scenarios. Specifically, we propose the Dual-Level Evidential face forgery Detection (DLED) approach, which collects and fuses category-specific evidence on the spatial and frequency levels to estimate prediction uncertainty. Extensive evaluations conducted across diverse experimental settings demonstrate that the proposed DLED method achieves state-of-the-art performance, outperforming various baseline models by an average of 20% in detecting forgeries from novel fake categories. Moreover, on the traditional Real-versus-Fake face forgery detection task, our DLED method concurrently exhibits competitive performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment</title>
<link>https://arxiv.org/abs/2512.04356</link>
<guid>https://arxiv.org/abs/2512.04356</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal LLMs, hallucination, video captioning, contrastive alignment, spurious correlations<br><br>Summary:<br><br>1. The paper addresses the problem of hallucination in video captioning by multimodal large language models (MLLMs), which causes factual inaccuracies in generated video descriptions involving objects and actions.<br>2. Unlike previous works focused on static images, this research targets the combined challenge of mitigating both visual object and temporal action hallucinations in dynamic video content.<br>3. To resolve this, the authors propose the Self-Augmented Contrastive Alignment (SANTA) framework, designed to improve faithfulness by reducing spurious correlations and emphasizing accurate visual facts.<br>4. SANTA introduces a hallucinative self-augmentation approach that generates contrasted negative captions by identifying potential hallucinations in the model’s output, thereby guiding the system to distinguish between factual and spurious content.<br>5. Additionally, the framework includes a tracklet-phrase contrastive alignment mechanism that aligns tracked regional objects and relation-guided temporal actions with their corresponding visual and temporal phrases to enforce semantic consistency.<br>6. Extensive experiments on benchmarks focused on hallucination detection demonstrate that SANTA significantly outperforms existing methods in reducing both object and action hallucinations, improving the overall descriptive accuracy of MLLMs for video captioning.<br><br>This study highlights an effective strategy to enhance the reliability of multimodal models in generating faithful video descriptions by jointly addressing spatial and temporal visual hallucinations. <div>
arXiv:2512.04356v1 Announce Type: new 
Abstract: Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAFNet:Multi-frequency Adaptive Fusion Network for Real-time Stereo Matching</title>
<link>https://arxiv.org/abs/2512.04358</link>
<guid>https://arxiv.org/abs/2512.04358</guid>
<content:encoded><![CDATA[
<div> Keywords: stereo matching, multi-frequency, adaptive fusion, disparity estimation, real-time performance<br><br>Summary:<br><br>The paper addresses limitations in existing stereo matching networks, which either rely on costly 3D convolution-based cost-volume aggregation or iterative deformation methods that fail to capture non-local contextual information. These constraints hinder deployment on resource-limited mobile devices, especially for real-time applications. To overcome this, the authors propose the Multi-frequency Adaptive Fusion Network (MAFNet), designed to generate high-quality disparity maps efficiently with only 2D convolutions. A key innovation is the adaptive frequency-domain filtering attention module that separates the full cost volume into high- and low-frequency components, enabling frequency-aware feature aggregation tailored to these distinct bands. Furthermore, the method incorporates a Linformer-based low-rank attention mechanism to effectively fuse the frequency-separated features in an adaptive manner, enhancing disparity estimation robustness. Experimental results on benchmarks like Scene Flow and KITTI 2015 demonstrate that MAFNet outperforms current real-time stereo matching methods, offering an improved trade-off between accuracy and computational efficiency. This approach shows promise for stereo vision applications requiring real-time processing on devices with limited resources. <div>
arXiv:2512.04358v1 Announce Type: new 
Abstract: Existing stereo matching networks typically rely on either cost-volume construction based on 3D convolutions or deformation methods based on iterative optimization. The former incurs significant computational overhead during cost aggregation, whereas the latter often lacks the ability to model non-local contextual information. These methods exhibit poor compatibility on resource-constrained mobile devices, limiting their deployment in real-time applications. To address this, we propose a Multi-frequency Adaptive Fusion Network (MAFNet), which can produce high-quality disparity maps using only efficient 2D convolutions. Specifically, we design an adaptive frequency-domain filtering attention module that decomposes the full cost volume into high-frequency and low-frequency volumes, performing frequency-aware feature aggregation separately. Subsequently, we introduce a Linformer-based low-rank attention mechanism to adaptively fuse high- and low-frequency information, yielding more robust disparity estimation. Extensive experiments demonstrate that the proposed MAFNet significantly outperforms existing real-time methods on public datasets such as Scene Flow and KITTI 2015, showing a favorable balance between accuracy and real-time performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring</title>
<link>https://arxiv.org/abs/2512.04390</link>
<guid>https://arxiv.org/abs/2512.04390</guid>
<content:encoded><![CDATA[
<div> Keywords: video restoration, dynamic exposure, super-resolution, deblurring, FMA-Net++

<br><br>Summary:  
1. The paper addresses the challenge of real-world video restoration affected by complex degradations stemming from motion combined with dynamically varying exposure, a problem often overlooked in prior research and common in auto-exposure or low-light video capture.  
2. The authors propose FMA-Net++, a novel framework for joint video super-resolution and deblurring that explicitly models the coupled effects of motion and changing exposure conditions to better restore video quality.  
3. FMA-Net++ features a sequence-level architecture using Hierarchical Refinement with Bidirectional Propagation blocks, which facilitates parallel processing and long-range temporal modeling to capture temporal dependencies effectively.  
4. Within each block, an Exposure Time-aware Modulation layer adjusts features based on each frame’s exposure, feeding into an exposure-aware Flow-Guided Dynamic Filtering module that infers degradation kernels sensitive to both motion and exposure variations.  
5. The method decouples degradation learning from restoration by first predicting exposure- and motion-aware priors to guide the restoration process, enhancing both accuracy and computational efficiency.  
6. To evaluate performance under realistic capture conditions, the authors introduce two new benchmarks: REDS-ME (multi-exposure) and REDS-RE (random exposure).  
7. Trained exclusively on synthetic data, FMA-Net++ achieves state-of-the-art results in restoration quality and temporal consistency on these benchmarks and the GoPro dataset, outperforming recent methods in accuracy and inference speed.  
8. Moreover, the approach generalizes effectively to challenging real-world videos, demonstrating robustness and practical applicability. <div>
arXiv:2512.04390v1 Announce Type: new 
Abstract: Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.04395</link>
<guid>https://arxiv.org/abs/2512.04395</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Fourier Analysis, Representation Disentanglement, Cross-Attention, Few-Shot Learning<br><br>Summary:<br>1. The paper addresses the limitation of current large-scale pre-trained Vision-Language Models (VLMs) that learn holistic image representations, where domain-invariant structures and domain-specific styles are entangled, which can hinder generalization.<br>2. It introduces a novel framework called Fourier-Attentive Representation Learning (FARL) that explicitly disentangles visual features into structural and stylistic components using Fourier analysis.<br>3. The core innovation involves a dual cross-attention mechanism where learnable tokens separately query the phase spectrum for structural features and the amplitude spectrum for stylistic features, producing enriched and disentangled representations.<br>4. These disentangled tokens are injected deeply into the VLM encoders via an asymmetric injection strategy, which enhances the robustness of vision-language alignment and model adaptation.<br>5. Extensive experiments conducted across 15 diverse datasets validate the effectiveness of FARL, showing improved few-shot learning capabilities due to the better disentanglement of structural and stylistic visual cues. <div>
arXiv:2512.04395v1 Announce Type: new 
Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Evaluation of Transfer Learning Based Medical Image Classification Techniques for Disease Detection</title>
<link>https://arxiv.org/abs/2512.04397</link>
<guid>https://arxiv.org/abs/2512.04397</guid>
<content:encoded><![CDATA[
<div> Keywords: transfer learning, medical image classification, deep convolutional neural networks, chest X-ray, InceptionV3  

<br><br>Summary:  
The paper addresses the challenge of training large deep learning models from scratch for medical image classification tasks, specifically using chest X-rays. It evaluates the effectiveness of transfer learning (TL) by reusing six pre-trained convolutional neural network models: AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3. The study finds that InceptionV3 consistently achieves the highest performance across standard evaluation metrics, while the ResNet models demonstrate improved accuracy with increasing depth. AlexNet and VGG16 perform adequately but lag behind the deeper architectures in terms of accuracy. Additionally, the authors examine the robustness of these models through uncertainty analysis and compare their computational efficiency via runtime measurements. The results indicate that TL provides clear benefits, particularly when the available dataset is limited in size. However, the degree of improvement depends on various factors including the model architecture, the size of the dataset, and how similar the source and target domains are. The research also highlights that leveraging a well-trained feature extractor allows the use of a simple lightweight feedforward network for efficient predictions. Overall, the findings offer valuable insights into selecting suitable TL models for medical image classification based on specific requirements such as accuracy, robustness, and computational resources. <div>
arXiv:2512.04397v1 Announce Type: new 
Abstract: Medical image classification plays an increasingly vital role in identifying various diseases by classifying medical images, such as X-rays, MRIs and CT scans, into different categories based on their features. In recent years, deep learning techniques have attracted significant attention in medical image classification. However, it is usually infeasible to train an entire large deep learning model from scratch. To address this issue, one of the solutions is the transfer learning (TL) technique, where a pre-trained model is reused for a new task. In this paper, we present a comprehensive analysis of TL techniques for medical image classification using deep convolutional neural networks. We evaluate six pre-trained models (AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3) on a custom chest X-ray dataset for disease detection. The experimental results demonstrate that InceptionV3 consistently outperforms other models across all the standard metrics. The ResNet family shows progressively better performance with increasing depth, whereas VGG16 and AlexNet perform reasonably well but with lower accuracy. In addition, we also conduct uncertainty analysis and runtime comparison to assess the robustness and computational efficiency of these models. Our findings reveal that TL is beneficial in most cases, especially with limited data, but the extent of improvement depends on several factors such as model architecture, dataset size, and domain similarity between source and target tasks. Moreover, we demonstrate that with a well-trained feature extractor, only a lightweight feedforward model is enough to provide efficient prediction. As such, this study contributes to the understanding of TL in medical image classification, and provides insights for selecting appropriate models based on specific requirements.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection</title>
<link>https://arxiv.org/abs/2512.04413</link>
<guid>https://arxiv.org/abs/2512.04413</guid>
<content:encoded><![CDATA[
<div> Knowledge distillation, remote sensing object detection, spectral decomposition, wavelet transform, density-independent scale weight<br><br>Summary:<br><br>This paper addresses the challenges of knowledge distillation in remote sensing object detection, particularly issues arising from mixed features and subtle variations leading to knowledge confusion. The authors propose a novel, architecture-agnostic distillation method called Dual-Stream Spectral Decoupling Distillation (DS2D2) that integrates both explicit and implicit distillation strategies using spectral decomposition. First, the method applies a first-order wavelet transform to decompose spectral features, preserving critical spatial characteristics of remote sensing images, which is important for accurate object detection. Building on this spatial preservation, a Density-Independent Scale Weight (DISW) is introduced to specifically target the difficulties associated with dense and small objects common in remote sensing imagery. Second, implicit knowledge is extracted from subtle discrepancies between student and teacher model features, which significantly impact prediction accuracy when processed through detection heads. These discrepancies are captured via full-frequency and high-frequency amplifiers, translating feature differences into prediction deviations. The effectiveness of DS2D2 is validated through extensive experiments on benchmark datasets DIOR and DOTA, demonstrating improvements of 4.2% and 3.8% in AP50 for RetinaNet and Faster R-CNN respectively, outperforming existing distillation approaches. The authors also provide the source code for reproducibility and further research. <div>
arXiv:2512.04413v1 Announce Type: new 
Abstract: Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UTrice: Unifying Primitives in Differentiable Ray Tracing and Rasterization via Triangles for Particle-Based 3D Scenes</title>
<link>https://arxiv.org/abs/2512.04421</link>
<guid>https://arxiv.org/abs/2512.04421</guid>
<content:encoded><![CDATA[
<div> Gaussian Particles, Ray Tracing, Triangle Rendering, Novel-View Synthesis, Real-Time Performance  

<br><br>Summary:  
This work addresses limitations in current 3D Gaussian particle ray tracing methods which depend on proxy geometry, such as intermediate mesh construction and expensive intersection computations. The authors propose a novel differentiable ray tracing pipeline that directly uses triangles as rendering primitives, eliminating the need for proxy geometry. By treating triangles as the fundamental unit for both ray tracing and rasterization, the method unifies rendering primitives used in novel-view synthesis. Experimental results demonstrate that this triangle-based approach achieves significantly better rendering quality compared to traditional Gaussian particle ray tracing techniques while still maintaining real-time rendering speeds. Additionally, the proposed pipeline supports direct rendering of triangles optimized via the rasterization-based technique known as Triangle Splatting. This enables seamless integration and improved flexibility for rendering realistic effects like depth of field and refraction in novel-view synthesis applications. The approach therefore overcomes inefficiencies and quality issues present in previous methods by combining the strengths of rasterization and ray tracing in a single, efficient framework. <div>
arXiv:2512.04421v1 Announce Type: new 
Abstract: Ray tracing 3D Gaussian particles enables realistic effects such as depth of field, refractions, and flexible camera modeling for novel-view synthesis. However, existing methods trace Gaussians through proxy geometry, which requires constructing complex intermediate meshes and performing costly intersection tests. This limitation arises because Gaussian-based particles are not well suited as unified primitives for both ray tracing and rasterization. In this work, we propose a differentiable triangle-based ray tracing pipeline that directly treats triangles as rendering primitives without relying on any proxy geometry. Our results show that the proposed method achieves significantly higher rendering quality than existing ray tracing approaches while maintaining real-time rendering performance. Moreover, our pipeline can directly render triangles optimized by the rasterization-based method Triangle Splatting, thus unifying the primitives used in novel-view synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models</title>
<link>https://arxiv.org/abs/2512.04425</link>
<guid>https://arxiv.org/abs/2512.04425</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson’s disease, gait analysis, multimodal fusion, explainability, RGB-D data<br><br>Summary:  
Accurate and interpretable gait analysis is essential for the early detection of Parkinson’s disease (PD). Most existing methods are limited by reliance on single-modality inputs, low robustness, and lack of clinical transparency. This paper proposes an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic and challenging conditions such as low lighting and occlusion. The system uses dual YOLOv11-based encoders to perform modality-specific feature extraction. A novel Multi-Scale Local-Global Extraction (MLGE) module along with a Cross-Spatial Neck Fusion mechanism enhances spatial-temporal representation, capturing detailed limb movements (e.g., reduced arm swing) and overall gait dynamics (e.g., short strides, turning difficulty). For clinical interpretability, a frozen Large Language Model (LLM) translates the fused visual embeddings and structured metadata into meaningful textual explanations. Experimental results demonstrate that this RGB-D fusion framework outperforms single-input baselines in accuracy and robustness, while providing clear visual-linguistic reasoning. By combining multimodal feature learning with language-based explainability, the study bridges the gap between visual recognition and clinical understanding, presenting a novel vision-language paradigm for reliable, interpretable Parkinson’s disease gait analysis. The code for the system is publicly available at the provided GitHub link. <div>
arXiv:2512.04425v1 Announce Type: new 
Abstract: Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Paced and Self-Corrective Masked Prediction for Movie Trailer Generation</title>
<link>https://arxiv.org/abs/2512.04426</link>
<guid>https://arxiv.org/abs/2512.04426</guid>
<content:encoded><![CDATA[
<div> Movie trailer generation, Transformer encoder, self-paced learning, masked prediction, self-correction  

<br><br>Summary:  
This paper addresses the task of automatic movie trailer generation, which involves selecting and reorganizing movie shots to create engaging trailers. It critiques the dominant "selection-then-ranking" approach that first selects key shots and then ranks them, noting this method suffers from error propagation that limits trailer quality. To overcome these limitations, the authors propose a novel approach called SSMP, a self-paced and self-corrective masked prediction method. SSMP employs a Transformer encoder that models bi-directional context by taking entire sequences of movie shots as input prompts and generates trailer shot sequences. Training involves reconstructing trailer shot sequences from randomly masked versions, with the mask ratio controlled in a self-paced manner to adapt task difficulty to the model's performance. During generation, SSMP progressively fills in shot positions with high confidence predictions and re-masks remaining positions for further refinement, implementing a self-correction mechanism inspired by human editing processes. Extensive quantitative evaluations and user studies verify that SSMP significantly outperforms existing automatic trailer generation techniques. A demonstration of the method is made available via a publicly accessible GitHub repository. <div>
arXiv:2512.04426v1 Announce Type: new 
Abstract: As a challenging video editing task, movie trailer generation involves selecting and reorganizing movie shots to create engaging trailers. Currently, most existing automatic trailer generation methods employ a "selection-then-ranking" paradigm (i.e., first selecting key shots and then ranking them), which suffers from inevitable error propagation and limits the quality of the generated trailers. Beyond this paradigm, we propose a new self-paced and self-corrective masked prediction method called SSMP, which achieves state-of-the-art results in automatic trailer generation via bi-directional contextual modeling and progressive self-correction. In particular, SSMP trains a Transformer encoder that takes the movie shot sequences as prompts and generates corresponding trailer shot sequences accordingly. The model is trained via masked prediction, reconstructing each trailer shot sequence from its randomly masked counterpart. The mask ratio is self-paced, allowing the task difficulty to adapt to the model and thereby improving model performance. When generating a movie trailer, the model fills the shot positions with high confidence at each step and re-masks the remaining positions for the next prediction, forming a progressive self-correction mechanism that is analogous to how human editors work. Both quantitative results and user studies demonstrate the superiority of SSMP in comparison to existing automatic movie trailer generation methods. Demo is available at: https://github.com/Dixin-Lab/SSMP.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.04441</link>
<guid>https://arxiv.org/abs/2512.04441</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, trajectory planning, context simulation, vision-language model, multi-objective evaluation<br><br>Summary:<br><br>This paper presents MindDrive, an innovative framework for End-to-End autonomous driving that harmonizes high-quality trajectory generation with comprehensive decision reasoning. The approach is structured into three key stages: context simulation, candidate trajectory generation, and multi-objective trade-off evaluation. The Future-aware Trajectory Generator (FaTG), leveraging a World Action Model, enables ego-conditioned "what-if" simulations to forecast potential future scenarios and produce foresighted trajectory candidates. Complementing this, the VLM-oriented Evaluator (VLoE) applies a large vision-language model's reasoning capabilities to evaluate these candidates on multiple objectives, including safety, comfort, and efficiency, facilitating decision-making aligned with human reasoning. Extensive testing on the NAVSIM-v1 and NAVSIM-v2 benchmarks confirms that MindDrive outperforms current methods by achieving superior performance in multi-dimensional driving metrics. The framework notably enhances safety, regulatory compliance, and generalization in autonomous driving tasks. Ultimately, this work marks a significant advancement toward interpretable and cognitively guided autonomous driving systems that are both robust and aligned with human-like decision processes. <div>
arXiv:2512.04441v1 Announce Type: new 
Abstract: End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of "context simulation - candidate generation - multi-objective trade-off". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned "what-if" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios</title>
<link>https://arxiv.org/abs/2512.04451</link>
<guid>https://arxiv.org/abs/2512.04451</guid>
<content:encoded><![CDATA[
<div> Embodied Intelligence, Streaming Video, Question Answering, Video-LLMs, Benchmark

<br><br>Summary:  
This paper introduces StreamEQA, a novel benchmark designed for streaming video question answering specifically in embodied scenarios, addressing the need for continuous perception and reasoning over real-world streaming visual inputs. The benchmark evaluates multimedia large language models (MLLMs) along two key dimensions: Embodied and Streaming. The Embodied dimension categorizes questions into three levels—perception (fine-grained visual recognition), interaction (reasoning about agent-object interactions), and planning (high-level goal-directed reasoning). The Streaming dimension divides questions into backward, real-time, and forward reasoning, each requiring different temporal contexts to answer. StreamEQA is built on 156 independent long videos, encompassing 42 distinct tasks and approximately 21,000 question-answer pairs annotated with precise timestamps. The dataset was created using a hybrid pipeline that combines automated question generation with human refinement to ensure quality. Evaluations conducted on 13 state-of-the-art video-based large language models show that, despite their strong performance on conventional benchmarks, these models still face significant challenges in understanding streaming videos within embodied scenarios. The authors anticipate that StreamEQA will drive future research efforts toward improving streaming video comprehension in embodied intelligence applications. <div>
arXiv:2512.04451v1 Announce Type: new 
Abstract: As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis</title>
<link>https://arxiv.org/abs/2512.04456</link>
<guid>https://arxiv.org/abs/2512.04456</guid>
<content:encoded><![CDATA[
<div> Keywords: image denoising, diffusion model, noise synthesis, guidance, data augmentation<br><br>Summary:<br><br>This paper addresses the challenge of acquiring real-world noisy image data for training denoising models by proposing a novel noise synthesis method called GuidNoise. Unlike previous generative approaches requiring extensive camera metadata and large noisy-clean image datasets, GuidNoise operates using only a single noisy/clean image pair as guidance, which is easier to obtain. The method leverages a diffusion model enhanced with a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss designed to improve the backward diffusion process, enabling the generation of realistic and diverse noise patterns. GuidNoise functions without the need for additional metadata during both training and inference, providing flexible noise synthesis across varying noise environments. A key advantage of GuidNoise is its ability to efficiently generate synthetic noisy-clean image pairs on demand at inference time. These synthetic pairs serve as a powerful data augmentation tool that significantly boosts denoising performance, particularly when using lightweight models or limited training data in practical applications. The paper also provides implementation code, facilitating reproducibility and adoption of the method in related image denoising tasks. <div>
arXiv:2512.04456v1 Announce Type: new 
Abstract: Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning</title>
<link>https://arxiv.org/abs/2512.04459</link>
<guid>https://arxiv.org/abs/2512.04459</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, vision-language models, discrete diffusion, end-to-end driving, behavior-trajectory consistency<br><br>Summary:<br><br>The paper addresses the challenge of out-of-distribution (OOD) scenarios in autonomous driving by focusing on improving end-to-end (E2E) driving systems through the integration of vision-language models (VLMs). It critiques existing autoregressive (AR) VLMs for their limitations caused by causal attention and sequential token generation, which hinder consistency and controllability between high-level reasoning and low-level planning. The authors propose dVLM-AD, a novel diffusion-based vision-language model employing discrete diffusion and bidirectional attention mechanisms to enhance controllability and reliability via iterative denoising. This unified model integrates perception, structured reasoning, and low-level planning into an E2E driving framework. Evaluations on nuScenes and WOD-E2E datasets show that dVLM-AD achieves more consistent reasoning-action pairs compared to AR-based baselines. Despite using a modest backbone, it yields planning performance on par with existing VLM/VLA driving systems. Quantitatively, dVLM-AD improves behavior-trajectory consistency by 9% and success rates (RFS) by 6% in long-tail driving scenarios on the WOD-E2E benchmark. The results highlight discrete diffusion VLMs as a promising and controllable approach for scalable and reliable autonomous driving solutions. <div>
arXiv:2512.04459v1 Announce Type: new 
Abstract: The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniTS: Unified Time Series Generative Model for Remote Sensing</title>
<link>https://arxiv.org/abs/2512.04461</link>
<guid>https://arxiv.org/abs/2512.04461</guid>
<content:encoded><![CDATA[
<div> Keywords: Unified Time Series Generative Model, spatiotemporal modeling, satellite remote sensing, cloud removal, time series forecasting<br><br>Summary: This paper addresses the challenge of modeling complex Earth environment dynamics from satellite remote sensing data, focusing on tasks such as reconstructing continuous cloud-free image time series, detecting land cover changes, and forecasting surface evolution. Existing approaches typically rely on specialized models for each task without a unified framework. To overcome this, the authors propose UniTS, a Unified Time Series Generative Model that applies a single framework to multiple time series tasks including reconstruction, cloud removal, semantic change detection, and forecasting. UniTS is built on a flow matching generative paradigm that deterministically evolves data from noise to target guided by task-specific conditions, enabling unified spatiotemporal feature modeling across tasks. Its architecture features a diffusion transformer with spatiotemporal blocks enhanced by an Adaptive Condition Injector (ACor) to better incorporate multimodal inputs and a Spatiotemporal-aware Modulator (STM) to capture complex dependencies. To support evaluation, the authors present two new multimodal datasets, TS-S12 and TS-S12CR, targeting cloud removal and forecasting tasks. Extensive experiments demonstrate that UniTS outperforms current methods, especially under difficult conditions such as heavy cloud contamination, missing modalities, and phenological changes, showing strong generation and cognition capabilities for both low- and high-level time series tasks. <div>
arXiv:2512.04461v1 Announce Type: new 
Abstract: One of the primary objectives of satellite remote sensing is to capture the complex dynamics of the Earth environment, which encompasses tasks such as reconstructing continuous cloud-free time series images, detecting land cover changes, and forecasting future surface evolution. However, existing methods typically require specialized models tailored to different tasks, lacking unified modeling of spatiotemporal features across multiple time series tasks. In this paper, we propose a Unified Time Series Generative Model (UniTS), a general framework applicable to various time series tasks, including time series reconstruction, time series cloud removal, time series semantic change detection, and time series forecasting. Based on the flow matching generative paradigm, UniTS constructs a deterministic evolution path from noise to targets under the guidance of task-specific conditions, achieving unified modeling of spatiotemporal representations for multiple tasks. The UniTS architecture consists of a diffusion transformer with spatio-temporal blocks, where we design an Adaptive Condition Injector (ACor) to enhance the model's conditional perception of multimodal inputs, enabling high-quality controllable generation. Additionally, we design a Spatiotemporal-aware Modulator (STM) to improve the ability of spatio-temporal blocks to capture complex spatiotemporal dependencies. Furthermore, we construct two high-quality multimodal time series datasets, TS-S12 and TS-S12CR, filling the gap of benchmark datasets for time series cloud removal and forecasting tasks. Extensive experiments demonstrate that UniTS exhibits exceptional generative and cognitive capabilities in both low-level and high-level time series tasks. It significantly outperforms existing methods, particularly when facing challenges such as severe cloud contamination, modality absence, and forecasting phenological variations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeRA: Decoupled Representation Alignment for Video Tokenization</title>
<link>https://arxiv.org/abs/2512.04483</link>
<guid>https://arxiv.org/abs/2512.04483</guid>
<content:encoded><![CDATA[
<div> Keywords: DeRA, video tokenizer, spatial-temporal representation, Symmetric Alignment-Conflict Projection, autoregressive video generation<br><br>Summary: This paper introduces DeRA, a novel one-dimensional (1D) video tokenizer designed to improve both training efficiency and performance by decoupling spatial and temporal representation learning. DeRA maintains a compact 1D latent space while factorizing video encoding into two distinct streams: appearance and motion. These streams are aligned with pretrained vision foundation models to effectively capture spatial semantics and temporal dynamics separately. To overcome the gradient conflicts caused by the heterogeneous supervision of these two streams, the authors propose a Symmetric Alignment-Conflict Projection (SACP) module. The SACP module proactively reformulates gradients by suppressing components along conflicting directions, thus stabilizing and improving training. Extensive evaluations demonstrate that DeRA significantly outperforms LARP, the previous state-of-the-art video tokenizer, by 25% on the UCF-101 dataset based on relative Frechet Video Distance (rFVD). Furthermore, when applied to autoregressive video generation tasks, DeRA achieves new state-of-the-art results both in class-conditional video generation on UCF-101 and frame prediction on Kinetics-600 (K600). Overall, DeRA presents a significant advancement in efficient and effective video tokenization through its novel factorization approach and conflict-aware gradient adjustment mechanism. <div>
arXiv:2512.04483v1 Announce Type: new 
Abstract: This paper presents DeRA, a novel 1D video tokenizer that decouples the spatial-temporal representation learning in video tokenization to achieve better training efficiency and performance. Specifically, DeRA maintains a compact 1D latent space while factorizing video encoding into appearance and motion streams, which are aligned with pretrained vision foundation models to capture the spatial semantics and temporal dynamics in videos separately. To address the gradient conflicts introduced by the heterogeneous supervision, we further propose the Symmetric Alignment-Conflict Projection (SACP) module that proactively reformulates gradients by suppressing the components along conflicting directions. Extensive experiments demonstrate that DeRA outperforms LARP, the previous state-of-the-art video tokenizer by 25% on UCF-101 in terms of rFVD. Moreover, using DeRA for autoregressive video generation, we also achieve new state-of-the-art results on both UCF-101 class-conditional generation and K600 frame prediction.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Birds Look The Same: Identity-Preserving Generation For Birds</title>
<link>https://arxiv.org/abs/2512.04485</link>
<guid>https://arxiv.org/abs/2512.04485</guid>
<content:encoded><![CDATA[
<div> birds, identity-preserving generation, NABirds Look-Alikes, zero-shot models, fine-grained recognition<br><br>Summary:<br><br>Since the development of controllable image generation, models allowing zero-shot and identity-preserving manipulation, like Insert Anything and OmniControl, have enhanced user customization without requiring fine-tuning. However, these models struggle with non-rigid and fine-grained categories, which lack high-quality accessible data, especially multi-view images or videos, limiting evaluation and improvement. Birds represent a challenging but important domain due to their diversity, fine-grained identification cues, and varied poses. To address this gap, the authors introduce the NABirds Look-Alikes (NABLA) dataset, featuring 4,759 expert-curated image pairs alongside 1,073 pairs from multi-image iNaturalist observations and a small set of videos, forming a benchmark for identity-preserving bird image generation. Evaluations show that existing state-of-the-art baselines perform poorly in maintaining bird identity on this dataset. The study further demonstrates that training methods grouping images by species, age, and sex—which act as proxies for identity—result in significant performance improvements for both species seen during training and novel unseen species. This work highlights the importance of specialized datasets and training strategies to improve fine-grained, identity-aware image generation for complex, non-rigid categories like birds. <div>
arXiv:2512.04485v1 Announce Type: new 
Abstract: Since the advent of controllable image generation, increasingly rich modes of control have enabled greater customization and accessibility for everyday users. Zero-shot, identity-preserving models such as Insert Anything and OminiControl now support applications like virtual try-on without requiring additional fine-tuning. While these models may be fitting for humans and rigid everyday objects, they still have limitations for non-rigid or fine-grained categories. These domains often lack accessible, high-quality data -- especially videos or multi-view observations of the same subject -- making them difficult both to evaluate and to improve upon. Yet, such domains are essential for moving beyond content creation toward applications that demand accuracy and fine detail. Birds are an excellent domain for this task: they exhibit high diversity, require fine-grained cues for identification, and come in a wide variety of poses. We introduce the NABirds Look-Alikes (NABLA) dataset, consisting of 4,759 expert-curated image pairs. Together with 1,073 pairs collected from multi-image observations on iNaturalist and a small set of videos, this forms a benchmark for evaluating identity-preserving generation of birds. We show that state-of-the-art baselines fail to maintain identity on this dataset, and we demonstrate that training on images grouped by species, age, and sex -- used as a proxy for identity -- substantially improves performance on both seen and unseen species.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable Long-term Motion Generation with Extended Joint Targets</title>
<link>https://arxiv.org/abs/2512.04487</link>
<guid>https://arxiv.org/abs/2512.04487</guid>
<content:encoded><![CDATA[
<div> COMET, character motion, Transformer, real-time, style transfer<br><br>Summary:<br><br>1. The paper addresses the challenge of generating stable and controllable character motion in real-time, a critical issue in computer animation. 2. It introduces COMET, an autoregressive framework built on an efficient Transformer-based conditional Variational Autoencoder (VAE), which enables versatile and precise control of character joints for various animation tasks such as goal-reaching and in-betweening. 3. Unlike existing methods that struggle with fine-grained control or experience motion degradation over long sequences, COMET maintains high-quality motion synthesis during extended durations through a novel reference-guided feedback mechanism, which prevents error accumulation. 4. This feedback mechanism also functions as a plug-and-play stylization module, allowing real-time style transfer to be integrated seamlessly into the motion generation process. 5. Extensive evaluations demonstrate COMET’s superiority over current state-of-the-art methods, producing robust, high-quality motion at real-time speeds, making it suitable for demanding interactive applications in computer animation and character control. <div>
arXiv:2512.04487v1 Announce Type: new 
Abstract: Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shift-Window Meets Dual Attention: A Multi-Model Architecture for Specular Highlight Removal</title>
<link>https://arxiv.org/abs/2512.04496</link>
<guid>https://arxiv.org/abs/2512.04496</guid>
<content:encoded><![CDATA[
<div> Keywords: specular highlight removal, multi-model architecture, convolutional neural networks, attention mechanism, Omni-Directional Attention Integration Block<br><br>Summary:<br><br>1. The presence of specular highlights in practical environments adversely affects visual quality and task performance, motivating the need for effective removal methods.  
2. Existing methods either focus on local details using convolutional neural networks or on global context using transformer models, but single-type approaches face a challenge balancing fine-grained local features and long-range dependencies across varying highlight scales.  
3. The proposed Multi-Model Specular Highlight Removal (MM-SHR) architecture integrates convolutional operations in shallow layers for local detail extraction, while employing attention mechanisms in deeper layers to capture global features efficiently and accurately.  
4. To handle long-range dependencies without excessive computational overhead, MM-SHR introduces two key modules: the Omni-Directional Attention Integration Block (OAIBlock) and the Adaptive Region-Aware Hybrid-Domain Dual Attention Convolutional Network (HDDAConv), which utilize omni-directional pixel-shifting and window-dividing operations on raw features.  
5. Extensive evaluations across three benchmarks and six surface material types show that MM-SHR surpasses state-of-the-art methods in both accuracy and efficiency for specular highlight removal. The authors will release their implementation publicly on GitHub. <div>
arXiv:2512.04496v1 Announce Type: new 
Abstract: Inevitable specular highlights in practical environments severely impair the visual performance, thus degrading the task effectiveness and efficiency. Although there exist considerable methods that focus on local information from convolutional neural network models or global information from transformer models, the single-type model falls into a modeling dilemma between local fine-grained details and global long-range dependencies, thus deteriorating for specular highlights with different scales. Therefore, to accommodate specular highlights of all scales, we propose a multi-model architecture for specular highlight removal (MM-SHR) that effectively captures fine-grained features in highlight regions and models long-range dependencies between highlight and highlight-free areas. Specifically, we employ convolution operations to extract local details in the shallow layers of MM-SHR, and utilize the attention mechanism to capture global features in the deep layers, ensuring both operation efficiency and removal accuracy. To model long-range dependencies without compromising computational complexity, we utilize a coarse-to-fine manner and propose Omni-Directional Attention Integration Block(OAIBlock) and Adaptive Region-Aware Hybrid-Domain Dual Attention Convolutional Network(HDDAConv) , which leverage omni-directiona pixel-shifting and window-dividing operations at the raw features to achieve specular highlight removal. Extensive experimental results on three benchmark tasks and six types of surface materials demonstrate that MM-SHR outperforms state-of-the-art methods in both accuracy and efficiency for specular highlight removal. The implementation will be made publicly available at https://github.com/Htcicv/MM-SHR.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to Basics: Motion Representation Matters for Human Motion Generation Using Diffusion Model</title>
<link>https://arxiv.org/abs/2512.04499</link>
<guid>https://arxiv.org/abs/2512.04499</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, motion synthesis, motion representations, loss functions, training efficiency  

<br><br>Summary:  
This paper explores fundamental aspects of diffusion models in human motion synthesis, focusing on task-oriented applications such as action-to-motion, text-to-motion, and audio-to-motion. The study uses a proxy motion diffusion model (MDM) and introduces the v loss objective, defined as a weighted sum of motion data and noise, to better understand latent data distributions. First, the authors evaluate six common motion representations from existing literature, comparing their performance using quality and diversity metrics across diverse datasets, revealing significant differences in effectiveness. Second, the paper investigates various training configurations, analyzing their impact on training time to offer insights into accelerating the training process of motion diffusion models. Third, extensive evaluation on a large motion dataset provides robust empirical evidence supporting the earlier findings. The results highlight how choices related to motion representation and training setup critically influence model performance and efficiency. Ultimately, the paper provides a comprehensive controlled study that advances the foundational understanding of conditional motion diffusion models, potentially guiding future research and development in the field for enhanced generative motion synthesis systems. <div>
arXiv:2512.04499v1 Announce Type: new 
Abstract: Diffusion models have emerged as a widely utilized and successful methodology in human motion synthesis. Task-oriented diffusion models have significantly advanced action-to-motion, text-to-motion, and audio-to-motion applications. In this paper, we investigate fundamental questions regarding motion representations and loss functions in a controlled study, and we enumerate the impacts of various decisions in the workflow of the generative motion diffusion model. To answer these questions, we conduct empirical studies based on a proxy motion diffusion model (MDM). We apply v loss as the prediction objective on MDM (vMDM), where v is the weighted sum of motion data and noise. We aim to enhance the understanding of latent data distributions and provide a foundation for improving the state of conditional motion diffusion models. First, we evaluate the six common motion representations in the literature and compare their performance in terms of quality and diversity metrics. Second, we compare the training time under various configurations to shed light on how to speed up the training process of motion diffusion models. Finally, we also conduct evaluation analysis on a large motion dataset. The results of our experiments indicate clear performance differences across motion representations in diverse datasets. Our results also demonstrate the impacts of distinct configurations on model training and suggest the importance and effectiveness of these decisions on the outcomes of motion diffusion models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.04504</link>
<guid>https://arxiv.org/abs/2512.04504</guid>
<content:encoded><![CDATA[
<div> Keywords: image diffusion transformers, frequency analysis, positional embeddings, adaptive attention, ultra-high-resolution generation<br><br>Summary: UltraImage addresses key challenges in image diffusion transformers related to large-scale image generation, particularly content repetition and quality degradation. By performing frequency-wise analysis of positional embeddings, the authors identify that content repetition is caused by the periodicity of a dominant frequency that corresponds to the training image resolution. To mitigate this, they propose a recursive dominant frequency correction method that confines the frequency within a single period during extrapolation. Furthermore, the research reveals that degraded image quality results from diluted attention in the transformer network. To counteract this, UltraImage employs entropy-guided adaptive attention concentration which adjusts the attention focus dynamically—sharpening local attention for fine details while maintaining lower attention on global structures to preserve overall consistency. Experimental results on Qwen-Image and Flux datasets (around 4K resolution) demonstrate that UltraImage reduces repetition and enhances visual fidelity compared to existing methods. Impressively, UltraImage can extrapolate beyond training resolution (1328p) to generate images up to 6K by 6K without relying on low-resolution guidance, underscoring its strong extrapolation capability. The project and additional resources can be accessed via their webpage. <div>
arXiv:2512.04504v1 Announce Type: new 
Abstract: Recent image diffusion transformers achieve high-fidelity generation, but struggle to generate images beyond these scales, suffering from content repetition and quality degradation. In this work, we present UltraImage, a principled framework that addresses both issues. Through frequency-wise analysis of positional embeddings, we identify that repetition arises from the periodicity of the dominant frequency, whose period aligns with the training resolution. We introduce a recursive dominant frequency correction to constrain it within a single period after extrapolation. Furthermore, we find that quality degradation stems from diluted attention and thus propose entropy-guided adaptive attention concentration, which assigns higher focus factors to sharpen local attention for fine detail and lower ones to global attention patterns to preserve structural consistency. Experiments show that UltraImage consistently outperforms prior methods on Qwen-Image and Flux (around 4K) across three generation scenarios, reducing repetition and improving visual fidelity. Moreover, UltraImage can generate images up to 6K*6K without low-resolution guidance from a training resolution of 1328p, demonstrating its extreme extrapolation capability. Project page is available at \href{https://thu-ml.github.io/ultraimage.github.io/}{https://thu-ml.github.io/ultraimage.github.io/}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DuGI-MAE: Improving Infrared Mask Autoencoders via Dual-Domain Guidance</title>
<link>https://arxiv.org/abs/2512.04511</link>
<guid>https://arxiv.org/abs/2512.04511</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared imaging, foundation model, masked autoencoder, dual-domain guidance, large-scale dataset

<br><br>Summary: Infrared imaging is essential for effective vision in low-light and adverse weather conditions but existing foundation models like Masked Autoencoder (MAE), trained primarily on visible spectrum data, do not perform optimally for infrared images. To address this issue, the authors developed an infrared-specific foundation model called InfMAE, pretrained on large infrared datasets. However, InfMAE has limitations such as missing informative tokens, weak modeling of global token relationships, and lack of noise handling for non-uniform background noise found in infrared imagery. To overcome these challenges, the paper proposes DuGI-MAE, a Dual-domain Guided Infrared MAE model. DuGI-MAE employs a deterministic masking strategy based on token entropy that retains only high-entropy tokens to improve informativeness in reconstruction. In addition, the model incorporates a Dual-Domain Guidance (DDG) module to simultaneously capture global token associations and adaptively filter out non-uniform background noise. To support extensive pretraining, the authors introduce Inf-590K, a comprehensive infrared image dataset with diverse scenes, target types, and spatial resolutions. The model pretrained on Inf-590K demonstrates strong generalization across various downstream tasks including infrared object detection, semantic segmentation, and small target detection. Experimental results show that DuGI-MAE outperforms both supervised and self-supervised baselines, confirming its effectiveness. The code is provided as supplementary material. <div>
arXiv:2512.04511v1 Announce Type: new 
Abstract: Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoLCD: Egocentric Video Generation with Long Context Diffusion</title>
<link>https://arxiv.org/abs/2512.04515</link>
<guid>https://arxiv.org/abs/2512.04515</guid>
<content:encoded><![CDATA[
<div> Keywords: egocentric video generation, long-term memory, content drift, memory management, temporal consistency<br><br>Summary:<br><br>1. Generating long and coherent egocentric videos is challenging due to the complexity of hand-object interactions and procedural tasks that require reliable long-term memory retention.<br>2. Existing autoregressive video generation models tend to suffer from content drift, where the identity of objects and overall scene semantics degrade as the video progresses.<br>3. The proposed framework, EgoLCD, addresses these limitations by framing long video synthesis as a problem of efficient and stable memory management, combining both long-term and short-term memory components.<br>4. EgoLCD integrates a Long-Term Sparse Key-Value (KV) Cache to maintain stable global context and an attention-based short-term memory module extended via Low-Rank Adaptation (LoRA) for local content adaptation.<br>5. A novel Memory Regulation Loss is introduced to enforce consistent memory usage throughout the sequence, while Structured Narrative Prompting provides explicit temporal guidance to improve coherence.<br>6. Experimental results on the EgoVid-5M benchmark show that EgoLCD outperforms existing methods in perceptual quality and temporal consistency, effectively reducing generative forgetting.<br>7. The work represents an important step toward scalable world models for embodied AI, supported by publicly available code and a project website for further research and development. <div>
arXiv:2512.04515v1 Announce Type: new 
Abstract: Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoSSM: Autoregressive Long Video Generation with Hybrid State-Space Memory</title>
<link>https://arxiv.org/abs/2512.04519</link>
<guid>https://arxiv.org/abs/2512.04519</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoregressive diffusion, long-video generation, state-space model, temporal consistency, interactive control<br><br>Summary:<br><br>This paper addresses the challenge of generating coherent long videos through autoregressive (AR) diffusion, which produces frames sequentially but faces issues like accumulated errors, motion drift, and repetitive content over minute-scale durations. To overcome these problems, the authors propose VideoSSM, a novel Long Video Model that combines autoregressive diffusion with a hybrid state-space memory system. The state-space model (SSM) acts as a global memory that evolves with the scene dynamics throughout the entire video sequence, while a localized context window captures finer motion details and local nuances. This hybrid memory design helps maintain global consistency in the video content, preventing frozen or repetitive patterns typically seen in long-video generation. Additionally, VideoSSM supports prompt-adaptive interaction, allowing dynamic user control during generation, and efficiently scales linearly with the length of the video sequence. Experimental results on both short- and long-range video benchmarks show that VideoSSM achieves state-of-the-art temporal consistency and motion stability for autoregressive video generation, especially over minute-long horizons. Overall, the work establishes a scalable framework that integrates memory-aware mechanisms for producing diverse, interactive, and coherent long-duration video content. <div>
arXiv:2512.04519v1 Announce Type: new 
Abstract: Autoregressive (AR) diffusion enables streaming, interactive long-video generation by producing frames causally, yet maintaining coherence over minute-scale horizons remains challenging due to accumulated errors, motion drift, and content repetition. We approach this problem from a memory perspective, treating video synthesis as a recurrent dynamical process that requires coordinated short- and long-term context. We propose VideoSSM, a Long Video Model that unifies AR diffusion with a hybrid state-space memory. The state-space model (SSM) serves as an evolving global memory of scene dynamics across the entire sequence, while a context window provides local memory for motion cues and fine details. This hybrid design preserves global consistency without frozen, repetitive patterns, supports prompt-adaptive interaction, and scales in linear time with sequence length. Experiments on short- and long-range benchmarks demonstrate state-of-the-art temporal consistency and motion stability among autoregressive video generator especially at minute-scale horizons, enabling content diversity and interactive prompt-based control, thereby establishing a scalable, memory-aware framework for long video generation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.04520</link>
<guid>https://arxiv.org/abs/2512.04520</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot segmentation, test-time adaptation, medical image segmentation, foundation models, boundary-aware attention<br><br>Summary: This paper addresses the limitations of conventional tuning methods in medical image segmentation, which struggle due to scarce annotated data and high computational costs. It highlights the promise of zero-shot segmentation using foundation models like SAM (Segment Anything Model), while noting SAM’s reduced effectiveness on medical datasets caused by domain shifts. To tackle these issues, the authors propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that enhances SAM's zero-shot segmentation performance without requiring any source-domain training data. The framework incorporates two key innovations: (1) encoder-level Gaussian prompt injection, which embeds Gaussian-based prompts into the image encoder for improved initial representation learning, and (2) cross-layer boundary-aware attention alignment, which leverages hierarchical interactions within the ViT backbone to align deep semantic features with shallow boundary cues. Experimental results across four public medical datasets (ISIC, Kvasir, BUSI, and REFUGE) demonstrate an average DICE score improvement of 12.4% over SAM’s zero-shot baseline. The proposed method consistently outperforms state-of-the-art models in medical image segmentation, significantly boosting SAM’s generalization capability without additional training on source domain data. The code is publicly available for further research and implementation. <div>
arXiv:2512.04520v1 Announce Type: new 
Abstract: Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM's zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WiFi-based Cross-Domain Gesture Recognition Using Attention Mechanism</title>
<link>https://arxiv.org/abs/2512.04521</link>
<guid>https://arxiv.org/abs/2512.04521</guid>
<content:encoded><![CDATA[
<div> Keywords: WiFi sensing, gesture recognition, cross-domain, Doppler spectrum, attention mechanism<br><br>Summary:<br><br>This paper addresses the challenge of cross-domain gesture recognition using WiFi signals, which are advantageous due to their widespread availability, low cost, and robustness against environmental factors. The authors propose a novel approach that extracts Doppler spectra from channel state information (CSI) collected by multiple receivers. These spectra are concatenated along the time axis to form fused multi-angle images, enriching the input features for gesture recognition. A new neural network architecture is introduced, inspired by the convolutional block attention module (CBAM), which combines a multi-semantic spatial attention mechanism with a self-attention-based channel mechanism. This design enables the network to generate attention maps that highlight important spatiotemporal gesture features, crucial for capturing domain-independent characteristics. The well-known ResNet18 model is integrated as the backbone to extract deeper-level features effectively. The authors validate their network on the public Widar3 dataset, demonstrating that their approach achieves near-perfect accuracy in the trained domain (99.72%) and maintains high accuracy when tested across different domains (97.61%). This performance significantly surpasses existing state-of-the-art methods, indicating that their multi-angle Doppler fusion and attention-based network effectively enhance cross-domain recognition capabilities in WiFi-based gesture sensing. <div>
arXiv:2512.04521v1 Announce Type: new 
Abstract: While fulfilling communication tasks, wireless signals can also be used to sense the environment. Among various types of sensing media, WiFi signals offer advantages such as widespread availability, low hardware cost, and strong robustness to environmental conditions like light, temperature, and humidity. By analyzing Wi-Fi signals in the environment, it is possible to capture dynamic changes of the human body and accomplish sensing applications such as gesture recognition. Although many existing gesture sensing solutions perform well in-domain but lack cross-domain capabilities (i.e., recognition performance in untrained environments). To address this, we extract Doppler spectra from the channel state information (CSI) received by all receivers and concatenate each Doppler spectrum along the same time axis to generate fused images with multi-angle information as input features. Furthermore, inspired by the convolutional block attention module (CBAM), we propose a gesture recognition network that integrates a multi-semantic spatial attention mechanism with a self-attention-based channel mechanism. This network constructs attention maps to quantify the spatiotemporal features of gestures in images, enabling the extraction of key domain-independent features. Additionally, ResNet18 is employed as the backbone network to further capture deep-level features. To validate the network performance, we evaluate the proposed network on the public Widar3 dataset, and the results show that it not only maintains high in-domain accuracy of 99.72%, but also achieves high performance in cross-domain recognition of 97.61%, significantly outperforming existing best solutions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identity Clue Refinement and Enhancement for Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2512.04522</link>
<guid>https://arxiv.org/abs/2512.04522</guid>
<content:encoded><![CDATA[
<div> Visible-Infrared Person Re-Identification, cross-modal matching, modality-specific attributes, semantic distillation, identity-aware features  

<br><br>Summary:  
This paper addresses the challenges of Visible-Infrared Person Re-Identification (VI-ReID), a cross-modal matching task complicated by significant differences between visual and infrared data modalities. Unlike existing approaches that primarily learn modality-invariant features by focusing only on shared discriminative semantics, this work emphasizes the importance of modality-specific identity-aware information for improved feature discrimination. The authors introduce the Identity Clue Refinement and Enhancement (ICRE) network, which incorporates several novel components. First, the Multi-Perception Feature Refinement (MPFR) module aggregates shallow features from shared network branches to better capture overlooked modality-specific attributes. Next, the Semantic Distillation Cascade Enhancement (SDCE) module distills identity-relevant knowledge from these shallow aggregated features to enhance the learning of modality-invariant features. Additionally, an Identity Clues Guided (ICG) Loss is proposed to mitigate modality discrepancies in the enhanced features and encourage a more diverse representation space. Extensive experiments validate the effectiveness of the ICRE framework, demonstrating its superior performance over state-of-the-art methods across multiple publicly available datasets. This work highlights the value of integrating modality-specific identity clues alongside invariant features for robust VI-ReID. <div>
arXiv:2512.04522v1 Announce Type: new 
Abstract: Visible-Infrared Person Re-Identification (VI-ReID) is a challenging cross-modal matching task due to significant modality discrepancies. While current methods mainly focus on learning modality-invariant features through unified embedding spaces, they often focus solely on the common discriminative semantics across modalities while disregarding the critical role of modality-specific identity-aware knowledge in discriminative feature learning. To bridge this gap, we propose a novel Identity Clue Refinement and Enhancement (ICRE) network to mine and utilize the implicit discriminative knowledge inherent in modality-specific attributes. Initially, we design a Multi-Perception Feature Refinement (MPFR) module that aggregates shallow features from shared branches, aiming to capture modality-specific attributes that are easily overlooked. Then, we propose a Semantic Distillation Cascade Enhancement (SDCE) module, which distills identity-aware knowledge from the aggregated shallow features and guide the learning of modality-invariant features. Finally, an Identity Clues Guided (ICG) Loss is proposed to alleviate the modality discrepancies within the enhanced features and promote the learning of a diverse representation space. Extensive experiments across multiple public datasets clearly show that our proposed ICRE outperforms existing SOTA methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto3R: Automated 3D Reconstruction and Scanning via Data-driven Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2512.04528</link>
<guid>https://arxiv.org/abs/2512.04528</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D scanning, uncertainty quantification, autonomous reconstruction, non-lambertian materials, robotic digitization  

<br><br>Summary:  
This article presents Auto3R, a novel data-driven uncertainty quantification model designed to automate the 3D scanning and reconstruction process for both scenes and objects. The motivation stems from the traditionally labor-intensive nature of high-quality 3D scanning, which requires human planning, and the emerging need for fully autonomous systems using drones and robots. Auto3R specifically addresses challenges posed by objects with complex surface properties such as non-lambertian and specular materials, which are difficult to scan accurately. The model operates in an iterative loop, predicting the uncertainty distribution of potential scanning viewpoints without prior knowledge of the ground truth geometry or appearance, enabling efficient and precise viewpoint selection. Extensive experiments demonstrate that Auto3R significantly outperforms existing state-of-the-art methods in accuracy and efficiency. Moreover, the approach is validated in practical deployment by integrating Auto3R onto a robotic arm equipped with a camera, successfully digitizing real-world objects to create photorealistic digital assets ready for use. The contribution facilitates fully automated, high-quality 3D digitization, advancing the capabilities of embodied systems in autonomous perception and reconstruction tasks. Additional resources and code are available on the project’s homepage for further exploration and application. <div>
arXiv:2512.04528v1 Announce Type: new 
Abstract: Traditional high-quality 3D scanning and reconstruction typically relies on human labor to plan the scanning procedure. With the rapid development of embodied systems such as drones and robots, there is a growing demand of performing accurate 3D scanning and reconstruction in an fully automated manner. We introduce Auto3R, a data-driven uncertainty quantification model that is designed to automate the 3D scanning and reconstruction of scenes and objects, including objects with non-lambertian and specular materials. Specifically, in a process of iterative 3D reconstruction and scanning, Auto3R can make efficient and accurate prediction of uncertainty distribution over potential scanning viewpoints, without knowing the ground truth geometry and appearance. Through extensive experiments, Auto3R achieves superior performance that outperforms the state-of-the-art methods by a large margin. We also deploy Auto3R on a robot arm equipped with a camera and demonstrate that Auto3R can be used to effectively digitize real-world 3D objects and delivers ready-to-use and photorealistic digital assets. Our homepage: https://tomatoma00.github.io/auto3r.github.io .
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement</title>
<link>https://arxiv.org/abs/2512.04532</link>
<guid>https://arxiv.org/abs/2512.04532</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Language Models, physical dynamics, Neural ODE, motion modeling, self-supervised learning<br><br>Summary:<br><br>Video Large Language Models (Video LLMs) excel at video-language tasks but struggle with deep physical dynamics understanding due to their reliance on appearance-based matching. To overcome this, the authors propose PhyVLLM, a framework designed to explicitly incorporate physical motion into Video LLMs. PhyVLLM addresses three main challenges: disentangling motion signals from appearance variations, modeling continuous-time physical dynamics, and avoiding the need for costly physical annotations. The approach utilizes a dual-branch encoder to separate visual appearance from object motion effectively. A Neural Ordinary Differential Equation (Neural ODE) module is integrated to model the temporal evolution of physical dynamics, producing differentiable motion-aware representations. These representations are then projected into the token space of a pretrained Large Language Model, allowing physics reasoning without sacrificing general multimodal capabilities. Importantly, PhyVLLM uses self-supervised learning to capture continuous motion evolution, thereby eliminating the requirement for explicit physical labels. Experimental results show that PhyVLLM significantly outperforms state-of-the-art Video LLMs in physical reasoning and overall video understanding, validating the benefits of embedding explicit physical modeling into video-language frameworks. <div>
arXiv:2512.04532v1 Announce Type: new 
Abstract: Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refa\c{c}ade: Editing Object with Given Reference Texture</title>
<link>https://arxiv.org/abs/2512.04534</link>
<guid>https://arxiv.org/abs/2512.04534</guid>
<content:encoded><![CDATA[

arXiv:2512.04534v1 Announce Type: new 
Abstract: Recent advances in diffusion models have brought remarkable progress in image and video editing, yet some tasks remain underexplored. In this paper, we introduce a new task, Object Retexture, which transfers local textures from a reference object to a target object in images or videos. To perform this task, a straightforward solution is to use ControlNet conditioned on the source structure and the reference texture. However, this approach suffers from limited controllability for two reasons: conditioning on the raw reference image introduces unwanted structural information, and it fails to disentangle the visual texture and structure information of the source. To address this problem, we propose Refa\c{c}ade, a method that consists of two key designs to achieve precise and controllable texture transfer in both images and videos. First, we employ a texture remover trained on paired textured/untextured 3D mesh renderings to remove appearance information while preserving the geometry and motion of source videos. Second, we disrupt the reference global layout using a jigsaw permutation, encouraging the model to focus on local texture statistics rather than the global layout of the object. Extensive experiments demonstrate superior visual quality, precise editing, and controllability, outperforming strong baselines in both quantitative and human evaluations. Code is available at https://github.com/fishZe233/Refacade.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model</title>
<link>https://arxiv.org/abs/2512.04536</link>
<guid>https://arxiv.org/abs/2512.04536</guid>
<content:encoded><![CDATA[

arXiv:2512.04536v1 Announce Type: new 
Abstract: Alcohol consumption is a significant public health concern and a major cause of accidents and fatalities worldwide. This study introduces a novel video-based facial sequence analysis approach dedicated to the detection of alcohol intoxication. The method integrates facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features extracted using a 3D ResNet. These features are dynamically fused with adaptive prioritization to enhance classification performance. Additionally, we introduce a curated dataset comprising 3,542 video segments derived from 202 individuals to support training and evaluation. Our model is compared against two baselines: a custom 3D-CNN and a VGGFace+LSTM architecture. Experimental results show that our approach achieves 95.82% accuracy, 0.977 precision, and 0.97 recall, outperforming prior methods. The findings demonstrate the model's potential for practical deployment in public safety systems for non-invasive, reliable alcohol intoxication detection.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale</title>
<link>https://arxiv.org/abs/2512.04537</link>
<guid>https://arxiv.org/abs/2512.04537</guid>
<content:encoded><![CDATA[

arXiv:2512.04537v1 Announce Type: new 
Abstract: The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to "robotize" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly "overlay" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million "robotized" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management</title>
<link>https://arxiv.org/abs/2512.04540</link>
<guid>https://arxiv.org/abs/2512.04540</guid>
<content:encoded><![CDATA[

arXiv:2512.04540v1 Announce Type: new 
Abstract: Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understanding as a sequential generation task via adaptive memory management. Specifically, VideoMem dynamically updates a global memory buffer, which adaptively retains critical information while discarding redundant content across the video timeline. To efficiently train VLMs for such long-term tasks, VideoMem integrates the Progressive Grouped Relative Policy Optimization (PRPO) algorithm, equipped with two core modules: Progressive State Propagation (PSP) adaptively retains valid current states, propagates them to the next rollout step, and gradually narrows the model exploration space. Temporal Cascading Reward (TCR) further alleviates reward sparsity, improving sample utilization and accelerating convergence. Extensive experiments demonstrate that VideoMem significantly outperforms existing open-source models across diverse benchmarks for ultra-long video understanding tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization</title>
<link>https://arxiv.org/abs/2512.04542</link>
<guid>https://arxiv.org/abs/2512.04542</guid>
<content:encoded><![CDATA[

arXiv:2512.04542v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency. \replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment. Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\&amp;T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360. Notably, superior Chamfer Distance (0.64) on DTU and F1 score (0.44) on T\&amp;T are obtained, alongside the best SSIM (0.855) and LPIPS (0.136) among baselines on Mip-NeRF 360, validating the framework's ability to enhance surface reconstruction accuracy without compromising photometric fidelity.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfeit Answers: Adversarial Forgery against OCR-Free Document Visual Question Answering</title>
<link>https://arxiv.org/abs/2512.04554</link>
<guid>https://arxiv.org/abs/2512.04554</guid>
<content:encoded><![CDATA[

arXiv:2512.04554v1 Announce Type: new 
Abstract: Document Visual Question Answering (DocVQA) enables end-to-end reasoning grounded on information present in a document input. While recent models have shown impressive capabilities, they remain vulnerable to adversarial attacks. In this work, we introduce a novel attack scenario that aims to forge document content in a visually imperceptible yet semantically targeted manner, allowing an adversary to induce specific or generally incorrect answers from a DocVQA model. We develop specialized attack algorithms that can produce adversarially forged documents tailored to different attackers' goals, ranging from targeted misinformation to systematic model failure scenarios. We demonstrate the effectiveness of our approach against two end-to-end state-of-the-art models: Pix2Struct, a vision-language transformer that jointly processes image and text through sequence-to-sequence modeling, and Donut, a transformer-based model that directly extracts text and answers questions from document images. Our findings highlight critical vulnerabilities in current DocVQA systems and call for the development of more robust defenses.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence</title>
<link>https://arxiv.org/abs/2512.04563</link>
<guid>https://arxiv.org/abs/2512.04563</guid>
<content:encoded><![CDATA[

arXiv:2512.04563v1 Announce Type: new 
Abstract: Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset creation for supervised deep learning-based analysis of microscopic images - review of important considerations and recommendations</title>
<link>https://arxiv.org/abs/2512.04564</link>
<guid>https://arxiv.org/abs/2512.04564</guid>
<content:encoded><![CDATA[

arXiv:2512.04564v1 Announce Type: new 
Abstract: Supervised deep learning (DL) receives great interest for automated analysis of microscopic images with an increasing body of literature supporting its potential. The development and validation of those DL models relies heavily on the availability of high-quality, large-scale datasets. However, creating such datasets is a complex and resource-intensive process, often hindered by challenges such as time constraints, domain variability, and risks of bias in image collection and label creation. This review provides a comprehensive guide to the critical steps in dataset creation, including: 1) image acquisition, 2) selection of annotation software, and 3) annotation creation. In addition to ensuring a sufficiently large number of images, it is crucial to address sources of image variability (domain shifts) - such as those related to slide preparation and digitization - that could lead to algorithmic errors if not adequately represented in the training data. Key quality criteria for annotations are the three "C"s: correctness, completeness, and consistency. This review explores methods to enhance annotation quality through the use of advanced techniques that mitigate the limitations of single annotators. To support dataset creators, a standard operating procedure (SOP) is provided as supplemental material, outlining best practices for dataset development. Furthermore, the article underscores the importance of open datasets in driving innovation and enhancing reproducibility of DL research. By addressing the challenges and offering practical recommendations, this review aims to advance the creation of and availability to high-quality, large-scale datasets, ultimately contributing to the development of generalizable and robust DL models for pathology applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt2Craft: Generating Functional Craft Assemblies with LLMs</title>
<link>https://arxiv.org/abs/2512.04568</link>
<guid>https://arxiv.org/abs/2512.04568</guid>
<content:encoded><![CDATA[

arXiv:2512.04568v1 Announce Type: new 
Abstract: Inspired by traditional handmade crafts, where a person improvises assemblies based on the available objects, we formally introduce the Craft Assembly Task. It is a robotic assembly task that involves building an accurate representation of a given target object using the available objects, which do not directly correspond to its parts. In this work, we focus on selecting the subset of available objects for the final craft, when the given input is an RGB image of the target in the wild. We use a mask segmentation neural network to identify visible parts, followed by retrieving labeled template meshes. These meshes undergo pose optimization to determine the most suitable template. Then, we propose to simplify the parts of the transformed template mesh to primitive shapes like cuboids or cylinders. Finally, we design a search algorithm to find correspondences in the scene based on local and global proportions. We develop baselines for comparison that consider all possible combinations, and choose the highest scoring combination for common metrics used in foreground maps and mask accuracy. Our approach achieves comparable results to the baselines for two different scenes, and we show qualitative results for an implementation in a real-world scenario.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARDis: Time Attenuated Representation Disentanglement for Incomplete Multi-Modal Tumor Segmentation and Classification</title>
<link>https://arxiv.org/abs/2512.04576</link>
<guid>https://arxiv.org/abs/2512.04576</guid>
<content:encoded><![CDATA[

arXiv:2512.04576v1 Announce Type: new 
Abstract: Tumor segmentation and diagnosis in contrast-enhanced Computed Tomography (CT) rely heavily on the physiological dynamics of contrast agents. However, obtaining a complete multi-phase series is often clinically unfeasible due to radiation concerns or scanning limitations, leading to the "missing modality" problem. Existing deep learning approaches typically treat missing phases as absent independent channels, ignoring the inherent temporal continuity of hemodynamics. In this work, we propose Time Attenuated Representation Disentanglement (TARDis), a novel physics-aware framework that redefines missing modalities as missing sample points on a continuous Time-Attenuation Curve. TARDis explicitly disentangles the latent feature space into a time-invariant static component (anatomy) and a time-dependent dynamic component (perfusion). We achieve this via a dual-path architecture: a quantization-based path using a learnable embedding dictionary to extract consistent anatomical structures, and a probabilistic path using a Conditional Variational Autoencoder to model dynamic enhancement conditioned on the estimated scan time. This design allows the network to hallucinate missing hemodynamic features by sampling from the learned latent distribution. Extensive experiments on a large-scale private abdominal CT dataset (2,282 cases) and two public datasets demonstrate that TARDis significantly outperforms state-of-the-art incomplete modality frameworks. Notably, our method maintains robust diagnostic performance even in extreme data-sparsity scenarios, highlighting its potential for reducing radiation exposure while maintaining diagnostic precision.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.04581</link>
<guid>https://arxiv.org/abs/2512.04581</guid>
<content:encoded><![CDATA[

arXiv:2512.04581v1 Announce Type: new 
Abstract: Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM3-I: Segment Anything with Instructions</title>
<link>https://arxiv.org/abs/2512.04585</link>
<guid>https://arxiv.org/abs/2512.04585</guid>
<content:encoded><![CDATA[

arXiv:2512.04585v1 Announce Type: new 
Abstract: Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering</title>
<link>https://arxiv.org/abs/2512.04597</link>
<guid>https://arxiv.org/abs/2512.04597</guid>
<content:encoded><![CDATA[

arXiv:2512.04597v1 Announce Type: new 
Abstract: Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot</title>
<link>https://arxiv.org/abs/2512.04599</link>
<guid>https://arxiv.org/abs/2512.04599</guid>
<content:encoded><![CDATA[

arXiv:2512.04599v1 Announce Type: new 
Abstract: Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence</title>
<link>https://arxiv.org/abs/2512.04619</link>
<guid>https://arxiv.org/abs/2512.04619</guid>
<content:encoded><![CDATA[

arXiv:2512.04619v1 Announce Type: new 
Abstract: In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding</title>
<link>https://arxiv.org/abs/2512.04643</link>
<guid>https://arxiv.org/abs/2512.04643</guid>
<content:encoded><![CDATA[

arXiv:2512.04643v1 Announce Type: new 
Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I2I-Bench: A Comprehensive Benchmark Suite for Image-to-Image Editing Models</title>
<link>https://arxiv.org/abs/2512.04660</link>
<guid>https://arxiv.org/abs/2512.04660</guid>
<content:encoded><![CDATA[

arXiv:2512.04660v1 Announce Type: new 
Abstract: Image editing models are advancing rapidly, yet comprehensive evaluation remains a significant challenge. Existing image editing benchmarks generally suffer from limited task scopes, insufficient evaluation dimensions, and heavy reliance on manual annotations, which significantly constrain their scalability and practical applicability. To address this, we propose \textbf{I2I-Bench}, a comprehensive benchmark for image-to-image editing models, which features (i) diverse tasks, encompassing 10 task categories across both single-image and multi-image editing tasks, (ii) comprehensive evaluation dimensions, including 30 decoupled and fine-grained evaluation dimensions with automated hybrid evaluation methods that integrate specialized tools and large multimodal models (LMMs), and (iii) rigorous alignment validation, justifying the consistency between our benchmark evaluations and human preferences. Using I2I-Bench, we benchmark numerous mainstream image editing models, investigating the gaps and trade-offs between editing models across various dimensions. We will open-source all components of I2I-Bench to facilitate future research.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
<link>https://arxiv.org/abs/2512.04677</link>
<guid>https://arxiv.org/abs/2512.04677</guid>
<content:encoded><![CDATA[

arXiv:2512.04677v1 Announce Type: new 
Abstract: Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation</title>
<link>https://arxiv.org/abs/2512.04678</link>
<guid>https://arxiv.org/abs/2512.04678</guid>
<content:encoded><![CDATA[

arXiv:2512.04678v1 Announce Type: new 
Abstract: Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Cross-View Point Correspondence in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.04686</link>
<guid>https://arxiv.org/abs/2512.04686</guid>
<content:encoded><![CDATA[

arXiv:2512.04686v1 Announce Type: new 
Abstract: Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of "perceive", "reason", and "correspond". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.04699</link>
<guid>https://arxiv.org/abs/2512.04699</guid>
<content:encoded><![CDATA[

arXiv:2512.04699v1 Announce Type: new 
Abstract: Arbitrary-scale super-resolution (ASSR) overcomes the limitation of traditional super-resolution (SR) methods that operate only at fixed scales (e.g., 4x), enabling a single model to handle arbitrary magnification. Most existing ASSR approaches rely on implicit neural representation (INR), but its regression-driven feature extraction and aggregation intrinsically limit the ability to synthesize fine details, leading to low realism. Recent diffusion-based realistic image super-resolution (Real-ISR) models leverage powerful pre-trained diffusion priors and show impressive results at the 4x setting. We observe that they can also achieve ASSR because the diffusion prior implicitly adapts to scale by encouraging high-realism generation. However, without explicit scale control, the diffusion process cannot be properly adjusted for different magnification levels, resulting in excessive hallucination or blurry outputs, especially under ultra-high scales. To address these issues, we propose OmniScaleSR, a diffusion-based realistic arbitrary-scale SR framework designed to achieve both high fidelity and high realism. We introduce explicit, diffusion-native scale control mechanisms that work synergistically with implicit scale adaptation, enabling scale-aware and content-aware modulation of the diffusion process. In addition, we incorporate multi-domain fidelity enhancement designs to further improve reconstruction accuracy. Extensive experiments on bicubic degradation benchmarks and real-world datasets show that OmniScaleSR surpasses state-of-the-art methods in both fidelity and perceptual realism, with particularly strong performance at large magnification factors. Code will be released at https://github.com/chaixinning/OmniScaleSR.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild</title>
<link>https://arxiv.org/abs/2512.04728</link>
<guid>https://arxiv.org/abs/2512.04728</guid>
<content:encoded><![CDATA[

arXiv:2512.04728v1 Announce Type: new 
Abstract: Generative psychological analysis of in-the-wild conversations faces two fundamental challenges: (1) existing Vision-Language Models (VLMs) fail to resolve Articulatory-Affective Ambiguity, where visual patterns of speech mimic emotional expressions; and (2) progress is stifled by a lack of verifiable evaluation metrics capable of assessing visual grounding and reasoning depth. We propose a complete ecosystem to address these twin challenges. First, we introduce Multilevel Insight Network for Disentanglement(MIND), a novel hierarchical visual encoder that introduces a Status Judgment module to algorithmically suppress ambiguous lip features based on their temporal feature variance, achieving explicit visual disentanglement. Second, we construct ConvoInsight-DB, a new large-scale dataset with expert annotations for micro-expressions and deep psychological inference. Third, Third, we designed the Mental Reasoning Insight Rating Metric (PRISM), an automated dimensional framework that uses expert-guided LLM to measure the multidimensional performance of large mental vision models. On our PRISM benchmark, MIND significantly outperforms all baselines, achieving a +86.95% gain in micro-expression detection over prior SOTA. Ablation studies confirm that our Status Judgment disentanglement module is the most critical component for this performance leap. Our code has been opened.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.04733</link>
<guid>https://arxiv.org/abs/2512.04733</guid>
<content:encoded><![CDATA[

arXiv:2512.04733v1 Announce Type: new 
Abstract: End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MT-Depth: Multi-task Instance feature analysis for the Depth Completion</title>
<link>https://arxiv.org/abs/2512.04734</link>
<guid>https://arxiv.org/abs/2512.04734</guid>
<content:encoded><![CDATA[

arXiv:2512.04734v1 Announce Type: new 
Abstract: Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order Matters: 3D Shape Generation from Sequential VR Sketches</title>
<link>https://arxiv.org/abs/2512.04761</link>
<guid>https://arxiv.org/abs/2512.04761</guid>
<content:encoded><![CDATA[

arXiv:2512.04761v1 Announce Type: new 
Abstract: VR sketching lets users explore and iterate on ideas directly in 3D, offering a faster and more intuitive alternative to conventional CAD tools. However, existing sketch-to-shape models ignore the temporal ordering of strokes, discarding crucial cues about structure and design intent. We introduce VRSketch2Shape, the first framework and multi-category dataset for generating 3D shapes from sequential VR sketches. Our contributions are threefold: (i) an automated pipeline that generates sequential VR sketches from arbitrary shapes, (ii) a dataset of over 20k synthetic and 900 hand-drawn sketch-shape pairs across four categories, and (iii) an order-aware sketch encoder coupled with a diffusion-based 3D generator. Our approach yields higher geometric fidelity than prior work, generalizes effectively from synthetic to real sketches with minimal supervision, and performs well even on partial sketches. All data and models will be released open-source at https://chenyizi086.github.io/VRSketch2Shape_website.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling</title>
<link>https://arxiv.org/abs/2512.04784</link>
<guid>https://arxiv.org/abs/2512.04784</guid>
<content:encoded><![CDATA[

arXiv:2512.04784v1 Announce Type: new 
Abstract: Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaFiTe: A Generative Latent Field for 3D Native Texturing</title>
<link>https://arxiv.org/abs/2512.04786</link>
<guid>https://arxiv.org/abs/2512.04786</guid>
<content:encoded><![CDATA[

arXiv:2512.04786v1 Announce Type: new 
Abstract: Generating high-fidelity, seamless textures directly on 3D surfaces, what we term 3D-native texturing, remains a fundamental open challenge, with the potential to overcome long-standing limitations of UV-based and multi-view projection methods. However, existing native approaches are constrained by the absence of a powerful and versatile latent representation, which severely limits the fidelity and generality of their generated textures. We identify this representation gap as the principal barrier to further progress. We introduce LaFiTe, a framework that addresses this challenge by learning to generate textures as a 3D generative sparse latent color field. At its core, LaFiTe employs a variational autoencoder (VAE) to encode complex surface appearance into a sparse, structured latent space, which is subsequently decoded into a continuous color field. This representation achieves unprecedented fidelity, exceeding state-of-the-art methods by >10 dB PSNR in reconstruction, by effectively disentangling texture appearance from mesh topology and UV parameterization. Building upon this strong representation, a conditional rectified-flow model synthesizes high-quality, coherent textures across diverse styles and geometries. Extensive experiments demonstrate that LaFiTe not only sets a new benchmark for 3D-native texturing but also enables flexible downstream applications such as material synthesis and texture super-resolution, paving the way for the next generation of 3D content creation workflows.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</title>
<link>https://arxiv.org/abs/2512.04810</link>
<guid>https://arxiv.org/abs/2512.04810</guid>
<content:encoded><![CDATA[

arXiv:2512.04810v1 Announce Type: new 
Abstract: We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS</title>
<link>https://arxiv.org/abs/2512.04815</link>
<guid>https://arxiv.org/abs/2512.04815</guid>
<content:encoded><![CDATA[

arXiv:2512.04815v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.04821</link>
<guid>https://arxiv.org/abs/2512.04821</guid>
<content:encoded><![CDATA[

arXiv:2512.04821v1 Announce Type: new 
Abstract: Generative models have achieved remarkable progress with the emergence of flow matching (FM). It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities. Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation. To model the data distribution, we first design two variational autoencoders (VAEs) to encode both medical images and their corresponding masks into a lower-dimensional latent space. We then estimate a conditional velocity field that guides the flow based on the input image. By sampling multiple latent representations, our method synthesizes diverse segmentation outputs whose pixel-wise variance reliably captures the underlying data distribution, enabling both highly accurate and uncertainty-aware predictions. Furthermore, we generate confidence maps that quantify the model certainty, providing clinicians with richer information for deeper analysis. We conduct experiments on two datasets, ISIC-2018 and CVC-Clinic, and compare our method with several prior baselines, including both deterministic and generative approach models. Through comprehensive evaluations, both qualitative and quantitative results show that our approach achieves superior segmentation accuracy while remaining highly efficient in the latent space.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis</title>
<link>https://arxiv.org/abs/2512.04830</link>
<guid>https://arxiv.org/abs/2512.04830</guid>
<content:encoded><![CDATA[

arXiv:2512.04830v1 Announce Type: new 
Abstract: Closed-loop simulation and scalable pre-training for autonomous driving require synthesizing free-viewpoint driving scenes. However, existing datasets and generative pipelines rarely provide consistent off-trajectory observations, limiting large-scale evaluation and training. While recent generative models demonstrate strong visual realism, they struggle to jointly achieve interpolation consistency and extrapolation realism without per-scene optimization. To address this, we propose FreeGen, a feed-forward reconstruction-generation co-training framework for free-viewpoint driving scene synthesis. The reconstruction model provides stable geometric representations to ensure interpolation consistency, while the generation model performs geometry-aware enhancement to improve realism at unseen viewpoints. Through co-training, generative priors are distilled into the reconstruction model to improve off-trajectory rendering, and the refined geometry in turn offers stronger structural guidance for generation. Experiments demonstrate that FreeGen achieves state-of-the-art performance for free-viewpoint driving scene synthesis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenizing Buildings: A Transformer for Layout Synthesis</title>
<link>https://arxiv.org/abs/2512.04832</link>
<guid>https://arxiv.org/abs/2512.04832</guid>
<content:encoded><![CDATA[

arXiv:2512.04832v1 Announce Type: new 
Abstract: We introduce Small Building Model (SBM), a Transformer-based architecture for layout synthesis in Building Information Modeling (BIM) scenes. We address the question of how to tokenize buildings by unifying heterogeneous feature sets of architectural elements into sequences while preserving compositional structure. Such feature sets are represented as a sparse attribute-feature matrix that captures room properties. We then design a unified embedding module that learns joint representations of categorical and possibly correlated continuous feature groups. Lastly, we train a single Transformer backbone in two modes: an encoder-only pathway that yields high-fidelity room embeddings, and an encoder-decoder pipeline for autoregressive prediction of room entities, referred to as Data-Driven Entity Prediction (DDEP). Experiments across retrieval and generative layout synthesis show that SBM learns compact room embeddings that reliably cluster by type and topology, enabling strong semantic retrieval. In DDEP mode, SBM produces functionally sound layouts, with fewer collisions and boundary violations and improved navigability.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Sanity Check for Multi-In-Domain Face Forgery Detection in the Real World</title>
<link>https://arxiv.org/abs/2512.04837</link>
<guid>https://arxiv.org/abs/2512.04837</guid>
<content:encoded><![CDATA[

arXiv:2512.04837v1 Announce Type: new 
Abstract: Existing methods for deepfake detection aim to develop generalizable detectors. Although "generalizable" is the ultimate target once and for all, with limited training forgeries and domains, it appears idealistic to expect generalization that covers entirely unseen variations, especially given the diversity of real-world deepfakes. Therefore, introducing large-scale multi-domain data for training can be feasible and important for real-world applications. However, within such a multi-domain scenario, the differences between multiple domains, rather than the subtle real/fake distinctions, dominate the feature space. As a result, despite detectors being able to relatively separate real and fake within each domain (i.e., high AUC), they struggle with single-image real/fake judgments in domain-unspecified conditions (i.e., low ACC). In this paper, we first define a new research paradigm named Multi-In-Domain Face Forgery Detection (MID-FFD), which includes sufficient volumes of real-fake domains for training. Then, the detector should provide definitive real-fake judgments to the domain-unspecified inputs, which simulate the frame-by-frame independent detection scenario in the real world. Meanwhile, to address the domain-dominant issue, we propose a model-agnostic framework termed DevDet (Developer for Detector) to amplify real/fake differences and make them dominant in the feature space. DevDet consists of a Face Forgery Developer (FFDev) and a Dose-Adaptive detector Fine-Tuning strategy (DAFT). Experiments demonstrate our superiority in predicting real-fake under the MID-FFD scenario while maintaining original generalization ability to unseen data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens</title>
<link>https://arxiv.org/abs/2512.04857</link>
<guid>https://arxiv.org/abs/2512.04857</guid>
<content:encoded><![CDATA[

arXiv:2512.04857v1 Announce Type: new 
Abstract: Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing</title>
<link>https://arxiv.org/abs/2512.04862</link>
<guid>https://arxiv.org/abs/2512.04862</guid>
<content:encoded><![CDATA[

arXiv:2512.04862v1 Announce Type: new 
Abstract: Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection</title>
<link>https://arxiv.org/abs/2512.04875</link>
<guid>https://arxiv.org/abs/2512.04875</guid>
<content:encoded><![CDATA[

arXiv:2512.04875v1 Announce Type: new 
Abstract: Automated lesion detection in chest X-rays has demonstrated significant potential for improving clinical diagnosis by precisely localizing pathological abnormalities. While recent promptable detection frameworks have achieved remarkable accuracy in target localization, existing methods typically rely on manual annotations as prompts, which are labor-intensive and impractical for clinical applications. To address this limitation, we propose SP-Det, a novel self-prompted detection framework that automatically generates rich textual context to guide multi-label lesion detection without requiring expert annotations. Specifically, we introduce an expert-free dual-text prompt generator (DTPG) that leverages two complementary textual modalities: semantic context prompts that capture global pathological patterns and disease beacon prompts that focus on disease-specific manifestations. Moreover, we devise a bidirectional feature enhancer (BFE) that synergistically integrates comprehensive diagnostic context with disease-specific embeddings to significantly improve feature representation and detection accuracy. Extensive experiments on two chest X-ray datasets with diverse thoracic disease categories demonstrate that our SP-Det framework outperforms state-of-the-art detection methods while completely eliminating the dependency on expert-annotated prompts compared to existing promptable architectures.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDG-Track: A Heterogeneous Observer-Follower Framework for High-Resolution UAV Tracking on Embedded Platforms</title>
<link>https://arxiv.org/abs/2512.04883</link>
<guid>https://arxiv.org/abs/2512.04883</guid>
<content:encoded><![CDATA[

arXiv:2512.04883v1 Announce Type: new 
Abstract: Real-time tracking of small unmanned aerial vehicles (UAVs) on edge devices faces a fundamental resolution-speed conflict. Downsampling high-resolution imagery to standard detector input sizes causes small target features to collapse below detectable thresholds. Yet processing native 1080p frames on resource-constrained platforms yields insufficient throughput for smooth gimbal control. We propose SDG-Track, a Sparse Detection-Guided Tracker that adopts an Observer-Follower architecture to reconcile this conflict. The Observer stream runs a high-capacity detector at low frequency on the GPU to provide accurate position anchors from 1920x1080 frames. The Follower stream performs high-frequency trajectory interpolation via ROI-constrained sparse optical flow on the CPU. To handle tracking failures from occlusion or model drift caused by spectrally similar distractors, we introduce Dual-Space Recovery, a training-free re-acquisition mechanism combining color histogram matching with geometric consistency constraints. Experiments on a ground-to-air tracking station demonstrate that SDG-Track achieves 35.1 FPS system throughput while retaining 97.2\% of the frame-by-frame detection precision. The system successfully tracks agile FPV drones under real-world operational conditions on an NVIDIA Jetson Orin Nano. Our paper code is publicly available at https://github.com/Jeffry-wen/SDG-Track
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Only Train Once (YOTO): A Retraining-Free Object Detection Framework</title>
<link>https://arxiv.org/abs/2512.04888</link>
<guid>https://arxiv.org/abs/2512.04888</guid>
<content:encoded><![CDATA[

arXiv:2512.04888v1 Announce Type: new 
Abstract: Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI</title>
<link>https://arxiv.org/abs/2512.04890</link>
<guid>https://arxiv.org/abs/2512.04890</guid>
<content:encoded><![CDATA[

arXiv:2512.04890v1 Announce Type: new 
Abstract: We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching</title>
<link>https://arxiv.org/abs/2512.04904</link>
<guid>https://arxiv.org/abs/2512.04904</guid>
<content:encoded><![CDATA[

arXiv:2512.04904v1 Announce Type: new 
Abstract: Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion</title>
<link>https://arxiv.org/abs/2512.04926</link>
<guid>https://arxiv.org/abs/2512.04926</guid>
<content:encoded><![CDATA[

arXiv:2512.04926v1 Announce Type: new 
Abstract: Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtually Unrolling the Herculaneum Papyri by Diffeomorphic Spiral Fitting</title>
<link>https://arxiv.org/abs/2512.04927</link>
<guid>https://arxiv.org/abs/2512.04927</guid>
<content:encoded><![CDATA[

arXiv:2512.04927v1 Announce Type: new 
Abstract: The Herculaneum Papyri are a collection of rolled papyrus documents that were charred and buried by the famous eruption of Mount Vesuvius. They promise to contain a wealth of previously unseen Greek and Latin texts, but are extremely fragile and thus most cannot be unrolled physically. A solution to access these texts is virtual unrolling, where the papyrus surface is digitally traced out in a CT scan of the scroll, to create a flattened representation. This tracing is very laborious to do manually in gigavoxel-sized scans, so automated approaches are desirable. We present the first top-down method that automatically fits a surface model to a CT scan of a severely damaged scroll. We take a novel approach that globally fits an explicit parametric model of the deformed scroll to existing neural network predictions of where the rolled papyrus likely passes. Our method guarantees the resulting surface is a single continuous 2D sheet, even passing through regions where the surface is not detectable in the CT scan. We conduct comprehensive experiments on high-resolution CT scans of two scrolls, showing that our approach successfully unrolls large regions, and exceeds the performance of the only existing automated unrolling method suitable for this data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging</title>
<link>https://arxiv.org/abs/2512.04939</link>
<guid>https://arxiv.org/abs/2512.04939</guid>
<content:encoded><![CDATA[

arXiv:2512.04939v1 Announce Type: new 
Abstract: 3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Adaptive Fusion of Multimodal Deep Networks for Human Action Recognition</title>
<link>https://arxiv.org/abs/2512.04943</link>
<guid>https://arxiv.org/abs/2512.04943</guid>
<content:encoded><![CDATA[

arXiv:2512.04943v1 Announce Type: new 
Abstract: This study introduces a pioneering methodology for human action recognition by harnessing deep neural network techniques and adaptive fusion strategies across multiple modalities, including RGB, optical flows, audio, and depth information. Employing gating mechanisms for multimodal fusion, we aim to surpass limitations inherent in traditional unimodal recognition methods while exploring novel possibilities for diverse applications. Through an exhaustive investigation of gating mechanisms and adaptive weighting-based fusion architectures, our methodology enables the selective integration of relevant information from various modalities, thereby bolstering both accuracy and robustness in action recognition tasks. We meticulously examine various gated fusion strategies to pinpoint the most effective approach for multimodal action recognition, showcasing its superiority over conventional unimodal methods. Gating mechanisms facilitate the extraction of pivotal features, resulting in a more holistic representation of actions and substantial enhancements in recognition performance. Our evaluations across human action recognition, violence action detection, and multiple self-supervised learning tasks on benchmark datasets demonstrate promising advancements in accuracy. The significance of this research lies in its potential to revolutionize action recognition systems across diverse fields. The fusion of multimodal information promises sophisticated applications in surveillance and human-computer interaction, especially in contexts related to active assisted living.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization</title>
<link>https://arxiv.org/abs/2512.04952</link>
<guid>https://arxiv.org/abs/2512.04952</guid>
<content:encoded><![CDATA[

arXiv:2512.04952v1 Announce Type: new 
Abstract: Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoPE:A Unified Geometric Positional Embedding for Structured Tensors</title>
<link>https://arxiv.org/abs/2512.04963</link>
<guid>https://arxiv.org/abs/2512.04963</guid>
<content:encoded><![CDATA[

arXiv:2512.04963v1 Announce Type: new 
Abstract: Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis</title>
<link>https://arxiv.org/abs/2512.04967</link>
<guid>https://arxiv.org/abs/2512.04967</guid>
<content:encoded><![CDATA[

arXiv:2512.04967v1 Announce Type: new 
Abstract: Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Use of Vision Transformers for AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2512.04969</link>
<guid>https://arxiv.org/abs/2512.04969</guid>
<content:encoded><![CDATA[

arXiv:2512.04969v1 Announce Type: new 
Abstract: Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks</title>
<link>https://arxiv.org/abs/2512.04970</link>
<guid>https://arxiv.org/abs/2512.04970</guid>
<content:encoded><![CDATA[

arXiv:2512.04970v1 Announce Type: new 
Abstract: We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.04981</link>
<guid>https://arxiv.org/abs/2512.04981</guid>
<content:encoded><![CDATA[

arXiv:2512.04981v1 Announce Type: new 
Abstract: Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs</title>
<link>https://arxiv.org/abs/2512.04996</link>
<guid>https://arxiv.org/abs/2512.04996</guid>
<content:encoded><![CDATA[

arXiv:2512.04996v1 Announce Type: new 
Abstract: This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reflection Removal through Efficient Adaptation of Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.05000</link>
<guid>https://arxiv.org/abs/2512.05000</guid>
<content:encoded><![CDATA[

arXiv:2512.05000v1 Announce Type: new 
Abstract: We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning for Transparent Object Depth Completion Using Depth from Non-Transparent Objects</title>
<link>https://arxiv.org/abs/2512.05006</link>
<guid>https://arxiv.org/abs/2512.05006</guid>
<content:encoded><![CDATA[

arXiv:2512.05006v1 Announce Type: new 
Abstract: The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Neural Video Compression via Video Diffusion Prior</title>
<link>https://arxiv.org/abs/2512.05016</link>
<guid>https://arxiv.org/abs/2512.05016</guid>
<content:encoded><![CDATA[

arXiv:2512.05016v1 Announce Type: new 
Abstract: We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition</title>
<link>https://arxiv.org/abs/2512.05021</link>
<guid>https://arxiv.org/abs/2512.05021</guid>
<content:encoded><![CDATA[

arXiv:2512.05021v1 Announce Type: new 
Abstract: Handwritten Text Recognition remains challenging due to the limited data, high writing style variance, and scripts with complex diacritics. Existing approaches, though partially address these issues, often struggle to generalize without massive synthetic data. To address these challenges, we propose HTR-ConvText, a model designed to capture fine-grained, stroke-level local features while preserving global contextual dependencies. In the feature extraction stage, we integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details. We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency. Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification. Evaluations on IAM, READ2016, LAM and HANDS-VNOnDB demonstrate that our approach achieves improved performance and better generalization compared to existing methods, especially in scenarios with limited training samples and high handwriting diversity.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation</title>
<link>https://arxiv.org/abs/2512.05025</link>
<guid>https://arxiv.org/abs/2512.05025</guid>
<content:encoded><![CDATA[

arXiv:2512.05025v1 Announce Type: new 
Abstract: Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Guided Two-Stage GAN for Face Inpainting with Hybrid Perceptual Encoding</title>
<link>https://arxiv.org/abs/2512.05039</link>
<guid>https://arxiv.org/abs/2512.05039</guid>
<content:encoded><![CDATA[

arXiv:2512.05039v1 Announce Type: new 
Abstract: Facial Image inpainting aim is to restore the missing or corrupted regions in face images while preserving identity, structural consistency and photorealistic image quality, a task specifically created for photo restoration. Though there are recent lot of advances in deep generative models, existing methods face problems with large irregular masks, often producing blurry textures on the edges of the masked region, semantic inconsistencies, or unconvincing facial structures due to direct pixel level synthesis approach and limited exploitation of facial priors. In this paper we propose a novel architecture, which address these above challenges through semantic-guided hierarchical synthesis. Our approach starts with a method that organizes and synthesizes information based on meaning, followed by refining the texture. This process gives clear insights into the facial structure before we move on to creating detailed images. In the first stage, we blend two techniques: one that focuses on local features with CNNs and global features with Vision Transformers. This helped us create clear and detailed semantic layouts. In the second stage, we use a Multi-Modal Texture Generator to refine these layouts by pulling in information from different scales, ensuring everything looks cohesive and consistent. The architecture naturally handles arbitrary mask configurations through dynamic attention without maskspecific training. Experiment on two datasets CelebA-HQ and FFHQ shows that our model outperforms other state-of-the-art methods, showing improvements in metrics like LPIPS, PSNR, and SSIM. It produces visually striking results with better semantic preservation, in challenging large-area inpainting situations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint 3D Geometry Reconstruction and Motion Generation for 4D Synthesis from a Single Image</title>
<link>https://arxiv.org/abs/2512.05044</link>
<guid>https://arxiv.org/abs/2512.05044</guid>
<content:encoded><![CDATA[

arXiv:2512.05044v1 Announce Type: new 
Abstract: Generating interactive and dynamic 4D scenes from a single static image remains a core challenge. Most existing generate-then-reconstruct and reconstruct-then-generate methods decouple geometry from motion, causing spatiotemporal inconsistencies and poor generalization. To address these, we extend the reconstruct-then-generate framework to jointly perform Motion generation and geometric Reconstruction for 4D Synthesis (MoRe4D). We first introduce TrajScene-60K, a large-scale dataset of 60,000 video samples with dense point trajectories, addressing the scarcity of high-quality 4D scene data. Based on this, we propose a diffusion-based 4D Scene Trajectory Generator (4D-STraG) to jointly generate geometrically consistent and motion-plausible 4D point trajectories. To leverage single-view priors, we design a depth-guided motion normalization strategy and a motion-aware module for effective geometry and dynamics integration. We then propose a 4D View Synthesis Module (4D-ViSM) to render videos with arbitrary camera trajectories from 4D point track representations. Experiments show that MoRe4D generates high-quality 4D scenes with multi-view consistency and rich dynamic details from a single image. Code: https://github.com/Zhangyr2022/MoRe4D.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer</title>
<link>https://arxiv.org/abs/2512.05060</link>
<guid>https://arxiv.org/abs/2512.05060</guid>
<content:encoded><![CDATA[

arXiv:2512.05060v1 Announce Type: new 
Abstract: Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BulletTime: Decoupled Control of Time and Camera Pose for Video Generation</title>
<link>https://arxiv.org/abs/2512.05076</link>
<guid>https://arxiv.org/abs/2512.05076</guid>
<content:encoded><![CDATA[

arXiv:2512.05076v1 Announce Type: new 
Abstract: Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints</title>
<link>https://arxiv.org/abs/2512.05079</link>
<guid>https://arxiv.org/abs/2512.05079</guid>
<content:encoded><![CDATA[

arXiv:2512.05079v1 Announce Type: new 
Abstract: Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression</title>
<link>https://arxiv.org/abs/2512.05081</link>
<guid>https://arxiv.org/abs/2512.05081</guid>
<content:encoded><![CDATA[

arXiv:2512.05081v1 Announce Type: new 
Abstract: Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2512.05091</link>
<guid>https://arxiv.org/abs/2512.05091</guid>
<content:encoded><![CDATA[

arXiv:2512.05091v1 Announce Type: new 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards</title>
<link>https://arxiv.org/abs/2512.05098</link>
<guid>https://arxiv.org/abs/2512.05098</guid>
<content:encoded><![CDATA[

arXiv:2512.05098v1 Announce Type: new 
Abstract: In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoIR: Towards All-in-One Image Restoration via Evolutionary Frequency Modulation</title>
<link>https://arxiv.org/abs/2512.05104</link>
<guid>https://arxiv.org/abs/2512.05104</guid>
<content:encoded><![CDATA[

arXiv:2512.05104v1 Announce Type: new 
Abstract: All-in-One Image Restoration (AiOIR) tasks often involve diverse degradation that require robust and versatile strategies. However, most existing approaches typically lack explicit frequency modeling and rely on fixed or heuristic optimization schedules, which limit the generalization across heterogeneous degradation. To address these limitations, we propose EvoIR, an AiOIR-specific framework that introduces evolutionary frequency modulation for dynamic and adaptive image restoration. Specifically, EvoIR employs the Frequency-Modulated Module (FMM) that decomposes features into high- and low-frequency branches in an explicit manner and adaptively modulates them to enhance both structural fidelity and fine-grained details. Central to EvoIR, an Evolutionary Optimization Strategy (EOS) iteratively adjusts frequency-aware objectives through a population-based evolutionary process, dynamically balancing structural accuracy and perceptual fidelity. Its evolutionary guidance further mitigates gradient conflicts across degradation and accelerates convergence. By synergizing FMM and EOS, EvoIR yields greater improvements than using either component alone, underscoring their complementary roles. Extensive experiments on multiple benchmarks demonstrate that EvoIR outperforms state-of-the-art AiOIR methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</title>
<link>https://arxiv.org/abs/2512.05106</link>
<guid>https://arxiv.org/abs/2512.05106</guid>
<content:encoded><![CDATA[

arXiv:2512.05106v1 Announce Type: new 
Abstract: Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion {\phi}-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. {\phi}-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, {\phi}-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, {\phi}-PD improves CARLA-to-Waymo planner performance by 50\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShadowDraw: From Any Object to Shadow-Drawing Compositional Art</title>
<link>https://arxiv.org/abs/2512.05110</link>
<guid>https://arxiv.org/abs/2512.05110</guid>
<content:encoded><![CDATA[

arXiv:2512.05110v1 Announce Type: new 
Abstract: We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning</title>
<link>https://arxiv.org/abs/2512.05111</link>
<guid>https://arxiv.org/abs/2512.05111</guid>
<content:encoded><![CDATA[

arXiv:2512.05111v1 Announce Type: new 
Abstract: Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</title>
<link>https://arxiv.org/abs/2512.05112</link>
<guid>https://arxiv.org/abs/2512.05112</guid>
<content:encoded><![CDATA[

arXiv:2512.05112v1 Announce Type: new 
Abstract: Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting</title>
<link>https://arxiv.org/abs/2512.05113</link>
<guid>https://arxiv.org/abs/2512.05113</guid>
<content:encoded><![CDATA[

arXiv:2512.05113v1 Announce Type: new 
Abstract: Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light-X: Generative 4D Video Rendering with Camera and Illumination Control</title>
<link>https://arxiv.org/abs/2512.05115</link>
<guid>https://arxiv.org/abs/2512.05115</guid>
<content:encoded><![CDATA[

arXiv:2512.05115v1 Announce Type: new 
Abstract: Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Centred Evaluation of Text-to-Image Generation Models for Self-expression of Mental Distress: A Dataset Based on GPT-4o</title>
<link>https://arxiv.org/abs/2512.04087</link>
<guid>https://arxiv.org/abs/2512.04087</guid>
<content:encoded><![CDATA[

arXiv:2512.04087v1 Announce Type: cross 
Abstract: Effective communication is central to achieving positive healthcare outcomes in mental health contexts, yet international students often face linguistic and cultural barriers that hinder their communication of mental distress. In this study, we evaluate the effectiveness of AI-generated images in supporting self-expression of mental distress. To achieve this, twenty Chinese international students studying at UK universities were invited to describe their personal experiences of mental distress. These descriptions were elaborated using GPT-4o with four persona-based prompt templates rooted in contemporary counselling practice to generate corresponding images. Participants then evaluated the helpfulness of generated images in facilitating the expression of their feelings based on their original descriptions. The resulting dataset comprises 100 textual descriptions of mental distress, 400 generated images, and corresponding human evaluation scores. Findings indicate that prompt design substantially affects perceived helpfulness, with the illustrator persona achieving the highest ratings. This work introduces the first publicly available text-to-image evaluation dataset with human judgment scores in the mental health domain, offering valuable resources for image evaluation, reinforcement learning with human feedback, and multi-modal research on mental health communication.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The changing surface of the world's roads</title>
<link>https://arxiv.org/abs/2512.04092</link>
<guid>https://arxiv.org/abs/2512.04092</guid>
<content:encoded><![CDATA[

arXiv:2512.04092v1 Announce Type: cross 
Abstract: Resilient road infrastructure is a cornerstone of the UN Sustainable Development Goals. Yet a primary indicator of network functionality and resilience is critically lacking: a comprehensive global baseline of road surface information. Here, we overcome this gap by applying a deep learning framework to a global mosaic of Planetscope satellite imagery from 2020 and 2024. The result is the first global multi-temporal dataset of road pavedness and width for 9.2 million km of critical arterial roads, achieving 95.5% coverage where nearly half the network was previously unclassified. This dataset reveals a powerful multi-scale geography of human development. At the planetary scale, we show that the rate of change in pavedness is a robust proxy for a country's development trajectory (correlation with HDI = 0.65). At the national scale, we quantify how unpaved roads constitute a fragile backbone for economic connectivity. We further synthesize our data into a global Humanitarian Passability Matrix with direct implications for humanitarian logistics. At the local scale, case studies demonstrate the framework's versatility: in Ghana, road quality disparities expose the spatial outcomes of governance; in Pakistan, the data identifies infrastructure vulnerabilities to inform climate resilience planning. Together, this work delivers both a foundational dataset and a multi-scale analytical framework for monitoring global infrastructure, from the dynamics of national development to the realities of local governance, climate adaptation, and equity. Unlike traditional proxies such as nighttime lights, which reflect economic activity, road surface data directly measures the physical infrastructure that underpins prosperity and resilience - at higher spatial resolution.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness</title>
<link>https://arxiv.org/abs/2512.04264</link>
<guid>https://arxiv.org/abs/2512.04264</guid>
<content:encoded><![CDATA[

arXiv:2512.04264v1 Announce Type: cross 
Abstract: Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we study the ML model robustness using ten different activation functions through adversarial training in centralized environments and explore the ML model robustness in federal learning environments. In the centralized environment, we first propose an advanced adversarial training approach to improving the ML model robustness by incorporating model architecture change, soft labeling, simplified data augmentation, and varying learning rates. Then, we conduct extensive experiments on ten well-known activation functions in addition to ReLU to better understand how they impact the ML model robustness. Furthermore, we extend the proposed adversarial training approach to the federal learning environment, where both independent and identically distributed (IID) and non-IID data settings are considered. Our proposed centralized adversarial training approach achieves a natural and robust accuracy of 77.08% and 67.96%, respectively on CIFAR-10 against the fast gradient sign attacks. Experiments on ten activation functions reveal ReLU usually performs best. In the federated learning environment, however, the robust accuracy decreases significantly, especially on non-IID data. To address the significant performance drop in the non-IID data case, we introduce data sharing and achieve the natural and robust accuracy of 70.09% and 54.79%, respectively, surpassing the CalFAT algorithm, when 40% data sharing is used. That is, a proper percentage of data sharing can significantly improve the ML model robustness, which is useful to some real-world applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting</title>
<link>https://arxiv.org/abs/2512.04385</link>
<guid>https://arxiv.org/abs/2512.04385</guid>
<content:encoded><![CDATA[

arXiv:2512.04385v1 Announce Type: cross 
Abstract: Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Engineering vs. Deep Learning for Automated Coin Grading: A Comparative Study on Saint-Gaudens Double Eagles</title>
<link>https://arxiv.org/abs/2512.04464</link>
<guid>https://arxiv.org/abs/2512.04464</guid>
<content:encoded><![CDATA[

arXiv:2512.04464v1 Announce Type: cross 
Abstract: We challenge the common belief that deep learning always trumps older techniques, using the example of grading Saint-Gaudens Double Eagle gold coins automatically. In our work, we put a feature-based Artificial Neural Network built around 192 custom features pulled from Sobel edge detection and HSV color analysis up against a hybrid Convolutional Neural Network that blends in EfficientNetV2, plus a straightforward Support Vector Machine as the control. Testing 1,785 coins graded by experts, the ANN nailed 86% exact matches and hit 98% when allowing a 3-grade leeway. On the flip side, CNN and SVM mostly just guessed the most common grade, scraping by with 31% and 30% exact hits. Sure, the CNN looked good on broader tolerance metrics, but that is because of some averaging trick in regression that hides how it totally flops at picking out specific grades. All told, when you are stuck with under 2,000 examples and lopsided classes, baking in real coin-expert knowledge through feature design beats out those inscrutable, all-in-one deep learning setups. This rings true for other niche quality checks where data's thin and know-how matters more than raw compute.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Spatially-Variant Convolution via Differentiable Sparse Kernel Complex</title>
<link>https://arxiv.org/abs/2512.04556</link>
<guid>https://arxiv.org/abs/2512.04556</guid>
<content:encoded><![CDATA[

arXiv:2512.04556v1 Announce Type: cross 
Abstract: Image convolution with complex kernels is a fundamental operation in photography, scientific imaging, and animation effects, yet direct dense convolution is computationally prohibitive on resource-limited devices. Existing approximations, such as simulated annealing or low-rank decompositions, either lack efficiency or fail to capture non-convex kernels. We introduce a differentiable kernel decomposition framework that represents a target spatially-variant, dense, complex kernel using a set of sparse kernel samples. Our approach features (i) a decomposition that enables differentiable optimization of sparse kernels, (ii) a dedicated initialization strategy for non-convex shapes to avoid poor local minima, and (iii) a kernel-space interpolation scheme that extends single-kernel filtering to spatially varying filtering without retraining and additional runtime overhead. Experiments on Gaussian and non-convex kernels show that our method achieves higher fidelity than simulated annealing and significantly lower cost than low-rank decompositions. Our approach provides a practical solution for mobile imaging and real-time rendering, while remaining fully differentiable for integration into broader learning pipelines.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective</title>
<link>https://arxiv.org/abs/2512.04625</link>
<guid>https://arxiv.org/abs/2512.04625</guid>
<content:encoded><![CDATA[

arXiv:2512.04625v1 Announce Type: cross 
Abstract: In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware-aware Neural Architecture Search of Early Exiting Networks on Edge Accelerators</title>
<link>https://arxiv.org/abs/2512.04705</link>
<guid>https://arxiv.org/abs/2512.04705</guid>
<content:encoded><![CDATA[

arXiv:2512.04705v1 Announce Type: cross 
Abstract: Advancements in high-performance computing and cloud technologies have enabled the development of increasingly sophisticated Deep Learning (DL) models. However, the growing demand for embedded intelligence at the edge imposes stringent computational and energy constraints, challenging the deployment of these large-scale models. Early Exiting Neural Networks (EENN) have emerged as a promising solution, allowing dynamic termination of inference based on input complexity to enhance efficiency. Despite their potential, EENN performance is highly influenced by the heterogeneity of edge accelerators and the constraints imposed by quantization, affecting accuracy, energy efficiency, and latency. Yet, research on the automatic optimization of EENN design for edge hardware remains limited. To bridge this gap, we propose a hardware-aware Neural Architecture Search (NAS) framework that systematically integrates the effects of quantization and hardware resource allocation to optimize the placement of early exit points within a network backbone. Experimental results on the CIFAR-10 dataset demonstrate that our NAS framework can discover architectures that achieve over a 50\% reduction in computational costs compared to conventional static networks, making them more suitable for deployment in resource-constrained edge environments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemLoRA: Distilling Expert Adapters for On-Device Memory Systems</title>
<link>https://arxiv.org/abs/2512.04763</link>
<guid>https://arxiv.org/abs/2512.04763</guid>
<content:encoded><![CDATA[

arXiv:2512.04763v1 Announce Type: cross 
Abstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shared Multi-modal Embedding Space for Face-Voice Association</title>
<link>https://arxiv.org/abs/2512.04814</link>
<guid>https://arxiv.org/abs/2512.04814</guid>
<content:encoded><![CDATA[

arXiv:2512.04814v1 Announce Type: cross 
Abstract: The FAME 2026 challenge comprises two demanding tasks: training face-voice associations combined with a multilingual setting that includes testing on languages on which the model was not trained. Our approach consists of separate uni-modal processing pipelines with general face and voice feature extraction, complemented by additional age-gender feature extraction to support prediction. The resulting single-modal features are projected into a shared embedding space and trained with an Adaptive Angular Margin (AAM) loss. Our approach achieved first place in the FAME 2026 challenge, with an average Equal-Error Rate (EER) of 23.99%.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Generated Human Videos to Physically Plausible Robot Trajectories</title>
<link>https://arxiv.org/abs/2512.05094</link>
<guid>https://arxiv.org/abs/2512.05094</guid>
<content:encoded><![CDATA[

arXiv:2512.05094v1 Announce Type: cross 
Abstract: Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TV2TV: A Unified Framework for Interleaved Language and Video Generation</title>
<link>https://arxiv.org/abs/2512.05103</link>
<guid>https://arxiv.org/abs/2512.05103</guid>
<content:encoded><![CDATA[

arXiv:2512.05103v1 Announce Type: cross 
Abstract: Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep infant brain segmentation from multi-contrast MRI</title>
<link>https://arxiv.org/abs/2512.05114</link>
<guid>https://arxiv.org/abs/2512.05114</guid>
<content:encoded><![CDATA[

arXiv:2512.05114v1 Announce Type: cross 
Abstract: Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value Gradient Guidance for Flow Matching Alignment</title>
<link>https://arxiv.org/abs/2512.05116</link>
<guid>https://arxiv.org/abs/2512.05116</guid>
<content:encoded><![CDATA[

arXiv:2512.05116v1 Announce Type: cross 
Abstract: While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Universal Weight Subspace Hypothesis</title>
<link>https://arxiv.org/abs/2512.05117</link>
<guid>https://arxiv.org/abs/2512.05117</guid>
<content:encoded><![CDATA[

arXiv:2512.05117v1 Announce Type: cross 
Abstract: We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient stereo matching on embedded GPUs with zero-means cross correlation</title>
<link>https://arxiv.org/abs/2212.00476</link>
<guid>https://arxiv.org/abs/2212.00476</guid>
<content:encoded><![CDATA[

arXiv:2212.00476v2 Announce Type: replace 
Abstract: Mobile stereo-matching systems have become an important part of many applications, such as automated-driving vehicles and autonomous robots. Accurate stereo-matching methods usually lead to high computational complexity; however, mobile platforms have only limited hardware resources to keep their power consumption low; this makes it difficult to maintain both an acceptable processing speed and accuracy on mobile platforms. To resolve this trade-off, we herein propose a novel acceleration approach for the well-known zero-means normalized cross correlation (ZNCC) matching cost calculation algorithm on a Jetson Tx2 embedded GPU. In our method for accelerating ZNCC, target images are scanned in a zigzag fashion to efficiently reuse one pixel's computation for its neighboring pixels; this reduces the amount of data transmission and increases the utilization of on-chip registers, thus increasing the processing speed. As a result, our method is 2X faster than the traditional image scanning method, and 26% faster than the latest NCC method. By combining this technique with the domain transformation (DT) algorithm, our system show real-time processing speed of 32 fps, on a Jetson Tx2 GPU for 1,280x384 pixel images with a maximum disparity of 128. Additionally, the evaluation results on the KITTI 2015 benchmark show that our combined system is more accurate than the same algorithm combined with census by 7.26%, while maintaining almost the same processing speed. Source Code: https://github.com/changqiong/Z2ZNCC.git
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polygon Intersection-over-Union Loss for Viewpoint-Agnostic Monocular 3D Vehicle Detection</title>
<link>https://arxiv.org/abs/2309.07104</link>
<guid>https://arxiv.org/abs/2309.07104</guid>
<content:encoded><![CDATA[

arXiv:2309.07104v2 Announce Type: replace 
Abstract: Monocular 3D object detection is a challenging task because depth information is difficult to obtain from 2D images. A subset of viewpoint-agnostic monocular 3D detection methods also do not explicitly leverage scene homography or geometry during training, meaning that a model trained thusly can detect objects in images from arbitrary viewpoints. Such works predict the projections of the 3D bounding boxes on the image plane to estimate the location of the 3D boxes, but these projections are not rectangular so the calculation of IoU between these projected polygons is not straightforward. This work proposes an efficient, fully differentiable algorithm for the calculation of IoU between two convex polygons, which can be utilized to compute the IoU between two 3D bounding box footprints viewed from an arbitrary angle. We test the performance of the proposed polygon IoU loss (PIoU loss) on three state-of-the-art viewpoint-agnostic 3D detection models. Experiments demonstrate that the proposed PIoU loss converges faster than L1 loss and that in 3D detection models, a combination of PIoU loss and L1 loss gives better results than L1 loss alone (+1.64% AP70 for MonoCon on cars, +0.18% AP70 for RTM3D on cars, and +0.83%/+2.46% AP50/AP25 for MonoRCNN on cyclists).
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surface-Based Visibility-Guided Uncertainty for Continuous Active 3D Neural Reconstruction</title>
<link>https://arxiv.org/abs/2405.02568</link>
<guid>https://arxiv.org/abs/2405.02568</guid>
<content:encoded><![CDATA[

arXiv:2405.02568v3 Announce Type: replace 
Abstract: View selection is critical in active 3D neural reconstruction as it impacts the contents of training set and resulting final output quality. Recent view selection strategies emphasize the visibility when evaluating model uncertainty in active 3D reconstruction. However, existing approaches estimate visibility only after the model fully converges, which has confined their application primarily to non-continuous active learning settings. This paper proposes Surface-Based Visibility field (SBV) that successfully estimates the visibility-guided uncertainty in continuous active 3D neural reconstruction. During learning neural implicit surfaces, our model learns rendering uncertainties and infers surface confidence values derived from signed distance functions. It then updates surface confidences using a voxel grid, robustly deducing the surface-based visibility for uncertainties. This approach captures uncertainties across all regions, whether well-defined surfaces or ambiguous areas, ensuring accurate visibility measurement in continuous active learning. Experiments on benchmark datasets-Tanks and Temples, BlendedMVS, Blender, DTU-and the newly proposed imbalanced viewpoint dataset (ImBView) show that view selection based on SBV-guided uncertainty improves performance by up to 11.6% over existing methods, highlighting its effectiveness in challenging reconstruction scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Adversarial Defense for Vision-Language Models by Leveraging One-To-Many Relationships</title>
<link>https://arxiv.org/abs/2405.18770</link>
<guid>https://arxiv.org/abs/2405.18770</guid>
<content:encoded><![CDATA[

arXiv:2405.18770v5 Announce Type: replace 
Abstract: Pre-trained vision-language (VL) models are highly vulnerable to adversarial attacks. However, existing defense methods primarily focus on image classification, overlooking two key aspects of VL tasks: multimodal attacks, where both image and text can be perturbed, and the one-to-many relationship of images and texts, where a single image can correspond to multiple textual descriptions and vice versa (1:N and N:1). This work is the first to explore defense strategies against multimodal attacks in VL tasks, whereas prior VL defense methods focus on vision robustness. We propose multimodal adversarial training (MAT), which incorporates adversarial perturbations in both image and text modalities during training, significantly outperforming existing unimodal defenses. Furthermore, we discover that MAT is limited by deterministic one-to-one (1:1) image-text pairs in VL training data. To address this, we conduct a comprehensive study on leveraging one-to-many relationships to enhance robustness, investigating diverse augmentation techniques. Our analysis shows that, for a more effective defense, augmented image-text pairs should be well-aligned, diverse, yet avoid distribution shift -- conditions overlooked by prior research. This work pioneers defense strategies against multimodal attacks, providing insights for building robust VLMs from both optimization and data perspectives. Our code is publicly available at https://github.com/CyberAgentAI/multimodal-adversarial-training.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OP-Align: Object-level and Part-level Alignment for Self-supervised Category-level Articulated Object Pose Estimation</title>
<link>https://arxiv.org/abs/2408.16547</link>
<guid>https://arxiv.org/abs/2408.16547</guid>
<content:encoded><![CDATA[

arXiv:2408.16547v2 Announce Type: replace 
Abstract: Category-level articulated object pose estimation focuses on the pose estimation of unknown articulated objects within known categories. Despite its significance, this task remains challenging due to the varying shapes and poses of objects, expensive dataset annotation costs, and complex real-world environments. In this paper, we propose a novel self-supervised approach that leverages a single-frame point cloud to solve this task. Our model consistently generates reconstruction with a canonical pose and joint state for the entire input object, and it estimates object-level poses that reduce overall pose variance and part-level poses that align each part of the input with its corresponding part of the reconstruction. Experimental results demonstrate that our approach significantly outperforms previous self-supervised methods and is comparable to the state-of-the-art supervised methods. To assess the performance of our model in real-world scenarios, we also introduce a new real-world articulated object benchmark dataset.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Markup Document Models for Graphic Design Completion</title>
<link>https://arxiv.org/abs/2409.19051</link>
<guid>https://arxiv.org/abs/2409.19051</guid>
<content:encoded><![CDATA[

arXiv:2409.19051v2 Announce Type: replace 
Abstract: We introduce MarkupDM, a multimodal markup document model that represents graphic design as an interleaved multimodal document consisting of both markup language and images. Unlike existing holistic approaches that rely on an element-by-attribute grid representation, our representation accommodates variable-length elements, type-dependent attributes, and text content. Inspired by fill-in-the-middle training in code generation, we train the model to complete the missing part of a design document from its surrounding context, allowing it to treat various design tasks in a unified manner. Our model also supports image generation by predicting discrete image tokens through a specialized tokenizer with support for image transparency. We evaluate MarkupDM on three tasks, attribute value, image, and text completion, and demonstrate that it can produce plausible designs consistent with the given context. To further illustrate the flexibility of our approach, we evaluate our approach on a new instruction-guided design completion task where our instruction-tuned MarkupDM compares favorably to state-of-the-art image editing models, especially in textual completion. These findings suggest that multimodal language models with our document representation can serve as a versatile foundation for broad design automation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Geodesics of Geometric Shape Deformations From Images</title>
<link>https://arxiv.org/abs/2410.18797</link>
<guid>https://arxiv.org/abs/2410.18797</guid>
<content:encoded><![CDATA[

arXiv:2410.18797v2 Announce Type: replace 
Abstract: This paper presents a novel method, named geodesic deformable networks (GDN), that for the first time enables the learning of geodesic flows of deformation fields derived from images. In particular, the capability of our proposed GDN being able to predict geodesics is important for quantifying and comparing deformable shape presented in images. The geodesic deformations, also known as optimal transformations that align pairwise images, are often parameterized by a time sequence of smooth vector fields governed by nonlinear differential equations. A bountiful literature has been focusing on learning the initial conditions (e.g., initial velocity fields) based on registration networks. However, the definition of geodesics central to deformation-based shape analysis is blind to the networks. To address this problem, we carefully develop an efficient neural operator to treat the geodesics as unknown mapping functions learned from the latent deformation spaces. A composition of integral operators and smooth activation functions is then formulated to effectively approximate such mappings. In contrast to previous works, our GDN jointly optimizes a newly defined geodesic loss, which adds additional benefits to promote the network regularizability and generalizability. We demonstrate the effectiveness of GDN on both 2D synthetic data and 3D real brain magnetic resonance imaging (MRI).
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Turbo-GS: Accelerating 3D Gaussian Fitting for High-Quality Radiance Fields</title>
<link>https://arxiv.org/abs/2412.13547</link>
<guid>https://arxiv.org/abs/2412.13547</guid>
<content:encoded><![CDATA[

arXiv:2412.13547v2 Announce Type: replace 
Abstract: Novel-view synthesis is an important problem in computer vision with applications in 3D reconstruction, mixed reality, and robotics. Recent methods like 3D Gaussian Splatting (3DGS) have become the preferred method for this task, providing high-quality novel views in real time. However, the training time of a 3DGS model is slow, often taking 30 minutes for a scene with 200 views. In contrast, our goal is to reduce the optimization time by training for fewer steps while maintaining high rendering quality. Specifically, we combine the guidance from both the position error and the appearance error to achieve a more effective densification. To balance the rate between adding new Gaussians and fitting old Gaussians, we develop a convergence-aware budget control mechanism. Moreover, to make the densification process more reliable, we selectively add new Gaussians from mostly visited regions. With these designs, we reduce the Gaussian optimization steps to one-third of the previous approach while achieving a comparable or even better novel view rendering quality. To further facilitate the rapid fitting of 4K resolution images, we introduce a dilation-based rendering technique. Our method, Turbo-GS, speeds up optimization for typical scenes and scales well to high-resolution (4K) scenarios on standard datasets. Through extensive experiments, we show that our method is significantly faster in optimization than other methods while retaining quality. Project page: https://ivl.cs.brown.edu/research/turbo-gs.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoLUT: Efficient Volumetric streaming enhanced by LUT-based super-resolution</title>
<link>https://arxiv.org/abs/2502.12151</link>
<guid>https://arxiv.org/abs/2502.12151</guid>
<content:encoded><![CDATA[

arXiv:2502.12151v2 Announce Type: replace 
Abstract: 3D volumetric video provides immersive experience and is gaining traction in digital media. Despite its rising popularity, the streaming of volumetric video content poses significant challenges due to the high data bandwidth requirement. A natural approach to mitigate the bandwidth issue is to reduce the volumetric video's data rate by downsampling the content prior to transmission. The video can then be upsampled at the receiver's end using a super-resolution (SR) algorithm to reconstruct the high-resolution details. While super-resolution techniques have been extensively explored and advanced for 2D video content, there is limited work on SR algorithms tailored for volumetric videos.
  To address this gap and the growing need for efficient volumetric video streaming, we have developed VoLUT with a new SR algorithm specifically designed for volumetric content. Our algorithm uniquely harnesses the power of lookup tables (LUTs) to facilitate the efficient and accurate upscaling of low-resolution volumetric data. The use of LUTs enables our algorithm to quickly reference precomputed high-resolution values, thereby significantly reducing the computational complexity and time required for upscaling. We further apply adaptive video bit rate algorithm (ABR) to dynamically determine the downsampling rate according to the network condition and stream the selected video rate to the receiver. Compared to related work, VoLUT is the first to enable high-quality 3D SR on commodity mobile devices at line-rate. Our evaluation shows VoLUT can reduce bandwidth usage by 70% , boost QoE by 36.7% for volumetric video streaming and achieve
  3D SR speed-up with no quality compromise.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SYNTHIA: Novel Concept Design with Affordance Composition</title>
<link>https://arxiv.org/abs/2502.17793</link>
<guid>https://arxiv.org/abs/2502.17793</guid>
<content:encoded><![CDATA[

arXiv:2502.17793v4 Announce Type: replace 
Abstract: Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence--the integration of multiple affordances into a single coherent concept--remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1% and 14.7% for novelty and functional coherence in human evaluation, respectively.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sliding-Window Merging for Compacting Patch-Redundant Layers in LLMs</title>
<link>https://arxiv.org/abs/2502.19159</link>
<guid>https://arxiv.org/abs/2502.19159</guid>
<content:encoded><![CDATA[

arXiv:2502.19159v4 Announce Type: replace 
Abstract: Depth-wise pruning accelerates LLM inference in resource-constrained scenarios but suffers from performance degradation due to direct removal of entire Transformer layers. This paper reveals ``Patch-like'' redundancy across layers via correlation analysis of the outputs of different layers in reproducing kernel Hilbert space, demonstrating consecutive layers exhibit high functional similarity. Building on this observation, this paper proposes Sliding-Window Merging (SWM) - a dynamic compression method that selects consecutive layers from top to bottom using a pre-defined similarity threshold, and compacts patch-redundant layers through a parameter consolidation, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35% pruning on the Vicuna-7B model, our method achieved a 1.654% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at https://github.com/920927/SLM-a-sliding-layer-merging-method.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVE: Towards End-to-End Video Subtitle Extraction with Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.04058</link>
<guid>https://arxiv.org/abs/2503.04058</guid>
<content:encoded><![CDATA[

arXiv:2503.04058v2 Announce Type: replace 
Abstract: Video subtitles play a crucial role in short videos and movies, as they not only help models better understand video content but also support applications such as video translation and content retrieval. Existing video subtitle extraction methods typically rely on multi-stage frameworks, where errors accumulate across stages and temporal dependencies are underutilized due to frame-wise processing. Moreover, although some Large Vision-Language Models (LVLMs) possess strong OCR capabilities, predicting accurate timestamps for subtitle texts remains challenging. To this end, we propose an End-to-end Video subtitle Extraction framework based on LVLMs, named EVE, which can output subtitles and their timestamps simultaneously. Specifically, we introduce a dual-branch Spatiotemporal Subtitle-Salient (S\textsuperscript{3}) Module that serves as an adapter for LVLMs, capable of representing subtitle-related content and considering inter-frame correlations using only a small number of tokens. Within this module, the Spatial Semantic Context Aggregate branch aggregates high-level global semantics to provide spatial visual contextual information, while the Temporal Subtitle Token Query branch explicitly queries subtitle-relevant tokens while considering temporal correlation across frames. The small number of tokens retained by the S\textsuperscript{3} module are fed to the language model, which then directly outputs the subtitle text along with its timestamps. Furthermore, we construct the first large-scale dataset dedicated to video subtitle extraction, ViSa, containing over 2.5M videos with timestamped and bilingual annotation, thereby providing the community with a well-organized training and evaluation benchmark.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CMMCoT: Enhancing Complex Multi-Image Comprehension via Multi-Modal Chain-of-Thought and Memory Augmentation</title>
<link>https://arxiv.org/abs/2503.05255</link>
<guid>https://arxiv.org/abs/2503.05255</guid>
<content:encoded><![CDATA[

arXiv:2503.05255v2 Announce Type: replace 
Abstract: While previous multimodal slow-thinking methods have demonstrated remarkable success in single-image understanding scenarios, their effectiveness becomes fundamentally constrained when extended to more complex multi-image comprehension tasks. This limitation stems from their predominant reliance on text-based intermediate reasoning processes. While for human, when engaging in sophisticated multi-image analysis, they typically perform two complementary cognitive operations: (1) continuous cross-image visual comparison through region-of-interest matching, and (2) dynamic memorization of critical visual concepts throughout the reasoning chain. Motivated by these observations, we propose the Complex Multi-Modal Chain-of-Thought (CMMCoT) framework, a multi-step reasoning framework that mimics human-like "slow thinking" for multi-image understanding. Our approach incorporates two key innovations: (1) The construction of interleaved multimodal multi-step reasoning chains, which utilize critical visual region tokens, extracted from intermediate reasoning steps, as supervisory signals. This mechanism not only facilitates comprehensive cross-modal understanding but also enhances model interpretability. (2) The introduction of a test-time memory augmentation module that expands the model's reasoning capacity during inference while preserving parameter efficiency. Furthermore, to facilitate research in this direction, we have curated a novel multi-image slow-thinking dataset. Extensive experiments demonstrate the effectiveness of our model. Code is available at https://github.com/zhangguanghao523/CMMCoT.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAVE: Diagnostic benchmark for Audio Visual Evaluation</title>
<link>https://arxiv.org/abs/2503.09321</link>
<guid>https://arxiv.org/abs/2503.09321</guid>
<content:encoded><![CDATA[

arXiv:2503.09321v2 Announce Type: replace 
Abstract: Audio-visual understanding is a rapidly evolving field that seeks to integrate and interpret information from both auditory and visual modalities. Despite recent advances in multi-modal learning, existing benchmarks often suffer from strong visual bias -- when answers can be inferred from visual data alone -- and provide only aggregate scores that conflate multiple sources of error. This makes it difficult to determine whether models struggle with visual understanding, audio interpretation, or audio-visual alignment. In this work, we introduce DAVE: Diagnostic Audio Visual Evaluation, a novel benchmark dataset designed to systematically evaluate audio-visual models across controlled settings. DAVE alleviates existing limitations by (i) ensuring both modalities are necessary to answer correctly and (ii) decoupling evaluation into atomic subcategories. Our detailed analysis of state-of-the-art models reveals specific failure modes and provides targeted insights for improvement. By offering this standardized diagnostic framework, we aim to facilitate more robust development of audio-visual models.
  Dataset: https://huggingface.co/datasets/gorjanradevski/dave
  Code: https://github.com/gorjanradevski/dave
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining</title>
<link>https://arxiv.org/abs/2503.15470</link>
<guid>https://arxiv.org/abs/2503.15470</guid>
<content:encoded><![CDATA[

arXiv:2503.15470v2 Announce Type: replace 
Abstract: Egocentric video-language pretraining has significantly advanced video representation learning. Humans perceive and interact with a fully 3D world, developing spatial awareness that extends beyond text-based understanding. However, most previous works learn from 1D text or 2D visual cues, such as bounding boxes, which inherently lack 3D understanding. To bridge this gap, we introduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained through large-scale 3D-aware video pretraining and video-text contrastive learning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently learn 3D-awareness from pseudo depth maps generated by depth estimation models. To further facilitate 3D-aware video pretraining, we enrich the original brief captions with hand-object visual cues by organically combining several foundation models. Extensive experiments demonstrate EgoDTM's superior performance across diverse downstream tasks, highlighting its superior 3D-aware visual understanding. Code: https://github.com/xuboshen/EgoDTM.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion</title>
<link>https://arxiv.org/abs/2503.22622</link>
<guid>https://arxiv.org/abs/2503.22622</guid>
<content:encoded><![CDATA[

arXiv:2503.22622v4 Announce Type: replace 
Abstract: Multi-view or 4D video generation has emerged as a significant research topic. Nonetheless, recent approaches to 4D generation still struggle with fundamental limitations, as they primarily rely on harnessing multiple video diffusion models with additional training or compute-intensive training of a full 4D diffusion model with limited real-world 4D data and large computational costs. To address these challenges, here we propose the first training-free 4D video generation method that leverages the off-the-shelf video diffusion models to generate multi-view videos from a single input video. Our approach consists of two key steps: (1) By designating the edge frames in the spatio-temporal sampling grid as key frames, we first synthesize them using a video diffusion model, leveraging a depth-based warping technique for guidance. This approach ensures structural consistency across the generated frames, preserving spatial and temporal coherence. (2) We then interpolate the remaining frames using a video diffusion model, constructing a fully populated and temporally coherent sampling grid while preserving spatial and temporal consistency. Through this approach, we extend a single video into a multi-view video along novel camera trajectories while maintaining spatio-temporal consistency. Our method is training-free and fully utilizes an off-the-shelf video diffusion model, offering a practical and effective solution for multi-view video generation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoCoIns: Consistent Subject Generation via Contrastive Instantiated Concepts</title>
<link>https://arxiv.org/abs/2503.24387</link>
<guid>https://arxiv.org/abs/2503.24387</guid>
<content:encoded><![CDATA[

arXiv:2503.24387v2 Announce Type: replace 
Abstract: While text-to-image generative models can synthesize diverse and faithful content, subject variation across multiple generations limits their application to long-form content generation. Existing approaches require time-consuming fine-tuning, reference images for all subjects, or access to previously generated content. We introduce Contrastive Concept Instantiation (CoCoIns), a framework that effectively synthesizes consistent subjects across multiple independent generations. The framework consists of a generative model and a mapping network that transforms input latent codes into pseudo-words associated with specific concept instances. Users can generate consistent subjects by reusing the same latent codes. To construct such associations, we propose a contrastive learning approach that trains the network to distinguish between different combinations of prompts and latent codes. Extensive evaluations on human faces with a single subject show that CoCoIns performs comparably to existing methods while maintaining greater flexibility. We also demonstrate the potential for extending CoCoIns to multiple subjects and other object categories.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution Can Improve Robust Scene Graph Generation</title>
<link>https://arxiv.org/abs/2504.12606</link>
<guid>https://arxiv.org/abs/2504.12606</guid>
<content:encoded><![CDATA[

arXiv:2504.12606v2 Announce Type: replace 
Abstract: In this paper, we propose Robo-SGG, a plug-and-play module for robust scene graph generation (SGG). Unlike standard SGG, the robust scene graph generation aims to perform inference on a diverse range of corrupted images, with the core challenge being the domain shift between the clean and corrupted images. Existing SGG methods suffer from degraded performance due to shifted visual features (e.g., corruption interference or occlusions). To obtain robust visual features, we leverage layout information, representing the global structure of an image, which is robust to domain shift, to enhance the robustness of SGG methods under corruption. Specifically, we employ Instance Normalization (IN) to alleviate the domain-specific variations and recover the robust structural features (i.e., the positional and semantic relationships among objects) by the proposed Layout-Oriented Restitution. Furthermore, under corrupted images, we introduce a Layout-Embedded Encoder (LEE) that adaptively fuses layout and visual features via a gating mechanism, enhancing the robustness of positional and semantic representations for objects and predicates. Note that our proposed Robo-SGG module is designed as a plug-and-play component, which can be easily integrated into any baseline SGG model. Extensive experiments demonstrate that by integrating the state-of-the-art method into our proposed Robo-SGG, we achieve relative improvements of 6.3%, 11.1%, and 8.0% in mR@50 for PredCls, SGCls, and SGDet tasks on the VG-C benchmark, respectively, and achieve new state-of-the-art performance in the corruption scene graph generation benchmark (VG-C and GQA-C). We will release our source code and model.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TongUI: Internet-Scale Trajectories from Multimodal Web Tutorials for Generalized GUI Agents</title>
<link>https://arxiv.org/abs/2504.12679</link>
<guid>https://arxiv.org/abs/2504.12679</guid>
<content:encoded><![CDATA[

arXiv:2504.12679v4 Announce Type: replace 
Abstract: Building Graphical User Interface (GUI) agents is a promising research direction, which simulates human interaction with computers or mobile phones to perform diverse GUI tasks. However, a major challenge in developing generalized GUI agents is the lack of sufficient trajectory data across various operating systems and applications, mainly due to the high cost of manual annotations. In this paper, we propose the TongUI framework that builds generalized GUI agents by learning from rich multimodal web tutorials. Concretely, we crawl and process online GUI tutorials (such as videos and articles) into GUI agent trajectory data, through which we produce the GUI-Net dataset containing 143K trajectory data across five operating systems and more than 200 applications. We develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net, which show remarkable performance improvements on commonly used grounding and navigation benchmarks, outperforming baseline agents about 10\% on multiple benchmarks, showing the effectiveness of the GUI-Net dataset and underscoring the significance of our TongUI framework. We will fully open-source the code, the GUI-Net dataset, and the trained models soon.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments</title>
<link>https://arxiv.org/abs/2505.04488</link>
<guid>https://arxiv.org/abs/2505.04488</guid>
<content:encoded><![CDATA[

arXiv:2505.04488v2 Announce Type: replace 
Abstract: The visually impaired population faces significant challenges in daily activities. While prior works employ vision language models for assistance, most focus on static content and cannot address real-time perception needs in complex environments. Recent VideoLLMs enable real-time vision and speech interaction, offering promising potential for assistive tasks. In this work, we conduct the first study evaluating their effectiveness in supporting daily life for visually impaired individuals. We first conducted a user survey with visually impaired participants to design the benchmark VisAssistDaily for daily life evaluation. Using VisAssistDaily, we evaluate popular VideoLLMs and find GPT-4o achieves the highest task success rate. We further conduct a user study to reveal concerns about hazard perception. To address this, we propose SafeVid, an environment-awareness dataset, and fine-tune VITA-1.5, improving risk recognition accuracy from 25.00% to 76.00%.We hope this work provides valuable insights and inspiration for future research in this field.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Chain-of-Focus Reasoning via Dynamic Visual Search and Zooming for Efficient VLMs</title>
<link>https://arxiv.org/abs/2505.15436</link>
<guid>https://arxiv.org/abs/2505.15436</guid>
<content:encoded><![CDATA[

arXiv:2505.15436v2 Announce Type: replace 
Abstract: Vision language models (VLMs) have achieved impressive performance across a variety of computer vision tasks. However, the multimodal reasoning capability has not been fully explored in existing models. In this paper, we propose a Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and zooming in on key image regions based on obtained visual cues and the given questions, achieving efficient multimodal reasoning. To enable this CoF capability, we present a two-stage training pipeline, including supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct the MM-CoF dataset, comprising 3K samples derived from a visual agent designed to adaptively identify key regions to solve visual tasks with different image resolutions and questions. We use MM-CoF to fine-tune the Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome accuracies and formats as rewards to update the Qwen2.5-VL model, enabling further refining the search and reasoning strategy of models without human priors. Our model achieves significant improvements on multiple benchmarks. On the V* benchmark that requires strong visual reasoning capability, our model outperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to 4K, demonstrating the effectiveness of the proposed CoF method and facilitating the more efficient deployment of VLMs in practical applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Average Calibration Losses for Reliable Uncertainty in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2506.03942</link>
<guid>https://arxiv.org/abs/2506.03942</guid>
<content:encoded><![CDATA[

arXiv:2506.03942v3 Announce Type: replace 
Abstract: Deep neural networks for medical image segmentation are often overconfident, compromising both reliability and clinical utility. In this work, we propose differentiable formulations of marginal L1 Average Calibration Error (mL1-ACE) as an auxiliary loss that can be computed on a per-image basis. We compare both hard- and soft-binning approaches to directly improve pixel-wise calibration. Our experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that incorporating mL1-ACE significantly reduces calibration errors, particularly Average Calibration Error (ACE) and Maximum Calibration Error (MCE), while largely maintaining high Dice Similarity Coefficients (DSCs). We find that the soft-binned variant yields the greatest improvements in calibration, over the Dice plus cross-entropy loss baseline, but often compromises segmentation performance, with hard-binned mL1-ACE maintaining segmentation performance, albeit with weaker calibration improvement. To gain further insight into calibration performance and its variability across an imaging dataset, we introduce dataset reliability histograms, an aggregation of per-image reliability diagrams. The resulting analysis highlights improved alignment between predicted confidences and true accuracies. Overall, our approach not only enhances the trustworthiness of segmentation predictions but also shows potential for safer integration of deep learning methods into clinical workflows. We share our code here: https://github.com/cai4cai/Average-Calibration-Losses
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Focus Temporal Shifting for Precise Event Spotting in Sports Videos</title>
<link>https://arxiv.org/abs/2507.07381</link>
<guid>https://arxiv.org/abs/2507.07381</guid>
<content:encoded><![CDATA[

arXiv:2507.07381v3 Announce Type: replace 
Abstract: Precise Event Spotting (PES) in sports videos requires frame-level recognition of fine-grained actions from single-camera footage. Existing PES models typically incorporate lightweight temporal modules such as the Gate Shift Module (GSM) or the Gate Shift Fuse to enrich 2D CNN feature extractors with temporal context. However, these modules are limited in both temporal receptive field and spatial adaptability. We propose Multi-Focus Temporal Shifting Module (MFS) that enhances GSM with multi-scale temporal shifts and Group Focus Module, enabling efficient modeling of both short and long-term dependencies while focusing on salient regions. MFS is a lightweight, plug-and-play module that integrates seamlessly with diverse 2D backbones. To further advance the field, we introduce the Table Tennis Australia dataset, the first PES benchmark for table tennis containing over 4,800 precisely annotated events. Extensive experiments across five PES benchmarks demonstrate that MFS consistently improves performance with minimal overhead, achieving leading results among lightweight methods (+4.09 mAP, 45 GFLOPs).
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-branch Prompting for Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2507.17588</link>
<guid>https://arxiv.org/abs/2507.17588</guid>
<content:encoded><![CDATA[

arXiv:2507.17588v2 Announce Type: replace 
Abstract: Multimodal Machine Translation (MMT) typically enhances text-only translation by incorporating aligned visual features. Despite the remarkable progress, state-of-the-art MMT approaches often rely on paired image-text inputs at inference and are sensitive to irrelevant visual noise, which limits their robustness and practical applicability. To address these issues, we propose D2P-MMT, a diffusion-based dual-branch prompting framework for robust vision-guided translation. Specifically, D2P-MMT requires only the source text and a reconstructed image generated by a pre-trained diffusion model, which naturally filters out distracting visual details while preserving semantic cues. During training, the model jointly learns from both authentic and reconstructed images using a dual-branch prompting strategy, encouraging rich cross-modal interactions. To bridge the modality gap and mitigate training-inference discrepancies, we introduce a distributional alignment loss that enforces consistency between the output distributions of the two branches. Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves superior translation performance compared to existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Face Experts Fusion in Video Generation: Boosting Identity Consistency Across Large Face Poses</title>
<link>https://arxiv.org/abs/2508.09476</link>
<guid>https://arxiv.org/abs/2508.09476</guid>
<content:encoded><![CDATA[

arXiv:2508.09476v3 Announce Type: replace 
Abstract: Current video generation models struggle with identity preservation under large face poses, primarily facing two challenges: the difficulty in exploring an effective mechanism to integrate identity features into DiT architectures, and the lack of targeted coverage of large face poses in existing open-source video datasets. To address these, we present two key innovations. First, we propose Collaborative Face Experts Fusion (CoFE), which dynamically fuses complementary signals from three specialized experts within the DiT backbone: an identity expert that captures cross-pose invariant features, a semantic expert that encodes high-level visual context, and a detail expert that preserves pixel-level attributes such as skin texture and color gradients. Second, we introduce a data curation pipeline comprising three key components: Face Constraints to ensure diverse large-pose coverage, Identity Consistency to maintain stable identity across frames, and Speech Disambiguation to align textual captions with actual speaking behavior. This pipeline yields LaFID-180K, a large-scale dataset of pose-annotated video clips designed for identity-preserving video generation. Experimental results on several benchmarks demonstrate that our approach significantly outperforms state-of-the-art methods in face similarity, FID, and CLIP semantic alignment. \href{https://rain152.github.io/CoFE/}{Project page}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization</title>
<link>https://arxiv.org/abs/2508.09560</link>
<guid>https://arxiv.org/abs/2508.09560</guid>
<content:encoded><![CDATA[

arXiv:2508.09560v3 Announce Type: replace 
Abstract: Visual geo-localization for drones faces critical degradation under weather perturbations, \eg, rain and fog, where existing methods struggle with two inherent limitations: 1) Heavy reliance on limited weather categories that constrain generalization, and 2) Suboptimal disentanglement of entangled scene-weather features through pseudo weather categories. We present WeatherPrompt, a multi-modality learning paradigm that establishes weather-invariant representations through fusing the image embedding with the text context. Our framework introduces two key contributions: First, a Training-free Weather Reasoning mechanism that employs off-the-shelf large multi-modality models to synthesize multi-weather textual descriptions through human-like reasoning. It improves the scalability to unseen or complex weather, and could reflect different weather strength. Second, to better disentangle the scene and weather feature, we propose a multi-modality framework with the dynamic gating mechanism driven by the text embedding to adaptively reweight and fuse visual features across modalities. The framework is further optimized by the cross-modal objectives, including image-text contrastive learning and image-text matching, which maps the same scene with different weather conditions closer in the respresentation space. Extensive experiments validate that, under diverse weather conditions, our method achieves competitive recall rates compared to state-of-the-art drone geo-localization methods. Notably, it improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog and snow conditions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Downscaling climate projections to 1 km with single-image super resolution</title>
<link>https://arxiv.org/abs/2509.21399</link>
<guid>https://arxiv.org/abs/2509.21399</guid>
<content:encoded><![CDATA[

arXiv:2509.21399v2 Announce Type: replace 
Abstract: High-resolution climate projections are essential for local decision-making. However, available climate projections have low spatial resolution (e.g. 12.5 km), which limits their usability. We address this limitation by leveraging single-image super-resolution models to statistically downscale climate projections to 1-km resolution. Since high-resolution climate projections are unavailable, we train models on a high-resolution observational gridded data set and apply them to low-resolution climate projections. We cannot evaluate downscaled climate projections with common metrics (e.g. pixel-wise root-mean-square error) because we lack ground-truth high-resolution climate projections. Therefore, we evaluate climate indicators computed at weather station locations. Experiments on daily mean temperature demonstrate that single-image super-resolution models can downscale climate projections without increasing the error of climate indicators compared to low-resolution climate projections.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MORPH: PDE Foundation Models with Arbitrary Data Modality</title>
<link>https://arxiv.org/abs/2509.21670</link>
<guid>https://arxiv.org/abs/2509.21670</guid>
<content:encoded><![CDATA[

arXiv:2509.21670v3 Announce Type: replace 
Abstract: We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning</title>
<link>https://arxiv.org/abs/2509.22761</link>
<guid>https://arxiv.org/abs/2509.22761</guid>
<content:encoded><![CDATA[

arXiv:2509.22761v2 Announce Type: replace 
Abstract: Reasoning-augmented machine learning systems have shown improved performance in various domains, including image generation. However, existing reasoning-based methods for image generation either restrict reasoning to a single modality (image or text) or rely on high-quality reasoning data for fine-tuning. To tackle these limitations, we propose MILR, a test-time method that jointly reasons over image and text in a unified latent vector space. Reasoning in MILR is performed by searching through vector representations of discrete image and text tokens. Practically, this is implemented via the policy gradient method, guided by an image quality critic. We instantiate MILR within the unified multimodal understanding and generation (MUG) framework that natively supports language reasoning before image synthesis and thus facilitates cross-modal reasoning. The intermediate model outputs, which are to be optimized, serve as the unified latent space, enabling MILR to operate entirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE, achieving state-of-the-art results on all benchmarks. Notably, on knowledge-intensive WISE, MILR attains an overall score of 0.63, improving over the baseline by 80%. Our further analysis indicates that joint reasoning in the unified latent space is the key to its strong performance. Moreover, our qualitative studies reveal MILR's non-trivial ability in temporal and cultural reasoning, highlighting the efficacy of our reasoning method.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Spatial-visual Adaptive Graph Exploration for Visual Place Recognition</title>
<link>https://arxiv.org/abs/2509.25723</link>
<guid>https://arxiv.org/abs/2509.25723</guid>
<content:encoded><![CDATA[

arXiv:2509.25723v2 Announce Type: replace 
Abstract: Visual Place Recognition (VPR) requires robust retrieval of geotagged images despite large appearance, viewpoint, and environmental variation. Prior methods focus on descriptor fine-tuning or fixed sampling strategies yet neglect the dynamic interplay between spatial context and visual similarity during training. We present SAGE (Spatial-visual Adaptive Graph Exploration), a unified training pipeline that enhances granular spatial-visual discrimination by jointly improving local feature aggregation, organize samples during training, and hard sample mining. We introduce a lightweight Soft Probing module that learns residual weights from training data for patch descriptors before bilinear aggregation, boosting distinctive local cues. During training we reconstruct an online geo-visual graph that fuses geographic proximity and current visual similarity so that candidate neighborhoods reflect the evolving embedding landscape. To concentrate learning on the most informative place neighborhoods, we seed clusters from high-affinity anchors and iteratively expand them with a greedy weighted clique expansion sampler. Implemented with a frozen DINOv2 backbone and parameter-efficient fine-tuning, SAGE achieves SOTA across eight benchmarks. It attains 98.9%, 95.8%, 94.5%, and 96.0% Recall@1 on SPED, Pitts30k-test, MSLS-val, and Nordland, respectively. Notably, our method obtains 100% Recall@10 on SPED only using 4096D global descriptors. Code and models will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VRWKV-Editor: Reducing quadratic complexity in transformer-based video editing</title>
<link>https://arxiv.org/abs/2509.25998</link>
<guid>https://arxiv.org/abs/2509.25998</guid>
<content:encoded><![CDATA[

arXiv:2509.25998v3 Announce Type: replace 
Abstract: In light of recent progress in video editing, deep learning models focusing on both spatial and temporal dependencies have emerged as the primary method. However, these models suffer from the quadratic computational complexity of traditional attention mechanisms, making them difficult to adapt to long-duration and high-resolution videos. This limitation restricts their applicability in practical contexts such as real-time video processing. To tackle this challenge, we introduce a method to reduce both time and space complexity of these systems by proposing VRWKV-Editor, a novel video editing model that integrates a linear spatio-temporal aggregation module into video-based diffusion models. VRWKV-Editor leverages bidirectional weighted key-value recurrence mechanism of the RWKV transformer to capture global dependencies while preserving temporal coherence, achieving linear complexity without sacrificing quality. Extensive experiments demonstrate that the proposed method achieves up to 3.7x speedup and 60% lower memory usage compared to state-of-the-art diffusion-based video editing methods, while maintaining competitive performance in frame consistency and text alignment. Furthermore, a comparative analysis we conducted on videos with different sequence lengths confirms that the gap in editing speed between our approach and architectures with self-attention becomes more significant with long videos.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTRV: Test-Time Reinforcement Learning for Vision Language Models</title>
<link>https://arxiv.org/abs/2510.06783</link>
<guid>https://arxiv.org/abs/2510.06783</guid>
<content:encoded><![CDATA[

arXiv:2510.06783v2 Announce Type: replace 
Abstract: Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model's output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model's output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets. Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions</title>
<link>https://arxiv.org/abs/2510.07828</link>
<guid>https://arxiv.org/abs/2510.07828</guid>
<content:encoded><![CDATA[

arXiv:2510.07828v3 Announce Type: replace 
Abstract: Real-world scenes often feature multiple humans interacting with multiple objects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D human-object interaction (HOI) benchmarks consider only a fraction of these complex interactions. To close this gap, we present MMHOI -- a large-scale, Multi-human Multi-object Interaction dataset consisting of images from 12 everyday scenarios. MMHOI offers complete 3D shape and pose annotations for every person and object, along with labels for 78 action categories and 14 interaction-specific body parts, providing a comprehensive testbed for next-generation HOI research. Building on MMHOI, we present MMHOI-Net, an end-to-end transformer-based neural network for jointly estimating human-object 3D geometries, their interactions, and associated actions. A key innovation in our framework is a structured dual-patch representation for modeling objects and their interactions, combined with action recognition to enhance the interaction prediction. Experiments on MMHOI and the recently proposed CORE4D datasets demonstrate that our approach achieves state-of-the-art performance in multi-HOI modeling, excelling in both accuracy and reconstruction quality. The MMHOI dataset is publicly available at https://zenodo.org/records/17711786.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training</title>
<link>https://arxiv.org/abs/2510.12586</link>
<guid>https://arxiv.org/abs/2510.12586</guid>
<content:encoded><![CDATA[

arXiv:2510.12586v2 Announce Type: replace 
Abstract: Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our framework achieves state-of-the-art (SOTA) performance on ImageNet. Specifically, our diffusion model reaches an FID of 1.58 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE) surpassing prior pixel-space methods and VAE-based counterparts by a large margin in both generation quality and training efficiency. In a direct comparison, our model significantly outperforms DiT while using only around 30\% of its training compute.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharp Eyes and Memory for VideoLLMs: Information-Aware Visual Token Pruning for Efficient and Reliable VideoLLM Reasoning</title>
<link>https://arxiv.org/abs/2511.08003</link>
<guid>https://arxiv.org/abs/2511.08003</guid>
<content:encoded><![CDATA[

arXiv:2511.08003v2 Announce Type: replace 
Abstract: Current Video Large Language Models (VideoLLMs) suffer from quadratic computational complexity and key-value cache scaling, due to their reliance on processing excessive redundant visual tokens. To address this problem, we propose SharpV, a minimalist and efficient method for adaptive pruning of visual tokens and KV cache. Different from most uniform compression approaches, SharpV dynamically adjusts pruning ratios based on spatial-temporal information. Remarkably, this adaptive mechanism occasionally achieves performance gains over dense models, offering a novel paradigm for adaptive pruning. During the KV cache pruning stage, based on observations of visual information degradation, SharpV prunes degraded visual features via a self-calibration manner, guided by similarity to original visual features. In this way, SharpV achieves hierarchical cache pruning from the perspective of information bottleneck, offering a new insight into VideoLLMs' information flow. Experiments on multiple public benchmarks demonstrate the superiority of SharpV. Moreover, to the best of our knowledge, SharpV is notably the first two-stage pruning framework that operates without requiring access to exposed attention scores, ensuring full compatibility with hardware acceleration techniques like Flash Attention.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOP-ASK: Object-Interaction Reasoning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.16857</link>
<guid>https://arxiv.org/abs/2511.16857</guid>
<content:encoded><![CDATA[

arXiv:2511.16857v2 Announce Type: replace 
Abstract: Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A lightweight detector for real-time detection of remote sensing images</title>
<link>https://arxiv.org/abs/2511.17147</link>
<guid>https://arxiv.org/abs/2511.17147</guid>
<content:encoded><![CDATA[

arXiv:2511.17147v2 Announce Type: replace 
Abstract: Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DE-KAN: A Kolmogorov Arnold Network with Dual Encoder for accurate 2D Teeth Segmentation</title>
<link>https://arxiv.org/abs/2511.18533</link>
<guid>https://arxiv.org/abs/2511.18533</guid>
<content:encoded><![CDATA[

arXiv:2511.18533v2 Announce Type: replace 
Abstract: Accurate segmentation of individual teeth from panoramic radiographs remains a challenging task due to anatomical variations, irregular tooth shapes, and overlapping structures. These complexities often limit the performance of conventional deep learning models. To address this, we propose DE-KAN, a novel Dual Encoder Kolmogorov Arnold Network, which enhances feature representation and segmentation precision. The framework employs a ResNet-18 encoder for augmented inputs and a customized CNN encoder for original inputs, enabling the complementary extraction of global and local spatial features. These features are fused through KAN-based bottleneck layers, incorporating nonlinear learnable activation functions derived from the Kolmogorov Arnold representation theorem to improve learning capacity and interpretability. Extensive experiments on two benchmark dental X-ray datasets demonstrate that DE-KAN outperforms state-of-the-art segmentation models, achieving mIoU of 94.5%, Dice coefficient of 97.1%, accuracy of 98.91%, and recall of 97.36%, representing up to +4.7% improvement in Dice compared to existing methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents</title>
<link>https://arxiv.org/abs/2511.18685</link>
<guid>https://arxiv.org/abs/2511.18685</guid>
<content:encoded><![CDATA[

arXiv:2511.18685v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) show promising results as decision-making engines for embodied agents operating in complex, physical environments. However, existing benchmarks often prioritize high-level planning or spatial reasoning, leaving the fine-grained action intelligence required for embodied physical interaction underexplored. To address this gap, we introduce CFG-Bench, a new benchmark designed to systematically evaluate this crucial capability. CFG-Bench consists of 1,368 curated videos paired with 19,562 three-modalities question-answer pairs targeting four cognitive abilities: 1) Physical Interaction, 2) Temporal-Causal Relation, 3) Intentional Understanding, and 4) Evaluative Judgment. Together, these dimensions provide a systematic framework for assessing a model's ability to translate visual observations into actionable knowledge, moving beyond mere surface-level recognition. Our comprehensive evaluation on CFG-Bench reveals that leading MLLMs struggle to produce detailed instructions for physical interactions and exhibit profound limitations in the higher-order reasoning of intention and evaluation. Moreover, supervised fine-tuning (SFT) on our data demonstrates that teaching an MLLMs to articulate fine-grained actions directly translates to significant performance gains on established embodied benchmarks. Our analysis highlights these limitations and offers insights for developing more capable and grounded embodied agents. Project page: \href{https://cfg-bench.github.io/}{https://cfg-bench.github.io/}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Changes in Gaza: DINOv3-Powered Multi-Class Change Detection for Damage Assessment in Conflict Zones</title>
<link>https://arxiv.org/abs/2511.19035</link>
<guid>https://arxiv.org/abs/2511.19035</guid>
<content:encoded><![CDATA[

arXiv:2511.19035v2 Announce Type: replace 
Abstract: Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. The multi-scale cross-attention mechanism allows for precise localization of subtle semantic changes, while the difference siamese structure enhances inter-class feature discrimination, enabling fine-grained semantic change detection. Furthermore, a simple yet powerful lightweight decoder is designed to generate clear detection maps while maintaining high efficiency. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. We evaluated our method on the Gaza-Change and two classical datasets: the SECOND and Landsat-SCD datasets. Experimental results demonstrate that our proposed approach effectively addresses the MCD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling</title>
<link>https://arxiv.org/abs/2511.20785</link>
<guid>https://arxiv.org/abs/2511.20785</guid>
<content:encoded><![CDATA[

arXiv:2511.20785v2 Announce Type: replace 
Abstract: Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables "Thinking with Long Videos" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?</title>
<link>https://arxiv.org/abs/2511.21523</link>
<guid>https://arxiv.org/abs/2511.21523</guid>
<content:encoded><![CDATA[

arXiv:2511.21523v2 Announce Type: replace 
Abstract: Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs. All codes and pretrained models are available at https://github.com/pierreadorni/EoS-FM.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImageNot: A contrast with ImageNet preserves model rankings</title>
<link>https://arxiv.org/abs/2404.02112</link>
<guid>https://arxiv.org/abs/2404.02112</guid>
<content:encoded><![CDATA[

arXiv:2404.02112v2 Announce Type: replace-cross 
Abstract: We introduce ImageNot, a dataset constructed explicitly to be drastically different than ImageNet while matching its scale. ImageNot is designed to test the external validity of deep learning progress on ImageNet. We show that key model architectures developed for ImageNet over the years rank identically to how they rank on ImageNet when trained from scratch and evaluated on ImageNot. Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets. Our work demonstrates a surprising degree of external validity in the relative performance of image classification models when trained and evaluated on an entirely different dataset. This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2407.11698</link>
<guid>https://arxiv.org/abs/2407.11698</guid>
<content:encoded><![CDATA[

arXiv:2407.11698v3 Announce Type: replace-cross 
Abstract: Quantization is a pivotal technique for managing the growing computational and memory demands of Deep Neural Networks (DNNs). By reducing the number of bits used to represent weights and activations (typically from 32-bit Floating-Point (FP) to 16-bit or 8-bit integers), quantization reduces memory footprint, energy consumption, and execution time of DNNs. However, most existing methods typically target DNN inference, while training still relies on FP operations, limiting applicability in environments where FP arithmetic is unavailable. To date, only one prior work has addressed integer-only training, and only for Multi-Layer Perceptron (MLP) architectures. This paper introduces NITRO-D, a novel framework for training deep integer-only Convolutional Neural Networks (CNNs) that operate entirely in the integer domain for both training and inference. NITRO-D enables training of integer CNNs without requiring a separate quantization scheme. Specifically, it introduces a novel architecture that integrates multiple local-loss blocks, which include the proposed NITRO-Scaling layer and NITRO-ReLU activation function. The proposed framework also features a novel learning algorithm that employs local error signals and leverages IntegerSGD, an optimizer specifically designed for integer computations. NITRO-D is implemented as an open-source Python library. Extensive evaluations on state-of-the-art image recognition datasets demonstrate its effectiveness. For integer-only MLPs, NITRO-D improves test accuracy by up to +5.96% over the state-of-the-art. It also successfully trains integer-only CNNs, reducing memory requirements and energy consumption by up to 76.14% and 32.42%, respectively, compared to the traditional FP backpropagation algorithm.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based Models</title>
<link>https://arxiv.org/abs/2501.07033</link>
<guid>https://arxiv.org/abs/2501.07033</guid>
<content:encoded><![CDATA[

arXiv:2501.07033v2 Announce Type: replace-cross 
Abstract: This study explores the use of Generative Adversarial Networks (GANs) to detect AI deepfakes and fraudulent activities in online payment systems. With the growing prevalence of deepfake technology, which can manipulate facial features in images and videos, the potential for fraud in online transactions has escalated. Traditional security systems struggle to identify these sophisticated forms of fraud. This research proposes a novel GAN-based model that enhances online payment security by identifying subtle manipulations in payment images. The model is trained on a dataset consisting of real-world online payment images and deepfake images generated using advanced GAN architectures, such as StyleGAN and DeepFake. The results demonstrate that the proposed model can accurately distinguish between legitimate transactions and deepfakes, achieving a high detection rate above 95%. This approach significantly improves the robustness of payment systems against AI-driven fraud. The paper contributes to the growing field of digital security, offering insights into the application of GANs for fraud detection in financial services. Keywords- Payment Security, Image Recognition, Generative Adversarial Networks, AI Deepfake, Fraudulent Activities
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ENTIRE: Learning-based Volume Rendering Time Prediction</title>
<link>https://arxiv.org/abs/2501.12119</link>
<guid>https://arxiv.org/abs/2501.12119</guid>
<content:encoded><![CDATA[

arXiv:2501.12119v2 Announce Type: replace-cross 
Abstract: We introduce ENTIRE, a novel deep learning-based approach for fast and accurate volume rendering time prediction. Predicting rendering time is inherently challenging due to its dependence on multiple factors, including volume data characteristics, image resolution, camera configuration, and transfer function settings. Our method addresses this by first extracting a feature vector that encodes structural volume properties relevant to rendering performance. This feature vector is then integrated with additional rendering parameters, such as image resolution, camera setup, and transfer function settings, to produce the final prediction. We evaluate ENTIRE across multiple rendering frameworks (CPU- and GPU-based) and configurations (with and without single-scattering) on diverse datasets. The results demonstrate that our model achieves high prediction accuracy with fast inference speed. Furthermore, we showcase ENTIRE's effectiveness in two case studies, where it enables dynamic parameter adaptation for stable frame rates and load balancing.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments</title>
<link>https://arxiv.org/abs/2505.19361</link>
<guid>https://arxiv.org/abs/2505.19361</guid>
<content:encoded><![CDATA[

arXiv:2505.19361v4 Announce Type: replace-cross 
Abstract: The deployment of pre-trained perception models in novel environments often leads to performance degradation due to distributional shifts. Although recent artificial intelligence approaches for metacognition use logical rules to characterize and filter model errors, improving precision often comes at the cost of reduced recall. This paper addresses the hypothesis that leveraging multiple pre-trained models can mitigate this recall reduction. We formulate the challenge of identifying and managing conflicting predictions from various models as a consistency-based abduction problem, building on the idea of abductive learning (ABL) but applying it to test-time instead of training. The input predictions and the learned error detection rules derived from each model are encoded in a logic program. We then seek an abductive explanation--a subset of model predictions--that maximizes prediction coverage while ensuring the rate of logical inconsistencies (derived from domain constraints) remains below a specified threshold. We propose two algorithms for this knowledge representation task: an exact method based on Integer Programming (IP) and an efficient Heuristic Search (HS). Through extensive experiments on a simulated aerial imagery dataset featuring controlled, complex distributional shifts, we demonstrate that our abduction-based framework outperforms individual models and standard ensemble baselines, achieving, for instance, average relative improvements of approximately 13.6\% in F1-score and 16.6\% in accuracy across 15 diverse test datasets when compared to the best individual model. Our results validate the use of consistency-based abduction as an effective mechanism to robustly integrate knowledge from multiple imperfect models in challenging, novel scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09532</link>
<guid>https://arxiv.org/abs/2506.09532</guid>
<content:encoded><![CDATA[

arXiv:2506.09532v4 Announce Type: replace-cross 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainPath: A Biologically-Informed AI Framework for Individualized Aging Brain Generation</title>
<link>https://arxiv.org/abs/2508.16667</link>
<guid>https://arxiv.org/abs/2508.16667</guid>
<content:encoded><![CDATA[

arXiv:2508.16667v3 Announce Type: replace-cross 
Abstract: The global population is aging rapidly, and aging is a major risk factor for various diseases. It is an important task to predict how each individual's brain will age, as the brain supports many human functions. This capability can greatly facilitate healthcare automation by enabling personalized, proactive intervention and efficient healthcare resource allocation. However, this task is extremely challenging because of the brain's complex 3D anatomy. While there have been successes in natural image generation and brain MRI synthesis, existing methods fall short in generating individualized, anatomically faithful aging brain trajectories. To address these gaps, we propose BrainPath, a novel AI model that, given a single structural MRI of an individual, generates synthetic longitudinal MRIs that represent that individual's expected brain anatomy as they age. BrainPath introduces three architectural innovations: an age-aware encoder with biologically grounded supervision, a differential age conditioned decoder for anatomically faithful MRI synthesis, and a swap-learning strategy that implicitly separates stable subject-specific anatomy from aging effects. We further design biologically informed loss functions, including an age calibration loss and an age and structural perceptual loss, to complement the conventional reconstruction loss. This enables the model to capture subtle, temporally meaningful anatomical changes associated with aging. We apply BrainPath to two of the largest public aging datasets and conduct a comprehensive, multifaceted evaluation. Our results demonstrate BrainPath's superior performance in generation accuracy, anatomical fidelity, and cross-dataset generalizability, outperforming competing methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignBind-LLM: Multi-Stage Modality Fusion for Sign Language Translation</title>
<link>https://arxiv.org/abs/2509.00030</link>
<guid>https://arxiv.org/abs/2509.00030</guid>
<content:encoded><![CDATA[

arXiv:2509.00030v3 Announce Type: replace-cross 
Abstract: Despite progress in gloss-free Sign Language Translation (SLT), traditional single modality end-to-end approaches consistently fail on two critical components of natural signing: the precise recognition of high-speed fingerspelling and the integration of asynchronous non-manual cues from the face. Recent progress in SLT with Large Language Models has side stepped this challenge, forcing a single network to learn these simultaneously resulting in poor performance when tasked with translating crucial information such as names, places, and technical terms. We introduce SignBind-LLM, a modular framework designed to overcome these limitations. Our approach employs separate, specialized predictors for continuous signing, fingerspelling, and lipreading. Each expert network first decodes its specific modality into a sequence of tokens. These parallel streams are then fused by a lightweight transformer that resolves temporal misalignments before passing the combined representation to a Large Language Model (LLM) for final sentence generation. Our method establishes a new state-of-the-art on the How2Sign, ChicagoFSWildPlus, and BOBSL datasets with a BLEU-4 score of 22.1, 73.2% letter accuracy and BLEU-4 score of 6.8 respectively. These results validate our core hypothesis: isolating and solving distinct recognition tasks before fusion provides a more powerful and effective pathway to robust, high-fidelity sign language translation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
<link>https://arxiv.org/abs/2509.04734</link>
<guid>https://arxiv.org/abs/2509.04734</guid>
<content:encoded><![CDATA[

arXiv:2509.04734v2 Announce Type: replace-cross 
Abstract: The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) supervised contrastive learning with Euclidean distance as the feature space metric is improved by replacing the standard loss function with Jenson-Shannon divergence (JSD); (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded $f$-divergence. Our results highlight the importance of considering divergence choices in representation learning optimization.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GigaBrain-0: A World Model-Powered Vision-Language-Action Model</title>
<link>https://arxiv.org/abs/2510.19430</link>
<guid>https://arxiv.org/abs/2510.19430</guid>
<content:encoded><![CDATA[

arXiv:2510.19430v3 Announce Type: replace-cross 
Abstract: Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceTrans: Translation and Spatial Tracing for Surgical Prediction</title>
<link>https://arxiv.org/abs/2510.22379</link>
<guid>https://arxiv.org/abs/2510.22379</guid>
<content:encoded><![CDATA[

arXiv:2510.22379v4 Announce Type: replace-cross 
Abstract: Image-to-image translation models have achieved notable success in converting images across visual domains and are increasingly used for medical tasks such as predicting post-operative outcomes and modeling disease progression. However, most existing methods primarily aim to match the target distribution and often neglect spatial correspondences between the source and translated images. This limitation can lead to structural inconsistencies and hallucinations, undermining the reliability and interpretability of the predictions. These challenges are accentuated in clinical applications by the stringent requirement for anatomical accuracy. In this work, we present TraceTrans, a novel deformable image translation model designed for post-operative prediction that generates images aligned with the target distribution while explicitly revealing spatial correspondences with the pre-operative input. The framework employs an encoder for feature extraction and dual decoders for predicting spatial deformations and synthesizing the translated image. The predicted deformation field imposes spatial constraints on the generated output, ensuring anatomical consistency with the source. Extensive experiments on medical cosmetology and brain MRI datasets demonstrate that TraceTrans delivers accurate and interpretable post-operative predictions, highlighting its potential for reliable clinical deployment.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VEDA: 3D Molecular Generation via Variance-Exploding Diffusion with Annealing</title>
<link>https://arxiv.org/abs/2511.09568</link>
<guid>https://arxiv.org/abs/2511.09568</guid>
<content:encoded><![CDATA[

arXiv:2511.09568v2 Announce Type: replace-cross 
Abstract: Diffusion models show promise for 3D molecular generation, but face a fundamental trade-off between sampling efficiency and conformational accuracy. While flow-based models are fast, they often produce geometrically inaccurate structures, as they have difficulty capturing the multimodal distributions of molecular conformations. In contrast, denoising diffusion models are more accurate but suffer from slow sampling, a limitation attributed to sub-optimal integration between diffusion dynamics and SE(3)-equivariant architectures. To address this, we propose VEDA, a unified SE(3)-equivariant framework that combines variance-exploding diffusion with annealing to efficiently generate conformationally accurate 3D molecular structures. Specifically, our key technical contributions include: (1) a VE schedule that enables noise injection functionally analogous to simulated annealing, improving 3D accuracy and reducing relaxation energy; (2) a novel preconditioning scheme that reconciles the coordinate-predicting nature of SE(3)-equivariant networks with a residual-based diffusion objective, and (3) a new arcsin-based scheduler that concentrates sampling in critical intervals of the logarithmic signal-to-noise ratio. On the QM9 and GEOM-DRUGS datasets, VEDA matches the sampling efficiency of flow-based models, achieving state-of-the-art valency stability and validity with only 100 sampling steps. More importantly, VEDA's generated structures are remarkably stable, as measured by their relaxation energy during GFN2-xTB optimization. The median energy change is only 1.72 kcal/mol, significantly lower than the 32.3 kcal/mol from its architectural baseline, SemlaFlow. Our framework demonstrates that principled integration of VE diffusion with SE(3)-equivariant architectures can achieve both high chemical accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Hierarchical Process Reward Models are Symbolic Vision Learners</title>
<link>https://arxiv.org/abs/2512.03126</link>
<guid>https://arxiv.org/abs/2512.03126</guid>
<content:encoded><![CDATA[
<div> Keywords: symbolic computer vision, self-supervised auto-encoder, hierarchical process reward, neuro-symbolic system, diagram reconstruction  

<br /><br />Summary:  
1. The paper introduces a novel approach in symbolic computer vision that models diagrams using explicit logical rules and structured primitives such as points, lines, and shapes, contrasting with conventional pixel-based methods that rely on textures and colors.  
2. A self-supervised symbolic auto-encoder is proposed, which encodes diagrams into structured geometric primitives and their relationships and decodes them via an executable engine to faithfully reconstruct the original diagrams.  
3. Central to the architecture is the Symbolic Hierarchical Process Reward Modeling, a hierarchical, step-level reward system that enforces consistency rules like point-on-line, line-on-shape, and shape-on-relation during parsing and reconstruction.  
4. The authors address poor exploration challenges in reinforcement learning by integrating stabilization mechanisms that balance exploration and exploitation in the policy space for improved diagram reconstruction.  
5. Their approach extends to fine-tuning the symbolic encoder for downstream tasks, resulting in a neuro-symbolic system that merges neural network reasoning with symbolic interpretability using reasoning-grounded visual rewards.  
6. Experimental results demonstrate significant improvements: a 98.2% reduction in mean squared error for geometric diagram reconstruction, outperforming GPT-4o by 0.6% in chart reconstruction with a 7B model, and notable enhancements on perception and reasoning benchmarks like MathGlance (+13%), MathVerse (+3%), and GeoQA (+3%). <div>
arXiv:2512.03126v1 Announce Type: new 
Abstract: Symbolic computer vision represents diagrams through explicit logical rules and structured representations, enabling interpretable understanding in machine vision. This requires fundamentally different learning paradigms from pixel-based visual models. Symbolic visual learners parse diagrams into geometric primitives-points, lines, and shapes-whereas pixel-based learners operate on textures and colors. We propose a novel self-supervised symbolic auto-encoder that encodes diagrams into structured primitives and their interrelationships within the latent space, and decodes them through our executable engine to reconstruct the input diagrams. Central to this architecture is Symbolic Hierarchical Process Reward Modeling, which applies hierarchical step-level parsing rewards to enforce point-on-line, line-on-shape, and shape-on-relation consistency. Since vanilla reinforcement learning exhibits poor exploration in the policy space during diagram reconstruction; we thus introduce stabilization mechanisms to balance exploration and exploitation. We fine-tune our symbolic encoder on downstream tasks, developing a neuro-symbolic system that integrates the reasoning capabilities of neural networks with the interpretability of symbolic models through reasoning-grounded visual rewards. Evaluations across reconstruction, perception, and reasoning tasks demonstrate the effectiveness of our approach: achieving a 98.2% reduction in MSE for geometric diagram reconstruction, surpassing GPT-4o by 0.6% with a 7B model on chart reconstruction, and improving by +13% on the MathGlance perception benchmark, and by +3% on MathVerse and GeoQA reasoning benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drainage: A Unifying Framework for Addressing Class Uncertainty</title>
<link>https://arxiv.org/abs/2512.03182</link>
<guid>https://arxiv.org/abs/2512.03182</guid>
<content:encoded><![CDATA[
<div> Keywords: noisy labels, drainage node, instance-dependent noise, open-set recognition, semi-supervised cleaning

<br /><br />Summary:  
This paper addresses challenges in deep learning related to noisy labels, class ambiguity, and the rejection of out-of-distribution or corrupted samples. The authors introduce a novel concept called the "drainage node," an additional output node in neural networks designed to reallocate probability mass toward uncertainty. This facilitates better handling of ambiguous, anomalous, or noisy inputs while maintaining end-to-end differentiability and training efficiency. Extensive experiments on CIFAR-10 and CIFAR-100 datasets with varying levels of instance-dependent and asymmetric noise demonstrate up to a 9% accuracy improvement over state-of-the-art methods in high noise scenarios. Real-world validations on datasets like mini-WebVision, mini-ImageNet, and Clothing-1M show competitive or superior performance compared to existing approaches. The drainage node helps absorb corrupted, mislabeled, or outlier samples, which stabilizes decision boundaries and produces a denoising effect. Moreover, the framework extends beyond classification, offering practical benefits for large-scale semi-supervised dataset cleaning and open-set recognition tasks. This unified solution thus provides a robust method for improving resilience and accuracy in challenging noise conditions in deep learning settings. <div>
arXiv:2512.03182v1 Announce Type: new 
Abstract: Modern deep learning faces significant challenges with noisy labels, class ambiguity, as well as the need to robustly reject out-of-distribution or corrupted samples. In this work, we propose a unified framework based on the concept of a "drainage node'' which we add at the output of the network. The node serves to reallocate probability mass toward uncertainty, while preserving desirable properties such as end-to-end training and differentiability. This mechanism provides a natural escape route for highly ambiguous, anomalous, or noisy samples, particularly relevant for instance-dependent and asymmetric label noise. In systematic experiments involving the addition of varying proportions of instance-dependent noise or asymmetric noise to CIFAR-10/100 labels, our drainage formulation achieves an accuracy increase of up to 9\% over existing approaches in the high-noise regime. Our results on real-world datasets, such as mini-WebVision, mini-ImageNet and Clothing-1M, match or surpass existing state-of-the-art methods. Qualitative analysis reveals a denoising effect, where the drainage neuron consistently absorbs corrupt, mislabeled, or outlier data, leading to more stable decision boundaries. Furthermore, our drainage formulation enables applications well beyond classification, with immediate benefits for web-scale, semi-supervised dataset cleaning, and open-set applications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Head Pose Correction Improve Biometric Facial Recognition?</title>
<link>https://arxiv.org/abs/2512.03199</link>
<guid>https://arxiv.org/abs/2512.03199</guid>
<content:encoded><![CDATA[
<div> Keywords: facial recognition, head-pose correction, image restoration, CFR-GAN, CodeFormer  

<br /><br />Summary:  
This study addresses the challenge that biometric facial recognition systems face when dealing with real-world images that are often low quality, exhibit non-frontal poses, and include occlusions. The authors explore whether AI-driven techniques aimed at head-pose correction and image restoration can enhance recognition accuracy. They employ a large-scale, model-agnostic forensic-evaluation pipeline to rigorously test three specific restoration methods: 3D reconstruction using NextFace, 2D frontalization through CFR-GAN, and feature enhancement via CodeFormer. Their results reveal that the straightforward, indiscriminate application of any of these restoration techniques actually harms facial recognition performance. However, they identify that a carefully targeted combination of CFR-GAN for 2D frontalization and CodeFormer for feature enhancement can lead to meaningful improvements in recognition accuracy. Thus, this work highlights the importance of selective application and integration of restoration technologies rather than naive usage, contributing valuable insights for improving real-world biometric facial recognition systems. <div>
arXiv:2512.03199v1 Announce Type: new 
Abstract: Biometric facial recognition models often demonstrate significant decreases in accuracy when processing real-world images, often characterized by poor quality, non-frontal subject poses, and subject occlusions. We investigate whether targeted, AI-driven, head-pose correction and image restoration can improve recognition accuracy. Using a model-agnostic, large-scale, forensic-evaluation pipeline, we assess the impact of three restoration approaches: 3D reconstruction (NextFace), 2D frontalization (CFR-GAN), and feature enhancement (CodeFormer). We find that naive application of these techniques substantially degrades facial recognition accuracy. However, we also find that selective application of CFR-GAN combined with CodeFormer yields meaningful improvements.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flux4D: Flow-based Unsupervised 4D Reconstruction</title>
<link>https://arxiv.org/abs/2512.03210</link>
<guid>https://arxiv.org/abs/2512.03210</guid>
<content:encoded><![CDATA[
<div> Keywords: Flux4D, 4D reconstruction, 3D Gaussian Splatting, unsupervised learning, dynamic scenes  

<br /><br />Summary:  
This paper introduces Flux4D, a novel framework designed for scalable 4D reconstruction of large-scale dynamic scenes from visual input. Unlike previous differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting, which are limited by scalability and require annotations to separate actor motion, Flux4D operates in a fully unsupervised manner without reliance on pre-trained models or foundational priors. It directly predicts 3D Gaussians and their motion dynamics using only photometric losses combined with an "as static as possible" regularization. This enables the model to decompose dynamic elements automatically by training across multiple scenes, avoiding the need for per-scene optimization and heavy hyperparameter tuning common in existing self-supervised methods. Flux4D demonstrates high efficiency, reconstructing dynamic scenes within seconds while scaling well to large datasets. Moreover, it generalizes effectively to unseen environments, including rare and unknown objects, addressing challenges in outdoor driving scenarios. Experimental results show that Flux4D significantly outperforms state-of-the-art methods in terms of scalability, reconstruction quality, and generalization, highlighting its potential for applications in robotics and autonomous systems where dynamic scene understanding is essential. <div>
arXiv:2512.03210v1 Announce Type: new 
Abstract: Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an "as static as possible" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object Counting with GPT-4o and GPT-5: A Comparative Study</title>
<link>https://arxiv.org/abs/2512.03233</link>
<guid>https://arxiv.org/abs/2512.03233</guid>
<content:encoded><![CDATA[
<div> Zero-shot object counting, large language models, multi-modal LLMs, GPT-4o, GPT-5

<br /><br />Summary:  
This paper addresses the task of zero-shot object counting, which involves estimating the number of object instances in novel categories without prior training on those categories. Traditional methods rely heavily on large annotated datasets and visual exemplars to guide the counting process, which can be labor-intensive and limiting. The authors propose leveraging the visual and reasoning capabilities of cutting-edge multi-modal large language models, specifically GPT-4o and GPT-5, to perform object counting using only textual prompts and no supervision. They conduct evaluations on two established datasets, FSC-147 and CARPK, to benchmark these models’ performance against current zero-shot counting approaches. The results demonstrate that the multi-modal LLMs achieve comparable accuracy to state-of-the-art zero-shot methods on FSC-147 and, in some cases, even outperform them. This suggests the promising potential of incorporating advanced multi-modal LLMs in computer vision tasks like object counting, reducing dependency on large labeled datasets and visual exemplars. The study highlights the expanded capabilities of LLMs beyond natural language understanding, emphasizing their integrative reasoning and data comprehension power in zero-shot scenarios. <div>
arXiv:2512.03233v1 Announce Type: new 
Abstract: Zero-shot object counting attempts to estimate the number of object instances belonging to novel categories that the vision model performing the counting has never encountered during training. Existing methods typically require large amount of annotated data and often require visual exemplars to guide the counting process. However, large language models (LLMs) are powerful tools with remarkable reasoning and data understanding abilities, which suggest the possibility of utilizing them for counting tasks without any supervision. In this work we aim to leverage the visual capabilities of two multi-modal LLMs, GPT-4o and GPT-5, to perform object counting in a zero-shot manner using only textual prompts. We evaluate both models on the FSC-147 and CARPK datasets and provide a comparative analysis. Our findings show that the models achieve performance comparable to the state-of-the-art zero-shot approaches on FSC-147, in some cases, even surpass them.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Guided Material Inference for 3D Point Clouds</title>
<link>https://arxiv.org/abs/2512.03237</link>
<guid>https://arxiv.org/abs/2512.03237</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D shape, material inference, large language model, zero-shot learning, semantic reasoning<br /><br />Summary:<br /><br />This paper addresses the limitation of current 3D shape datasets and models that predominantly focus on geometry while neglecting material properties crucial for object appearance. The authors propose a novel two-stage method utilizing large language models (LLMs) to infer material composition directly from 3D point clouds that include coarse segmentations. The key innovation lies in decoupling the reasoning process: the first stage uses an LLM to predict the object's semantic category, and the second stage assigns plausible materials to each geometric segment based on the semantics inferred earlier. Both stages function in a zero-shot fashion, eliminating the need for task-specific training data. To evaluate their approach, the authors use an LLM-based judge implemented within DeepEval, compensating for the lack of reliable material annotations in existing datasets. Empirical results on 1,000 shapes drawn from Fusion/ABS and ShapeNet demonstrate the method’s high performance regarding semantic correctness and material plausibility. Overall, the study showcases how language models can act as versatile priors that bridge the gap between geometric understanding and material reasoning in 3D data analysis. <div>
arXiv:2512.03237v1 Announce Type: new 
Abstract: Most existing 3D shape datasets and models focus solely on geometry, overlooking the material properties that determine how objects appear. We introduce a two-stage large language model (LLM) based method for inferring material composition directly from 3D point clouds with coarse segmentations. Our key insight is to decouple reasoning about what an object is from what it is made of. In the first stage, an LLM predicts the object's semantic; in the second stage, it assigns plausible materials to each geometric segment, conditioned on the inferred semantics. Both stages operate in a zero-shot manner, without task-specific training. Because existing datasets lack reliable material annotations, we evaluate our method using an LLM-as-a-Judge implemented in DeepEval. Across 1,000 shapes from Fusion/ABS and ShapeNet, our method achieves high semantic and material plausibility. These results demonstrate that language models can serve as general-purpose priors for bridging geometric reasoning and material understanding in 3D data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>2-Shots in the Dark: Low-Light Denoising with Minimal Data Acquisition</title>
<link>https://arxiv.org/abs/2512.03245</link>
<guid>https://arxiv.org/abs/2512.03245</guid>
<content:encoded><![CDATA[
<div> keywords: noise synthesis, low-light denoising, Poisson distribution, Fourier-domain spectral sampling, sensor noise

<br /><br />Summary: This article addresses the challenge of denoising raw images taken under low-light conditions, where noise arises from low photon counts and sensor imperfections. It highlights the limitations of learning-based denoisers, which typically require large paired datasets of clean and noisy images that are difficult to obtain. To overcome this, the authors propose a novel noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. Their approach models signal-dependent noise using a Poisson distribution, while signal-independent noise is accurately captured through a Fourier-domain spectral sampling algorithm. This method generates diverse noise patterns that preserve the spatial and statistical characteristics of real sensor noise. Unlike existing approaches, it avoids reliance on simplified parametric noise models or extensive clean-noisy image pairs. The paper demonstrates that the proposed noise synthesis method is both practical and accurate, leading to state-of-the-art performance on several low-light denoising benchmarks. Overall, the contribution offers a scalable and effective alternative for noise modeling and synthesis in image denoising tasks, particularly under challenging low-light scenarios. <div>
arXiv:2512.03245v1 Announce Type: new 
Abstract: Raw images taken in low-light conditions are very noisy due to low photon count and sensor noise. Learning-based denoisers have the potential to reconstruct high-quality images. For training, however, these denoisers require large paired datasets of clean and noisy images, which are difficult to collect. Noise synthesis is an alternative to large-scale data acquisition: given a clean image, we can synthesize a realistic noisy counterpart. In this work, we propose a general and practical noise synthesis method that requires only one single noisy image and one single dark frame per ISO setting. We represent signal-dependent noise with a Poisson distribution and introduce a Fourier-domain spectral sampling algorithm to accurately model signal-independent noise. The latter generates diverse noise realizations that maintain the spatial and statistical properties of real sensor noise. As opposed to competing approaches, our method neither relies on simplified parametric models nor on large sets of clean-noisy image pairs. Our synthesis method is not only accurate and practical, it also leads to state-of-the-art performances on multiple low-light denoising benchmarks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement</title>
<link>https://arxiv.org/abs/2512.03247</link>
<guid>https://arxiv.org/abs/2512.03247</guid>
<content:encoded><![CDATA[
<div> Latent Diffusion Models, image inpainting, pixel-level refinement, artifact simulation, local editing<br /><br />Summary:<br /><br />Latent Diffusion Models (LDMs) have significantly improved image inpainting and local editing, but their latent compression often leads to pixel-level artifacts such as chromatic shifts, texture mismatches, and seam visibility at editing boundaries. Existing solutions, like background-conditioned latent decoding and pixel-space harmonization, fail to fully remove these artifacts and lack generalizability across different latent representations and editing tasks. To address this, the paper introduces PixPerfect, a novel pixel-level refinement framework designed to provide seamless and high-fidelity local edits compatible with various LDM architectures and tasks. PixPerfect operates through three main components: a differentiable discriminative pixel space that highlights subtle color and texture inconsistencies for effective correction, an artifact simulation pipeline that trains the system on realistic editing errors, and a direct pixel-space refinement method ensuring broad applicability. Extensive experiments conducted on benchmarks including inpainting, object removal, and insertion reveal that PixPerfect greatly improves perceptual quality and editing performance. The approach establishes a new baseline for robust, high-quality localized image editing across diverse settings, overcoming the limitations of previous methods and enhancing the visual coherence of edited images. <div>
arXiv:2512.03247v1 Announce Type: new 
Abstract: Latent Diffusion Models (LDMs) have markedly advanced the quality of image inpainting and local editing. However, the inherent latent compression often introduces pixel-level inconsistencies, such as chromatic shifts, texture mismatches, and visible seams along editing boundaries. Existing remedies, including background-conditioned latent decoding and pixel-space harmonization, usually fail to fully eliminate these artifacts in practice and do not generalize well across different latent representations or tasks. We introduce PixPerfect, a pixel-level refinement framework that delivers seamless, high-fidelity local edits across diverse LDM architectures and tasks. PixPerfect leverages (i) a differentiable discriminative pixel space that amplifies and suppresses subtle color and texture discrepancies, (ii) a comprehensive artifact simulation pipeline that exposes the refiner to realistic local editing artifacts during training, and (iii) a direct pixel-space refinement scheme that ensures broad applicability across diverse latent representations and tasks. Extensive experiments on inpainting, object removal, and insertion benchmarks demonstrate that PixPerfect substantially enhances perceptual fidelity and downstream editing performance, establishing a new standard for robust and high-fidelity localized image editing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2512.03257</link>
<guid>https://arxiv.org/abs/2512.03257</guid>
<content:encoded><![CDATA[
<div> wildfire detection, deep learning, onboard processing, fire radiative power, multi-class classification<br /><br />Summary:<br /><br />1. The study addresses the critical need for rapid and accurate wildfire detection in airborne and spaceborne platforms, emphasizing real-time distinction between no fire, active fire, and post-fire conditions alongside fire intensity estimation.<br /><br />2. It highlights challenges posed by multispectral and hyperspectral thermal imagers, which provide detailed spectral data but demand computationally efficient algorithms due to high dimensionality and limited onboard resources.<br /><br />3. Multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, are systematically evaluated for multi-class fire classification tasks.<br /><br />4. The authors propose PyroFocus, a novel two-stage pipeline that first classifies fire presence and then performs either fire radiative power regression or fire segmentation to optimize inference speed and reduce computational burden.<br /><br />5. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), they demonstrate that PyroFocus achieves an effective balance between accuracy and latency, confirming its suitability for real-time edge deployment in future wildfire monitoring missions. <div>
arXiv:2512.03257v1 Announce Type: new 
Abstract: Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.
  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.
  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialReasoner: Active Perception for Large-Scale 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2512.03284</link>
<guid>https://arxiv.org/abs/2512.03284</guid>
<content:encoded><![CDATA[
<div> Spatial reasoning, 3D visual question answering, multi-floor environments, active perception, reinforcement learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of spatial reasoning in large-scale 3D environments for vision-language models, which are commonly limited to room-scale settings.  
2. It introduces H$^2$U3D (Holistic House Understanding in 3D), a novel 3D visual question answering dataset featuring complex house-scale environments with up to three floors, 10-20 rooms, and spanning more than 300 m².  
3. The dataset is created using an automated annotation pipeline that generates hierarchical coarse-to-fine visual representations and diverse question-answer pairs enriched with chain-of-thought annotations.  
4. The authors propose SpatialReasoner, an active perception framework that autonomously leverages spatial tools to explore 3D scenes conditioned on textual queries.  
5. SpatialReasoner is trained in two stages: a supervised cold start phase followed by reinforcement learning guided by an adaptive exploration reward that encourages exploration efficiency and avoids redundant actions.  
6. Extensive experiments demonstrate that SpatialReasoner outperforms strong baselines such as GPT-4o and Gemini-2.5-Pro on the H$^2$U3D dataset.  
7. Notably, SpatialReasoner achieves superior results while using only 3-4 images on average, compared to baselines which require 16 or more images, underscoring the effectiveness of the proposed coarse-to-fine active exploration approach. <div>
arXiv:2512.03284v1 Announce Type: new 
Abstract: Spatial reasoning in large-scale 3D environments remains challenging for current vision-language models, which are typically constrained to room-scale scenarios. We introduce H$^2$U3D (Holistic House Understanding in 3D), a 3D visual question answering dataset designed for house-scale scene understanding. H$^2$U3D features multi-floor environments spanning up to three floors and 10-20 rooms, covering more than 300 m$^2$. Through an automated annotation pipeline, it constructs hierarchical coarse-to-fine visual representations and generates diverse question-answer pairs with chain-of-thought annotations. We further propose SpatialReasoner, an active perception framework that autonomously invokes spatial tools to explore 3D scenes based on textual queries. SpatialReasoner is trained through a two-stage strategy: a supervised cold start followed by reinforcement learning with an adaptive exploration reward that promotes efficient exploration while discouraging redundant operations. Extensive experiments demonstrate that SpatialReasoner achieves state-of-the-art performance on H$^2$U3D, outperforming strong baselines including GPT-4o and Gemini-2.5-Pro. Notably, our method attains superior results while using only 3-4 images in total on average, compared to baselines requiring 16+ images, highlighting the effectiveness of our coarse-to-fine active exploration paradigm.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction</title>
<link>https://arxiv.org/abs/2512.03317</link>
<guid>https://arxiv.org/abs/2512.03317</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous driving, map fusion, diffusion models, online map construction, sensor data<br /><br />Summary:<br /><br />This paper introduces NavMapFusion, a novel diffusion-based framework designed for online construction of accurate environmental maps crucial for autonomous driving. Traditional high-definition (HD) maps provide detailed static road infrastructure but may become outdated, necessitating online updates using on-board sensor data. NavMapFusion leverages standard-definition (SD) navigation maps, which are widely available but low-resolution, as coarse priors to guide the mapping process. The key innovation is modeling discrepancies between prior maps and real-time sensor data as noise within the diffusion model's iterative denoising process. This approach naturally suppresses outdated map regions while reinforcing consistent areas, enhancing robustness in map fusion. The paper addresses two core questions: how outdated coarse maps can effectively guide online map construction, and what benefits diffusion models bring to this fusion task. Experimental results on the nuScenes benchmark show a significant 21.4% relative improvement at 100 meters range when fusing OpenStreetMap road line priors with sensor data, with even higher gains at greater perception distances, all while preserving real-time processing capabilities. This fusion strategy produces accurate, up-to-date environmental representations, ultimately contributing to safer and more reliable autonomous navigation. The authors have made their code publicly available to facilitate further research and application. <div>
arXiv:2512.03317v1 Announce Type: new 
Abstract: Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-by-step Layered Design Generation</title>
<link>https://arxiv.org/abs/2512.03335</link>
<guid>https://arxiv.org/abs/2512.03335</guid>
<content:encoded><![CDATA[
<div> Keywords: design generation, step-by-step process, layered changes, multi-modal LLMs, benchmark dataset<br /><br />Summary:<br /><br />1. The paper identifies a mismatch between the real-world step-by-step nature of the design process and existing AI approaches that treat design generation as a single-step task. 2. To address this, the authors propose the new problem formulation called Step-by-Step Layered Design Generation, which requires a model to generate designs progressively based on a sequence of designer instructions. 3. They introduce SLEDGE (Step-by-step LayEred Design GEnerator), a method leveraging multi-modal large language models (LLMs) to produce incremental, atomic layered changes grounded on the given instructions, effectively modeling each design update as a modification over the previous state. 4. A new evaluation suite is created, comprising a specialized dataset and benchmark tailored to this stepwise layered design generation task, enabling rigorous assessment of model performance in this novel setting. 5. Extensive experiments demonstrate that SLEDGE outperforms existing state-of-the-art approaches adapted to the new problem, highlighting the importance and effectiveness of modeling design as a sequential, layered generation process. The work aims to stimulate further research in this practical and underexplored area of design synthesis. <div>
arXiv:2512.03335v1 Announce Type: new 
Abstract: Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography</title>
<link>https://arxiv.org/abs/2512.03339</link>
<guid>https://arxiv.org/abs/2512.03339</guid>
<content:encoded><![CDATA[
<div> Ejection fraction, prototype learning, video-based model, explainability, cardiac function<br /><br />Summary:<br /><br />1. Ejection fraction (EF) is a vital clinical metric used to evaluate cardiac function and diagnose heart failure, but traditional EF assessment involves manual tracing and expert judgment, which is time-consuming and variable across observers.<br />2. Most existing deep learning approaches for EF prediction operate as black-box models, lacking transparency and thus limiting clinical trust and adoption.<br />3. Post-hoc explainability methods attempt to interpret deep learning outcomes after prediction but fail to influence or improve the model’s internal reasoning, resulting in limited clinical reliability.<br />4. ProtoEFNet is introduced as a novel video-based prototype learning model designed for continuous EF regression, which learns dynamic spatiotemporal prototypes representing meaningful cardiac motion patterns.<br />5. A new Prototype Angular Separation (PAS) loss is proposed to enforce discriminative representations along the continuous EF scale, enhancing model performance.<br />6. Experiments on the EchonetDynamic dataset demonstrate that ProtoEFNet matches the accuracy of non-interpretable models while providing improved clinical insights.<br />7. An ablation study shows that adding the PAS loss increases the F1 score by approximately 2%, from 77.67±2.68 to 79.64±2.10.<br />8. The source code for ProtoEFNet is publicly available, promoting transparency and reproducibility in EF prediction research. <div>
arXiv:2512.03339v1 Announce Type: new 
Abstract: Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\pm$2.68 to 79.64$\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluGen: Synthesizing Realistic and Controllable Hallucinations for Evaluating Image Restoration</title>
<link>https://arxiv.org/abs/2512.03345</link>
<guid>https://arxiv.org/abs/2512.03345</guid>
<content:encoded><![CDATA[
<div> Keywords: hallucinations, image restoration, low-field MRI, diffusion models, hallucination detection<br /><br />Summary:  
Generative models used in image restoration often produce hallucinations—plausible but incorrect image structures absent from the ground truth—which pose significant risks in safety-critical areas like medical imaging, industrial inspection, and remote sensing. This challenge is particularly acute in low-field MRI enhancement, prevalent in resource-limited settings, where hallucinations can cause serious diagnostic errors. A key limitation in addressing hallucinations is the dependency on labeled data for evaluation, which is costly and subjective. To overcome this, the authors introduce HalluGen, a diffusion-based framework capable of synthesizing realistic hallucinations with controllable attributes such as type, location, and severity, resulting in perceptually realistic yet semantically incorrect images (demonstrated by segmentation IoU dropping from 0.86 to 0.36). Using HalluGen, they create the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MRI scans for low-field MRI enhancement. This dataset enables systematic evaluation of hallucination detection and mitigation methods. The work demonstrates two main applications: (1) benchmarking image quality metrics and proposing SHAFE, a feature-based metric with soft-attention pooling that improves hallucination sensitivity beyond traditional metrics, and (2) training reference-free hallucination detectors that generalize well to real-world restoration failures. Together, HalluGen and the dataset provide the first scalable foundation for evaluating hallucinations in safety-critical image restoration tasks. <div>
arXiv:2512.03345v1 Announce Type: new 
Abstract: Generative models are prone to hallucinations: plausible but incorrect structures absent in the ground truth. This issue is problematic in image restoration for safety-critical domains such as medical imaging, industrial inspection, and remote sensing, where such errors undermine reliability and trust. For example, in low-field MRI, widely used in resource-limited settings, restoration models are essential for enhancing low-quality scans, yet hallucinations can lead to serious diagnostic errors. Progress has been hindered by a circular dependency: evaluating hallucinations requires labeled data, yet such labels are costly and subjective. We introduce HalluGen, a diffusion-based framework that synthesizes realistic hallucinations with controllable type, location, and severity, producing perceptually realistic but semantically incorrect outputs (segmentation IoU drops from 0.86 to 0.36). Using HalluGen, we construct the first large-scale hallucination dataset comprising 4,350 annotated images derived from 1,450 brain MR images for low-field enhancement, enabling systematic evaluation of hallucination detection and mitigation. We demonstrate its utility in two applications: (1) benchmarking image quality metrics and developing Semantic Hallucination Assessment via Feature Evaluation (SHAFE), a feature-based metric with soft-attention pooling that improves hallucination sensitivity over traditional metrics; and (2) training reference-free hallucination detectors that generalize to real restoration failures. Together, HalluGen and its open dataset establish the first scalable foundation for evaluating hallucinations in safety-critical image restoration.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Attention for Sparse Volumetric Anomaly Detection in Subclinical Keratoconus</title>
<link>https://arxiv.org/abs/2512.03346</link>
<guid>https://arxiv.org/abs/2512.03346</guid>
<content:encoded><![CDATA[
<div> Keywords: hierarchical attention, volumetric medical imaging, subclinical keratoconus, deep learning architectures, anomaly detection<br /><br />Summary:  
This study addresses the challenge of detecting subtle, spatially distributed anomalies in volumetric medical imaging, focusing on early disease signals that are often missed due to suboptimal model architectures. It compares sixteen modern deep learning architectures, including 2D/3D CNNs, hybrid models, and volumetric transformers, specifically for subclinical keratoconus detection using 3D anterior segment OCT volumes. The research finds that hierarchical attention models deliver superior sensitivity and specificity (21-23% improvement) compared to traditional CNNs and Vision Transformers by better matching the spatial scale of sparse anomalies. This improvement is attributed to hierarchical windowing, which creates receptive fields aligned with the intermediate range of subclinical abnormalities, avoiding drawbacks of excessive locality in CNNs and the overly global attention in standard ViTs. Measurements of attention-distance reveal that spatial integration requirements vary with signal strength, necessitating longer integration for subclinical cases than for healthy or manifest disease states. Additional analyses, including representational similarity and auxiliary tasks such as age and sex prediction, affirm the broad applicability and robustness of these inductive biases. The study thus provides important design insights for future volumetric anomaly detection systems, highlighting hierarchical attention as an effective architectural strategy for early pathological change detection in 3D medical imaging. <div>
arXiv:2512.03346v1 Announce Type: new 
Abstract: The detection of weak, spatially distributed anomalies in volumetric medical imaging remains a major challenge. The subtle, non-adjacent nature of early disease signals is often lost due to suboptimal architectural inductive biases: 2D/3D CNNs impose strong locality, while ViTs diffuse unconstrained global attention. This conflict leaves the optimal inductive structure for robust, sparse volumetric pattern recognition unresolved. This study presents a controlled comparison of sixteen modern deep learning architectures spanning 2D/3D convolutional, hybrid, and volumetric transformer families for subclinical keratoconus (SKC) detection from 3D anterior segment OCT volumes. We demonstrate that hierarchical attention models offer a superior and more parameter-efficient inductive bias, surpassing the performance of both 2D and 3D CNNs and ViTs. Our results show 21-23% higher sensitivity and specificity in the sparse anomaly (subclinical) regime. Mechanistic analyses reveal that this advantage stems from precise spatial scale alignment: hierarchical windowing produces effective receptive fields matched to the intermediate, multi-slice extent of subclinical abnormalities. This avoids excessive CNN locality and diffuse global attention. Attention-distance measurements confirm a key insight into architectural adaptation: the required spatial integration length shifts significantly based on the signal strength, with subclinical cases necessitating longer integration compared to both healthy and manifest disease states. Representational similarity and auxiliary age/sex prediction tasks further support the generalizability of these inductive principles. The findings provide design guidance for future volumetric anomaly detection systems, establishing hierarchical attention as a principled and effective approach for early pathological change analysis in 3D medical imaging.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation</title>
<link>https://arxiv.org/abs/2512.03350</link>
<guid>https://arxiv.org/abs/2512.03350</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D dynamics, visual generation, 2D to 4D reconstruction, continuous modeling, video editing<br /><br />Summary:  
The paper introduces SeeU, a novel method designed to improve visual understanding and generation by leveraging the continuous 4D dynamics of the world, incorporating 3D spatial information plus time rather than operating solely on 2D projections. Firstly, SeeU reconstructs the continuous 4D world from sparse monocular 2D frames, establishing a 2D→4D learning process. Secondly, it models the continuous temporal dynamics on a low-rank representation constrained by physical laws, transitioning from discrete 4D data to continuous 4D understanding. Thirdly, by rolling the learned 4D world forward in time and re-projecting it to 2D viewpoints, SeeU can generate unseen spatial and temporal visual content using spatial-temporal context awareness. This approach allows for physically consistent and continuous novel visual generation, overcoming limitations of typical 2D-based techniques. The authors demonstrate SeeU’s versatility and strong potential in tasks such as generating unseen temporal frames, filling in previously unobserved spatial regions, and video editing applications. Overall, SeeU advances the capability of modeling and synthesizing dynamic visual scenes by bridging 2D observations and continuous 4D world representations. <div>
arXiv:2512.03350v1 Announce Type: new 
Abstract: Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\to$4D$\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Deep Learning Framework with Explainable AI for Lung Cancer Classification with DenseNet169 and SVM</title>
<link>https://arxiv.org/abs/2512.03359</link>
<guid>https://arxiv.org/abs/2512.03359</guid>
<content:encoded><![CDATA[
<div> Keywords: Lung cancer, CT scans, Deep learning, DenseNet169, Model interpretability<br /><br />Summary:<br /><br />1. Lung cancer is a highly lethal disease globally, and early detection through computed tomography (CT) scans is critical for improving survival rates. Manual interpretation of CT scans is time-intensive and susceptible to human error.<br /><br />2. The study proposes an automatic lung cancer classification system based on deep learning to increase diagnostic accuracy and interpretability.<br /><br />3. The IQOTHNCCD dataset, a public CT scan dataset with cases labeled as Normal, Benign, and Malignant, was used for model training and evaluation.<br /><br />4. The primary model is DenseNet169 with integrated Squeeze-and-Excitation blocks, Focal Loss for managing class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion.<br /><br />5. Additionally, an SVM classifier was developed using features extracted by MobileNetV2, enhancing classification performance.<br /><br />6. To improve model transparency, Grad-CAM was applied to visualize important decision regions in CT images, and SHAP was used to explain feature importance in the SVM model.<br /><br />7. Both DenseNet169 and SVM models achieved high accuracy of 98%, indicating their effectiveness and robustness for real-world medical applications.<br /><br />8. The results highlight the potential of deep learning approaches to improve lung cancer diagnosis by providing high accuracy, greater interpretability, and reliable performance. <div>
arXiv:2512.03359v1 Announce Type: new 
Abstract: Lung cancer is a very deadly disease worldwide, and its early diagnosis is crucial for increasing patient survival rates. Computed tomography (CT) scans are widely used for lung cancer diagnosis as they can give detailed lung structures. However, manual interpretation is time-consuming and prone to human error. To surmount this challenge, the study proposes a deep learning-based automatic lung cancer classification system to enhance detection accuracy and interpretability. The IQOTHNCCD lung cancer dataset is utilized, which is a public CT scan dataset consisting of cases categorized into Normal, Benign, and Malignant and used DenseNet169, which includes Squeezeand-Excitation blocks for attention-based feature extraction, Focal Loss for handling class imbalance, and a Feature Pyramid Network (FPN) for multi-scale feature fusion. In addition, an SVM model was developed using MobileNetV2 for feature extraction, improving its classification performance. For model interpretability enhancement, the study integrated Grad-CAM for the visualization of decision-making regions in CT scans and SHAP (Shapley Additive Explanations) for explanation of feature contributions within the SVM model. Intensive evaluation was performed, and it was found that both DenseNet169 and SVM models achieved 98% accuracy, suggesting their robustness for real-world medical practice. These results open up the potential for deep learning to improve the diagnosis of lung cancer by a higher level of accuracy, transparency, and robustness.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting</title>
<link>https://arxiv.org/abs/2512.03369</link>
<guid>https://arxiv.org/abs/2512.03369</guid>
<content:encoded><![CDATA[
<div> Wildfire prediction, multi-modal dataset, UAV, FiReDiff, generative models<br /><br />Summary:<br /><br />This paper addresses the challenge of fine-grained wildfire spread prediction, which is essential for improving emergency response and decision-making accuracy. Existing studies typically use coarse spatiotemporal scales and low-resolution satellite imagery, limiting precise modeling of localized fire dynamics. To overcome these limitations, the authors introduce FireSentry, a provincial-scale multi-modal wildfire dataset with sub-meter spatial and sub-second temporal resolution, captured using synchronized UAVs. FireSentry includes visible and infrared video streams, in-situ environmental data, and manually validated fire masks. Using this dataset, the authors build a benchmark incorporating physics-based, data-driven, and generative wildfire spread models, highlighting the shortcomings of current mask-only prediction methods. They propose FiReDiff, a novel dual-modality approach that first generates future infrared video sequences to capture fire dynamics, then accurately segments fire masks based on the generated data. FiReDiff demonstrates significant improvements over existing generative models, achieving substantial gains in video quality metrics (PSNR, SSIM, LPIPS, FVD) and mask accuracy metrics (AUPRC, F1 score, IoU, MSE). The FireSentry dataset and FiReDiff framework together represent a major advance in fine-grained wildfire forecasting and dynamic disaster simulation. The benchmark dataset is publicly accessible at the provided GitHub link. <div>
arXiv:2512.03369v1 Announce Type: new 
Abstract: Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShelfGaussian: Shelf-Supervised Open-Vocabulary Gaussian-based 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2512.03370</link>
<guid>https://arxiv.org/abs/2512.03370</guid>
<content:encoded><![CDATA[
<div> Keywords: ShelfGaussian, Gaussian-based methods, multi-modal transformer, vision foundation models, zero-shot semantic occupancy prediction<br /><br />Summary:<br /><br />1. ShelfGaussian is a novel open-vocabulary multi-modal 3D scene understanding framework that leverages Gaussian-based representations supervised by off-the-shelf vision foundation models (VFMs).<br />2. Existing Gaussian-based methods are limited either by closed-set semantic Gaussians tied to annotated 3D labels without rendering capabilities or by purely 2D self-supervised open-set Gaussians that degrade geometric quality and depend solely on camera data.<br />3. The proposed Multi-Modal Gaussian Transformer enables Gaussians to query and integrate features from diverse sensor modalities, enhancing the expressive power of Gaussian representations.<br />4. Shelf-Supervised Learning Paradigm jointly optimizes Gaussians by utilizing VFM-derived features at both the 2D image and the 3D scene levels, improving learning efficiency and effectiveness.<br />5. Evaluations on Occ3D-nuScenes highlight ShelfGaussian’s state-of-the-art zero-shot semantic occupancy prediction capabilities, and additional tests on unmanned ground vehicles demonstrate its robust real-world performance in complex urban environments.<br />6. The framework shows promising results for perception and planning tasks, pushing forward the generalization and applicability of Gaussian-based 3D scene understanding methods across multiple sensor modalities.<br />7. Further details and resources are made available at the project website: https://lunarlab-gatech.github.io/ShelfGaussian/. <div>
arXiv:2512.03370v1 Announce Type: new 
Abstract: We introduce ShelfGaussian, an open-vocabulary multi-modal Gaussian-based 3D scene understanding framework supervised by off-the-shelf vision foundation models (VFMs). Gaussian-based methods have demonstrated superior performance and computational efficiency across a wide range of scene understanding tasks. However, existing methods either model objects as closed-set semantic Gaussians supervised by annotated 3D labels, neglecting their rendering ability, or learn open-set Gaussian representations via purely 2D self-supervision, leading to degraded geometry and limited to camera-only settings. To fully exploit the potential of Gaussians, we propose a Multi-Modal Gaussian Transformer that enables Gaussians to query features from diverse sensor modalities, and a Shelf-Supervised Learning Paradigm that efficiently optimizes Gaussians with VFM features jointly at 2D image and 3D scene levels. We evaluate ShelfGaussian on various perception and planning tasks. Experiments on Occ3D-nuScenes demonstrate its state-of-the-art zero-shot semantic occupancy prediction performance. ShelfGaussian is further evaluated on an unmanned ground vehicle (UGV) to assess its in the-wild performance across diverse urban scenarios. Project website: https://lunarlab-gatech.github.io/ShelfGaussian/.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification</title>
<link>https://arxiv.org/abs/2512.03404</link>
<guid>https://arxiv.org/abs/2512.03404</guid>
<content:encoded><![CDATA[
<div> Cross-modal ship re-identification, Optical imagery, SAR imagery, Modality gap, Diffusion model<br /><br />Summary:<br /><br />This paper addresses the challenge of cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) images, which is a crucial yet underexplored problem in maritime surveillance due to the large modality gap. To overcome this, the authors propose MOS, a novel framework aimed at reducing this modality gap and enabling modality-consistent feature learning. MOS is composed of two main components: (1) Modality-Consistent Representation Learning (MCRL), which includes denoising SAR images and employing a class-wise modality alignment loss to better align feature distributions of the same ship identity across modalities; (2) Cross-modal Data Generation and Feature Fusion (CDGF), which uses a Brownian bridge diffusion model to synthesize cross-modal samples that are fused with original features during inference to improve feature alignment and discrimination. The approach is evaluated extensively on the HOSS ReID dataset, where MOS outperforms current state-of-the-art techniques significantly. Results show improvements of +3.0%, +6.2%, and +16.4% in Rank-1 accuracy under ALL to ALL, Optical to SAR, and SAR to Optical evaluation protocols, respectively. The authors also plan to release their code and trained models upon publication. <div>
arXiv:2512.03404v1 Announce Type: new 
Abstract: Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViDiC: Video Difference Captioning</title>
<link>https://arxiv.org/abs/2512.03405</link>
<guid>https://arxiv.org/abs/2512.03405</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Difference Captioning, Multimodal Large Language Models, ViDiC-1K dataset, comparative reasoning, video understanding<br /><br />Summary: Understanding visual differences in dynamic scenes necessitates analyzing compositional, spatial, and temporal changes, a challenge inadequately addressed by current vision-language systems. Past research on Image Difference Captioning (IDC) primarily focused on semantic changes between static images but failed to capture motion continuity, event progression, and editing consistency over time. To overcome this limitation, the paper introduces the Video Difference Captioning (ViDiC) task alongside the ViDiC-1K dataset, specifically designed to evaluate Multimodal Large Language Models' (MLLMs) capacity for fine-grained descriptions of similarities and differences between video pairs. The ViDiC-1K dataset comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items across seven categories: subject, style, background, cinematography, motion, location, and playback techniques. The authors propose a dual-checklist evaluation framework that separately measures the accuracy of similarity and difference descriptions, utilizing an LLM-as-a-Judge protocol for reliable performance assessment. Extensive experiments involving nineteen multimodal models reveal a substantial performance gap in these models’ ability to perceive differences and generate comparative descriptions accurately. This work aims to provide a challenging benchmark that advances video understanding, editing awareness, and comparative reasoning capabilities in multimodal AI systems. <div>
arXiv:2512.03405v1 Announce Type: new 
Abstract: Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOLOA: Real-Time Affordance Detection via LLM Adapter</title>
<link>https://arxiv.org/abs/2512.03418</link>
<guid>https://arxiv.org/abs/2512.03418</guid>
<content:encoded><![CDATA[
<div> affordance detection, embodied AI, YOLOA, large language model adapter, real-time performance<br /><br />Summary:  
This paper addresses the challenge of affordance detection in embodied AI, which requires understanding "what" an object is, "where" it is located, and "how" it can be used. Existing methods often focus only on the "how" aspect or treat object detection and affordance learning as independent tasks, limiting interaction and real-time feasibility. To overcome these issues, the authors propose YOLO Affordance (YOLOA), a novel real-time model that jointly performs object detection and affordance learning. YOLOA incorporates a lightweight detector with two branches refined through a large language model (LLM) adapter. The LLM adapter interacts with preliminary predictions during training, enhancing both branches by generating accurate class priors, bounding box offsets, and affordance gates. Experimental results on the relabeled ADG-Det and IIT-Heat benchmarks demonstrate YOLOA’s state-of-the-art accuracy, achieving 52.8 and 73.1 mean Average Precision (mAP) respectively. Additionally, YOLOA maintains real-time processing speeds of up to 89.77 frames per second (FPS), with a lightweight variant reaching up to 846.24 FPS. These results show that YOLOA successfully balances high accuracy with efficiency, making it suitable for real-time affordance detection applications in embodied AI. <div>
arXiv:2512.03418v1 Announce Type: new 
Abstract: Affordance detection aims to jointly address the fundamental "what-where-how" challenge in embodied AI by understanding "what" an object is, "where" the object is located, and "how" it can be used. However, most affordance learning methods focus solely on "how" objects can be used while neglecting the "what" and "where" aspects. Other affordance detection methods treat object detection and affordance learning as two independent tasks, lacking effective interaction and real-time capability. To overcome these limitations, we introduce YOLO Affordance (YOLOA), a real-time affordance detection model that jointly handles these two tasks via a large language model (LLM) adapter. Specifically, YOLOA employs a lightweight detector consisting of object detection and affordance learning branches refined through the LLM Adapter. During training, the LLM Adapter interacts with object and affordance preliminary predictions to refine both branches by generating more accurate class priors, box offsets, and affordance gates. Experiments on our relabeled ADG-Det and IIT-Heat benchmarks demonstrate that YOLOA achieves state-of-the-art accuracy (52.8 / 73.1 mAP on ADG-Det / IIT-Heat) while maintaining real-time performance (up to 89.77 FPS, and up to 846.24 FPS for the lightweight variant). This indicates that YOLOA achieves an excellent trade-off between accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DM3D: Deformable Mamba via Offset-Guided Gaussian Sequencing for Point Cloud Understanding</title>
<link>https://arxiv.org/abs/2512.03424</link>
<guid>https://arxiv.org/abs/2512.03424</guid>
<content:encoded><![CDATA[
<div> Keywords: State Space Models, Point Clouds, Deformable Mamba, Gaussian Sequencing, Adaptive Serialization<br /><br />Summary: State Space Models (SSMs) have shown great potential for modeling long sequences but face challenges when applied to point clouds due to their dependence on input order, which conflicts with the irregular and unordered nature of point clouds. To address this, the paper proposes DM3D, a novel deformable Mamba architecture designed specifically for point cloud understanding. DM3D introduces an offset-guided Gaussian sequencing mechanism that integrates local resampling and global reordering into a single deformable scan process, enabling adaptive serialization of point clouds. The architecture includes two key components: Gaussian-based KNN Resampling (GKR), which adaptively reorganizes neighboring points to enhance structural awareness, and Gaussian-based Differentiable Reordering (GDR), which allows for end-to-end optimization of the serialization order. Additionally, the Tri-Path Frequency Fusion module improves feature complementarity and reduces aliasing effects. Together, these innovations enable DM3D to adaptively serialize point clouds based on their geometric structure, unlocking the potential of SSMs for this domain. Extensive experiments on benchmark datasets demonstrate that DM3D achieves state-of-the-art results in classification, few-shot learning, and part segmentation tasks, validating the effectiveness of adaptive serialization for point cloud understanding. <div>
arXiv:2512.03424v1 Announce Type: new 
Abstract: State Space Models (SSMs) demonstrate significant potential for long-sequence modeling, but their reliance on input order conflicts with the irregular nature of point clouds. Existing approaches often rely on predefined serialization strategies, which cannot adjust based on diverse geometric structures. To overcome this limitation, we propose \textbf{DM3D}, a deformable Mamba architecture for point cloud understanding. Specifically, DM3D introduces an offset-guided Gaussian sequencing mechanism that unifies local resampling and global reordering within a deformable scan. The Gaussian-based KNN Resampling (GKR) enhances structural awareness by adaptively reorganizing neighboring points, while the Gaussian-based Differentiable Reordering (GDR) enables end-to-end optimization of serialization order. Furthermore, a Tri-Path Frequency Fusion module enhances feature complementarity and reduces aliasing. Together, these components enable structure-adaptive serialization of point clouds. Extensive experiments on benchmark datasets show that DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Evaluation of Deep Stereo Matching Methods for UAV-Based Forestry Applications</title>
<link>https://arxiv.org/abs/2512.03427</link>
<guid>https://arxiv.org/abs/2512.03427</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV forestry, depth estimation, zero-shot evaluation, stereo methods, cross-domain generalization

<br /><br />Summary:  
This paper addresses the need for robust depth estimation methods tailored for autonomous UAV operations in dense vegetation environments, an area underrepresented in existing evaluations that focus mainly on urban and indoor scenes. The authors conduct the first systematic zero-shot evaluation of eight state-of-the-art stereo depth estimation techniques: RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM, and two baseline methods ACVNet and PSMNet, all trained exclusively on the Scene Flow dataset. The evaluation spans four standard benchmarks—ETH3D, KITTI 2012/2015, Middlebury—and introduces a new forestry dataset from Canterbury, consisting of 5,313 stereo pairs captured with a ZED Mini camera at 1920x1080 resolution. Results show distinct performance patterns depending on scene structure: foundation models excel in structured scenes with low pixel error, while iterative refinement-based methods demonstrate greater cross-domain robustness. A critical failure is noted for RAFT-Stereo on ETH3D caused by negative disparity predictions, although it performs well on KITTI. Qualitative analysis on the Canterbury forestry dataset highlights DEFOM as the preferred baseline for vegetation depth estimation due to superior smoothness, occlusion handling, and consistency across domains. IGEV++ provides finer detail preservation but lacks some of DEFOM’s robustness. <div>
arXiv:2512.03427v1 Announce Type: new 
Abstract: Autonomous UAV forestry operations require robust depth estimation methods with strong cross-domain generalization. However, existing evaluations focus on urban and indoor scenarios, leaving a critical gap for specialized vegetation-dense environments. We present the first systematic zero-shot evaluation of eight state-of-the-art stereo methods--RAFT-Stereo, IGEV, IGEV++, BridgeDepth, StereoAnywhere, DEFOM (plus baseline methods ACVNet, PSMNet, TCstereo)--spanning iterative refinement, foundation model, and zero-shot adaptation paradigms. All methods are trained exclusively on Scene Flow and evaluated without fine-tuning on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury forestry dataset captured with ZED Mini camera (1920x1080). Performance reveals scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D, 0.83-1.07 px on KITTI; DEFOM: 0.35-4.65 px across benchmarks), while iterative methods maintain cross-domain robustness (IGEV++: 0.36-6.77 px; IGEV: 0.33-21.91 px). Critical finding: RAFT-Stereo exhibits catastrophic ETH3D failure (26.23 px EPE, 98 percent error rate) due to negative disparity predictions, while performing normally on KITTI (0.90-1.11 px). Qualitative evaluation on Canterbury forestry dataset identifies DEFOM as the optimal gold-standard baseline for vegetation depth estimation, exhibiting superior depth smoothness, occlusion handling, and cross-domain consistency compared to IGEV++, despite IGEV++'s finer detail preservation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Label-Efficient Hyperspectral Image Classification via Spectral FiLM Modulation of Low-Level Pretrained Diffusion Features</title>
<link>https://arxiv.org/abs/2512.03430</link>
<guid>https://arxiv.org/abs/2512.03430</guid>
<content:encoded><![CDATA[
<div> Hyperspectral Imaging, Diffusion Models, Label Efficiency, Multimodal Fusion, Remote Sensing<br /><br />Summary:<br /><br />1. The paper addresses challenges in hyperspectral imaging (HSI) such as low spatial resolution and sparse annotation labels, which hinder detailed land cover classification.<br /><br />2. It proposes a label-efficient framework leveraging spatial features extracted from a frozen diffusion model pretrained on natural images, which provides robust low-level representations relevant for HSI despite domain differences.<br /><br />3. Key to their approach is the use of high-resolution decoder layers at early denoising steps of the diffusion model, capturing low-texture spatial structures typical in hyperspectral data.<br /><br />4. To effectively integrate spectral and spatial modalities, the authors introduce a lightweight Feature-wise Linear Modulation (FiLM) based fusion module that adaptively modulates the frozen spatial features using spectral information.<br /><br />5. Experimental validation on two recent hyperspectral datasets shows the method outperforms state-of-the-art models under sparse supervision, and ablation studies confirm the importance of diffusion-derived features and their spectral-aware fusion design.<br /><br />6. The results indicate pretrained diffusion models as powerful, domain-agnostic tools for label-efficient representation learning in remote sensing and potentially other scientific imaging fields. <div>
arXiv:2512.03430v1 Announce Type: new 
Abstract: Hyperspectral imaging (HSI) enables detailed land cover classification, yet low spatial resolution and sparse annotations pose significant challenges. We present a label-efficient framework that leverages spatial features from a frozen diffusion model pretrained on natural images. Our approach extracts low-level representations from high-resolution decoder layers at early denoising timesteps, which transfer effectively to the low-texture structure of HSI. To integrate spectral and spatial information, we introduce a lightweight FiLM-based fusion module that adaptively modulates frozen spatial features using spectral cues, enabling robust multimodal learning under sparse supervision. Experiments on two recent hyperspectral datasets demonstrate that our method outperforms state-of-the-art approaches using only the provided sparse training labels. Ablation studies further highlight the benefits of diffusion-derived features and spectral-aware fusion. Overall, our results indicate that pretrained diffusion models can support domain-agnostic, label-efficient representation learning for remote sensing and broader scientific imaging tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Aspect Knowledge-Enhanced Medical Vision-Language Pretraining with Multi-Agent Data Generation</title>
<link>https://arxiv.org/abs/2512.03445</link>
<guid>https://arxiv.org/abs/2512.03445</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language pretraining, Multi-Agent data generation, Ontology-based knowledge, Medical image analysis, Dermatology<br /><br />Summary:<br />1. This study addresses challenges in vision-language pretraining (VLP) for medical image analysis, particularly the noise in web-collected data and the complexity of long, unstructured medical texts.<br />2. The authors propose a novel VLP framework integrating two main components: a Multi-Agent data GENeration system (MAGEN) and an Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining method.<br />3. MAGEN improves data quality by generating knowledge-enriched descriptions using a foundation model-assisted captioning approach paired with a retrieval-based verification process.<br />4. O-MAKE tackles the difficulty of learning from lengthy, unstructured text by decomposing it into multiple knowledge aspects, enabling fine-grained alignment between images and text at both global and patch levels while explicitly modeling relationships between medical concepts using ontology-guided mechanisms.<br />5. The framework is validated in dermatology through extensive experiments, achieving state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight different datasets.<br />6. The authors will release their code and an augmented dataset named Derm1M-AgentAug, containing over 400,000 skin-image-text pairs, to support further research in this domain. <div>
arXiv:2512.03445v1 Announce Type: new 
Abstract: Vision-language pretraining (VLP) has emerged as a powerful paradigm in medical image analysis, enabling representation learning from large-scale image-text pairs without relying on expensive manual annotations. However, existing methods often struggle with the noise inherent in web-collected data and the complexity of unstructured long medical texts. To address these challenges, we propose a novel VLP framework integrating a Multi-Agent data GENeration (MAGEN) system and Ontology-based Multi-Aspect Knowledge-Enhanced (O-MAKE) pretraining. First, MAGEN enhances data quality by synthesizing knowledge-enriched descriptions via a foundation model-assisted captioning and retrieval-based verification pipeline. Second, O-MAKE addresses the difficulty of learning from long, unstructured texts by decomposing them into distinct knowledge aspects. This facilitates fine-grained alignment at both global and patch levels, while explicitly modeling medical concept relationships through ontology-guided mechanisms. We validate our framework in the field of dermatology, where comprehensive experiments demonstrate the effectiveness of each component. Our approach achieves state-of-the-art zero-shot performance on disease classification and cross-modal retrieval tasks across eight datasets. Our code and the augmented dataset Derm1M-AgentAug, comprising over 400k skin-image-text pairs, will be released at https://github.com/SiyuanYan1/Derm1M.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis</title>
<link>https://arxiv.org/abs/2512.03449</link>
<guid>https://arxiv.org/abs/2512.03449</guid>
<content:encoded><![CDATA[
<div> Keywords: knee MRI, cartilage segmentation, radiomics, lateral/medial compartmentalisation, osteoarthritis  

<br /><br />Summary:  
This study introduces LM-CartSeg, a fully automatic pipeline designed for robust cartilage and subchondral bone segmentation in knee MRI, integrating geometric lateral/medial compartmentalisation and radiomics analysis. Two 3D nnU-Net models were trained on two datasets (SKM-TEA with 138 knees and OAIZIB-CM with 404 knees) and combined with geometric post-processing steps such as connected-component cleaning, construction of 10 mm subchondral bone bands, and a data-driven tibial lateral/medial split using PCA and k-means clustering. The method was evaluated on independent test datasets (OAIZIB-CM with 103 knees and SKI-10 with 100 knees), showing significant improvement in segmentation accuracy with a macro average symmetric surface distance (ASSD) reduction from 2.63 mm to 0.36 mm and Hausdorff distance (HD95) reduction from 25.2 mm to 3.35 mm, achieving a Dice similarity coefficient (DSC) of 0.91; zero-shot performance on SKI-10 had DSC of 0.80. The geometric lateral/medial compartmentalisation proved more stable and reliable than a direct nnU-Net classifier, which showed domain-dependent inaccuracies. Quality control used volume and thickness signatures to ensure ROI consistency. Radiomic analysis demonstrated that only a small portion (6–12%) of features correlated with size metrics, and radiomics models focusing on these size-linked features performed differently. Overall, LM-CartSeg provides automatic, quality-controlled ROIs and radiomic features that extend beyond simple morphometry, offering a practical tool for multi-centre knee osteoarthritis radiomics research. <div>
arXiv:2512.03449v1 Announce Type: new 
Abstract: Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03450</link>
<guid>https://arxiv.org/abs/2512.03450</guid>
<content:encoded><![CDATA[
<div> 3D keypoints, unsupervised learning, point cloud, Elucidated Diffusion Model, generative models<br /><br />Summary:<br /><br />This paper addresses the challenge of understanding and representing 3D object structures in an unsupervised manner, focusing on learning spatially structured 3D keypoints from point cloud data. Unlike most existing methods, which are not suited for unconditional generative setups, the proposed framework bridges this gap by integrating learned keypoints with an Elucidated Diffusion Model (EDM) for full shape reconstruction. The keypoints serve as a compact and interpretable representation that retains consistent spatial structure across different object instances, making them reliable and meaningful for 3D shape understanding. Moreover, the keypoints enable smooth interpolation within their latent space, highlighting their effectiveness in capturing geometric variations across shapes. Experimental results demonstrate strong performance across a range of diverse object categories, with the approach surpassing previous methods by a significant margin—achieving a 6 percentage-point increase in keypoint consistency. This advancement suggests that the framework not only improves unsupervised keypoint extraction but also enhances the integration of keypoints into modern generative 3D models, potentially benefiting various applications in computer vision and graphics. <div>
arXiv:2512.03450v1 Announce Type: new 
Abstract: Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.03451</link>
<guid>https://arxiv.org/abs/2512.03451</guid>
<content:encoded><![CDATA[
<div> Diffusion models, video generation, classifier-free guidance, GalaxyDiT, computational efficiency  

<br /><br />Summary:  
The paper addresses the computational inefficiency of diffusion models used in video generation, particularly those based on transformer architectures (DiTs) and utilizing classifier-free guidance (CFG), which requires extensive iterative computation and doubles compute needs. To mitigate this challenge, the authors propose GalaxyDiT, a training-free acceleration method that leverages guidance alignment and systematic proxy selection through rank-order correlation analysis. This approach identifies optimal proxies for different video models, accommodating various model families and parameter scales to maximize computational reuse. Experiments demonstrate significant speedups of 1.87× and 2.37× on Wan2.1-1.3B and Wan2.1-14B models respectively, with minimal performance degradation of only 0.97% and 0.72% on the VBench-2.0 benchmark. Furthermore, GalaxyDiT outperforms previous state-of-the-art methods by maintaining higher fidelity at elevated speedup rates, reflected in a 5 to 10 dB improvement in peak signal-to-noise ratio (PSNR). Overall, this work presents a practical and effective solution to accelerate video diffusion models without retraining, thereby facilitating wider applicability in downstream video generation tasks. <div>
arXiv:2512.03451v1 Announce Type: new 
Abstract: Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.
  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\times$ and $2.37\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoVideo: Introducing Geometric Regularization into Video Generation Model</title>
<link>https://arxiv.org/abs/2512.03453</link>
<guid>https://arxiv.org/abs/2512.03453</guid>
<content:encoded><![CDATA[
<div> Depth prediction, latent diffusion, video generation, geometric consistency, multi-view loss<br /><br />Summary:<br /><br />This paper addresses the challenge of 3D structural consistency in video generation, which most current 2D pixel-based diffusion transformer models lack. The authors introduce geometric regularization by incorporating per-frame depth prediction into latent diffusion models, leveraging advances in depth estimation and their synergy with image-based latent encoders. To maintain temporal structural coherence, a novel multi-view geometric loss is proposed, aligning predicted depth maps across frames within a unified 3D coordinate system. This approach tightly integrates appearance synthesis with explicit 3D modeling, improving spatio-temporal coherence and shape stability in generated videos. Experiments on several benchmark datasets demonstrate that this method significantly reduces temporal artifacts, implausible motions, and structural inconsistencies compared to existing video generation baselines. Overall, the work bridges the gap between 2D video synthesis and 3D geometry understanding, resulting in more physically plausible and visually stable video outputs. <div>
arXiv:2512.03453v1 Announce Type: new 
Abstract: Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2512.03454</link>
<guid>https://arxiv.org/abs/2512.03454</guid>
<content:encoded><![CDATA[
<div> Spatial-Aware World Model, Visual Grounding, Autonomous Driving, Hypergraph Decoder, Retrieval-Augmented Generation  

<br /><br />Summary:  
This paper addresses the challenge of interpreting natural-language commands to localize target objects for autonomous driving (AD), highlighting the limitations of current visual grounding (VG) methods that fail to reason about 3D spatial relations and future scene evolution. The authors introduce ThinkDeeper, a novel framework that anticipates future spatial states before making localization decisions. Central to this approach is the Spatial-Aware World Model (SA-WM), which distills the current scene into a command-aware latent representation and rolls out future latent states, enabling forward-looking disambiguation cues. To enhance reasoning over complex spatial dependencies, a hypergraph-guided decoder hierarchically fuses these future states with multimodal inputs. Additionally, the paper presents DrivePilot, a new multi-source VG dataset for autonomous vehicles, assembled with semantic annotations generated via a Retrieval-Augmented Generation (RAG) combined with Chain-of-Thought (CoT) prompted large language models (LLMs). Extensive evaluations on six benchmarks demonstrate ThinkDeeper's superior performance, ranking first on the Talk2Car leaderboard and outperforming state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g datasets. The framework also proves robust and efficient in challenging scenarios involving long commands, multiple agents, and ambiguous instructions, maintaining high accuracy even when trained with only half of the available data. <div>
arXiv:2512.03454v1 Announce Type: new 
Abstract: Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.03463</link>
<guid>https://arxiv.org/abs/2512.03463</guid>
<content:encoded><![CDATA[
<div> Text-centric training, vision-language models, synthetic images, Text-Printed Image, data augmentation<br /><br />Summary:<br /><br />1. Vision-language models (LVLMs) typically require large image-text datasets for effective performance on visual question answering (VQA) tasks, which are costly and labor-intensive to obtain.<br />2. The authors propose text-centric training, a novel paradigm that relies solely on textual descriptions without the need for real images, making data collection more scalable, affordable, and less restricted by privacy or niche domain scarcity.<br />3. A key challenge is the modality gap between text and images that limits gains when training on text alone.<br />4. To bridge this gap, the paper introduces the Text-Printed Image (TPI) method, which creates synthetic images by rendering textual descriptions directly onto plain white canvases, preserving text semantics better than conventional text-to-image generation.<br />5. Extensive experiments across four different LVLMs and seven benchmarks show that TPI-enhanced text-centric training outperforms synthetic images generated by diffusion models.<br />6. Additionally, TPI serves as an effective, low-cost data augmentation technique.<br />7. Overall, the study highlights the promising potential of text-centric training and automated synthetic data generation to support scalable, cost-effective improvements in LVLM development. <div>
arXiv:2512.03463v1 Announce Type: new 
Abstract: Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Difference Decomposition Networks for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2512.03470</link>
<guid>https://arxiv.org/abs/2512.03470</guid>
<content:encoded><![CDATA[
<div> Keywords: Infrared small target detection, Basis Decomposition Module, Spatial Difference, Temporal Difference, U-shaped architecture<br /><br />Summary:<br /><br />The article addresses the challenges in Infrared Small Target Detection (ISTD), mainly the lack of clear target texture and significant background clutter that obscures targets. To overcome these challenges, the authors introduce the Basis Decomposition Module (BDM), a lightweight and extensible module that decomposes complex features into several basis features, enhancing useful information while reducing redundancy. Building upon BDM, several specialized modules are developed: the Spatial Difference Decomposition Module (SD²M), the Spatial Difference Decomposition Downsampling Module (SD³M), and the Temporal Difference Decomposition Module (TD²M). These modules are integrated into two networks: Spatial Difference Decomposition Network (SD²Net) for single-frame ISTD (SISTD) and Spatiotemporal Difference Decomposition Network (STD²Net) for multi-frame ISTD (MISTD). SD²Net uses SD²M and SD³M within a modified U-shaped architecture, and STD²Net incorporates TD²M to include motion information from multiple frames. Experimental results demonstrate state-of-the-art performance in both SISTD and MISTD tasks. Specifically, STD²Net significantly outperforms SD²Net on MISTD datasets, achieving a mean Intersection over Union (mIoU) of 87.68% versus 64.97%. The code for these networks is publicly available on GitHub. <div>
arXiv:2512.03470v1 Announce Type: new 
Abstract: Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Procedural Mistake Detection via Action Effect Modeling</title>
<link>https://arxiv.org/abs/2512.03474</link>
<guid>https://arxiv.org/abs/2512.03474</guid>
<content:encoded><![CDATA[
<div> Keywords: mistake detection, procedural tasks, action effect, effect-aware representations, one-class classification<br /><br />Summary:<br /><br />This paper addresses mistake detection in procedural tasks by focusing not only on how an action is performed but also on its resulting outcome, termed the action effect. The authors identify a limitation in existing methods that typically analyze execution alone, neglecting errors that appear only in the produced outcome such as unintended object states or spatial misarrangements. To overcome this, they propose Action Effect Modeling (AEM), a unified probabilistic framework that jointly captures both action execution and its outcomes. AEM selects the most informative effect frame for each action based on semantic relevance and visual quality, and integrates complementary visual and symbolic cues from visual grounding and scene graphs into a shared latent space for robust effect-aware representations. For mistake detection, a prompt-based detector is designed to incorporate task-specific prompts and align each action segment with its intended semantics. The approach achieves state-of-the-art results on EgoPER and CaptainCook4D benchmarks under the challenging one-class classification setting. The study demonstrates that modeling both execution and outcomes enhances mistake detection reliability and suggests that effect-aware representations have potential utility across various downstream applications. <div>
arXiv:2512.03474v1 Announce Type: new 
Abstract: Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis</title>
<link>https://arxiv.org/abs/2512.03477</link>
<guid>https://arxiv.org/abs/2512.03477</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, fairness, Low-Rank Adaptation, MaxAccGap loss, medical imaging<br /><br />Summary:<br />1. The paper addresses diagnostic accuracy disparities across demographic groups in vision-language models (VLMs) applied to medical imaging tasks.<br />2. It introduces a fairness-aware Low-Rank Adaptation (LoRA) approach for medical VLMs, which combines parameter efficiency with explicit fairness optimization.<br />3. The key contribution is the differentiable MaxAccGap loss function that enables end-to-end optimization focused on accuracy parity among different demographic groups.<br />4. Three methods are proposed: FR-LoRA (integrates MaxAccGap as a regularizer in training), GR-LoRA (uses inverse frequency weighting to balance gradients), and Hybrid-LoRA (combines both techniques).<br />5. The methods were evaluated on a dataset of 10,000 glaucoma fundus images, where GR-LoRA reduced accuracy disparities by 69% while maintaining an overall accuracy of 53.15%.<br />6. Ablation studies showed that stronger regularization improves fairness with minimal accuracy loss, and race-specific optimization achieved a 60% reduction in disparity.<br />7. The approach is parameter-efficient, requiring only 0.24% of trainable parameters, enabling deployment of fair medical AI in resource-limited healthcare settings. <div>
arXiv:2512.03477v1 Announce Type: new 
Abstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Object-centric Understanding for Instructional Videos</title>
<link>https://arxiv.org/abs/2512.03479</link>
<guid>https://arxiv.org/abs/2512.03479</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural activities, object-centric reasoning, instructional video benchmark, state transitions, vision-language models<br /><br />Summary:<br /><br />Understanding procedural activities is essential for developing advanced assistive AI capable of reasoning about complex real-world tasks. Traditional action-centric approaches fall short when task step orders vary depending on object states. This work introduces a shift towards an object-centric paradigm, viewing actions as drivers of state transitions in objects. To support this approach, the authors present Object-IVQA, a benchmark consisting of 107 long-form instructional videos with 514 open-ended question-answer pairs annotated with temporally grounded evidence. Object-IVQA evaluates four critical aspects of object-centric reasoning: state evolution, precondition verification, counterfactual reasoning, and mistake recognition. The paper further proposes an AI agent framework that integrates object-centric planning, perception, analysis, and generation tools. This framework facilitates explicit evidence retrieval and multi-hop reasoning across non-contiguous video segments. Experimental results demonstrate that existing large vision-language models struggle with object-level recognition and reasoning tasks in this domain. By contrast, the proposed framework substantially improves performance, highlighting the importance and effectiveness of object-centric approaches for procedural activity understanding and reasoning in instructional videos. <div>
arXiv:2512.03479v1 Announce Type: new 
Abstract: Understanding procedural activities is crucial for developing future assistive AI that can reason about complex real-world tasks. Existing action-centric methods struggle with the flexibility of real procedures, where step order varies depending on object states. In this work, we propose to shift the focus to an object-centric paradigm by regarding actions as mechanisms that drive state transitions. To advance this direction, we introduce Object-IVQA, a long-form instructional video benchmark with 107 videos and 514 open-ended question-answer pairs annotated with temporally grounded evidence. The benchmark evaluates four dimensions of object-centric reasoning, including state evolution, precondition verification, counterfactual reasoning and mistake recognition. We further propose an agent framework that orchestrates object-centric planning, perception, analysis and generation tools, enabling explicit evidence retrieval and multi-hop reasoning across disjoint segments. Experiments show that existing large vision-language models struggle in object-level recognition and reasoning, whereas our framework achieves substantially improvement.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation</title>
<link>https://arxiv.org/abs/2512.03499</link>
<guid>https://arxiv.org/abs/2512.03499</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, Low-Rank Adaptation, Neural Architecture Search, Parameter-Efficient Fine-Tuning, Vision Transformer  

<br /><br />Summary:  
The Segment Anything Model (SAM) is a robust visual foundation model for image segmentation but faces challenges when adapting to specialized fields like medical and agricultural imaging. To enhance SAM's adaptability, Low-Rank Adaptation (LoRA) and its variants are commonly used to fine-tune the model efficiently across different domains. However, a major limitation is that the Transformer encoder within SAM lacks spatial priors for image patches, which may restrict the learning of high-level semantic features necessary for domain-specific tasks. To overcome this, the paper introduces NAS-LoRA, a novel Parameter-Efficient Fine-Tuning (PEFT) approach that integrates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder in LoRA. This block dynamically optimizes the incorporation of prior knowledge into weight updates, addressing the semantic gap between SAM's pretrained features and the target domain. Additionally, a stage-wise optimization method is proposed to balance weight updating and architectural adjustments within the Vision Transformer encoder, promoting a gradual and effective acquisition of semantic understanding. Experimental results show that NAS-LoRA outperforms existing PEFT methods, achieving a 24.14% reduction in training cost without incurring additional inference overhead. This work demonstrates the promising role NAS can play in boosting PEFT techniques for visual foundation models. <div>
arXiv:2512.03499v1 Announce Type: new 
Abstract: The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEA: Exploration-Exploitation Agent for Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.03500</link>
<guid>https://arxiv.org/abs/2512.03500</guid>
<content:encoded><![CDATA[
<div> Keywords: long-form video understanding, exploration-exploitation balance, hierarchical tree search, semantic guidance, vision-language models  

<br /><br />Summary: This paper addresses the challenges in long-form video understanding, where efficiently navigating large volumes of visual data is critical to identify sparse but important content. Existing methods either incur high computational costs due to dense preprocessing or fail to effectively balance exploration and exploitation, leading to incomplete coverage and inefficiency. The authors propose EEA, a novel video agent framework that achieves a balance between exploration and exploitation through semantic guidance coupled with a hierarchical tree search process. EEA autonomously discovers and dynamically updates semantic queries relevant to the task, using these queries to collect video frames as semantic anchors. During the search, instead of uniformly expanding nodes, EEA prioritizes semantically relevant frames while maintaining adequate coverage in unexplored segments. Additionally, EEA incorporates intrinsic rewards derived from vision-language models combined with semantic priors and explicitly models uncertainty to ensure stable and accurate evaluation of video segments. Extensive experiments on multiple long-video benchmarks demonstrate that EEA outperforms existing approaches both in terms of performance and computational efficiency, validating the effectiveness of the proposed method. <div>
arXiv:2512.03500v1 Announce Type: new 
Abstract: Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.03508</link>
<guid>https://arxiv.org/abs/2512.03508</guid>
<content:encoded><![CDATA[
<div> Keywords: Domain Generalization, Semantic Segmentation, Vision-Language Models, Prompt Learning, Contrastive Learning<br /><br />Summary: This paper addresses the challenge of domain generalized semantic segmentation (DGSS) by tackling semantic misalignment between visual and textual contexts caused by fixed context prompts trained on a single source domain. The authors propose a new framework called Domain-aware Prompt-driven Masked Transformer (DPMFormer) to improve domain generalization. First, they introduce domain-aware prompt learning to better align visual features with textual cues, enhancing semantic understanding across domains. To handle domain-specific variations within a single source dataset, domain-aware contrastive learning is employed alongside texture perturbation techniques that simulate diverse observable domains. Finally, the framework incorporates domain-robust consistency learning to ensure stable predictions under different environmental conditions by minimizing discrepancies between original and augmented images. Experimental results demonstrate that DPMFormer outperforms existing methods on multiple DGSS benchmarks, setting a new state-of-the-art for robustness and accuracy. The paper also provides a publicly available implementation to facilitate further research and adoption. <div>
arXiv:2512.03508v1 Announce Type: new 
Abstract: Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model</title>
<link>https://arxiv.org/abs/2512.03509</link>
<guid>https://arxiv.org/abs/2512.03509</guid>
<content:encoded><![CDATA[
<div> Keywords: automated dance analysis, YOLOv8, Segment Anything Model, motion quantification, AfroBeats dance<br /><br />Summary:<br /><br />This paper introduces a preliminary investigation into automated analysis of dance movements leveraging modern computer vision techniques. A proof-of-concept framework is proposed that combines YOLOv8 and v11 models for dancer detection with the Segment Anything Model (SAM) for accurate segmentation, enabling markerless tracking and quantification of dance movements from video. The system identifies dancers in video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency throughout performances. The framework was tested on a single 49-second video of Ghanaian AfroBeats dance, achieving about 94% precision and 89% recall in dancer detection based on manual inspection. Pixel-level segmentation by SAM achieved approximately 83% intersection-over-union with visual verification, allowing for detailed motion quantification that surpasses bounding-box methods in capturing body configuration changes. Analysis revealed that the system-designated primary dancer performed 23% more steps, showed 37% higher motion intensity, and utilized 42% more performance space compared to secondary dancers. The study acknowledges significant limitations, including validation on only one video, absence of systematic ground truth data, and no comparison with established pose estimation approaches. This work aims to demonstrate technical feasibility, highlight promising quantitative dance metrics, and lay groundwork for future validation efforts. <div>
arXiv:2512.03509v1 Announce Type: new 
Abstract: This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSMapping: Scalable Crowdsourced Semantic Mapping and Topology Inference for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.03510</link>
<guid>https://arxiv.org/abs/2512.03510</guid>
<content:encoded><![CDATA[
<div> Keywords: Crowdsourcing, Semantic Mapping, Latent Diffusion Model, Topological Mapping, Autonomous Driving<br /><br />Summary:<br />1. The paper addresses the challenge of improving autonomous driving map quality using crowdsourced data that often suffers from noise due to low-cost sensors.<br />2. It introduces CSMapping, a novel system designed to generate accurate semantic maps and topological road centerlines, with map quality that improves as more crowdsourced data becomes available.<br />3. For semantic mapping, CSMapping leverages a latent diffusion model trained on high-definition (HD) maps, optionally conditioned on standard-definition (SD) maps, to learn a generative prior of real-world map structures without requiring paired crowdsourced and HD map data.<br />4. This generative prior is integrated via constrained maximum a posteriori (MAP) optimization in latent space, which enhances robustness against severe noise and allows plausible completion in areas with missing observations.<br />5. The initialization involves a robust vectorized mapping module followed by diffusion inversion, while optimization includes Gaussian-basis reparameterization, projected gradient descent with multi-start, and a latent-space factor graph to ensure global consistency.<br />6. For topological mapping, confidence-weighted k-medoids clustering and kinematic refinement are applied to trajectory data to produce smooth, human-like road centerlines that are robust to variations in crowdsourced trajectories.<br />7. Extensive experiments on the nuScenes, Argoverse 2, and a large proprietary dataset demonstrate state-of-the-art performance in both semantic and topological mapping, supplemented by thorough ablation and scalability analyses. <div>
arXiv:2512.03510v1 Announce Type: new 
Abstract: Crowdsourcing enables scalable autonomous driving map construction, but low-cost sensor noise hinders quality from improving with data volume. We propose CSMapping, a system that produces accurate semantic maps and topological road centerlines whose quality consistently increases with more crowdsourced data. For semantic mapping, we train a latent diffusion model on HD maps (optionally conditioned on SD maps) to learn a generative prior of real-world map structure, without requiring paired crowdsourced/HD-map supervision. This prior is incorporated via constrained MAP optimization in latent space, ensuring robustness to severe noise and plausible completion in unobserved areas. Initialization uses a robust vectorized mapping module followed by diffusion inversion; optimization employs efficient Gaussian-basis reparameterization, projected gradient descent zobracket multi-start, and latent-space factor-graph for global consistency. For topological mapping, we apply confidence-weighted k-medoids clustering and kinematic refinement to trajectories, yielding smooth, human-like centerlines robust to trajectory variation. Experiments on nuScenes, Argoverse 2, and a large proprietary dataset achieve state-of-the-art semantic and topological mapping performance, with thorough ablation and scalability studies.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation</title>
<link>https://arxiv.org/abs/2512.03520</link>
<guid>https://arxiv.org/abs/2512.03520</guid>
<content:encoded><![CDATA[
<div> Keywords: FloodDiffusion, human motion generation, diffusion forcing, streaming text-driven motion, time-varying control

<br /><br />Summary:  
The paper introduces FloodDiffusion, a novel framework designed for streaming human motion generation driven by time-varying text prompts. Unlike previous approaches that process motion in discrete chunks or use auto-regressive diffusion heads, FloodDiffusion employs a diffusion forcing framework tailored for continuous time-series generation under evolving control signals. The authors identify that the vanilla diffusion forcing method, previously used in video tasks, fails to capture the true distribution of human motion data when applied directly. To address this, three key modifications are proposed: (i) replacing causal attention with bi-directional attention during training for better temporal modeling, (ii) implementing a lower triangular time scheduler rather than a random scheduler to guide the diffusion process appropriately, and (iii) integrating text conditioning in a continuous, time-varying manner to align motions closely with the input prompts. These improvements enable FloodDiffusion to outmatch current state-of-the-art methods on the streaming motion generation task, achieving a superior Frechet Inception Distance (FID) score of 0.057 on the HumanML3D benchmark. The authors also provide access to models, code, and pretrained weights for broader use and reproducibility. <div>
arXiv:2512.03520v1 Announce Type: new 
Abstract: We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation</title>
<link>https://arxiv.org/abs/2512.03532</link>
<guid>https://arxiv.org/abs/2512.03532</guid>
<content:encoded><![CDATA[
<div> Open-vocabulary 3D instance segmentation, mesh-free environments, visual-spatial tracker, multi-modal large language model, compositional reasoning  

<br /><br />Summary:  
This paper addresses the challenge of generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments, which is important for applications in robotics and AR/VR. Existing methods fall short due to their reliance on dataset-specific proposal networks or mesh-based superpoints, limiting applicability and generalization in mesh-free scenarios. Additionally, CLIP-based classifiers have weak textual reasoning abilities and struggle with compositional and functional queries. To overcome these limitations, the authors propose OpenTrack3D, a novel and generalizable framework that constructs object proposals online using a visual-spatial tracker without relying on pre-generated proposals. Given an RGB-D input stream, the pipeline uses a 2D open-vocabulary segmenter to create masks, which are then lifted to 3D point clouds with depth information. Instance features guided by these masks are extracted using DINO feature maps, and the tracker fuses visual and spatial data to maintain consistent instances across views. The pipeline operates fully mesh-free but includes an optional superpoints refinement module if a scene mesh is available. Lastly, the method replaces CLIP with a multi-modal large language model (MLLM), significantly boosting compositional reasoning for complex user queries. Extensive experiments on ScanNet200, Replica, ScanNet++, and SceneFun3D datasets demonstrate state-of-the-art performance and strong generalization. <div>
arXiv:2512.03532v1 Announce Type: new 
Abstract: Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation</title>
<link>https://arxiv.org/abs/2512.03534</link>
<guid>https://arxiv.org/abs/2512.03534</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt redesign, inference-time scaling, text-to-visual generation, element-level factual correction, alignment feedback<br /><br />Summary:<br /><br />1. The paper addresses the challenge of achieving precise alignment between user prompts and generated visuals in text-to-visual generation tasks, where a single generation attempt often fails to meet the desired output.  
2. Prior approaches to improve generation quality mainly focus on scaling visual generation parameters such as sampling steps or random seeds but encounter a quality plateau because the guiding prompt remains fixed.  
3. To overcome this limitation, the authors propose PRIS (Prompt Redesign for Inference-time Scaling), a framework that adaptively revises prompts during inference based on analysis of the previously generated visuals.  
4. PRIS identifies recurring failure patterns across multiple visuals and redesigns the prompt accordingly before regenerating the visuals, enabling improved alignment between intent and output.  
5. A novel verifier, element-level factual correction, is introduced to provide fine-grained, interpretable feedback by evaluating alignment between specific prompt attributes and generated visuals, outperforming holistic evaluation methods.  
6. Extensive experiments on text-to-image and text-to-video benchmarks demonstrate the effectiveness of PRIS, showing up to a 15% improvement on the VBench 2.0 benchmark.  
7. The findings emphasize that jointly scaling both prompts and visual generation during inference is crucial for fully leveraging scaling laws, leading to better output quality and alignment. <div>
arXiv:2512.03534v1 Announce Type: new 
Abstract: Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation</title>
<link>https://arxiv.org/abs/2512.03540</link>
<guid>https://arxiv.org/abs/2512.03540</guid>
<content:encoded><![CDATA[
<div> Keywords: Cooking, Diffusion Models, Recipe Illustration, Step-wise Regional Control, Positional Encoding

<br /><br />Summary:  
The paper addresses the challenges of generating coherent and structured multi-step cooking illustrations using diffusion models. It highlights that existing text-to-image diffusion models struggle with multi-step, visually grounded tasks like recipe illustration and often generate a fixed number of images regardless of the recipe's length. To overcome these limitations, the authors propose CookAnything, a diffusion-based framework capable of producing semantically consistent image sequences aligned with each textual cooking instruction step, irrespective of recipe length. CookAnything introduces three novel components: Step-wise Regional Control (SRC) to spatially align individual cooking steps within a single denoising process; Flexible RoPE, a step-aware positional encoding method that improves both temporal coherence across steps and spatial diversity within images; and Cross-Step Consistency Control (CSCC) to ensure fine-grained ingredient consistency throughout the sequence. Experimental evaluations on recipe illustration benchmarks demonstrate that CookAnything outperforms existing methods in both training-based and training-free settings. This framework enables scalable, high-quality visual synthesis for complex procedural instructions and has significant potential applications in instructional media and procedural content creation domains. <div>
arXiv:2512.03540v1 Announce Type: new 
Abstract: Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention</title>
<link>https://arxiv.org/abs/2512.03542</link>
<guid>https://arxiv.org/abs/2512.03542</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, visual neglect, hallucinations, visual inference-time intervention, V-ITI  

<br /><br />Summary:  
Multimodal Large Language Models (MLLMs) perform well on vision-language tasks but often generate hallucinations—content that contradicts the input images—compromising their reliability in precise domains. This problem arises from visual neglect, where models insufficiently prioritize the input visuals during processing. Prior solutions mostly focus on "how to intervene" by adjusting attention scores or output logits but neglect the crucial aspect of "when to intervene," leading to over-intervention issues that cause new hallucinations and increase computational costs. Addressing this, the authors analyze the visual neglect mechanism and find it can be reliably detected through head-level activation patterns in MLLMs. Based on this insight, they propose V-ITI, a lightweight framework consisting of a Visual Neglect Detector and a Visual Recall Intervenor. The detector uses discriminative probes to identify visual neglect during inference, while the recall intervenor selectively modifies activations with stored visual information only when neglect is detected. Experiments across eight benchmarks and multiple MLLM families show that V-ITI effectively reduces vision-related hallucinations without sacrificing general task performance, demonstrating its efficiency and robustness in improving the reliability of vision-language models. <div>
arXiv:2512.03542v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on "how to intervene" but overlooking the prerequisite "when to intervene", which leads to the "over-intervention" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching</title>
<link>https://arxiv.org/abs/2512.03553</link>
<guid>https://arxiv.org/abs/2512.03553</guid>
<content:encoded><![CDATA[
<div> Content Moderation, Livestreaming, Multimodal Large Language Model, Hybrid Framework, Similarity Matching<br /><br />Summary:<br /><br />This paper addresses the critical challenge of content moderation on large-scale user-generated livestreaming platforms, emphasizing the need for timely, multimodal, and robust approaches. The authors propose a hybrid moderation framework that integrates supervised classification for detecting known violations with reference-based similarity matching designed to catch novel or subtle cases that traditional classifiers might miss. The system processes multimodal inputs, including text, audio, and visual data, via two pipelines enhanced by a multimodal large language model (MLLM), which distills knowledge to improve accuracy while keeping inference efficient. In production settings, the classification pipeline achieves a recall of 67% at 80% precision, while the similarity matching pipeline performs better with 76% recall at the same precision level. Large-scale A/B testing demonstrates the framework’s effectiveness by reducing user exposure to unwanted livestream content by 6-8%. Overall, the proposed solution offers a scalable, adaptable, and multimodal strategy for content governance capable of managing both explicit policy violations and emerging adversarial behaviors in dynamic livestream environments. <div>
arXiv:2512.03553v1 Announce Type: new 
Abstract: Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding</title>
<link>https://arxiv.org/abs/2512.03558</link>
<guid>https://arxiv.org/abs/2512.03558</guid>
<content:encoded><![CDATA[
<div> cartographic maps, visual-language models, question answering, geospatial reasoning, OCR errors<br /><br />Summary:<br /><br />The paper introduces CartoMapQA, a novel benchmark created to assess Visual-Language Models’ (LVLMs) capabilities in understanding cartographic maps via question-answering tasks. The dataset comprises over 2,000 samples, each including a map, a question—either open-ended or multiple-choice—and a verified correct answer. Tasks cover a range of map interpretation skills, from basic symbol recognition to complex route-based reasoning, scale interpretation, and extraction of embedded information. Evaluations conducted on both open-source and proprietary LVLMs reveal persistent challenges, including difficulties in grasping map-specific semantics and limitations in geospatial reasoning abilities. Additionally, models frequently suffer from OCR-related errors when interpreting map text. By pinpointing these shortcomings, CartoMapQA serves as a valuable benchmark to inform future LVLM architectural improvements tailored for cartographic data. Ultimately, this work supports advances in creating more robust and reliable models capable of addressing real-world applications such as navigation, geographic information retrieval, and urban planning. The authors have made the source code and dataset publicly accessible, encouraging further research and development in this specialized domain. <div>
arXiv:2512.03558v1 Announce Type: new 
Abstract: The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAOT: Generating Articulated Objects Through Text-Guided Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03566</link>
<guid>https://arxiv.org/abs/2512.03566</guid>
<content:encoded><![CDATA[
<div> Articulated object generation, text conditioning, diffusion models, hypergraph learning, PartNet-Mobility<br /><br />Summary: This paper introduces GAOT, a novel three-phase framework designed to generate 3D articulated objects from text prompts. The method addresses the challenge of linking textual descriptions to detailed 3D object representations. In the first phase, a point cloud generation model is fine-tuned to create a coarse object structure based on the input text. The second phase employs a hypergraph-based learning approach, which refines the initial representation by modeling object parts as graph vertices, effectively capturing the intrinsic connection between articulated objects and graph structures. The final phase focuses on generating the joints of the articulated objects—represented as graph edges—using a diffusion model that operates on the refined graph. The approach was extensively evaluated on the PartNet-Mobility dataset, showing both qualitative and quantitative improvements over previous methods. Overall, GAOT successfully bridges the gap between textual prompts and complex articulated object generation by integrating diffusion techniques with graph-based learning, achieving superior object articulation and structural fidelity. <div>
arXiv:2512.03566v1 Announce Type: new 
Abstract: Articulated object generation has seen increasing advancements, yet existing models often lack the ability to be conditioned on text prompts. To address the significant gap between textual descriptions and 3D articulated object representations, we propose GAOT, a three-phase framework that generates articulated objects from text prompts, leveraging diffusion models and hypergraph learning in a three-step process. First, we fine-tune a point cloud generation model to produce a coarse representation of objects from text prompts. Given the inherent connection between articulated objects and graph structures, we design a hypergraph-based learning method to refine these coarse representations, representing object parts as graph vertices. Finally, leveraging a diffusion model, the joints of articulated objects-represented as graph edges-are generated based on the object parts. Extensive qualitative and quantitative experiments on the PartNet-Mobility dataset demonstrate the effectiveness of our approach, achieving superior performance over previous methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global-Local Aware Scene Text Editing</title>
<link>https://arxiv.org/abs/2512.03574</link>
<guid>https://arxiv.org/abs/2512.03574</guid>
<content:encoded><![CDATA[
<div> Keywords: Scene Text Editing, Global-Local Features, Text Style Consistency, Length Insensitivity, Affine Fusion<br /><br />Summary: Scene Text Editing (STE) aims to replace text within scene images while preserving the original text style and background texture. Existing STE methods face two critical problems: inconsistency between edited local areas and their surrounding context, and difficulty handling variations in text length during editing. To address these, the paper introduces GLASTE, an end-to-end framework that integrates both global contextual information and fine-grained local features. GLASTE employs a novel global-local combination structure alongside joint global and local loss functions to maintain style consistency within local patches and ensure harmony between edited regions and the broader image. The framework represents text style as a vector independent of image size, allowing style transfer across target text images of different dimensions. An affine fusion technique is used to insert the target text into the editing patch while preserving the aspect ratio. Extensive experiments on real-world datasets demonstrate that GLASTE surpasses previous approaches in quantitative evaluation metrics and qualitative visual fidelity. Moreover, it effectively tackles the challenges of context inconsistency and length insensitivity, making it a robust solution for scene text editing tasks. <div>
arXiv:2512.03574v1 Announce Type: new 
Abstract: Scene Text Editing (STE) involves replacing text in a scene image with new target text while preserving both the original text style and background texture. Existing methods suffer from two major challenges: inconsistency and length-insensitivity. They often fail to maintain coherence between the edited local patch and the surrounding area, and they struggle to handle significant differences in text length before and after editing. To tackle these challenges, we propose an end-to-end framework called Global-Local Aware Scene Text Editing (GLASTE), which simultaneously incorporates high-level global contextual information along with delicate local features. Specifically, we design a global-local combination structure, joint global and local losses, and enhance text image features to ensure consistency in text style within local patches while maintaining harmony between local and global areas. Additionally, we express the text style as a vector independent of the image size, which can be transferred to target text images of various sizes. We use an affine fusion to fill target text images into the editing patch while maintaining their aspect ratio unchanged. Extensive experiments on real-world datasets validate that our GLASTE model outperforms previous methods in both quantitative metrics and qualitative results and effectively mitigates the two challenges.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniComp: Rethinking Video Compression Through Informational Uniqueness</title>
<link>https://arxiv.org/abs/2512.03575</link>
<guid>https://arxiv.org/abs/2512.03575</guid>
<content:encoded><![CDATA[
arXiv:2512.03575v1 Announce Type: new 
Abstract: Distinct from attention-based compression methods, this paper presents an information uniqueness driven video compression framework, termed UniComp, which aims to maximize the information fidelity of video representations under constrained computational budgets. Starting from the information-theoretic perspective, we formulate the vision compression as an optimization problem that minimizes conditional entropy (reconstruction error) between retained and full tokens. To achieve this, we introduce the notion of information uniqueness to measure intrinsic redundancy among tokens to link with reconstruction error. Based on uniqueness, we design three modules-Frame Group Fusion, Token Allocation, and Spatial Dynamic Compression-that progressively perform semantic frame grouping, adaptive resource allocation, and fine-grained spatial compression. Extensive experiments demonstrate that UniComp consistently outperforms existing compression methods in preserving essential visual tokens under limited computational budgets, highlighting the pivotal role of information uniqueness in token compression efficacy.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning</title>
<link>https://arxiv.org/abs/2512.03577</link>
<guid>https://arxiv.org/abs/2512.03577</guid>
<content:encoded><![CDATA[
arXiv:2512.03577v1 Announce Type: new 
Abstract: Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&amp;E enriches H&amp;E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&amp;E, HER2, KI67, ER, PGR) to enable paired H&amp;E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&amp;E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&amp;E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes</title>
<link>https://arxiv.org/abs/2512.03580</link>
<guid>https://arxiv.org/abs/2512.03580</guid>
<content:encoded><![CDATA[
arXiv:2512.03580v1 Announce Type: new 
Abstract: We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Boundary Frames: Audio-Visual Semantic Guidance for Context-Aware Video Interpolation</title>
<link>https://arxiv.org/abs/2512.03590</link>
<guid>https://arxiv.org/abs/2512.03590</guid>
<content:encoded><![CDATA[
arXiv:2512.03590v1 Announce Type: new 
Abstract: Handling fast, complex, and highly non-linear motion patterns has long posed challenges for video frame interpolation. Although recent diffusion-based approaches improve upon traditional optical-flow-based methods, they still struggle to cover diverse application scenarios and often fail to produce sharp, temporally consistent frames in fine-grained motion tasks such as audio-visual synchronized interpolation. To address these limitations, we introduce BBF (Beyond Boundary Frames), a context-aware video frame interpolation framework, which could be guided by audio/visual semantics. First, we enhance the input design of the interpolation model so that it can flexibly handle multiple conditional modalities, including text, audio, images, and video. Second, we propose a decoupled multimodal fusion mechanism that sequentially injects different conditional signals into a DiT backbone. Finally, to maintain the generation abilities of the foundation model, we adopt a progressive multi-stage training paradigm, where the start-end frame difference embedding is used to dynamically adjust both the data sampling and the loss weighting. Extensive experimental results demonstrate that BBF outperforms specialized state-of-the-art methods on both generic interpolation and audio-visual synchronized interpolation tasks, establishing a unified framework for video frame interpolation under coordinated multi-channel conditioning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Hypergraphs in Geometric Deep Learning for 3D RNA Inverse Folding</title>
<link>https://arxiv.org/abs/2512.03592</link>
<guid>https://arxiv.org/abs/2512.03592</guid>
<content:encoded><![CDATA[
arXiv:2512.03592v1 Announce Type: new 
Abstract: The RNA inverse folding problem, a key challenge in RNA design, involves identifying nucleotide sequences that can fold into desired secondary structures, which are critical for ensuring molecular stability and function. The inherent complexity of this task stems from the intricate relationship between sequence and structure, making it particularly challenging. In this paper, we propose a framework, named HyperRNA, a generative model with an encoder-decoder architecture that leverages hypergraphs to design RNA sequences. Specifically, our HyperRNA model consists of three main components: preprocessing, encoding and decoding.
  In the preprocessing stage, graph structures are constructed by extracting the atom coordinates of RNA backbone based on 3-bead coarse-grained representation. The encoding stage processes these graphs, capturing higher order dependencies and complex biomolecular interactions using an attention embedding module and a hypergraph-based encoder. Finally, the decoding stage generates the RNA sequence in an autoregressive manner. We conducted quantitative and qualitative experiments on the PDBBind and RNAsolo datasets to evaluate the inverse folding task for RNA sequence generation and RNA-protein complex sequence generation. The experimental results demonstrate that HyperRNA not only outperforms existing RNA design methods but also highlights the potential of leveraging hypergraphs in RNA engineering.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CloseUpAvatar: High-Fidelity Animatable Full-Body Avatars with Mixture of Multi-Scale Textures</title>
<link>https://arxiv.org/abs/2512.03593</link>
<guid>https://arxiv.org/abs/2512.03593</guid>
<content:encoded><![CDATA[
arXiv:2512.03593v1 Announce Type: new 
Abstract: We present a CloseUpAvatar - a novel approach for articulated human avatar representation dealing with more general camera motions, while preserving rendering quality for close-up views. CloseUpAvatar represents an avatar as a set of textured planes with two sets of learnable textures for low and high-frequency detail. The method automatically switches to high-frequency textures only for cameras positioned close to the avatar's surface and gradually reduces their impact as the camera moves farther away. Such parametrization of the avatar enables CloseUpAvatar to adjust rendering quality based on camera distance ensuring realistic rendering across a wider range of camera orientations than previous approaches. We provide experiments using the ActorsHQ dataset with high-resolution input images. CloseUpAvatar demonstrates both qualitative and quantitative improvements over existing methods in rendering from novel wide range camera positions, while maintaining high FPS by limiting the number of required primitives.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HBFormer: A Hybrid-Bridge Transformer for Microtumor and Miniature Organ Segmentation</title>
<link>https://arxiv.org/abs/2512.03597</link>
<guid>https://arxiv.org/abs/2512.03597</guid>
<content:encoded><![CDATA[
arXiv:2512.03597v1 Announce Type: new 
Abstract: Medical image segmentation is a cornerstone of modern clinical diagnostics. While Vision Transformers that leverage shifted window-based self-attention have established new benchmarks in this field, they are often hampered by a critical limitation: their localized attention mechanism struggles to effectively fuse local details with global context. This deficiency is particularly detrimental to challenging tasks such as the segmentation of microtumors and miniature organs, where both fine-grained boundary definition and broad contextual understanding are paramount. To address this gap, we propose HBFormer, a novel Hybrid-Bridge Transformer architecture. The 'Hybrid' design of HBFormer synergizes a classic U-shaped encoder-decoder framework with a powerful Swin Transformer backbone for robust hierarchical feature extraction. The core innovation lies in its 'Bridge' mechanism, a sophisticated nexus for multi-scale feature integration. This bridge is architecturally embodied by our novel Multi-Scale Feature Fusion (MFF) decoder. Departing from conventional symmetric designs, the MFF decoder is engineered to fuse multi-scale features from the encoder with global contextual information. It achieves this through a synergistic combination of channel and spatial attention modules, which are constructed from a series of dilated and depth-wise convolutions. These components work in concert to create a powerful feature bridge that explicitly captures long-range dependencies and refines object boundaries with exceptional precision. Comprehensive experiments on challenging medical image segmentation datasets, including multi-organ, liver tumor, and bladder tumor benchmarks, demonstrate that HBFormer achieves state-of-the-art results, showcasing its outstanding capabilities in microtumor and miniature organ segmentation. Code and models are available at: https://github.com/lzeeorno/HBFormer.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Guided Point Cloud Completion for Dental Reconstruction</title>
<link>https://arxiv.org/abs/2512.03598</link>
<guid>https://arxiv.org/abs/2512.03598</guid>
<content:encoded><![CDATA[
arXiv:2512.03598v1 Announce Type: new 
Abstract: Partial dental point clouds often suffer from large missing regions caused by occlusion and limited scanning views, which bias encoder-only global features and force decoders to hallucinate structures. We propose a retrieval-augmented framework for tooth completion that integrates a prototype memory into standard encoder--decoder pipelines. After encoding a partial input into a global descriptor, the model retrieves the nearest manifold prototype from a learnable memory and fuses it with the query feature through confidence-gated weighting before decoding. The memory is optimized end-to-end and self-organizes into reusable tooth-shape prototypes without requiring tooth-position labels, thereby providing structural priors that stabilize missing-region inference and free decoder capacity for detail recovery. The module is plug-and-play and compatible with common completion backbones, while keeping the same training losses. Experiments on a self-processed Teeth3DS benchmark demonstrate consistent improvements in Chamfer Distance, with visualizations showing sharper cusps, ridges, and interproximal transitions. Our approach provides a simple yet effective way to exploit cross-sample regularities for more accurate and faithful dental point-cloud completion.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding</title>
<link>https://arxiv.org/abs/2512.03601</link>
<guid>https://arxiv.org/abs/2512.03601</guid>
<content:encoded><![CDATA[
arXiv:2512.03601v1 Announce Type: new 
Abstract: Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAMP: Language-Assisted Motion Planning for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2512.03619</link>
<guid>https://arxiv.org/abs/2512.03619</guid>
<content:encoded><![CDATA[
arXiv:2512.03619v1 Announce Type: new 
Abstract: Video generation has achieved remarkable progress in visual fidelity and controllability, enabling conditioning on text, layout, or motion. Among these, motion control - specifying object dynamics and camera trajectories - is essential for composing complex, cinematic scenes, yet existing interfaces remain limited. We introduce LAMP that leverages large language models (LLMs) as motion planners to translate natural language descriptions into explicit 3D trajectories for dynamic objects and (relatively defined) cameras. LAMP defines a motion domain-specific language (DSL), inspired by cinematography conventions. By harnessing program synthesis capabilities of LLMs, LAMP generates structured motion programs from natural language, which are deterministically mapped to 3D trajectories. We construct a large-scale procedural dataset pairing natural text descriptions with corresponding motion programs and 3D trajectories. Experiments demonstrate LAMP's improved performance in motion controllability and alignment with user intent compared to state-of-the-art alternatives establishing the first framework for generating both object and camera motions directly from natural language specifications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation</title>
<link>https://arxiv.org/abs/2512.03621</link>
<guid>https://arxiv.org/abs/2512.03621</guid>
<content:encoded><![CDATA[
arXiv:2512.03621v1 Announce Type: new 
Abstract: We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FeatureLens: A Highly Generalizable and Interpretable Framework for Detecting Adversarial Examples Based on Image Features</title>
<link>https://arxiv.org/abs/2512.03625</link>
<guid>https://arxiv.org/abs/2512.03625</guid>
<content:encoded><![CDATA[
arXiv:2512.03625v1 Announce Type: new 
Abstract: Although the remarkable performance of deep neural networks (DNNs) in image classification, their vulnerability to adversarial attacks remains a critical challenge. Most existing detection methods rely on complex and poorly interpretable architectures, which compromise interpretability and generalization. To address this, we propose FeatureLens, a lightweight framework that acts as a lens to scrutinize anomalies in image features. Comprising an Image Feature Extractor (IFE) and shallow classifiers (e.g., SVM, MLP, or XGBoost) with model sizes ranging from 1,000 to 30,000 parameters, FeatureLens achieves high detection accuracy ranging from 97.8% to 99.75% in closed-set evaluation and 86.17% to 99.6% in generalization evaluation across FGSM, PGD, CW, and DAmageNet attacks, using only 51 dimensional features. By combining strong detection performance with excellent generalization, interpretability, and computational efficiency, FeatureLens offers a practical pathway toward transparent and effective adversarial defense.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms</title>
<link>https://arxiv.org/abs/2512.03640</link>
<guid>https://arxiv.org/abs/2512.03640</guid>
<content:encoded><![CDATA[
arXiv:2512.03640v1 Announce Type: new 
Abstract: Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optical Context Compression Is Just (Bad) Autoencoding</title>
<link>https://arxiv.org/abs/2512.03643</link>
<guid>https://arxiv.org/abs/2512.03643</guid>
<content:encoded><![CDATA[
arXiv:2512.03643v1 Announce Type: new 
Abstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Visual Prompting for Lightweight Small-Image Classification</title>
<link>https://arxiv.org/abs/2512.03663</link>
<guid>https://arxiv.org/abs/2512.03663</guid>
<content:encoded><![CDATA[
arXiv:2512.03663v1 Announce Type: new 
Abstract: Visual prompting has recently emerged as an efficient strategy to adapt vision models using lightweight, learnable parameters injected into the input space. However, prior work mainly targets large Vision Transformers and high-resolution datasets such as ImageNet. In contrast, small-image benchmarks like MNIST, Fashion-MNIST, and CIFAR-10 remain widely used in education, prototyping, and research, yet have received little attention in the context of prompting. In this paper, we introduce \textbf{Multi-Scale Visual Prompting (MSVP)}, a simple and generic module that learns a set of global, mid-scale, and local prompt maps fused with the input image via a lightweight $1 \times 1$ convolution. MSVP is backbone-agnostic, adds less than $0.02\%$ parameters, and significantly improves performance across CNN and Vision Transformer backbones.
  We provide a unified benchmark on MNIST, Fashion-MNIST, and CIFAR-10 using a simple CNN, ResNet-18, and a small Vision Transformer. Our method yields consistent improvements with negligible computational overhead. We further include ablations on prompt scales, fusion strategies, and backbone architectures, along with qualitative analyzes using prompt visualizations and Grad-CAM. Our results demonstrate that multi-scale prompting provides an effective inductive bias even on low-resolution images.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos</title>
<link>https://arxiv.org/abs/2512.03666</link>
<guid>https://arxiv.org/abs/2512.03666</guid>
<content:encoded><![CDATA[
arXiv:2512.03666v1 Announce Type: new 
Abstract: A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning</title>
<link>https://arxiv.org/abs/2512.03667</link>
<guid>https://arxiv.org/abs/2512.03667</guid>
<content:encoded><![CDATA[
arXiv:2512.03667v1 Announce Type: new 
Abstract: In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.03673</link>
<guid>https://arxiv.org/abs/2512.03673</guid>
<content:encoded><![CDATA[
arXiv:2512.03673v1 Announce Type: new 
Abstract: Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces</title>
<link>https://arxiv.org/abs/2512.03683</link>
<guid>https://arxiv.org/abs/2512.03683</guid>
<content:encoded><![CDATA[
arXiv:2512.03683v1 Announce Type: new 
Abstract: 3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Visual Perception: Opportunities and Challenges</title>
<link>https://arxiv.org/abs/2512.03687</link>
<guid>https://arxiv.org/abs/2512.03687</guid>
<content:encoded><![CDATA[
arXiv:2512.03687v1 Announce Type: new 
Abstract: Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic, Interpretable, Perceptual Metric Between Images</title>
<link>https://arxiv.org/abs/2512.03701</link>
<guid>https://arxiv.org/abs/2512.03701</guid>
<content:encoded><![CDATA[
arXiv:2512.03701v1 Announce Type: new 
Abstract: Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS, achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties.
  We introduce the Structured Uncertainty Similarity Score (SUSS); it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling.
  SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction</title>
<link>https://arxiv.org/abs/2512.03715</link>
<guid>https://arxiv.org/abs/2512.03715</guid>
<content:encoded><![CDATA[
arXiv:2512.03715v1 Announce Type: new 
Abstract: This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images. The
  method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and
  matching. DINO is employed to retrieve semantically relevant image pairs in large collections, while
  rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue. Experiments on the Kaggle Image Matching Challenge 2025 demonstrate consistent improve ments in mean Average Accuracy (mAA), achieving a Silver Award (47th of 943 teams). The results
  confirm that combining self-supervised global descriptors with rotation-enhanced local matching offers
  a robust and scalable solution for large-scale 3D reconstruction.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention</title>
<link>https://arxiv.org/abs/2512.03724</link>
<guid>https://arxiv.org/abs/2512.03724</guid>
<content:encoded><![CDATA[
arXiv:2512.03724v1 Announce Type: new 
Abstract: The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-the-box: Black-box Causal Attacks on Object Detectors</title>
<link>https://arxiv.org/abs/2512.03730</link>
<guid>https://arxiv.org/abs/2512.03730</guid>
<content:encoded><![CDATA[
arXiv:2512.03730v1 Announce Type: new 
Abstract: Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2512.03745</link>
<guid>https://arxiv.org/abs/2512.03745</guid>
<content:encoded><![CDATA[
arXiv:2512.03745v1 Announce Type: new 
Abstract: Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking with Programming Vision: Towards a Unified View for Thinking with Images</title>
<link>https://arxiv.org/abs/2512.03746</link>
<guid>https://arxiv.org/abs/2512.03746</guid>
<content:encoded><![CDATA[
arXiv:2512.03746v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fully Unsupervised Self-debiasing of Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03749</link>
<guid>https://arxiv.org/abs/2512.03749</guid>
<content:encoded><![CDATA[
arXiv:2512.03749v1 Announce Type: new 
Abstract: Text-to-image (T2I) diffusion models have achieved widespread success due to their ability to generate high-resolution, photorealistic images. These models are trained on large-scale datasets, like LAION-5B, often scraped from the internet. However, since this data contains numerous biases, the models inherently learn and reproduce them, resulting in stereotypical outputs. We introduce SelfDebias, a fully unsupervised test-time debiasing method applicable to any diffusion model that uses a UNet as its noise predictor. SelfDebias identifies semantic clusters in an image encoder's embedding space and uses these clusters to guide the diffusion process during inference, minimizing the KL divergence between the output distribution and the uniform distribution. Unlike supervised approaches, SelfDebias does not require human-annotated datasets or external classifiers trained for each generated concept. Instead, it is designed to automatically identify semantic modes. Extensive experiments show that SelfDebias generalizes across prompts and diffusion model architectures, including both conditional and unconditional models. It not only effectively debiases images along key demographic dimensions while maintaining the visual fidelity of the generated images, but also more abstract concepts for which identifying biases is also challenging.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Research on Brain Tumor Classification Method Based on Improved ResNet34 Network</title>
<link>https://arxiv.org/abs/2512.03751</link>
<guid>https://arxiv.org/abs/2512.03751</guid>
<content:encoded><![CDATA[
arXiv:2512.03751v1 Announce Type: new 
Abstract: Previously, image interpretation in radiology relied heavily on manual methods. However, manual classification of brain tumor medical images is time-consuming and labor-intensive. Even with shallow convolutional neural network models, the accuracy is not ideal. To improve the efficiency and accuracy of brain tumor image classification, this paper proposes a brain tumor classification model based on an improved ResNet34 network. This model uses the ResNet34 residual network as the backbone network and incorporates multi-scale feature extraction. It uses a multi-scale input module as the first layer of the ResNet34 network and an Inception v2 module as the residual downsampling layer. Furthermore, a channel attention mechanism module assigns different weights to different channels of the image from a channel domain perspective, obtaining more important feature information. The results after a five-fold crossover experiment show that the average classification accuracy of the improved network model is approximately 98.8%, which is not only 1% higher than ResNet34, but also only 80% of the number of parameters of the original model. Therefore, the improved network model not only improves accuracy but also reduces clutter, achieving a classification effect with fewer parameters and higher accuracy.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition</title>
<link>https://arxiv.org/abs/2512.03794</link>
<guid>https://arxiv.org/abs/2512.03794</guid>
<content:encoded><![CDATA[
arXiv:2512.03794v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling</title>
<link>https://arxiv.org/abs/2512.03796</link>
<guid>https://arxiv.org/abs/2512.03796</guid>
<content:encoded><![CDATA[
arXiv:2512.03796v1 Announce Type: new 
Abstract: Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English</title>
<link>https://arxiv.org/abs/2512.03817</link>
<guid>https://arxiv.org/abs/2512.03817</guid>
<content:encoded><![CDATA[
arXiv:2512.03817v1 Announce Type: new 
Abstract: Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Robust Camera-based Method for Breath Rate Measurement</title>
<link>https://arxiv.org/abs/2512.03827</link>
<guid>https://arxiv.org/abs/2512.03827</guid>
<content:encoded><![CDATA[
arXiv:2512.03827v1 Announce Type: new 
Abstract: Proliferation of cheap and accessible cameras makes it possible to measure a subject's breath rate from video footage alone. Recent works on this topic have proposed a variety of approaches for accurately measuring human breath rate, however they are either tested in near-ideal conditions, or produce results that are not sufficiently accurate. The present study proposes a more robust method to measure breath rate in humans with minimal hardware requirements using a combination of mathematical transforms with a relative deviation from the ground truth of less than 5%. The method was tested on videos taken from 14 volunteers with a total duration of over 2 hours 30 minutes. The obtained results were compared to reference data and the average mean absolute error was found to be at 0.57 respirations per minute, which is noticeably better than the results from previous works. The breath rate measurement method proposed in the present article is more resistant to distortions caused by subject movement and thus allows one to remotely measure the subject's breath rate without any significant limitations on the subject's behavior.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lean Unet: A Compact Model for Image Segmentation</title>
<link>https://arxiv.org/abs/2512.03834</link>
<guid>https://arxiv.org/abs/2512.03834</guid>
<content:encoded><![CDATA[
arXiv:2512.03834v1 Announce Type: new 
Abstract: Unet and its variations have been standard in semantic image segmentation, especially for computer assisted radiology. Current Unet architectures iteratively downsample spatial resolution while increasing channel dimensions to preserve information content. Such a structure demands a large memory footprint, limiting training batch sizes and increasing inference latency. Channel pruning compresses Unet architecture without accuracy loss, but requires lengthy optimization and may not generalize across tasks and datasets. By investigating Unet pruning, we hypothesize that the final structure is the crucial factor, not the channel selection strategy of pruning. Based on our observations, we propose a lean Unet architecture (LUnet) with a compact, flat hierarchy where channels are not doubled as resolution is halved. We evaluate on a public MRI dataset allowing comparable reporting, as well as on two internal CT datasets. We show that a state-of-the-art pruning solution (STAMP) mainly prunes from the layers with the highest number of channels. Comparatively, simply eliminating a random channel at the pruning-identified layer or at the largest layer achieves similar or better performance. Our proposed LUnet with fixed architectures and over 30 times fewer parameters achieves performance comparable to both conventional Unet counterparts and data-adaptively pruned networks. The proposed lean Unet with constant channel count across layers requires far fewer parameters while achieving performance superior to standard Unet for the same total number of parameters. Skip connections allow Unet bottleneck channels to be largely reduced, unlike standard encoder-decoder architectures requiring increased bottleneck channels for information propagation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heatmap Pooling Network for Action Recognition from RGB Videos</title>
<link>https://arxiv.org/abs/2512.03837</link>
<guid>https://arxiv.org/abs/2512.03837</guid>
<content:encoded><![CDATA[
arXiv:2512.03837v1 Announce Type: new 
Abstract: Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation</title>
<link>https://arxiv.org/abs/2512.03844</link>
<guid>https://arxiv.org/abs/2512.03844</guid>
<content:encoded><![CDATA[
arXiv:2512.03844v1 Announce Type: new 
Abstract: Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the "intrinsic core distribution" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation</title>
<link>https://arxiv.org/abs/2512.03848</link>
<guid>https://arxiv.org/abs/2512.03848</guid>
<content:encoded><![CDATA[
arXiv:2512.03848v1 Announce Type: new 
Abstract: Cardiac image analysis remains fragmented across tasks: anatomical segmentation, disease classification, and grounded clinical report generation are typically handled by separate networks trained under different data regimes. No existing framework unifies these objectives within a single architecture while retaining generalization across imaging modalities and datasets. We introduce PULSE, a multi-task vision-language framework built on self-supervised representations and optimized through a composite supervision strategy that balances region overlap learning, pixel wise classification fidelity, and boundary aware IoU refinement. A multi-scale token reconstruction decoder enables anatomical segmentation, while shared global representations support disease classification and clinically grounded text output allowing the model to transition from pixels to structures and finally clinical reasoning within one architecture. Unlike prior task-specific pipelines, PULSE learns task-invariant cardiac priors, generalizes robustly across datasets, and can be adapted to new imaging modalities with minimal supervision. This moves the field closer to a scalable, foundation style cardiac analysis framework.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Traffic Image Restoration under Adverse Weather via Frequency-Aware Mamba</title>
<link>https://arxiv.org/abs/2512.03852</link>
<guid>https://arxiv.org/abs/2512.03852</guid>
<content:encoded><![CDATA[
arXiv:2512.03852v1 Announce Type: new 
Abstract: Traffic image restoration under adverse weather conditions remains a critical challenge for intelligent transportation systems. Existing methods primarily focus on spatial-domain modeling but neglect frequency-domain priors. Although the emerging Mamba architecture excels at long-range dependency modeling through patch-wise correlation analysis, its potential for frequency-domain feature extraction remains unexplored. To address this, we propose Frequency-Aware Mamba (FAMamba), a novel framework that integrates frequency guidance with sequence modeling for efficient image restoration. Our architecture consists of two key components: (1) a Dual-Branch Feature Extraction Block (DFEB) that enhances local-global interaction via bidirectional 2D frequency-adaptive scanning, dynamically adjusting traversal paths based on sub-band texture distributions; and (2) a Prior-Guided Block (PGB) that refines texture details through wavelet-based high-frequency residual learning, enabling high-quality image reconstruction with precise details. Meanwhile, we design a novel Adaptive Frequency Scanning Mechanism (AFSM) for the Mamba architecture, which enables the Mamba to achieve frequency-domain scanning across distinct subgraphs, thereby fully leveraging the texture distribution characteristics inherent in subgraph structures. Extensive experiments demonstrate the efficiency and effectiveness of FAMamba.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population</title>
<link>https://arxiv.org/abs/2512.03854</link>
<guid>https://arxiv.org/abs/2512.03854</guid>
<content:encoded><![CDATA[
arXiv:2512.03854v1 Announce Type: new 
Abstract: Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diminishing Returns in Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2512.03862</link>
<guid>https://arxiv.org/abs/2512.03862</guid>
<content:encoded><![CDATA[
arXiv:2512.03862v1 Announce Type: new 
Abstract: While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Automated Framework for Large-Scale Graph-Based Cerebrovascular Analysis</title>
<link>https://arxiv.org/abs/2512.03869</link>
<guid>https://arxiv.org/abs/2512.03869</guid>
<content:encoded><![CDATA[
arXiv:2512.03869v1 Announce Type: new 
Abstract: We present CaravelMetrics, a computational framework for automated cerebrovascular analysis that models vessel morphology through skeletonization-derived graph representations. The framework integrates atlas-based regional parcellation, centerline extraction, and graph construction to compute fifteen morphometric, topological, fractal, and geometric features. The features can be estimated globally from the complete vascular network or regionally within arterial territories, enabling multiscale characterization of cerebrovascular organization. Applied to 570 3D TOF-MRA scans from the IXI dataset (ages 20-86), CaravelMetrics yields reproducible vessel graphs capturing age- and sex-related variations and education-associated increases in vascular complexity, consistent with findings reported in the literature. The framework provides a scalable and fully automated approach for quantitative cerebrovascular feature extraction, supporting normative modeling and population-level studies of vascular health and aging.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy</title>
<link>https://arxiv.org/abs/2512.03883</link>
<guid>https://arxiv.org/abs/2512.03883</guid>
<content:encoded><![CDATA[
arXiv:2512.03883v1 Announce Type: new 
Abstract: Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images during WW are essential to manage care and prevent distant metastases. Hence, we developed a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to combine longitudinal endoscopic images at restaging and follow-up and distinguish cCR from LR. SSDCA leverages pretrained Swin transformers to extract domain agnostic features and enhance robustness to imaging variations. Dual cross attention is implemented to emphasize features from the two scans without requiring any spatial alignment of images to predict response. SSDCA as well as Swin-based baselines were trained using image pairs from 135 patients and evaluated on a held-out set of image pairs from 62 patients. SSDCA produced the best balanced accuracy (81.76\% $\pm$ 0.04), sensitivity (90.07\% $\pm$ 0.08), and specificity (72.86\% $\pm$ 0.05). Robustness analysis showed stable performance irrespective of artifacts including blood, stool, telangiectasia, and poor image quality. UMAP clustering of extracted features showed maximal inter-cluster separation (1.45 $\pm$ 0.18) and minimal intra-cluster dispersion (1.07 $\pm$ 0.19) with SSDCA, confirming discriminative representation learning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence</title>
<link>https://arxiv.org/abs/2512.03905</link>
<guid>https://arxiv.org/abs/2512.03905</guid>
<content:encoded><![CDATA[
arXiv:2512.03905v1 Announce Type: new 
Abstract: The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework</title>
<link>https://arxiv.org/abs/2512.03918</link>
<guid>https://arxiv.org/abs/2512.03918</guid>
<content:encoded><![CDATA[
arXiv:2512.03918v1 Announce Type: new 
Abstract: We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Ground Truth: Enhanced Supervision for Image Restoration</title>
<link>https://arxiv.org/abs/2512.03932</link>
<guid>https://arxiv.org/abs/2512.03932</guid>
<content:encoded><![CDATA[
arXiv:2512.03932v1 Announce Type: new 
Abstract: Deep learning-based image restoration has achieved significant success. However, when addressing real-world degradations, model performance is limited by the quality of ground-truth images in datasets due to practical constraints in data acquisition. To address this limitation, we propose a novel framework that enhances existing ground truth images to provide higher-quality supervision for real-world restoration. Our framework generates perceptually enhanced ground truth images using super-resolution by incorporating adaptive frequency masks, which are learned by a conditional frequency mask generator. These masks guide the optimal fusion of frequency components from the original ground truth and its super-resolved variants, yielding enhanced ground truth images. This frequency-domain mixup preserves the semantic consistency of the original content while selectively enriching perceptual details, preventing hallucinated artifacts that could compromise fidelity. The enhanced ground truth images are used to train a lightweight output refinement network that can be seamlessly integrated with existing restoration models. Extensive experiments demonstrate that our approach consistently improves the quality of restored images. We further validate the effectiveness of both supervision enhancement and output refinement through user studies. Code is available at https://github.com/dhryougit/Beyond-the-Ground-Truth.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUT3R: Motion-aware Updating Transformer for Dynamic 3D Reconstruction</title>
<link>https://arxiv.org/abs/2512.03939</link>
<guid>https://arxiv.org/abs/2512.03939</guid>
<content:encoded><![CDATA[
arXiv:2512.03939v1 Announce Type: new 
Abstract: Recent stateful recurrent neural networks have achieved remarkable progress on static 3D reconstruction but remain vulnerable to motion-induced artifacts, where non-rigid regions corrupt attention propagation between the spatial memory and image feature. By analyzing the internal behaviors of the state and image token updating mechanism, we find that aggregating self-attention maps across layers reveals a consistent pattern: dynamic regions are naturally down-weighted, exposing an implicit motion cue that the pretrained transformer already encodes but never explicitly uses. Motivated by this observation, we introduce MUT3R, a training-free framework that applies the attention-derived motion cue to suppress dynamic content in the early layers of the transformer during inference. Our attention-level gating module suppresses the influence of dynamic regions before their artifacts propagate through the feature hierarchy. Notably, we do not retrain or fine-tune the model; we let the pretrained transformer diagnose its own motion cues and correct itself. This early regulation stabilizes geometric reasoning in streaming scenarios and leads to improvements in temporal consistency and camera pose robustness across multiple dynamic benchmarks, offering a simple and training-free pathway toward motion-aware streaming reconstruction.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.03963</link>
<guid>https://arxiv.org/abs/2512.03963</guid>
<content:encoded><![CDATA[
arXiv:2512.03963v1 Announce Type: new 
Abstract: Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training for Identity, Inference for Controllability: A Unified Approach to Tuning-Free Face Personalization</title>
<link>https://arxiv.org/abs/2512.03964</link>
<guid>https://arxiv.org/abs/2512.03964</guid>
<content:encoded><![CDATA[
arXiv:2512.03964v1 Announce Type: new 
Abstract: Tuning-free face personalization methods have developed along two distinct paradigms: text embedding approaches that map facial features into the text embedding space, and adapter-based methods that inject features through auxiliary cross-attention layers. While both paradigms have shown promise, existing methods struggle to simultaneously achieve high identity fidelity and flexible text controllability. We introduce UniID, a unified tuning-free framework that synergistically integrates both paradigms. Our key insight is that when merging these approaches, they should mutually reinforce only identity-relevant information while preserving the original diffusion prior for non-identity attributes. We realize this through a principled training-inference strategy: during training, we employ an identity-focused learning scheme that guides both branches to capture identity features exclusively; at inference, we introduce a normalized rescaling mechanism that recovers the text controllability of the base diffusion model while enabling complementary identity signals to enhance each other. This principled design enables UniID to achieve high-fidelity face personalization with flexible text controllability. Extensive experiments against six state-of-the-art methods demonstrate that UniID achieves superior performance in both identity preservation and text controllability. Code will be available at https://github.com/lyuPang/UniID
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlurDM: A Blur Diffusion Model for Image Deblurring</title>
<link>https://arxiv.org/abs/2512.03979</link>
<guid>https://arxiv.org/abs/2512.03979</guid>
<content:encoded><![CDATA[
arXiv:2512.03979v1 Announce Type: new 
Abstract: Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment</title>
<link>https://arxiv.org/abs/2512.03981</link>
<guid>https://arxiv.org/abs/2512.03981</guid>
<content:encoded><![CDATA[
arXiv:2512.03981v1 Announce Type: new 
Abstract: Drag-based image editing using generative models provides intuitive control over image structures. However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision. Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts. To address these limitations, we propose DirectDrag, a novel mask- and prompt-free editing framework. DirectDrag enables precise and efficient manipulation with minimal user input while maintaining high image fidelity and accurate point alignment. DirectDrag introduces two key innovations. First, we design an Auto Soft Mask Generation module that intelligently infers editable regions from point displacement, automatically localizing deformation along movement paths while preserving contextual integrity through the generative model's inherent capacity. Second, we develop a Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits, substantially improving visual fidelity. Despite operating without manual mask or prompt, DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy. Extensive experiments on DragBench and real-world scenarios demonstrate the effectiveness and practicality of DirectDrag for high-quality, interactive image manipulation. Project Page: https://frakw.github.io/DirectDrag/. Code is available at: https://github.com/frakw/DirectDrag.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation</title>
<link>https://arxiv.org/abs/2512.03992</link>
<guid>https://arxiv.org/abs/2512.03992</guid>
<content:encoded><![CDATA[
arXiv:2512.03992v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation</title>
<link>https://arxiv.org/abs/2512.03996</link>
<guid>https://arxiv.org/abs/2512.03996</guid>
<content:encoded><![CDATA[
arXiv:2512.03996v1 Announce Type: new 
Abstract: Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2512.04000</link>
<guid>https://arxiv.org/abs/2512.04000</guid>
<content:encoded><![CDATA[
arXiv:2512.04000v1 Announce Type: new 
Abstract: The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Temporality for Sketch Representation Learning</title>
<link>https://arxiv.org/abs/2512.04007</link>
<guid>https://arxiv.org/abs/2512.04007</guid>
<content:encoded><![CDATA[
arXiv:2512.04007v1 Announce Type: new 
Abstract: Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Outlier View Rejection in Visual Geometry Grounded Transformers</title>
<link>https://arxiv.org/abs/2512.04012</link>
<guid>https://arxiv.org/abs/2512.04012</guid>
<content:encoded><![CDATA[
arXiv:2512.04012v1 Announce Type: new 
Abstract: Reliable 3D reconstruction from in-the-wild image collections is often hindered by "noisy" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Group Actions In Disentangled Latent Image Representations</title>
<link>https://arxiv.org/abs/2512.04015</link>
<guid>https://arxiv.org/abs/2512.04015</guid>
<content:encoded><![CDATA[
arXiv:2512.04015v1 Announce Type: new 
Abstract: Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ultra-lightweight Neural Video Representation Compression</title>
<link>https://arxiv.org/abs/2512.04019</link>
<guid>https://arxiv.org/abs/2512.04019</guid>
<content:encoded><![CDATA[
arXiv:2512.04019v1 Announce Type: new 
Abstract: Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C3G: Learning Compact 3D Representations with 2K Gaussians</title>
<link>https://arxiv.org/abs/2512.04021</link>
<guid>https://arxiv.org/abs/2512.04021</guid>
<content:encoded><![CDATA[
arXiv:2512.04021v1 Announce Type: new 
Abstract: Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.04025</link>
<guid>https://arxiv.org/abs/2512.04025</guid>
<content:encoded><![CDATA[
arXiv:2512.04025v1 Announce Type: new 
Abstract: Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast &amp; Efficient Normalizing Flows and Applications of Image Generative Models</title>
<link>https://arxiv.org/abs/2512.04039</link>
<guid>https://arxiv.org/abs/2512.04039</guid>
<content:encoded><![CDATA[
arXiv:2512.04039v1 Announce Type: new 
Abstract: This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.
  The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RELIC: Interactive Video World Model with Long-Horizon Memory</title>
<link>https://arxiv.org/abs/2512.04040</link>
<guid>https://arxiv.org/abs/2512.04040</guid>
<content:encoded><![CDATA[
arXiv:2512.04040v1 Announce Type: new 
Abstract: A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable Signer: Hierarchical Sign Language Generative Model</title>
<link>https://arxiv.org/abs/2512.04048</link>
<guid>https://arxiv.org/abs/2512.04048</guid>
<content:encoded><![CDATA[
arXiv:2512.04048v1 Announce Type: new 
Abstract: Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL</title>
<link>https://arxiv.org/abs/2512.04069</link>
<guid>https://arxiv.org/abs/2512.04069</guid>
<content:encoded><![CDATA[
arXiv:2512.04069v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design</title>
<link>https://arxiv.org/abs/2512.04082</link>
<guid>https://arxiv.org/abs/2512.04082</guid>
<content:encoded><![CDATA[
arXiv:2512.04082v1 Announce Type: new 
Abstract: Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows</title>
<link>https://arxiv.org/abs/2512.04084</link>
<guid>https://arxiv.org/abs/2512.04084</guid>
<content:encoded><![CDATA[
arXiv:2512.04084v1 Announce Type: new 
Abstract: Normalizing Flows (NFs) learn invertible mappings between the data and a Gaussian distribution. Prior works usually suffer from two limitations. First, they add random noise to training samples or VAE latents as data augmentation, introducing complex pipelines including extra noising and denoising steps. Second, they use a pretrained and frozen VAE encoder, resulting in suboptimal reconstruction and generation quality. In this paper, we find that the two issues can be solved in a very simple way: just fixing the variance (which would otherwise be predicted by the VAE encoder) to a constant (e.g., 0.5). On the one hand, this method allows the encoder to output a broader distribution of tokens and the decoder to learn to reconstruct clean images from the augmented token distribution, avoiding additional noise or denoising design. On the other hand, fixed variance simplifies the VAE evidence lower bound, making it stable to train an NF with a VAE jointly. On the ImageNet $256 \times 256$ generation task, our model SimFlow obtains a gFID score of 2.15, outperforming the state-of-the-art method STARFlow (gFID 2.40). Moreover, SimFlow can be seamlessly integrated with the end-to-end representation alignment (REPA-E) method and achieves an improved gFID of 1.91, setting a new state of the art among NFs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unique Lives, Shared World: Learning from Single-Life Videos</title>
<link>https://arxiv.org/abs/2512.04085</link>
<guid>https://arxiv.org/abs/2512.04085</guid>
<content:encoded><![CDATA[
arXiv:2512.04085v1 Announce Type: new 
Abstract: We introduce the "single-life" learning paradigm, where we train a distinct vision model exclusively on egocentric videos captured by one individual. We leverage the multiple viewpoints naturally captured within a single life to learn a visual encoder in a self-supervised manner. Our experiments demonstrate three key findings. First, models trained independently on different lives develop a highly aligned geometric understanding. We demonstrate this by training visual encoders on distinct datasets each capturing a different life, both indoors and outdoors, as well as introducing a novel cross-attention-based metric to quantify the functional alignment of the internal representations developed by different models. Second, we show that single-life models learn generalizable geometric representations that effectively transfer to downstream tasks, such as depth estimation, in unseen environments. Third, we demonstrate that training on up to 30 hours from one week of the same person's life leads to comparable performance to training on 30 hours of diverse web data, highlighting the strength of single-life representation learning. Overall, our results establish that the shared structure of the world, both leads to consistency in models trained on individual lives, and provides a powerful signal for visual representation learning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LATTICE: Democratize High-Fidelity 3D Generation at Scale</title>
<link>https://arxiv.org/abs/2512.03052</link>
<guid>https://arxiv.org/abs/2512.03052</guid>
<content:encoded><![CDATA[
arXiv:2512.03052v1 Announce Type: cross 
Abstract: We present LATTICE, a new framework for high-fidelity 3D asset generation that bridges the quality and scalability gap between 3D and 2D generative models. While 2D image synthesis benefits from fixed spatial grids and well-established transformer architectures, 3D generation remains fundamentally more challenging due to the need to predict both spatial structure and detailed geometric surfaces from scratch. These challenges are exacerbated by the computational complexity of existing 3D representations and the lack of structured and scalable 3D asset encoding schemes. To address this, we propose VoxSet, a semi-structured representation that compresses 3D assets into a compact set of latent vectors anchored to a coarse voxel grid, enabling efficient and position-aware generation. VoxSet retains the simplicity and compression advantages of prior VecSet methods while introducing explicit structure into the latent space, allowing positional embeddings to guide generation and enabling strong token-level test-time scaling. Built upon this representation, LATTICE adopts a two-stage pipeline: first generating a sparse voxelized geometry anchor, then producing detailed geometry using a rectified flow transformer. Our method is simple at its core, but supports arbitrary resolution decoding, low-cost training, and flexible inference schemes, achieving state-of-the-art performance on various aspects, and offering a significant step toward scalable, high-quality 3D asset creation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research</title>
<link>https://arxiv.org/abs/2512.03054</link>
<guid>https://arxiv.org/abs/2512.03054</guid>
<content:encoded><![CDATA[
arXiv:2512.03054v1 Announce Type: cross 
Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PanFoMa: A Lightweight Foundation Model and Benchmark for Pan-Cancer</title>
<link>https://arxiv.org/abs/2512.03111</link>
<guid>https://arxiv.org/abs/2512.03111</guid>
<content:encoded><![CDATA[
arXiv:2512.03111v1 Announce Type: cross 
Abstract: Single-cell RNA sequencing (scRNA-seq) is essential for decoding tumor heterogeneity. However, pan-cancer research still faces two key challenges: learning discriminative and efficient single-cell representations, and establishing a comprehensive evaluation benchmark. In this paper, we introduce PanFoMa, a lightweight hybrid neural network that combines the strengths of Transformers and state-space models to achieve a balance between performance and efficiency. PanFoMa consists of a front-end local-context encoder with shared self-attention layers to capture complex, order-independent gene interactions; and a back-end global sequential feature decoder that efficiently integrates global context using a linear-time state-space model. This modular design preserves the expressive power of Transformers while leveraging the scalability of Mamba to enable transcriptome modeling, effectively capturing both local and global regulatory signals. To enable robust evaluation, we also construct a large-scale pan-cancer single-cell benchmark, PanFoMaBench, containing over 3.5 million high-quality cells across 33 cancer subtypes, curated through a rigorous preprocessing pipeline. Experimental results show that PanFoMa outperforms state-of-the-art models on our pan-cancer benchmark (+4.0\%) and across multiple public tasks, including cell type annotation (+7.4\%), batch integration (+4.0\%) and multi-omics integration (+3.1\%). The code is available at https://github.com/Xiaoshui-Huang/PanFoMa.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments</title>
<link>https://arxiv.org/abs/2512.03166</link>
<guid>https://arxiv.org/abs/2512.03166</guid>
<content:encoded><![CDATA[
arXiv:2512.03166v1 Announce Type: cross 
Abstract: The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Culture Affordance Atlas: Reconciling Object Diversity Through Functional Mapping</title>
<link>https://arxiv.org/abs/2512.03173</link>
<guid>https://arxiv.org/abs/2512.03173</guid>
<content:encoded><![CDATA[
arXiv:2512.03173v1 Announce Type: cross 
Abstract: Culture shapes the objects people use and for what purposes, yet mainstream Vision-Language (VL) datasets frequently exhibit cultural biases, disproportionately favoring higher-income, Western contexts. This imbalance reduces model generalizability and perpetuates performance disparities, especially impacting lower-income and non-Western communities. To address these disparities, we propose a novel function-centric framework that categorizes objects by the functions they fulfill, across diverse cultural and economic contexts. We implement this framework by creating the Culture Affordance Atlas, a re-annotated and culturally grounded restructuring of the Dollar Street dataset spanning 46 functions and 288 objects publicly available at https://lit.eecs.umich.edu/CultureAffordance-Atlas/index.html. Through extensive empirical analyses using the CLIP model, we demonstrate that function-centric labels substantially reduce socioeconomic performance gaps between high- and low-income groups by a median of 6 pp (statistically significant), improving model effectiveness for lower-income contexts. Furthermore, our analyses reveals numerous culturally essential objects that are frequently overlooked in prominent VL datasets. Our contributions offer a scalable pathway toward building inclusive VL datasets and equitable AI systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kaleidoscopic Scintillation Event Imaging</title>
<link>https://arxiv.org/abs/2512.03216</link>
<guid>https://arxiv.org/abs/2512.03216</guid>
<content:encoded><![CDATA[
arXiv:2512.03216v1 Announce Type: cross 
Abstract: Scintillators are transparent materials that interact with high-energy particles and emit visible light as a result. They are used in state of the art methods of measuring high-energy particles and radiation sources. Most existing methods use fast single-pixel detectors to detect and time scintillation events. Cameras provide spatial resolution but can only capture an average over many events, making it difficult to image the events associated with an individual particle. Emerging single-photon avalanche diode cameras combine speed and spatial resolution to enable capturing images of individual events. This allows us to use machine vision techniques to analyze events, enabling new types of detectors. The main challenge is the very low brightness of the events. Techniques have to work with a very limited number of photons.
  We propose a kaleidoscopic scintillator to increase light collection in a single-photon camera while preserving the event's spatial information. The kaleidoscopic geometry creates mirror reflections of the event in known locations for a given event location that are captured by the camera. We introduce theory for imaging an event in a kaleidoscopic scintillator and an algorithm to estimate the event's 3D position. We find that the kaleidoscopic scintillator design provides sufficient light collection to perform high-resolution event measurements for advanced radiation imaging techniques using a commercial CMOS single-photon camera. Code and data are available at https://github.com/bocchs/kaleidoscopic_scintillator.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models</title>
<link>https://arxiv.org/abs/2512.03422</link>
<guid>https://arxiv.org/abs/2512.03422</guid>
<content:encoded><![CDATA[
arXiv:2512.03422v1 Announce Type: cross 
Abstract: In this paper, we provide a comprehensive overview of existing scene representation methods for robotics, covering traditional representations such as point clouds, voxels, signed distance functions (SDF), and scene graphs, as well as more recent neural representations like Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and the emerging Foundation Models. While current SLAM and localization systems predominantly rely on sparse representations like point clouds and voxels, dense scene representations are expected to play a critical role in downstream tasks such as navigation and obstacle avoidance. Moreover, neural representations such as NeRF, 3DGS, and foundation models are well-suited for integrating high-level semantic features and language-based priors, enabling more comprehensive 3D scene understanding and embodied intelligence. In this paper, we categorized the core modules of robotics into five parts (Perception, Mapping, Localization, Navigation, Manipulation). We start by presenting the standard formulation of different scene representation methods and comparing the advantages and disadvantages of scene representation across different modules. This survey is centered around the question: What is the best 3D scene representation for robotics? We then discuss the future development trends of 3D scene representations, with a particular focus on how the 3D Foundation Model could replace current methods as the unified solution for future robotic applications. The remaining challenges in fully realizing this model are also explored. We aim to offer a valuable resource for both newcomers and experienced researchers to explore the future of 3D scene representations and their application in robotics. We have published an open-source project on GitHub and will continue to add new works and technologies to this project.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3DR: Towards Universal Multilingual Multimodal Document Retrieval</title>
<link>https://arxiv.org/abs/2512.03514</link>
<guid>https://arxiv.org/abs/2512.03514</guid>
<content:encoded><![CDATA[
arXiv:2512.03514v1 Announce Type: cross 
Abstract: Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization</title>
<link>https://arxiv.org/abs/2512.03522</link>
<guid>https://arxiv.org/abs/2512.03522</guid>
<content:encoded><![CDATA[
arXiv:2512.03522v1 Announce Type: cross 
Abstract: Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL</title>
<link>https://arxiv.org/abs/2512.03556</link>
<guid>https://arxiv.org/abs/2512.03556</guid>
<content:encoded><![CDATA[
arXiv:2512.03556v1 Announce Type: cross 
Abstract: Achieving generalizable embodied policies remains a key challenge. Traditional policy learning paradigms, including both Imitation Learning (IL) and Reinforcement Learning (RL), struggle to cultivate generalizability across diverse scenarios. While IL policies often overfit to specific expert trajectories, RL suffers from the inherent lack of a unified and general reward signal necessary for effective multi-scene generalization. We posit that the world model is uniquely capable of serving as a universal environment proxy to address this limitation. However, current world models primarily focus on their ability to predict observations and still rely on task-specific, handcrafted reward functions, thereby failing to provide a truly general training environment. Toward this problem, we propose RoboScape-R, a framework leveraging the world model to serve as a versatile, general-purpose proxy for the embodied environment within the RL paradigm. We introduce a novel world model-based general reward mechanism that generates ''endogenous'' rewards derived from the model's intrinsic understanding of real-world state transition dynamics. Extensive experiments demonstrate that RoboScape-R effectively addresses the limitations of traditional RL methods by providing an efficient and general training environment that substantially enhances the generalization capability of embodied policies. Our approach offers critical insights into utilizing the world model as an online training strategy and achieves an average 37.5% performance improvement over baselines under out-of-domain scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting</title>
<link>https://arxiv.org/abs/2512.03656</link>
<guid>https://arxiv.org/abs/2512.03656</guid>
<content:encoded><![CDATA[
arXiv:2512.03656v1 Announce Type: cross 
Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction</title>
<link>https://arxiv.org/abs/2512.03962</link>
<guid>https://arxiv.org/abs/2512.03962</guid>
<content:encoded><![CDATA[
arXiv:2512.03962v1 Announce Type: cross 
Abstract: Deep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Microsaccade Compensation: Stable Vision for an Ornithopter</title>
<link>https://arxiv.org/abs/2512.03995</link>
<guid>https://arxiv.org/abs/2512.03995</guid>
<content:encoded><![CDATA[
arXiv:2512.03995v1 Announce Type: cross 
Abstract: Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for "Artificial Microsaccade Compensation". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jina-VLM: Small Multilingual Vision Language Model</title>
<link>https://arxiv.org/abs/2512.04032</link>
<guid>https://arxiv.org/abs/2512.04032</guid>
<content:encoded><![CDATA[
arXiv:2512.04032v1 Announce Type: cross 
Abstract: We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radiance Meshes for Volumetric Reconstruction</title>
<link>https://arxiv.org/abs/2512.04076</link>
<guid>https://arxiv.org/abs/2512.04076</guid>
<content:encoded><![CDATA[
arXiv:2512.04076v1 Announce Type: cross 
Abstract: We introduce radiance meshes, a technique for representing radiance fields with constant density tetrahedral cells produced with a Delaunay tetrahedralization. Unlike a Voronoi diagram, a Delaunay tetrahedralization yields simple triangles that are natively supported by existing hardware. As such, our model is able to perform exact and fast volume rendering using both rasterization and ray-tracing. We introduce a new rasterization method that achieves faster rendering speeds than all prior radiance field representations (assuming an equivalent number of primitives and resolution) across a variety of platforms. Optimizing the positions of Delaunay vertices introduces topological discontinuities (edge flips). To solve this, we use a Zip-NeRF-style backbone which allows us to express a smoothly varying field even when the topology changes. Our rendering method exactly evaluates the volume rendering equation and enables high quality, real-time view synthesis on standard consumer hardware. Our tetrahedral meshes also lend themselves to a variety of exciting applications including fisheye lens distortion, physics-based simulation, editing, and mesh extraction.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Learning Paradigm for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2209.15402</link>
<guid>https://arxiv.org/abs/2209.15402</guid>
<content:encoded><![CDATA[
arXiv:2209.15402v4 Announce Type: replace 
Abstract: Due to the subjective crowdsourcing annotations and the inherent inter-class similarity of facial expressions, the real-world Facial Expression Recognition (FER) datasets usually exhibit ambiguous annotation. To simplify the learning paradigm, most previous methods convert ambiguous annotation results into precise one-hot annotations and train FER models in an end-to-end supervised manner. In this paper, we rethink the existing training paradigm and propose that it is better to use weakly supervised strategies to train FER models with original ambiguous annotation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference</title>
<link>https://arxiv.org/abs/2405.14430</link>
<guid>https://arxiv.org/abs/2405.14430</guid>
<content:encoded><![CDATA[
arXiv:2405.14430v4 Announce Type: replace 
Abstract: This paper presents PipeFusion, an innovative parallel methodology to tackle the high latency issues associated with generating high-resolution images using diffusion transformers (DiTs) models. PipeFusion partitions images into patches and the model layers across multiple GPUs. It employs a patch-level pipeline parallel strategy to orchestrate communication and computation efficiently. By capitalizing on the high similarity between inputs from successive diffusion steps, PipeFusion reuses one-step stale feature maps to provide context for the current pipeline step. This approach notably reduces communication costs compared to existing DiTs inference parallelism, including tensor parallel, sequence parallel and DistriFusion. PipeFusion enhances memory efficiency through parameter distribution across devices, ideal for large DiTs like Flux.1. Experimental results demonstrate that PipeFusion achieves state-of-the-art performance on 8$\times$L40 PCIe GPUs for Pixart, Stable-Diffusion 3, and Flux.1 models. Our source code is available at https://github.com/xdit-project/xDiT.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Margin-aware Preference Optimization for Aligning Diffusion Models without Reference</title>
<link>https://arxiv.org/abs/2406.06424</link>
<guid>https://arxiv.org/abs/2406.06424</guid>
<content:encoded><![CDATA[
arXiv:2406.06424v2 Announce Type: replace 
Abstract: Modern preference alignment methods, such as DPO, rely on divergence regularization to a reference model for training stability-but this creates a fundamental problem we call "reference mismatch." In this paper, we investigate the negative impacts of reference mismatch in aligning text-to-image (T2I) diffusion models, showing that larger reference mismatch hinders effective adaptation given the same amount of data, e.g., as when learning new artistic styles, or personalizing to specific objects. We demonstrate this phenomenon across text-to-image (T2I) diffusion models and introduce margin-aware preference optimization (MaPO), a reference-agnostic approach that breaks free from this constraint. By directly optimizing the likelihood margin between preferred and dispreferred outputs under the Bradley-Terry model without anchoring to a reference, MaPO transforms diverse T2I tasks into unified pairwise preference optimization. We validate MaPO's versatility across five challenging domains: (1) safe generation, (2) style adaptation, (3) cultural representation, (4) personalization, and (5) general preference alignment. Our results reveal that MaPO's advantage grows dramatically with reference mismatch severity, outperforming both DPO and specialized methods like DreamBooth while reducing training time by 15%. MaPO thus emerges as a versatile and memory-efficient method for generic T2I adaptation tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NVRC: Neural Video Representation Compression</title>
<link>https://arxiv.org/abs/2409.07414</link>
<guid>https://arxiv.org/abs/2409.07414</guid>
<content:encoded><![CDATA[
arXiv:2409.07414v2 Announce Type: replace 
Abstract: Recent advances in implicit neural representation (INR)-based video coding have demonstrated its potential to compete with both conventional and other learning-based approaches. With INR methods, a neural network is trained to overfit a video sequence, with its parameters compressed to obtain a compact representation of the video content. However, although promising results have been achieved, the best INR-based methods are still out-performed by the latest standard codecs, such as VVC VTM, partially due to the simple model compression techniques employed. In this paper, rather than focusing on representation architectures as in many existing works, we propose a novel INR-based video compression framework, Neural Video Representation Compression (NVRC), targeting compression of the representation. Based on the novel entropy coding and quantization models proposed, NVRC, for the first time, is able to optimize an INR-based video codec in a fully end-to-end manner. To further minimize the additional bitrate overhead introduced by the entropy models, we have also proposed a new model compression framework for coding all the network, quantization and entropy model parameters hierarchically. Our experiments show that NVRC outperforms many conventional and learning-based benchmark codecs, with a 24% average coding gain over VVC VTM (Random Access) on the UVG dataset, measured in PSNR. As far as we are aware, this is the first time an INR-based video codec achieving such performance. The implementation of NVRC will be released.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Efficient Variants of Segment Anything Model: A Survey</title>
<link>https://arxiv.org/abs/2410.04960</link>
<guid>https://arxiv.org/abs/2410.04960</guid>
<content:encoded><![CDATA[
arXiv:2410.04960v4 Announce Type: replace 
Abstract: The Segment Anything Model (SAM) is a foundational model for image segmentation tasks, known for its strong generalization across diverse applications. However, its impressive performance comes with significant computational and resource demands, making it challenging to deploy in resource-limited environments such as edge devices. To address this, a variety of SAM variants have been proposed to enhance efficiency while keeping accuracy. This survey provides the first comprehensive review of these efficient SAM variants. We begin by exploring the motivations driving this research. We then present core techniques used in SAM and model acceleration. This is followed by a detailed exploration of SAM acceleration strategies, categorized by approach, and a discussion of several future research directions. Finally, we offer a unified and extensive evaluation of these methods across various hardware, assessing their efficiency and accuracy on representative benchmarks, and providing a clear comparison of their overall performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes</title>
<link>https://arxiv.org/abs/2410.18084</link>
<guid>https://arxiv.org/abs/2410.18084</guid>
<content:encoded><![CDATA[
arXiv:2410.18084v3 Announce Type: replace 
Abstract: Urban scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D occupancy generation framework capable of generating large-scale, high-quality dynamic 4D scenes with semantics. DynamicCity mainly consists of two key models. 1) A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novel Projection Module to effectively compress 4D features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supporting versatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D occupancy generation methods across multiple metrics. The code and models have been released to facilitate future research.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Prose: Advancing Multi-Modal Language Models for Remote Sensing</title>
<link>https://arxiv.org/abs/2411.05826</link>
<guid>https://arxiv.org/abs/2411.05826</guid>
<content:encoded><![CDATA[
arXiv:2411.05826v2 Announce Type: replace 
Abstract: Remote sensing has evolved from simple image acquisition to complex systems capable of integrating and processing visual and textual data. This review examines the development and application of multi-modal language models (MLLMs) in remote sensing, focusing on their ability to interpret and describe satellite imagery using natural language. We cover the technical underpinnings of MLLMs, including dual-encoder architectures, Transformer models, self-supervised and contrastive learning, and cross-modal integration. The unique challenges of remote sensing data--varying spatial resolutions, spectral richness, and temporal changes--are analyzed for their impact on MLLM performance. Key applications such as scene description, object detection, change detection, text-to-image retrieval, image-to-text generation, and visual question answering are discussed to demonstrate their relevance in environmental monitoring, urban planning, and disaster response. We review significant datasets and resources supporting the training and evaluation of these models. Challenges related to computational demands, scalability, data quality, and domain adaptation are highlighted. We conclude by proposing future research directions and technological advancements to further enhance MLLM utility in remote sensing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-time Correction: An Online 3D Detection System via Visual Prompting</title>
<link>https://arxiv.org/abs/2412.07768</link>
<guid>https://arxiv.org/abs/2412.07768</guid>
<content:encoded><![CDATA[
arXiv:2412.07768v3 Announce Type: replace 
Abstract: This paper introduces Test-time Correction (TTC), an online 3D detection system designed to rectify test-time errors using various auxiliary feedback, aiming to enhance the safety of deployed autonomous driving systems. Unlike conventional offline 3D detectors that remain fixed during inference, TTC enables immediate online error correction without retraining, allowing autonomous vehicles to adapt to new scenarios and reduce deployment risks. To achieve this, we equip existing 3D detectors with an Online Adapter (OA) module -- a prompt-driven query generator for real-time correction. At the core of OA module are visual prompts: image-based descriptions of objects of interest derived from auxiliary feedback such as mismatches with 2D detections, road descriptions, or user clicks. These visual prompts, collected from risky objects during inference, are maintained in a visual prompt buffer to enable continuous correction in future frames. By leveraging this mechanism, TTC consistently detects risky objects, achieving reliable, adaptive, and versatile driving autonomy. Extensive experiments show that TTC significantly improves instant error rectification over frozen 3D detectors, even under limited labels, zero-shot settings, and adverse conditions. We hope this work inspires future research on post-deployment online rectification systems for autonomous driving.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GT23D-Bench: A Comprehensive General Text-to-3D Generation Benchmark</title>
<link>https://arxiv.org/abs/2412.09997</link>
<guid>https://arxiv.org/abs/2412.09997</guid>
<content:encoded><![CDATA[
arXiv:2412.09997v2 Announce Type: replace 
Abstract: Text-to-3D (T23D) generation has emerged as a crucial visual generation task, aiming at synthesizing 3D content from textual descriptions. Studies of this task are currently shifting from per-scene T23D, which requires optimization of the model for every content generated, to General T23D (GT23D), which requires only one pre-trained model to generate different content without re-optimization, for more generalized and efficient 3D generation. Despite notable advancements, GT23D is severely bottlenecked by two interconnected challenges: the lack of high-quality, large-scale training data and the prevalence of evaluation metrics that overlook intrinsic 3D properties. Existing datasets often suffer from incomplete annotations, noisy organization, and inconsistent quality, while current evaluations rely heavily on 2D image-text similarity or scoring, failing to thoroughly assess 3D geometric integrity and semantic relevance. To address these fundamental gaps, we introduce GT23D-Bench, the first comprehensive benchmark specifically designed for GT23D training and evaluation. We first construct a high-quality dataset of 400K 3D assets, featuring diverse visual annotations (70M+ visual samples) and multi-granularity hierarchical captions (1M+ descriptions) to foster robust semantic learning. Second, we propose a comprehensive evaluation suite with 10 metrics assessing both text-3D alignment and 3D visual quality at multiple levels. Crucially, we demonstrate through rigorous experiments that our proposed metrics exhibit significantly higher correlation with human judgment compared to existing methods. Our in-depth analysis of eight leading GT23D models using this benchmark provides the community with critical insights into current model capabilities and their shared failure modes. GT23D-Bench will be publicly available to facilitate rigorous and reproducible research.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.04005</link>
<guid>https://arxiv.org/abs/2501.04005</guid>
<content:encoded><![CDATA[
arXiv:2501.04005v3 Announce Type: replace 
Abstract: Recent advancements in vision foundation models (VFMs) have revolutionized visual perception in 2D, yet their potential for 3D scene understanding, particularly in autonomous driving applications, remains underexplored. In this paper, we introduce LargeAD, a versatile and scalable framework designed for large-scale 3D pretraining across diverse real-world driving datasets. Our framework leverages VFMs to extract semantically rich superpixels from 2D images, which are aligned with LiDAR point clouds to generate high-quality contrastive samples. This alignment facilitates cross-modal representation learning, enhancing the semantic consistency between 2D and 3D data. We introduce several key innovations: (i) VFM-driven superpixel generation for detailed semantic representation, (ii) a VFM-assisted contrastive learning strategy to align multimodal features, (iii) superpoint temporal consistency to maintain stable representations across time, and (iv) multi-source data pretraining to generalize across various LiDAR configurations. Our approach achieves substantial gains over state-of-the-art methods in linear probing and fine-tuning for LiDAR-based segmentation and object detection. Extensive experiments on 11 large-scale multi-sensor datasets highlight our superior performance, demonstrating adaptability, efficiency, and robustness in real-world autonomous driving scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Point, I Learn: Online Adaptation of Interactive Segmentation Models for Handling Distribution Shifts in Medical Imaging</title>
<link>https://arxiv.org/abs/2503.06717</link>
<guid>https://arxiv.org/abs/2503.06717</guid>
<content:encoded><![CDATA[
arXiv:2503.06717v2 Announce Type: replace 
Abstract: Interactive segmentation uses real-time user inputs, such as mouse clicks, to iteratively refine model predictions. Although not originally designed to address distribution shifts, this paradigm naturally lends itself to such challenges. In medical imaging, where distribution shifts are common, interactive methods can use user inputs to guide models towards improved predictions. Moreover, once a model is deployed, user corrections can be used to adapt the network parameters to the new data distribution, mitigating distribution shift. Based on these insights, we aim to develop a practical, effective method for improving the adaptive capabilities of interactive segmentation models to new data distributions in medical imaging. Firstly, we found that strengthening the model's responsiveness to clicks is important for the initial training process. Moreover, we show that by treating the post-interaction user-refined model output as pseudo-ground-truth, we can design a lean, practical online adaptation method that enables a model to learn effectively across sequential test images. The framework includes two components: (i) a Post-Interaction adaptation process, updating the model after the user has completed interactive refinement of an image, and (ii) a Mid-Interaction adaptation process, updating incrementally after each click. Both processes include a Click-Centered Gaussian loss that strengthens the model's reaction to clicks and enhances focus on user-guided, clinically relevant regions. Experiments on 5 fundus and 4 brain-MRI databases show that our approach consistently outperforms existing methods under diverse distribution shifts, including unseen imaging modalities and pathologies. Code and pretrained models will be released upon publication.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ActiveInitSplat: How Active Image Selection Helps Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.06859</link>
<guid>https://arxiv.org/abs/2503.06859</guid>
<content:encoded><![CDATA[
arXiv:2503.06859v2 Announce Type: replace 
Abstract: Gaussian splatting (GS) along with its extensions and variants provides outstanding performance in real-time scene rendering while meeting reduced storage demands and computational efficiency. While the selection of 2D images capturing the scene of interest is crucial for the proper initialization and training of GS, hence markedly affecting the rendering performance, prior works rely on passively and typically densely selected 2D images. In contrast, this paper proposes `ActiveInitSplat', a novel framework for active selection of training images for proper initialization and training of GS. ActiveInitSplat relies on density and occupancy criteria of the resultant 3D scene representation from the selected 2D images, to ensure that the latter are captured from diverse viewpoints leading to better scene coverage and that the initialized Gaussian functions are well aligned with the actual 3D structure. Numerical tests on well-known simulated and real environments demonstrate the merits of ActiveInitSplat resulting in significant GS rendering performance improvement over passive GS baselines in both dense- and sparse-view settings, in the widely adopted LPIPS, SSIM, and PSNR metrics.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Radiance and Gaze Fields for Visual Attention Modeling in 3D Environments</title>
<link>https://arxiv.org/abs/2503.07828</link>
<guid>https://arxiv.org/abs/2503.07828</guid>
<content:encoded><![CDATA[
arXiv:2503.07828v2 Announce Type: replace 
Abstract: We introduce Neural Radiance and Gaze Fields (NeRGs), a novel approach for representing visual attention in complex environments. Much like how Neural Radiance Fields (NeRFs) perform novel view synthesis, NeRGs reconstruct gaze patterns from arbitrary viewpoints, implicitly mapping visual attention to 3D surfaces. We achieve this by augmenting a standard NeRF with an additional network that models local egocentric gaze probability density, conditioned on scene geometry and observer position. The output of a NeRG is a rendered view of the scene alongside a pixel-wise salience map representing the conditional probability that a given observer fixates on visible surfaces. Unlike prior methods, our system is lightweight and enables visualization of gaze fields at interactive framerates. Moreover, NeRGs allow the observer perspective to be decoupled from the rendering camera and correctly account for gaze occlusion due to intervening geometry. We demonstrate the effectiveness of NeRGs using head pose from skeleton tracking as a proxy for gaze, employing our proposed gaze probes to aggregate noisy rays into robust probability density targets for supervision.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow to the Mode: Mode-Seeking Diffusion Autoencoders for State-of-the-Art Image Tokenization</title>
<link>https://arxiv.org/abs/2503.11056</link>
<guid>https://arxiv.org/abs/2503.11056</guid>
<content:encoded><![CDATA[
arXiv:2503.11056v3 Announce Type: replace 
Abstract: Since the advent of popular visual generation frameworks like VQGAN and latent diffusion models, state-of-the-art image generation systems have generally been two-stage systems that first tokenize or compress visual data into a lower-dimensional latent space before learning a generative model. Tokenizer training typically follows a standard recipe in which images are compressed and reconstructed subject to a combination of MSE, perceptual, and adversarial losses. Diffusion autoencoders have been proposed in prior work as a way to learn end-to-end perceptually-oriented image compression, but have not yet shown state-of-the-art performance on the competitive task of ImageNet-1K reconstruction. We propose FlowMo, a transformer-based diffusion autoencoder that achieves a new state-of-the-art for image tokenization at multiple compression rates without using convolutions, adversarial losses, spatially-aligned two-dimensional latent codes, or distilling from other tokenizers. Our key insight is that FlowMo training should be broken into a mode-matching pre-training stage and a mode-seeking post-training stage. In addition, we conduct extensive analyses and explore the training of generative models atop the FlowMo tokenizer. Our code and models will be available at http://kylesargent.github.io/flowmo .
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction</title>
<link>https://arxiv.org/abs/2503.13430</link>
<guid>https://arxiv.org/abs/2503.13430</guid>
<content:encoded><![CDATA[
arXiv:2503.13430v2 Announce Type: replace 
Abstract: Autonomous driving requires understanding infrastructure elements, such as lanes and crosswalks. To navigate safely, this understanding must be derived from sensor data in real-time and needs to be represented in vectorized form. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set of camera images from multiple views into one joint latent BEV grid. Traditionally, from this latent space, an intermediate raster map is predicted, providing dense spatial supervision but requiring post-processing into the desired vectorized form. More recent models directly derive infrastructure elements as polylines using vectorized map decoders, providing instance-level information. Our approach, Augmentation Map Network (AugMapNet), proposes latent BEV feature grid augmentation, a novel technique that significantly enhances the latent BEV representation. AugMapNet combines vector decoding and dense spatial supervision more effectively than existing architectures while remaining easy to integrate compared to other hybrid approaches. It additionally benefits from extra processing on its latent BEV features. Experiments on nuScenes and Argoverse2 datasets demonstrate significant improvements on vectorized map prediction of up to 13.3% over the StreamMapNet baseline on 60 m range and greater improvements on larger ranges. We confirm transferability by applying our method to another baseline, SQD-MapNet, and find similar improvements. A detailed analysis of the latent BEV grid confirms a more structured latent space of AugMapNet and shows the value of our novel concept beyond pure performance improvement. The code can be found at https://github.com/tmonnin/augmapnet
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables</title>
<link>https://arxiv.org/abs/2503.23793</link>
<guid>https://arxiv.org/abs/2503.23793</guid>
<content:encoded><![CDATA[
arXiv:2503.23793v2 Announce Type: replace 
Abstract: Recently, deep learning-based pan-sharpening algorithms have achieved notable advancements over traditional methods. However, deep learning-based methods incur substantial computational overhead during inference, especially with large images. This excessive computational demand limits the applicability of these methods in real-world scenarios, particularly in the absence of dedicated computing devices such as GPUs and TPUs. To address these challenges, we propose Pan-LUT, a novel learnable look-up table (LUT) framework for pan-sharpening that strikes a balance between performance and computational efficiency for large remote sensing images. Our method makes it possible to process 15K*15K remote sensing images on a 24GB GPU. To finely control the spectral transformation, we devise the PAN-guided look-up table (PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained spatial details, we introduce the spatial details look-up table (SDLUT). Furthermore, to adaptively aggregate channel information for generating high-resolution multispectral images, we design an adaptive output look-up table (AOLUT). Our model contains fewer than 700K parameters and processes a 9K*9K image in under 1 ms using one RTX 2080 Ti GPU, demonstrating significantly faster performance compared to other methods. Experiments reveal that Pan-LUT efficiently processes large remote sensing images in a lightweight manner, bridging the gap to real-world applications. Furthermore, our model surpasses SOTA methods in full-resolution scenes under real-world conditions, highlighting its effectiveness and efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video</title>
<link>https://arxiv.org/abs/2504.15122</link>
<guid>https://arxiv.org/abs/2504.15122</guid>
<content:encoded><![CDATA[
arXiv:2504.15122v4 Announce Type: replace 
Abstract: We present MoBGS, a novel motion deblurring 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method using a proposed Blur-adaptive Neural Ordinary Differential Equation (ODE) solver for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both a global camera and local object motions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent methods, achieving state-of-the-art performance for dynamic NVS under motion blur.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can VLMs Detect and Localize Fine-Grained AI-Edited Images?</title>
<link>https://arxiv.org/abs/2505.15644</link>
<guid>https://arxiv.org/abs/2505.15644</guid>
<content:encoded><![CDATA[
arXiv:2505.15644v2 Announce Type: replace 
Abstract: Fine-grained detection and localization of localized image edits is crucial for assessing content authenticity, especially as modern diffusion models and image editors can produce highly realistic manipulations. However, this problem faces three key challenges: (1) most AIGC detectors produce only a global real-or-fake label without indicating where edits occur; (2) traditional computer vision methods for edit localization typically rely on costly pixel-level annotations; and (3) there is no large-scale, modern benchmark specifically targeting edited-image detection. To address these gaps, we develop an automated data-generation pipeline and construct FragFake, a large-scale benchmark of AI-edited images spanning multiple source datasets, diverse editing models, and several common edit types. Building on FragFake, we are the first to systematically study vision language models (VLMs) for edited-image classification and edited-region localization. Our experiments show that pretrained VLMs, including GPT4o, perform poorly on this task, whereas fine-tuned models such as Qwen2.5-VL achieve high accuracy and substantially higher object precision across all settings. We further explore GRPO-based RLVR training, which yields modest metric gains while improving the interpretability of model outputs. Ablation and transfer analyses reveal how data balancing, training size, LoRA rank, and training domain affect performance, and highlight both the potential and the limitations of cross-editor and cross-dataset generalization. We anticipate that this work will establish a solid foundation to facilitate and inspire subsequent research endeavors in the domain of multimodal content authenticity.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATORI-R1: Incentivizing Multimodal Reasoning through Explicit Visual Anchoring</title>
<link>https://arxiv.org/abs/2505.19094</link>
<guid>https://arxiv.org/abs/2505.19094</guid>
<content:encoded><![CDATA[
arXiv:2505.19094v2 Announce Type: replace 
Abstract: DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI ($\textbf{S}patially$ $\textbf{A}nchored$ $\textbf{T}ask$ $\textbf{O}ptimization$ with $\textbf{R}e\textbf{I}nforcement$ Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to $15.7\%$ improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guard Me If You Know Me: Protecting Specific Face-Identity from Deepfakes</title>
<link>https://arxiv.org/abs/2505.19582</link>
<guid>https://arxiv.org/abs/2505.19582</guid>
<content:encoded><![CDATA[
arXiv:2505.19582v2 Announce Type: replace 
Abstract: Securing personal identity against deepfake attacks is increasingly critical in the digital age, especially for celebrities and political figures whose faces are easily accessible and frequently targeted. Most existing deepfake detection methods focus on general-purpose scenarios and often ignore the valuable prior knowledge of known facial identities, e.g., "VIP individuals" whose authentic facial data are already available. In this paper, we propose \textbf{VIPGuard}, a unified multimodal framework designed to capture fine-grained and comprehensive facial representations of a given identity, compare them against potentially fake or similar-looking faces, and reason over these comparisons to make accurate and explainable predictions. Specifically, our framework consists of three main stages. First, fine-tune a multimodal large language model (MLLM) to learn detailed and structural facial attributes. Second, we perform identity-level discriminative learning to enable the model to distinguish subtle differences between highly similar faces, including real and fake variations. Finally, we introduce user-specific customization, where we model the unique characteristics of the target face identity and perform semantic reasoning via MLLM to enable personalized and explainable deepfake detection. Our framework shows clear advantages over previous detection works, where traditional detectors mainly rely on low-level visual cues and provide no human-understandable explanations, while other MLLM-based models often lack a detailed understanding of specific face identities. To facilitate the evaluation of our method, we built a comprehensive identity-aware benchmark called \textbf{VIPBench} for personalized deepfake detection, involving the latest 7 face-swapping and 7 entire face synthesis techniques for generation. The code is available at https://github.com/KQL11/VIPGuard .
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query</title>
<link>https://arxiv.org/abs/2506.03144</link>
<guid>https://arxiv.org/abs/2506.03144</guid>
<content:encoded><![CDATA[
arXiv:2506.03144v3 Announce Type: replace 
Abstract: Semantic retrieval is crucial for modern applications yet remains underexplored in current research. Existing datasets are limited to single languages, single images, or singular retrieval conditions, often failing to fully exploit the expressive capacity of visual information as evidenced by maintained performance when images are replaced with captions. However, practical retrieval scenarios frequently involve interleaved multi-condition queries with multiple images. Hence, this paper introduces MERIT, the first multilingual dataset for interleaved multi-condition semantic retrieval, comprising 320,000 queries with 135,000 products in 5 languages, covering 7 distinct product categories. Extensive experiments on MERIT identify existing models's limitation: focusing solely on global semantic information while neglecting specific conditional elements in queries. Consequently, we propose Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by integrating embedding reconstruction to preserve fine-grained conditional elements and contrastive learning to extract comprehensive global semantics. Experiments demonstrate that Coral achieves a 45.9% performance improvement over conventional approaches on MERIT, with strong generalization capabilities validated across 8 established retrieval benchmarks. Collectively, our contributions - a novel dataset, identification of critical limitations in existing approaches, and an innovative fine-tuning framework - establish a foundation for future research in interleaved multi-condition semantic retrieval.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GS4: Generalizable Sparse Splatting Semantic SLAM</title>
<link>https://arxiv.org/abs/2506.06517</link>
<guid>https://arxiv.org/abs/2506.06517</guid>
<content:encoded><![CDATA[
arXiv:2506.06517v3 Announce Type: replace 
Abstract: Traditional SLAM algorithms excel at camera tracking, but typically produce incomplete and low-resolution maps that are not tightly integrated with semantics prediction. Recent work integrates Gaussian Splatting (GS) into SLAM to enable dense, photorealistic 3D mapping, yet existing GS-based SLAM methods require per-scene optimization that is slow and consumes an excessive number of Gaussians. We present GS4, the first generalizable GS-based semantic SLAM system. Compared with prior approaches, GS4 runs 10x faster, uses 10x fewer Gaussians, and achieves state-of-the-art performance across color, depth, semantic mapping and camera tracking. From an RGB-D video stream, GS4 incrementally builds and updates a set of 3D Gaussians using a feed-forward network. First, the Gaussian Prediction Model estimates a sparse set of Gaussian parameters from input frame, which integrates both color and semantic prediction with the same backbone. Then, the Gaussian Refinement Network merges new Gaussians with the existing set while avoiding redundancy. Finally, when significant pose changes are detected, we perform only 1-5 iterations of joint Gaussian-pose optimization to correct drift, remove floaters, and further improve tracking accuracy. Experiments on the real-world ScanNet and ScanNet++ benchmarks demonstrate state-of-the-art semantic SLAM performance, with strong generalization capability shown through zero-shot transfer to the NYUv2 and TUM RGB-D datasets.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Top Activations: Efficient and Reliable Crowdsourced Evaluation of Automated Interpretability</title>
<link>https://arxiv.org/abs/2506.07985</link>
<guid>https://arxiv.org/abs/2506.07985</guid>
<content:encoded><![CDATA[
arXiv:2506.07985v2 Announce Type: replace 
Abstract: Interpreting individual neurons or directions in activation space is an important topic in mechanistic interpretability. Numerous automated interpretability methods have been proposed to generate such explanations, but it remains unclear how reliable these explanations are, and which methods produce the most accurate descriptions. While crowd-sourced evaluations are commonly used, existing pipelines are noisy, costly, and typically assess only the highest-activating inputs, leading to unreliable results. In this paper, we introduce two techniques to enable cost-effective and accurate crowdsourced evaluation of automated interpretability methods beyond top activating inputs. First, we propose Model-Guided Importance Sampling (MG-IS) to select the most informative inputs to show human raters. In our experiments, we show this reduces the number of inputs needed to reach the same evaluation accuracy by ~13x. Second, we address label noise in crowd-sourced ratings through Bayesian Rating Aggregation (BRAgg), which allows us to reduce the number of ratings per input required to overcome noise by ~3x. Together, these techniques reduce the evaluation cost by ~40x, making large-scale evaluation feasible. Finally, we use our methods to conduct a large scale crowd-sourced study comparing recent automated interpretability methods for vision networks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting</title>
<link>https://arxiv.org/abs/2506.08710</link>
<guid>https://arxiv.org/abs/2506.08710</guid>
<content:encoded><![CDATA[
arXiv:2506.08710v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient encoding of scene geometry, appearance, and semantics. Moreover, grounding language in 3D scenes has proven to be an effective strategy for 3D scene understanding. Current Language Gaussian Splatting line of work fall into three main groups: (i) per-scene optimization-based, (ii) per-scene optimization-free, and (iii) generalizable approach. However, most of them are evaluated only on rendered 2D views of a handful of scenes and viewpoints close to the training views, limiting ability and insight into holistic 3D understanding. To address this gap, we propose the first large-scale benchmark that systematically assesses these three groups of methods directly in 3D space, evaluating on 1060 scenes across three indoor datasets and one outdoor dataset. Benchmark results demonstrate a clear advantage of the generalizable paradigm, particularly in relaxing the scene-specific limitation, enabling fast feed-forward inference on novel scenes, and achieving superior segmentation performance. We further introduce GaussianWorld-49K a carefully curated 3DGS dataset comprising around 49K diverse indoor and outdoor scenes obtained from multiple sources, with which we demonstrate the generalizable approach could harness strong data priors. Our codes, benchmark, and datasets are public to accelerate research in generalizable 3DGS scene understanding.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GNSS-Inertial State Initialization Using Inter-Epoch Baseline Residuals</title>
<link>https://arxiv.org/abs/2506.11534</link>
<guid>https://arxiv.org/abs/2506.11534</guid>
<content:encoded><![CDATA[
arXiv:2506.11534v2 Announce Type: replace 
Abstract: Initializing the state of a sensorized platform can be challenging, as a limited set of measurements often provide low-informative constraints that are in addition highly non-linear. This may lead to poor initial estimates that may converge to local minima during subsequent non-linear optimization. We propose an adaptive GNSS-inertial initialization strategy that delays the incorporation of global GNSS constraints until they become sufficiently informative. In the initial stage, our method leverages inter-epoch baseline vector residuals between consecutive GNSS fixes to mitigate inertial drift. To determine when to activate global constraints, we introduce a general criterion based on the evolution of the Hessian matrix's singular values, effectively quantifying system observability. Experiments on EuRoC, GVINS and MARS-LVIG datasets show that our approach consistently outperforms the naive strategy of fusing all measurements from the outset, yielding more accurate and robust initializations.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A$^2$LC: Active and Automated Label Correction for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2506.11599</link>
<guid>https://arxiv.org/abs/2506.11599</guid>
<content:encoded><![CDATA[
arXiv:2506.11599v2 Announce Type: replace 
Abstract: Active Label Correction (ALC) has emerged as a promising solution to the high cost and error-prone nature of manual pixel-wise annotation in semantic segmentation, by actively identifying and correcting mislabeled data. Although recent work has improved correction efficiency by generating pseudo-labels using foundation models, substantial inefficiencies still remain. In this paper, we introduce A$^2$LC, an Active and Automated Label Correction framework for semantic segmentation, where manual and automatic correction stages operate in a cascaded manner. Specifically, the automatic correction stage leverages human feedback to extend label corrections beyond the queried samples, thereby maximizing cost efficiency. In addition, we introduce an adaptively balanced acquisition function that emphasizes underrepresented tail classes, working in strong synergy with the automatic correction stage. Extensive experiments on Cityscapes and PASCAL VOC 2012 demonstrate that A$^2$LC significantly outperforms previous state-of-the-art methods. Notably, A$^2$LC exhibits high efficiency by outperforming previous methods with only 20% of their budget, and shows strong effectiveness by achieving a 27.23% performance gain under the same budget on Cityscapes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BitMark: Watermarking Bitwise Autoregressive Image Generative Models</title>
<link>https://arxiv.org/abs/2506.21209</link>
<guid>https://arxiv.org/abs/2506.21209</guid>
<content:encoded><![CDATA[
arXiv:2506.21209v2 Announce Type: replace 
Abstract: State-of-the-art text-to-image models generate photorealistic images at an unprecedented speed. This work focuses on models that operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework. Our method embeds a watermark directly at the bit level of the token stream during the image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs. The code is available at https://github.com/sprintml/BitMark.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Labelling for Low-Light Pedestrian Detection</title>
<link>https://arxiv.org/abs/2507.02513</link>
<guid>https://arxiv.org/abs/2507.02513</guid>
<content:encoded><![CDATA[
arXiv:2507.02513v2 Announce Type: replace 
Abstract: Pedestrian detection in RGB images is a key task in pedestrian safety, as the most common sensor in autonomous vehicles and advanced driver assistance systems is the RGB camera. A challenge in RGB pedestrian detection, that does not appear to have large public datasets, is low-light conditions. As a solution, in this research, we propose an automated infrared-RGB labeling pipeline. The proposed pipeline consists of 1) Infrared detection, where a fine-tuned model for infrared pedestrian detection is used 2) Label transfer process from the infrared detections to their RGB counterparts 3) Training object detection models using the generated labels for low-light RGB pedestrian detection. The research was performed using the KAIST dataset. For the evaluation, object detection models were trained on the generated autolabels and ground truth labels. When compared on a previously unseen image sequence, the results showed that the models trained on generated labels outperformed the ones trained on ground-truth labels in 6 out of 9 cases for the mAP@50 and mAP@50-95 metrics. The source code for this research is available at https://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying</title>
<link>https://arxiv.org/abs/2508.03142</link>
<guid>https://arxiv.org/abs/2508.03142</guid>
<content:encoded><![CDATA[
arXiv:2508.03142v2 Announce Type: replace 
Abstract: While Unified Vision-Language Models promise to synergistically combine the high-level semantic understanding of vision-language models with the generative fidelity of diffusion models, current editing methodologies remain fundamentally decoupled and open loop performing static, pre-defined transformations without dynamic feedback between semantic interpretation and visual generation. A central limitation stems from the representation gap: understanding typically leverages high-level, language aligned encoders, whereas generation relies on low level, pixel-space autoencoders, resulting in misaligned feature spaces. To bridge this gap, Recent advances such as Representation Autoencoders and BLIP3-o advocate performing diffusion-based modeling directly in high level features from pretrained semantic encoders. We find editing in the semantic latent space modifies conceptual representations rather than pixels, ensuring intermediates that are both semantically coherent and visually plausible. Building on this insight, We propose UniEdit-I, the first training-free, closed-loop image editing framework that operates entirely within the semantic latent space of a unified VLM by introducing an Understanding-Editing-Verifying (UEV) loop, By transforming the VLM from a posthoc evaluator into an in-process conductor, UniEdit-I establishes the first semantics-driven, self-correcting closed-loop image editing pipeline. Evaluated on GEdit-Bench, UniEdit-I achieves state of the art performance without any fine tuning or architectural modifications, and even surpasses several largescale pretrained editors.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.08136</link>
<guid>https://arxiv.org/abs/2508.08136</guid>
<content:encoded><![CDATA[
arXiv:2508.08136v2 Announce Type: replace 
Abstract: The success of 3DGS in generative and editing applications has sparked growing interest in 3DGS-based style transfer. However, current methods still face two major challenges: (1) multi-view inconsistency often leads to style conflicts, resulting in appearance smoothing and distortion; and (2) heavy reliance on VGG features, which struggle to disentangle style and content from style images, often causing content leakage and excessive stylization. To tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style transfer framework, and the first to rely entirely on diffusion model distillation. It comprises two key components: (1) \textbf{Multi-View Frequency Consistency}. We enhance cross-view consistency by applying a 3D filter to multi-view noisy latent, selectively reducing low-frequency components to mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized Distillation}. To suppress content leakage from style images, we introduce negative guidance to exclude undesired content. In addition, we identify the limitations of Score Distillation Sampling and Delta Denoising Score in 3D style transfer and remove the reconstruction term accordingly. Building on these insights, we propose a controllable stylized distillation that leverages negative guidance to more effectively optimize the 3D Gaussians. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art approaches, achieving higher stylization quality and visual realism across various scenes and styles. The code is available at https://github.com/yangyt46/FantasyStyle.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2508.11323</link>
<guid>https://arxiv.org/abs/2508.11323</guid>
<content:encoded><![CDATA[
arXiv:2508.11323v2 Announce Type: replace 
Abstract: 3D multi-object tracking is a critical and challenging task in the field of autonomous driving. A common paradigm relies on modeling individual object motion, e.g., Kalman filters, to predict trajectories. While effective in simple scenarios, this approach often struggles in crowded environments or with inaccurate detections, as it overlooks the rich geometric relationships between objects. This highlights the need to leverage spatial cues. However, existing geometry-aware methods can be susceptible to interference from irrelevant objects, leading to ambiguous features and incorrect associations. To address this, we propose focusing on cue-consistency: identifying and matching stable spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency Tracker (DSC-Track) to implement this principle. Firstly, we design a unified spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative trajectory embeddings while suppressing interference. Secondly, our cue-consistency transformer module explicitly aligns consistent feature representations between historical tracks and current detections. Finally, a dynamic update mechanism preserves salient spatiotemporal information for stable online tracking. Extensive experiments on the nuScenes and Waymo Open Datasets validate the effectiveness and robustness of our approach. On the nuScenes benchmark, for instance, our method achieves state-of-the-art performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets, respectively.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing</title>
<link>https://arxiv.org/abs/2508.12409</link>
<guid>https://arxiv.org/abs/2508.12409</guid>
<content:encoded><![CDATA[
arXiv:2508.12409v3 Announce Type: replace 
Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS) analysis by leveraging unlabeled data through pseudo-labeling and consistency learning. However, existing S4 studies often rely on small-scale datasets and models, limiting their practical applicability. To address this, we propose S5, the first scalable framework for semi-supervised semantic segmentation in RS, which unlocks the potential of vast unlabeled Earth observation data typically underutilized due to costly pixel-level annotations. Built upon existing large-scale RS datasets, S5 introduces a data selection strategy that integrates entropy-based filtering and diversity expansion, resulting in the RS4P-1M dataset. Using this dataset, we systematically scale up S4 into a new pretraining paradigm, S4 pre-training (S4P), to pretrain RS foundation models (RSFMs) of varying sizes on this extensive corpus, significantly boosting their performance on land cover segmentation and object detection tasks. Furthermore, during fine-tuning, we incorporate a Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which enables efficient adaptation to multiple RS benchmarks with fewer parameters. This approach improves the generalization and versatility of RSFMs across diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance across all benchmarks, underscoring the viability of scaling semi-supervised learning for RS applications. All datasets, code, and models will be released at https://github.com/MiliLab/S5
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecGen: Neural Spectral BRDF Generation via Spectral-Spatial Tri-plane Aggregation</title>
<link>https://arxiv.org/abs/2508.17316</link>
<guid>https://arxiv.org/abs/2508.17316</guid>
<content:encoded><![CDATA[
arXiv:2508.17316v2 Announce Type: replace 
Abstract: Synthesizing spectral images across different wavelengths is essential for photorealistic rendering. Unlike conventional spectral uplifting methods that convert RGB images into spectral ones, we introduce SpecGen, a novel method that generates spectral bidirectional reflectance distribution functions (BRDFs) from a single RGB image of a sphere. This enables spectral image rendering under arbitrary illuminations and shapes covered by the corresponding material. A key challenge in spectral BRDF generation is the scarcity of measured spectral BRDF data. To address this, we propose the Spectral-Spatial Tri-plane Aggregation (SSTA) network, which models reflectance responses across wavelengths and incident-outgoing directions, allowing the training strategy to leverage abundant RGB BRDF data to enhance spectral BRDF generation. Experiments show that our method accurately reconstructs spectral BRDFs from limited spectral data and surpasses state-of-the-art methods in hyperspectral image reconstruction, achieving an improvement of 8 dB in PSNR. Codes and data will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery</title>
<link>https://arxiv.org/abs/2508.19499</link>
<guid>https://arxiv.org/abs/2508.19499</guid>
<content:encoded><![CDATA[
arXiv:2508.19499v2 Announce Type: replace 
Abstract: Origin-Destination (OD) flow matrices are critical for urban mobility analysis, supporting traffic forecasting, infrastructure planning, and policy design. Existing methods face two key limitations: (1) reliance on costly auxiliary features (e.g., Points of Interest, socioeconomic statistics) with limited spatial coverage, and (2) fragility to spatial topology changes, where reordering urban regions disrupts the structural coherence of generated flows. We propose Sat2Flow, a structure-aware diffusion framework that generates structurally coherent OD flows using only satellite imagery. Our approach employs a multi-kernel encoder to capture diverse regional interactions and a permutation-aware diffusion process that maintains consistency across regional orderings. Through joint contrastive training linking satellite features with OD patterns and equivariant diffusion training enforcing structural invariance, Sat2Flow ensures topological robustness under arbitrary regional reindexing. Experiments on real-world datasets show that Sat2Flow outperforms physics-based and data-driven baselines in accuracy while preserving flow distributions and spatial structures under index permutations. Sat2Flow offers a globally scalable solution for OD flow generation in data-scarce environments, eliminating region-specific auxiliary data dependencies while maintaining structural robustness for reliable mobility modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Driven Object-Oriented Two-Stage Method for Scene Graph Anticipation</title>
<link>https://arxiv.org/abs/2509.05661</link>
<guid>https://arxiv.org/abs/2509.05661</guid>
<content:encoded><![CDATA[
arXiv:2509.05661v2 Announce Type: replace 
Abstract: A scene graph is a structured representation of objects and their spatio-temporal relationships in dynamic scenes. Scene Graph Anticipation (SGA) involves predicting future scene graphs from video clips, enabling applications in intelligent surveillance and human-machine collaboration. While recent SGA approaches excel at leveraging visual evidence, long-horizon forecasting fundamentally depends on semantic priors and commonsense temporal regularities that are challenging to extract purely from visual features. To explicitly model these semantic dynamics, we propose Linguistic Scene Graph Anticipation (LSGA), a linguistic formulation of SGA that performs temporal relational reasoning over sequences of textualized scene graphs, with visual scene-graph detection handled by a modular front-end when operating on video. Building on this formulation, we introduce Object-Oriented Two-Stage Method (OOTSM), a language-based framework that anticipates object-set dynamics and forecasts object-centric relation trajectories with temporal consistency regularization, and we evaluate it on a dedicated benchmark constructed from Action Genome annotations. Extensive experiments show that compact fine-tuned language models with up to 3B parameters consistently outperform strong zero- and one-shot API baselines, including GPT-4o, GPT-4o-mini, and DeepSeek-V3, under matched textual inputs and context windows. When coupled with off-the-shelf visual scene-graph generators, the resulting multimodal system achieves substantial improvements on video-based SGA, boosting long-horizon mR@50 by up to 21.9\% over strong visual SGA baselines.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D and 4D World Modeling: A Survey</title>
<link>https://arxiv.org/abs/2509.07996</link>
<guid>https://arxiv.org/abs/2509.07996</guid>
<content:encoded><![CDATA[
arXiv:2509.07996v3 Announce Type: replace 
Abstract: World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/awesome-3d-4d-world-models
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception</title>
<link>https://arxiv.org/abs/2509.09828</link>
<guid>https://arxiv.org/abs/2509.09828</guid>
<content:encoded><![CDATA[
arXiv:2509.09828v2 Announce Type: replace 
Abstract: Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DeLiVER datasets. Code and models will be available at https://github.com/timbroed/DGFusion
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Data Challenges of Computational Pathology: A Pack-based Multiple Instance Learning Training Framework</title>
<link>https://arxiv.org/abs/2509.20923</link>
<guid>https://arxiv.org/abs/2509.20923</guid>
<content:encoded><![CDATA[
arXiv:2509.20923v2 Announce Type: replace 
Abstract: Computational pathology (CPath) digitizes pathology slides into whole slide images (WSIs), enabling analysis for critical healthcare tasks such as cancer diagnosis and prognosis. However, WSIs possess extremely long sequence lengths (up to 200K), significant length variations (from 200 to 200K), and limited supervision. These extreme variations in sequence length lead to high data heterogeneity and redundancy. Conventional methods often compromise on training efficiency and optimization to preserve such heterogeneity under limited supervision. To comprehensively address these challenges, we propose a pack-based MIL framework. It packs multiple sampled, variable-length feature sequences into fixed-length ones, enabling batched training while preserving data heterogeneity. Moreover, we introduce a residual branch that composes discarded features from multiple slides into a hyperslide which is trained with tailored labels. It offers multi-slide supervision while mitigating feature loss from sampling. Meanwhile, an attention-driven downsampler is introduced to compress features in both branches to reduce redundancy. By alleviating these challenges, our approach achieves an accuracy improvement of up to 8% while using only 12% of the training time in the PANDA(UNI). Extensive experiments demonstrate that focusing data challenges in CPath holds significant potential in the era of foundation models. The code is https://github.com/FangHeng/PackMIL
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Alignment of Popular CNNs to the Brain for Valence Appraisal</title>
<link>https://arxiv.org/abs/2509.21384</link>
<guid>https://arxiv.org/abs/2509.21384</guid>
<content:encoded><![CDATA[
arXiv:2509.21384v2 Announce Type: replace 
Abstract: Convolutional Neural Networks (CNNs) are a popular type of computer model that have proven their worth in many computer vision tasks. Moreover, they form an interesting study object for the field of psychology, with shown correspondences between the workings of CNNs and the human brain. However, these correspondences have so far mostly been studied in the context of general visual perception. In contrast, this paper explores to what extent this correspondence also holds for a more complex brain process, namely social cognition. To this end, we assess the alignment between popular CNN architectures and both human behavioral and fMRI data for image valence appraisal through a correlation analysis. We show that for this task CNNs struggle to go beyond simple visual processing, and do not seem to reflect higher-order brain processing. Furthermore, we present Object2Brain, a novel framework that combines GradCAM and object detection at the CNN-filter level with the aforementioned correlation analysis to study the influence of different object classes on the CNN-to-human correlations. Despite similar correlation trends, different CNN architectures are shown to display different object class sensitivities.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDPose: Exploiting Diffusion Priors for Out-of-Domain and Robust Pose Estimation</title>
<link>https://arxiv.org/abs/2509.24980</link>
<guid>https://arxiv.org/abs/2509.24980</guid>
<content:encoded><![CDATA[
arXiv:2509.24980v2 Announce Type: replace 
Abstract: Pre-trained diffusion models provide rich multi-scale latent features and are emerging as powerful vision backbones. While recent works such as Marigold and Lotus adapt diffusion priors for dense prediction with strong cross-domain generalization, their potential for structured outputs remains underexplored. In this paper, we propose SDPose, a fine-tuning framework built upon Stable Diffusion to fully exploit pre-trained diffusion priors for human pose estimation. First, rather than modifying cross-attention modules or introducing learnable embeddings, we directly predict keypoint heatmaps in the SD U-Net's image latent space to preserve the original generative priors. Second, we map these latent features into keypoint heatmaps through a lightweight convolutional pose head, which avoids disrupting the pre-trained backbone. Finally, to prevent overfitting and enhance out-of-distribution robustness, we incorporate an auxiliary RGB reconstruction branch that preserves domain-transferable generative semantics. To evaluate robustness under domain shift, we further construct COCO-OOD, a style-transferred variant of COCO with preserved annotations. With just one-fifth of the training schedule used by Sapiens on COCO, SDPose attains parity with Sapiens-1B/2B on the COCO validation set and establishes a new state of the art on the cross-domain benchmarks HumanArt and COCO-OOD. Extensive ablations highlight the importance of diffusion priors, RGB reconstruction, and multi-scale SD U-Net features for cross-domain generalization, and t-SNE analyses further explain SD's domain-invariant latent structure. We also show that SDPose serves as an effective zero-shot pose annotator for controllable image and video generation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Score Distillation of Flow Matching Models</title>
<link>https://arxiv.org/abs/2509.25127</link>
<guid>https://arxiv.org/abs/2509.25127</guid>
<content:encoded><![CDATA[
arXiv:2509.25127v2 Announce Type: replace 
Abstract: Diffusion models achieve high-quality image generation but are limited by slow iterative sampling. Distillation methods alleviate this by enabling one- or few-step generation. Flow matching, originally introduced as a distinct framework, has since been shown to be theoretically equivalent to diffusion under Gaussian assumptions, raising the question of whether distillation techniques such as score distillation transfer directly. We provide a simple derivation -- based on Bayes' rule and conditional expectations -- that unifies Gaussian diffusion and flow matching without relying on ODE/SDE formulations. Building on this view, we extend Score identity Distillation (SiD) to pretrained text-to-image flow-matching models, including SANA, SD3-Medium, SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show that, with only modest flow-matching- and DiT-specific adjustments, SiD works out of the box across these models, in both data-free and data-aided settings, without requiring teacher finetuning or architectural changes. This provides the first systematic evidence that score distillation applies broadly to text-to-image flow matching models, resolving prior concerns about stability and soundness and unifying acceleration techniques across diffusion- and flow-based generators. A project page is available at https://yigu1008.github.io/SiD-DiT.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes</title>
<link>https://arxiv.org/abs/2510.03747</link>
<guid>https://arxiv.org/abs/2510.03747</guid>
<content:encoded><![CDATA[
arXiv:2510.03747v2 Announce Type: replace 
Abstract: Deepfakes pose significant societal risks, motivating the development of proactive defenses that embed adversarial perturbations in facial images to prevent manipulation. However, in this paper, we show that these preemptive defenses often lack robustness and reliability. We propose a novel approach, Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch into Deepfake generators to bypass state-of-the-art defenses. A learnable gating mechanism adaptively controls the effect of the LoRA patch and prevents gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature Alignment (MMFA) loss, encouraging the features of adversarial outputs to align with those of the desired outputs at the semantic level. Beyond bypassing, we present defensive LoRA patching, embedding visible warnings in the outputs as a complementary solution to mitigate this newly identified security vulnerability. With only 1,000 facial examples and a single epoch of fine-tuning, LoRA patching successfully defeats multiple proactive defenses. These results reveal a critical weakness in current paradigms and underscore the need for more robust Deepfake defense strategies. Our code is available at https://github.com/ZOMIN28/LoRA-Patching.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control</title>
<link>https://arxiv.org/abs/2510.13186</link>
<guid>https://arxiv.org/abs/2510.13186</guid>
<content:encoded><![CDATA[
arXiv:2510.13186v4 Announce Type: replace 
Abstract: Edge Gaussian splatting (EGS), which aggregates data from distributed clients (e.g., drones) and trains a global GS model at the edge (e.g., ground server), is an emerging paradigm for scene reconstruction in low-altitude economy. Unlike traditional edge resource management methods that emphasize communication throughput or general-purpose learning performance, EGS explicitly aims to maximize the GS qualities, rendering existing approaches inapplicable. To address this problem, this paper formulates a novel GS-oriented objective function that distinguishes the heterogeneous view contributions of different clients. However, evaluating this function in turn requires clients' images, leading to a causality dilemma. To this end, this paper further proposes a sample-then-transmit EGS (or STT-GS for short) strategy, which first samples a subset of images as pilot data from each client for loss prediction. Based on the first-stage evaluation, communication resources are then prioritized towards more valuable clients. To achieve efficient sampling, a feature-domain clustering (FDC) scheme is proposed to select the most representative data and pilot transmission time minimization (PTTM) is adopted to reduce the pilot overhead. Subsequently, we develop a joint client selection and power control (JCSPC) framework to maximize the GS-oriented function under communication resource constraints. Despite the nonconvexity of the problem, we propose a low-complexity efficient solution based on the penalty alternating majorization minimization (PAMM) algorithm. Experiments reveal that the proposed scheme significantly outperforms existing benchmarks on real-world datasets. The GS-oriented objective can be accurately predicted with low sampling ratios (e.g., 10%), and our method achieves an excellent tradeoff between view contributions and communication costs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue</title>
<link>https://arxiv.org/abs/2510.13747</link>
<guid>https://arxiv.org/abs/2510.13747</guid>
<content:encoded><![CDATA[
arXiv:2510.13747v2 Announce Type: replace 
Abstract: We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch</title>
<link>https://arxiv.org/abs/2510.16088</link>
<guid>https://arxiv.org/abs/2510.16088</guid>
<content:encoded><![CDATA[
arXiv:2510.16088v4 Announce Type: replace 
Abstract: Quantization of neural networks provides benefits of inference in less compute and memory requirements. Previous work in quantization lack two important aspects which this work provides. First almost all previous work in quantization used a non-differentiable approach and for learning; the derivative is usually set manually in backpropogation which make the learning ability of algorithm questionable, our approach is not just differentiable, we also provide proof of convergence of our approach to the optimal neural network. Second previous work in shift/logrithmic quantization either have avoided activation quantization along with weight quantization or achieved less accuracy. Learning logrithmic quantize values of form $2^n$ requires the quantization function can scale to more than 1 bit quantization which is another benifit of our quantization that it provides $n$ bits quantization as well. Our approach when tested with image classification task using imagenet dataset, resnet18 and weight quantization only achieves less than 1 percent accuracy compared to full precision accuracy while taking only 15 epochs to train using shift bit quantization and achieves comparable to SOTA approaches accuracy in both weight and activation quantization using shift bit quantization in 15 training epochs with slightly higher(only higher cpu instructions) inference cost compared to 1 bit quantization(without logrithmic quantization) and not requiring any higher precision multiplication.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
<link>https://arxiv.org/abs/2510.18214</link>
<guid>https://arxiv.org/abs/2510.18214</guid>
<content:encoded><![CDATA[
arXiv:2510.18214v2 Announce Type: replace 
Abstract: Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Based Mistake Analysis in Procedural Activities: A Review of Advances and Challenges</title>
<link>https://arxiv.org/abs/2510.19292</link>
<guid>https://arxiv.org/abs/2510.19292</guid>
<content:encoded><![CDATA[
arXiv:2510.19292v2 Announce Type: replace 
Abstract: Mistake analysis in procedural activities is a critical area of research with applications spanning industrial automation, physical rehabilitation, education and human-robot collaboration. This paper reviews vision-based methods for detecting and predicting mistakes in structured tasks, focusing on procedural and executional errors. By leveraging advancements in computer vision, including action recognition, anticipation and activity understanding, vision-based systems can identify deviations in task execution, such as incorrect sequencing, use of improper techniques, or timing errors. We explore the challenges posed by intra-class variability, viewpoint differences and compositional activity structures, which complicate mistake detection. Additionally, we provide a comprehensive overview of existing datasets, evaluation metrics and state-of-the-art methods, categorizing approaches based on their use of procedural structure, supervision levels and learning strategies. Open challenges, such as distinguishing permissible variations from true mistakes and modeling error propagation are discussed alongside future directions, including neuro-symbolic reasoning and counterfactual state modeling. This work aims to establish a unified perspective on vision-based mistake analysis in procedural activities, highlighting its potential to enhance safety, efficiency and task performance across diverse domains.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2510.26641</link>
<guid>https://arxiv.org/abs/2510.26641</guid>
<content:encoded><![CDATA[
arXiv:2510.26641v2 Announce Type: replace 
Abstract: Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MagicView: Multi-View Consistent Identity Customization via Priors-Guided In-Context Learning</title>
<link>https://arxiv.org/abs/2511.00293</link>
<guid>https://arxiv.org/abs/2511.00293</guid>
<content:encoded><![CDATA[
arXiv:2511.00293v2 Announce Type: replace 
Abstract: Recent advances in personalized generative models have demonstrated impressive capabilities in producing identity-consistent images of the same individual across diverse scenes. However, most existing methods lack explicit viewpoint control and fail to ensure multi-view consistency of generated identities. To address this limitation, we present MagicView, a lightweight adaptation framework that equips existing generative models with multi-view generation capability through 3D priors-guided in-context learning. While prior studies have shown that in-context learning preserves identity consistency across grid samples, its effectiveness in multi-view settings remains unexplored. Building upon this insight, we conduct an in-depth analysis of the multi-view in-context learning ability, and design a conditioning architecture that leverages 3D priors to activate this capability for multi-view consistent identity customization. On the other hand, acquiring robust multi-view capability typically requires large-scale multi-dimensional datasets, which makes incorporating multi-view contextual learning under limited data regimes prone to textual controllability degradation. To address this issue, we introduce a novel Semantic Correspondence Alignment loss, which effectively preserves semantic alignment while maintaining multi-view consistency. Extensive experiments demonstrate that MagicView substantially outperforms recent baselines in multi-view consistency, text alignment, identity similarity, and visual quality, achieving strong results with only 100 multi-view training samples.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation</title>
<link>https://arxiv.org/abs/2511.12528</link>
<guid>https://arxiv.org/abs/2511.12528</guid>
<content:encoded><![CDATA[
arXiv:2511.12528v2 Announce Type: replace 
Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images</title>
<link>https://arxiv.org/abs/2511.16717</link>
<guid>https://arxiv.org/abs/2511.16717</guid>
<content:encoded><![CDATA[
arXiv:2511.16717v2 Announce Type: replace 
Abstract: Neutron imaging is essential for diagnosing and optimizing inertial confinement fusion implosions at the National Ignition Facility. Due to the required 10-micrometer resolution, however, neutron image require image reconstruction using iterative algorithms. For low-yield sources, the images may be degraded by various types of noise. Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring the edges where the source information is encoded. Traditional denoising techniques, such as filtering and thresholding, can inadvertently alter critical features or reshape the noise statistics, potentially impacting the ultimate fidelity of the iterative image reconstruction pipeline. However, recent advances in synthetic data production and machine learning have opened new opportunities to address these challenges. In this study, we present an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space, designed to suppress for mixed Gaussian-Poisson noise while preserving essential image features. The network successfully denoises neutron imaging data. Benchmarking against both simulated and experimental NIF datasets demonstrates that our approach achieves lower reconstruction error and superior edge preservation compared to conventional filtering methods such as Block-matching and 3D filtering (BM3D). By validating the effectiveness of unsupervised learning for denoising neutron images, this study establishes a critical first step towards fully AI-driven, end-to-end reconstruction frameworks for ICF diagnostics.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Transferable Optimal Transport via Min-Sliced Transport Plans</title>
<link>https://arxiv.org/abs/2511.19741</link>
<guid>https://arxiv.org/abs/2511.19741</guid>
<content:encoded><![CDATA[
arXiv:2511.19741v2 Announce Type: replace 
Abstract: Optimal Transport (OT) offers a powerful framework for finding correspondences between distributions and addressing matching and alignment problems in various areas of computer vision, including shape analysis, image generation, and multimodal tasks. The computation cost of OT, however, hinders its scalability. Slice-based transport plans have recently shown promise for reducing the computational cost by leveraging the closed-form solutions of 1D OT problems. These methods optimize a one-dimensional projection (slice) to obtain a conditional transport plan that minimizes the transport cost in the ambient space. While efficient, these methods leave open the question of whether learned optimal slicers can transfer to new distribution pairs under distributional shift. Understanding this transferability is crucial in settings with evolving data or repeated OT computations across closely related distributions. In this paper, we study the min-Sliced Transport Plan (min-STP) framework and investigate the transferability of optimized slicers: can a slicer trained on one distribution pair yield effective transport plans for new, unseen pairs? Theoretically, we show that optimized slicers remain close under slight perturbations of the data distributions, enabling efficient transfer across related tasks. To further improve scalability, we introduce a minibatch formulation of min-STP and provide statistical guarantees on its accuracy. Empirically, we demonstrate that the transferable min-STP achieves strong one-shot matching performance and facilitates amortized training for point cloud alignment and flow-based generative modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IW-Bench: Evaluating Large Multimodal Models for Converting Image-to-Web</title>
<link>https://arxiv.org/abs/2409.18980</link>
<guid>https://arxiv.org/abs/2409.18980</guid>
<content:encoded><![CDATA[
arXiv:2409.18980v2 Announce Type: replace-cross 
Abstract: Recently advancements in large multimodal models have led to significant strides in image comprehension capabilities. Despite these advancements, there is a lack of the robust benchmark specifically for assessing the Image-to-Web conversion proficiency of these large models. Primarily, it is essential to ensure the integrity of the web elements generated. These elements comprise visible and invisible categories. Previous evaluation methods (e.g.,BLEU) are notably susceptible to significant alterations due to the presence of invisible elements in Web. Furthermore, it is crucial to measure the layout information of web pages, referring to the positional relationships between elements, which is overlooked by previous work. To address challenges, we have curated and aligned a benchmark of images and corresponding web codes (IW-BENCH). Specifically, we propose the Element Accuracy, which tests the completeness of the elements by parsing the Document Object Model (DOM) tree. Layout Accuracy is also proposed to analyze the positional relationships of elements by converting DOM tree into a common subsequence. Besides, we design a five-hop multimodal Chain-of-Thought Prompting for better performance, which contains five hop: 1) SoM prompt injection. 2) Inferring Elements. 3) Inferring Layout. 4) Inferring Web code. 5) Reflection. Our benchmark comprises 1200 pairs of images and web codes with varying levels of difficulty. We have conducted extensive experiments on existing large multimodal models, offering insights into their performance and areas for improvement in image-to-web domain.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tractable Two-Step Linear Mixing Model Solved with Second-Order Optimization for Spectral Unmixing under Variability</title>
<link>https://arxiv.org/abs/2502.17212</link>
<guid>https://arxiv.org/abs/2502.17212</guid>
<content:encoded><![CDATA[
arXiv:2502.17212v3 Announce Type: replace-cross 
Abstract: In this paper, we propose a Two-Step Linear Mixing Model (2LMM) that bridges the gap between model complexity and computational tractability. The model achieves this by introducing two distinct scaling steps: an endmember scaling step across the image, and another for pixel-wise scaling. We show that this model leads to only a mildly non-convex optimization problem, which we solve with an optimization algorithm that incorporates second-order information. To the authors' knowledge, this work represents the first application of second-order optimization techniques to solve a spectral unmixing problem that models endmember variability. Our method is highly robust, as it requires virtually no hyperparameter tuning and can therefore be used easily and quickly in a wide range of unmixing tasks. We show through extensive experiments on both simulated and real data that the new model is competitive and in some cases superior to the state of the art in unmixing. The model also performs very well in challenging scenarios, such as blind unmixing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PixCell: A generative foundation model for digital histopathology images</title>
<link>https://arxiv.org/abs/2506.05127</link>
<guid>https://arxiv.org/abs/2506.05127</guid>
<content:encoded><![CDATA[
arXiv:2506.05127v2 Announce Type: replace-cross 
Abstract: The digitization of histology slides has revolutionized pathology, providing massive datasets for cancer diagnosis and research. Self-supervised and vision-language models have been shown to effectively mine large pathology datasets to learn discriminative representations. On the other hand, there are unique problems in pathology, such as annotated data scarcity, privacy regulations in data sharing, and inherently generative tasks like virtual staining. Generative models, capable of synthesizing realistic and diverse images, present a compelling solution to address these problems through image synthesis. We introduce PixCell, the first generative foundation model for histopathology images. PixCell is a diffusion model trained on PanCan-30M, a large, diverse dataset derived from 69,184 H&amp;E-stained whole slide images of various cancer types. We employ a progressive training strategy and a self-supervision-based conditioning that allows us to scale up training without any human-annotated data. By conditioning on real slides, the synthetic images capture the properties of the real data and can be used as data augmentation for small-scale datasets to boost classification performance. We prove the foundational versatility of PixCell by applying it to two generative downstream tasks: privacy-preserving synthetic data generation and virtual IHC staining. PixCell's high-fidelity conditional generation enables institutions to use their private data to synthesize highly realistic, site-specific surrogate images that can be shared in place of raw patient data. Furthermore, using datasets of roughly paired H&amp;E-IHC tiles, we learn to translate PixCell's conditioning from H&amp;E to multiple IHC stains, allowing the generation of IHC images from H&amp;E inputs. Our trained models are publicly released to accelerate research in computational pathology.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism</title>
<link>https://arxiv.org/abs/2507.01513</link>
<guid>https://arxiv.org/abs/2507.01513</guid>
<content:encoded><![CDATA[
arXiv:2507.01513v2 Announce Type: replace-cross 
Abstract: By incorporating visual inputs, Multimodal Large Language Models (MLLMs) extend LLMs to support visual reasoning. However, this integration also introduces new vulnerabilities, making MLLMs susceptible to multimodal jailbreak attacks and hindering their safe deployment.Existing defense methods, including Image-to-Text Translation, Safe Prompting, and Multimodal Safety Tuning, attempt to address this by aligning multimodal inputs with LLMs' built-in safeguards.Yet, they fall short in uncovering root causes of multimodal vulnerabilities, particularly how harmful multimodal tokens trigger jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing heavy training overhead.To bridge this gap, we present an comprehensive analysis of where, how and which harmful multimodal tokens bypass safeguards in MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers are responsible for inducing unsafe behaviors, highlighting the potential of precisely removing a small subset of harmful tokens, without requiring safety tuning, can still effectively improve safety against jailbreaks. Motivated by this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense framework that selectively prunes harmful tokens at vulnerable layers while restoring benign features at subsequent layers.Without incurring additional computational overhead, SafePTR significantly enhances the safety of MLLMs while preserving efficiency. Extensive evaluations across three MLLMs and five benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating jailbreak risks without compromising utility.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation</title>
<link>https://arxiv.org/abs/2508.03758</link>
<guid>https://arxiv.org/abs/2508.03758</guid>
<content:encoded><![CDATA[
arXiv:2508.03758v3 Announce Type: replace-cross 
Abstract: Automated segmentation of diabetic foot ulcers (DFUs) plays a critical role in clinical diagnosis, therapeutic planning, and longitudinal wound monitoring. However, this task remains challenging due to the heterogeneous appearance, irregular morphology, and complex backgrounds associated with ulcer regions in clinical photographs. Traditional convolutional neural networks (CNNs), such as U-Net, provide strong localization capabilities but struggle to model long-range spatial dependencies due to their inherently limited receptive fields. To address this, we employ the TransUNet architecture, a hybrid framework that integrates the global attention mechanism of Vision Transformers (ViTs) into the U-Net structure. This combination allows the model to extract global contextual features while maintaining fine-grained spatial resolution. We trained the model on the public Foot Ulcer Segmentation Challenge (FUSeg) dataset using a robust augmentation pipeline and a hybrid loss function to mitigate class imbalance. On the validation set, the model achieved a Dice Similarity Coefficient (F1-score) of 0.8799 using an optimized threshold of 0.4389. To ensure clinical transparency, we integrated Grad-CAM visualizations to highlight model focus areas. Furthermore, a clinical utility analysis demonstrated a strong correlation (Pearson r = 0.9631) between predicted and ground-truth wound areas. These outcomes demonstrate that our approach effectively integrates global and local feature extraction, offering a reliable, effective, and explainable solution for automated foot ulcer assessment.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Attention-Augmented Wavelet YOLO System for Real-time Brain Vessel Segmentation on Transcranial Color-coded Doppler</title>
<link>https://arxiv.org/abs/2508.13875</link>
<guid>https://arxiv.org/abs/2508.13875</guid>
<content:encoded><![CDATA[
arXiv:2508.13875v2 Announce Type: replace-cross 
Abstract: The Circle of Willis (CoW), vital for ensuring consistent blood flow to the brain, is closely linked to ischemic stroke. Accurate assessment of the CoW is important for identifying individuals at risk and guiding appropriate clinical management. Among existing imaging methods, Transcranial Color-coded Doppler (TCCD) offers unique advantages due to its radiation-free nature, affordability, and accessibility. However, reliable TCCD assessments depend heavily on operator expertise for identifying anatomical landmarks and performing accurate angle correction, which limits its widespread adoption. To address this challenge, we propose an AI-powered, real-time CoW auto-segmentation system capable of efficiently capturing cerebral arteries. No prior studies have explored AI-driven cerebrovascular segmentation using TCCD. In this work, we introduce a novel Attention-Augmented Wavelet YOLO (AAW-YOLO) network tailored for TCCD data, designed to provide real-time guidance for brain vessel segmentation in the CoW. We prospectively collected TCCD data comprising 738 annotated frames and 3,419 labeled artery instances to establish a high-quality dataset for model training and evaluation. The proposed AAW-YOLO demonstrated strong performance in segmenting both ipsilateral and contralateral CoW vessels, achieving an average Dice score of 0.901, IoU of 0.823, precision of 0.882, recall of 0.926, and mAP of 0.953, with a per-frame inference speed of 14.199 ms. This system offers a practical solution to reduce reliance on operator experience in TCCD-based cerebrovascular screening, with potential applications in routine clinical workflows and resource-constrained settings. Future research will explore bilateral modeling and larger-scale validation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accuracy-Robustness Trade Off via Spiking Neural Network Gradient Sparsity Trail</title>
<link>https://arxiv.org/abs/2509.23762</link>
<guid>https://arxiv.org/abs/2509.23762</guid>
<content:encoded><![CDATA[
arXiv:2509.23762v3 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) have attracted growing interest in both computational neuroscience and artificial intelligence, primarily due to their inherent energy efficiency and compact memory footprint. However, achieving adversarial robustness in SNNs, (particularly for vision-related tasks) remains a nascent and underexplored challenge. Recent studies have proposed leveraging sparse gradients as a form of regularization to enhance robustness against adversarial perturbations. In this work, we present a surprising finding: under specific architectural configurations, SNNs exhibit natural gradient sparsity and can achieve state-of-the-art adversarial defense performance without the need for any explicit regularization. Further analysis reveals a trade-off between robustness and generalization: while sparse gradients contribute to improved adversarial resilience, they can impair the model's ability to generalize; conversely, denser gradients support better generalization but increase vulnerability to attacks. Our findings offer new insights into the dual role of gradient sparsity in SNN training.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MACS: Measurement-Aware Consistency Sampling for Inverse Problems</title>
<link>https://arxiv.org/abs/2510.02208</link>
<guid>https://arxiv.org/abs/2510.02208</guid>
<content:encoded><![CDATA[
arXiv:2510.02208v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative priors for solving inverse imaging problems. However, their practical deployment is hindered by the substantial computational cost of slow, multi-step sampling. Although Consistency Models (CMs) address this limitation by enabling high-quality generation in only one or a few steps, their direct application to inverse problems has remained largely unexplored. This paper introduces a modified consistency sampling framework specifically designed for inverse problems. The proposed approach regulates the sampler's stochasticity through a measurement-consistency mechanism that leverages the degradation operator, thereby enforcing fidelity to the observed data while preserving the computational efficiency of consistency-based generation. Comprehensive experiments on the Fashion-MNIST and LSUN Bedroom datasets demonstrate consistent improvements across both perceptual and pixel-level metrics, including the Fr\'echet Inception Distance (FID), Kernel Inception Distance (KID), peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM), compared with baseline consistency and diffusion-based sampling methods. The proposed method achieves competitive or superior reconstruction quality with only a small number of sampling steps.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Multi-Domain Translation via Diffusion Routers</title>
<link>https://arxiv.org/abs/2510.03252</link>
<guid>https://arxiv.org/abs/2510.03252</guid>
<content:encoded><![CDATA[
arXiv:2510.03252v2 Announce Type: replace-cross 
Abstract: Multi-domain translation (MDT) aims to learn translations between multiple domains, yet existing approaches either require fully aligned tuples or can only handle domain pairs seen in training, limiting their practicality and excluding many cross-domain mappings. We introduce universal MDT (UMDT), a generalization of MDT that seeks to translate between any pair of $K$ domains using only $K-1$ paired datasets with a central domain. To tackle this problem, we propose Diffusion Router (DR), a unified diffusion-based framework that models all central$\leftrightarrow$non-central translations with a single noise predictor conditioned on the source and target domain labels. DR enables indirect non-central translations by routing through the central domain. We further introduce a novel scalable learning strategy with a variational-bound objective and an efficient Tweedie refinement procedure to support direct non-central mappings. Through evaluation on three large-scale UMDT benchmarks, DR achieves state-of-the-art results for both indirect and direct translations, while lowering sampling cost and unlocking novel tasks such as sketch$\leftrightarrow$segmentation. These results establish DR as a scalable and versatile framework for universal translation across multiple domains.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Cascade Compression: Exploring the Upper Limits of Text Compression</title>
<link>https://arxiv.org/abs/2511.15244</link>
<guid>https://arxiv.org/abs/2511.15244</guid>
<content:encoded><![CDATA[
arXiv:2511.15244v2 Announce Type: replace-cross 
Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale</title>
<link>https://arxiv.org/abs/2512.02055</link>
<guid>https://arxiv.org/abs/2512.02055</guid>
<content:encoded><![CDATA[
<div> Keywords: Flood mapping, Geospatial Foundation Models, Sentinel-1, Sentinel-2, FloodsNet<br /><br />Summary:  
1. Floods are among the most damaging weather-related hazards, with extreme events affecting communities globally, especially in 2024, the warmest year on record.  
2. Earth observation satellites, like Sentinel-1 (SAR) and Sentinel-2 (optical), provide crucial data for flood inundation mapping, but operational accuracy relies heavily on quality labeled datasets and model generalization.  
3. Geospatial Foundation Models (GFMs), exemplified by ESA-IBM's TerraMind, improve generalization through large-scale self-supervised pretraining, yet their performance on diverse global flood events has been underexplored.  
4. This study fine-tuned TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset covering 85 global flood events, testing four configurations varying model sizes and whether backbones were frozen or unfrozen.  
5. The base model with unfrozen backbone balanced accuracy, precision, and recall with significantly lower computational cost. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed those trained on Sen1Floods11 in recall, with similar overall accuracy.  
6. A U-Net trained on both datasets achieved higher recall than all GFM configurations, but with slightly lower accuracy and precision.  
7. Results highlight that integrating multimodal optical and SAR data and fine-tuning a GFM enhances near-real-time flood mapping capabilities.  
8. This represents one of the first global-scale evaluations of a GFM for flood segmentation, outlining its promise and current limitations for climate adaptation and disaster resilience. <div>
arXiv:2512.02055v1 Announce Type: new 
Abstract: Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.
  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.
  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework</title>
<link>https://arxiv.org/abs/2512.02152</link>
<guid>https://arxiv.org/abs/2512.02152</guid>
<content:encoded><![CDATA[
<div> Keywords: Contrastive learning, contrastive loss, information distortion, augmented samples, image classification<br /><br />Summary:<br /><br />This paper addresses shortcomings in traditional contrastive learning where models often over-rely on label-based similarities and neglect positive pairs from the same source image, causing information distortion especially in large datasets. The authors propose a novel context-enriched contrastive loss function that incorporates two convergence targets: one sensitive to label contrasts to effectively distinguish features of identical versus distinct classes, and another that brings augmented samples from the same original image closer while distancing others. This dual-target approach aims to enhance both learning efficiency and reduce information distortion. The method was evaluated on eight large-scale image classification benchmark datasets including CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA. Experimental results showed the proposed approach outperformed 16 state-of-the-art contrastive learning methods in terms of generalization and convergence speed. Notably, the approach demonstrated a significant 22.9% improvement over original contrastive loss functions on the BiasedMNIST dataset, which involves systematic distortion tasks. These results suggest that the context-enriched contrastive loss enhances training efficiency and leads to more equitable downstream model performance across diverse tasks and datasets. <div>
arXiv:2512.02152v1 Announce Type: new 
Abstract: Contrastive learning has gained popularity and pushes state-of-the-art performance across numerous large-scale benchmarks. In contrastive learning, the contrastive loss function plays a pivotal role in discerning similarities between samples through techniques such as rotation or cropping. However, this learning mechanism can also introduce information distortion from the augmented samples. This is because the trained model may develop a significant overreliance on information from samples with identical labels, while concurrently neglecting positive pairs that originate from the same initial image, especially in expansive datasets. This paper proposes a context-enriched contrastive loss function that concurrently improves learning effectiveness and addresses the information distortion by encompassing two convergence targets. The first component, which is notably sensitive to label contrast, differentiates between features of identical and distinct classes which boosts the contrastive training efficiency. Meanwhile, the second component draws closer the augmented samples from the same source image and distances all other samples. We evaluate the proposed approach on image classification tasks, which are among the most widely accepted 8 recognition large-scale benchmark datasets: CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA datasets. The experimental results demonstrate that the proposed method achieves improvements over 16 state-of-the-art contrastive learning methods in terms of both generalization performance and learning convergence speed. Interestingly, our technique stands out in addressing systematic distortion tasks. It demonstrates a 22.9% improvement compared to original contrastive loss functions in the downstream BiasedMNIST dataset, highlighting its promise for more efficient and equitable downstream training.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges</title>
<link>https://arxiv.org/abs/2512.02161</link>
<guid>https://arxiv.org/abs/2512.02161</guid>
<content:encoded><![CDATA[
<div> Text-to-image generation, Vision language models, Failure modes, Attribute fidelity, Benchmarking<br /><br />Summary:  
This paper addresses the limitations of current text-to-image (T2I) models, which, despite generating visually impressive images, often fail to accurately depict specific attributes from user prompts, such as the correct number and color of objects. To better evaluate these models, the authors propose a hierarchical evaluation framework designed to compare how well different T2I models adhere to prompt details. Recognizing a gap in benchmarks for vision language models (VLMs), especially with complex scenes, the study introduces a structured methodology for jointly assessing T2I models and VLMs. This involves testing whether VLMs can detect 27 distinct failure modes present in images generated by T2I systems under challenging prompts. The authors contribute a novel dataset comprising prompts, images generated by five T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large), and annotations from three VLMs (Molmo, InternVL3, Pixtral), with labels curated by the Llama3 large language model to verify failure mode identifications. The analysis uncovers systematic errors related to attribute fidelity and object representation in generated images. The findings underscore the inadequacy of existing evaluation metrics to fully capture these nuanced errors and highlight the critical need for targeted benchmarks to enhance the reliability and interpretability of generative models. <div>
arXiv:2512.02161v1 Announce Type: new 
Abstract: Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mapping of Lesion Images to Somatic Mutations</title>
<link>https://arxiv.org/abs/2512.02162</link>
<guid>https://arxiv.org/abs/2512.02162</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, somatic mutations, deep latent variable model, variational autoencoders, cancer diagnosis<br /><br />Summary:  
The article presents a novel deep latent variable model designed to predict patients' somatic mutation profiles based on medical imaging data. Initially, the authors introduce a point cloud representation of lesion images to achieve invariance across different imaging modalities. The core of the approach is LLOST, a model that integrates dual variational autoencoders connected through a shared latent space, which combines features from lesion point clouds and counts of distinct somatic mutations. Three separate latent spaces are employed, each modeled with conditional normalizing flow priors to handle the diverse distribution characteristics of each domain effectively. The study conducts both qualitative and quantitative experiments using de-identified medical images from The Cancer Imaging Archive alongside somatic mutation data from the Pan Cancer dataset of The Cancer Genomic Archive. Results demonstrate that the model can predict specific mutation counts and the occurrence of mutations with notable accuracy. Moreover, it reveals shared patterns between imaging data and somatic mutation profiles that correspond to cancer types. The authors discuss potential improvements for the model and propose future research directions involving the inclusion of additional genetic information domains to enhance predictive capability and clinical relevance. <div>
arXiv:2512.02162v1 Announce Type: new 
Abstract: Medical imaging is a critical initial tool used by clinicians to determine a patient's cancer diagnosis, allowing for faster intervention and more reliable patient prognosis. At subsequent stages of patient diagnosis, genetic information is extracted to help select specific patient treatment options. As the efficacy of cancer treatment often relies on early diagnosis and treatment, we build a deep latent variable model to determine patients' somatic mutation profiles based on their corresponding medical images. We first introduce a point cloud representation of lesions images to allow for invariance to the imaging modality. We then propose, LLOST, a model with dual variational autoencoders coupled together by a separate shared latent space that unifies features from the lesion point clouds and counts of distinct somatic mutations. Therefore our model consists of three latent space, each of which is learned with a conditional normalizing flow prior to account for the diverse distributions of each domain. We conduct qualitative and quantitative experiments on de-identified medical images from The Cancer Imaging Archive and the corresponding somatic mutations from the Pan Cancer dataset of The Cancer Genomic Archive. We show the model's predictive performance on the counts of specific mutations as well as it's ability to accurately predict the occurrence of mutations. In particular, shared patterns between the imaging and somatic mutation domain that reflect cancer type. We conclude with a remark on how to improve the model and possible future avenues of research to include other genetic domains.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.02172</link>
<guid>https://arxiv.org/abs/2512.02172</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, Super-Resolution, Multi-View Consistency, Camera Pose, Novel View Synthesis

<br /><br />Summary:  
1. The paper addresses the challenge of generating higher-resolution renders from 3D Gaussian Splatting (3DGS) for novel view synthesis, which is limited by the resolution of training images.  
2. Applying super-resolution (SR) independently to each low-resolution (LR) input image results in multi-view inconsistencies, causing blurriness in rendered views.  
3. Existing methods mitigate inconsistencies by learned neural components, temporal video priors, or joint optimization, but these methods apply uniform SR across all views.  
4. The authors propose SplatSuRe, a method that selectively applies SR only in undersampled regions identified using camera pose relative to scene geometry, preserving high-frequency details where multiple views overlap.  
5. This selective approach leads to sharper and more consistent renders, outperforming baseline methods on benchmarks such as Tanks & Temples, Deep Blending, and Mip-NeRF 360.  
6. Improvements are especially notable in localized foreground regions where higher detail is most beneficial.  
7. Overall, SplatSuRe enhances fidelity and perceptual quality by intelligently incorporating SR information based on multi-view sampling density and scene geometry. <div>
arXiv:2512.02172v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustSurg: Tackling domain generalisation for out-of-distribution surgical scene segmentation</title>
<link>https://arxiv.org/abs/2512.02188</link>
<guid>https://arxiv.org/abs/2512.02188</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical scene segmentation, domain generalisation, instance normalisation, feature covariance mapping, multicentre dataset<br /><br />Summary:<br /><br />1. This study addresses the challenge of generalising deep learning models for surgical scene segmentation across different centres and imaging modalities, where existing methods often fail due to distribution shifts and modality gaps.<br /><br />2. Inspired by successful approaches in natural scene generalisation, the authors propose exploiting style and content information through instance normalisation and feature covariance mapping to reduce appearance variability caused by factors like blood and imaging artefacts.<br /><br />3. To prevent the loss of important feature information linked to objects of interest, a novel restitution module is integrated within a ResNet backbone to retain task-relevant features during feature learning.<br /><br />4. Due to the scarcity of multiclass, multicentre surgical datasets, the work also introduces a newly curated dataset aimed at improving generalisability research in this domain.<br /><br />5. The proposed method, named RobustSurg, demonstrates significant performance gains, including approximately 23% improvement over the DeepLabv3+ baseline and 10-32% improvement over state-of-the-art (SOTA) on an unseen centre dataset (HeiCholSeg), as well as nearly 22% and 11% improvements on the EndoUDA polyp dataset compared to baseline and recent SOTA methods, respectively. <div>
arXiv:2512.02188v1 Announce Type: new 
Abstract: While recent advances in deep learning for surgical scene segmentation have demonstrated promising results on single-centre and single-imaging modality data, these methods usually do not generalise to unseen distribution (i.e., from other centres) and unseen modalities. Current literature for tackling generalisation on out-of-distribution data and domain gaps due to modality changes has been widely researched but mostly for natural scene data. However, these methods cannot be directly applied to the surgical scenes due to limited visual cues and often extremely diverse scenarios compared to the natural scene data. Inspired by these works in natural scenes to push generalisability on OOD data, we hypothesise that exploiting the style and content information in the surgical scenes could minimise the appearances, making it less variable to sudden changes such as blood or imaging artefacts. This can be achieved by performing instance normalisation and feature covariance mapping techniques for robust and generalisable feature representations. Further, to eliminate the risk of removing salient feature representation associated with the objects of interest, we introduce a restitution module within the feature learning ResNet backbone that can enable the retention of useful task-relevant features. To tackle the lack of multiclass and multicentre data for surgical scene segmentation, we also provide a newly curated dataset that can be vital for addressing generalisability in this domain. Our proposed RobustSurg obtained nearly 23% improvement on the baseline DeepLabv3+ and from 10-32% improvement on the SOTA in terms of mean IoU score on an unseen centre HeiCholSeg dataset when trained on CholecSeg8K. Similarly, RobustSurg also obtained nearly 22% improvement over the baseline and nearly 11% improvement on a recent SOTA method for the target set of the EndoUDA polyp dataset.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multifractal Recalibration of Neural Networks for Medical Imaging Segmentation</title>
<link>https://arxiv.org/abs/2512.02198</link>
<guid>https://arxiv.org/abs/2512.02198</guid>
<content:encoded><![CDATA[
<div> Multifractal analysis, deep learning, channel-attention, semantic segmentation, medical imaging

<br /><br />Summary:  
This paper addresses the limited use of multifractal analysis in deep learning, especially for tasks like semantic segmentation where traditional multifractal methods are hampered by heavy pooling and feature-space decimation. The authors introduce two inductive priors, Monofractal and Multifractal Recalibration, which exploit the relationship between probability mass of exponents and the multifractal spectrum to statistically describe encoder embeddings. These recalibration methods are implemented as channel-attention mechanisms within convolutional neural networks. Using a U-Net-based architecture, they demonstrate that multifractal recalibration outperforms baseline models equipped with other channel-attention strategies that leverage higher-order statistics. The approach is validated on three public medical imaging datasets: ISIC18 for dermoscopy, Kvasir-SEG for endoscopy, and BUSI for ultrasound imaging, highlighting its applicability to pathological image analysis. Their empirical studies reveal that excitation responses in U-Net encoders do not progressively specialize with depth due to skip connections. Additionally, the effectiveness of the attention layers may correlate with the global statistics of instance variability. This work offers new insights into the integration of multifractal methods within modern deep learning frameworks, improving segmentation accuracy in medical imaging tasks. <div>
arXiv:2512.02198v1 Announce Type: new 
Abstract: Multifractal analysis has revealed regularities in many self-seeding phenomena, yet its use in modern deep learning remains limited. Existing end-to-end multifractal methods rely on heavy pooling or strong feature-space decimation, which constrain tasks such as semantic segmentation. Motivated by these limitations, we introduce two inductive priors: Monofractal and Multifractal Recalibration. These methods leverage relationships between the probability mass of the exponents and the multifractal spectrum to form statistical descriptions of encoder embeddings, implemented as channel-attention functions in convolutional networks.
  Using a U-Net-based framework, we show that multifractal recalibration yields substantial gains over a baseline equipped with other channel-attention mechanisms that also use higher-order statistics. Given the proven ability of multifractal analysis to capture pathological regularities, we validate our approach on three public medical-imaging datasets: ISIC18 (dermoscopy), Kvasir-SEG (endoscopy), and BUSI (ultrasound).
  Our empirical analysis also provides insights into the behavior of these attention layers. We find that excitation responses do not become increasingly specialized with encoder depth in U-Net architectures due to skip connections, and that their effectiveness may relate to global statistics of instance variability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Video Quality Assessment</title>
<link>https://arxiv.org/abs/2512.02224</link>
<guid>https://arxiv.org/abs/2512.02224</guid>
<content:encoded><![CDATA[
<div> Keywords: video quality assessment, Diagnostic Mixture-of-Experts, multi-proxy expert training, interpretable feedback, streaming artifacts<br /><br />Summary:<br /><br />1. Traditional video quality assessment (VQA) models often provide only a single quality score per video, lacking diagnostic and interpretable feedback to explain quality degradation.<br />2. Many existing VQA methods are format-specific and fail to generalize across different video formats and distortion types, limiting their practicality.<br />3. This paper introduces Unified-VQA, a novel unified quality assessment framework that treats VQA as a Diagnostic Mixture-of-Experts (MoE) problem, employing multiple perceptual experts specialized for distinct perceptual domains.<br />4. A new multi-proxy expert training strategy is proposed, which optimizes each expert using a ranking-inspired loss guided by the most appropriate proxy metric corresponding to its domain.<br />5. The framework features a diagnostic multi-task head that outputs both a global quality score and a multi-dimensional artifact vector, leveraging weakly-supervised learning based on known training data properties.<br />6. Without retraining or fine-tuning, Unified-VQA consistently outperforms more than 18 benchmark methods on generic VQA and diagnostic artifact detection tasks across 17 diverse databases including HD, UHD, HDR, and HFR streaming artifacts.<br />7. The approach marks a significant advancement towards practical, actionable, and interpretable video quality assessment applicable across multiple video formats and distortion types. <div>
arXiv:2512.02224v1 Announce Type: new 
Abstract: Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.02231</link>
<guid>https://arxiv.org/abs/2512.02231</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, audiovisual reasoning, speaker-centric, AV-SpeakerBench, Gemini models<br /><br />Summary:  
(1) Multimodal large language models (MLLMs) aim to jointly interpret vision, audio, and language, but current video benchmarks lack fine-grained evaluation of human speech understanding.  
(2) Many existing tasks can be solved primarily through visual information or only coarsely assess speech, limiting assessment of models’ ability to align the speaker identity, spoken content, and timing.  
(3) The authors introduce AV-SpeakerBench, a new benchmark consisting of 3,212 multiple-choice questions designed specifically for speaker-centric audiovisual reasoning in real-world videos.  
(4) AV-SpeakerBench emphasizes speakers as the central reasoning unit rather than scenes, embeds audiovisual dependencies directly into question semantics, and uses expert annotations to ensure temporal accuracy and cross-modal validity.  
(5) Evaluation results show that the Gemini model family consistently outperforms open-source alternatives, with Gemini 2.5 Pro achieving the best performance; Qwen3-Omni-30B is competitive with Gemini 2.0 Flash but remains behind Gemini 2.5 Pro mainly due to weaker audiovisual fusion capabilities rather than visual perception.  
(6) This benchmark establishes a rigorous foundation for future development of fine-grained audiovisual reasoning in multimodal AI systems. <div>
arXiv:2512.02231v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring the Potentials of Spiking Neural Networks for Image Deraining</title>
<link>https://arxiv.org/abs/2512.02258</link>
<guid>https://arxiv.org/abs/2512.02258</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Neural Networks, image deraining, Visual LIF neuron, energy efficiency, multi-scale representation<br /><br />Summary:<br /><br />This paper explores the application of Spiking Neural Networks (SNNs), which are biologically plausible and energy-efficient, in the domain of low-level vision tasks, focusing on image deraining. The study identifies that traditional spiking neurons inherently represent high-pass characteristics but lack adequate spatial contextual understanding needed for effective deraining. To overcome this, the authors propose a novel Visual LIF (VLIF) neuron that enhances spatial information processing. They also address the problem of frequency-domain saturation common in conventional spiking neurons by introducing the Spiking Decomposition and Enhancement Module alongside a lightweight Spiking Multi-scale Unit. These innovations enable hierarchical multi-scale representation learning, improving the network’s ability to restore images degraded by rain. The method was evaluated extensively on five benchmark deraining datasets, showing significant improvements over existing state-of-the-art SNN-based deraining methods. Moreover, the proposed framework achieves this high performance with only 13% of the energy consumption compared to prior works, demonstrating both efficiency and effectiveness. The results support the feasibility of deploying SNNs for high-performance, energy-conscious low-level vision applications. <div>
arXiv:2512.02258v1 Announce Type: new 
Abstract: Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatiotemporal Pyramid Flow Matching for Climate Emulation</title>
<link>https://arxiv.org/abs/2512.02268</link>
<guid>https://arxiv.org/abs/2512.02268</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative models, climate emulation, Spatiotemporal Pyramid Flows, flow matching, ClimateSuite<br /><br />Summary:<br /><br />This paper introduces Spatiotemporal Pyramid Flows (SPF), a novel generative modeling approach for emulating Earth's climate that improves upon previous weather-scale autoregressive methods by enabling faster and more stable long-term simulations. SPF employs a hierarchical spatiotemporal pyramid design that progressively increases spatial resolution while associating each stage with distinct temporal scales, allowing direct and efficient sampling across multiple timescales. Conditioning on prescribed physical forcings such as greenhouse gases and aerosols enhances the model's ability to generate realistic climate scenarios. When evaluated on ClimateBench, SPF outperforms state-of-the-art flow matching baselines and pre-trained models at both yearly and monthly scales, offering especially fast sampling at coarser temporal levels. To support scalability and further research, the authors present ClimateSuite, the largest Earth system simulation dataset to date, containing more than 33,000 simulation-years from ten different climate models, including novel climate intervention scenarios. The scaled SPF model exhibits strong generalization capabilities to unseen scenarios and climate models, demonstrating its robustness and applicability. The combination of SPF and ClimateSuite establishes a comprehensive framework for accurate, efficient, and probabilistic climate emulation that can aid in forecasting and understanding future climate changes. All associated data and code are publicly accessible at the provided GitHub repository. <div>
arXiv:2512.02268v1 Announce Type: new 
Abstract: Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Image Restoration via Text-Conditioned Video Generation</title>
<link>https://arxiv.org/abs/2512.02273</link>
<guid>https://arxiv.org/abs/2512.02273</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, image restoration, CogVideo, super-resolution, zero-shot<br /><br />Summary: Recent advances in text-to-video models have shown strong temporal generation capabilities, but their use for image restoration is less explored. This work repurposes the CogVideo model to handle progressive visual restoration tasks such as super-resolution, deblurring, and low-light enhancement by fine-tuning it to generate restoration trajectories instead of typical video motions. Synthetic datasets are created where each sample illustrates a gradual transition from degraded to clean frames, enabling the model to learn temporal progression linked to restoration quality. Two prompting strategies are evaluated: a uniform text prompt applied across all samples and a scene-specific prompt generated via the LLaVA multi-modal large language model and refined with ChatGPT. The fine-tuned CogVideo produces sequences that progressively improve key perceptual metrics, including PSNR, SSIM, and LPIPS, indicating enhanced spatial detail and illumination consistency. Experimental results confirm the model maintains temporal coherence throughout the restoration process. Notably, the model also demonstrates strong zero-shot robustness by generalizing effectively to real-world datasets like ReLoBlur without further training. This showcases its interpretability through temporal restoration and suggests promising directions for applying video generation models in image restoration tasks. <div>
arXiv:2512.02273v1 Announce Type: new 
Abstract: Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Cross Domain SAR Oil Spill Segmentation via Morphological Region Perturbation and Synthetic Label-to-SAR Generation</title>
<link>https://arxiv.org/abs/2512.02290</link>
<guid>https://arxiv.org/abs/2512.02290</guid>
<content:encoded><![CDATA[
<div> Keywords: SAR oil spill segmentation, domain adaptation, synthetic augmentation, Morphological Region Perturbation, Sentinel-1 dataset<br /><br />Summary:<br /><br />This paper addresses the challenge of poor generalization of deep learning models for oil spill segmentation in Synthetic Aperture Radar (SAR) images across different geographic regions, notably from the Mediterranean to the Peruvian coast where labeled data is limited. The authors introduce MORP--Synth, a two-stage synthetic augmentation framework designed to enhance domain transfer. The first stage applies Morphological Region Perturbation, a curvature-guided method that generates realistic geometric variations of oil spills and look-alike areas within label masks. The second stage uses a conditional generative INADE model to synthesize SAR-like textures from these modified masks, creating realistic augmented images. To validate their approach, the authors compile and harmonize a novel Peruvian dataset of 2112 labeled 512×512 patches from 40 Sentinel-1 scenes with the Mediterranean CleanSeaNet benchmark. They evaluate seven different segmentation architectures and find that models pretrained on Mediterranean data drop significantly in performance (from 67.8% to 51.8% mean Intersection over Union, mIoU) when applied directly to Peruvian data. Incorporating MORP--Synth synthetic augmentation improves the mIoU by up to 6 points overall and leads to substantial gains in minority-class IoUs, especially for oil (+10.8) and look-alike (+14.6) regions, demonstrating the efficacy of the method in enhancing generalization to new SAR domains. <div>
arXiv:2512.02290v1 Announce Type: new 
Abstract: Deep learning models for SAR oil spill segmentation often fail to generalize across regions due to differences in sea-state, backscatter statistics, and slick morphology, a limitation that is particularly severe along the Peruvian coast where labeled Sentinel-1 data remain scarce. To address this problem, we propose \textbf{MORP--Synth}, a two-stage synthetic augmentation framework designed to improve transfer from Mediterranean to Peruvian conditions. Stage~A applies Morphological Region Perturbation, a curvature guided label space method that generates realistic geometric variations of oil and look-alike regions. Stage~B renders SAR-like textures from the edited masks using a conditional generative INADE model. We compile a Peruvian dataset of 2112 labeled 512$\times$512 patches from 40 Sentinel-1 scenes (2014--2024), harmonized with the Mediterranean CleanSeaNet benchmark, and evaluate seven segmentation architectures. Models pretrained on Mediterranean data degrade from 67.8\% to 51.8\% mIoU on the Peruvian domain; MORP--Synth improves performance up to +6 mIoU and boosts minority-class IoU (+10.8 oil, +14.6 look-alike).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision</title>
<link>https://arxiv.org/abs/2512.02339</link>
<guid>https://arxiv.org/abs/2512.02339</guid>
<content:encoded><![CDATA[
<div> motion representations, self-supervised tracking, video diffusion models, visually similar objects, denoising process<br /><br />Summary:<br /><br />This paper addresses the challenge of distinguishing visually similar objects in video tracking, a critical problem in computer vision. It identifies limitations in current self-supervised trackers, which struggle with ambiguous visual cues and thus require extensive labeled data to generalize effectively. The authors discover that pre-trained video diffusion models naturally learn motion representations suitable for tracking tasks without additional task-specific training. This capability emerges from the denoising process within these models, where motion information is isolated during early, high-noise stages, separate from later stages focused on appearance refinement. Building on this insight, the proposed self-supervised tracker leverages diffusion-derived motion features to improve performance, especially in scenarios involving visually similar or even identical objects. Experiments demonstrate up to a 6-point improvement over recent self-supervised methods on standard benchmarks and a newly introduced test suite emphasizing similar object tracking challenges. Visualizations further validate that the learned representations enable robust tracking across difficult viewpoint changes and object deformations, highlighting the method’s robustness and generalization potential without reliance on large labeled datasets. This work opens new directions for exploiting video diffusion models in motion-centric vision tasks. <div>
arXiv:2512.02339v1 Announce Type: new 
Abstract: Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TALO: Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction</title>
<link>https://arxiv.org/abs/2512.02341</link>
<guid>https://arxiv.org/abs/2512.02341</guid>
<content:encoded><![CDATA[
<div> 3D vision, temporal consistency, Thin Plate Spline, submap registration, noisy geometry<br /><br />Summary:<br /><br />This paper addresses the challenge of maintaining temporal consistency in 3D vision foundation models when deployed in online or dynamic environments like driving scenarios, where predictions occur over temporal windows. The authors identify limitations in current alignment methods that rely on global transformations, specifically issues with assumption validity, limited local alignment scope, and vulnerability to noisy geometric inputs. To overcome these, they propose a novel alignment framework that employs Thin Plate Spline (TPS) transformations offering higher degrees of freedom (DOF) for spatially varying corrections. Their approach uses globally propagated control points to ensure long-term and flexible alignment of consecutive predictions. Additionally, the framework introduces a point-agnostic submap registration strategy, enhancing robustness against noisy and imperfect geometric data. Designed as a plug-and-play solution, it is compatible with various 3D foundation models and camera configurations, including monocular and surround-view setups. Extensive experiments across multiple datasets, backbone models, and camera types demonstrate that their method significantly improves the coherence of reconstructed 3D geometry and reduces trajectory errors. The code for the proposed framework is publicly released, promoting reproducibility and further research in temporal alignment for 3D vision applications. <div>
arXiv:2512.02341v1 Announce Type: new 
Abstract: 3D vision foundation models have shown strong generalization in reconstructing key 3D attributes from uncalibrated images through a single feed-forward pass. However, when deployed in online settings such as driving scenarios, predictions are made over temporal windows, making it non-trivial to maintain consistency across time. Recent strategies align consecutive predictions by solving global transformation, yet our analysis reveals their fundamental limitations in assumption validity, local alignment scope, and robustness under noisy geometry. In this work, we propose a higher-DOF and long-term alignment framework based on Thin Plate Spline, leveraging globally propagated control points to correct spatially varying inconsistencies. In addition, we adopt a point-agnostic submap registration design that is inherently robust to noisy geometry predictions. The proposed framework is fully plug-and-play, compatible with diverse 3D foundation models and camera configurations (e.g., monocular or surround-view). Extensive experiments demonstrate that our method consistently yields more coherent geometry and lower trajectory errors across multiple datasets, backbone models, and camera setups, highlighting its robustness and generality. Codes are publicly available at \href{https://github.com/Xian-Bei/TALO}{https://github.com/Xian-Bei/TALO}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multi-weight self-matching visual explanation for cnns on sar images</title>
<link>https://arxiv.org/abs/2512.02344</link>
<guid>https://arxiv.org/abs/2512.02344</guid>
<content:encoded><![CDATA[
<div> Keywords: convolutional neural networks, interpretability, multi-weight self-matching class activation mapping, synthetic aperture radar, weakly-supervised object localization  

<br /><br />Summary:  
This paper addresses the challenge of improving interpretability in convolutional neural networks (CNNs) applied to synthetic aperture radar (SAR) tasks. The authors propose a novel visual explanation method named multi-weight self-matching class activation mapping (MS-CAM), designed to enhance understanding of the decision-making process within CNNs. MS-CAM functions by matching SAR images with the CNN-extracted feature maps and their gradients, integrating both channel-wise and element-wise weights to visualize the model's learned decision basis effectively. The method is validated on a self-constructed SAR target classification dataset where it demonstrates superior ability to highlight important network regions of interest and capture detailed target features, thereby improving the overall interpretability of the network. Additionally, the paper explores the potential of MS-CAM in weakly-supervised object localization within SAR imagery, investigating key factors influencing localization accuracy, such as pixel threshold settings. The thorough analysis and experimental results underscore MS-CAM's value in advancing reliable CNN applications in SAR by making their internal mechanisms more transparent and interpretable, which is crucial for high-reliability requirements in this domain. <div>
arXiv:2512.02344v1 Announce Type: new 
Abstract: In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Harnessing Sparsity in Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2512.02351</link>
<guid>https://arxiv.org/abs/2512.02351</guid>
<content:encoded><![CDATA[
<div> Keywords: unified multimodal models, training-free pruning, Mixture-of-Experts, sparse activation, model compressibility  

<br /><br />Summary:  
This paper addresses the inefficiencies inherent in unified multimodal models, which combine diverse components for both understanding and generation tasks but often use more resources than necessary for specific tasks or samples. 1) The authors perform a systematic analysis leveraging training-free pruning techniques, including both depth pruning and width reduction, to evaluate model component compressibility. 2) Their findings indicate that the understanding components are quite compressible across tasks, especially for generation, whereas generation components are sensitive and degrade rapidly when compressed. 3) To overcome the sensitivity in generation modules, the work introduces a Mixture-of-Experts (MoE) Adaptation method, partitioning the generation module into multiple experts and selectively activating them based on the input. 4) Sparse activation through expert-frozen tuning effectively restores and maintains generation quality despite compression, with further performance improvements found via full training adaptation. 5) The resulting adapted BAGEL model achieves comparable performance to the original, using only approximately half of the parameters during inference. The authors also provide open-source code to facilitate further research and application. <div>
arXiv:2512.02351v1 Announce Type: new 
Abstract: Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting</title>
<link>https://arxiv.org/abs/2512.02359</link>
<guid>https://arxiv.org/abs/2512.02359</guid>
<content:encoded><![CDATA[
<div> Multi-view crowd counting, weakly supervised learning, calibration-free, self-supervised ranking loss, semantic information<br /><br />Summary:<br /><br />This paper addresses the challenges of multi-view crowd counting, particularly the occlusion problems in single-image counting. Existing multi-view methods rely on projecting images onto a common ground-plane density map space, which demands costly crowd annotations and camera calibrations. To overcome these limitations, the authors propose a novel weakly-supervised, calibration-free multi-view crowd counting method (WSCF-MVCC). Unlike previous methods that require expensive image-level density map annotations, WSCF-MVCC uses only crowd count as supervision, eliminating the need for detailed annotation. It incorporates a self-supervised ranking loss leveraging multi-scale priors, enhancing the single-view counting module’s perceptual ability without extra annotation costs. Additionally, the model employs semantic information to improve the accuracy of view matching, leading to more precise scene-level crowd count estimations. Extensive experiments on three widely used multi-view counting datasets show that WSCF-MVCC outperforms existing state-of-the-art approaches under weakly supervised settings. The results indicate that this method is not only effective but also more practical for real-world deployment compared to calibration-dependent methods. The authors release their code publicly, contributing to the community’s development of efficient, annotation-light multi-view crowd counting solutions. <div>
arXiv:2512.02359v1 Announce Type: new 
Abstract: Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VACoT: Rethinking Visual Data Augmentation with VLMs</title>
<link>https://arxiv.org/abs/2512.02361</link>
<guid>https://arxiv.org/abs/2512.02361</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Augmentation Chain-of-Thought, visual language models, image augmentations, OCR adversarial scenarios, reinforcement learning<br /><br />Summary: This paper addresses the challenge of improving robustness in visual language models (VLMs) by introducing a novel framework called Visual Augmentation Chain-of-Thought (VACoT). Unlike traditional data augmentation methods mostly used during training, VACoT dynamically applies image augmentations during model inference to enhance perception capabilities. The framework incorporates a diverse set of post-hoc visual transformations, including denoising, that go beyond simple cropping techniques, thereby broadening the range of input views processed by the model. VACoT leverages efficient agentic reinforcement learning with a conditional reward scheme, which balances the need for necessary augmentations while penalizing overly verbose outputs, promoting concise yet effective reasoning. This approach not only improves model robustness against out-of-distribution inputs but is particularly effective in optical character recognition (OCR) tasks subject to adversarial attacks. Extensive evaluation across 13 perception benchmarks validates VACoT’s superiority over existing methods. Additionally, the authors introduce a new adversarial OCR dataset, AdvOCR, to demonstrate the generalization advantages brought by their post-hoc augmentation approach in adversarial contexts. Overall, VACoT offers a promising direction for enhancing VLM performance with reduced training overhead and improved resilience to challenging real-world visual scenarios. <div>
arXiv:2512.02361v1 Announce Type: new 
Abstract: While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Tuberculosis: A Comparative Dive into Machine Learning for Tuberculosis Detection</title>
<link>https://arxiv.org/abs/2512.02364</link>
<guid>https://arxiv.org/abs/2512.02364</guid>
<content:encoded><![CDATA[
<div> Keywords: tuberculosis, machine learning, ResNet-50, SqueezeNet, chest X-ray

<br /><br />Summary:  
This study investigates the use of machine learning models, specifically a pretrained ResNet-50 and a general SqueezeNet, for diagnosing tuberculosis (TB) using chest X-ray images. TB remains a challenging infectious disease to diagnose, particularly in resource-poor settings where traditional diagnostic methods like sputum smear microscopy and culture are inefficient. The research utilized a Kaggle dataset consisting of 4,200 chest X-rays, applying preprocessing steps such as data splitting, augmentation, and resizing to optimize model training. Performance metrics including accuracy, precision, recall, and confusion matrix were employed to evaluate how well each model detected TB. Results indicated that SqueezeNet outperformed ResNet-50, achieving a loss of 32%, accuracy of 89%, precision of 98%, recall of 80%, and F1 score of 87%, whereas ResNet-50 recorded a loss of 54%, accuracy of 73%, precision of 88%, recall of 52%, and F1 score of 65%. The findings highlight the promise of machine learning methods for early TB detection and timely treatment initiation. Additionally, integrating such lightweight models into mobile devices could expand TB screening capabilities in underserved regions. Despite these encouraging outcomes, the study underscores the ongoing need to develop faster, smaller, and more accurate TB detection models to aid global TB control efforts. <div>
arXiv:2512.02364v1 Announce Type: new 
Abstract: This study explores the application of machine learning models, specifically a pretrained ResNet-50 model and a general SqueezeNet model, in diagnosing tuberculosis (TB) using chest X-ray images. TB, a persistent infectious disease affecting humanity for millennia, poses challenges in diagnosis, especially in resource-limited settings. Traditional methods, such as sputum smear microscopy and culture, are inefficient, prompting the exploration of advanced technologies like deep learning and computer vision. The study utilized a dataset from Kaggle, consisting of 4,200 chest X-rays, to develop and compare the performance of the two machine learning models. Preprocessing involved data splitting, augmentation, and resizing to enhance training efficiency. Evaluation metrics, including accuracy, precision, recall, and confusion matrix, were employed to assess model performance. Results showcase that the SqueezeNet achieved a loss of 32%, accuracy of 89%, precision of 98%, recall of 80%, and an F1 score of 87%. In contrast, the ResNet-50 model exhibited a loss of 54%, accuracy of 73%, precision of 88%, recall of 52%, and an F1 score of 65%. This study emphasizes the potential of machine learning in TB detection and possible implications for early identification and treatment initiation. The possibility of integrating such models into mobile devices expands their utility in areas lacking TB detection resources. However, despite promising results, the need for continued development of faster, smaller, and more accurate TB detection models remains crucial in contributing to the global efforts in combating TB.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention</title>
<link>https://arxiv.org/abs/2512.02368</link>
<guid>https://arxiv.org/abs/2512.02368</guid>
<content:encoded><![CDATA[
<div> Keywords: trajectory prediction, autonomous driving, Mixture of Experts, selective attention, multimodal decoder<br /><br />Summary:<br /><br />Trajectory prediction is essential for the safety and reliability of autonomous driving but remains challenging in complex interactive environments. The paper identifies that current methods often suffer from inefficiencies due to redundant data, which impacts both computational efficiency and prediction accuracy. To overcome these challenges, the authors introduce a novel map-free trajectory prediction algorithm that operates across temporal, spatial, and frequency domains. For temporal processing, a Mixture of Experts (MoE) mechanism is employed to adaptively select important frequency components, which are then extracted and enriched using multi-scale temporal features. To reduce redundant information, a selective attention module filters data in both temporal sequences and spatial interactions. The system incorporates a multimodal decoder trained under patch-level and point-level loss functions to enhance trajectory prediction quality. Experimental results on the Nuscenes dataset demonstrate the method's superiority, proving its effectiveness in handling complex scenarios involving multiple interacting agents without relying on map data. Overall, the approach advances trajectory prediction by blending frequency-domain analysis, adaptive information selection, and multimodal decoding, resulting in improved accuracy and efficiency in challenging autonomous driving conditions. <div>
arXiv:2512.02368v1 Announce Type: new 
Abstract: Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains</title>
<link>https://arxiv.org/abs/2512.02369</link>
<guid>https://arxiv.org/abs/2512.02369</guid>
<content:encoded><![CDATA[
<div> Keywords: domain generalization, semantic segmentation, style adaptive generalization, privacy constraints, visual prompts<br /><br />Summary:  
1. The paper addresses domain generalization for semantic segmentation, focusing on mitigating performance drops caused by domain shifts when model parameters and architectures remain inaccessible due to privacy and security concerns.  
2. Traditional fine-tuning or adaptation techniques cannot be applied under such constraints, creating a need for input-level strategies that improve generalization without modifying model weights.  
3. The authors propose SAGE (Style-Adaptive Generalization), a framework that enhances the generalization ability of frozen models by synthesizing visual prompts that implicitly align feature distributions across various styles.  
4. SAGE leverages style transfer to generate a diverse set of style representations from the source domain, capturing a wide spectrum of visual features to cover potential domain variations.  
5. The model dynamically fuses these style cues depending on the visual context of each input image, creating adaptive prompts that harmonize image appearance and bridge the gap between model invariance and the diversity of unseen domains.  
6. Experimental evaluation on five benchmark datasets shows that SAGE achieves competitive or superior results compared to state-of-the-art methods under privacy constraints and outperforms models with full fine-tuning across all tested scenarios. <div>
arXiv:2512.02369v1 Announce Type: new 
Abstract: Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \textbf{S}tyle-\textbf{A}daptive \textbf{GE}neralization framework (\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning</title>
<link>https://arxiv.org/abs/2512.02375</link>
<guid>https://arxiv.org/abs/2512.02375</guid>
<content:encoded><![CDATA[
<div> Keywords: real-time UAV photogrammetry, Structure-from-Motion (SfM), online mesh quality assessment, predictive path planning, adaptive exploration

<br /><br />Summary:  
This work addresses the limitations of conventional offline UAV photogrammetry by proposing a real-time, adaptive framework called On-the-fly Feedback SfM for time-sensitive geospatial applications like disaster response and digital-twin maintenance. Unlike existing methods that process images without real-time quality evaluation or guidance, this approach integrates three main modules: (1) an online incremental coarse-mesh generator that dynamically expands a sparse 3D point cloud as the UAV explores; (2) an online mesh quality assessment module providing actionable indicators to evaluate reconstruction quality on the fly; and (3) a predictive path planning system that refines UAV trajectories in real time to improve coverage and reduce gaps. Through iterative exploration and exploitation of known and unknown areas, the system enhances 3D reconstruction accuracy and efficiency. Extensive experiments validate that the method supports near real-time in-situ reconstruction and quality evaluation, significantly reducing coverage gaps and the need for re-flight missions. By combining data collection, processing, 3D reconstruction, assessment, and feedback, the framework shifts traditional UAV photogrammetry from passive operation to an intelligent, adaptive exploration workflow. Open-source code for the project is available at the provided GitHub repository. <div>
arXiv:2512.02375v1 Announce Type: new 
Abstract: Compared with conventional offline UAV photogrammetry, real-time UAV photogrammetry is essential for time-critical geospatial applications such as disaster response and active digital-twin maintenance. However, most existing methods focus on processing captured images or sequential frames in real time, without explicitly evaluating the quality of the on-the-go 3D reconstruction or providing guided feedback to enhance image acquisition in the target area. This work presents On-the-fly Feedback SfM, an explore-and-exploit framework for real-time UAV photogrammetry, enabling iterative exploration of unseen regions and exploitation of already observed and reconstructed areas in near real time. Built upon SfM on-the-fly , the proposed method integrates three modules: (1) online incremental coarse-mesh generation for dynamically expanding sparse 3D point cloud; (2) online mesh quality assessment with actionable indicators; and (3) predictive path planning for on-the-fly trajectory refinement. Comprehensive experiments demonstrate that our method achieves in-situ reconstruction and evaluation in near real time while providing actionable feedback that markedly reduces coverage gaps and re-flight costs. Via the integration of data collection, processing, 3D reconstruction and assessment, and online feedback, our on the-fly feedback SfM could be an alternative for the transition from traditional passive working mode to a more intelligent and adaptive exploration workflow. Code is now available at https://github.com/IRIS-LAB-whu/OntheflySfMFeedback.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2512.02392</link>
<guid>https://arxiv.org/abs/2512.02392</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-object tracking, DETR, feature refinement, contrastive learning, temporal continuity  

<br /><br />Summary:  
This paper addresses the challenge of low association accuracy in end-to-end multi-object tracking (MOT) methods that unify detection and association in a single framework.  
The authors find that the shared DETR architecture produces object embeddings with high inter-object similarity because it focuses only on category-level discrimination within single frames, which is insufficient for tracking that requires instance-level distinction across frames.  
To improve embedding discriminativeness, the authors propose FDTA (From Detection to Association), a feature refinement framework enhancing object embeddings from three complementary aspects: spatial, temporal, and identity levels.  
Specifically, FDTA introduces a Spatial Adapter (SA) to incorporate depth-aware information for maintaining spatial continuity, a Temporal Adapter (TA) to aggregate historical features for capturing temporal dependencies, and an Identity Adapter (IA) that applies quality-aware contrastive learning to improve instance-level separability.  
Extensive experiments on challenging MOT benchmarks such as DanceTrack, SportsMOT, and BFT show that FDTA achieves state-of-the-art performance, validating the effectiveness of its discriminative embedding enhancement strategy. The authors also provide their code publicly to facilitate further research. <div>
arXiv:2512.02392v1 Announce Type: new 
Abstract: End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reproducing and Extending RaDelft 4D Radar with Camera-Assisted Labels</title>
<link>https://arxiv.org/abs/2512.02394</link>
<guid>https://arxiv.org/abs/2512.02394</guid>
<content:encoded><![CDATA[
<div> 4D radar, semantic segmentation, RaDelft dataset, camera-guided labeling, fog conditions<br /><br />Summary:<br /><br />This work addresses the challenge of semantic segmentation for 4D radar data, which is hindered by the lack of open-source datasets and annotated labels. The authors reproduce the results from the RaDelft dataset, identifying limitations such as its reliance on LiDAR annotations and the absence of publicly available code for radar label generation. To overcome this, they propose a novel camera-guided radar labeling pipeline that creates accurate labels for radar point clouds without human annotation. Their method involves projecting radar data into camera-based semantic segmentation outputs combined with spatial clustering to produce enhanced radar labels. This pipeline significantly improves the accuracy and reproducibility of radar label generation, making it accessible for broader research use. Furthermore, the study evaluates how varying fog levels impact the radar labeling performance, providing insights into the robustness of radar perception under adverse weather conditions. Overall, this work establishes a reproducible framework that enriches radar data labeling capabilities and facilitates future developments in radar-based environment perception. <div>
arXiv:2512.02394v1 Announce Type: new 
Abstract: Recent advances in 4D radar highlight its potential for robust environment perception under adverse conditions, yet progress in radar semantic segmentation remains constrained by the scarcity of open source datasets and labels. The RaDelft data set, although seminal, provides only LiDAR annotations and no public code to generate radar labels, limiting reproducibility and downstream research. In this work, we reproduce the numerical results of the RaDelft group and demonstrate that a camera-guided radar labeling pipeline can generate accurate labels for radar point clouds without relying on human annotations. By projecting radar point clouds into camera-based semantic segmentation and applying spatial clustering, we create labels that significantly enhance the accuracy of radar labels. These results establish a reproducible framework that allows the research community to train and evaluate the labeled 4D radar data. In addition, we study and quantify how different fog levels affect the radar labeling performance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch</title>
<link>https://arxiv.org/abs/2512.02395</link>
<guid>https://arxiv.org/abs/2512.02395</guid>
<content:encoded><![CDATA[
<div> Keywords: Skywork-R1V4, multimodal agentic model, image manipulation, supervised fine-tuning, multimodal search<br /><br />Summary: Skywork-R1V4 is a 30 billion parameter multimodal agentic model designed to unify image manipulation, web search, and multimodal planning into a single framework. Unlike previous systems that treat these capabilities separately and depend heavily on reinforcement learning, Skywork-R1V4 relies solely on supervised fine-tuning of fewer than 30,000 high-quality trajectories aligned with planning and execution consistency. The model dynamically interleaves reasoning between visual operations and external knowledge retrieval, supporting active image manipulation, deep multimodal search, and complex planning. It demonstrates emergent long-horizon reasoning by successfully orchestrating over 10 tool calls to address intricate multi-step tasks. Skywork-R1V4 achieves state-of-the-art performance, scoring 66.1 on the MMSearch benchmark and 67.2 on FVQA, outperforming prior systems like Gemini 2.5 Flash across 11 metrics. The approach highlights that sophisticated agentic multimodal intelligence can be developed without reinforcement learning, relying instead on carefully curated supervised datasets and stepwise consistency filtering for validation. Overall, the work showcases a novel pathway for building advanced multimodal AI agents capable of integrated reasoning, planning, and tool use within a unified model architecture. <div>
arXiv:2512.02395v1 Announce Type: new 
Abstract: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation ("thinking with images"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation</title>
<link>https://arxiv.org/abs/2512.02400</link>
<guid>https://arxiv.org/abs/2512.02400</guid>
<content:encoded><![CDATA[
<div> Keywords: object-goal navigation, open-vocabulary, Chain-of-Thought reasoning, Similarity-Aware Memory, unseen objects<br /><br />Summary:<br /><br />1. This paper addresses the challenge of object-goal navigation in environments where agents need to locate novel, unseen objects without prior category knowledge, a task complicated by opaque decision-making and low success rates in existing methods.<br />2. The authors introduce Nav-$R^2$, a novel framework that explicitly models two key relationships: target-environment modeling and environment-action planning, by leveraging structured Chain-of-Thought (CoT) reasoning.<br />3. They develop a Nav$R^2$-CoT dataset designed to train the model to perceive the environment effectively, identify target-related objects within its context, and plan future actions accordingly.<br />4. A critical component of Nav-$R^2$ is the Similarity-Aware Memory (SA-Mem), which compresses video frames and fuses historical observations to preserve the most relevant features both temporally and semantically, enhancing memory without adding extra parameters.<br />5. Experimental results demonstrate that Nav-$R^2$ achieves state-of-the-art performance in locating unseen objects, maintains real-time inference at 2Hz, avoids overfitting to known categories, and streamlines the navigation pipeline.<br />6. The resources and code for Nav-$R^2$ will be publicly available on GitHub to facilitate further research and development in this area. <div>
arXiv:2512.02400v1 Announce Type: new 
Abstract: Object-goal navigation in open-vocabulary settings requires agents to locate novel objects in unseen environments, yet existing approaches suffer from opaque decision-making processes and low success rate on locating unseen objects. To address these challenges, we propose Nav-$R^2$, a framework that explicitly models two critical types of relationships, target-environment modeling and environment-action planning, through structured Chain-of-Thought (CoT) reasoning coupled with a Similarity-Aware Memory. We construct a Nav$R^2$-CoT dataset that teaches the model to perceive the environment, focus on target-related objects in the surrounding context and finally make future action plans. Our SA-Mem preserves the most target-relevant and current observation-relevant features from both temporal and semantic perspectives by compressing video frames and fusing historical observations, while introducing no additional parameters. Compared to previous methods, Nav-R^2 achieves state-of-the-art performance in localizing unseen objects through a streamlined and efficient pipeline, avoiding overfitting to seen object categories while maintaining real-time inference at 2Hz. Resources will be made publicly available at \href{https://github.com/AMAP-EAI/Nav-R2}{github link}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2512.02405</link>
<guid>https://arxiv.org/abs/2512.02405</guid>
<content:encoded><![CDATA[
<div> Multi-agent debate, multimodal reasoning, Weighted Iterative Society-of-Experts, Dawid-Skene algorithm, vision-and-language tasks<br /><br />Summary:<br /><br />This paper addresses the challenge of applying multi-agent debate (MAD) frameworks to multimodal vision-and-language reasoning tasks, an area that has been less explored compared to language-only problems. The authors introduce Weighted Iterative Society-of-Experts (WISE), a modular and generalized MAD system that organizes agents into two roles: Solvers, who propose solutions, and Reflectors, who verify these solutions, assign weights, and offer natural language feedback. The framework supports heterogeneous agents with both single- and multi-modal capabilities, allowing a nuanced debate process. To effectively combine multiple agents’ solutions across debate rounds and account for variability in responses and feedback, the authors adapt the Dawid-Skene algorithm in a two-stage post-processing method tailored to their debate model. Experiments are conducted on several vision-and-language datasets, including SMART-840, VisualPuzzles, EvoChart-QA, and a novel SMART-840++ dataset designed with programmatically generated problems of varying difficulty. Results demonstrate that WISE consistently improves accuracy by 2 to 7% compared to existing state-of-the-art MAD approaches and aggregation strategies, highlighting its effectiveness for complex multimodal reasoning tasks and various large language model configurations. <div>
arXiv:2512.02405v1 Announce Type: new 
Abstract: Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MitUNet: Enhancing Floor Plan Recognition using a Hybrid Mix-Transformer and U-Net Architecture</title>
<link>https://arxiv.org/abs/2512.02413</link>
<guid>https://arxiv.org/abs/2512.02413</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D reconstruction, semantic segmentation, Mix-Transformer, U-Net, Tversky loss<br /><br />Summary:<br />1. The paper addresses the challenge of accurately segmenting walls from 2D floor plans to enable precise 3D reconstruction of indoor spaces.<br />2. Existing segmentation techniques often fail to detect thin structural elements and produce irregular mask boundaries, which negatively impacts vectorization.<br />3. The authors propose MitUNet, a hybrid neural network that combines a hierarchical Mix-Transformer encoder for capturing global context with a U-Net decoder enhanced by scSE attention blocks to improve boundary precision.<br />4. They introduce an optimization strategy using the Tversky loss function, which balances precision and recall by prioritizing the suppression of false positives along wall boundaries while still detecting thin structures.<br />5. Experimental results on the CubiCasa5k dataset and a proprietary regional dataset show that MitUNet outperforms standard single-task models, producing structurally correct masks with improved boundary accuracy.<br />6. MitUNet serves as a robust tool for preparing data in automated 3D reconstruction pipelines, enhancing the quality and reliability of 3D indoor models derived from 2D floor plans. <div>
arXiv:2512.02413v1 Announce Type: new 
Abstract: Automatic 3D reconstruction of indoor spaces from 2D floor plans requires high-precision semantic segmentation of structural elements, particularly walls. However, existing methods optimized for standard metrics often struggle to detect thin structural components and yield masks with irregular boundaries, lacking the geometric precision required for subsequent vectorization. To address this issue, we introduce MitUNet, a hybrid neural network architecture specifically designed for wall segmentation tasks in the context of 3D modeling. In MitUNet, we utilize a hierarchical Mix-Transformer encoder to capture global context and a U-Net decoder enhanced with scSE attention blocks for precise boundary recovery. Furthermore, we propose an optimization strategy based on the Tversky loss function to effectively balance precision and recall. By fine-tuning the hyperparameters of the loss function, we prioritize the suppression of false positive noise along wall boundaries while maintaining high sensitivity to thin structures. Our experiments on the public CubiCasa5k dataset and a proprietary regional dataset demonstrate that the proposed approach ensures the generation of structurally correct masks with high boundary accuracy, outperforming standard single-task models. MitUNet provides a robust tool for data preparation in automated 3D reconstruction pipelines.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Vision-Language Models with Dedicated Prompt Guidance</title>
<link>https://arxiv.org/abs/2512.02421</link>
<guid>https://arxiv.org/abs/2512.02421</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, domain generalization, prompt tuning, expert models, cross-modal attention<br /><br />Summary:<br /><br />This paper addresses the challenge of balancing domain specificity and domain generalization (DG) when fine-tuning large pretrained vision-language models (VLMs). The authors reveal through theoretical analysis that training multiple parameter-efficient expert models on partitioned source domains improves DG performance compared to fine-tuning a single universal model. Building on this insight, they propose a novel two-step framework called domain-expert-Guided DG (GuiDG). First, GuiDG uses prompt tuning to create specialized source domain experts. Next, it applies a Cross-Modal Attention module to adaptively integrate these experts during the fine-tuning of the vision encoder, enhancing model adaptation. To better evaluate few-shot domain generalization, the authors introduce the ImageNet-DG dataset, which comprises ImageNet and its variants. Experiments conducted on standard DG benchmarks and the newly constructed ImageNet-DG demonstrate that GuiDG outperforms current state-of-the-art fine-tuning methods. Additionally, it achieves this improvement while maintaining parameter efficiency, illustrating an effective trade-off between model specialization and generalization. Overall, GuiDG offers a theoretically grounded and practically effective solution for improving domain generalization in vision-language model fine-tuning. <div>
arXiv:2512.02421v1 Announce Type: new 
Abstract: Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02423</link>
<guid>https://arxiv.org/abs/2512.02423</guid>
<content:encoded><![CDATA[
<div> GUI, reinforcement learning, navigation, simulation environment, agent training<br /><br />Summary:<br /><br />With the advancement of Large Vision Language Models, GUI agent tasks have evolved from handling single-screen tasks to managing complex screen navigation challenges. Real-world GUI environments like PC software and mobile apps are often complex and proprietary, limiting access to comprehensive environment information necessary for agent training and evaluation. To overcome this, the authors present GUI Exploration Lab, a simulation environment engine that allows flexible creation and composition of screens, icons, and navigation graphs, providing full environment access for thorough training and benchmarking. Experiments show that supervised fine-tuning effectively helps the agent memorize fundamental knowledge, acting as a base for further learning. Building on this foundation, single-turn reinforcement learning improves the agent's ability to generalize to unseen scenarios. Multi-turn reinforcement learning further enhances performance by encouraging agents to develop exploration strategies through interactive trial and error. The proposed methods are validated on both static and interactive benchmarks, demonstrating that the approach generalizes well to real-world scenarios. Overall, the study highlights the benefits of reinforcement learning in GUI navigation and offers guidance for developing more capable and generalizable GUI agents. <div>
arXiv:2512.02423v1 Announce Type: new 
Abstract: With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning</title>
<link>https://arxiv.org/abs/2512.02425</link>
<guid>https://arxiv.org/abs/2512.02425</guid>
<content:encoded><![CDATA[
<div> Keywords: WorldMM, multimodal memory, long video reasoning, adaptive retrieval, video question answering<br /><br />Summary: Recent video large language models excel at understanding short clips but struggle with hours- or days-long videos due to limited context capacity and loss of vital visual details when abstracting information. Traditional memory-augmented approaches depend on textual summaries, which overlook critical visual evidence, especially in complex scenes, and their fixed temporal retrieval scales lack flexibility for events of varying duration. To overcome these issues, WorldMM is introduced as a novel multimodal memory agent that combines textual and visual representations across multiple complementary memory types. Specifically, WorldMM includes episodic memory for indexing factual events at multiple temporal scales, semantic memory for continual conceptual knowledge updates, and visual memory for retaining detailed scene information. The system features an adaptive retrieval agent that iteratively selects the most relevant memory source and temporal granularity according to the query until sufficient information is gathered. Experimental results show that WorldMM substantially surpasses existing methods on five long video question-answering benchmarks, achieving an average 8.4% performance improvement, highlighting its effectiveness in reasoning over long videos. <div>
arXiv:2512.02425v1 Announce Type: new 
Abstract: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightHCG: a Lightweight yet powerful HSIC Disentanglement based Causal Glaucoma Detection Model framework</title>
<link>https://arxiv.org/abs/2512.02437</link>
<guid>https://arxiv.org/abs/2512.02437</guid>
<content:encoded><![CDATA[
<div> Keywords: glaucoma detection, causal representation, LightHCG, convolutional VAE, AI intervention analysis

<br /><br />Summary:  
Glaucoma is a serious optic degenerative disease causing irreversible vision loss primarily due to optic nerve damage from high intraocular pressure or retinal neovascularization. Traditional diagnosis methods rely on perimetry tests, optic papilla inspections, and tonometer-based IOP measurements to identify optic nerve damage. Recent advances in AI utilizing computer vision models like VGG16 and Vision Transformers have enhanced glaucoma detection and optic cup segmentation from retinal fundus and OCT images, aiding diagnosis with improved accuracy. However, current AI approaches face challenges such as excessive parameters, potential spurious correlations, limited reliability, and inadequate applications in clinical intervention analysis. To overcome these, the study introduces LightHCG, a novel and extremely lightweight glaucoma detection model based on a convolutional variational autoencoder (VAE) designed to learn causal representations of glaucoma-related factors in the optic nerve region. LightHCG employs HSIC-based latent space disentanglement and unsupervised causal representation learning through a Graph Autoencoder, effectively capturing true causality. This model achieves glaucoma classification performance comparable to or better than advanced models like InceptionV3, MobileNetV2, and VGG16, while reducing model size by 93–99%. Additionally, LightHCG enhances the potential for AI-driven clinical intervention analysis and simulation, providing a promising direction for more reliable and interpretable glaucoma diagnostics using AI. <div>
arXiv:2512.02437v1 Announce Type: new 
Abstract: As a representative optic degenerative condition, glaucoma has been a threat to millions due to its irreversibility and severe impact on human vision fields. Mainly characterized by dimmed and blurred visions, or peripheral vision loss, glaucoma is well known to occur due to damages in the optic nerve from increased intraocular pressure (IOP) or neovascularization within the retina. Traditionally, most glaucoma related works and clinical diagnosis focused on detecting these damages in the optic nerve by using patient data from perimetry tests, optic papilla inspections and tonometer-based IOP measurements. Recently, with advancements in computer vision AI models, such as VGG16 or Vision Transformers (ViT), AI-automatized glaucoma detection and optic cup segmentation based on retinal fundus images or OCT recently exhibited significant performance in aiding conventional diagnosis with high performance. However, current AI-driven glaucoma detection approaches still have significant room for improvement in terms of reliability, excessive parameter usage, possibility of spurious correlation within detection, and limitations in applications to intervention analysis or clinical simulations. Thus, this research introduced a novel causal representation driven glaucoma detection model: LightHCG, an extremely lightweight Convolutional VAE-based latent glaucoma representation model that can consider the true causality among glaucoma-related physical factors within the optic nerve region. Using HSIC-based latent space disentanglement and Graph Autoencoder based unsupervised causal representation learning, LightHCG not only exhibits higher performance in classifying glaucoma with 93~99% less weights, but also enhances the possibility of AI-driven intervention analysis, compared to existing advanced vision models such as InceptionV3, MobileNetV2 or VGG16.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources</title>
<link>https://arxiv.org/abs/2512.02438</link>
<guid>https://arxiv.org/abs/2512.02438</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, contrastive learning, momentum self-distillation, few-shot learning, computational efficiency

<br /><br />Summary:  
This paper addresses challenges in training Vision-Language Models (VLMs) for medical healthcare, where obtaining detailed annotations is difficult and data is limited. The authors highlight the significance of contrastive learning (CL) for VLM training but note its reliance on large batch sizes, which demands significant computational resources often unavailable to many institutions. To overcome these constraints, the study proposes leveraging momentum-based self-distillation combined with gradient accumulation. First, momentum self-distillation is employed to enhance multimodal learning by extracting more knowledge during training. Second, the integration of momentum mechanisms with gradient accumulation effectively enlarges the batch size without increasing hardware or memory consumption. Empirical results demonstrate that the method achieves competitive zero-shot classification performance comparable to state-of-the-art approaches while substantially improving few-shot adaptation, reaching over 90% AUC-ROC and enhancing retrieval tasks by 2-3%. Importantly, the training process is efficient enough to run on a single GPU with reasonable training times, making it accessible for institutions with limited resources. Overall, the proposed approach advances efficient multimodal learning by balancing improved performance and reduced resource requirements. The implementation is publicly available at their GitHub repository. <div>
arXiv:2512.02438v1 Announce Type: new 
Abstract: In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2512.02441</link>
<guid>https://arxiv.org/abs/2512.02441</guid>
<content:encoded><![CDATA[
<div> Keywords: meta-learning, transfer learning, parameter-efficient fine-tuning, orthogonal basis, low-rank adaptation  

<br /><br />Summary:  
This paper addresses the challenge of adapting large pre-trained models to new, unseen tasks under constraints of limited data and computational resources. Existing meta-learning methods are effective but costly and unstable due to the need for extensive meta-training over multiple tasks. In contrast, the authors propose BOLT (Basis-Oriented Low-rank Transfer), which leverages a collection of already fine-tuned models by extracting an orthogonal spectral basis informed by tasks rather than merging weights. In an offline phase, dominant singular vectors from different task-specific models are collected and orthogonalized per layer to form reusable bases. During the online phase, these bases are frozen and only a small number of diagonal coefficients per layer are trained for the new task, yielding a rank-controlled and parameter-efficient adaptation. BOLT provides a strong, training-free initialization for unseen tasks by pooling source-task coefficients with a lightweight rescaling step, which leverages the shared orthogonal bases to facilitate transfer. Experimentally, BOLT achieves competitive performance compared to common parameter-efficient fine-tuning baselines and a meta-learned initialization, demonstrating that constraining adaptation within a task-informed orthogonal subspace offers an effective alternative for unseen-task transfer. <div>
arXiv:2512.02441v1 Announce Type: new 
Abstract: Adapting large pre-trained models to unseen tasks under tight data and compute budgets remains challenging. Meta-learning approaches explicitly learn good initializations, but they require an additional meta-training phase over many tasks, incur high training cost, and can be unstable. At the same time, the number of task-specific pre-trained models continues to grow, yet the question of how to transfer them to new tasks with minimal additional training remains relatively underexplored. We propose BOLT (Basis-Oriented Low-rank Transfer), a framework that reuses existing fine-tuned models not by merging weights, but instead by extracting an orthogonal, task-informed spectral basis and adapting within that subspace. In the offline phase, BOLT collects dominant singular directions from multiple task vectors and orthogonalizes them per layer to form reusable bases. In the online phase, we freeze these bases and train only a small set of diagonal coefficients per layer for the new task, yielding a rank-controlled update with very few trainable parameters. This design provides (i) a strong, training-free initialization for unseen tasks, obtained by pooling source-task coefficients, along with a lightweight rescaling step while leveraging the shared orthogonal bases, and (ii) a parameter-efficient fine-tuning (PEFT) path that, in our experiments, achieves robust performance compared to common PEFT baselines as well as a representative meta-learned initialization. Our results show that constraining adaptation to a task-informed orthogonal subspace provides an effective alternative for unseen-task transfer.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors</title>
<link>https://arxiv.org/abs/2512.02447</link>
<guid>https://arxiv.org/abs/2512.02447</guid>
<content:encoded><![CDATA[
<div> Spiking Neural Networks, Temporal Dynamics Enhancer, Spike-Driven Attention, Object Detection, Energy Efficiency  

<br /><br />Summary:  
Spiking Neural Networks (SNNs) are recognized for their brain-inspired temporal dynamics and energy-efficient spike-based computations, making them promising alternatives to traditional Artificial Neural Networks (ANNs). However, conventional SNNs typically process inputs either by direct replication or fixed-interval aggregation, resulting in neurons receiving repetitive stimuli over time and thereby limiting the network's ability to capture complex temporal features needed for tasks like object detection. To address this, the authors introduce the Temporal Dynamics Enhancer (TDE), which enhances temporal information modeling in SNNs. TDE incorporates two key components: a Spiking Encoder (SE) that produces varied input stimuli across time steps, and an Attention Gating Module (AGM) that leverages inter-temporal dependencies to regulate SE output. To combat the increased energy demands caused by AGM’s multiplication operations, a Spike-Driven Attention (SDA) mechanism is proposed, significantly lowering attention-related energy consumption. Experimental evaluations demonstrate that integrating TDE into existing SNN-based detectors consistently boosts performance, achieving mean Average Precision (mAP50-95) scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. Additionally, the SDA module operates with only 24% of the energy consumption typical of conventional attention modules, highlighting its efficiency. <div>
arXiv:2512.02447v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nuScenes Revisited: Progress and Challenges in Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.02448</link>
<guid>https://arxiv.org/abs/2512.02448</guid>
<content:encoded><![CDATA[
<div> Keywords: nuScenes, autonomous vehicles, dataset, multi-modal sensor fusion, deep learning<br /><br />Summary:<br /><br />1. The paper revisits the nuScenes dataset, a pioneering resource in autonomous vehicle (AV) research that includes radar data, diverse urban scenes from multiple continents, and data collected from fully autonomous vehicles on public roads.<br /><br />2. nuScenes supports multi-modal sensor fusion by integrating data from cameras, LiDAR, radar, and other sensors, enabling comprehensive AV perception and decision-making tasks.<br /><br />3. The authors reveal extensive technical details on the creation of nuScenes and its extensions—nuImages and Panoptic nuScenes—that had not been published before, providing valuable insights into dataset construction and annotation.<br /><br />4. The influence of nuScenes on the autonomous driving community is traced, showing how it set new standards and inspired many subsequent datasets and benchmarks used for tasks like perception, localization, mapping, prediction, and planning.<br /><br />5. The paper also surveys a wide array of research tasks, both official and unofficial, that leverage nuScenes, summarizing major methodological advances and offering a comprehensive overview of the literature centered around this influential dataset. <div>
arXiv:2512.02448v1 Announce Type: new 
Abstract: Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) have been revolutionized by Deep Learning. As a data-driven approach, Deep Learning relies on vast amounts of driving data, typically labeled in great detail. As a result, datasets, alongside hardware and algorithms, are foundational building blocks for the development of AVs. In this work we revisit one of the most widely used autonomous driving datasets: the nuScenes dataset. nuScenes exemplifies key trends in AV development, being the first dataset to include radar data, to feature diverse urban driving scenes from two continents, and to be collected using a fully autonomous vehicle operating on public roads, while also promoting multi-modal sensor fusion, standardized benchmarks, and a broad range of tasks including perception, localization \& mapping, prediction and planning. We provide an unprecedented look into the creation of nuScenes, as well as its extensions nuImages and Panoptic nuScenes, summarizing many technical details that have hitherto not been revealed in academic publications. Furthermore, we trace how the influence of nuScenes impacted a large number of other datasets that were released later and how it defined numerous standards that are used by the community to this day. Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild</title>
<link>https://arxiv.org/abs/2512.02450</link>
<guid>https://arxiv.org/abs/2512.02450</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D layout estimation, multi-floor buildings, HouseLayout3D, MultiFloor3D, benchmark dataset<br /><br />Summary:<br />1. Existing 3D layout estimation models typically train on synthetic datasets that feature simple environments such as single rooms or single floors, limiting their applicability to larger, more complex structures.<br />2. These models require scenes to be divided into individual floors prior to processing, which causes a loss of global spatial context necessary for understanding multi-floor connections like staircases.<br />3. To address these limitations, the authors introduce HouseLayout3D, a real-world benchmark dataset designed specifically for full building scale layout estimation including multiple floors and architecturally complex spaces.<br />4. The paper also presents MultiFloor3D, a training-free baseline method that builds upon recent advances in scene understanding and effectively outperforms current 3D layout estimation models on both the new HouseLayout3D benchmark and existing datasets.<br />5. The availability of data and code at https://houselayout3d.github.io aims to facilitate further research and development toward comprehensive multi-floor 3D layout estimation models that retain global spatial context. <div>
arXiv:2512.02450v1 Announce Type: new 
Abstract: Current 3D layout estimation models are primarily trained on synthetic datasets containing simple single room or single floor environments. As a consequence, they cannot natively handle large multi floor buildings and require scenes to be split into individual floors before processing, which removes global spatial context that is essential for reasoning about structures such as staircases that connect multiple levels. In this work, we introduce HouseLayout3D, a real world benchmark designed to support progress toward full building scale layout estimation, including multiple floors and architecturally intricate spaces. We also present MultiFloor3D, a simple training free baseline that leverages recent scene understanding methods and already outperforms existing 3D layout estimation models on both our benchmark and prior datasets, highlighting the need for further research in this direction. Data and code are available at: https://houselayout3d.github.io.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation</title>
<link>https://arxiv.org/abs/2512.02453</link>
<guid>https://arxiv.org/abs/2512.02453</guid>
<content:encoded><![CDATA[
<div> Stylized motion generation, intra-style diversity, clustering framework, style embedding, motion style transfer<br /><br />Summary:<br /><br />This paper addresses the challenge of capturing intra-style diversity in stylized motion generation, where motions of a single style exhibit diverse variations. Existing models typically extract an unstructured embedding for each style motion, but this approach limits diversity representation. To overcome this, the authors propose ClusterStyle, a clustering-based framework that uses multiple prototypes to represent diverse style patterns within the same style category. They identify two types of style diversity: global-level diversity, which exists among different style motions of the same category, and local-level diversity, which manifests within the temporal dynamics of motion sequences. ClusterStyle jointly structures two embedding spaces—global and local—through alignment with fixed prototype anchors, which are non-learnable. Additionally, the paper introduces the Stylistic Modulation Adapter (SMA), which integrates these enriched style features into a pretrained text-to-motion generation model, enhancing its style control capability. Extensive experiments demonstrate that the proposed method surpasses current state-of-the-art approaches in both stylized motion generation and motion style transfer, indicating its effectiveness in producing diverse and high-quality stylistic motions consistent with specified styles. <div>
arXiv:2512.02453v1 Announce Type: new 
Abstract: Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion. However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge. In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation. Instead of learning an unstructured embedding from each style motion, we leverage a set of prototypes to effectively model diverse style patterns across motions belonging to the same style category. We consider two types of style diversity: global-level diversity among style motions of the same category, and local-level diversity within the temporal dynamics of motion sequences. These components jointly shape two structured style embedding spaces, i.e., global and local, optimized via alignment with non-learnable prototype anchors. Furthermore, we augment the pretrained text-to-motion generation model with the Stylistic Modulation Adapter (SMA) to integrate the style features. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art models in stylized motion generation and motion style transfer.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See, Think, Learn: A Self-Taught Multimodal Reasoner</title>
<link>https://arxiv.org/abs/2512.02456</link>
<guid>https://arxiv.org/abs/2512.02456</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, multimodal reasoning, self-training, structured rationales, negative rationales

<br /><br />Summary: The paper presents See-Think-Learn (STL), a self-training framework designed to improve multimodal reasoning in Vision-Language Models (VLMs) by jointly enhancing their perception and reasoning capabilities. STL introduces a structured reasoning template that encourages the model to first "see" by extracting visual attributes into textual form before "thinking" or reasoning over these attributes. Unlike previous methods reliant on expensive human annotations or proprietary models for Chain-of-Thought reasoning data, STL enables the model to generate and learn from its own structured rationales in a self-training loop. Additionally, the framework incorporates negative rationales, explanations for why certain answer choices are incorrect, which help the model better distinguish between correct and misleading options, fostering more robust learning. Experimental results across various domains indicate that STL consistently outperforms baseline models trained solely on answers or self-generated reasoning. Qualitative analysis further confirms the high quality and effectiveness of the generated rationales. Ultimately, STL offers a cost-effective and scalable solution to enhance the multimodal reasoning ability of VLMs by combining improved visual perception with structured reasoning and discriminative learning through positive and negative explanations. <div>
arXiv:2512.02456v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation</title>
<link>https://arxiv.org/abs/2512.02457</link>
<guid>https://arxiv.org/abs/2512.02457</guid>
<content:encoded><![CDATA[
<div> audio-video generation, joint denoising, video quality, cross-modal co-training, AVFullDiT<br /><br />Summary:<br /><br />1. This paper investigates whether training audio and video modalities together through joint denoising improves video generation quality, even when only video output is of interest. 2. The authors introduce AVFullDiT, a parameter-efficient architecture that combines pre-trained text-to-video (T2V) and text-to-audio (T2A) models to perform audio-video joint denoising. 3. They conduct experiments comparing a T2AV model using AVFullDiT to a T2V-only baseline under identical conditions. 4. Results show that joint audio-video denoising enhances video generation beyond improving mere audio-video synchrony, particularly on challenging video subsets with large motions and object contact interactions. 5. The study hypothesizes that predicting audio serves as a privileged signal, helping the model learn causal links between visual events and their sounds (e.g., collisions producing impact noises), which improves temporal coherence and physical realism in videos. 6. The findings support cross-modal co-training as a promising direction for building stronger, more physically grounded generative world models. 7. The authors plan to publicly release their code and dataset to facilitate further research in this area. <div>
arXiv:2512.02457v1 Announce Type: new 
Abstract: Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration</title>
<link>https://arxiv.org/abs/2512.02458</link>
<guid>https://arxiv.org/abs/2512.02458</guid>
<content:encoded><![CDATA[
<div> Keywords: embodied AI, sequential tasks, spatial memory, multi-modal large language models, embodied exploration<br /><br />Summary:<br /><br />1. This work addresses the challenge of performing sequential indoor embodied tasks, where an agent must complete a sequence of sub-tasks, some of which may be infeasible, such as searching for objects that do not exist. 2. The key difficulty compared to single-task settings is how to effectively reuse spatial knowledge accumulated from previous explorations to support ongoing reasoning and exploration. 3. To study this practically important issue, the authors introduce SEER-Bench, a new benchmark that integrates two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN), designed for sequential task evaluation. 4. Building on SEER-Bench, they propose 3DSPMR, a 3D Spatial Memory Reasoning method that leverages relational, visual, and geometric information from explored environments to augment Multi-Modal Large Language Models (MLLMs) for improved reasoning and exploration in sequential embodied tasks. 5. This approach is pioneering in explicitly incorporating geometric information into MLLM-based spatial understanding, and experimental results demonstrate that 3DSPMR significantly improves performance on sequential EQA and EMN tasks. <div>
arXiv:2512.02458v1 Announce Type: new 
Abstract: Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution</title>
<link>https://arxiv.org/abs/2512.02469</link>
<guid>https://arxiv.org/abs/2512.02469</guid>
<content:encoded><![CDATA[
<div> dataset distillation, distribution matching, feature evolution, trajectory alignment, semantic diversity<br /><br />Summary:<br /><br />1. Dataset distillation aims to compress large datasets into smaller, synthetic ones to save storage and computational costs, with distribution matching (DM)-based methods being prominent for their efficiency.<br /><br />2. Existing DM methods often neglect the dynamic evolution of feature representations during model training, which reduces the quality and expressiveness of the distilled synthetic data and weakens performance on downstream tasks.<br /><br />3. The proposed Trajectory Guided Dataset Distillation (TGDD) addresses this limitation by reformulating distribution matching as a dynamic alignment process along the model’s training trajectory, capturing evolving semantic information at each training stage.<br /><br />4. TGDD incorporates a distribution constraint regularization to reduce class overlap, ensuring that synthetic data maintains both semantic diversity and representativeness.<br /><br />5. This approach achieves an improved trade-off between performance and efficiency without adding optimization overhead, demonstrating state-of-the-art results on ten datasets, including a notable 5.0% accuracy improvement on high-resolution benchmarks. <div>
arXiv:2512.02469v1 Announce Type: new 
Abstract: Dataset distillation compresses large datasets into compact synthetic ones to reduce storage and computational costs. Among various approaches, distribution matching (DM)-based methods have attracted attention for their high efficiency. However, they often overlook the evolution of feature representations during training, which limits the expressiveness of synthetic data and weakens downstream performance. To address this issue, we propose Trajectory Guided Dataset Distillation (TGDD), which reformulates distribution matching as a dynamic alignment process along the model's training trajectory. At each training stage, TGDD captures evolving semantics by aligning the feature distribution between the synthetic and original dataset. Meanwhile, it introduces a distribution constraint regularization to reduce class overlap. This design helps synthetic data preserve both semantic diversity and representativeness, improving performance in downstream tasks. Without additional optimization overhead, TGDD achieves a favorable balance between performance and efficiency. Experiments on ten datasets demonstrate that TGDD achieves state-of-the-art performance, notably a 5.0% accuracy gain on high-resolution benchmarks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling</title>
<link>https://arxiv.org/abs/2512.02473</link>
<guid>https://arxiv.org/abs/2512.02473</guid>
<content:encoded><![CDATA[
<div> Keywords: video world models, compressed memory, long-term generation, spatial consistency, LoopNav<br /><br />Summary:<br /><br />1. This paper addresses the challenge of achieving temporally and spatially consistent long-term video world modeling, a problem made difficult by the high computational costs associated with long-context inputs in current models. <br />2. The authors propose WorldPack, a novel video world model that employs an efficient compressed memory system to overcome these limitations by using significantly shorter context lengths without sacrificing generation quality. <br />3. WorldPack's compressed memory consists of two main components: trajectory packing, which enhances context efficiency by compressing past trajectories, and memory retrieval, which maintains spatial consistency and supports long-term generation involving spatial reasoning. <br />4. The model was evaluated on LoopNav, a specialized benchmark based on the Minecraft environment designed to test long-term consistency in video generation. <br />5. Experimental results demonstrate that WorldPack substantially outperforms existing state-of-the-art methods in terms of spatial consistency, visual fidelity, and overall quality for long-term video generation tasks. <div>
arXiv:2512.02473v1 Announce Type: new 
Abstract: Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline</title>
<link>https://arxiv.org/abs/2512.02482</link>
<guid>https://arxiv.org/abs/2512.02482</guid>
<content:encoded><![CDATA[
<div> Keywords: G-SHARP, real-time surgical reconstruction, Gaussian splatting, deformable tissue modeling, EndoNeRF benchmark<br /><br />Summary:<br /><br />1. The article introduces G-SHARP, a surgical scene reconstruction framework designed to provide fast and accurate 3D modeling of deformable tissue during minimally invasive procedures.<br />2. G-SHARP addresses limitations of prior Gaussian splatting methods by offering a commercially compatible, real-time pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer.<br />3. The framework supports principled deformation modeling and robust occlusion handling, which enhances the quality and reliability of reconstructions.<br />4. The authors demonstrate that G-SHARP achieves state-of-the-art reconstruction performance on the EndoNeRF pulling benchmark, balancing speed and accuracy for intra-operative suitability.<br />5. To facilitate practical deployment, the article presents a Holoscan SDK application that runs G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in clinical operating-room environments. <div>
arXiv:2512.02482v1 Announce Type: new 
Abstract: We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making</title>
<link>https://arxiv.org/abs/2512.02485</link>
<guid>https://arxiv.org/abs/2512.02485</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, medical diagnosis, reasoning detachment, multi-agent framework, visual evidence<br /><br />Summary:<br /><br />This paper addresses the challenge of reasoning detachment in Vision-Language Models (VLMs) used for medical diagnosis, where explanations may deviate from actual image evidence, reducing clinical trust. Existing multi-agent frameworks simulate team discussions to reduce bias but often increase textual noise and computational costs without sufficiently anchoring reasoning to visual data. The authors propose UCAgents, a hierarchical multi-agent framework inspired by clinical workflows that enforces unidirectional convergence and structured evidence auditing. UCAgents restricts agent interactions to focused evidence verification and prevents position changes, thereby suppressing rhetorical drift while enhancing the extraction of visual signals. A one-round inquiry discussion is introduced to identify potential misalignments between the visual content and textual explanations. This design jointly addresses the dual-noise bottleneck—visual ambiguity and textual noise—formalized through information theory. Experiments on four medical Visual Question Answering benchmarks demonstrate that UCAgents achieves superior accuracy (71.3% on PathVQA, a 6.0% improvement over state-of-the-art) with an 87.7% reduction in token cost. The results confirm that UCAgents balances thorough evidence discovery and minimizes confusing textual interference, exhibiting both diagnostic reliability and computational efficiency important for real-world clinical applications. The code is publicly available on GitHub. <div>
arXiv:2512.02485v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding</title>
<link>https://arxiv.org/abs/2512.02487</link>
<guid>https://arxiv.org/abs/2512.02487</guid>
<content:encoded><![CDATA[
<div> 3D scene-language understanding, Large Language Models, attention mask, spatial structure, multi-modal reasoning<br /><br />Summary:<br /><br />1. The paper addresses challenges in 3D scene-language understanding where Large Language Models (LLMs) are used for reasoning in 3D multi-modal contexts. 2. Existing approaches utilize standard language modeling decoders with causal attention masks, which introduce conflicts such as sequential bias among order-agnostic 3D objects and limited attention between object tokens and instruction context. 3. To resolve these issues, the authors propose 3D Spatial Language Instruction Mask (3D-SLIM), a novel masking strategy designed specifically for 3D scene spatial structures. 4. 3D-SLIM consists of two main components: a Geometry-adaptive Mask that defines attention based on spatial density instead of token order, and an Instruction-aware Mask that allows direct attention between object tokens and instruction context. 5. This approach requires no architectural changes or additional parameters but significantly improves performance on various 3D scene-language tasks, demonstrating the important role of decoder design in enhancing 3D multi-modal reasoning capabilities. <div>
arXiv:2512.02487v1 Announce Type: new 
Abstract: Recent advances in 3D scene-language understanding have leveraged Large Language Models (LLMs) for 3D reasoning by transferring their general reasoning ability to 3D multi-modal contexts. However, existing methods typically adopt standard decoders from language modeling, which rely on a causal attention mask. This design introduces two fundamental conflicts in 3D scene understanding: sequential bias among order-agnostic 3D objects and restricted object-instruction attention, hindering task-specific reasoning. To overcome these limitations, we propose 3D Spatial Language Instruction Mask (3D-SLIM), an effective masking strategy that replaces the causal mask with an adaptive attention mask tailored to the spatial structure of 3D scenes. Our 3D-SLIM introduces two key components: a Geometry-adaptive Mask that constrains attention based on spatial density rather than token order, and an Instruction-aware Mask that enables object tokens to directly access instruction context. This design allows the model to process objects based on their spatial relationships while being guided by the user's task. 3D-SLIM is simple, requires no architectural modifications, and adds no extra parameters, yet it yields substantial performance improvements across diverse 3D scene-language tasks. Extensive experiments across multiple benchmarks and LLM baselines validate its effectiveness and underscore the critical role of decoder design in 3D multi-modal reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YingVideo-MV: Music-Driven Multi-Stage Video Generation</title>
<link>https://arxiv.org/abs/2512.02492</link>
<guid>https://arxiv.org/abs/2512.02492</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion model, music-performance video, camera motion, temporal-aware diffusion Transformer, long-sequence consistency<br /><br />Summary: The paper presents YingVideo-MV, a novel cascaded framework designed for music-driven long-video generation, specifically targeting music-performance videos with dynamic camera motions. It addresses the challenges of producing long sequences that maintain natural audio-visual synchronization and preserve performer identity. The framework integrates multiple components: audio semantic analysis for extracting meaningful music features, an interpretable shot planning module called MV-Director to control video shots, temporal-aware diffusion Transformer architectures for effective video generation, and long-sequence consistency modeling to ensure coherent output over extended durations. To support the development and evaluation of their approach, the authors create a large-scale Music-in-the-Wild Dataset sourced from real web data, enabling diverse and high-quality video generation. Recognizing that existing long-video generation methods lack explicit control over camera movements, YingVideo-MV introduces a camera adapter module that embeds camera pose information into latent noise, allowing precise manipulation of camera motion. Additionally, a time-aware dynamic window range strategy is proposed to adaptively adjust denoising ranges during long-sequence inference, improving continuity between video clips based on audio embeddings. Benchmark results demonstrate that YingVideo-MV excels in generating coherent, expressive music videos with accurate synchronization of music, performer motion, and camera movement, advancing the state-of-the-art in music-driven video synthesis. <div>
arXiv:2512.02492v1 Announce Type: new 
Abstract: While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention-guided reference point shifting for Gaussian-mixture-based partial point set registration</title>
<link>https://arxiv.org/abs/2512.02496</link>
<guid>https://arxiv.org/abs/2512.02496</guid>
<content:encoded><![CDATA[
<div> Keywords: partial-to-partial point set registration, deep learning, Gaussian mixture models, attention-based reference point shifting, transformation-invariant features<br /><br />Summary:  
This study focuses on the challenges in partial-to-partial point set registration when input point sets undergo translation and rotation, especially in deep learning methods using Gaussian mixture models (GMMs). It highlights theoretical and practical limitations found in DeepGMR, a foundational approach in this domain, specifically its shortcomings for partial-to-partial registration scenarios. The authors aim to identify root causes for these limitations and propose an effective solution. They introduce an attention-based reference point shifting (ARPS) layer designed to robustly detect a common reference point between two partial point sets. This approach helps in obtaining features that are invariant to transformations like translation and rotation. Unlike previous methods that focus on finding overlapping regions, the ARPS layer employs an attention mechanism to identify a common reference point directly. Integrating the ARPS layer significantly improves the registration performance of DeepGMR and its variant UGMMReg. Furthermore, these enhanced models outperform other recent deep learning techniques that utilize attention blocks or Transformers aimed at overlap or reference point detection. The findings offer new insights and practical improvements for registration methods involving deep learning and GMMs. <div>
arXiv:2512.02496v1 Announce Type: new 
Abstract: This study investigates the impact of the invariance of feature vectors for partial-to-partial point set registration under translation and rotation of input point sets, particularly in the realm of techniques based on deep learning and Gaussian mixture models (GMMs). We reveal both theoretical and practical problems associated with such deep-learning-based registration methods using GMMs, with a particular focus on the limitations of DeepGMR, a pioneering study in this line, to the partial-to-partial point set registration. Our primary goal is to uncover the causes behind such methods and propose a comprehensible solution for that. To address this, we introduce an attention-based reference point shifting (ARPS) layer, which robustly identifies a common reference point of two partial point sets, thereby acquiring transformation-invariant features. The ARPS layer employs a well-studied attention module to find a common reference point rather than the overlap region. Owing to this, it significantly enhances the performance of DeepGMR and its recent variant, UGMMReg. Furthermore, these extension models outperform even prior deep learning methods using attention blocks and Transformer to extract the overlap region or common reference points. We believe these findings provide deeper insights into registration methods using deep learning and GMMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Large Scale Benchmark for Test Time Adaptation Methods in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.02497</link>
<guid>https://arxiv.org/abs/2512.02497</guid>
<content:encoded><![CDATA[
<div> Test Time Adaptation, Medical Image Segmentation, Domain Shift, Benchmark, Multimodality<br /><br />Summary:  
This paper introduces MedSeg-TTA, a comprehensive benchmark designed to evaluate test time adaptation (TTA) methods for medical image segmentation across seven imaging modalities: MRI, CT, ultrasound, pathology, dermoscopy, OCT, and chest X-ray. The benchmark standardizes data preprocessing, backbone configurations, and test time protocols to ensure methodological consistency. MedSeg-TTA covers twenty representative adaptation methods categorized into four paradigms: Input-level Transformation, Feature-level Alignment, Output-level Regularization, and Prior Estimation, facilitating the first systematic cross-modality comparison of their reliability and applicability. Key findings reveal that no single paradigm excels universally; input-level methods perform more stably under mild appearance shifts, feature-level and output-level methods improve boundary-related metrics, while prior-based methods are highly modality-dependent. Some methods notably degrade under large inter-center and inter-device variations, highlighting the critical need for careful method selection in clinical settings. The benchmark offers standardized datasets, validated implementations, and a public leaderboard, creating a rigorous platform for advancing robust, clinically reliable test time adaptation techniques in medical imaging. All source codes and datasets are openly available at the provided GitHub repository. <div>
arXiv:2512.02497v1 Announce Type: new 
Abstract: Test time Adaptation is a promising approach for mitigating domain shift in medical image segmentation; however, current evaluations remain limited in terms of modality coverage, task diversity, and methodological consistency. We present MedSeg-TTA, a comprehensive benchmark that examines twenty representative adaptation methods across seven imaging modalities, including MRI, CT, ultrasound, pathology, dermoscopy, OCT, and chest X-ray, under fully unified data preprocessing, backbone configuration, and test time protocols. The benchmark encompasses four significant adaptation paradigms: Input-level Transformation, Feature-level Alignment, Output-level Regularization, and Prior Estimation, enabling the first systematic cross-modality comparison of their reliability and applicability. The results show that no single paradigm performs best in all conditions. Input-level methods are more stable under mild appearance shifts. Feature-level and Output-level methods offer greater advantages in boundary-related metrics, whereas prior-based methods exhibit strong modality dependence. Several methods degrade significantly under large inter-center and inter-device shifts, which highlights the importance of principled method selection for clinical deployment. MedSeg-TTA provides standardized datasets, validated implementations, and a public leaderboard, establishing a rigorous foundation for future research on robust, clinically reliable test-time adaptation. All source codes and open-source datasets are available at https://github.com/wenjing-gg/MedSeg-TTA.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.02498</link>
<guid>https://arxiv.org/abs/2512.02498</guid>
<content:encoded><![CDATA[
arXiv:2512.02498v1 Announce Type: new 
Abstract: Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots.ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, dots.ocr establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding</title>
<link>https://arxiv.org/abs/2512.02505</link>
<guid>https://arxiv.org/abs/2512.02505</guid>
<content:encoded><![CDATA[
arXiv:2512.02505v1 Announce Type: new 
Abstract: Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-Stage Vision Transformer for Image Restoration: Colorization Pretraining + Residual Upsampling</title>
<link>https://arxiv.org/abs/2512.02512</link>
<guid>https://arxiv.org/abs/2512.02512</guid>
<content:encoded><![CDATA[
arXiv:2512.02512v1 Announce Type: new 
Abstract: In computer vision, Single Image Super-Resolution (SISR) is still a difficult problem. We present ViT-SR, a new technique to improve the performance of a Vision Transformer (ViT) employing a two-stage training strategy. In our method, the model learns rich, generalizable visual representations from the data itself through a self-supervised pretraining phase on a colourization task. The pre-trained model is then adjusted for 4x super-resolution. By predicting the addition of a high-frequency residual image to an initial bicubic interpolation, this design simplifies residual learning. ViT-SR, trained and evaluated on the DIV2K benchmark dataset, achieves an impressive SSIM of 0.712 and PSNR of 22.90 dB. These results demonstrate the efficacy of our two-stage approach and highlight the potential of self-supervised pre-training for complex image restoration tasks. Further improvements may be possible with larger ViT architectures or alternative pretext tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts</title>
<link>https://arxiv.org/abs/2512.02517</link>
<guid>https://arxiv.org/abs/2512.02517</guid>
<content:encoded><![CDATA[
arXiv:2512.02517v1 Announce Type: new 
Abstract: The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.02520</link>
<guid>https://arxiv.org/abs/2512.02520</guid>
<content:encoded><![CDATA[
arXiv:2512.02520v1 Announce Type: new 
Abstract: Zero-shot anomaly classification and segmentation (AC/AS) aim to detect anomalous samples and regions without any training data, a capability increasingly crucial in industrial inspection and medical imaging. This dissertation aims to investigate the core challenges of zero-shot AC/AS and presents principled solutions rooted in theory and algorithmic design.
  We first formalize the problem of consistent anomalies, a failure mode in which recurring similar anomalies systematically bias distance-based methods. By analyzing the statistical and geometric behavior of patch representations from pre-trained Vision Transformers, we identify two key phenomena - similarity scaling and neighbor-burnout - that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects.
  We then introduce CoDeGraph, a graph-based framework for filtering consistent anomalies built on the similarity scaling and neighbor-burnout phenomena. Through multi-stage graph construction, community detection, and structured refinement, CoDeGraph effectively suppresses the influence of consistent anomalies.
  Next, we extend this framework to 3D medical imaging by proposing a training-free, computationally efficient volumetric tokenization strategy for MRI data. This enables a genuinely zero-shot 3D anomaly detection pipeline and shows that volumetric anomaly segmentation is achievable without any 3D training samples.
  Finally, we bridge batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models. Together, this dissertation provides theoretical understanding and practical solutions for the zero-shot AC/AS problem.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens</title>
<link>https://arxiv.org/abs/2512.02536</link>
<guid>https://arxiv.org/abs/2512.02536</guid>
<content:encoded><![CDATA[
arXiv:2512.02536v1 Announce Type: new 
Abstract: Recent progress in multimodal large language models (MLLMs) has highlighted the challenge of efficiently bridging pre-trained Vision-Language Models (VLMs) with Diffusion Models. While methods using a fixed number of learnable query tokens offer computational efficiency, they suffer from task generalization collapse, failing to adapt to new tasks that are distant from their pre-training tasks. To overcome this, we propose Noisy Query Tokens, which learn a distributed representation space between the VLM and Diffusion Model via end-to-end optimization, enhancing continual learning. Additionally, we introduce a VAE branch with linear projection to recover fine-grained image details. Experimental results confirm our approach mitigates generalization collapse and enables stable continual learning across diverse tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVGGT: Rethinking Global Attention for Accelerating VGGT</title>
<link>https://arxiv.org/abs/2512.02541</link>
<guid>https://arxiv.org/abs/2512.02541</guid>
<content:encoded><![CDATA[
arXiv:2512.02541v1 Announce Type: new 
Abstract: Since DUSt3R, models such as VGGT and $\pi^3$ have shown strong multi-view 3D performance, but their heavy reliance on global self-attention results in high computational cost. Existing sparse-attention variants offer partial speedups, yet lack a systematic analysis of how global attention contributes to multi-view reasoning. In this paper, we first conduct an in-depth investigation of the global attention modules in VGGT and $\pi^3$ to better understand their roles. Our analysis reveals a clear division of roles in the alternating global-frame architecture: early global layers do not form meaningful correspondences, middle layers perform cross-view alignment, and last layers provide only minor refinements. Guided by these findings, we propose a training-free two-step acceleration scheme: (1) converting early global layers into frame attention, and (2) subsampling global attention by subsampling K/V over patch tokens with diagonal preservation and a mean-fill component. We instantiate this strategy on VGGT and $\pi^3$ and evaluate across standard pose and point-map benchmarks. Our method achieves up to $8$-$10\times$ speedup in inference time while matching or slightly improving the accuracy of the original models, and remains robust even in extremely dense multi-view settings where prior sparse-attention baselines fail.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniPerson: Unified Identity-Preserving Pedestrian Generation</title>
<link>https://arxiv.org/abs/2512.02554</link>
<guid>https://arxiv.org/abs/2512.02554</guid>
<content:encoded><![CDATA[
arXiv:2512.02554v1 Announce Type: new 
Abstract: Person re-identification (ReID) suffers from a lack of large-scale high-quality training data due to challenges in data privacy and annotation costs. While previous approaches have explored pedestrian generation for data augmentation, they often fail to ensure identity consistency and suffer from insufficient controllability, thereby limiting their effectiveness in dataset augmentation. To address this, We introduce OmniPerson, the first unified identity-preserving pedestrian generation pipeline for visible/infrared image/video ReID tasks. Our contributions are threefold: 1) We proposed OmniPerson, a unified generation model, offering holistic and fine-grained control over all key pedestrian attributes. Supporting RGB/IR modality image/video generation with any number of reference images, two kinds of person poses, and text. Also including RGB-to-IR transfer and image super-resolution abilities.2) We designed Multi-Refer Fuser for robust identity preservation with any number of reference images as input, making OmniPerson could distill a unified identity from a set of multi-view reference images, ensuring our generated pedestrians achieve high-fidelity pedestrian generation.3) We introduce PersonSyn, the first large-scale dataset for multi-reference, controllable pedestrian generation, and present its automated curation pipeline which transforms public, ID-only ReID benchmarks into a richly annotated resource with the dense, multi-modal supervision required for this task. Experimental results demonstrate that OmniPerson achieves SoTA in pedestrian generation, excelling in both visual fidelity and identity consistency. Furthermore, augmenting existing datasets with our generated data consistently improves the performance of ReID models. We will open-source the full codebase, pretrained model, and the PersonSyn dataset.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature</title>
<link>https://arxiv.org/abs/2512.02566</link>
<guid>https://arxiv.org/abs/2512.02566</guid>
<content:encoded><![CDATA[
arXiv:2512.02566v1 Announce Type: new 
Abstract: There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-speech Gesture Video Generation via Motion-Based Graph Retrieval</title>
<link>https://arxiv.org/abs/2512.02576</link>
<guid>https://arxiv.org/abs/2512.02576</guid>
<content:encoded><![CDATA[
arXiv:2512.02576v1 Announce Type: new 
Abstract: Synthesizing synchronized and natural co-speech gesture videos remains a formidable challenge. Recent approaches have leveraged motion graphs to harness the potential of existing video data. To retrieve an appropriate trajectory from the graph, previous methods either utilize the distance between features extracted from the input audio and those associated with the motions in the graph or embed both the input audio and motion into a shared feature space. However, these techniques may not be optimal due to the many-to-many mapping nature between audio and gestures, which cannot be adequately addressed by one-to-one mapping. To alleviate this limitation, we propose a novel framework that initially employs a diffusion model to generate gesture motions. The diffusion model implicitly learns the joint distribution of audio and motion, enabling the generation of contextually appropriate gestures from input audio sequences. Furthermore, our method extracts both low-level and high-level features from the input audio to enrich the training process of the diffusion model. Subsequently, a meticulously designed motion-based retrieval algorithm is applied to identify the most suitable path within the graph by assessing both global and local similarities in motion. Given that not all nodes in the retrieved path are sequentially continuous, the final step involves seamlessly stitching together these segments to produce a coherent video output. Experimental results substantiate the efficacy of our proposed method, demonstrating a significant improvement over prior approaches in terms of synchronization accuracy and naturalness of generated gestures.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content-Aware Texturing for Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.02621</link>
<guid>https://arxiv.org/abs/2512.02621</guid>
<content:encoded><![CDATA[
arXiv:2512.02621v1 Announce Type: new 
Abstract: Gaussian Splatting has become the method of choice for 3D reconstruction and real-time rendering of captured real scenes. However, fine appearance details need to be represented as a large number of small Gaussian primitives, which can be wasteful when geometry and appearance exhibit different frequency characteristics.
  Inspired by the long tradition of texture mapping, we propose to use texture to represent detailed appearance where possible. Our main focus is to incorporate per-primitive texture maps that adapt to the scene in a principled manner during Gaussian Splatting optimization. We do this by proposing a new appearance representation for 2D Gaussian primitives with textures where the size of a texel is bounded by the image sampling frequency and adapted to the content of the input images. We achieve this by adaptively upscaling or downscaling the texture resolution during optimization. In addition, our approach enables control of the number of primitives during optimization based on texture resolution. We show that our approach performs favorably in image quality and total number of parameters used compared to alternative solutions for textured Gaussian primitives. Project page: https://repo-sam.inria.fr/nerphys/gs-texturing/
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence</title>
<link>https://arxiv.org/abs/2512.02622</link>
<guid>https://arxiv.org/abs/2512.02622</guid>
<content:encoded><![CDATA[
arXiv:2512.02622v1 Announce Type: new 
Abstract: Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding</title>
<link>https://arxiv.org/abs/2512.02624</link>
<guid>https://arxiv.org/abs/2512.02624</guid>
<content:encoded><![CDATA[
arXiv:2512.02624v1 Announce Type: new 
Abstract: PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening</title>
<link>https://arxiv.org/abs/2512.02643</link>
<guid>https://arxiv.org/abs/2512.02643</guid>
<content:encoded><![CDATA[
arXiv:2512.02643v1 Announce Type: new 
Abstract: Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking</title>
<link>https://arxiv.org/abs/2512.02648</link>
<guid>https://arxiv.org/abs/2512.02648</guid>
<content:encoded><![CDATA[
arXiv:2512.02648v1 Announce Type: new 
Abstract: We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hear What Matters! Text-conditioned Selective Video-to-Audio Generation</title>
<link>https://arxiv.org/abs/2512.02650</link>
<guid>https://arxiv.org/abs/2512.02650</guid>
<content:encoded><![CDATA[
arXiv:2512.02650v1 Announce Type: new 
Abstract: This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation</title>
<link>https://arxiv.org/abs/2512.02660</link>
<guid>https://arxiv.org/abs/2512.02660</guid>
<content:encoded><![CDATA[
arXiv:2512.02660v1 Announce Type: new 
Abstract: Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes</title>
<link>https://arxiv.org/abs/2512.02664</link>
<guid>https://arxiv.org/abs/2512.02664</guid>
<content:encoded><![CDATA[
arXiv:2512.02664v1 Announce Type: new 
Abstract: Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions. However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence. We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation. This process achieves high-fidelity reflection separation and full-scene reconstruction without requiring environment maps or restrictive material assumptions. We demonstrate on public and self-collected datasets that PolarGuide-GSDR achieves state-of-the-art performance in specular reconstruction, normal estimation, and novel view synthesis, all while maintaining real-time rendering capabilities. To our knowledge, this is the first framework embedding polarization priors directly into 3DGS optimization, yielding superior interpretability and real-time performance for modeling complex reflective scenes.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking</title>
<link>https://arxiv.org/abs/2512.02668</link>
<guid>https://arxiv.org/abs/2512.02668</guid>
<content:encoded><![CDATA[
arXiv:2512.02668v1 Announce Type: new 
Abstract: Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.02681</link>
<guid>https://arxiv.org/abs/2512.02681</guid>
<content:encoded><![CDATA[
arXiv:2512.02681v1 Announce Type: new 
Abstract: Although diffusion-based models have achieved impressive results in image super-resolution, they often rely on large-scale backbones such as Stable Diffusion XL (SDXL) and Diffusion Transformers (DiT), which lead to excessive computational and memory costs during training and inference. To address this issue, we develop a lightweight diffusion method, PGP-DiffSR, by removing redundant information from diffusion models under the guidance of the phase information of inputs for efficient image super-resolution. We first identify the intra-block redundancy within the diffusion backbone and propose a progressive pruning approach that removes redundant blocks while reserving restoration capability. We note that the phase information of the restored images produced by the pruned diffusion model is not well estimated. To solve this problem, we propose a phase-exchange adapter module that explores the phase information of the inputs to guide the pruned diffusion model for better restoration performance. We formulate the progressive pruning approach and the phase-exchange adapter module into a unified model. Extensive experiments demonstrate that our method achieves competitive restoration quality while significantly reducing computational load and memory consumption. The code is available at https://github.com/yzb1997/PGP-DiffSR.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance</title>
<link>https://arxiv.org/abs/2512.02685</link>
<guid>https://arxiv.org/abs/2512.02685</guid>
<content:encoded><![CDATA[
arXiv:2512.02685v1 Announce Type: new 
Abstract: Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data</title>
<link>https://arxiv.org/abs/2512.02686</link>
<guid>https://arxiv.org/abs/2512.02686</guid>
<content:encoded><![CDATA[
arXiv:2512.02686v1 Announce Type: new 
Abstract: Anomaly segmentation seeks to detect and localize unknown or out-of-distribution (OoD) objects that fall outside predefined semantic classes a capability essential for safe autonomous driving. However, the scarcity and limited diversity of anomaly data severely constrain model generalization in open-world environments. Existing approaches mitigate this issue through synthetic data generation, either by copy-pasting external objects into driving scenes or by leveraging text-to-image diffusion models to inpaint anomalous regions. While these methods improve anomaly diversity, they often lack contextual coherence and physical realism, resulting in domain gaps between synthetic and real data. In this paper, we present ClimaDrive, a semantics-guided image-to-image framework for synthesizing semantically coherent, weather-diverse, and physically plausible OoD driving data. ClimaDrive unifies structure-guided multi-weather generation with prompt-driven anomaly inpainting, enabling the creation of visually realistic training data. Based on this framework, we construct ClimaOoD, a large-scale benchmark spanning six representative driving scenarios under both clear and adverse weather conditions. Extensive experiments on four state-of-the-art methods show that training with ClimaOoD leads to robust improvements in anomaly segmentation. Across all methods, AUROC, AP, and FPR95 show notable gains, with FPR95 dropping from 3.97 to 3.52 for RbA on Fishyscapes LAF. These results demonstrate that ClimaOoD enhances model robustness, offering valuable training data for better generalization in open-world anomaly detection.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection</title>
<link>https://arxiv.org/abs/2512.02696</link>
<guid>https://arxiv.org/abs/2512.02696</guid>
<content:encoded><![CDATA[
arXiv:2512.02696v1 Announce Type: new 
Abstract: Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization</title>
<link>https://arxiv.org/abs/2512.02697</link>
<guid>https://arxiv.org/abs/2512.02697</guid>
<content:encoded><![CDATA[
arXiv:2512.02697v1 Announce Type: new 
Abstract: Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm</title>
<link>https://arxiv.org/abs/2512.02700</link>
<guid>https://arxiv.org/abs/2512.02700</guid>
<content:encoded><![CDATA[
arXiv:2512.02700v1 Announce Type: new 
Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tissue-mask supported inter-subject whole-body image registration in the UK Biobank - A method benchmarking study</title>
<link>https://arxiv.org/abs/2512.02702</link>
<guid>https://arxiv.org/abs/2512.02702</guid>
<content:encoded><![CDATA[
arXiv:2512.02702v1 Announce Type: new 
Abstract: The UK Biobank is a large-scale study collecting whole-body MR imaging and non-imaging health data. Robust and accurate inter-subject image registration of these whole-body MR images would enable their body-wide spatial standardization, and region-/voxel-wise correlation analysis of non-imaging data with image-derived parameters (e.g., tissue volume or fat content).
  We propose a sex-stratified inter-subject whole-body MR image registration approach that uses subcutaneous adipose tissue- and muscle-masks from the state-of-the-art VIBESegmentator method to augment intensity-based graph-cut registration. The proposed method was evaluated on a subset of 4000 subjects by comparing it to an intensity-only method as well as two previously published registration methods, uniGradICON and MIRTK. The evaluation comprised overlap measures applied to the 71 VIBESegmentator masks: 1) Dice scores, and 2) voxel-wise label error frequency. Additionally, voxel-wise correlation between age and each of fat content and tissue volume was studied to exemplify the usefulness for medical research.
  The proposed method exhibited a mean dice score of 0.77 / 0.75 across the cohort and the 71 masks for males/females, respectively. When compared to the intensity-only registration, the mean values were 6 percentage points (pp) higher for both sexes, and the label error frequency was decreased in most tissue regions. These differences were 9pp / 8pp against uniGradICON and 12pp / 13pp against MIRTK. Using the proposed method, the age-correlation maps were less noisy and showed higher anatomical alignment.
  In conclusion, the image registration method using two tissue masks improves whole-body registration of UK Biobank images.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding</title>
<link>https://arxiv.org/abs/2512.02715</link>
<guid>https://arxiv.org/abs/2512.02715</guid>
<content:encoded><![CDATA[
arXiv:2512.02715v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions</title>
<link>https://arxiv.org/abs/2512.02727</link>
<guid>https://arxiv.org/abs/2512.02727</guid>
<content:encoded><![CDATA[
arXiv:2512.02727v1 Announce Type: new 
Abstract: Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone</title>
<link>https://arxiv.org/abs/2512.02737</link>
<guid>https://arxiv.org/abs/2512.02737</guid>
<content:encoded><![CDATA[
arXiv:2512.02737v1 Announce Type: new 
Abstract: Image-based localization in GNSS-denied environments is critical for UAV autonomy. Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training. Such data are costly to acquire and often unavailable, limiting their applicability. To address this challenge, we adopt a training paradigm that removes the need for UAV imagery during training by learning directly from satellite-view reference images. This is achieved through a dedicated augmentation strategy that simulates the visual domain shift between satellite and real-world UAV views. We introduce CAEVL, an efficient model designed to exploit this paradigm, and validate it on ViLD, a new and challenging dataset of real-world UAV images that we release to the community. Our method achieves competitive performance compared to approaches trained with paired data, demonstrating its effectiveness and strong generalization capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning-Aware Multimodal Fusion for Hateful Video Detection</title>
<link>https://arxiv.org/abs/2512.02743</link>
<guid>https://arxiv.org/abs/2512.02743</guid>
<content:encoded><![CDATA[
arXiv:2512.02743v1 Announce Type: new 
Abstract: Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AttMetNet: Attention-Enhanced Deep Neural Network for Methane Plume Detection in Sentinel-2 Satellite Imagery</title>
<link>https://arxiv.org/abs/2512.02751</link>
<guid>https://arxiv.org/abs/2512.02751</guid>
<content:encoded><![CDATA[
arXiv:2512.02751v1 Announce Type: new 
Abstract: Methane is a powerful greenhouse gas that contributes significantly to global warming. Accurate detection of methane emissions is the key to taking timely action and minimizing their impact on climate change. We present AttMetNet, a novel attention-enhanced deep learning framework for methane plume detection with Sentinel-2 satellite imagery. The major challenge in developing a methane detection model is to accurately identify methane plumes from Sentinel-2's B11 and B12 bands while suppressing false positives caused by background variability and diverse land cover types. Traditional detection methods typically depend on the differences or ratios between these bands when comparing the scenes with and without plumes. However, these methods often require verification by a domain expert because they generate numerous false positives. Recent deep learning methods make some improvements using CNN-based architectures, but lack mechanisms to prioritize methane-specific features. AttMetNet introduces a methane-aware architecture that fuses the Normalized Difference Methane Index (NDMI) with an attention-enhanced U-Net. By jointly exploiting NDMI's plume-sensitive cues and attention-driven feature selection, AttMetNet selectively amplifies methane absorption features while suppressing background noise. This integration establishes a first-of-its-kind architecture tailored for robust methane plume detection in real satellite imagery. Additionally, we employ focal loss to address the severe class imbalance arising from both limited positive plume samples and sparse plume pixels within imagery. Furthermore, AttMetNet is trained on the real methane plume dataset, making it more robust to practical scenarios. Extensive experiments show that AttMetNet surpasses recent methods in methane plume detection with a lower false positive rate, better precision recall balance, and higher IoU.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Surgical Smoke: A Smoke-Type-Aware Laparoscopic Video Desmoking Method and Dataset</title>
<link>https://arxiv.org/abs/2512.02780</link>
<guid>https://arxiv.org/abs/2512.02780</guid>
<content:encoded><![CDATA[
arXiv:2512.02780v1 Announce Type: new 
Abstract: Electrocautery or lasers will inevitably generate surgical smoke, which hinders the visual guidance of laparoscopic videos for surgical procedures. The surgical smoke can be classified into different types based on its motion patterns, leading to distinctive spatio-temporal characteristics across smoky laparoscopic videos. However, existing desmoking methods fail to account for such smoke-type-specific distinctions. Therefore, we propose the first Smoke-Type-Aware Laparoscopic Video Desmoking Network (STANet) by introducing two smoke types: Diffusion Smoke and Ambient Smoke. Specifically, a smoke mask segmentation sub-network is designed to jointly conduct smoke mask and smoke type predictions based on the attention-weighted mask aggregation, while a smokeless video reconstruction sub-network is proposed to perform specially desmoking on smoky features guided by two types of smoke mask. To address the entanglement challenges of two smoke types, we further embed a coarse-to-fine disentanglement module into the mask segmentation sub-network, which yields more accurate disentangled masks through the smoke-type-aware cross attention between non-entangled and entangled regions. In addition, we also construct the first large-scale synthetic video desmoking dataset with smoke type annotations. Extensive experiments demonstrate that our method not only outperforms state-of-the-art approaches in quality evaluations, but also exhibits superior generalization across multiple downstream surgical tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LumiX: Structured and Coherent Text-to-Intrinsic Generation</title>
<link>https://arxiv.org/abs/2512.02781</link>
<guid>https://arxiv.org/abs/2512.02781</guid>
<content:encoded><![CDATA[
arXiv:2512.02781v1 Announce Type: new 
Abstract: We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking</title>
<link>https://arxiv.org/abs/2512.02789</link>
<guid>https://arxiv.org/abs/2512.02789</guid>
<content:encoded><![CDATA[
arXiv:2512.02789v1 Announce Type: new 
Abstract: The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits</title>
<link>https://arxiv.org/abs/2512.02790</link>
<guid>https://arxiv.org/abs/2512.02790</guid>
<content:encoded><![CDATA[
arXiv:2512.02790v1 Announce Type: new 
Abstract: With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, \textbf{Qwen-Verify}, for efficient failure detection and instruction recaptioning. This pipeline yields \textbf{UnicEdit-10M}, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose \textbf{UnicBench}, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including \textit{Non-edit Consistency} and \textit{Reasoning Accuracy}. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval</title>
<link>https://arxiv.org/abs/2512.02792</link>
<guid>https://arxiv.org/abs/2512.02792</guid>
<content:encoded><![CDATA[
arXiv:2512.02792v1 Announce Type: new 
Abstract: Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video. The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning. Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality. However, previous works have largely overlooked the disparity in information density between these two modalities. This limitation can lead to two critical issues: 1) modification subject referring ambiguity and 2) limited detailed semantic focus, both of which degrade the performance of CVR models. To address the aforementioned issues, we propose a novel CVR framework, namely the Hierarchical Uncertainty-aware Disambiguation network (HUD). HUD is the first framework that leverages the disparity in information density between video and text to enhance multi-modal query understanding. It comprises three key components: (a) Holistic Pronoun Disambiguation, (b) Atomistic Uncertainty Modeling, and (c) Holistic-to-Atomistic Alignment. By exploiting overlapping semantics through holistic cross-modal interaction and fine-grained semantic alignment via atomistic-level cross-modal interaction, HUD enables effective object disambiguation and enhances the focus on detailed semantics, thereby achieving precise composed feature learning. Moreover, our proposed HUD is also applicable to the Composed Image Retrieval (CIR) task and achieves state-of-the-art performance across three benchmark datasets for both CVR and CIR tasks. The codes are available on https://zivchen-ty.github.io/HUD.github.io/.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IC-World: In-Context Generation for Shared World Modeling</title>
<link>https://arxiv.org/abs/2512.02793</link>
<guid>https://arxiv.org/abs/2512.02793</guid>
<content:encoded><![CDATA[
arXiv:2512.02793v1 Announce Type: new 
Abstract: Video-based world models have recently garnered increasing attention for their ability to synthesize diverse and dynamic visual environments. In this paper, we focus on shared world modeling, where a model generates multiple videos from a set of input images, each representing the same underlying world in different camera poses. We propose IC-World, a novel generation framework, enabling parallel generation for all input images via activating the inherent in-context generation capability of large video models. We further finetune IC-World via reinforcement learning, Group Relative Policy Optimization, together with two proposed novel reward models to enforce scene-level geometry consistency and object-level motion consistency among the set of generated videos. Extensive experiments demonstrate that IC-World substantially outperforms state-of-the-art methods in both geometry and motion consistency. To the best of our knowledge, this is the first work to systematically explore the shared world modeling problem with video-based world models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhyCustom: Towards Realistic Physical Customization in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.02794</link>
<guid>https://arxiv.org/abs/2512.02794</guid>
<content:encoded><![CDATA[
arXiv:2512.02794v1 Announce Type: new 
Abstract: Recent diffusion-based text-to-image customization methods have achieved significant success in understanding concrete concepts to control generation processes, such as styles and shapes. However, few efforts dive into the realistic yet challenging customization of physical concepts. The core limitation of current methods arises from the absence of explicitly introducing physical knowledge during training. Even when physics-related words appear in the input text prompts, our experiments consistently demonstrate that these methods fail to accurately reflect the corresponding physical properties in the generated results. In this paper, we propose PhyCustom, a fine-tuning framework comprising two novel regularization losses to activate diffusion model to perform physical customization. Specifically, the proposed isometric loss aims at activating diffusion models to learn physical concepts while decouple loss helps to eliminate the mixture learning of independent concepts. Experiments are conducted on a diverse dataset and our benchmark results demonstrate that PhyCustom outperforms previous state-of-the-art and popular methods in terms of physical customization quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Defense That Attacks: How Robust Models Become Better Attackers</title>
<link>https://arxiv.org/abs/2512.02830</link>
<guid>https://arxiv.org/abs/2512.02830</guid>
<content:encoded><![CDATA[
arXiv:2512.02830v1 Announce Type: new 
Abstract: Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02835</link>
<guid>https://arxiv.org/abs/2512.02835</guid>
<content:encoded><![CDATA[
arXiv:2512.02835v1 Announce Type: new 
Abstract: Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?</title>
<link>https://arxiv.org/abs/2512.02846</link>
<guid>https://arxiv.org/abs/2512.02846</guid>
<content:encoded><![CDATA[
arXiv:2512.02846v1 Announce Type: new 
Abstract: Anticipating actions before they occur is a core challenge in action understanding research. While conventional methods rely on extracting and aggregating temporal information from videos, as humans we can often predict upcoming actions by observing a single moment from a scene, when given sufficient context. Can a model achieve this competence? The short answer is yes, although its effectiveness depends on the complexity of the task. In this work, we investigate to what extent video aggregation can be replaced with alternative modalities. To this end, based on recent advances in visual feature extraction and language-based reasoning, we introduce AAG, a method for Action Anticipation at a Glimpse. AAG combines RGB features with depth cues from a single frame for enhanced spatial reasoning, and incorporates prior action information to provide long-term context. This context is obtained either through textual summaries from Vision-Language Models, or from predictions generated by a single-frame action recognizer. Our results demonstrate that multimodal single-frame action anticipation using AAG can perform competitively compared to both temporally aggregated video baselines and state-of-the-art methods across three instructional activity datasets: IKEA-ASM, Meccano, and Assembly101.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study</title>
<link>https://arxiv.org/abs/2512.02850</link>
<guid>https://arxiv.org/abs/2512.02850</guid>
<content:encoded><![CDATA[
arXiv:2512.02850v1 Announce Type: new 
Abstract: Modern image editors can produce identity-preserving AIGC (IP-AIGC), where the same person appears with new attire, background, or lighting. The robustness and fairness of current detectors in this regime remain unclear, especially for under-represented populations. We present what we believe is the first systematic study of IP-AIGC detection for Indian and South-Asian faces, quantifying cross-generator generalization and intra-population performance. We assemble Indian-focused training splits from FairFD and HAV-DF, and construct two held-out IP-AIGC test sets (HIDF-img-ip-genai and HIDF-vid-ip-genai) using commercial web-UI generators (Gemini and ChatGPT) with identity-preserving prompts. We evaluate two state-of-the-art detectors (AIDE and Effort) under pretrained (PT) and fine-tuned (FT) regimes and report AUC, AP, EER, and accuracy. Fine-tuning yields strong in-domain gains (for example, Effort AUC 0.739 to 0.944 on HAV-DF-test; AIDE EER 0.484 to 0.259), but consistently degrades performance on held-out IP-AIGC for Indian cohorts (for example, AIDE AUC 0.923 to 0.563 on HIDF-img-ip-genai; Effort 0.740 to 0.533), which indicates overfitting to training-generator cues. On non-IP HIDF images, PT performance remains high, which suggests a specific brittleness to identity-preserving edits rather than a generic distribution shift. Our study establishes IP-AIGC-Indian as a challenging and practically relevant scenario and motivates representation-preserving adaptation and India-aware benchmark curation to close generalization gaps in AIGC detection.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RFOP: Rethinking Fusion and Orthogonal Projection for Face-Voice Association</title>
<link>https://arxiv.org/abs/2512.02860</link>
<guid>https://arxiv.org/abs/2512.02860</guid>
<content:encoded><![CDATA[
arXiv:2512.02860v1 Announce Type: new 
Abstract: Face-voice association in multilingual environment challenge 2026 aims to investigate the face-voice association task in multilingual scenario. The challenge introduces English-German face-voice pairs to be utilized in the evaluation phase. To this end, we revisit the fusion and orthogonal projection for face-voice association by effectively focusing on the relevant semantic information within the two modalities. Our method performs favorably on the English-German data split and ranked 3rd in the FAME 2026 challenge by achieving the EER of 33.1.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MICCAI STSR 2025 Challenge: Semi-Supervised Teeth and Pulp Segmentation and CBCT-IOS Registration</title>
<link>https://arxiv.org/abs/2512.02867</link>
<guid>https://arxiv.org/abs/2512.02867</guid>
<content:encoded><![CDATA[
arXiv:2512.02867v1 Announce Type: new 
Abstract: Cone-Beam Computed Tomography (CBCT) and Intraoral Scanning (IOS) are essential for digital dentistry, but annotated data scarcity limits automated solutions for pulp canal segmentation and cross-modal registration. To benchmark semi-supervised learning (SSL) in this domain, we organized the STSR 2025 Challenge at MICCAI 2025, featuring two tasks: (1) semi-supervised segmentation of teeth and pulp canals in CBCT, and (2) semi-supervised rigid registration of CBCT and IOS. We provided 60 labeled and 640 unlabeled IOS samples, plus 30 labeled and 250 unlabeled CBCT scans with varying resolutions and fields of view. The challenge attracted strong community participation, with top teams submitting open-source deep learning-based SSL solutions. For segmentation, leading methods used nnU-Net and Mamba-like State Space Models with pseudo-labeling and consistency regularization, achieving a Dice score of 0.967 and Instance Affinity of 0.738 on the hidden test set. For registration, effective approaches combined PointNetLK with differentiable SVD and geometric augmentation to handle modality gaps; hybrid neural-classical refinement enabled accurate alignment despite limited labels. All data and code are publicly available at https://github.com/ricoleehduu/STS-Challenge-2025 to ensure reproducibility.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Camera-Controlled Video Generation with Verifiable Geometry Reward</title>
<link>https://arxiv.org/abs/2512.02870</link>
<guid>https://arxiv.org/abs/2512.02870</guid>
<content:encoded><![CDATA[
arXiv:2512.02870v1 Announce Type: new 
Abstract: Recent advances in video diffusion models have remarkably improved camera-controlled video generation, but most methods rely solely on supervised fine-tuning (SFT), leaving online reinforcement learning (RL) post-training largely underexplored. In this work, we introduce an online RL post-training framework that optimizes a pretrained video generator for precise camera control. To make RL effective in this setting, we design a verifiable geometry reward that delivers dense segment-level feedback to guide model optimization. Specifically, we estimate the 3D camera trajectories for both generated and reference videos, divide each trajectory into short segments, and compute segment-wise relative poses. The reward function then compares each generated-reference segment pair and assigns an alignment score as the reward signal, which helps alleviate reward sparsity and improve optimization efficiency. Moreover, we construct a comprehensive dataset featuring diverse large-amplitude camera motions and scenes with varied subject dynamics. Extensive experiments show that our online RL post-training clearly outperforms SFT baselines across multiple aspects, including camera-control accuracy, geometric consistency, and visual quality, demonstrating its superiority in advancing camera-controlled video generation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm</title>
<link>https://arxiv.org/abs/2512.02895</link>
<guid>https://arxiv.org/abs/2512.02895</guid>
<content:encoded><![CDATA[
arXiv:2512.02895v1 Announce Type: new 
Abstract: We present MindGPT-4ov, a multimodal large language model (MLLM) that introduces a general post-training paradigm spanning data production, model training, and efficient deployment. It achieves state-of-the-art performance across multiple benchmarks at low cost, effectively enhancing the foundational capabilities of MLLMs and the generalization ability. Focusing on data construction, supervised fine-tuning strategies, and multimodal reinforcement learning methods, this work proposes three key innovations: (1) An information density-based data generation scheme, integrated with a dual-dimensional tree-structured label system, enabling automated generation of high-quality cross-domain data. (2) A collaborative curriculum supervised fine-tuning approach that balances the injection of domain-specific knowledge with the preservation of general capabilities. (3) A hybrid reinforcement learning paradigm that enhances reasoning ability while simultaneously addressing multi-objective optimization such as diversity exploration, maintenance of multimodal perception, and response conciseness. Moreover, we implement a series of infrastructure optimizations, such as 5D parallel training, operator optimization, and inference quantization to enhance training and inference efficiency while reducing the cost of domain adaptation. Experimental results demonstrate that the MindGPT-4ov model outperforms state-of-the-art models on benchmarks such as MMBench, MMStar, MathVision, and MathVista. In addition, MindGPT-4ov also demonstrates superior user experience in vertical domain tasks, enabling a seamless transition from academic research to industrial deployment. MindGPT-4ov provides a general post-training paradigm applicable to a wide range of MLLMs. The model weights, datasets, and code for the Qwen3-VL-based variants will be recently open-sourced to support the community's development of MLLMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models</title>
<link>https://arxiv.org/abs/2512.02897</link>
<guid>https://arxiv.org/abs/2512.02897</guid>
<content:encoded><![CDATA[
arXiv:2512.02897v1 Announce Type: new 
Abstract: This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model. We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself. Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy. Experiments with different datasets, including integration into an operational place recognition policy, validate the practical relevance of these findings and demonstrate that carefully designed projections can serve as an effective surrogate for end-to-end 3-D learning in LiDAR place recognition.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Glance: Accelerating Diffusion Models with 1 Sample</title>
<link>https://arxiv.org/abs/2512.02899</link>
<guid>https://arxiv.org/abs/2512.02899</guid>
<content:encoded><![CDATA[
arXiv:2512.02899v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding</title>
<link>https://arxiv.org/abs/2512.02906</link>
<guid>https://arxiv.org/abs/2512.02906</guid>
<content:encoded><![CDATA[
arXiv:2512.02906v1 Announce Type: new 
Abstract: Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2512.02931</link>
<guid>https://arxiv.org/abs/2512.02931</guid>
<content:encoded><![CDATA[
arXiv:2512.02931v1 Announce Type: new 
Abstract: In this paper, we investigate the underexplored challenge of sample diversity in autoregressive (AR) generative models with bitwise visual tokenizers. We first analyze the factors that limit diversity in bitwise AR models and identify two key issues: (1) the binary classification nature of bitwise modeling, which restricts the prediction space, and (2) the overly sharp logits distribution, which causes sampling collapse and reduces diversity. Building on these insights, we propose DiverseAR, a principled and effective method that enhances image diversity without sacrificing visual quality. Specifically, we introduce an adaptive logits distribution scaling mechanism that dynamically adjusts the sharpness of the binary output distribution during sampling, resulting in smoother predictions and greater diversity. To mitigate potential fidelity loss caused by distribution smoothing, we further develop an energy-based generation path search algorithm that avoids sampling low-confidence tokens, thereby preserving high visual quality. Extensive experiments demonstrate that DiverseAR substantially improves sample diversity in bitwise autoregressive image generation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis</title>
<link>https://arxiv.org/abs/2512.02932</link>
<guid>https://arxiv.org/abs/2512.02932</guid>
<content:encoded><![CDATA[
arXiv:2512.02932v1 Announce Type: new 
Abstract: Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization</title>
<link>https://arxiv.org/abs/2512.02933</link>
<guid>https://arxiv.org/abs/2512.02933</guid>
<content:encoded><![CDATA[
arXiv:2512.02933v1 Announce Type: new 
Abstract: Text-guided video editing, particularly for object removal and addition, remains a challenging task due to the need for precise spatial and temporal consistency. Existing methods often rely on auxiliary masks or reference images for editing guidance, which limits their scalability and generalization. To address these issues, we propose LoVoRA, a novel framework for mask-free video object removal and addition using object-aware localization mechanism. Our approach utilizes a unique dataset construction pipeline that integrates image-to-video translation, optical flow-based mask propagation, and video inpainting, enabling temporally consistent edits. The core innovation of LoVoRA is its learnable object-aware localization mechanism, which provides dense spatio-temporal supervision for both object insertion and removal tasks. By leveraging a Diffusion Mask Predictor, LoVoRA achieves end-to-end video editing without requiring external control signals during inference. Extensive experiments and human evaluation demonstrate the effectiveness and high-quality performance of LoVoRA.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench</title>
<link>https://arxiv.org/abs/2512.02942</link>
<guid>https://arxiv.org/abs/2512.02942</guid>
<content:encoded><![CDATA[
arXiv:2512.02942v1 Announce Type: new 
Abstract: The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layout Anything: One Transformer for Universal Room Layout Estimation</title>
<link>https://arxiv.org/abs/2512.02952</link>
<guid>https://arxiv.org/abs/2512.02952</guid>
<content:encoded><![CDATA[
arXiv:2512.02952v1 Announce Type: new 
Abstract: We present Layout Anything, a transformer-based framework for indoor layout estimation that adapts the OneFormer's universal segmentation architecture to geometric structure prediction. Our approach integrates OneFormer's task-conditioned queries and contrastive learning with two key modules: (1) a layout degeneration strategy that augments training data while preserving Manhattan-world constraints through topology-aware transformations, and (2) differentiable geometric losses that directly enforce planar consistency and sharp boundary predictions during training. By unifying these components in an end-to-end framework, the model eliminates complex post-processing pipelines while achieving high-speed inference at 114ms. Extensive experiments demonstrate state-of-the-art performance across standard benchmarks, with pixel error (PE) of 5.43% and corner error (CE) of 4.02% on the LSUN, PE of 7.04% (CE 5.17%) on the Hedau and PE of 4.03% (CE 3.15%) on the Matterport3D-Layout datasets. The framework's combination of geometric awareness and computational efficiency makes it particularly suitable for augmented reality applications and large-scale 3D scene reconstruction tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight Real-Time Low-Light Enhancement Network for Embedded Automotive Vision Systems</title>
<link>https://arxiv.org/abs/2512.02965</link>
<guid>https://arxiv.org/abs/2512.02965</guid>
<content:encoded><![CDATA[
arXiv:2512.02965v1 Announce Type: new 
Abstract: In low-light environments like nighttime driving, image degradation severely challenges in-vehicle camera safety. Since existing enhancement algorithms are often too computationally intensive for vehicular applications, we propose UltraFast-LieNET, a lightweight multi-scale shifted convolutional network for real-time low-light image enhancement. We introduce a Dynamic Shifted Convolution (DSConv) kernel with only 12 learnable parameters for efficient feature extraction. By integrating DSConv with varying shift distances, a Multi-scale Shifted Residual Block (MSRB) is constructed to significantly expand the receptive field. To mitigate lightweight network instability, a residual structure and a novel multi-level gradient-aware loss function are incorporated. UltraFast-LieNET allows flexible parameter configuration, with a minimum size of only 36 parameters. Results on the LOLI-Street dataset show a PSNR of 26.51 dB, outperforming state-of-the-art methods by 4.6 dB while utilizing only 180 parameters. Experiments across four benchmark datasets validate its superior balance of real-time performance and enhancement quality under limited resources. Code is available at https://githubhttps://github.com/YuhanChen2024/UltraFast-LiNET
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.02972</link>
<guid>https://arxiv.org/abs/2512.02972</guid>
<content:encoded><![CDATA[
arXiv:2512.02972v1 Announce Type: new 
Abstract: Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities</title>
<link>https://arxiv.org/abs/2512.02973</link>
<guid>https://arxiv.org/abs/2512.02973</guid>
<content:encoded><![CDATA[
arXiv:2512.02973v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\% and 91.07\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2512.02981</link>
<guid>https://arxiv.org/abs/2512.02981</guid>
<content:encoded><![CDATA[
arXiv:2512.02981v1 Announce Type: new 
Abstract: Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences</title>
<link>https://arxiv.org/abs/2512.02982</link>
<guid>https://arxiv.org/abs/2512.02982</guid>
<content:encoded><![CDATA[
arXiv:2512.02982v1 Announce Type: new 
Abstract: Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a "hard-to-easy" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.02991</link>
<guid>https://arxiv.org/abs/2512.02991</guid>
<content:encoded><![CDATA[
arXiv:2512.02991v1 Announce Type: new 
Abstract: Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond</title>
<link>https://arxiv.org/abs/2512.02993</link>
<guid>https://arxiv.org/abs/2512.02993</guid>
<content:encoded><![CDATA[
arXiv:2512.02993v1 Announce Type: new 
Abstract: Prevailing 3D texture generation methods, which often rely on multi-view fusion, are frequently hindered by inter-view inconsistencies and incomplete coverage of complex surfaces, limiting the fidelity and completeness of the generated content. To overcome these challenges, we introduce TEXTRIX, a native 3D attribute generation framework for high-fidelity texture synthesis and downstream applications such as precise 3D part segmentation. Our approach constructs a latent 3D attribute grid and leverages a Diffusion Transformer equipped with sparse attention, enabling direct coloring of 3D models in volumetric space and fundamentally avoiding the limitations of multi-view fusion. Built upon this native representation, the framework naturally extends to high-precision 3D segmentation by training the same architecture to predict semantic attributes on the grid. Extensive experiments demonstrate state-of-the-art performance on both tasks, producing seamless, high-fidelity textures and accurate 3D part segmentation with precise boundaries.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling</title>
<link>https://arxiv.org/abs/2512.03000</link>
<guid>https://arxiv.org/abs/2512.03000</guid>
<content:encoded><![CDATA[
arXiv:2512.03000v1 Announce Type: new 
Abstract: Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images</title>
<link>https://arxiv.org/abs/2512.03004</link>
<guid>https://arxiv.org/abs/2512.03004</guid>
<content:encoded><![CDATA[
arXiv:2512.03004v1 Announce Type: new 
Abstract: Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting</title>
<link>https://arxiv.org/abs/2512.03010</link>
<guid>https://arxiv.org/abs/2512.03010</guid>
<content:encoded><![CDATA[
arXiv:2512.03010v1 Announce Type: new 
Abstract: LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials. Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions. However, the accuracy of LiDAR for featureless regions is rarely reached. Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme. We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges. We use this insight to introduce an ambiguity heuristic for completed scans by evaluating the change in density in the point cloud. This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan. For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas. Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud. To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion. We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Sync-LoRA for Portrait Video Editing</title>
<link>https://arxiv.org/abs/2512.03013</link>
<guid>https://arxiv.org/abs/2512.03013</guid>
<content:encoded><![CDATA[
arXiv:2512.03013v1 Announce Type: new 
Abstract: Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks</title>
<link>https://arxiv.org/abs/2512.03014</link>
<guid>https://arxiv.org/abs/2512.03014</guid>
<content:encoded><![CDATA[
arXiv:2512.03014v1 Announce Type: new 
Abstract: When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoBrep: Autoregressive B-Rep Generation with Unified Topology and Geometry</title>
<link>https://arxiv.org/abs/2512.03018</link>
<guid>https://arxiv.org/abs/2512.03018</guid>
<content:encoded><![CDATA[
arXiv:2512.03018v1 Announce Type: new 
Abstract: The boundary representation (B-Rep) is the standard data structure used in Computer-Aided Design (CAD) for defining solid models. Despite recent progress, directly generating B-Reps end-to-end with precise geometry and watertight topology remains a challenge. This paper presents AutoBrep, a novel Transformer model that autoregressively generates B-Reps with high quality and validity. AutoBrep employs a unified tokenization scheme that encodes both geometric and topological characteristics of a B-Rep model as a sequence of discrete tokens. Geometric primitives (i.e., surfaces and curves) are encoded as latent geometry tokens, and their structural relationships are defined as special topological reference tokens. Sequence order in AutoBrep naturally follows a breadth first traversal of the B-Rep face adjacency graph. At inference time, neighboring faces and edges along with their topological structure are progressively generated. Extensive experiments demonstrate the advantages of our unified representation when coupled with next-token prediction for B-Rep generation. AutoBrep outperforms baselines with better quality and watertightness. It is also highly scalable to complex solids with good fidelity and inference speed. We further show that autocompleting B-Reps is natively supported through our unified tokenization, enabling user-controllable CAD generation with minimal changes. Code is available at https://github.com/AutodeskAILab/AutoBrep.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unrolled Networks are Conditional Probability Flows in MRI Reconstruction</title>
<link>https://arxiv.org/abs/2512.03020</link>
<guid>https://arxiv.org/abs/2512.03020</guid>
<content:encoded><![CDATA[
arXiv:2512.03020v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI) offers excellent soft-tissue contrast without ionizing radiation, but its long acquisition time limits clinical utility. Recent methods accelerate MRI by under-sampling $k$-space and reconstructing the resulting images using deep learning. Unrolled networks have been widely used for the reconstruction task due to their efficiency, but suffer from unstable evolving caused by freely-learnable parameters in intermediate steps. In contrast, diffusion models based on stochastic differential equations offer theoretical stability in both medical and natural image tasks but are computationally expensive. In this work, we introduce flow ODEs to MRI reconstruction by theoretically proving that unrolled networks are discrete implementations of conditional probability flow ODEs. This connection provides explicit formulations for parameters and clarifies how intermediate states should evolve. Building on this insight, we propose Flow-Aligned Training (FLAT), which derives unrolled parameters from the ODE discretization and aligns intermediate reconstructions with the ideal ODE trajectory to improve stability and convergence. Experiments on three MRI datasets show that FLAT achieves high-quality reconstructions with up to $3\times$ fewer iterations than diffusion-based generative models and significantly greater stability than unrolled networks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.03034</link>
<guid>https://arxiv.org/abs/2512.03034</guid>
<content:encoded><![CDATA[
arXiv:2512.03034v1 Announce Type: new 
Abstract: We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation</title>
<link>https://arxiv.org/abs/2512.03036</link>
<guid>https://arxiv.org/abs/2512.03036</guid>
<content:encoded><![CDATA[
arXiv:2512.03036v1 Announce Type: new 
Abstract: Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation</title>
<link>https://arxiv.org/abs/2512.03040</link>
<guid>https://arxiv.org/abs/2512.03040</guid>
<content:encoded><![CDATA[
arXiv:2512.03040v1 Announce Type: new 
Abstract: We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiShotMaster: A Controllable Multi-Shot Video Generation Framework</title>
<link>https://arxiv.org/abs/2512.03041</link>
<guid>https://arxiv.org/abs/2512.03041</guid>
<content:encoded><![CDATA[
arXiv:2512.03041v1 Announce Type: new 
Abstract: Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPTArena: A Benchmark for Agentic PowerPoint Editing</title>
<link>https://arxiv.org/abs/2512.03042</link>
<guid>https://arxiv.org/abs/2512.03042</guid>
<content:encoded><![CDATA[
arXiv:2512.03042v1 Announce Type: new 
Abstract: We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OneThinker: All-in-one Reasoning Model for Image and Video</title>
<link>https://arxiv.org/abs/2512.03043</link>
<guid>https://arxiv.org/abs/2512.03043</guid>
<content:encoded><![CDATA[
arXiv:2512.03043v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03045</link>
<guid>https://arxiv.org/abs/2512.03045</guid>
<content:encoded><![CDATA[
arXiv:2512.03045v1 Announce Type: new 
Abstract: Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear. In this work, we first verify that the attention maps of these models acquire geometric correspondence throughout training, attending to the geometrically corresponding regions across reference and target views for view-consistent generation. However, this correspondence signal remains incomplete, with its accuracy degrading under large viewpoint changes. Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models. Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance. CAMEO reduces the number of training iterations required for convergence by half while achieving superior performance at the same iteration counts. We further demonstrate that CAMEO is model-agnostic and can be applied to any multi-view diffusion model.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues</title>
<link>https://arxiv.org/abs/2512.03046</link>
<guid>https://arxiv.org/abs/2512.03046</guid>
<content:encoded><![CDATA[
arXiv:2512.03046v1 Announce Type: new 
Abstract: We propose MagicQuill V2, a novel system that introduces a \textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superpixel Attack: Enhancing Black-box Adversarial Attack with Image-driven Division Areas</title>
<link>https://arxiv.org/abs/2512.02062</link>
<guid>https://arxiv.org/abs/2512.02062</guid>
<content:encoded><![CDATA[
arXiv:2512.02062v1 Announce Type: cross 
Abstract: Deep learning models are used in safety-critical tasks such as automated driving and face recognition. However, small perturbations in the model input can significantly change the predictions. Adversarial attacks are used to identify small perturbations that can lead to misclassifications. More powerful black-box adversarial attacks are required to develop more effective defenses. A promising approach to black-box adversarial attacks is to repeat the process of extracting a specific image area and changing the perturbations added to it. Existing attacks adopt simple rectangles as the areas where perturbations are changed in a single iteration. We propose applying superpixels instead, which achieve a good balance between color variance and compactness. We also propose a new search method, versatile search, and a novel attack method, Superpixel Attack, which applies superpixels and performs versatile search. Superpixel Attack improves attack success rates by an average of 2.10% compared with existing attacks. Most models used in this study are robust against adversarial attacks, and this improvement is significant for black-box adversarial attacks. The code is avilable at https://github.com/oe1307/SuperpixelAttack.git.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Baseline and Day-1 Diffusion MRI Using Multimodal Deep Embeddings for Stroke Outcome Prediction</title>
<link>https://arxiv.org/abs/2512.02088</link>
<guid>https://arxiv.org/abs/2512.02088</guid>
<content:encoded><![CDATA[
arXiv:2512.02088v1 Announce Type: cross 
Abstract: This study compares baseline (J0) and 24-hour (J1) diffusion magnetic resonance imaging (MRI) for predicting three-month functional outcomes after acute ischemic stroke (AIS). Seventy-four AIS patients with paired apparent diffusion coefficient (ADC) scans and clinical data were analyzed. Three-dimensional ResNet-50 embeddings were fused with structured clinical variables, reduced via principal component analysis (<=12 components), and classified using linear support vector machines with eight-fold stratified group cross-validation. J1 multimodal models achieved the highest predictive performance (AUC = 0.923 +/- 0.085), outperforming J0-based configurations (AUC <= 0.86). Incorporating lesion-volume features further improved model stability and interpretability. These findings demonstrate that early post-treatment diffusion MRI provides superior prognostic value to pre-treatment imaging and that combining MRI, clinical, and lesion-volume features produces a robust and interpretable framework for predicting three-month functional outcomes in AIS patients.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoatFusion: Controllable Material Coating in Images</title>
<link>https://arxiv.org/abs/2512.02143</link>
<guid>https://arxiv.org/abs/2512.02143</guid>
<content:encoded><![CDATA[
arXiv:2512.02143v1 Announce Type: cross 
Abstract: We introduce Material Coating, a novel image editing task that simulates applying a thin material layer onto an object while preserving its underlying coarse and fine geometry. Material coating is fundamentally different from existing "material transfer" methods, which are designed to replace an object's intrinsic material, often overwriting fine details. To address this new task, we construct a large-scale synthetic dataset (110K images) of 3D objects with varied, physically-based coatings, named DataCoat110K. We then propose CoatFusion, a novel architecture that enables this task by conditioning a diffusion model on both a 2D albedo texture and granular, PBR-style parametric controls, including roughness, metalness, transmission, and a key thickness parameter. Experiments and user studies show CoatFusion produces realistic, controllable coatings and significantly outperforms existing material editing and transfer methods on this new task.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhishSnap: Image-Based Phishing Detection Using Perceptual Hashing</title>
<link>https://arxiv.org/abs/2512.02243</link>
<guid>https://arxiv.org/abs/2512.02243</guid>
<content:encoded><![CDATA[
arXiv:2512.02243v1 Announce Type: cross 
Abstract: Phishing remains one of the most prevalent online threats, exploiting human trust to harvest sensitive credentials. Existing URL- and HTML-based detection systems struggle against obfuscation and visual deception. This paper presents \textbf{PhishSnap}, a privacy-preserving, on-device phishing detection system leveraging perceptual hashing (pHash). Implemented as a browser extension, PhishSnap captures webpage screenshots, computes visual hashes, and compares them against legitimate templates to identify visually similar phishing attempts. A \textbf{2024 dataset of 10,000 URLs} (70\%/20\%/10\% train/validation/test) was collected from PhishTank and Netcraft. Due to security takedowns, a subset of phishing pages was unavailable, reducing dataset diversity. The system achieved \textbf{0.79 accuracy}, \textbf{0.76 precision}, and \textbf{0.78 recall}, showing that visual similarity remains a viable anti-phishing measure. The entire inference process occurs locally, ensuring user privacy and minimal latency.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.02280</link>
<guid>https://arxiv.org/abs/2512.02280</guid>
<content:encoded><![CDATA[
arXiv:2512.02280v1 Announce Type: cross 
Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIGS-SLAM: Visual Inertial Gaussian Splatting SLAM</title>
<link>https://arxiv.org/abs/2512.02293</link>
<guid>https://arxiv.org/abs/2512.02293</guid>
<content:encoded><![CDATA[
arXiv:2512.02293v1 Announce Type: cross 
Abstract: We present VIGS-SLAM, a visual-inertial 3D Gaussian Splatting SLAM system that achieves robust real-time tracking and high-fidelity reconstruction. Although recent 3DGS-based SLAM methods achieve dense and photorealistic mapping, their purely visual design degrades under motion blur, low texture, and exposure variations. Our method tightly couples visual and inertial cues within a unified optimization framework, jointly refining camera poses, depths, and IMU states. It features robust IMU initialization, time-varying bias modeling, and loop closure with consistent Gaussian updates. Experiments on four challenging datasets demonstrate our superiority over state-of-the-art methods. Project page: https://vigs-slam.github.io
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning</title>
<link>https://arxiv.org/abs/2512.02306</link>
<guid>https://arxiv.org/abs/2512.02306</guid>
<content:encoded><![CDATA[
arXiv:2512.02306v1 Announce Type: cross 
Abstract: Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective</title>
<link>https://arxiv.org/abs/2512.02340</link>
<guid>https://arxiv.org/abs/2512.02340</guid>
<content:encoded><![CDATA[
arXiv:2512.02340v1 Announce Type: cross 
Abstract: Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction</title>
<link>https://arxiv.org/abs/2512.02609</link>
<guid>https://arxiv.org/abs/2512.02609</guid>
<content:encoded><![CDATA[
arXiv:2512.02609v1 Announce Type: cross 
Abstract: Imitation learning for robotic grasping is often plagued by the multimodal problem: when a scene contains multiple valid targets, demonstrations of grasping different objects create conflicting training signals. Standard imitation learning policies fail by averaging these distinct actions into a single, invalid action. In this paper, we introduce SAM2Grasp, a novel framework that resolves this issue by reformulating the task as a uni-modal, prompt-conditioned prediction problem. Our method leverages the frozen SAM2 model to use its powerful visual temporal tracking capability and introduces a lightweight, trainable action head that operates in parallel with its native segmentation head. This design allows for training only the small action head on pre-computed temporal-visual features from SAM2. During inference, an initial prompt, such as a bounding box provided by an upstream object detection model, designates the specific object to be grasped. This prompt conditions the action head to predict a unique, unambiguous grasp trajectory for that object alone. In all subsequent video frames, SAM2's built-in temporal tracking capability automatically maintains stable tracking of the selected object, enabling our model to continuously predict the grasp trajectory from the video stream without further external guidance. This temporal-prompted approach effectively eliminates ambiguity from the visuomotor policy. We demonstrate through extensive experiments that SAM2Grasp achieves state-of-the-art performance in cluttered, multi-object grasping tasks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models</title>
<link>https://arxiv.org/abs/2512.02636</link>
<guid>https://arxiv.org/abs/2512.02636</guid>
<content:encoded><![CDATA[
arXiv:2512.02636v1 Announce Type: cross 
Abstract: Log-likelihood evaluation enables important capabilities in generative models, including model comparison, certain fine-tuning objectives, and many downstream applications. Yet paradoxically, some of today's best generative models -- diffusion and flow-based models -- still require hundreds to thousands of neural function evaluations (NFEs) to compute a single likelihood. While recent distillation methods have successfully accelerated sampling to just a few steps, they achieve this at the cost of likelihood tractability: existing approaches either abandon likelihood computation entirely or still require expensive integration over full trajectories. We present fast flow joint distillation (F2D2), a framework that simultaneously reduces the number of NFEs required for both sampling and likelihood evaluation by two orders of magnitude. Our key insight is that in continuous normalizing flows, the coupled ODEs for sampling and likelihood are computed from a shared underlying velocity field, allowing us to jointly distill both the sampling trajectory and cumulative divergence using a single model. F2D2 is modular, compatible with existing flow-based few-step sampling models, and requires only an additional divergence prediction head. Experiments demonstrate F2D2's capability of achieving accurate log-likelihood with few-step evaluations while maintaining high sample quality, solving a long-standing computational bottleneck in flow-based generative models. As an application of our approach, we propose a lightweight self-guidance method that enables a 2-step MeanFlow model to outperform a 1024 step teacher model with only a single additional backward NFE.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Multimodal Data Collection Using Smartwatches and Its Visualization in Education</title>
<link>https://arxiv.org/abs/2512.02651</link>
<guid>https://arxiv.org/abs/2512.02651</guid>
<content:encoded><![CDATA[
arXiv:2512.02651v1 Announce Type: cross 
Abstract: Wearable sensors, such as smartwatches, have become increasingly prevalent across domains like healthcare, sports, and education, enabling continuous monitoring of physiological and behavioral data. In the context of education, these technologies offer new opportunities to study cognitive and affective processes such as engagement, attention, and performance. However, the lack of scalable, synchronized, and high-resolution tools for multimodal data acquisition continues to be a significant barrier to the widespread adoption of Multimodal Learning Analytics in real-world educational settings. This paper presents two complementary tools developed to address these challenges: Watch-DMLT, a data acquisition application for Fitbit Sense 2 smartwatches that enables real-time, multi-user monitoring of physiological and motion signals; and ViSeDOPS, a dashboard-based visualization system for analyzing synchronized multimodal data collected during oral presentations. We report on a classroom deployment involving 65 students and up to 16 smartwatches, where data streams including heart rate, motion, gaze, video, and contextual annotations were captured and analyzed. Results demonstrate the feasibility and utility of the proposed system for supporting fine-grained, scalable, and interpretable Multimodal Learning Analytics in real learning environments.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs</title>
<link>https://arxiv.org/abs/2512.02719</link>
<guid>https://arxiv.org/abs/2512.02719</guid>
<content:encoded><![CDATA[
arXiv:2512.02719v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols</title>
<link>https://arxiv.org/abs/2512.02787</link>
<guid>https://arxiv.org/abs/2512.02787</guid>
<content:encoded><![CDATA[
arXiv:2512.02787v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation</title>
<link>https://arxiv.org/abs/2512.02920</link>
<guid>https://arxiv.org/abs/2512.02920</guid>
<content:encoded><![CDATA[
arXiv:2512.02920v1 Announce Type: cross 
Abstract: We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes. Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings. In this work, we construct a large multimodal dataset across six U.S. states, containing nine million traffic accident records from official sources, and one million high-resolution satellite images for each node of the road network. Additionally, every node is annotated with features such as the region's weather statistics and road type (e.g., residential vs. motorway), and each edge is annotated with traffic volume information (i.e., Average Annual Daily Traffic). Utilizing this dataset, we conduct a comprehensive evaluation of multimodal learning methods that integrate both visual and network embeddings. Our findings show that integrating both data modalities improves prediction accuracy, achieving an average AUROC of $90.1\%$, which is a $3.7\%$ gain over graph neural network models that only utilize graph structures. With the improved embeddings, we conduct a causal analysis based on a matching estimator to estimate the key contributing factors influencing traffic accidents. We find that accident rates rise by $24\%$ under higher precipitation, by $22\%$ on higher-speed roads such as motorways, and by $29\%$ due to seasonal patterns, after adjusting for other confounding factors. Ablation studies confirm that satellite imagery features are essential for achieving accurate prediction.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control</title>
<link>https://arxiv.org/abs/2512.03028</link>
<guid>https://arxiv.org/abs/2512.03028</guid>
<content:encoded><![CDATA[
arXiv:2512.03028v1 Announce Type: cross 
Abstract: Data-driven motion priors that can guide agents toward producing naturalistic behaviors play a pivotal role in creating life-like virtual characters. Adversarial imitation learning has been a highly effective method for learning motion priors from reference motion data. However, adversarial priors, with few exceptions, need to be retrained for each new controller, thereby limiting their reusability and necessitating the retention of the reference motion data when training on downstream tasks. In this work, we present Score-Matching Motion Priors (SMP), which leverages pre-trained motion diffusion models and score distillation sampling (SDS) to create reusable task-agnostic motion priors. SMPs can be pre-trained on a motion dataset, independent of any control policy or task. Once trained, SMPs can be kept frozen and reused as general-purpose reward functions to train policies to produce naturalistic behaviors for downstream tasks. We show that a general motion prior trained on large-scale datasets can be repurposed into a variety of style-specific priors. Furthermore SMP can compose different styles to synthesize new styles not present in the original dataset. Our method produces high-quality motion comparable to state-of-the-art adversarial imitation learning methods through reusable and modular motion priors. We demonstrate the effectiveness of SMP across a diverse suite of control tasks with physically simulated humanoid characters. Video demo available at https://youtu.be/ravlZJteS20
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Permutation-Aware Action Segmentation via Unsupervised Frame-to-Segment Alignment</title>
<link>https://arxiv.org/abs/2305.19478</link>
<guid>https://arxiv.org/abs/2305.19478</guid>
<content:encoded><![CDATA[
arXiv:2305.19478v5 Announce Type: replace 
Abstract: This paper presents an unsupervised transformer-based framework for temporal activity segmentation which leverages not only frame-level cues but also segment-level cues. This is in contrast with previous methods which often rely on frame-level information only. Our approach begins with a frame-level prediction module which estimates framewise action classes via a transformer encoder. The frame-level prediction module is trained in an unsupervised manner via temporal optimal transport. To exploit segment-level information, we utilize a segment-level prediction module and a frame-to-segment alignment module. The former includes a transformer decoder for estimating video transcripts, while the latter matches frame-level features with segment-level features, yielding permutation-aware segmentation results. Moreover, inspired by temporal optimal transport, we introduce simple-yet-effective pseudo labels for unsupervised training of the above modules. Our experiments on four public datasets, i.e., 50 Salads, YouTube Instructions, Breakfast, and Desktop Assembly show that our approach achieves comparable or better performance than previous methods in unsupervised activity segmentation. Our code and dataset are available on our research website: https://retrocausal.ai/research/.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios</title>
<link>https://arxiv.org/abs/2404.15272</link>
<guid>https://arxiv.org/abs/2404.15272</guid>
<content:encoded><![CDATA[
arXiv:2404.15272v4 Announce Type: replace 
Abstract: 3D medical vision-language (VL) pretraining has shown potential in radiology by leveraging large-scale multimodal datasets with CT-report pairs. However, existing methods primarily rely on a global VL alignment directly adapted from 2D scenarios. The entire 3D image is transformed into one global embedding, resulting in a loss of sparse but critical semantics essential for accurately aligning with the corresponding diagnosis. To address this limitation, we propose CT-GLIP, a 3D Grounded Language-Image Pretrained model that constructs fine-grained CT-report pairs to enhance \textit{grounded} cross-modal contrastive learning, effectively aligning grounded visual features with precise textual descriptions. Leveraging the grounded cross-modal alignment, CT-GLIP improves performance across diverse downstream tasks and can even identify organs and abnormalities in a zero-shot manner using natural language. CT-GLIP is trained on a multimodal CT dataset comprising 44,011 organ-level CT-report pairs from 17,702 patients, covering 104 organs. Evaluation is conducted on four downstream tasks: zero-shot organ recognition (OR), zero-shot abnormality detection (AD), tumor detection (TD), and tumor segmentation (TS). Empirical results show that it outperforms its counterparts with global VL alignment. Compared to vanilla CLIP, CT-GLIP achieves average performance improvements of 15.1% of F1 score, 1.9% of AUC, and 3.2% of DSC for zero-shot AD, TD, and TS tasks, respectively. This study highlights the significance of grounded VL alignment in enabling 3D medical VL foundation models to understand sparse representations within CT scans.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Diffusion Models with Noise-Conditioned Perception</title>
<link>https://arxiv.org/abs/2406.17636</link>
<guid>https://arxiv.org/abs/2406.17636</guid>
<content:encoded><![CDATA[
arXiv:2406.17636v2 Announce Type: replace 
Abstract: Recent advancements in human preference optimization, initially developed for Language Models (LMs), have shown promise for text-to-image Diffusion Models, enhancing prompt alignment, visual appeal, and user preference. Unlike LMs, Diffusion Models typically optimize in pixel or VAE space, which does not align well with human perception, leading to slower and less efficient training during the preference alignment stage. We propose using a perceptual objective in the U-Net embedding space of the diffusion model to address these issues. Our approach involves fine-tuning Stable Diffusion 1.5 and XL using Direct Preference Optimization (DPO), Contrastive Preference Optimization (CPO), and supervised fine-tuning (SFT) within this embedding space. This method significantly outperforms standard latent-space implementations across various metrics, including quality and computational cost. For SDXL, our approach provides 60.8\% general preference, 62.2\% visual appeal, and 52.1\% prompt following against original open-sourced SDXL-DPO on the PartiPrompts dataset, while significantly reducing compute. Our approach not only improves the efficiency and quality of human preference alignment for diffusion models but is also easily integrable with other optimization techniques. The training code and LoRA weights will be available here: https://huggingface.co/alexgambashidze/SDXL\_NCP-DPO\_v0.1
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mutually-Aware Feature Learning for Few-Shot Object Counting</title>
<link>https://arxiv.org/abs/2408.09734</link>
<guid>https://arxiv.org/abs/2408.09734</guid>
<content:encoded><![CDATA[
arXiv:2408.09734v2 Announce Type: replace 
Abstract: Few-shot object counting has garnered significant attention for its practicality as it aims to count target objects in a query image based on given exemplars without additional training. However, the prevailing extract-and-match approach has a shortcoming: query and exemplar features lack interaction during feature extraction since they are extracted independently and later correlated based on similarity. This can lead to insufficient target awareness and confusion in identifying the actual target when multiple class objects coexist. To address this, we propose a novel framework, Mutually-Aware FEAture learning (MAFEA), which encodes query and exemplar features with mutual awareness from the outset. By encouraging interaction throughout the pipeline, we obtain target-aware features robust to a multi-category scenario. Furthermore, we introduce background token to effectively associate the query's target region with exemplars and decouple its background region. Our extensive experiments demonstrate that our model achieves state-of-the-art performance on FSCD-LVIS and FSC-147 benchmarks with remarkably reduced target confusion.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2410.12669</link>
<guid>https://arxiv.org/abs/2410.12669</guid>
<content:encoded><![CDATA[
arXiv:2410.12669v2 Announce Type: replace 
Abstract: The increasing demand for controllable outputs in text-to-image generation has spurred advancements in multi-instance generation (MIG), allowing users to define both instance layouts and attributes. However, unlike image-conditional generation methods such as ControlNet, MIG techniques have not been widely adopted in state-of-the-art models like SD2 and SDXL, primarily due to the challenge of building robust renderers that simultaneously handle instance positioning and attribute rendering. In this paper, we introduce Depth-Driven Decoupled Instance Synthesis (3DIS), a novel framework that decouples the MIG process into two stages: (i) generating a coarse scene depth map for accurate instance positioning and scene composition, and (ii) rendering fine-grained attributes using pre-trained ControlNet on any foundational model, without additional training. Our 3DIS framework integrates a custom adapter into LDM3D for precise depth-based layouts and employs a finetuning-free method for enhanced instance-level attribute rendering. Extensive experiments on COCO-Position and COCO-MIG benchmarks demonstrate that 3DIS significantly outperforms existing methods in both layout precision and attribute rendering. Notably, 3DIS offers seamless compatibility with diverse foundational models, providing a robust, adaptable solution for advanced multi-instance generation. The code is available at: https://github.com/limuloo/3DIS.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convolution goes higher-order: a biologically inspired mechanism empowers image classification</title>
<link>https://arxiv.org/abs/2412.06740</link>
<guid>https://arxiv.org/abs/2412.06740</guid>
<content:encoded><![CDATA[
arXiv:2412.06740v2 Announce Type: replace 
Abstract: We propose a novel approach to image classification inspired by complex nonlinear biological visual processing, whereby classical convolutional neural networks (CNNs) are equipped with learnable higher-order convolutions. Our model incorporates a Volterra-like expansion of the convolution operator, capturing multiplicative interactions akin to those observed in early and advanced stages of biological visual processing. We evaluated this approach on synthetic datasets by measuring sensitivity to testing higher-order correlations and performance in standard benchmarks (MNIST, FashionMNIST, CIFAR10, CIFAR100 and Imagenette). Our architecture outperforms traditional CNN baselines, and achieves optimal performance with expansions up to 3rd/4th order, aligning remarkably well with the distribution of pixel intensities in natural images. Through systematic perturbation analysis, we validate this alignment by isolating the contributions of specific image statistics to model performance, demonstrating how different orders of convolution process distinct aspects of visual information. Furthermore, Representational Similarity Analysis reveals distinct geometries across network layers, indicating qualitatively different modes of visual information processing. Our work bridges neuroscience and deep learning, offering a path towards more effective, biologically inspired computer vision models. It provides insights into visual information processing and lays the groundwork for neural networks that better capture complex visual patterns, particularly in resource-constrained scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping</title>
<link>https://arxiv.org/abs/2502.01846</link>
<guid>https://arxiv.org/abs/2502.01846</guid>
<content:encoded><![CDATA[
arXiv:2502.01846v4 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D objects and scenes. However, generating 3DGS remains challenging due to their discrete, unstructured, and permutation-invariant nature. In this work, we present a simple yet effective method to overcome these challenges. We utilize spherical mapping to transform 3DGS into a structured 2D representation, termed UVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a concatenation of Gaussian attributes such as position, scale, color, opacity, and rotation. We further find that these heterogeneous features can be compressed into a lower-dimensional (e.g., 3-channel) shared feature space using a carefully designed multi-branch network. The compressed UVGS can be treated as typical RGB images. Remarkably, we discover that typical VAEs trained with latent diffusion models can directly generalize to this new representation without additional training. Our novel representation makes it effortless to leverage foundational 2D models, such as diffusion models, to directly model 3DGS. Additionally, one can simply increase the 2D UV resolution to accommodate more Gaussians, making UVGS a scalable solution compared to typical 3D backbones. This approach immediately unlocks various novel generation applications of 3DGS by inherently utilizing the already developed superior 2D generation capabilities. In our experiments, we demonstrate various unconditional, conditional generation, and inpainting applications of 3DGS based on diffusion models, which were previously non-trivial.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairT2I: Mitigating Social Bias in Text-to-Image Generation via Large Language Model-Assisted Detection and Attribute Rebalancing</title>
<link>https://arxiv.org/abs/2502.03826</link>
<guid>https://arxiv.org/abs/2502.03826</guid>
<content:encoded><![CDATA[
arXiv:2502.03826v3 Announce Type: replace 
Abstract: Text-to-image (T2I) models have advanced creative content generation, yet their reliance on large uncurated datasets often reproduces societal biases. We present FairT2I, a training-free and interactive framework grounded in a mathematically principled latent variable guidance formulation. This formulation decomposes the generative score function into attribute-conditioned components and reweights them according to a defined distribution, providing a unified and flexible mechanism for bias-aware generation that also subsumes many existing ad hoc debiasing approaches as special cases. Building upon this foundation, FairT2I incorporates (1) latent variable guidance as the core mechanism, (2) LLM-based bias detection to automatically infer bias-prone categories and attributes from text prompts as part of the latent structure, and (3) attribute resampling, which allows users to adjust or redefine the attribute distribution based on uniform, real-world, or user-specified statistics. The accompanying user interface supports this pipeline by enabling users to inspect detected biases, modify attributes or weights, and generate debiased images in real time. Experimental results show that LLMs outperform average human annotators in the number and granularity of detected bias categories and attributes. Moreover, FairT2I achieves superior performance to baseline models in both societal bias mitigation and image diversity, while preserving image quality and prompt fidelity.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alligat0R: Pre-Training Through Co-Visibility Segmentation for Relative Camera Pose Regression</title>
<link>https://arxiv.org/abs/2503.07561</link>
<guid>https://arxiv.org/abs/2503.07561</guid>
<content:encoded><![CDATA[
arXiv:2503.07561v2 Announce Type: replace 
Abstract: Pre-training techniques have greatly advanced computer vision, with CroCo's cross-view completion approach yielding impressive results in tasks like 3D reconstruction and pose regression. However, cross-view completion is ill-posed in non-covisible regions, limiting its effectiveness. We introduce Alligat0R, a novel pre-training approach that replaces cross-view learning with a covisibility segmentation task. Our method predicts whether each pixel in one image is covisible in the second image, occluded, or outside the field of view, making the pre-training effective in both covisible and non-covisible regions, and provides interpretable predictions. To support this, we present Cub3, a large-scale dataset with 5M image pairs and dense covisibility annotations derived from the nuScenes and ScanNet datasets. Cub3 includes diverse scenarios with varying degrees of overlap. The experiments show that our novel pre-training method Alligat0R significantly outperforms CroCo in relative pose regression. Code is available at https://github.com/thibautloiseau/alligat0r.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MegaSR: Mining Customized Semantics and Expressive Guidance for Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2503.08096</link>
<guid>https://arxiv.org/abs/2503.08096</guid>
<content:encoded><![CDATA[
arXiv:2503.08096v2 Announce Type: replace 
Abstract: Text-to-image (T2I) models have ushered in a new era of real-world image super-resolution (Real-ISR) due to their rich internal implicit knowledge for multimodal learning. Although bringing high-level semantic priors and dense pixel guidance have led to advances in reconstruction, we identified several critical phenomena by analyzing the behavior of existing T2I-based Real-ISR methods: (1) Fine detail deficiency, which ultimately leads to incorrect reconstruction in local regions. (2) Block-wise semantic inconsistency, which results in distracted semantic interpretations across U-Net blocks. (3) Edge ambiguity, which causes noticeable structural degradation. Building upon these observations, we first introduce MegaSR, which enhances the T2I-based Real-ISR models with fine-grained customized semantics and expressive guidance to unlock semantically rich and structurally consistent reconstruction. Then, we propose the Customized Semantics Module (CSM) to supplement fine-grained semantics from the image modality and regulate the semantic fusion between multi-level knowledge to realize customization for different U-Net blocks. Besides the semantic adaptation, we identify expressive multimodal signals through pair-wise comparisons and introduce the Multimodal Signal Fusion Module (MSFM) to aggregate them for structurally consistent reconstruction. Extensive experiments on real-world and synthetic datasets demonstrate the superiority of the method. Notably, it not only achieves state-of-the-art performance on quality-driven metrics but also remains competitive on fidelity-focused metrics, striking a balance between perceptual realism and faithful content reconstruction.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2503.14537</link>
<guid>https://arxiv.org/abs/2503.14537</guid>
<content:encoded><![CDATA[
arXiv:2503.14537v5 Announce Type: replace 
Abstract: Learning-based 3D reconstruction has emerged as a transformative technique in autonomous driving, enabling precise modeling of environments through advanced neural representations. It has inspired pioneering solutions for vital tasks in autonomous driving, such as dense mapping and closed-loop simulation, as well as comprehensive scene feature for driving scene understanding and reasoning. Given the rapid growth in related research, this survey provides a comprehensive review of both technical evolutions and practical applications in autonomous driving. We begin with an introduction to the preliminaries of learning-based 3D reconstruction to provide a solid technical background foundation, then progress to a rigorous, multi-dimensional examination of cutting-edge methodologies, systematically organized according to the distinctive technical requirements and fundamental challenges of autonomous driving. Through analyzing and summarizing development trends and cutting-edge research, we identify existing technical challenges, along with insufficient disclosure of on-board validation and safety verification details in the current literature, and ultimately suggest potential directions to guide future studies.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeSearch: Hierarchical Video Search with Spotlight and Reflection for Human-like Long Video Understanding</title>
<link>https://arxiv.org/abs/2504.01407</link>
<guid>https://arxiv.org/abs/2504.01407</guid>
<content:encoded><![CDATA[
arXiv:2504.01407v2 Announce Type: replace 
Abstract: Large video-language models (LVLMs) have shown remarkable performance across various video-language tasks. However, they encounter significant challenges when processing long videos because of the large number of video frames involved. Downsampling long videos in either space or time can lead to visual hallucinations, making it difficult to accurately interpret long videos. Motivated by human hierarchical temporal search strategies, we propose \textbf{TimeSearch}, a novel framework enabling LVLMs to understand long videos in a human-like manner. TimeSearch integrates two human-like primitives into a unified autoregressive LVLM: 1) \textbf{Spotlight} efficiently identifies relevant temporal events through a Temporal-Augmented Frame Representation (TAFR), explicitly binding visual features with timestamps; 2) \textbf{Reflection} evaluates the correctness of the identified events, leveraging the inherent temporal self-reflection capabilities of LVLMs. TimeSearch progressively explores key events and prioritizes temporal search based on reflection confidence. Extensive experiments on challenging long-video benchmarks confirm that TimeSearch substantially surpasses previous state-of-the-art, improving the accuracy from 41.8\% to 51.5\% on the LVBench. Additionally, experiments on temporal grounding demonstrate that appropriate TAFR is adequate to effectively stimulate the surprising temporal grounding ability of LVLMs in a simpler yet versatile manner, which improves mIoU on Charades-STA by 11.8\%. The code will be released.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detect Anything 3D in the Wild</title>
<link>https://arxiv.org/abs/2504.07958</link>
<guid>https://arxiv.org/abs/2504.07958</guid>
<content:encoded><![CDATA[
arXiv:2504.07958v3 Announce Type: replace 
Abstract: Despite the success of deep learning in close-set 3D object detection, existing approaches struggle with zero-shot generalization to novel objects and camera configurations. We introduce DetAny3D, a promptable 3D detection foundation model capable of detecting any novel object under arbitrary camera configurations using only monocular inputs. Training a foundation model for 3D detection is fundamentally constrained by the limited availability of annotated 3D data, which motivates DetAny3D to leverage the rich prior knowledge embedded in extensively pre-trained 2D foundation models to compensate for this scarcity. To effectively transfer 2D knowledge to 3D, DetAny3D incorporates two core modules: the 2D Aggregator, which aligns features from different 2D foundation models, and the 3D Interpreter with Zero-Embedding Mapping, which stabilizes early training in 2D-to-3D knowledge transfer. Experimental results validate the strong generalization of our DetAny3D, which not only achieves state-of-the-art performance on unseen categories and novel camera configurations, but also surpasses most competitors on in-domain data. DetAny3D sheds light on the potential of the 3D foundation model for diverse applications in real-world scenarios, e.g., rare object detection in autonomous driving, and demonstrates promise for further exploration of 3D-centric tasks in open-world settings. More visualization results can be found at our code repository.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ST-Booster: An Iterative SpatioTemporal Perception Booster for Vision-and-Language Navigation in Continuous Environments</title>
<link>https://arxiv.org/abs/2504.09843</link>
<guid>https://arxiv.org/abs/2504.09843</guid>
<content:encoded><![CDATA[
arXiv:2504.09843v2 Announce Type: replace 
Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires agents to navigate unknown, continuous spaces based on natural language instructions. Compared to discrete settings, VLN-CE poses two core perception challenges. First, the absence of predefined observation points leads to heterogeneous visual memories and weakened global spatial correlations. Second, cumulative reconstruction errors in three-dimensional scenes introduce structural noise, impairing local feature perception. To address these challenges, this paper proposes ST-Booster, an iterative spatiotemporal booster that enhances navigation performance through multi-granularity perception and instruction-aware reasoning. ST-Booster consists of three key modules -- Hierarchical SpatioTemporal Encoding (HSTE), Multi-Granularity Aligned Fusion (MGAF), and ValueGuided Waypoint Generation (VGWG). HSTE encodes long-term global memory using topological graphs and captures shortterm local details via grid maps. MGAF aligns these dualmap representations with instructions through geometry-aware knowledge fusion. The resulting representations are iteratively refined through pretraining tasks. During reasoning, VGWG generates Guided Attention Heatmaps (GAHs) to explicitly model environment-instruction relevance and optimize waypoint selection. Extensive comparative experiments and performance analyses are conducted, demonstrating that ST-Booster outperforms existing state-of-the-art methods, particularly in complex, disturbance-prone environments.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldMem: Long-term Consistent World Simulation with Memory</title>
<link>https://arxiv.org/abs/2504.12369</link>
<guid>https://arxiv.org/abs/2504.12369</guid>
<content:encoded><![CDATA[
arXiv:2504.12369v2 Announce Type: replace 
Abstract: World simulation has gained increasing popularity due to its ability to model virtual environments and predict the consequences of actions. However, the limited temporal context window often leads to failures in maintaining long-term consistency, particularly in preserving 3D spatial consistency. In this work, we present WorldMem, a framework that enhances scene generation with a memory bank consisting of memory units that store memory frames and states (e.g., poses and timestamps). By employing a memory attention mechanism that effectively extracts relevant information from these memory frames based on their states, our method is capable of accurately reconstructing previously observed scenes, even under significant viewpoint or temporal gaps. Furthermore, by incorporating timestamps into the states, our framework not only models a static world but also captures its dynamic evolution over time, enabling both perception and interaction within the simulated world. Extensive experiments in both virtual and real scenarios validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReSpace: Text-Driven 3D Indoor Scene Synthesis and Editing with Preference Alignment</title>
<link>https://arxiv.org/abs/2506.02459</link>
<guid>https://arxiv.org/abs/2506.02459</guid>
<content:encoded><![CDATA[
arXiv:2506.02459v4 Announce Type: replace 
Abstract: Scene synthesis and editing has emerged as a promising direction in computer graphics. Current trained approaches for 3D indoor scenes either oversimplify object semantics through one-hot class encodings (e.g., 'chair' or 'table'), require masked diffusion for editing, ignore room boundaries, or rely on floor plan renderings that fail to capture complex layouts. LLM-based methods enable richer semantics via natural language (e.g., 'modern studio with light wood furniture'), but lack editing functionality, are limited to rectangular layouts, or rely on weak spatial reasoning from implicit world models. We introduce ReSpace, a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models. Our approach features a compact structured scene representation with explicit room boundaries that enables asset-agnostic deployment and frames scene editing as a next-token prediction task. We leverage a dual-stage training approach combining supervised fine-tuning and preference alignment, enabling a specially trained language model for object addition that accounts for user instructions, spatial geometry, object semantics, and scene-level composition. For scene editing, we employ a zero-shot LLM to handle object removal and prompts for addition. We further introduce a voxelization-based evaluation capturing fine-grained geometry beyond 3D bounding boxes. Experimental results surpass state-of-the-art on addition and achieve superior human-perceived quality on full scene synthesis.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing Hour-Scale Video Training for Long Video-Language Understanding</title>
<link>https://arxiv.org/abs/2506.05332</link>
<guid>https://arxiv.org/abs/2506.05332</guid>
<content:encoded><![CDATA[
arXiv:2506.05332v2 Announce Type: replace 
Abstract: Recent long-form video-language understanding benchmarks have driven progress in video large multimodal models (Video-LMMs). However, the scarcity of well-annotated long videos has left the training of hour-long Video-LMMs underexplored. To close this gap, we present VideoMarathon, a large-scale hour-long video instruction-following dataset. This dataset includes around 9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60 minutes per video. Specifically, it contains 3.3M high-quality QA pairs, spanning six fundamental topics: temporality, spatiality, object, action, scene, and event. Compared to existing video instruction datasets, VideoMarathon significantly extends training video durations up to 1 hour, and supports 22 diverse tasks requiring both short- and long-term video comprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and efficient Video-LMM for hour-scale video-language modeling. It enables hour-long video training and inference at 1-FPS sampling by leveraging a memory augmentation module, which adaptively integrates question-relevant and spatiotemporally informative semantics from the cached full video context. In our experiments, Hour-LLaVA achieves the best performance on multiple representative long video-language benchmarks, demonstrating the high quality of the VideoMarathon dataset and the superiority of the Hour-LLaVA model.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Calibrating BCIs: Ranking and Recovery of Mental Targets Without Labels</title>
<link>https://arxiv.org/abs/2506.11151</link>
<guid>https://arxiv.org/abs/2506.11151</guid>
<content:encoded><![CDATA[
arXiv:2506.11151v2 Announce Type: replace 
Abstract: We consider the problem of recovering a mental target (e.g., an image of a face) that a participant has in mind from paired EEG (i.e., brain responses) and image (i.e., perceived faces) data collected during interactive sessions without access to labeled information. The problem has been previously explored with labeled data but not via self-calibration, where labeled data is unavailable. Here, we present the first framework and an algorithm, CURSOR, that learns to recover unknown mental targets without access to labeled data or pre-trained decoders. Our experiments on naturalistic images of faces demonstrate that CURSOR can (1) predict image similarity scores that correlate with human perceptual judgments without any label information, (2) use these scores to rank stimuli against an unknown mental target, and (3) generate new stimuli indistinguishable from the unknown mental target (validated via a user study, N=53).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Urban Scenes</title>
<link>https://arxiv.org/abs/2506.19117</link>
<guid>https://arxiv.org/abs/2506.19117</guid>
<content:encoded><![CDATA[
arXiv:2506.19117v2 Announce Type: replace 
Abstract: Existing approaches to 3D semantic urban scene generation predominantly rely on voxel-based representations, which are bound by fixed resolution, challenging to edit, and memory-intensive in their dense form. In contrast, we advocate for a primitive-based paradigm where urban scenes are represented using compact, semantically meaningful 3D elements that are easy to manipulate and compose. To this end, we introduce PrITTI, a latent diffusion model that leverages vectorized object primitives and rasterized ground surfaces for generating diverse, controllable, and editable 3D semantic urban scenes. This hybrid representation yields a structured latent space that facilitates object- and ground-level manipulation. Experiments on KITTI-360 show that primitive-based representations unlock the full capabilities of diffusion transformers, achieving state-of-the-art 3D scene generation quality with lower memory requirements, faster inference, and greater editability than voxel-based methods. Beyond generation, PrITTI supports a range of downstream applications, including scene editing, inpainting, outpainting, and photo-realistic street-view synthesis. Code and models are publicly available at $\href{https://raniatze.github.io/pritti/}{https://raniatze.github.io/pritti}$.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation</title>
<link>https://arxiv.org/abs/2507.01463</link>
<guid>https://arxiv.org/abs/2507.01463</guid>
<content:encoded><![CDATA[
arXiv:2507.01463v4 Announce Type: replace 
Abstract: Instance segmentation of novel objects instances in RGB images, given some example images for each object, is a well known problem in computer vision. Designing a model general enough to be employed for all kinds of novel objects without (re-) training has proven to be a difficult task. To handle this, we present a new training-free framework, called: Novel Object Cyclic Threshold based Instance Segmentation (NOCTIS). NOCTIS integrates two pre-trained models: Grounded-SAM 2 for object proposals with precise bounding boxes and corresponding segmentation masks; and DINOv2 for robust class and patch embeddings, due to its zero-shot capabilities. Internally, the proposal-object matching is realized by determining an object matching score based on the similarity of the class embeddings and the average maximum similarity of the patch embeddings with a new cyclic thresholding (CT) mechanism that mitigates unstable matches caused by repetitive textures or visually similar patterns. Beyond CT, NOCTIS introduces: (i) an appearance score that is unaffected by object selection bias; (ii) the usage of the average confidence of the proposals' bounding box and mask as a scoring component; and (iii) an RGB-only pipeline that performs even better than RGB-D ones. We empirically show that NOCTIS, without further training/fine tuning, outperforms the best RGB and RGB-D methods regarding the mean AP score on the seven core datasets of the BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects" task.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ov3R: Open-Vocabulary Semantic 3D Reconstruction from RGB Videos</title>
<link>https://arxiv.org/abs/2507.22052</link>
<guid>https://arxiv.org/abs/2507.22052</guid>
<content:encoded><![CDATA[
arXiv:2507.22052v2 Announce Type: replace 
Abstract: We present Ov3R, a novel framework for open-vocabulary semantic 3D reconstruction from RGB video streams, designed to advance Spatial AI. The system features two key components: CLIP3R, a CLIP-informed 3D reconstruction module that predicts dense point maps from overlapping clips while embedding object-level semantics; and 2D-3D OVS, a 2D-3D open-vocabulary semantic module that lifts 2D features into 3D by learning fused descriptors integrating spatial, geometric, and semantic cues. Unlike prior methods, Ov3R incorporates CLIP semantics directly into the reconstruction process, enabling globally consistent geometry and fine-grained semantic alignment. Our framework achieves state-of-the-art performance in both dense 3D reconstruction and open-vocabulary 3D segmentation, marking a step forward toward real-time, semantics-aware Spatial AI.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias Beyond Demographics: Probing Decision Boundaries in Black-Box LVLMs via Counterfactual VQA</title>
<link>https://arxiv.org/abs/2508.03079</link>
<guid>https://arxiv.org/abs/2508.03079</guid>
<content:encoded><![CDATA[
arXiv:2508.03079v2 Announce Type: replace 
Abstract: Recent advances in large vision-language models (LVLMs) have amplified concerns about fairness, yet existing evaluations remain confined to demographic attributes and often conflate fairness with refusal behavior. This paper broadens the scope of fairness by introducing a counterfactual VQA benchmark that probes the decision boundaries of closed-source LVLMs under controlled context shifts. Each image pair differs in a single visual attribute that has been validated as irrelevant to the question, enabling ground-truth-free and refusal-aware analysis of reasoning stability. Comprehensive experiments reveal that non-demographic attributes, such as environmental context or social behavior, distort LVLM decision-making more strongly than demographic ones. Moreover, instruction-based debiasing shows limited effectiveness and can even amplify these asymmetries, whereas exposure to a small number of human norm validated examples from our benchmark encourages more consistent and balanced responses, highlighting its potential not only as an evaluative framework but also as a means for understanding and improving model behavior. Together, these results provide a practial basis for auditing contextual biases even in black-box LVLMs and contribute to more transparent and equitable multimodal reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</title>
<link>https://arxiv.org/abs/2508.03692</link>
<guid>https://arxiv.org/abs/2508.03692</guid>
<content:encoded><![CDATA[
arXiv:2508.03692v3 Announce Type: replace 
Abstract: Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering One-Step Diffusion Model with Fidelity-Rich Decoder for Fast Image Compression</title>
<link>https://arxiv.org/abs/2508.04979</link>
<guid>https://arxiv.org/abs/2508.04979</guid>
<content:encoded><![CDATA[
arXiv:2508.04979v2 Announce Type: replace 
Abstract: Diffusion-based image compression has demonstrated impressive perceptual performance. However, it suffers from two critical drawbacks: (1) excessive decoding latency due to multi-step sampling, and (2) poor fidelity resulting from over-reliance on generative priors. To address these issues, we propose SODEC, a novel single-step diffusion image compression model. We argue that in image compression, a sufficiently informative latent renders multi-step refinement unnecessary. Based on this insight, we leverage a pre-trained VAE-based model to produce latents with rich information, and replace the iterative denoising process with a single-step decoding. Meanwhile, to improve fidelity, we introduce the fidelity guidance module, encouraging output that is faithful to the original image. Furthermore, we design the rate annealing training strategy to enable effective training under extremely low bitrates. Extensive experiments show that SODEC significantly outperforms existing methods, achieving superior rate-distortion-perception performance. Moreover, compared to previous diffusion-based compression models, SODEC improves decoding speed by more than 20$\times$. Code is released at: https://github.com/zhengchen1999/SODEC.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal LLMs See Sentiment</title>
<link>https://arxiv.org/abs/2508.16873</link>
<guid>https://arxiv.org/abs/2508.16873</guid>
<content:encoded><![CDATA[
arXiv:2508.16873v2 Announce Type: replace 
Abstract: Understanding how visual content communicates sentiment is critical in an era where online interaction is increasingly dominated by this kind of media on social platforms. However, this remains a challenging problem, as sentiment perception is closely tied to complex, scene-level semantics. In this paper, we propose an original framework, MLLMsent, to investigate the sentiment reasoning capabilities of Multimodal Large Language Models (MLLMs) through three perspectives: (1) using those MLLMs for direct sentiment classification from images; (2) associating them with pre-trained LLMs for sentiment analysis on automatically generated image descriptions; and (3) fine-tuning the LLMs on sentiment-labeled image descriptions. Experiments on a recent and established benchmark demonstrate that our proposal, particularly the fine-tuned approach, achieves state-of-the-art results outperforming Lexicon-, CNN-, and Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively, across different levels of evaluators' agreement and sentiment polarity categories. Remarkably, in a cross-dataset test, without any training on these new data, our model still outperforms, by up to 8.26%, the best runner-up, which has been trained directly on them. These results highlight the potential of the proposed visual reasoning scheme for advancing affective computing, while also establishing new benchmarks for future research.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems</title>
<link>https://arxiv.org/abs/2509.06996</link>
<guid>https://arxiv.org/abs/2509.06996</guid>
<content:encoded><![CDATA[
arXiv:2509.06996v5 Announce Type: replace 
Abstract: Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether advanced vision language models (VLMs) share this resilience. We construct two psychophysics inspired benchmarks across distinct writing systems, Chinese logographs and English alphabetic words, by splicing, recombining, and overlaying glyphs to yield ''visible but unreadable'' stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations</title>
<link>https://arxiv.org/abs/2509.26004</link>
<guid>https://arxiv.org/abs/2509.26004</guid>
<content:encoded><![CDATA[
arXiv:2509.26004v2 Announce Type: replace 
Abstract: Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations $\unicode{x2013}$ natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations in a weakly-supervised regime. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations. Code and data can be found at https://fpv-iplab.github.io/WISH.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation</title>
<link>https://arxiv.org/abs/2509.26219</link>
<guid>https://arxiv.org/abs/2509.26219</guid>
<content:encoded><![CDATA[
arXiv:2509.26219v2 Announce Type: replace 
Abstract: Dataset distillation has emerged as a promising paradigm that synthesizes compact, informative datasets capable of retaining the knowledge of large-scale counterparts, thereby addressing the substantial computational and storage burdens of modern model training. Conventional approaches typically rely on dense pixel-level representations, which introduce redundancy and are difficult to scale up. In this work, we propose GSDD, a novel and efficient sparse representation for dataset distillation based on 2D Gaussians. Instead of representing all pixels equally, GSDD encodes critical discriminative information in a distilled image using only a small number of Gaussian primitives. This sparse representation could improve dataset diversity under the same storage budget, enhancing coverage of difficult samples and boosting distillation performance. To ensure both efficiency and scalability, we adapt CUDA-based splatting operators for parallel inference and training, enabling high-quality rendering with minimal computational and memory overhead. Our method is simple yet effective, broadly applicable to different distillation pipelines, and highly scalable. Experiments show that GSDD achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet subsets, while remaining highly efficient encoding and decoding cost. Our code is available at https://github.com/j-cyoung/GSDatasetDistillation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROGR: Relightable 3D Objects using Generative Relighting</title>
<link>https://arxiv.org/abs/2510.03163</link>
<guid>https://arxiv.org/abs/2510.03163</guid>
<content:encoded><![CDATA[
arXiv:2510.03163v2 Announce Type: replace 
Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting. The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately. The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. We evaluate our approach on the established TensoIR and Stanford-ORB datasets, where it improves upon the state-of-the-art on most metrics, and showcase our approach on real-world object captures.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.16295</link>
<guid>https://arxiv.org/abs/2510.16295</guid>
<content:encoded><![CDATA[
arXiv:2510.16295v2 Announce Type: replace 
Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in evaluating membership inference attacks (MIA) against large vision-language models (LVLMs). While prior work has reported high attack success rates, our analysis suggests that these results often arise from detecting distributional bias introduced during dataset construction rather than from identifying true membership status. To address this issue, we introduce a controlled benchmark of 6{,}000 images where the distributions of member and non-member samples are carefully balanced, and ground-truth membership labels are provided across three distinct training stages. Experiments using OpenLVLM-MIA demonstrated that the performance of state-of-the-art MIA methods approached chance-level. OpenLVLM-MIA, designed to be transparent and unbiased benchmark, clarifies certain limitations of MIA research on LVLMs and provides a solid foundation for developing stronger privacy-preserving techniques.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</title>
<link>https://arxiv.org/abs/2511.03317</link>
<guid>https://arxiv.org/abs/2511.03317</guid>
<content:encoded><![CDATA[
arXiv:2511.03317v2 Announce Type: replace 
Abstract: Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIDEN: Design and Pilot Study of an AI Assistant for the Visually Impaired</title>
<link>https://arxiv.org/abs/2511.06080</link>
<guid>https://arxiv.org/abs/2511.06080</guid>
<content:encoded><![CDATA[
arXiv:2511.06080v3 Announce Type: replace 
Abstract: This paper presents AIDEN, an artificial intelligence-based assistant designed to enhance the autonomy and daily quality of life of visually impaired individuals, who often struggle with object identification, text reading, and navigation in unfamiliar environments. Existing solutions such as screen readers or audio-based assistants facilitate access to information but frequently lead to auditory overload and raise privacy concerns in open environments. AIDEN addresses these limitations with a hybrid architecture that integrates You Only Look Once (YOLO) for real-time object detection and a Large Language and Vision Assistant (LLaVA) for scene description and Optical Character Recognition (OCR). A key novelty of the system is a continuous haptic guidance mechanism based on a Geiger-counter metaphor, which supports object centering without occupying the auditory channel, while privacy is preserved by ensuring that no personal data are stored. Empirical evaluations with visually impaired participants assessed perceived ease of use and acceptance using the Technology Acceptance Model (TAM). Results indicate high user satisfaction, particularly regarding intuitiveness and perceived autonomy. Moreover, the ``Find an Object'' achieved effective real-time performance. These findings provide promising evidence that multimodal haptic-visual feedback can improve daily usability and independence compared to traditional audio-centric methods, motivating larger-scale clinical validations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Otter: Mitigating Background Distractions of Wide-Angle Few-Shot Action Recognition with Enhanced RWKV</title>
<link>https://arxiv.org/abs/2511.06741</link>
<guid>https://arxiv.org/abs/2511.06741</guid>
<content:encoded><![CDATA[
arXiv:2511.06741v5 Announce Type: replace 
Abstract: Wide-angle videos in few-shot action recognition (FSAR) effectively express actions within specific scenarios. However, without a global understanding of both subjects and background, recognizing actions in such samples remains challenging because of the background distractions. Receptance Weighted Key Value (RWKV), which learns interaction between various dimensions, shows promise for global modeling. While directly applying RWKV to wide-angle FSAR may fail to highlight subjects due to excessive background information. Additionally, temporal relation degraded by frames with similar backgrounds is difficult to reconstruct, further impacting performance. Therefore, we design the CompOund SegmenTation and Temporal REconstructing RWKV (Otter). Specifically, the Compound Segmentation Module~(CSM) is devised to segment and emphasize key patches in each frame, effectively highlighting subjects against background information. The Temporal Reconstruction Module (TRM) is incorporated into the temporal-enhanced prototype construction to enable bidirectional scanning, allowing better reconstruct temporal relation. Furthermore, a regular prototype is combined with the temporal-enhanced prototype to simultaneously enhance subject emphasis and temporal modeling, improving wide-angle FSAR performance. Extensive experiments on benchmarks such as SSv2, Kinetics, UCF101, and HMDB51 demonstrate that Otter achieves state-of-the-art performance. Extra evaluation on the VideoBadminton dataset further validates the superiority of Otter in wide-angle FSAR.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering</title>
<link>https://arxiv.org/abs/2511.08294</link>
<guid>https://arxiv.org/abs/2511.08294</guid>
<content:encoded><![CDATA[
arXiv:2511.08294v2 Announce Type: replace 
Abstract: Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.09809</link>
<guid>https://arxiv.org/abs/2511.09809</guid>
<content:encoded><![CDATA[
arXiv:2511.09809v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer</title>
<link>https://arxiv.org/abs/2511.13208</link>
<guid>https://arxiv.org/abs/2511.13208</guid>
<content:encoded><![CDATA[
arXiv:2511.13208v2 Announce Type: replace 
Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames. Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation. Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a 6.0 mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video based approaches, while offering significant gains in efficiency. Project page: https://github.com/zgspose/PAVENet.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions</title>
<link>https://arxiv.org/abs/2511.17722</link>
<guid>https://arxiv.org/abs/2511.17722</guid>
<content:encoded><![CDATA[
arXiv:2511.17722v2 Announce Type: replace 
Abstract: Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives</title>
<link>https://arxiv.org/abs/2511.18507</link>
<guid>https://arxiv.org/abs/2511.18507</guid>
<content:encoded><![CDATA[
arXiv:2511.18507v2 Announce Type: replace 
Abstract: Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs</title>
<link>https://arxiv.org/abs/2511.20515</link>
<guid>https://arxiv.org/abs/2511.20515</guid>
<content:encoded><![CDATA[
arXiv:2511.20515v3 Announce Type: replace 
Abstract: Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContourDiff: Unpaired Medical Image Translation with Structural Consistency</title>
<link>https://arxiv.org/abs/2403.10786</link>
<guid>https://arxiv.org/abs/2403.10786</guid>
<content:encoded><![CDATA[
arXiv:2403.10786v3 Announce Type: replace-cross 
Abstract: Accurately translating medical images between different modalities, such as Computed Tomography (CT) to Magnetic Resonance Imaging (MRI), has numerous downstream clinical and machine learning applications. While several methods have been proposed to achieve this, they often prioritize perceptual quality with respect to output domain features over preserving anatomical fidelity. However, maintaining anatomy during translation is essential for many tasks, e.g., when leveraging masks from the input domain to develop a segmentation model with images translated to the output domain. To address these challenges, we propose ContourDiff with Spatially Coherent Guided Diffusion (SCGD), a novel framework that leverages domain-invariant anatomical contour representations of images. These representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. We introduce a diffusion model that converts contour representations of images from arbitrary input domains into images in the output domain of interest. By applying the contour as a constraint at every diffusion sampling step, we ensure the preservation of anatomical content. We evaluate our method on challenging lumbar spine and hip-and-thigh CT-to-MRI translation tasks, via (1) the performance of segmentation models trained on translated images applied to real MRIs, and (2) the foreground FID and KID of translated images with respect to real MRIs. Our method outperforms other unpaired image translation methods by a significant margin across almost all metrics and scenarios. Moreover, it achieves this without the need to access any input domain information during training and we further verify its zero-shot capability, showing that a model trained on one anatomical region can be directly applied to unseen regions without retraining (GitHub: https://github.com/mazurowski-lab/ContourDiff).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniBench: Towards The Future of Universal Omni-Language Models</title>
<link>https://arxiv.org/abs/2409.15272</link>
<guid>https://arxiv.org/abs/2409.15272</guid>
<content:encoded><![CDATA[
arXiv:2409.15272v5 Announce Type: replace-cross 
Abstract: Recent advancements in multimodal large language models (MLLMs) have focused on integrating multiple modalities, yet their ability to simultaneously process and reason across different inputs remains underexplored. We introduce OmniBench, a novel benchmark designed to evaluate models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. We define language models capable of such tri-modal processing as omni-language models (OLMs). OmniBench features high-quality human annotations that require integrated understanding across all modalities. Our evaluation reveals that: i) open-source OLMs show significant limitations in instruction-following and reasoning in tri-modal contexts; and ii) most baseline models perform poorly (around 50% accuracy) even with textual alternatives to image/audio inputs. To address these limitations, we develop OmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. We advocate for developing more robust tri-modal integration techniques and training strategies to enhance OLM performance. Codes and data could be found at our repo (https://github.com/multimodal-art-projection/OmniBench).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaxSup: Overcoming Representation Collapse in Label Smoothing</title>
<link>https://arxiv.org/abs/2502.15798</link>
<guid>https://arxiv.org/abs/2502.15798</guid>
<content:encoded><![CDATA[
arXiv:2502.15798v3 Announce Type: replace-cross 
Abstract: Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS. Code is available at: https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image-Based Relocalization and Alignment for Long-Term Monitoring of Dynamic Underwater Environments</title>
<link>https://arxiv.org/abs/2503.04096</link>
<guid>https://arxiv.org/abs/2503.04096</guid>
<content:encoded><![CDATA[
arXiv:2503.04096v2 Announce Type: replace-cross 
Abstract: Effective monitoring of underwater ecosystems is crucial for tracking environmental changes, guiding conservation efforts, and ensuring long-term ecosystem health. However, automating underwater ecosystem management with robotic platforms remains challenging due to the complexities of underwater imagery, which pose significant difficulties for traditional visual localization methods. We propose an integrated pipeline that combines Visual Place Recognition (VPR), feature matching, and image segmentation on video-derived images. This method enables robust identification of revisited areas, estimation of rigid transformations, and downstream analysis of ecosystem changes. Furthermore, we introduce the SQUIDLE+ VPR Benchmark-the first large-scale underwater VPR benchmark designed to leverage an extensive collection of unstructured data from multiple robotic platforms, spanning time intervals from days to years. The dataset encompasses diverse trajectories, arbitrary overlap and diverse seafloor types captured under varying environmental conditions, including differences in depth, lighting, and turbidity. Our code is available at: https://github.com/bev-gorry/underloc
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Walk Before You Dance: High-fidelity and Editable Dance Synthesis via Generative Masked Motion Prior</title>
<link>https://arxiv.org/abs/2504.04634</link>
<guid>https://arxiv.org/abs/2504.04634</guid>
<content:encoded><![CDATA[
arXiv:2504.04634v3 Announce Type: replace-cross 
Abstract: Recent advances in dance generation have enabled the automatic synthesis of 3D dance motions. However, existing methods still face significant challenges in simultaneously achieving high realism, precise dance-music synchronization, diverse motion expression, and physical plausibility. To address these limitations, we propose a novel approach that leverages a generative masked text-to-motion model as a distribution prior to learn a probabilistic mapping from diverse guidance signals, including music, genre, and pose, into high-quality dance motion sequences. Our framework also supports semantic motion editing, such as motion inpainting and body part modification. Specifically, we introduce a multi-tower masked motion model that integrates a text-conditioned masked motion backbone with two parallel, modality-specific branches: a music-guidance tower and a pose-guidance tower. The model is trained using synchronized and progressive masked training, which allows effective infusion of the pretrained text-to-motion prior into the dance synthesis process while enabling each guidance branch to optimize independently through its own loss function, mitigating gradient interference. During inference, we introduce classifier-free logits guidance and pose-guided token optimization to strengthen the influence of music, genre, and pose signals. Extensive experiments demonstrate that our method sets a new state of the art in dance generation, significantly advancing both the quality and editability over existing approaches. Project Page available at https://foram-s1.github.io/DanceMosaic/
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeLU: Variance-enhanced Learning Unit for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2504.15051</link>
<guid>https://arxiv.org/abs/2504.15051</guid>
<content:encoded><![CDATA[
arXiv:2504.15051v2 Announce Type: replace-cross 
Abstract: Activation functions play a critical role in deep neural networks by shaping gradient flow, optimization stability, and generalization. While ReLU remains widely used due to its simplicity, it suffers from gradient sparsity and dead-neuron issues and offers no adaptivity to input statistics. Smooth alternatives such as Swish and GELU improve gradient propagation but still apply a fixed transformation regardless of the activation distribution. In this paper, we propose VeLU, a Variance-enhanced Learning Unit that introduces variance-aware and distributionally aligned nonlinearity through a principled combination of ArcTan-ArcSin transformations, adaptive scaling, and Wasserstein-2 regularization (Optimal Transport). This design enables VeLU to modulate its response based on local activation variance, mitigate internal covariate shift at the activation level, and improve training stability without adding learnable parameters or architectural overhead. Extensive experiments across six deep neural networks show that VeLU outperforms ReLU, ReLU6, Swish, and GELU on 12 vision benchmarks. The implementation of VeLU is publicly available in GitHub.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation</title>
<link>https://arxiv.org/abs/2507.14270</link>
<guid>https://arxiv.org/abs/2507.14270</guid>
<content:encoded><![CDATA[
arXiv:2507.14270v5 Announce Type: replace-cross 
Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that integrates non-linear activation and linear transformation into a single trainable expression. The APTx Neuron is derived from the APTx activation function, thereby eliminating the need for separate activation layers and making the architecture both optimization-efficient and elegant. The proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters $\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our APTx Neuron-based architecture on the MNIST dataset, achieving up to $96.69\%$ test accuracy within 11 epochs using approximately 332K trainable parameters. The results highlight the superior expressiveness and training efficiency of the APTx Neuron compared to traditional neurons, pointing toward a new paradigm in unified neuron design and the architectures built upon it. Source code is available at https://github.com/mr-ravin/aptx_neuron.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rainbow Noise: Stress-Testing Multimodal Harmful-Meme Detectors on LGBTQ Content</title>
<link>https://arxiv.org/abs/2507.19551</link>
<guid>https://arxiv.org/abs/2507.19551</guid>
<content:encoded><![CDATA[
arXiv:2507.19551v3 Announce Type: replace-cross 
Abstract: Hateful memes aimed at LGBTQ\,+ communities often evade detection by tweaking either the caption, the image, or both. We build the first robustness benchmark for this setting, pairing four realistic caption attacks with three canonical image corruptions and testing all combinations on the PrideMM dataset. Two state-of-the-art detectors, MemeCLIP and MemeBLIP2, serve as case studies, and we introduce a lightweight \textbf{Text Denoising Adapter (TDA)} to enhance the latter's resilience. Across the grid, MemeCLIP degrades more gently, while MemeBLIP2 is particularly sensitive to the caption edits that disrupt its language processing. However, the addition of the TDA not only remedies this weakness but makes MemeBLIP2 the most robust model overall. Ablations reveal that all systems lean heavily on text, but architectural choices and pre-training data significantly impact robustness. Our benchmark exposes where current multimodal safety models crack and demonstrates that targeted, lightweight modules like the TDA offer a powerful path towards stronger defences.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIMU: Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Coverage</title>
<link>https://arxiv.org/abs/2508.02443</link>
<guid>https://arxiv.org/abs/2508.02443</guid>
<content:encoded><![CDATA[
arXiv:2508.02443v2 Announce Type: replace-cross 
Abstract: We introduce Primitive-based Representations of Uncertainty (PRIMU), a post-hoc uncertainty estimation (UE) framework for Gaussian Splatting (GS). Reliable UE is essential for deploying GS in safety-critical domains such as robotics and medicine. Existing approaches typically estimate Gaussian-primitive variances and rely on the rendering process to obtain pixel-wise uncertainties. In contrast, we construct primitive-level representations of error and visibility/coverage from training views, capturing interpretable uncertainty information. These representations are obtained by projecting view-dependent training errors and coverage statistics onto the primitives. Uncertainties for novel views are inferred by rendering these primitive-level representations, producing uncertainty feature maps, which are aggregate through pixel-wise regression on holdout data. We analyze combinations of uncertainty feature maps and regression models to understand how their interactions affect prediction accuracy and generalization. PRIMU also enables an effective active view selection strategy by directly leveraging these uncertainty feature maps. Additionally, we study the effect of separating splatting into foreground and background regions. Our estimates show strong correlations with true errors, outperforming state-of-the-art methods, especially for depth UE and foreground objects. Finally, our regression models show generalization capabilities to unseen scenes, enabling UE without additional holdout data.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction</title>
<link>https://arxiv.org/abs/2508.09200</link>
<guid>https://arxiv.org/abs/2508.09200</guid>
<content:encoded><![CDATA[
arXiv:2508.09200v2 Announce Type: replace-cross 
Abstract: To investigate the feasibility of zero-shot self-supervised learning reconstruction for reducing breath-hold times in magnetic resonance cholangiopancreatography (MRCP). Breath-hold MRCP was acquired from 11 healthy volunteers on 3T scanners using an incoherent k-space sampling pattern, leading to 14-second acquisition time and an acceleration factor of R=25. Zero-shot reconstruction was compared with parallel imaging of respiratory-triggered MRCP (338s, R=3) and compressed sensing reconstruction. For two volunteers, breath-hold scans (40s, R=6) were additionally acquired and retrospectively undersampled to R=25 to compute peak signal-to-noise ratio (PSNR). To address long zero-shot training time, the n+m full stages of the zero-shot learning were divided into two parts to reduce backpropagation depth during training: 1) n frozen stages initialized with n-stage pretrained network and 2) m trainable stages initialized either randomly or m-stage pretrained network. Efficiency of our approach was assessed by varying initialization strategies and the number of trainable stages using the retrospectively undersampled data. Zero-shot reconstruction significantly improved visual image quality over compressed sensing, particularly in SNR and ductal delineation, and achieved image quality comparable to that of successful respiratory-triggered acquisitions with regular breathing patterns. Improved initializations enhanced PSNR and reduced reconstruction time. Adjusting frozen/trainable configurations demonstrated that PSNR decreased only slightly from 38.25 dB (0/13) to 37.67 dB (12/1), while training time decreased up to 6.7-fold. Zero-shot learning delivers high-fidelity MRCP reconstructions with reduced breath-hold times, and the proposed partially trainable approach offers a practical solution for translation into time-constrained clinical workflows.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title>
<link>https://arxiv.org/abs/2508.19112</link>
<guid>https://arxiv.org/abs/2508.19112</guid>
<content:encoded><![CDATA[
arXiv:2508.19112v2 Announce Type: replace-cross 
Abstract: Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. Transformer-based models with self-supervised pretraining have achieved strong performance on in-distribution (ID) data but often generalize poorly on out-of-distribution (OOD) inputs. We investigate this behavior for lung cancer segmentation using an encoder-decoder model. Our encoder is a Swin Transformer pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans spanning cancerous and non-cancerous conditions, and the decoder was randomly initialized. This model was evaluated on an independent ID test set and four OOD scenarios, including chest CT cohorts (pulmonary embolism and negative COVID-19) and abdomen CT cohorts (kidney cancers and non-cancerous pancreas). OOD detection was performed at the scan level using RF-Deep, a random forest classifier applied to contextual tumor-anchored feature representations. We evaluated 920 3D CTs (172,650 images) and observed that RF-Deep achieved FPR95 values of 18.26% and 27.66% on the chest CT cohorts, and near-perfect detection (less than 0.1% FPR95) on the abdomen CT cohorts, consistently outperforming established OOD methods. These results demonstrate that our RF-Deep classifier provides a simple, lightweight, and effective approach for enhancing the reliability of segmentation models in clinical deployment.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast 3D Surrogate Modeling for Data Center Thermal Management</title>
<link>https://arxiv.org/abs/2511.11722</link>
<guid>https://arxiv.org/abs/2511.11722</guid>
<content:encoded><![CDATA[
arXiv:2511.11722v2 Announce Type: replace-cross 
Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and significantly speed up computations (20,000x), from hundreds of milliseconds to hours. This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Compression and Artifact Correction for Streaming Underwater Imaging Sonar</title>
<link>https://arxiv.org/abs/2511.13922</link>
<guid>https://arxiv.org/abs/2511.13922</guid>
<content:encoded><![CDATA[
arXiv:2511.13922v2 Announce Type: replace-cross 
Abstract: Real-time imaging sonar is crucial for underwater monitoring where optical sensing fails, but its use is limited by low uplink bandwidth and severe sonar-specific artifacts (speckle, motion blur, reverberation, acoustic shadows) affecting up to 98% of frames. We present SCOPE, a self-supervised framework that jointly performs compression and artifact correction without clean-noise pairs or synthetic assumptions. SCOPE combines (i) Adaptive Codebook Compression (ACC), which learns frequency-encoded latent representations tailored to sonar, with (ii) Frequency-Aware Multiscale Segmentation (FAMS), which decomposes frames into low-frequency structure and sparse high-frequency dynamics while suppressing rapidly fluctuating artifacts. A hedging training strategy further guides frequency-aware learning using low-pass proxy pairs generated without labels. Evaluated on months of in-situ ARIS sonar data, SCOPE achieves a structural similarity index (SSIM) of 0.77, representing a 40% improvement over prior self-supervised denoising baselines, at bitrates down to <= 0.0118 bpp. It reduces uplink bandwidth by more than 80% while improving downstream detection. The system runs in real time, with 3.1 ms encoding on an embedded GPU and 97 ms full multi-layer decoding on the server end. SCOPE has been deployed for months in three Pacific Northwest rivers to support real-time salmon enumeration and environmental monitoring in the wild. Results demonstrate that learning frequency-structured latents enables practical, low-bitrate sonar streaming with preserved signal details under real-world deployment conditions.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention</title>
<link>https://arxiv.org/abs/2511.18960</link>
<guid>https://arxiv.org/abs/2511.18960</guid>
<content:encoded><![CDATA[
arXiv:2511.18960v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Massively Multitask World Models for Continuous Control</title>
<link>https://arxiv.org/abs/2511.19584</link>
<guid>https://arxiv.org/abs/2511.19584</guid>
<content:encoded><![CDATA[
arXiv:2511.19584v2 Announce Type: replace-cross 
Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models</title>
<link>https://arxiv.org/abs/2511.12547</link>
<guid>https://arxiv.org/abs/2511.12547</guid>
<content:encoded><![CDATA[
<div> Keywords: generative diffusion models, fine-grained augmentation, classifier-free guidance, hierarchical guidance, dynamic confidence modulation<br /><br />Summary: Generative diffusion models are promising tools for data augmentation but struggle with fine-grained tasks due to their inability to capture subtle, category-specific details accurately. Traditional methods like text-based Classifier-Free Guidance (CFG) often lack the necessary precision, which can lead to misleading synthetic images and impair classifier performance. To overcome this, the paper introduces Hierarchically Guided Fine-grained Augmentation (HiGFA), a novel approach that leverages the temporal progression of diffusion sampling. HiGFA uses strong text and contour guidance during the early to middle sampling phases to set the overall scene, style, and structure. In the final stages, it switches to a specialized fine-grained classifier guidance, dynamically adjusting the guidance strength based on prediction confidence. This hierarchical and confidence-driven modulation balances the formation of global structure and the refinement of precise details. Experimental results on multiple fine-grained visual classification (FGVC) datasets confirm HiGFA’s effectiveness in generating diverse and faithful synthetic images that improve classifier performance. <div>
arXiv:2511.12547v4 Announce Type: replace 
Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Semi-Supervised Active Learning for Remote Sensing</title>
<link>https://arxiv.org/abs/2511.18058</link>
<guid>https://arxiv.org/abs/2511.18058</guid>
<content:encoded><![CDATA[
<div> Keywords: Hierarchical Semi-Supervised Active Learning, remote sensing, semi-supervised learning, active learning, sample selection<br /><br />Summary: The article addresses the challenge of limited labeled data in deep learning for remote sensing (RS) by proposing a Hierarchical Semi-Supervised Active Learning (HSSAL) framework. HSSAL combines semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) method into an iterative loop to enhance model performance. In each iteration, SSL improves the model by leveraging both labeled data and unlabeled data through weak-to-strong self-training, which enhances feature representation and uncertainty estimation. Subsequently, HAL selects the most informative unlabeled samples for labeling by employing a progressive clustering strategy that ensures scalability, diversity, and uncertainty in sample selection. This hierarchical approach boosts efficiency and representativeness in choosing samples to annotate. The framework is empirically validated on three popular remote sensing scene classification datasets: UCM, AID, and NWPU-RESISC45. Results demonstrate that HSSAL consistently outperforms baselines relying solely on either SSL or active learning. Notably, HSSAL achieves over 95% of the accuracy of fully supervised models with significantly reduced labeled data—only 8%, 4%, and 2% for the UCM, AID, and NWPU datasets, respectively—highlighting its efficiency in exploiting unlabeled data. The authors also commit to releasing the code publicly. <div>
arXiv:2511.18058v2 Announce Type: replace 
Abstract: The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pistachio: Towards Synthetic, Balanced, and Long-Form Video Anomaly Benchmarks</title>
<link>https://arxiv.org/abs/2511.19474</link>
<guid>https://arxiv.org/abs/2511.19474</guid>
<content:encoded><![CDATA[
<div> Video Anomaly Detection, Video Anomaly Understanding, Benchmark, Video Generation, Anomaly Detection

<br /><br />Summary:  
This paper addresses the limitations of current Video Anomaly Detection (VAD) benchmarks, which often lack scene diversity, balanced anomaly coverage, and the temporal complexity needed to reflect real-world scenarios reliably. It highlights the community’s shift towards Video Anomaly Understanding (VAU), which demands deeper semantic and causal reasoning but is challenging to benchmark due to the intensive manual annotation required. To overcome these challenges, the authors present Pistachio, a novel VAD/VAU benchmark generated entirely through a controlled, video generation-based pipeline. Leveraging state-of-the-art video generation models, Pistachio offers precise control over scenes, types of anomalies, and temporal narratives, thereby minimizing biases typical in datasets collected from the Internet. The pipeline incorporates scene-conditioned anomaly assignment, multi-step storyline generation, and a long-form synthesis approach that produces 41-second coherent videos with minimal human intervention. Experimental results demonstrate Pistachio’s scale, diversity, and temporal complexity, revealing new challenges for existing anomaly detection methods. The benchmark promotes further research into dynamic and multi-event anomaly understanding, pushing the field beyond static anomaly detection towards deeper comprehension of abnormal events in videos. <div>
arXiv:2511.19474v3 Announce Type: replace 
Abstract: Automatically detecting abnormal events in videos is crucial for modern autonomous systems, yet existing Video Anomaly Detection (VAD) benchmarks lack the scene diversity, balanced anomaly coverage, and temporal complexity needed to reliably assess real-world performance. Meanwhile, the community is increasingly moving toward Video Anomaly Understanding (VAU), which requires deeper semantic and causal reasoning but remains difficult to benchmark due to the heavy manual annotation effort it demands. In this paper, we introduce Pistachio, a new VAD/VAU benchmark constructed entirely through a controlled, generation-based pipeline. By leveraging recent advances in video generation models, Pistachio provides precise control over scenes, anomaly types, and temporal narratives, effectively eliminating the biases and limitations of Internet-collected datasets. Our pipeline integrates scene-conditioned anomaly assignment, multi-step storyline generation, and a temporally consistent long-form synthesis strategy that produces coherent 41-second videos with minimal human intervention. Extensive experiments demonstrate the scale, diversity, and complexity of Pistachio, revealing new challenges for existing methods and motivating future research on dynamic and multi-event anomaly understanding.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GigaWorld-0: World Models as Data Engine to Empower Embodied AI</title>
<link>https://arxiv.org/abs/2511.19861</link>
<guid>https://arxiv.org/abs/2511.19861</guid>
<content:encoded><![CDATA[
<div> Keyword: world models, Vision-Language-Action, video generation, 3D generative modeling, embodied AI<br /><br />Summary:  
1. The paper introduces GigaWorld-0, a unified world model framework designed specifically as a data engine for Vision-Language-Action (VLA) learning in embodied AI.  
2. GigaWorld-0 consists of two main components: GigaWorld-0-Video, which generates large-scale, diverse, and texture-rich video sequences with precise control over appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which provides 3D generative modeling combined with Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to maintain geometric consistency and physical realism.  
3. The integration and joint optimization of video and 3D components enable the scalable synthesis of data that is visually compelling, spatially coherent, physically plausible, and aligns with instructions.  
4. The efficient GigaTrain framework supports large-scale training by leveraging FP8 precision and sparse attention, significantly reducing memory and computational costs.  
5. Comprehensive evaluations demonstrate that models trained on GigaWorld-0 data, such as GigaBrain-0, perform strongly in real-world robotic tasks, showing improved generalization and task success without requiring any real-world training interaction. <div>
arXiv:2511.19861v2 Announce Type: replace 
Abstract: World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriSciQA: An Auto-Verified Dataset for Scientific Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.19899</link>
<guid>https://arxiv.org/abs/2511.19899</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.19899v2

Keywords: Vision-Language Models, Scientific Visual Question Answering, Dataset, Data Synthesis, Cross-modal Verification  

<br /><br />Summary:  
Large Vision-Language Models (LVLMs) have notable potential for scientific applications but currently face challenges with Scientific Visual Question Answering (SVQA), especially due to limited public, large-scale, and high-quality SVQA datasets. Prior attempts to generate datasets using LVLMs have resulted in systematic errors in question-answer pairs because of inherent model limitations and mismatches between figures and their textual contexts. To overcome these issues, the authors introduce a verification-centric Generate-then-Verify framework that first produces QA pairs using figure-associated text, then performs cross-modal consistency checks along with additional filtering to remove incorrect QA pairs. Using this approach, they create VeriSciQA, a large dataset containing 20,351 QA pairs covering 20 scientific domains and 12 figure types, designed to be a challenging benchmark for open-source SVQA models. Evaluation reveals a significant performance gap between top open-source models achieving 64% accuracy and a proprietary model at 82%. Furthermore, fine-tuning models on VeriSciQA results in consistent improvements on SVQA tasks, with gains scaling with dataset size and outperforming existing datasets. Human evaluation confirms higher correctness of VeriSciQA data. Overall, these results demonstrate that the framework’s scalable data expansion can significantly advance SVQA capabilities in the open-source community. <div>
arXiv:2511.19899v2 Announce Type: replace 
Abstract: Large Vision-Language Models (LVLMs) show promise for scientific applications, yet open-source models still struggle with Scientific Visual Question Answering (SVQA), namely answering questions about figures from scientific papers. A key bottleneck lies in the lack of public, large-scale, high-quality SVQA datasets. Although recent work uses LVLMs to synthesize data at scale, we identify systematic errors in their resulting QA pairs, stemming from LVLMs' inherent limitations and information asymmetry between figures and text. To address these challenges, we propose a verification-centric Generate-then-Verify framework that first generates QA pairs with figure-associated textual context, then applies cross-modal consistency checks against figures along with auxiliary filters to eliminate erroneous pairs. We instantiate this framework to curate VeriSciQA, a dataset of 20,351 QA pairs spanning 20 scientific domains and 12 figure types. VeriSciQA poses a challenging benchmark for open-source models, with a substantial accuracy gap between the leading open-source models (64%) and a proprietary model (82%). Moreover, models fine-tuned on VeriSciQA achieve consistent improvements on SVQA benchmarks, with performance gains that scale with data size and surpass models trained on existing datasets. Human evaluation further validates the superior correctness of VeriSciQA. Together, these evidences demonstrate that continued data expansion by our scalable framework can further advance SVQA capability in the open-source community.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Motion Perception of Binocular Vision Target with PID-CNN</title>
<link>https://arxiv.org/abs/2511.20332</link>
<guid>https://arxiv.org/abs/2511.20332</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D motion perception, PID neural networks, convolutional neural network, feature reuse, computational efficiency

<br /><br />Summary:  
This article presents the training of a neural network designed to perceive three-dimensional motion information from binocular vision targets, providing real-time 3D coordinates, velocity, and acceleration with fundamental spatiotemporal perception capabilities. The authors examined neural networks' ability to fit nonlinear problems using a PID (Proportional-Integral-Derivative) control perspective. A single-layer neural network is modeled as a combination of a second-order difference equation and a nonlinearity to solve local problems, while multilayer networks achieve the desired representation by stacking multiple such units. Reference principles for neural network design were analyzed, leading to the development of a relatively compact PID-based convolutional neural network comprising 17 layers and 413,000 parameters. The network incorporates a practical feature reuse strategy through concatenation and pooling operations. Training and testing were conducted on simulated datasets featuring randomly moving balls, and results demonstrated prediction accuracy nearing the theoretical limits imposed by input image resolution. The article discusses experimental outcomes, error analyses, current shortcomings, and avenues for future improvements. Finally, it highlights the benefits of high-dimensional convolution for enhancing computational efficiency and feature space utilization as well as the promising role of PID-informed mechanisms in implementing memory and attention functionalities in neural networks. <div>
arXiv:2511.20332v2 Announce Type: replace 
Abstract: This article trained a network for perceiving three-dimensional motion information of binocular vision target, which can provide real-time three-dimensional coordinate, velocity, and acceleration, and has a basic spatiotemporal perception capability. Understood the ability of neural networks to fit nonlinear problems from the perspective of PID. Considered a single-layer neural network as using a second-order difference equation and a nonlinearity to describe a local problem. Multilayer networks gradually transform the raw representation to the desired representation through multiple such combinations. Analysed some reference principles for designing neural networks. Designed a relatively small PID convolutional neural network, with a total of 17 layers and 413 thousand parameters. Implemented a simple but practical feature reuse method by concatenation and pooling. The network was trained and tested using the simulated randomly moving ball datasets, and the experimental results showed that the prediction accuracy was close to the upper limit that the input image resolution can represent. Analysed the experimental results and errors, as well as the existing shortcomings and possible directions for improvement. Finally, discussed the advantages of high-dimensional convolution in improving computational efficiency and feature space utilization. As well as the potential advantages of using PID information to implement memory and attention mechanisms.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward</title>
<link>https://arxiv.org/abs/2511.20561</link>
<guid>https://arxiv.org/abs/2511.20561</guid>
<content:encoded><![CDATA[
<div> Keywords: Unified Multimodal Models, Chain-of-Thought, reasoning generation, knowledge transfer, UniSandbox<br /><br />Summary: This paper addresses the critical question of whether understanding truly informs generation within Unified Multimodal Models. To analyze this relationship, the authors introduce UniSandbox, a decoupled evaluation framework combined with controlled synthetic datasets designed to prevent data leakage and enable granular analysis. Their findings reveal a notable gap between understanding and generation capabilities, particularly across two main areas: reasoning generation and knowledge transfer. For reasoning generation tasks, the study demonstrates that incorporating explicit Chain-of-Thought (CoT) processes in the understanding module effectively bridges the gap. Moreover, a self-training approach is shown to internalize this ability, allowing the model to perform implicit reasoning during generation. In terms of knowledge transfer, CoT aids the generative process by facilitating retrieval of newly learned knowledge. The work also discovers that query-based architectures inherently possess latent CoT-like features, which influence this knowledge transfer. Overall, UniSandbox offers valuable preliminary insights for creating future unified architectures and training strategies that genuinely close the understanding-generation divide. The authors provide code and data to support further research at the linked GitHub repository. <div>
arXiv:2511.20561v2 Announce Type: replace 
Abstract: Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation</title>
<link>https://arxiv.org/abs/2511.20635</link>
<guid>https://arxiv.org/abs/2511.20635</guid>
<content:encoded><![CDATA[
<div> Keywords: pre-trained video models, image generation, temporal coherence, iMontage, image manipulation<br /><br />Summary:<br /><br />1. This work introduces iMontage, a unified framework that adapts powerful pre-trained video models for versatile image generation tasks, capable of producing and consuming variable-length image sets.<br /><br />2. The core hypothesis is that incorporating the diverse and unconstrained content from image data into the temporal coherence framework of video models enables the generation of image sequences with natural transitions and an expansive dynamic range.<br /><br />3. iMontage employs a minimally invasive adaptation strategy combined with a specialized data curation and training paradigm to enhance the model’s image manipulation capabilities without compromising the original temporal motion priors.<br /><br />4. The framework supports a wide array of many-in-many-out image tasks, maintaining strong contextual consistency across images and enabling dynamic scene generation beyond conventional video model capabilities.<br /><br />5. Results demonstrate that iMontage excels in cross-image contextual consistency as well as producing scenes with richer dynamics, supporting its application in complex image editing and generation scenarios. The project homepage provides additional resources and examples: https://kr1sjfu.github.io/iMontage-web/. <div>
arXiv:2511.20635v2 Announce Type: replace 
Abstract: Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOTION: ML-Assisted On-Device Low-Latency Motion Recognition</title>
<link>https://arxiv.org/abs/2512.00008</link>
<guid>https://arxiv.org/abs/2512.00008</guid>
<content:encoded><![CDATA[
<div> Keywords: gesture recognition, triaxial accelerometer, AutoML, wearable device, neural network  

<br /><br />Summary:  
This study addresses the need for efficient, low-latency gesture recognition systems in wearable devices, particularly targeting medical monitoring applications such as fall detection and rehabilitation tracking. It focuses on utilizing data from triaxial accelerometer sensors to build motion-based models that are both lightweight and precise. The research employs Automated Machine Learning (AutoML) pipelines to automatically extract the most salient features from segmented sensor data, optimizing the model input for performance. Multiple lightweight machine learning algorithms are trained on these extracted features to compare their effectiveness in terms of accuracy, latency, and memory consumption. The experiments use the WeBe Band, a multi-sensor wearable device equipped with a capable microcontroller unit (MCU) that can execute gesture recognition directly on the device without external computation. Among the models evaluated, a neural network architecture offered the best compromise, providing high recognition accuracy while maintaining low latency and minimal memory usage. The study demonstrates that real-time, reliable gesture recognition is achievable on the WeBe Band, which holds promise for responsive, secure medical monitoring systems that demand quick and trustworthy movement tracking without false alarms. <div>
arXiv:2512.00008v1 Announce Type: new 
Abstract: The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tracking of movements while avoiding unwanted false alarms. This study presents an efficient solution on how to build very efficient motion-based models only using triaxial accelerometer sensors. We explore the capability of the AutoML pipelines to extract the most important features from the data segments. This approach also involves training multiple lightweight machine learning algorithms using the extracted features. We use WeBe Band, a multi-sensor wearable device that is equipped with a powerful enough MCU to effectively perform gesture recognition entirely on the device. Of the models explored, we found that the neural network provided the best balance between accuracy, latency, and memory use. Our results also demonstrate that reliable real-time gesture recognition can be achieved in WeBe Band, with great potential for real-time medical monitoring solutions that require a secure and fast response time.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions</title>
<link>https://arxiv.org/abs/2512.00042</link>
<guid>https://arxiv.org/abs/2512.00042</guid>
<content:encoded><![CDATA[
<div> Multimodal reasoning, supervised fine-tuning, curriculum data, vision language models, exam benchmarks<br /><br />Summary:<br /><br />1. The paper addresses multimodal reasoning in AI, focusing on the integration of visual contexts with language for rigorous testing through standardized exam questions.<br /><br />2. It highlights the underexplored role of data-centric approaches in improving vision-language reasoning, contrasting with recent algorithmic advances like reinforcement learning.<br /><br />3. The authors compile an extensive multimodal dataset of 161.4 million tokens, incorporating textbook question-solution pairs, aligned curriculum diagrams, and contextual materials to enhance the training process.<br /><br />4. Using this dataset, they fine-tune the Qwen-2.5VL-32B model with an optimized reasoning syntax called QMSA, demonstrating that supervised fine-tuning with high-quality data can approach performance levels of proprietary systems.<br /><br />5. Their fine-tuned model achieves 78.6% accuracy on the newly released YKSUniform benchmark, which standardizes 1,854 multimodal exam questions across 309 curriculum topics, coming within 1.0% of the top-performing Gemini 2.0 Flash model.<br /><br />6. The study emphasizes the critical importance of data composition and representational syntax in advancing multimodal AI capabilities and establishes a data-centric framework for developing open-weight vision-language models with near state-of-the-art performance. <div>
arXiv:2512.00042v1 Announce Type: new 
Abstract: Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.
  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.
  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.00060</link>
<guid>https://arxiv.org/abs/2512.00060</guid>
<content:encoded><![CDATA[
<div> Keywords: PEFT-DML, deep metric learning, multi-modal 3D object detection, parameter-efficient, sensor dropout<br /><br />Summary:  
This study presents PEFT-DML, a novel deep metric learning framework designed for efficient and robust multi-modal 3D object detection in autonomous driving scenarios. PEFT-DML addresses the limitation of conventional models that assume a fixed set of sensors by mapping various modalities such as LiDAR, radar, camera, IMU, and GNSS into a shared latent space. This shared representation enables the system to perform reliable detection even when some sensors fail or when encountering previously unseen combinations of sensor modalities. The framework integrates Low-Rank Adaptation (LoRA) and adapter layers, which together improve training efficiency by reducing the number of trainable parameters without sacrificing accuracy. PEFT-DML also enhances robustness to challenges such as fast vehicle motion, varying weather conditions, and domain shifts often present in real-world autonomous driving environments. Experimental results on the widely used nuScenes benchmark demonstrate that PEFT-DML achieves superior detection accuracy compared to existing approaches, validating its effectiveness and generalizability. Overall, this work contributes a parameter-efficient and adaptable solution that improves the resilience and reliability of multi-sensor fusion for 3D object detection in autonomous vehicles. <div>
arXiv:2512.00060v1 Announce Type: new 
Abstract: This study introduces PEFT-DML, a parameter-efficient deep metric learning framework for robust multi-modal 3D object detection in autonomous driving. Unlike conventional models that assume fixed sensor availability, PEFT-DML maps diverse modalities (LiDAR, radar, camera, IMU, GNSS) into a shared latent space, enabling reliable detection even under sensor dropout or unseen modality class combinations. By integrating Low-Rank Adaptation (LoRA) and adapter layers, PEFT-DML achieves significant training efficiency while enhancing robustness to fast motion, weather variability, and domain shifts. Experiments on benchmarks nuScenes demonstrate superior accuracy.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DL-CapsNet: A Deep and Light Capsule Network</title>
<link>https://arxiv.org/abs/2512.00061</link>
<guid>https://arxiv.org/abs/2512.00061</guid>
<content:encoded><![CDATA[
<div> Capsule Network, DL-CapsNet, Capsule Summarization layer, deep variant, parameter reduction<br /><br />Summary: Capsule Network (CapsNet) is an emerging classifier recognized as a potential successor to traditional Convolutional Neural Networks (CNNs), particularly excelling in image recognition tasks involving overlapping categories and affine transformations. This work introduces DL-CapsNet, a deep variant of CapsNet that incorporates multiple capsule layers to enhance representational power and accuracy. To address the increased complexity that often accompanies deeper networks, the authors design a Capsule Summarization layer that strategically reduces the number of parameters, thereby simplifying the model without sacrificing performance. As a result, DL-CapsNet maintains high accuracy while significantly lowering parameter count, which contributes to faster training and inference times. This efficiency makes DL-CapsNet suitable for handling complex datasets featuring a large number of categories. Overall, the proposed model offers a balanced approach between deep capsule architectures and computational efficiency, suggesting its potential for broader application in advanced image classification problems. <div>
arXiv:2512.00061v1 Announce Type: new 
Abstract: Capsule Network (CapsNet) is among the promising classifiers and a possible successor of the classifiers built based on Convolutional Neural Network (CNN). CapsNet is more accurate than CNNs in detecting images with overlapping categories and those with applied affine transformations. In this work, we propose a deep variant of CapsNet consisting of several capsule layers. In addition, we design the Capsule Summarization layer to reduce the complexity by reducing the number of parameters. DL-CapsNet, while being highly accurate, employs a small number of parameters and delivers faster training and inference. DL-CapsNet can process complex datasets with a high number of categories.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Satellite to Street : Disaster Impact Estimator</title>
<link>https://arxiv.org/abs/2512.00065</link>
<guid>https://arxiv.org/abs/2512.00065</guid>
<content:encoded><![CDATA[
<div> Keywords: disaster assessment, satellite imagery, deep learning, U-Net, damage detection<br /><br />Summary:<br /><br />1. The article addresses the critical need for accurate post-disaster damage assessment to prioritize emergency response efforts effectively.<br /><br />2. It highlights the limitations of manual interpretation of satellite images, which is slow, subjective, and difficult to scale.<br /><br />3. Existing deep learning methods such as U-Net-based segmentation and change-detection models often underperform due to subtle structural changes and severe class imbalance, especially in detecting highly damaged areas.<br /><br />4. The proposed framework, "Satellite-to-Street: Disaster Impact Estimator," introduces a modified dual-input U-Net architecture designed to jointly process pre- and post-disaster satellite images, enhancing feature fusion to capture both local structural changes and broader contextual cues.<br /><br />5. The integration of class-aware weighted loss functions helps to mitigate the dominance of undamaged pixels in datasets, improving sensitivity to major and destroyed damage categories.<br /><br />6. Experiments on public disaster datasets demonstrate that this approach achieves superior localization and classification performance compared to traditional segmentation and baseline change-detection models.<br /><br />7. The resulting fine-grained pixel-level damage maps provide rapid, consistent assessments that support expert decision-making, enabling more efficient and data-driven disaster management. <div>
arXiv:2512.00065v1 Announce Type: new 
Abstract: Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN</title>
<link>https://arxiv.org/abs/2512.00073</link>
<guid>https://arxiv.org/abs/2512.00073</guid>
<content:encoded><![CDATA[
<div> Keywords: provident vehicle detection, night vision, noise reduction, MobileNet-U-Net, rainy conditions<br /><br />Summary:<br /><br />This paper addresses the challenge of detecting vehicles during nighttime, particularly under adverse weather conditions like rain and snow that introduce significant noise in camera images. The authors propose a novel pipeline called ProvRain, which incorporates a lightweight MobileNet-U-Net architecture designed for effective denoising and enhanced detection robustness. ProvRain employs curriculum training to better generalize across diverse and challenging weather scenarios. The training data combines synthetic images with real-world samples from the PVDN dataset, ensuring comprehensive learning. The performance of ProvRain is rigorously compared to a baseline Faster RCNN model trained solely on the PVDN dataset. Results demonstrate that integrating a dedicated denoising network significantly improves vehicle detection metrics in rainy, nighttime conditions, yielding an 8.94% increase in accuracy and a 10.25% boost in recall. Furthermore, the custom MobileNet-U-Net shows superior image restoration capabilities, with a 10-15% improvement in Peak Signal-to-Noise Ratio (PSNR), a 5-6% rise in Structural Similarity Index (SSIM), and up to a 67% reduction in Learned Perceptual Image Patch Similarity (LPIPS) error compared to transformer-based approaches. These findings highlight the effectiveness of incorporating denoising modules in vehicle detection pipelines to tackle noise challenges in night vision scenarios. <div>
arXiv:2512.00073v1 Announce Type: new 
Abstract: Provident vehicle detection has a lot of scope in the detection of vehicle during night time. The extraction of features other than the headlamps of vehicles allows us to detect oncoming vehicles before they appear directly on the camera. However, it faces multiple issues especially in the field of night vision, where a lot of noise caused due to weather conditions such as rain or snow as well as camera conditions. This paper focuses on creating a pipeline aimed at dealing with such noise while at the same time maintaining the accuracy of provident vehicular detection. The pipeline in this paper, ProvRain, uses a lightweight MobileNet-U-Net architecture tuned to generalize to robust weather conditions by using the concept of curricula training. A mix of synthetic as well as available data from the PVDN dataset is used for this. This pipeline is compared to the base Faster RCNN architecture trained on the PVDN dataset to see how much the addition of a denoising architecture helps increase the detection model's performance in rainy conditions. The system boasts an 8.94\% increase in accuracy and a 10.25\% increase in recall in the detection of vehicles in rainy night time frames. Similarly, the custom MobileNet-U-Net architecture that was trained also shows a 10-15\% improvement in PSNR, a 5-6\% increase in SSIM, and upto a 67\% reduction in perceptual error (LPIPS) compared to other transformer approaches.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.00075</link>
<guid>https://arxiv.org/abs/2512.00075</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot image generation, diffusion models, image protection, reversible encryption, adversarial perturbation<br /><br />Summary:  
This work addresses the risks of intellectual property violations that arise from zero-shot image-to-image generation using diffusion models, particularly unauthorized identity cloning and artistic style imitation. It introduces Adapter Shield, the first universal and authentication-integrated defense mechanism designed to protect personal images from misuse in such zero-shot generation settings. The method exploits how current zero-shot generation models use image encoders to extract embeddings from input images, which are then injected into the diffusion model’s UNet via cross-attention layers. Inspired by this, the authors propose a reversible encryption system that transforms original embeddings into encrypted representations tied to secret keys. Authorized users can decrypt these embeddings using the correct key and restore normal functionality for legitimate generation tasks. To enforce protection, a multi-target adversarial perturbation technique is developed to actively alter original embeddings, embedding a defensive layer that forces unauthorized users to generate distorted or encrypted outputs instead of authentic ones. Extensive experiments demonstrate that Adapter Shield significantly outperforms existing defenses in preventing unauthorized zero-shot image synthesis while offering secure and flexible access control for verified users, thereby balancing creativity and protection effectively. <div>
arXiv:2512.00075v1 Announce Type: new 
Abstract: With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection</title>
<link>https://arxiv.org/abs/2512.00078</link>
<guid>https://arxiv.org/abs/2512.00078</guid>
<content:encoded><![CDATA[
<div> Keywords: single cell detection, brightfield microscopy, synthetic data, diffusion model, object detection<br /><br />Summary:  
Accurate single cell detection in brightfield microscopy is essential for biological research but is often hindered by limited data and annotation challenges. The study explores the use of unconditional diffusion models, specifically a U-Net based diffusion model, to generate synthetic brightfield microscopy images. These synthetic datasets, created with varying ratios of synthetic to real images, were used to train and evaluate three detection models: YOLOv8, YOLOv9, and RT-DETR. The experiments demonstrated that incorporating synthetic data into training can improve detection accuracy with minimal additional cost. Furthermore, a human expert survey was conducted to assess the realism of the generated images. Experts were unable to reliably distinguish synthetic images from real ones, achieving an accuracy of only 50%. This finding underscores the high fidelity of the generated images and their potential utility. The results suggest that diffusion-based synthetic data generation is a promising technique to augment real microscopy datasets, thereby reducing the burden of extensive manual annotation. Additionally, the approach may enhance the robustness and performance of cell detection models in microscopy image analysis, paving the way for more efficient and scalable biological research workflows. <div>
arXiv:2512.00078v1 Announce Type: new 
Abstract: Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels</title>
<link>https://arxiv.org/abs/2512.00080</link>
<guid>https://arxiv.org/abs/2512.00080</guid>
<content:encoded><![CDATA[
<div> MARWIN robot, deep visual stereo odometry, autonomous navigation, accelerator tunnels, European XFEL<br /><br />Summary:<br /><br />The MARWIN robot currently operates at the European XFEL, performing autonomous radiation monitoring in long accelerator tunnels where traditional localization methods face challenges. Its existing navigation system integrates lidar-based edge detection, wheel and lidar odometry, periodic QR-code references, and fuzzy control mechanisms for wall distance and positioning, which works well in predefined tunnel sections but lacks adaptability in unfamiliar geometries and obstacle scenarios. This paper investigates using deep visual stereo odometry (DVSO) as a vision-based alternative that combines stereo disparity, optical flow, and self-supervised learning to estimate depth and ego-motion jointly without requiring labeled data. DVSO can maintain global consistency by fusing with absolute references or additional sensors. The authors conceptually evaluate DVSO within accelerator tunnel environments, focusing on the European XFEL as a representative case. Anticipated advantages include minimizing scale drift through stereo vision, reducing costs by relying on low-cost sensors, and enabling scalable data collection. However, the approach faces challenges such as dealing with low-texture surfaces, lighting variations, computational demands, and maintaining robustness in radiation-rich environments. The paper concludes by outlining a research agenda aimed at enhancing MARWIN’s autonomy for navigation in constrained and safety-critical infrastructure settings. <div>
arXiv:2512.00080v1 Announce Type: new 
Abstract: The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages</title>
<link>https://arxiv.org/abs/2512.00082</link>
<guid>https://arxiv.org/abs/2512.00082</guid>
<content:encoded><![CDATA[
<div> diagnostic prompting, Multimodal Large Language Model, visual complexity, Amazon Search Results Pages, human alignment

<br /><br />Summary: This study evaluates the effectiveness of diagnostic prompting in enhancing the reliability of Multimodal Large Language Models (MLLMs) for assessing the visual complexity of Amazon Search Results Pages (SRP). The researchers compared diagnostic prompting against traditional gestalt principles-based prompting using a dataset of 200 Amazon SRP pages alongside human expert annotations. Results demonstrated a substantial improvement in MLLM performance, with the F1-score increasing from 0.031 to 0.297, reflecting an 858% relative improvement, although the absolute performance remained modest (Cohen's κ = 0.071). Analysis through a decision tree showed that MLLMs prioritize visual design elements—specifically badge clutter, accounting for 38.6% of decision importance—whereas human experts focus more on content similarity, indicating a partial alignment in reasoning between models and humans. Failure case examination highlighted ongoing challenges for MLLMs in visual perception tasks, notably in evaluating product similarity and color intensity. The findings suggest that diagnostic prompting is a promising initial method to bring MLLM evaluations closer to human judgment. However, persistent disagreements between human and model assessments underscore the need for further research, refined prompting strategies, and larger ground truth datasets to enable reliable real-world applications. <div>
arXiv:2512.00082v1 Announce Type: new 
Abstract: This study investigates whether diagnostic prompting can improve Multimodal Large Language Model (MLLM) reliability for visual complexity assessment of Amazon Search Results Pages (SRP). We compare diagnostic prompting with standard gestalt principles-based prompting using 200 Amazon SRP pages and human expert annotations. Diagnostic prompting showed notable improvements in predicting human complexity judgments, with F1-score increasing from 0.031 to 0.297 (+858\% relative improvement), though absolute performance remains modest (Cohen's $\kappa$ = 0.071). The decision tree revealed that models prioritize visual design elements (badge clutter: 38.6\% importance) while humans emphasize content similarity, suggesting partial alignment in reasoning patterns. Failure case analysis reveals persistent challenges in MLLM visual perception, particularly for product similarity and color intensity assessment. Our findings indicate that diagnostic prompting represents a promising initial step toward human-aligned MLLM-based evaluation, though failure cases with consistent human-MLLM disagreement require continued research and refinement in prompting approaches with larger ground truth datasets for reliable practical deployment.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.00084</link>
<guid>https://arxiv.org/abs/2512.00084</guid>
<content:encoded><![CDATA[
<div> Keywords: denoising diffusion probabilistic models, medical image segmentation, ModernBERT, multi-modal learning, FlashAttention 2<br /><br />Summary:<br />1. The paper addresses limitations in medical image segmentation due to the reliance on dense pixel-wise annotations, which are costly and require expert knowledge.<br />2. The authors propose FastTextDiff, a diffusion-based segmentation model that integrates textual medical annotations to improve semantic representations.<br />3. FastTextDiff leverages ModernBERT, a transformer model trained on clinical texts from MIMIC-III and MIMIC-IV, to encode clinical knowledge and link it with visual image features through cross-modal attention.<br />4. ModernBERT is presented as a fast and scalable alternative to Clinical BioBERT, demonstrating advantages in diffusion-based segmentation pipelines.<br />5. By using ModernBERT with FlashAttention 2, which employs an alternating attention mechanism and is trained on a massive 2-trillion-token corpus, FastTextDiff achieves improved segmentation accuracy and training efficiency compared to traditional diffusion-based models.<br />6. The study highlights the promise of multi-modal learning approaches combining textual and visual data for enhancing medical image analysis tasks. <div>
arXiv:2512.00084v1 Announce Type: new 
Abstract: In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs</title>
<link>https://arxiv.org/abs/2512.00086</link>
<guid>https://arxiv.org/abs/2512.00086</guid>
<content:encoded><![CDATA[
<div> Monocular depth estimation, On-device learning, IoT, Domain shift, Sparse update<br /><br />Summary:<br /><br />1. The paper addresses the challenge of monocular depth estimation (MDE) on ultra-low-power (ULP) Internet-of-Things (IoT) devices, focusing on the significant accuracy drop caused by domain shifts when sensor data differs from the training dataset.<br /><br />2. To mitigate this domain shift, the authors propose a multi-modal on-device learning (ODL) approach that integrates a Greenwaves GAP9 microcontroller unit (MCU), a low-power monocular camera, and an 8×8 pixel depth sensor on a device consuming approximately 300mW.<br /><br />3. The system operates normally by running a tiny 107k-parameter μPyD-Net model on monocular images for depth inference, while the depth sensor remains mostly off to conserve energy.<br /><br />4. When deployed in a new environment, the depth sensor activates briefly to collect pseudo-labels used to fine-tune the model entirely on the MCU, enabling adaptation to new data.<br /><br />5. A memory-driven sparse update scheme is introduced to minimize memory usage during backpropagation-based training, reducing fine-tuning memory to 1.2 MB, 2.2 times less than a full update, while maintaining accuracy with minimal drops.<br /><br />6. Experimental validation shows that the ODL on the IoT node takes 17.8 minutes to complete and improves depth estimation performance significantly, reducing root mean squared error from 4.9 m to 0.6 m using only 3,000 self-labeled samples collected in real-life deployment scenarios.<br /><br />7. This work demonstrates, for the first time, the feasibility and effectiveness of on-device learning for MDE in real-world low-power IoT systems. <div>
arXiv:2512.00086v1 Announce Type: new 
Abstract: Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $\mu$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data</title>
<link>https://arxiv.org/abs/2512.00087</link>
<guid>https://arxiv.org/abs/2512.00087</guid>
<content:encoded><![CDATA[
<div> Keywords: classroom interactions, multimodal analysis, video transformers, discourse recognition, teacher feedback<br /><br />Summary:<br /><br />1. The paper addresses the challenge of providing concrete feedback to teachers through observation of classroom interactions, highlighting limitations of manual annotation such as being resource-intensive and difficult to scale. <br /><br />2. The authors explore AI-driven methods for analyzing classroom recordings, focusing on multimodal instructional activity recognition from video and discourse recognition from lesson transcripts to lay the groundwork for actionable feedback. <br /><br />3. They utilize a dataset consisting of 164 hours of densely annotated video and 68 corresponding lesson transcripts, developing separate pipelines tailored to each modality—video and text. <br /><br />4. For video data, they evaluate three approaches: zero-shot multimodal large language models (LLMs), fine-tuned vision-language models, and self-supervised video transformers across 24 activity labels. For transcript analysis, they fine-tune a transformer-based classifier with contextual inputs and benchmark it against prompting-based LLMs on 19 discourse labels. <br /><br />5. To manage the challenges of class imbalance and multi-label classification, techniques such as per-label thresholding, context windows, and imbalance-aware loss functions are applied. The fine-tuned models surpass prompting-based counterparts, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. <br /><br />6. The results demonstrate the feasibility of automated multimodal classroom analysis and establish a scalable AI foundation for offering actionable teacher feedback systems. <div>
arXiv:2512.00087v1 Announce Type: new 
Abstract: Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features</title>
<link>https://arxiv.org/abs/2512.00088</link>
<guid>https://arxiv.org/abs/2512.00088</guid>
<content:encoded><![CDATA[
<div> Keywords: SemImage, semantic image representation, HSV color space, multi-task learning, text classification<br /><br />Summary:<br /><br />1. The paper introduces SemImage, a novel method for representing text documents as two-dimensional semantic images designed for processing by convolutional neural networks (CNNs).<br />2. In SemImage, each word corresponds to a pixel within a 2D image where rows represent sentences and dynamic boundary rows are inserted between sentences to highlight semantic transitions.<br />3. Unlike standard RGB pixels, each pixel encodes linguistic features in a disentangled HSV color space: Hue (represented by H_cos and H_sin components) encodes topics, Saturation encodes sentiment, and Value encodes intensity or certainty.<br />4. The system employs a multi-task learning framework with a ColorMapper network that projects word embeddings into the HSV space, supervised to predict topic labels from Hue and sentiment labels from Saturation along with the main classification task.<br />5. The dynamic boundary rows create sharp visual separations between semantically different sentences, making paragraph breaks visually salient.<br />6. SemImage is integrated with 2D CNNs (e.g., ResNet) for document classification and evaluated on multi-label and single-label benchmarks.<br />7. Results show that SemImage achieves competitive or better accuracy than strong baselines such as BERT and hierarchical attention networks, with enhanced interpretability.<br />8. Ablation studies demonstrate the significance of the HSV multi-channel representation and boundary rows.<br />9. Visualization of SemImage images reveals clear patterns that correspond to topic shifts and sentiment changes, making these features visible to humans and machines alike. <div>
arXiv:2512.00088v1 Announce Type: new 
Abstract: We propose SemImage, a novel method for representing a text document as a two-dimensional semantic image to be processed by convolutional neural networks (CNNs). In a SemImage, each word is represented as a pixel in a 2D image: rows correspond to sentences and an additional boundary row is inserted between sentences to mark semantic transitions. Each pixel is not a typical RGB value but a vector in a disentangled HSV color space, encoding different linguistic features: the Hue with two components H_cos and H_sin to account for circularity encodes the topic, Saturation encodes the sentiment, and Value encodes intensity or certainty. We enforce this disentanglement via a multi-task learning framework: a ColorMapper network maps each word embedding to the HSV space, and auxiliary supervision is applied to the Hue and Saturation channels to predict topic and sentiment labels, alongside the main task objective. The insertion of dynamically computed boundary rows between sentences yields sharp visual boundaries in the image when consecutive sentences are semantically dissimilar, effectively making paragraph breaks salient. We integrate SemImage with standard 2D CNNs (e.g., ResNet) for document classification. Experiments on multi-label datasets (with both topic and sentiment annotations) and single-label benchmarks demonstrate that SemImage can achieve competitive or better accuracy than strong text classification baselines (including BERT and hierarchical attention networks) while offering enhanced interpretability. An ablation study confirms the importance of the multi-channel HSV representation and the dynamic boundary rows. Finally, we present visualizations of SemImage that qualitatively reveal clear patterns corresponding to topic shifts and sentiment changes in the generated image, suggesting that our representation makes these linguistic features visible to both humans and machines.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts</title>
<link>https://arxiv.org/abs/2512.00089</link>
<guid>https://arxiv.org/abs/2512.00089</guid>
<content:encoded><![CDATA[
<div> wildfire forecasting, teleconnections, Vision Transformer, multi-scale fusion, subseasonal-to-seasonal prediction<br /><br />Summary:<br /><br />1. Forecasting wildfires weeks to months ahead is challenging but vital for resource planning and fuel treatment management. 2. Traditional short-term forecasts focus on local weather, whereas long-term predictions need to incorporate global teleconnection patterns reflecting Earth's interconnected climate system. 3. The authors propose TeleViT, a Teleconnection-aware Vision Transformer that fuses fine-scale local fire drivers, coarsened global climate fields, and teleconnection indices using an asymmetric tokenization approach. 4. TeleViT processes heterogeneous tokens with a transformer encoder and decodes predictions while preserving spatial structure relevant to local prediction patches. 5. Evaluations on the global SeasFire dataset (2001-2021, 8-day resolution) show TeleViT outperforming U-Net++, ViT, and climatology across forecast leads up to four months, achieving higher AUPRC scores notably at zero lead (0.630) and at ~4-month lead (0.601-0.603). 6. Regional skill is highest in seasonally consistent fire regimes like African savannas and lower in boreal and arid regions. 7. Attention analyses reveal that predictive skill primarily stems from local information, with global data providing coarse context. 8. Results suggest that models incorporating large-scale Earth system context can extend wildfire forecast skill on subseasonal-to-seasonal timescales. <div>
arXiv:2512.00089v1 Announce Type: new 
Abstract: Forecasting wildfires weeks to months in advance is difficult, yet crucial for planning fuel treatments and allocating resources. While short-term predictions typically rely on local weather conditions, long-term forecasting requires accounting for the Earth's interconnectedness, including global patterns and teleconnections. We introduce TeleViT, a Teleconnection-aware Vision Transformer that integrates (i) fine-scale local fire drivers, (ii) coarsened global fields, and (iii) teleconnection indices. This multi-scale fusion is achieved through an asymmetric tokenization strategy that produces heterogeneous tokens processed jointly by a transformer encoder, followed by a decoder that preserves spatial structure by mapping local tokens to their corresponding prediction patches.
  Using the global SeasFire dataset (2001-2021, 8-day resolution), TeleViT improves AUPRC performance over U-Net++, ViT, and climatology across all lead times, including horizons up to four months. At zero lead, TeleViT with indices and global inputs reaches AUPRC 0.630 (ViT 0.617, U-Net 0.620), at 16x8day lead (around 4 months), TeleViT variants using global input maintain 0.601-0.603 (ViT 0.582, U-Net 0.578), while surpassing the climatology (0.572) at all lead times. Regional results show the highest skill in seasonally consistent fire regimes, such as African savannas, and lower skill in boreal and arid regions. Attention and attribution analyses indicate that predictions rely mainly on local tokens, with global fields and indices contributing coarse contextual information. These findings suggest that architectures explicitly encoding large-scale Earth-system context can extend wildfire predictability on subseasonal-to-seasonal timescales.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Filament Extraction for 3D Concrete Printing</title>
<link>https://arxiv.org/abs/2512.00091</link>
<guid>https://arxiv.org/abs/2512.00091</guid>
<content:encoded><![CDATA[
<div> 3D concrete printing, extrusion-based, shotcrete 3D printing, filament quality control, automated procedure

<br /><br />Summary:  
1. The architecture, engineering, and construction (AEC) industry is evolving to meet the demand for sustainable and efficient design and construction techniques.  
2. Two main deposition methods for large-scale 3D concrete printing are extrusion-based (Contour Crafting - CC) and shotcrete 3D printing (SC3DP), both using digitally controlled nozzles to build structures layer by layer.  
3. The continuous flow of concrete material used to form layers is termed a filament, which defines the structural quality of the printed object.  
4. Ensuring the geometric quality of these filaments is crucial for the integrity and precision of 3D printed concrete structures.  
5. This paper introduces an automated quality control (QC) procedure for filaments applicable to both extrusion-based and shotcrete printing methods.  
6. The QC workflow presented is sensor-independent, compatible with various data acquisition tools such as cameras, structured light systems (SLS), and terrestrial laser scanners (TLS).  
7. The method supports quality control for materials in either fresh or cured states, thereby allowing for both online (real-time) and post-printing inspections.  
8. This flexible and automated QC approach aims to improve the reliability and consistency of 3D concrete printed components in the AEC sector. <div>
arXiv:2512.00091v1 Announce Type: new 
Abstract: The architecture, engineering and construction (AEC) industry is constantly evolving to meet the demand for sustainable and effective design and construction of the built environment. In the literature, two primary deposition techniques for large-scale 3D concrete printing (3DCP) have been described, namely extrusion-based (Contour Crafting-CC) and shotcrete 3D printing (SC3DP) methods. The deposition methods use a digitally controlled nozzle to print material layer by layer. The continuous flow of concrete material used to create the printed structure is called a filament or layer. As these filaments are the essential structure defining the printed object, the filaments' geometry quality control is crucial. This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and SC3DP printing methods. The paper also describes a workflow that is independent of the sensor used for data acquisition, such as a camera, a structured light system (SLS) or a terrestrial laser scanner (TLS). This method can be used with materials in either the fresh or cured state. Thus, it can be used for online and post-printing QC.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images</title>
<link>https://arxiv.org/abs/2512.00103</link>
<guid>https://arxiv.org/abs/2512.00103</guid>
<content:encoded><![CDATA[
<div> Keywords: actigraphy, depression, schizophrenia, image-based classification, CoAtNet-Tiny<br /><br />Summary:<br /><br />This study evaluates the effectiveness of three image-based deep learning methods—VGG16, ViT-B/16, and CoAtNet-Tiny—in identifying depression, schizophrenia, and healthy controls using daily actigraphy data converted into 30 by 48 pixel images. The datasets used were Psykose and Depresjon, and the evaluation was conducted using a three-fold subject-wise cross-validation approach. While all models fitted the training data well, their generalizability to unseen data varied significantly. VGG16 showed gradual improvement but generally achieved lower accuracy compared to other methods. ViT-B/16 occasionally reached high accuracy but exhibited inconsistent performance across different folds, indicating variability in its reliability. CoAtNet-Tiny emerged as the best-performing model, demonstrating the highest average accuracy along with the most stable performance across folds. It also excelled in precision, recall, and F1-scores, particularly in the more challenging classification of underrepresented depression and schizophrenia cases. These findings highlight that hybrid architectures like CoAtNet-Tiny may offer superior and more dependable results in mental health diagnostics based on actigraphy-derived image data, compared to traditional convolutional or pure transformer-based approaches. This suggests a promising direction for future research and application in actigraphy-based mental health assessments. <div>
arXiv:2512.00103v1 Announce Type: new 
Abstract: This work examines how three different image-based methods, VGG16, ViT-B/16, and CoAtNet-Tiny, perform in identifying depression, schizophrenia, and healthy controls using daily actigraphy records. Wrist-worn activity signals from the Psykose and Depresjon datasets were converted into 30 by 48 images and evaluated through a three-fold subject-wise split. Although all methods fitted the training data well, their behaviour on unseen data differed. VGG16 improved steadily but often settled at lower accuracy. ViT-B/16 reached strong results in some runs, but its performance shifted noticeably from fold to fold. CoAtNet-Tiny stood out as the most reliable, recording the highest average accuracy and the most stable curves across folds. It also produced the strongest precision, recall, and F1-scores, particularly for the underrepresented depression and schizophrenia classes. Overall, the findings indicate that CoAtNet-Tiny performed most consistently on the actigraphy images, while VGG16 and ViT-B/16 yielded mixed results. These observations suggest that certain hybrid designs may be especially suited for mental-health work that relies on actigraphy-derived images.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening</title>
<link>https://arxiv.org/abs/2512.00117</link>
<guid>https://arxiv.org/abs/2512.00117</guid>
<content:encoded><![CDATA[
<div> solar photovoltaic, surface faults, deep learning, TinyViT, severity estimation<br /><br />Summary: This article addresses the challenge of maintaining solar photovoltaic (PV) assets by accurately detecting and prioritizing surface faults on extensive and geographically dispersed PV modules. It highlights the limitations of multi-modal imaging techniques, such as electroluminescence and infrared sensors, which pose logistical and financial barriers for routine maintenance at the farm level. The study introduces TinyViT, a compact and efficient pipeline that combines Transformer-based segmentation, spectral-spatial feature engineering, and ensemble regression to analyze planar visible band imagery captured from consumer-grade color cameras. TinyViT is capable of classifying seven nuanced surface fault types and estimating their severity to aid maintenance prioritization. By relying solely on visible spectrum images, the approach significantly reduces costs and enhances scalability, making it viable for resource-constrained solar installations. The method supports affordable and universal PV health monitoring in the field without needing specialized imaging equipment. Experimental validation on real-world public datasets demonstrates that both the classification and regression components of the system perform with accuracy and interpretability comparable to more specialized and complex approaches. This work advances solar health monitoring by offering an accessible, robust, and economically feasible solution for fault detection and severity grading. <div>
arXiv:2512.00117v1 Announce Type: new 
Abstract: Sustained operation of solar photovoltaic assets hinges on accurate detection and prioritization of surface faults across vast, geographically distributed modules. While multi modal imaging strategies are popular, they introduce logistical and economic barriers for routine farm level deployment. This work demonstrates that deep learning and classical machine learning may be judiciously combined to achieve robust surface anomaly categorization and severity estimation from planar visible band imagery alone. We introduce TinyViT which is a compact pipeline integrating Transformer based segmentation, spectral-spatial feature engineering, and ensemble regression. The system ingests consumer grade color camera mosaics of PV panels, classifies seven nuanced surface faults, and generates actionable severity grades for maintenance triage. By eliminating reliance on electroluminescence or IR sensors, our method enables affordable, scalable upkeep for resource limited installations, and advances the state of solar health monitoring toward universal field accessibility. Experiments on real public world datasets validate both classification and regression sub modules, achieving accuracy and interpretability competitive with specialized approaches.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance</title>
<link>https://arxiv.org/abs/2512.00125</link>
<guid>https://arxiv.org/abs/2512.00125</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data generation, industrial quality inspection, zero-shot learning, class imbalance, deep learning<br /><br />Summary:<br /><br />1. The paper addresses challenges in industrial quality inspection where deep learning models require large, high-quality labeled datasets that are costly and time-consuming to produce, especially due to the rarity of defective samples causing class imbalance.<br /><br />2. To overcome data limitations, the authors propose a hybrid synthetic data generation (SDG) framework combining simulation-based rendering, domain randomization, and real background compositing, which allows creation of large, balanced, and fully annotated datasets without manual labeling.<br /><br />3. The SDG pipeline can generate 12,960 labeled images within one hour by varying part geometry, lighting, and surface properties, while compositing synthetic parts onto real backgrounds, enabling zero-shot learning for computer vision inspection.<br /><br />4. The proposed two-stage architecture employs a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification, trained exclusively on synthetic data and tested on 300 real industrial parts.<br /><br />5. Results show high performance with mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy, significantly outperforming few-shot real-data baselines that yield only 50% accuracy under severe class imbalance.<br /><br />6. The approach demonstrates scalable, annotation-free, and robust quality inspection capabilities suitable for real manufacturing environments. <div>
arXiv:2512.00125v1 Announce Type: new 
Abstract: Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation</title>
<link>https://arxiv.org/abs/2512.00129</link>
<guid>https://arxiv.org/abs/2512.00129</guid>
<content:encoded><![CDATA[
<div> breast cancer detection, out-of-distribution (OOD), ResNet50, YOLO, mammographic images<br /><br />Summary:<br /><br /> The article addresses reliability challenges in deep learning models for breast cancer detection when presented with out-of-distribution (OOD) inputs such as CT, MRI, or X-ray images. To tackle this, the authors integrate ResNet50-based OOD filtering with advanced YOLO architectures (YOLOv8, YOLOv11, YOLOv12) to ensure accurate detection from mammographic images. The strategy involves creating an in-domain gallery using cosine similarity to strictly reject non-mammographic images before feeding data into the detection pipeline. The ResNet50 backbone was selected after evaluating 12 CNN architectures for best performance. The OOD detection component achieves exceptional results with 99.77% overall accuracy and a perfect 100% accuracy on OOD test sets, effectively filtering out irrelevant imaging modalities. The combined framework delivers high breast cancer detection performance, achieving a mAP@0.5 of 0.947, while also providing enhanced interpretability using Grad-CAM visualizations. Experimental results demonstrate that incorporating OOD filtering markedly improves system reliability by preventing false positives arising from irrelevant inputs and simultaneously boosting detection accuracy on valid mammographic data. This work lays a robust foundation for deploying trustworthy AI-based breast cancer detection systems across diverse clinical environments with heterogeneous datasets. <div>
arXiv:2512.00129v1 Announce Type: new 
Abstract: Deep learning models for breast cancer detection from mammographic images have significant reliability problems when presented with Out-of-Distribution (OOD) inputs such as other imaging modalities (CT, MRI, X-ray) or equipment variations, leading to unreliable detection and misdiagnosis. The current research mitigates the fundamental OOD issue through a comprehensive approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8, YOLOv11, YOLOv12) for accurate detection of breast cancer. Our strategy establishes an in-domain gallery via cosine similarity to rigidly reject non-mammographic inputs prior to processing, ensuring that only domain-associated images supply the detection pipeline. The OOD detection component achieves 99.77\% general accuracy with immaculate 100\% accuracy on OOD test sets, effectively eliminating irrelevant imaging modalities. ResNet50 was selected as the optimum backbone after 12 CNN architecture searches. The joint framework unites OOD robustness with high detection performance (mAP@0.5: 0.947) and enhanced interpretability through Grad-CAM visualizations. Experimental validation establishes that OOD filtering significantly improves system reliability by preventing false alarms on out-of-distribution inputs while maintaining higher detection accuracy on mammographic data. The present study offers a fundamental foundation for the deployment of reliable AI-based breast cancer detection systems in diverse clinical environments with inherent data heterogeneity.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition</title>
<link>https://arxiv.org/abs/2512.00130</link>
<guid>https://arxiv.org/abs/2512.00130</guid>
<content:encoded><![CDATA[
<div> Cutmix, Data Augmentation, Superpixel, Local Context, Label Mixing<br /><br />Summary:  
This paper introduces LGCOAMix, a novel data augmentation method based on cutmix that addresses key limitations of existing approaches. First, unlike traditional cutmix methods that operate on rectangular or square patches, LGCOAMix uses superpixel-based grid blending to better preserve object part information and local context. Second, it proposes a superpixel attention mechanism for label mixing, marking the first use of this strategy for cutmix augmentation. Third, the method efficiently learns local features from discriminative superpixel-wise regions and cross-image superpixel contrasts, thereby improving focus on meaningful local details rather than just global semantics. Fourth, LGCOAMix overcomes inefficiencies found in prior methods, such as the need for double forward propagation or reliance on pre-trained networks for object centering, resulting in more computationally efficient augmentation. Fifth, extensive experiments on various benchmark datasets demonstrate that LGCOAMix consistently outperforms state-of-the-art cutmix-based methods in classification tasks and weakly supervised object localization on CUB200-2011. Lastly, the authors confirm the effectiveness of LGCOAMix for both convolutional neural networks (CNNs) and Transformer architectures, broadening its applicability. The source code for LGCOAMix is publicly available, facilitating reproducibility and further research in this direction. <div>
arXiv:2512.00130v1 Announce Type: new 
Abstract: Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Edge-Compatible CNN for Speckle-Based Material Recognition in Laser Cutting Systems</title>
<link>https://arxiv.org/abs/2512.00179</link>
<guid>https://arxiv.org/abs/2512.00179</guid>
<content:encoded><![CDATA[
<div> Keywords: material recognition, laser speckle sensing, lightweight CNN, edge deployment, SensiCut dataset<br /><br />Summary:<br /><br />1. Accurate material recognition is essential for safe and efficient laser cutting, preventing poor cuts, machine damage, and harmful emissions.  
2. Laser speckle sensing offers a low-cost, non-destructive way to classify materials but previous approaches used heavy neural networks or focused on limited material types.  
3. This paper proposes a lightweight convolutional neural network specifically designed for laser speckle patterns, drastically reducing model size while retaining high accuracy.  
4. The model was trained and tested on the full SensiCut dataset, which includes 59 material classes across various categories like wood, acrylic, composites, textiles, metals, and paper products.  
5. The network achieves a test accuracy of 95.05% with macro and weighted F1-scores of 0.951, containing only 341k trainable parameters—over 70 times fewer than ResNet-50—and runs at 295 images per second.  
6. This efficiency allows the model to be deployed on resource-limited hardware such as Raspberry Pi and Jetson devices.  
7. When materials are grouped into broader practical families (nine or five classes), recall exceeds 98% and approaches 100%, enabling reliable presets for laser cutter power and speed.  
8. These results highlight that compact, domain-specific CNNs can outperform larger backbone networks for material classification based on speckle patterns, advancing low-cost, edge-deployable, material-aware laser cutting systems. <div>
arXiv:2512.00179v1 Announce Type: new 
Abstract: Accurate material recognition is critical for safe and effective laser cutting, as misidentification can lead to poor cut quality, machine damage, or the release of hazardous fumes. Laser speckle sensing has recently emerged as a low-cost and non-destructive modality for material classification; however, prior work has either relied on computationally expensive backbone networks or addressed only limited subsets of materials. In this study, A lightweight convolutional neural network (CNN) tailored for speckle patterns is proposed, designed to minimize parameters while maintaining high discriminative power. Using the complete SensiCut dataset of 59 material classes spanning woods, acrylics, composites, textiles, metals, and paper-based products, the proposed model achieves 95.05% test accuracy, with macro and weighted F1-scores of 0.951. The network contains only 341k trainable parameters (~1.3 MB) -- over 70X fewer than ResNet-50 -- and achieves an inference speed of 295 images per second, enabling deployment on Raspberry Pi and Jetson-class devices. Furthermore, when materials are regrouped into nine and five practical families, recall exceeds 98% and approaches 100%, directly supporting power and speed preset selection in laser cutters. These results demonstrate that compact, domain-specific CNNs can outperform large backbones for speckle-based material classification, advancing the feasibility of material-aware, edge-deployable laser cutting systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI</title>
<link>https://arxiv.org/abs/2512.00194</link>
<guid>https://arxiv.org/abs/2512.00194</guid>
<content:encoded><![CDATA[
<div> EEG, ICA, AI-agent vision, GPT-4 Vision, component classification<br /><br />Summary:<br /><br />This article presents EEG Autoclean Vision Language AI (ICVision), an innovative AI system designed to replicate expert-level classification of EEG Independent Component Analysis (ICA) components by using vision and natural language reasoning. Unlike traditional classifiers like ICLabel that depend on handcrafted features, ICVision leverages a multimodal large language model, GPT-4 Vision, to directly interpret ICA dashboard visualizations such as topographies, time series, power spectra, and ERP plots. This approach enables the AI to examine and explain EEG components in a manner akin to trained neurologists, marking the first scientific application of AI-agent visual cognition in neurophysiology. ICVision classifies components into six canonical categories: brain, eye, heart, muscle, channel noise, and other noise, providing both confidence scores and human-like explanations. Evaluation on 3,168 ICA components from 124 EEG datasets demonstrated a kappa agreement of 0.677 with expert consensus, outperforming MNE ICLabel, while effectively maintaining clinically relevant brain signals in ambiguous cases. Over 97% of the AI’s outputs were judged interpretable and actionable by expert reviewers. As a fundamental component of the open-source EEG Autoclean platform, ICVision signifies a paradigm shift toward AI models that not only classify but also see, reason, and communicate, enabling scalable, explainable, and reproducible EEG workflows and pioneering expert-level AI visual decision-making in neuroscience and beyond. <div>
arXiv:2512.00194v1 Announce Type: new 
Abstract: We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting</title>
<link>https://arxiv.org/abs/2512.00198</link>
<guid>https://arxiv.org/abs/2512.00198</guid>
<content:encoded><![CDATA[
<div> Keywords: Mammo-FM, mammography, foundation model, breast cancer diagnosis, clinical interpretability<br /><br />Summary:<br /><br />1. The article introduces Mammo-FM, the first foundation model specifically designed for mammography, aimed at advancing breast cancer detection and analysis. <br />2. Mammo-FM is pretrained on the largest and most diverse mammography dataset to date, comprising 140,677 patients and 821,326 mammograms collected from four U.S. institutions. <br />3. The model supports multiple core clinical tasks within a single framework, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis, making it highly versatile in breast imaging. <br />4. Mammo-FM achieves alignment between image data and text, enhancing both visual and textual interpretability, which improves transparency and clinical auditability—key factors for real-world clinical adoption. <br />5. Evaluations conducted on in-distribution and out-of-distribution datasets across various diagnostic, prognostic, and report-generation tasks show that Mammo-FM outperforms state-of-the-art generalist foundation models, despite using only one-third of their parameters and operating on native-resolution mammograms. <br />6. The results emphasize the efficiency and value of domain-specific foundation models tailored to the full set of clinical tasks, as well as the importance of rigorous, domain-aligned evaluations for meaningful advancements in medical AI applications. <div>
arXiv:2512.00198v1 Announce Type: new 
Abstract: Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReactionMamba: Generating Short &amp;Long Human Reaction Sequences</title>
<link>https://arxiv.org/abs/2512.00208</link>
<guid>https://arxiv.org/abs/2512.00208</guid>
<content:encoded><![CDATA[
<div> Keywords: ReactionMamba, 3D human reaction motions, motion VAE, Mamba-based state-space models, long-sequence generation<br /><br />Summary:<br /><br />1. ReactionMamba is a novel framework designed for generating long 3D human reaction motions that effectively capture complex actions such as dance and martial arts. <br />2. It combines a motion Variational Autoencoder (VAE) for efficient motion encoding with Mamba-based state-space models to ensure temporally consistent motion decoding. <br />3. This integrated design enables the system to generate both short sequences of simple movements and extended sequences of complex reaction motions.<br />4. ReactionMamba was evaluated on three diverse datasets: NTU120-AS, Lindy Hop, and InterX, which cover a range of human activities and reaction scenarios.<br />5. The method demonstrates competitive or superior performance compared to existing approaches such as InterFormer, ReMoS, and Ready-to-React in terms of realism, motion diversity, and capacity for long-sequence generation, while also providing significant improvements in inference speed. <div>
arXiv:2512.00208v1 Announce Type: new 
Abstract: We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation</title>
<link>https://arxiv.org/abs/2512.00226</link>
<guid>https://arxiv.org/abs/2512.00226</guid>
<content:encoded><![CDATA[
<div> 3D understanding, DenseScan, multi-level descriptions, multimodal large language models, visual-language tasks<br /><br />Summary:  
This work introduces DenseScan, a novel 3D scene understanding dataset featuring richly detailed multi-level semantic descriptions generated via an automated pipeline. The pipeline leverages multi-view 2D images combined with multimodal large language models (MLLMs) to create dense captioning of scene elements, providing comprehensive object-level descriptions that capture context-sensitive details. DenseScan further extends these annotations by generating scenario-based high-level questions that incorporate object properties, spatial relationships, and overall scene context. By combining precise geometric information with deep semantic richness, the dataset enables a broader spectrum of downstream tasks such as detailed visual-language navigation and interactive question answering in 3D environments. Experimental results demonstrate that this approach significantly improves object-level understanding and question-answering performance compared to traditional annotation methods. Both the dataset and the annotation pipeline are publicly released to support further research and applications, including robotics and augmented reality. DenseScan aims to catalyze advances in 3D scene understanding, empowering researchers and practitioners to address the complexities of real-world environments with richer and more contextually aware annotations. <div>
arXiv:2512.00226v1 Announce Type: new 
Abstract: 3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views</title>
<link>https://arxiv.org/abs/2512.00255</link>
<guid>https://arxiv.org/abs/2512.00255</guid>
<content:encoded><![CDATA[
<div> Keywords: Relightable Holoported Characters, human relighting, transformer-based RelightNet, multi-view lightstage dataset, physics-informed rendering features<br /><br />Summary:<br /><br />1. The paper introduces Relightable Holoported Characters (RHC), a new person-specific approach for free-viewpoint rendering and relighting of full-body, highly dynamic humans based only on sparse-view RGB videos at inference time.<br />2. Unlike traditional one-light-at-a-time (OLAT) relighting methods, the proposed transformer-based RelightNet predicts relit appearance in a single network pass, eliminating the need for costly OLAT basis capture.<br />3. The authors develop a novel capture strategy and dataset using a multi-view lightstage that alternates frames illuminated by random environment maps with uniformly lit frames, enabling accurate motion tracking while capturing diverse lighting and dynamic motion.<br />4. Inspired by the rendering equation, the method extracts physics-informed features encoding geometry, albedo, shading, and camera view from a coarse mesh proxy and input RGB views.<br />5. RelightNet uses cross-attention to combine these features with new lighting conditions and regresses the relit appearance as texel-aligned 3D Gaussian splats attached to the mesh, implicitly approximating the rendering equation efficiently in a single forward pass.<br />6. Experimental results highlight superior visual fidelity and lighting reproduction compared to existing state-of-the-art human relighting approaches.<br />7. The project page with visual examples and more details is available at https://vcai.mpi-inf.mpg.de/projects/RHC/. <div>
arXiv:2512.00255v1 Announce Type: new 
Abstract: We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: https://vcai.mpi-inf.mpg.de/projects/RHC/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations</title>
<link>https://arxiv.org/abs/2512.00261</link>
<guid>https://arxiv.org/abs/2512.00261</guid>
<content:encoded><![CDATA[
<div> Sparse annotations, multimodal remote sensing, diffusion model, parameter-efficient adaptation, pseudo-RGB anchoring  

<br /><br />Summary:  
This paper addresses the challenge of limited labeled data in multimodal remote sensing, which hampers the performance and deployment of state-of-the-art supervised methods like MSFMamba. The authors acknowledge that while ImageNet-pretrained models provide strong visual representations, adapting them to heterogeneous sensing modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) remains difficult without large labeled datasets. To overcome this, they propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain (unlabeled) data. UniDiff leverages FiLM-based timestep-modality conditioning, adapts about 5% of model parameters, and utilizes pseudo-RGB anchoring to maintain the integrity of pre-trained features and avoid catastrophic forgetting. This design ensures effective feature extraction from remote sensing modalities despite sparse annotations. The approach is validated on two established multi-modal benchmark datasets, demonstrating that unsupervised adaptation of a pre-trained diffusion model can effectively overcome annotation limitations and yield efficient fusion of multi-modal remote sensing data. This work highlights a promising direction for practical remote sensing analysis with minimal annotation requirements. <div>
arXiv:2512.00261v1 Announce Type: new 
Abstract: Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction</title>
<link>https://arxiv.org/abs/2512.00264</link>
<guid>https://arxiv.org/abs/2512.00264</guid>
<content:encoded><![CDATA[
<div> Keywords: geometric deep learning, point cloud, cardiac reconstruction, transformer network, cine MRI<br /><br />Summary:<br /><br />1. This work introduces HeartFormer, the first geometric deep learning framework using point cloud representation for 3D reconstruction of the heart’s four chambers from cine MRI data. <br />2. It addresses the limitation of conventional cine MRI, which provides only 2D slices, limiting comprehensive cardiac morphology and function analysis in both healthy and diseased states.<br />3. HeartFormer features a novel point cloud completion network designed for multi-class outputs, consisting of two components: Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet).<br />4. SA-DSTNet generates an initial coarse 3D point cloud capturing global and substructure geometry features, while SA-GFRTNet refines this output by leveraging semantic-geometry representations, ensuring geometric consistency and high fidelity.<br />5. The authors also create HeartCompv1, a large-scale dataset with 17,000 high-resolution multi-class cardiac meshes and point clouds, establishing a benchmark for this research area.<br />6. Extensive experiments on HeartCompv1 and UK Biobank datasets demonstrate that HeartFormer outperforms current state-of-the-art methods in accuracy, robustness, and generalizability.<br />7. The code and dataset will be publicly released upon acceptance to facilitate further research. <div>
arXiv:2512.00264v1 Announce Type: new 
Abstract: We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: https://github.com/10Darren/HeartFormer.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing</title>
<link>https://arxiv.org/abs/2512.00269</link>
<guid>https://arxiv.org/abs/2512.00269</guid>
<content:encoded><![CDATA[
<div> Keywords: brain image generation, pathological-healthy editing, diffusion model, neuroimaging, anatomical consistency<br /><br />Summary:<br /><br />1. The paper addresses the challenge of understanding relationships between pathological and healthy brain structures in neuroimaging, which is crucial for diagnosis, prediction, and treatment planning.<br />2. Existing brain image generation and editing methods often treat pathological and healthy images independently and rely heavily on visual quality, lacking unified modeling due to the scarcity of paired pathological-healthy data.<br />3. The authors introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of both pathological and healthy brain images.<br />4. USB uses a paired diffusion mechanism to jointly model the distribution of brain anatomy and lesions, enabling realistic generation of both pathological and healthy images.<br />5. A consistency guidance algorithm within USB maintains anatomical consistency and lesion correspondence during bidirectional editing between pathological and healthy images.<br />6. Extensive experiments on six public brain MRI datasets, including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to generate diverse, realistic images.<br />7. USB establishes the first unified benchmark for brain image generation and editing, facilitating scalable dataset creation and robust neuroimaging analysis.<br />8. The code for USB is publicly available at the provided GitHub repository, promoting reproducibility and further research in this field. <div>
arXiv:2512.00269v1 Announce Type: new 
Abstract: Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at https://github.com/jhuldr/USB.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention</title>
<link>https://arxiv.org/abs/2512.00275</link>
<guid>https://arxiv.org/abs/2512.00275</guid>
<content:encoded><![CDATA[
<div> Keywords: remote sensing, super-resolution, lightweight model, sparse attention, hierarchical window expansion<br /><br />Summary:<br /><br />1. The paper addresses the need for efficient and lightweight super-resolution models in remote sensing applications where real-time processing is crucial, such as disaster monitoring. <br />2. Existing methods often face a compromise between high performance in image reconstruction and computational efficiency; this motivates the development of a novel approach.<br />3. The authors propose HIMOSA, a lightweight super-resolution framework designed specifically for remote sensing images, which exploits the redundancy commonly found in such imagery.<br />4. HIMOSA introduces a content-aware sparse attention mechanism that allows the model to perform fast inference without sacrificing reconstruction quality.<br />5. To further improve performance with efficiency, a hierarchical window expansion strategy is employed, which adapts to multi-scale repetitive patterns in remote sensing images by controlling the sparsity of the attention.<br />6. Experimental results on various remote sensing datasets validate that HIMOSA achieves state-of-the-art super-resolution performance while maintaining lower computational costs, making it suitable for real-time applications.<br />7. Overall, the study demonstrates that combining content-aware attention with hierarchical windowing can effectively balance model accuracy and speed in remote sensing image super-resolution tasks. <div>
arXiv:2512.00275v1 Announce Type: new 
Abstract: In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth</title>
<link>https://arxiv.org/abs/2512.00281</link>
<guid>https://arxiv.org/abs/2512.00281</guid>
<content:encoded><![CDATA[
<div> Keywords: lung cancer screening, AI system, low-dose CT, malignancy diagnosis, early detection<br /><br />Summary: This study introduces an advanced AI system designed to enhance early detection of malignant lung nodules using low-dose CT scans. Unlike traditional screening methods that rely on size and growth, this system directly performs both detection and malignancy diagnosis at the individual nodule level. To overcome challenges related to dataset scale and explainability, the researchers developed an ensemble approach combining shallow deep learning models with feature-based specialized models. The AI system was trained and tested on a large dataset of 25,709 CT scans containing 69,449 annotated nodules. Its performance surpasses that of radiologists, Lung-RADS, and several state-of-the-art AI models such as Sybil, Brock, Google, and Kaggle. The internal validation achieved an area under the receiver operating characteristic curve (AUC) of 0.98, with 0.945 AUC on an independent cohort. Operating at 99.3% sensitivity with only 0.5 false positives per scan, the system addresses key limitations hindering AI adoption in clinical settings. It outperforms radiologists across all nodule sizes and cancer stages, showing particular strength in early-stage (stage 1) cancers and all growth-based evaluation metrics, including the challenging Volume-Doubling Time metric. Notably, it can diagnose indeterminate and slow-growing nodules up to one year earlier than radiologists. <div>
arXiv:2512.00281v1 Announce Type: new 
Abstract: Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR</title>
<link>https://arxiv.org/abs/2512.00294</link>
<guid>https://arxiv.org/abs/2512.00294</guid>
<content:encoded><![CDATA[
<div> Keywords: augmented reality, multimodal large language models, spatial reasoning, scene graphs, language grounding  

<br /><br />Summary:  
This paper introduces a modular augmented reality (AR) agent system that overcomes limitations of traditional AR relying on fixed class detectors or fiducial markers by enabling understanding of complex, open-vocabulary natural language queries. The system integrates multimodal large language models (MLLMs) with grounded vision models to perform relational reasoning in 3D physical spaces and facilitates language-conditioned spatial retrieval. A key component, the adaptive task agent, dynamically coordinates MLLMs and coordinate-aware perception tools to handle varying query complexities, from simple object identification to multi-object relational reasoning, outputting meter-accurate 3D anchors. The AR agent constructs dynamic scene graphs encoding nine typed relations—including spatial, structural-semantic, and causal-functional relations—allowing MLLMs to comprehend object existence, interactions, and relations in 3D environments. The system employs task-adaptive region-of-interest highlighting and contextual spatial retrieval to guide human attention to important information-rich regions, supporting human-in-the-loop refinement. For complex queries involving selection, measurement, comparison, and actuation, the agent invokes coordinate-aware tools that ground language understanding in physical operations. Its modular architecture facilitates plug-and-use vision-language models without retraining, positioning AR agents as intermediaries that augment MLLMs with spatial intelligence for interactive scene understanding. Finally, the paper introduces GroundedAR-Bench, an evaluation framework designed for benchmarking language-driven real-world localization and relational grounding across varied environments. <div>
arXiv:2512.00294v1 Announce Type: new 
Abstract: Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries. We present a modular AR agent system that integrates multimodal large language models (MLLMs) with grounded vision models to enable relational reasoning in space and language-conditioned spatial retrieval in physical environments. Our adaptive task agent coordinates MLLMs and coordinate-aware perception tools to address varying query complexities, ranging from simple object identification to multi-object relational reasoning, while returning meter-accurate 3D anchors. It constructs dynamic AR scene graphs encoding nine typed relations (spatial, structural-semantic, causal-functional), enabling MLLMs to understand not just what objects exist, but how they relate and interact in 3D space. Through task-adaptive region-of-interest highlighting and contextual spatial retrieval, the system guides human attention to information-dense areas while supporting human-in-the-loop refinement. The agent dynamically invokes coordinate-aware tools for complex queries-selection, measurement, comparison, and actuation-grounding language understanding in physical operations. The modular architecture supports plug-and-use vision-language models without retraining, establishing AR agents as intermediaries that augment MLLMs with real-world spatial intelligence for interactive scene understanding. We also introduce GroundedAR-Bench, an evaluation framework for language-driven real world localization and relation grounding across diverse environments.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion</title>
<link>https://arxiv.org/abs/2512.00300</link>
<guid>https://arxiv.org/abs/2512.00300</guid>
<content:encoded><![CDATA[
<div> Gaussian-based methods, Temporal Gaussian Splatting, embodied 3D semantic scene completion, temporal fusion, voxel fusion<br /><br />Summary:<br /><br />This paper addresses the problem of Embodied 3D Semantic Scene Completion (SSC), which involves inferring dense geometric and semantic information from continuous egocentric observations. Existing Gaussian-based SSC methods suffer from poor scalability and redundancy due to random initialization of numerous primitives within fixed spatial bounds. Recent depth-guided approaches improve upon this but remain limited to local scenes and encounter latency and memory issues as scene scale grows. To overcome these limitations, the authors propose TGSFormer, a novel Temporal Gaussian Splatting framework designed to be scalable for embodied SSC tasks. TGSFormer maintains a persistent Gaussian memory that enables temporal predictions without relying on image coherence or frame caching, thus improving efficiency. The framework introduces a Dual Temporal Encoder that processes both current and historical Gaussian features through confidence-aware cross-attention, facilitating effective temporal fusion. Additionally, a Confidence-aware Voxel Fusion module is employed to merge overlapping primitives into voxel-aligned representations, controlling density and keeping the representation compact. Experimental results demonstrate that TGSFormer achieves state-of-the-art accuracy and scalability on both local and embodied SSC benchmarks, requiring significantly fewer primitives while preserving consistent long-term scene integrity. The authors plan to release their code upon acceptance. <div>
arXiv:2512.00300v1 Announce Type: new 
Abstract: Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation</title>
<link>https://arxiv.org/abs/2512.00308</link>
<guid>https://arxiv.org/abs/2512.00308</guid>
<content:encoded><![CDATA[
<div> Dataset distillation, Optimal Transport, distribution alignment, image synthesis, label smoothing  

<br /><br />Summary:  
The article presents a novel approach to dataset distillation, aiming to create a compact version of large-scale datasets that allow models trained on them to perform comparably to those trained on the full datasets. Recent methods mainly match global statistics such as mean and variance but miss critical instance-level details and intraclass variations, limiting generalization. To overcome this, the authors reformulate dataset distillation as an Optimal Transport (OT) distance minimization problem, which enables a fine-grained distribution matching both globally and at the instance level. OT provides a geometrically faithful framework that preserves local modes, intra-class patterns, and subtle variations in high-dimensional data distributions. The proposed method incorporates three key components: (1) OT-guided diffusion sampling to align the latent distributions of real and distilled images, (2) label-image-aligned soft relabeling that adapts label distributions to the complexity of the synthesized images, and (3) OT-based logit matching to align the outputs of student models with soft-label distributions. Extensive experiments on multiple architectures and large-scale datasets, notably ImageNet-1K, show the method consistently surpasses state-of-the-art results by at least 4% accuracy improvement under the IPC=10 setting, demonstrating both efficiency and effectiveness. <div>
arXiv:2512.00308v1 Announce Type: new 
Abstract: Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays</title>
<link>https://arxiv.org/abs/2512.00310</link>
<guid>https://arxiv.org/abs/2512.00310</guid>
<content:encoded><![CDATA[
<div> Anomaly detection, Synthetic anomalies, Chest X-rays, Lung segmentation, Texture augmentation  

<br /><br />Summary:  
1. This paper introduces ART-ASyn, a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework designed specifically for chest X-rays to improve unsupervised anomaly detection.  
2. ART-ASyn generates synthetic lung opacity anomalies that are both visually realistic and anatomically consistent, addressing limitations in existing synthetic anomaly methods which often produce visually unrealistic and structure-agnostic anomalies.  
3. The approach employs a Progressive Binary Thresholding Segmentation method (PBTSeg) to accurately segment lungs, guiding texture-based augmentation to create realistic anomaly textures in anatomically correct regions.  
4. ART-ASyn produces paired data of normal samples with corresponding pixel-level precise synthetic anomaly masks, enabling explicit supervised learning for anomaly segmentation rather than just one-class classification.  
5. The framework demonstrates strong generalizability through zero-shot anomaly segmentation evaluation on an unseen dataset without requiring any target-domain annotations, highlighting its practical value for real-world medical imaging applications.  
6. The authors have made the code publicly available at the provided GitHub repository to facilitate reproducibility and further research. <div>
arXiv:2512.00310v1 Announce Type: new 
Abstract: Unsupervised anomaly detection aims to identify anomalies without pixel-level annotations. Synthetic anomaly-based methods exhibit a unique capacity to introduce controllable irregularities with known masks, enabling explicit supervision during training. However, existing methods often produce synthetic anomalies that are visually distinct from real pathological patterns and ignore anatomical structure. This paper presents a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework (ART-ASyn) for chest X-rays that generates realistic and anatomically consistent lung opacity related anomalies using texture-based augmentation guided by our proposed Progressive Binary Thresholding Segmentation method (PBTSeg) for lung segmentation. The generated paired samples of synthetic anomalies and their corresponding precise pixel-level anomaly mask for each normal sample enable explicit segmentation supervision. In contrast to prior work limited to one-class classification, ART-ASyn is further evaluated for zero-shot anomaly segmentation, demonstrating generalizability on an unseen dataset without target-domain annotations. Code availability is available at https://github.com/angelacao-hub/ART-ASyn.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Odometry Without Correspondence from Inertially Constrained Ruled Surfaces</title>
<link>https://arxiv.org/abs/2512.00327</link>
<guid>https://arxiv.org/abs/2512.00327</guid>
<content:encoded><![CDATA[
<div> Visual odometry, ruled surfaces, IMU sensor, event cameras, 3D reconstruction<br /><br />Summary:<br /><br />This paper addresses limitations in traditional visual odometry techniques, which often depend on point-to-point correspondence through feature extraction and optical flow calculations, processes that can be computationally expensive and prone to accuracy issues. To overcome these challenges, the authors propose leveraging the geometric property that a moving camera observing a straight line causes its image to sweep out a ruled surface in image-space time. By analyzing the shape of this ruled surface, they extract odometry information, shifting away from conventional correspondence-dependent methods. The approach requires only differential updates from point-to-line associations rather than detailed point-to-point matches, reducing computational complexity. Inspired by event cameras' edge sensitivity, the method is designed to reconstruct 3D scenes and estimate visual odometry based on these ruled surfaces. To improve robustness and reduce the solution space, data from an onboard IMU sensor is integrated, providing inertial constraints that refine the estimation process. This fusion of image-based ruled surface analysis with inertial measurements results in a novel and efficient visual odometry algorithm that avoids some common pitfalls of prior methods while enabling more accurate 3D scene reconstruction. <div>
arXiv:2512.00327v1 Announce Type: new 
Abstract: Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection</title>
<link>https://arxiv.org/abs/2512.00336</link>
<guid>https://arxiv.org/abs/2512.00336</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated content, multimodal dataset, video-audio forgery, deepfake detection, data diversity<br /><br />Summary: The paper presents the Multimodal Video-Audio Dataset (MVAD), a novel dataset created to address the lack of comprehensive resources for detecting AI-generated multimodal video-audio content. This dataset is crucial due to growing concerns about information security and the authenticity of synthetic multimedia. MVAD stands out for its genuine multimodality, featuring samples generated based on three realistic video-audio forgery patterns, which reflects real-world manipulation scenarios. It achieves high perceptual quality by leveraging a variety of state-of-the-art generative models, ensuring the content appears authentic and challenging for detection systems. The dataset's diversity is extensive, including both realistic and anime visual styles and spanning four content categories: humans, animals, objects, and scenes. Additionally, it encompasses four distinct video-audio multimodal data types, broadening its applicability to various detection tasks. MVAD aims to facilitate the development of more trustworthy and robust detection methodologies for synthetic multimedia, overcoming the limitations of existing datasets that are either unimodal or restricted to facial deepfakes. The dataset is publicly available on GitHub, promoting open research and collaboration in this critical area of multimedia security. <div>
arXiv:2512.00336v1 Announce Type: new 
Abstract: The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at https://github.com/HuMengXue0104/MVAD.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models</title>
<link>https://arxiv.org/abs/2512.00343</link>
<guid>https://arxiv.org/abs/2512.00343</guid>
<content:encoded><![CDATA[
<div> Backdoor detection, vision-language pretrained models, CLIP, feature assimilation, gradient-based inversion<br /><br />Summary:<br /><br />1. Vision-language pretrained models (VLPs), such as CLIP, have shown great success but remain highly susceptible to backdoor attacks, posing significant security risks when models are fine-tuned by untrusted sources.<br /><br />2. Existing backdoor detection methods often depend on prior knowledge about the training data, triggers, or downstream classifiers, limiting their applicability in real-world scenarios.<br /><br />3. The authors propose AMDET (Assimilation Matters in DETection), a novel model-level detection framework that does not require any prior knowledge of datasets, triggers, or targets.<br /><br />4. AMDET leverages the discovered feature assimilation property in backdoored text encoders, where the representations of tokens in backdoor samples are highly similar, primarily due to attention weight concentration on trigger tokens.<br /><br />5. The method performs gradient-based inversion on token embeddings to recover implicit features that can activate backdoors and further distinguishes natural backdoor features in CLIP models from intentionally injected ones by analyzing loss landscapes.<br /><br />6. Extensive experiments on 3,600 backdoored and benign-finetuned models, covering multiple attack paradigms and VLP architectures, demonstrate AMDET’s effectiveness with an F1 score of 89.90%, quick detection time (about 5 minutes on RTX 4090), and robustness against adaptive attacks.<br /><br />7. The authors provide open-source code to facilitate further research and application at their GitHub repository. <div>
arXiv:2512.00343v1 Announce Type: new 
Abstract: Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mmPred: Radar-based Human Motion Prediction in the Dark</title>
<link>https://arxiv.org/abs/2512.00345</link>
<guid>https://arxiv.org/abs/2512.00345</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Motion Prediction, mmWave radar, diffusion model, Time-domain Pose Refinement, Global Skeleton-relational Transformer<br /><br />Summary:<br /><br />This paper introduces radar as a novel sensing modality for Human Motion Prediction (HMP), addressing the limitations of RGB-D cameras which are sensitive to lighting and raise privacy issues, thus restricting applications like firefighting and healthcare. Radar, specifically millimeter-wave (mmWave) radar, offers robustness and privacy-preservation but suffers from challenges including specular reflections and multipath effects that cause noisy and temporally inconsistent data such as body-part miss-detection. To overcome these artifacts, the authors propose mmPred, the first diffusion-based framework specifically designed for radar-based HMP. mmPred employs a dual-domain historical motion representation that integrates two branches: a Time-domain Pose Refinement (TPR) branch that focuses on learning detailed motion features, and a Frequency-domain Dominant Motion (FDM) branch that captures overarching motion trends and reduces frame-level inconsistencies. The framework’s backbone is a Global Skeleton-relational Transformer (GST) that models global inter-joint relationships, allowing corrupted joint data to dynamically leverage information from other joints. Experiments demonstrate that mmPred sets a new state-of-the-art in radar-based human motion prediction, outperforming existing methods by 8.6% on the mmBody dataset and 22% on the mm-Fi dataset, validating its effectiveness and robustness. <div>
arXiv:2512.00345v1 Announce Type: new 
Abstract: Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction</title>
<link>https://arxiv.org/abs/2512.00355</link>
<guid>https://arxiv.org/abs/2512.00355</guid>
<content:encoded><![CDATA[
<div> Keywords: human motion prediction, diffusion model, spatial-temporal coherence, residual-DCT encoding, spatial-mamba module<br /><br />Summary:<br /><br />1. The paper addresses the challenge of improving human motion prediction (HMP) for intelligent room-side sensing and service robots, focusing on the balance between accurately forecasting motion and maintaining kinematic plausibility while handling uncertainty.<br /><br />2. Existing HMP approaches often provide either deterministic predictions that overlook uncertainty or probabilistic models that compromise motion realism; diffusion models help improve this trade-off but usually require costly multi-stage pipelines unsuitable for edge deployment.<br /><br />3. The authors propose SMamDiff, a novel single-stage diffusion model designed to preserve spatial-temporal coherence in HMP by introducing two key components: a residual-DCT motion encoding and a stickman-drawing spatial-mamba module.<br /><br />4. The residual-DCT motion encoding subtracts the last observed pose before applying temporal discrete cosine transform (DCT), reducing the dominance of the DC component (f=0) and emphasizing higher-frequency movement details, helping the model learn joint movement dynamics rather than static joint positions.<br /><br />5. The spatial-mamba module processes joints sequentially, conditioning each joint on the previously processed ones to capture long-range dependencies across joints, enhancing spatial coherence.<br /><br />6. Experimental results on Human3.6M and HumanEva datasets show that SMamDiff achieves state-of-the-art performance amongst single-stage probabilistic HMP methods while having lower latency and memory use compared to multi-stage diffusion baselines, enabling practical edge deployment. <div>
arXiv:2512.00355v1 Announce Type: new 
Abstract: With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters</title>
<link>https://arxiv.org/abs/2512.00363</link>
<guid>https://arxiv.org/abs/2512.00363</guid>
<content:encoded><![CDATA[
<div> Multimodal object detection, fusion encoder, modality completion, frequency-aware adapter, remote sensing<br /><br />Summary:<br /><br /> This paper addresses the challenge of multimodal remote sensing object detection, aiming to improve accuracy and robustness by fusing complementary data from different modalities under difficult conditions. Existing fusion methods either involve complex attention-based or deformable convolution blocks, which compromise between performance and lightweight design, or use shared backbones leading to suboptimal modality representations. Dual-stream architectures, while effective, nearly double the parameter count, limiting practicality. To overcome these issues, the authors propose MM-DETR, a lightweight, efficient framework featuring a Mamba-based dual granularity fusion encoder. This encoder reformulates global interaction using channel-wise dynamic gating and incorporates a 1D selective scan for efficient cross-modal modeling with linear complexity. They reinterpret multimodal fusion as a modality completion problem, introducing a region-aware 2D selective scanning completion branch to recover modality-specific cues, enabling fine-grained fusion via a bidirectional pyramid pathway with minimal overhead. Additionally, a frequency-aware modality adapter is embedded within the shared backbone to reduce parameter redundancy while enhancing feature extraction. This adapter uses a spatial-frequency co-expert structure and a pixel-wise router to dynamically balance expert contributions for efficient spatial-frequency fusion. Extensive experiments on four benchmarks demonstrate the method’s effectiveness and generalization capabilities. <div>
arXiv:2512.00363v1 Announce Type: new 
Abstract: Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards aligned body representations in vision models</title>
<link>https://arxiv.org/abs/2512.00365</link>
<guid>https://arxiv.org/abs/2512.00365</guid>
<content:encoded><![CDATA[
<div> Keywords: human physical reasoning, body representations, semantic segmentation, vision models, coarse representations<br /><br />Summary:  
1. The study explores how humans use internal "body" representations—simplified, volumetric models of objects—to intuitively predict motion and physical interactions.  
2. Despite psychophysical evidence supporting the use of coarse representations in human cognition, the detailed internal structure of these representations remains unclear.  
3. The authors adapted a psychophysical experiment, originally conducted on 50 human participants, into a semantic segmentation task applied to vision models to investigate if these models develop similar internal object representations.  
4. Seven segmentation networks of varying sizes were tested, revealing that smaller models tend to form coarse, human-like body representations, while larger models develop overly detailed and fine-grained encodings.  
5. The findings suggest that limited computational resources encourage the emergence of coarse object representations in vision models, providing a scalable and interpretable framework that may shed light on the underlying structure of physical reasoning processes in the human brain. <div>
arXiv:2512.00365v1 Announce Type: new 
Abstract: Human physical reasoning relies on internal "body" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering</title>
<link>https://arxiv.org/abs/2512.00368</link>
<guid>https://arxiv.org/abs/2512.00368</guid>
<content:encoded><![CDATA[
<div> Multi-View Clustering, Contrastive Learning, Noise Reduction, Hierarchical Fusion, Nearest Neighbors  

<br /><br />Summary:  
Multi-View Clustering (MVC) aims to group data samples by learning a unified consensus representation from multiple views. However, existing MVC methods face the challenge of untrustworthy fusion due to two main issues: the neglect of inherent noise in individual views and reliance on contrastive learning strategies that consider only cross-view similarities of the same instance, while overlooking structural information from nearest neighbors within clusters. To overcome these challenges, the authors propose a novel approach called Trusted Hierarchical Contrastive Representation Learning (THCRL). THCRL contains two core modules: the Deep Symmetry Hierarchical Fusion (DSHF) module and the Average K-Nearest Neighbors Contrastive Learning (AKCL) module. The DSHF module employs a UNet-based architecture with integrated denoising mechanisms to robustly fuse multi-view data, effectively addressing noise-related issues. The AKCL module aligns the fused representations with the individual view-specific ones by enhancing similarity among samples within the same cluster rather than focusing solely on the same sample across views. This strategy leverages nearest neighbor structural information to guide fusion more accurately. Experimental results validate that THCRL achieves state-of-the-art performance in deep multi-view clustering tasks, demonstrating its effectiveness in producing more reliable and confident fused representations. <div>
arXiv:2512.00368v1 Announce Type: new 
Abstract: Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.00369</link>
<guid>https://arxiv.org/abs/2512.00369</guid>
<content:encoded><![CDATA[
<div> Inversion-Denoising, diffusion models, noise approximation error, POLARIS, image restoration<br /><br />Summary:<br /><br />The paper addresses a critical issue in the Inversion-Denoising Paradigm used in diffusion-based image editing and restoration: the accumulation of noise approximation errors during the inversion process. This error arises because the noise at each step t is approximated using predictions from the previous step t-1, leading to significant reconstruction degradation. To solve this, the authors propose a novel method named POLARIS (Projection-Orthogonal Least Squares for Robust and Adaptive Inversion), which reframes the inversion task from merely compensating for errors to identifying and addressing their origin. Instead of adjusting embeddings or latent codes to counteract drift, POLARIS treats the guidance scale (\(\omega\)) as a variable that changes step-wise and derives a mathematically sound formula to minimize inversion errors at every step. This approach can be implemented with just one line of code, making it efficient and practical. Experimental results show that POLARIS significantly reduces noise approximation errors, improves the quality of recovery latent variables, and enhances the accuracy of downstream tasks with minimal computational overhead. This advancement offers a robust and adaptive framework for improving image reconstruction and editing performance in diffusion models. <div>
arXiv:2512.00369v1 Announce Type: new 
Abstract: The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale {\omega} as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pore-scale Image Patch Dataset and A Comparative Evaluation of Pore-scale Facial Features</title>
<link>https://arxiv.org/abs/2512.00381</link>
<guid>https://arxiv.org/abs/2512.00381</guid>
<content:encoded><![CDATA[
<div> Keywords: facial skin, local descriptor matching, pore-scale image patch, deep learning descriptors, 3D face reconstruction<br /><br />Summary: The paper addresses the challenge of local descriptor matching in facial skin regions, which are characterized by weak texture and present difficulties in applications like facial motion analysis and 3D face reconstruction. It introduces the PorePatch dataset, a high-quality collection of pore-scale image patches created to support the development of deep learning-based descriptors in the facial domain, which has previously suffered from a lack of adequate datasets. The authors propose a Data-Model Co-Evolution (DMCE) framework, designed to progressively refine and generate this dataset from high-resolution facial images, ensuring high quality. Using PorePatch, state-of-the-art (SOTA) descriptor models are trained and benchmarked, yielding a significant improvement in matching performance: a false positive rate at 95% recall (FPR95) of 1.91%, outperforming the traditional PSIFT descriptor which scores 22.41%. Despite these advances in matching accuracy, the enhanced deep learning descriptors show limited advantage in the downstream task of 3D face reconstruction, where their performance is comparable to that of traditional descriptors. This outcome reveals the continuing challenges and suggests that deep learning solutions still have considerable room for improvement in handling weak-texture facial regions. <div>
arXiv:2512.00381v1 Announce Type: new 
Abstract: The weak-texture nature of facial skin regions presents significant challenges for local descriptor matching in applications such as facial motion analysis and 3D face reconstruction. Although deep learning-based descriptors have demonstrated superior performance to traditional hand-crafted descriptors in many applications, the scarcity of pore-scale image patch datasets has hindered their further development in the facial domain. In this paper, we propose the PorePatch dataset, a high-quality pore-scale image patch dataset, and establish a rational evaluation benchmark. We introduce a Data-Model Co-Evolution (DMCE) framework to generate a progressively refined, high-quality dataset from high-resolution facial images. We then train existing SOTA models on our dataset and conduct extensive experiments. Our results show that the SOTA model achieves a FPR95 value of 1.91% on the matching task, outperforming PSIFT (22.41%) by a margin of 20.5%. However, its advantage is diminished in the 3D reconstruction task, where its overall performance is not significantly better than that of traditional descriptors. This indicates that deep learning descriptors still have limitations in addressing the challenges of facial weak-texture regions, and much work remains to be done in this field.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EZ-SP: Fast and Lightweight Superpoint-Based 3D Segmentation</title>
<link>https://arxiv.org/abs/2512.00385</link>
<guid>https://arxiv.org/abs/2512.00385</guid>
<content:encoded><![CDATA[
<div> Superpoint, 3D semantic segmentation, GPU partitioning, real-time inference, multi-domain

<br /><br />Summary: This paper presents a novel superpoint-based pipeline for 3D semantic segmentation that addresses the common CPU-bound bottleneck seen in partitioning steps. The authors introduce a fully GPU-based, learnable partitioning algorithm that achieves a 13× speedup compared to previous methods by generating superpoints that are both geometrically and semantically coherent. The proposed module is highly compact with fewer than 60,000 parameters and can be trained within 20 minutes using a differentiable surrogate loss without relying on handcrafted features. When combined with a lightweight superpoint classifier, the entire pipeline requires less than 2 MB of VRAM, seamlessly scales to scenes with millions of points, and supports real-time inference. The approach, named EZ-SP, offers an impressive 72× faster inference speed with 120× fewer parameters while matching the accuracy of state-of-the-art point-based models across multiple domains, including indoor scans (S3DIS), autonomous driving data (KITTI-360), and aerial LiDAR scans (DALES). Additionally, the authors provide access to their code and pretrained models on GitHub, facilitating adoption and further research in efficient 3D segmentation pipelines. <div>
arXiv:2512.00385v1 Announce Type: new 
Abstract: Superpoint-based pipelines provide an efficient alternative to point- or voxel-based 3D semantic segmentation, but are often bottlenecked by their CPU-bound partition step. We propose a learnable, fully GPU partitioning algorithm that generates geometrically and semantically coherent superpoints 13$\times$ faster than prior methods. Our module is compact (under 60k parameters), trains in under 20 minutes with a differentiable surrogate loss, and requires no handcrafted features. Combine with a lightweight superpoint classifier, the full pipeline fits in $<$2 MB of VRAM, scales to multi-million-point scenes, and supports real-time inference. With 72$\times$ faster inference and 120$\times$ fewer parameters, EZ-SP matches the accuracy of point-based SOTA models across three domains: indoor scans (S3DIS), autonomous driving (KITTI-360), and aerial LiDAR (DALES). Code and pretrained models are accessible at github.com/drprojects/superpoint_transformer.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing</title>
<link>https://arxiv.org/abs/2512.00387</link>
<guid>https://arxiv.org/abs/2512.00387</guid>
<content:encoded><![CDATA[
<div> Keywords: WiseEdit, image editing, cognition, creativity, benchmark<br /><br />Summary:<br /><br />This paper presents WiseEdit, a novel benchmark designed to evaluate advanced image editing models with greater depth and breadth than existing benchmarks. WiseEdit addresses the limitation of current evaluations that fail to comprehensively assess cognition- and creativity-informed image editing capabilities. The benchmark models image editing after human cognitive creation by decomposing the process into three sequential steps: Awareness, Interpretation, and Imagination. Each step acts as a distinct task that challenges models to demonstrate specific cognitive abilities. In addition to these discrete tasks, WiseEdit includes complex image editing cases where completing any single step independently is insufficient. WiseEdit incorporates three fundamental knowledge types critical to cognitive tasks: Declarative knowledge (factual information), Procedural knowledge (how to perform tasks), and Metacognitive knowledge (awareness and control over cognitive processes). The dataset consists of 1,220 test cases developed to objectively reveal current state-of-the-art image editing models' limitations in knowledge-based cognitive reasoning and creative composition. The authors commit to publicly releasing the benchmark, evaluation code, and generated images to facilitate further research and improvement in intelligent image editing. The project webpage offers additional details and resources for interested researchers. <div>
arXiv:2512.00387v1 Announce Type: new 
Abstract: Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction</title>
<link>https://arxiv.org/abs/2512.00395</link>
<guid>https://arxiv.org/abs/2512.00395</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, segmentation, all-mask prediction, STAMP, inference speed  

<br /><br />Summary:  
The paper addresses a fundamental trilemma in integrating segmentation capabilities into Multimodal Large Language Models (MLLMs): maintaining dialogue ability, achieving high segmentation accuracy, and ensuring fast inference. Existing methods fall short by either harming dialogue capabilities through conflicting pixel-level embedding objectives or by sacrificing segmentation quality or speed due to autoregressive next-token prediction. To resolve this, the authors propose a novel all-mask prediction paradigm that separates dialogue generation from mask prediction. They introduce STAMP (Simultaneous Textual All-Mask Prediction), an MLLM that after producing a textual response predicts the entire segmentation mask in one non-autoregressive forward pass, treating mask prediction as a parallel "fill-in-the-blank" task over image patches. This design avoids conflicting objectives, maintains strong dialogue performance, leverages bidirectional spatial context for superior segmentation, and achieves exceptionally fast inference. Extensive experiments demonstrate that STAMP outperforms state-of-the-art segmentation methods across various benchmarks, successfully delivering a unified approach that excels simultaneously in dialogue ability, segmentation accuracy, and inference speed without compromise. <div>
arXiv:2512.00395v1 Announce Type: new 
Abstract: Integrating segmentation into Multimodal Large Language Models (MLLMs) presents a core trilemma: simultaneously preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference. Prevailing paradigms are forced into a compromise. Embedding prediction methods introduce a conflicting pixel-level objective that degrades the MLLM's general dialogue abilities. The alternative, next-token prediction, reframes segmentation as an autoregressive task, which preserves dialogue but forces a trade-off between poor segmentation performance with sparse outputs or prohibitive inference speeds with rich ones. We resolve this trilemma with all-mask prediction, a novel paradigm that decouples autoregressive dialogue generation from non-autoregressive mask prediction. We present STAMP: Simultaneous Textual All-Mask Prediction, an MLLM that embodies this paradigm. After generating a textual response, STAMP predicts an entire segmentation mask in a single forward pass by treating it as a parallel "fill-in-the-blank" task over image patches. This design maintains the MLLM's dialogue ability by avoiding conflicting objectives, enables high segmentation performance by leveraging rich, bidirectional spatial context for all mask tokens, and achieves exceptional speed. Extensive experiments show that STAMP significantly outperforms state-of-the-art methods across multiple segmentation benchmarks, providing a solution that excels in dialogue, segmentation, and speed without compromise.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Bitrate Video Compression through Semantic-Conditioned Diffusion</title>
<link>https://arxiv.org/abs/2512.00408</link>
<guid>https://arxiv.org/abs/2512.00408</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic video compression, video diffusion model, ultra-low bitrate, multimodal generation, perceptual quality<br /><br />Summary:<br /><br />This paper addresses the limitations of traditional video codecs at ultra-low bitrates, where pixel fidelity-focused methods fail and create severe visual artifacts. The authors propose DiSCo, a semantic video compression framework that emphasizes transmitting semantically meaningful information rather than raw pixels. DiSCo decomposes the source video into three compact modalities: textual descriptions (capturing semantics), spatiotemporally degraded videos (capturing appearance), and optional sketches or poses (capturing motion). These representations are much more compact and meaningful than pixel data at low bitrates. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact modalities, leveraging generative priors to synthesize fine details. The method introduces new techniques such as temporal forward filling, token interleaving, and modality-specific codecs to enhance multimodal generation quality and compression efficiency. Experimental results demonstrate that DiSCo surpasses both traditional and baseline semantic codecs by 2 to 10 times on perceptual quality metrics when operating at low bitrates. This work reveals the advantage of semantic-driven video compression paired with generative models in preserving human-perceived video quality where conventional codecs struggle. <div>
arXiv:2512.00408v1 Announce Type: new 
Abstract: Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control</title>
<link>https://arxiv.org/abs/2512.00413</link>
<guid>https://arxiv.org/abs/2512.00413</guid>
<content:encoded><![CDATA[
arXiv:2512.00413v1 Announce Type: new 
Abstract: Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysGen: Physically Grounded 3D Shape Generation for Industrial Design</title>
<link>https://arxiv.org/abs/2512.00422</link>
<guid>https://arxiv.org/abs/2512.00422</guid>
<content:encoded><![CDATA[
arXiv:2512.00422v1 Announce Type: new 
Abstract: Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recovering Origin Destination Flows from Bus CCTV: Early Results from Nairobi and Kigali</title>
<link>https://arxiv.org/abs/2512.00424</link>
<guid>https://arxiv.org/abs/2512.00424</guid>
<content:encoded><![CDATA[
arXiv:2512.00424v1 Announce Type: new 
Abstract: Public transport in sub-Saharan Africa (SSA) often operates in overcrowded conditions where existing automated systems fail to capture reliable passenger flow data. Leveraging onboard CCTV already deployed for security, we present a baseline pipeline that combines YOLOv12 detection, BotSORT tracking, OSNet embeddings, OCR-based timestamping, and telematics-based stop classification to recover bus origin--destination (OD) flows. On annotated CCTV segments from Nairobi and Kigali buses, the system attains high counting accuracy under low-density, well-lit conditions (recall $\approx$95\%, precision $\approx$91\%, F1 $\approx$93\%). It produces OD matrices that closely match manual tallies. Under realistic stressors such as overcrowding, color-to-monochrome shifts, posture variation, and non-standard door use, performance degrades sharply (e.g., $\sim$40\% undercount in peak-hour boarding and a $\sim$17 percentage-point drop in recall for monochrome segments), revealing deployment-specific failure modes and motivating more robust, deployment-focused Re-ID methods for SSA transit.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards</title>
<link>https://arxiv.org/abs/2512.00425</link>
<guid>https://arxiv.org/abs/2512.00425</guid>
<content:encoded><![CDATA[
arXiv:2512.00425v1 Announce Type: new 
Abstract: Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\texttt{NewtonRewards}$ extracts $\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana</title>
<link>https://arxiv.org/abs/2512.00428</link>
<guid>https://arxiv.org/abs/2512.00428</guid>
<content:encoded><![CDATA[
arXiv:2512.00428v1 Announce Type: new 
Abstract: We trained a classifier with synthetic chest X-ray (CXR) images generated by Nano Banana, the latest AI model for image generation and editing, released by Google. When directly applied to real-world CXRs having only been trained with synthetic data, the classifier achieved an AUROC of 0.923 (95% CI: 0.919 - 0.927), and an AUPR of 0.900 (95% CI: 0.894 - 0.907) in recognizing pneumonia in the 2018 RSNA Pneumonia Detection dataset (14,863 CXRs), and an AUROC of 0.824 (95% CI: 0.810 - 0.836), and an AUPR of 0.913 (95% CI: 0.904 - 0.922) in the Chest X-Ray dataset (5,856 CXRs). These external validation results on real-world data demonstrate the feasibility of this approach and suggest potential for synthetic data in medical AI development. Nonetheless, several limitations remain at present, including challenges in prompt design for controlling the diversity of synthetic CXR data and the requirement for post-processing to ensure alignment with real-world data. However, the growing sophistication and accessibility of medical intelligence will necessitate substantial validation, regulatory approval, and ethical oversight prior to clinical translation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal</title>
<link>https://arxiv.org/abs/2512.00438</link>
<guid>https://arxiv.org/abs/2512.00438</guid>
<content:encoded><![CDATA[
arXiv:2512.00438v1 Announce Type: new 
Abstract: Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RecruitView: A Multimodal Dataset for Predicting Personality and Interview Performance for Human Resources Applications</title>
<link>https://arxiv.org/abs/2512.00450</link>
<guid>https://arxiv.org/abs/2512.00450</guid>
<content:encoded><![CDATA[
arXiv:2512.00450v1 Announce Type: new 
Abstract: Automated personality and soft skill assessment from multimodal behavioral data remains challenging due to limited datasets and methods that fail to capture geometric structure inherent in human traits. We introduce RecruitView, a dataset of 2,011 naturalistic video interview clips from 300+ participants with 27,000 pairwise comparative judgments across 12 dimensions: Big Five personality traits, overall personality score, and six interview performance metrics. To leverage this data, we propose Cross-Modal Regression with Manifold Fusion (CRMF), a geometric deep learning framework that explicitly models behavioral representations across hyperbolic, spherical, and Euclidean manifolds. CRMF employs geometry-specific expert networks to capture hierarchical trait structures, directional behavioral patterns, and continuous performance variations simultaneously. An adaptive routing mechanism dynamically weights expert contributions based on input characteristics. Through principled tangent space fusion, CRMF achieves superior performance while training 40-50% fewer trainable parameters than large multimodal models. Extensive experiments demonstrate that CRMF substantially outperforms the selected baselines, achieving up to 11.4% improvement in Spearman correlation and 6.0% in concordance index. Our RecruitView dataset is publicly available at https://huggingface.co/datasets/AI4A-lab/RecruitView
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CausalAffect: Causal Discovery for Facial Affective Understanding</title>
<link>https://arxiv.org/abs/2512.00456</link>
<guid>https://arxiv.org/abs/2512.00456</guid>
<content:encoded><![CDATA[
arXiv:2512.00456v1 Announce Type: new 
Abstract: Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards</title>
<link>https://arxiv.org/abs/2512.00473</link>
<guid>https://arxiv.org/abs/2512.00473</guid>
<content:encoded><![CDATA[
arXiv:2512.00473v1 Announce Type: new 
Abstract: With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce " fake" images with distinct AI artifacts, often characterized by "overly smooth skin" and "oily facial sheens". To recapture the original goal of "indistinguishable-from-reality" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a "Detector Reward" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Context Learning for Generic Event Boundary Detection</title>
<link>https://arxiv.org/abs/2512.00475</link>
<guid>https://arxiv.org/abs/2512.00475</guid>
<content:encoded><![CDATA[
arXiv:2512.00475v1 Announce Type: new 
Abstract: Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What Helps: Task-Aligned Context Selection for Vision Tasks</title>
<link>https://arxiv.org/abs/2512.00489</link>
<guid>https://arxiv.org/abs/2512.00489</guid>
<content:encoded><![CDATA[
arXiv:2512.00489v1 Announce Type: new 
Abstract: Humans often resolve visual uncertainty by comparing an image with relevant examples, but ViTs lack the ability to identify which examples would improve their predictions. We present Task-Aligned Context Selection (TACS), a framework that learns to select paired examples which truly improve task performance rather than those that merely appear similar. TACS jointly trains a selector network with the task model through a hybrid optimization scheme combining gradient-based supervision and reinforcement learning, making retrieval part of the learning objective. By aligning selection with task rewards, TACS enables discriminative models to discover which contextual examples genuinely help. Across 18 datasets covering fine-grained recognition, medical image classification, and medical image segmentation, TACS consistently outperforms similarity-based retrieval, particularly in challenging or data-limited settings.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration</title>
<link>https://arxiv.org/abs/2512.00493</link>
<guid>https://arxiv.org/abs/2512.00493</guid>
<content:encoded><![CDATA[
arXiv:2512.00493v1 Announce Type: new 
Abstract: High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Terrain Sensing with Smartphone Structured Light: 2D Dynamic Time Warping for Grid Pattern Matching</title>
<link>https://arxiv.org/abs/2512.00514</link>
<guid>https://arxiv.org/abs/2512.00514</guid>
<content:encoded><![CDATA[
arXiv:2512.00514v1 Announce Type: new 
Abstract: Low-cost mobile rovers often operate on uneven terrain where small bumps or tilts are difficult to perceive visually but can significantly affect locomotion stability. To address this problem, we explore a smartphone-based structured-light system that projects a grid pattern onto the ground and reconstructs local terrain unevenness from a single handheld device. The system is inspired by face-recognition projectors, but adapted for ground sensing. A key technical challenge is robustly matching the projected grid with its deformed observation under perspective distortion and partial occlusion. Conventional one-dimensional dynamic time warping (1D-DTW) is not directly applicable to such two-dimensional grid patterns. We therefore propose a topology-constrained two-dimensional dynamic time warping (2D-DTW) algorithm that performs column-wise alignment under a global grid consistency constraint. The proposed method is designed to be simple enough to run on resource limited platforms while preserving the grid structure required for accurate triangulation. We demonstrate that our 2D-DTW formulation can be used not only for terrain sensing but also as a general tool for matching structured grid patterns in image processing scenarios. This paper describes the overall system design as well as the 2D-DTW extension that emerged from this application.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Generation as a Visual Planner for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.00532</link>
<guid>https://arxiv.org/abs/2512.00532</guid>
<content:encoded><![CDATA[
arXiv:2512.00532v1 Announce Type: new 
Abstract: Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.
  We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.
  Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update</title>
<link>https://arxiv.org/abs/2512.00534</link>
<guid>https://arxiv.org/abs/2512.00534</guid>
<content:encoded><![CDATA[
arXiv:2512.00534v1 Announce Type: new 
Abstract: Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning</title>
<link>https://arxiv.org/abs/2512.00539</link>
<guid>https://arxiv.org/abs/2512.00539</guid>
<content:encoded><![CDATA[
arXiv:2512.00539v1 Announce Type: new 
Abstract: The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\% and 40.57\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\% compared to the current SOTA method.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions</title>
<link>https://arxiv.org/abs/2512.00547</link>
<guid>https://arxiv.org/abs/2512.00547</guid>
<content:encoded><![CDATA[
arXiv:2512.00547v1 Announce Type: new 
Abstract: Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives</title>
<link>https://arxiv.org/abs/2512.00557</link>
<guid>https://arxiv.org/abs/2512.00557</guid>
<content:encoded><![CDATA[
arXiv:2512.00557v1 Announce Type: new 
Abstract: What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Describe Anything Anywhere At Any Moment</title>
<link>https://arxiv.org/abs/2512.00565</link>
<guid>https://arxiv.org/abs/2512.00565</guid>
<content:encoded><![CDATA[
arXiv:2512.00565v1 Announce Type: new 
Abstract: Computer vision and robotics applications ranging from augmented reality to robot autonomy in large-scale environments require spatio-temporal memory frameworks that capture both geometric structure for accurate language-grounding as well as semantic detail. Existing methods face a tradeoff, where producing rich open-vocabulary descriptions comes at the expense of real-time performance when these descriptions have to be grounded in 3D. To address these challenges, we propose Describe Anything, Anywhere, at Any Moment (DAAAM), a novel spatio-temporal memory framework for large-scale and real-time 4D scene understanding. DAAAM introduces a novel optimization-based frontend to infer detailed semantic descriptions from localized captioning models, such as the Describe Anything Model (DAM), leveraging batch processing to speed up inference by an order of magnitude for online processing. It leverages such semantic understanding to build a hierarchical 4D scene graph (SG), which acts as an effective globally spatially and temporally consistent memory representation. DAAAM constructs 4D SGs with detailed, geometrically grounded descriptions while maintaining real-time performance. We show that DAAAM's 4D SG interfaces well with a tool-calling agent for inference and reasoning.
  We thoroughly evaluate DAAAM in the complex task of spatio-temporal question answering on the NaVQA benchmark and show its generalization capabilities for sequential task grounding on the SG3D benchmark. We further curate an extended OC-NaVQA benchmark for large-scale and long-time evaluations. DAAAM achieves state-of-the-art results in both tasks, improving OC-NaVQA question accuracy by 53.6%, position errors by 21.9%, temporal errors by 21.6%, and SG3D task grounding accuracy by 27.8% over the most competitive baselines, respectively. We release our data and code open-source.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models</title>
<link>https://arxiv.org/abs/2512.00572</link>
<guid>https://arxiv.org/abs/2512.00572</guid>
<content:encoded><![CDATA[
arXiv:2512.00572v1 Announce Type: new 
Abstract: Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception) using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross validation analysis.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension</title>
<link>https://arxiv.org/abs/2512.00582</link>
<guid>https://arxiv.org/abs/2512.00582</guid>
<content:encoded><![CDATA[
arXiv:2512.00582v1 Announce Type: new 
Abstract: Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Down to Scale Up: Towards Operationally-Efficient and Deployable Clinical Models via Cross-Modal Low-Rank Adaptation for Medical Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.00597</link>
<guid>https://arxiv.org/abs/2512.00597</guid>
<content:encoded><![CDATA[
arXiv:2512.00597v1 Announce Type: new 
Abstract: Foundation models trained via vision-language pretraining have demonstrated strong zero-shot capabilities across diverse image domains, yet their application to volumetric medical imaging remains limited. We introduce MedCT-VLM: Medical CT Vision-Language Model, a parameter-efficient vision-language framework designed to adapt large-scale CT foundation models for downstream clinical tasks. MedCT-VLM uses a parameter-efficient approach to adapt CT-CLIP, a contrastive vision-language model trained on 25,692 chest CT volumes, for multi-label pathology classification using Low-Rank Adaptation (LoRA). Rather than fine-tuning the model's 440 M parameters directly, we insert low-rank decomposition matrices into attention layers of both vision and text encoders, training only 1.67M parameters (0.38\% of total). We evaluate on zero-shot classification across 18 thoracic pathologies, where the model must align CT embeddings with unseen text prompts at inference without task-specific training. LoRA fine-tuning improves mean AUROC from 61.3\% to 68.9\% (+7.6 pp), accuracy from 67.2\% to 73.6\% (+6.4 pp), and macro-F1 from 32.1\% to 36.9\% (+4.8 pp). These results demonstrate that parameter-efficient methods can effectively transfer large-scale pretraining to downstream medical imaging tasks, particularly for zero-shot scenarios where labeled data is scarce.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning</title>
<link>https://arxiv.org/abs/2512.00625</link>
<guid>https://arxiv.org/abs/2512.00625</guid>
<content:encoded><![CDATA[
arXiv:2512.00625v1 Announce Type: new 
Abstract: Pith detection in tree cross-sections is essential for forestry and wood quality analysis but remains a manual, error-prone task. This study evaluates deep learning models -- YOLOv9, U-Net, Swin Transformer, DeepLabV3, and Mask R-CNN -- to automate the process efficiently. A dataset of 582 labeled images was dynamically augmented to improve generalization. Swin Transformer achieved the highest accuracy (0.94), excelling in fine segmentation. YOLOv9 performed well for bounding box detection but struggled with boundary precision. U-Net was effective for structured patterns, while DeepLabV3 captured multi-scale features with slight boundary imprecision. Mask R-CNN initially underperformed due to overlapping detections, but applying Non-Maximum Suppression (NMS) improved its IoU from 0.45 to 0.80. Generalizability was next tested using an oak dataset of 11 images from Oregon State University's Tree Ring Lab. Additionally, for exploratory analysis purposes, an additional dataset of 64 labeled tree cross-sections was used to train the worst-performing model to see if this would improve its performance generalizing to the unseen oak dataset. Key challenges included tensor mismatches and boundary inconsistencies, addressed through hyperparameter tuning and augmentation. Our results highlight deep learning's potential for tree cross-section pith detection, with model choice depending on dataset characteristics and application needs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance</title>
<link>https://arxiv.org/abs/2512.00626</link>
<guid>https://arxiv.org/abs/2512.00626</guid>
<content:encoded><![CDATA[
arXiv:2512.00626v1 Announce Type: new 
Abstract: Accurate and timely diagnosis of multi-class skin lesions is hampered by subjective methods, inherent data imbalance in datasets like HAM10000, and the "black box" nature of Deep Learning (DL) models. This study proposes a trustworthy and highly accurate Computer-Aided Diagnosis (CAD) system to overcome these limitations. The approach utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) for per class data augmentation to resolve the critical class imbalance problem. A fine-tuned ResNet-50 classifier is then trained on the augmented dataset to classify seven skin disease categories. Crucially, LIME and SHAP Explainable AI (XAI) techniques are integrated to provide transparency by confirming that predictions are based on clinically relevant features like irregular morphology. The system achieved a high overall Accuracy of 92.50 % and a Macro-AUC of 98.82 %, successfully outperforming various prior benchmarked architectures. This work successfully validates a verifiable framework that combines high performance with the essential clinical interpretability required for safe diagnostic deployment. Future research should prioritize enhancing discrimination for critical categories, such as Melanoma NOS (F1-Score is 0.8602).
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation</title>
<link>https://arxiv.org/abs/2512.00639</link>
<guid>https://arxiv.org/abs/2512.00639</guid>
<content:encoded><![CDATA[
arXiv:2512.00639v1 Announce Type: new 
Abstract: The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Attention Network with Adversarial Domain Alignment for Robust Cross-Domain Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2512.00641</link>
<guid>https://arxiv.org/abs/2512.00641</guid>
<content:encoded><![CDATA[
arXiv:2512.00641v1 Announce Type: new 
Abstract: Cross-domain facial expression recognition (CD-FER) remains difficult due to severe domain shift between training and deployment data. We propose Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA), a hybrid framework that couples a ResNet-50 as backbone with a batch-level Graph Attention Network (GAT) to model inter-sample relations under shift. Each mini-batch is cast as a sparse ring graph so that attention aggregates cross-sample cues that are informative for adaptation. To align distributions, GAT-ADA combines adversarial learning via a Gradient Reversal Layer (GRL) with statistical alignment using CORAL and MMD. GAT-ADA is evaluated under a standard unsupervised domain adaptation protocol: training on one labeled source (RAF-DB) and adapting to multiple unlabeled targets (CK+, JAFFE, SFEW 2.0, FER2013, and ExpW). GAT-ADA attains 74.39% mean cross-domain accuracy. On RAF-DB to FER2013, it reaches 98.0% accuracy, corresponding to approximately a 36-point improvement over the best baseline we re-implemented with the same backbone and preprocessing.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba</title>
<link>https://arxiv.org/abs/2512.00647</link>
<guid>https://arxiv.org/abs/2512.00647</guid>
<content:encoded><![CDATA[
arXiv:2512.00647v1 Announce Type: new 
Abstract: Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss, as they discard or compress token representations. This problem is exacerbated when applied uniformly to fine-grained token representations across all images, regardless of visual complexity. We observe that not all inputs require fine-grained processing. Simple images can be effectively handled at coarse resolution, while only complex ones may warrant refinement. Based on this insight, we propose \textit{Coarse-to-Fine Vision Mamba (CF-ViM)}, an adaptive framework for efficient inference. CF-ViM first performs coarse-grained inference by dividing the input image into large patches, significantly reducing the token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover critical visual details with minimal additional cost. This dynamic resolution assignment strategy allows CF-ViM to allocate computation adaptively according to image complexity, ensuring efficient processing without compromising essential visual information. Experiments on ImageNet demonstrate that CF-ViM outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realistic Handwritten Multi-Digit Writer (MDW) Number Recognition Challenges</title>
<link>https://arxiv.org/abs/2512.00676</link>
<guid>https://arxiv.org/abs/2512.00676</guid>
<content:encoded><![CDATA[
arXiv:2512.00676v1 Announce Type: new 
Abstract: Isolated digit classification has served as a motivating problem for decades of machine learning research. In real settings, numbers often occur as multiple digits, all written by the same person. Examples include ZIP Codes, handwritten check amounts, and appointment times. In this work, we leverage knowledge about the writers of NIST digit images to create more realistic benchmark multi-digit writer (MDW) data sets. As expected, we find that classifiers may perform well on isolated digits yet do poorly on multi-digit number recognition. If we want to solve real number recognition problems, additional advances are needed. The MDW benchmarks come with task-specific performance metrics that go beyond typical error calculations to more closely align with real-world impact. They also create opportunities to develop methods that can leverage task-specific knowledge to improve performance well beyond that of individual digit classification methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer</title>
<link>https://arxiv.org/abs/2512.00677</link>
<guid>https://arxiv.org/abs/2512.00677</guid>
<content:encoded><![CDATA[
arXiv:2512.00677v1 Announce Type: new 
Abstract: Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Silhouette-based Gait Foundation Model</title>
<link>https://arxiv.org/abs/2512.00691</link>
<guid>https://arxiv.org/abs/2512.00691</guid>
<content:encoded><![CDATA[
arXiv:2512.00691v1 Announce Type: new 
Abstract: Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affordance-First Decomposition for Continual Learning in Video-Language Understanding</title>
<link>https://arxiv.org/abs/2512.00694</link>
<guid>https://arxiv.org/abs/2512.00694</guid>
<content:encoded><![CDATA[
arXiv:2512.00694v1 Announce Type: new 
Abstract: Continual learning for video--language understanding is increasingly important as models face non-stationary data, domains, and query styles, yet prevailing solutions blur what should stay stable versus what should adapt, rely on static routing/capacity, or require replaying past videos. We aim to explicitly specify where stability lives and where plasticity should be focused under realistic memory and privacy constraints. We introduce Affordance-First Decomposition (AFD): videos are mapped to slowly varying affordance tokens that form a shared, time-aligned substrate, while a lightweight, query-routed, conflict-aware scheduler concentrates adaptation and grows capacity only when needed. The substrate is stabilized via weak alignment and teacher consistency, and training uses question-only replay. AFD achieves state-of-the-art across protocols: 51.6% average accuracy with -1.8% forgetting on domain-incremental VideoQA, ViLCo R@1@0.5 of 29.6% (MQ) and 20.7% (NLQ) with 18.4% stAP@0.25 (VQ), and 39.5% accuracy with -1.6% forgetting on time-incremental iVQA. Overall, AFD offers an explicit, interpretable split between a stable interaction-centered substrate and targeted adaptation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAR-Net: A Cascade Refinement Network for Rotational Motion Deblurring under Angle Information Uncertainty</title>
<link>https://arxiv.org/abs/2512.00700</link>
<guid>https://arxiv.org/abs/2512.00700</guid>
<content:encoded><![CDATA[
arXiv:2512.00700v1 Announce Type: new 
Abstract: We propose a new neural network architecture called CAR-net (CAscade Refinement Network) to deblur images that are subject to rotational motion blur. Our architecture is specifically designed for the semi-blind scenarios where only noisy information of the rotational motion blur angle is available. The core of our approach is progressive refinement process that starts with an initial deblurred estimate obtained from frequency-domain inversion; A series of refinement stages take the current deblurred image to predict and apply residual correction to the current estimate, progressively suppressing artifacts and restoring fine details. To handle parameter uncertainty, our architecture accommodates an optional angle detection module which can be trained end-to-end with refinement modules. We provide a detailed description of our architecture and illustrate its efficiency through experiments using both synthetic and real-life images. Our code and model as well as the links to the datasets are available at https://github.com/tony123105/CAR-Net
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation</title>
<link>https://arxiv.org/abs/2512.00706</link>
<guid>https://arxiv.org/abs/2512.00706</guid>
<content:encoded><![CDATA[
arXiv:2512.00706v1 Announce Type: new 
Abstract: Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical challenge.In this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-Based Computer Vision Models for Early Cancer Detection Using Multimodal Medical Imaging and Radiogenomic Integration Frameworks</title>
<link>https://arxiv.org/abs/2512.00714</link>
<guid>https://arxiv.org/abs/2512.00714</guid>
<content:encoded><![CDATA[
arXiv:2512.00714v1 Announce Type: new 
Abstract: Early cancer detection remains one of the most critical challenges in modern healthcare, where delayed diagnosis significantly reduces survival outcomes. Recent advancements in artificial intelligence, particularly deep learning, have enabled transformative progress in medical imaging analysis. Deep learning-based computer vision models, such as convolutional neural networks (CNNs), transformers, and hybrid attention architectures, can automatically extract complex spatial, morphological, and temporal patterns from multimodal imaging data including MRI, CT, PET, mammography, histopathology, and ultrasound. These models surpass traditional radiological assessment by identifying subtle tissue abnormalities and tumor microenvironment variations invisible to the human eye. At a broader scale, the integration of multimodal imaging with radiogenomics linking quantitative imaging features with genomics, transcriptomics, and epigenetic biomarkers has introduced a new paradigm for personalized oncology. This radiogenomic fusion allows the prediction of tumor genotype, immune response, molecular subtypes, and treatment resistance without invasive biopsies.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images</title>
<link>https://arxiv.org/abs/2512.00718</link>
<guid>https://arxiv.org/abs/2512.00718</guid>
<content:encoded><![CDATA[
arXiv:2512.00718v1 Announce Type: new 
Abstract: Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrajDiff: End-to-end Autonomous Driving without Perception Annotation</title>
<link>https://arxiv.org/abs/2512.00723</link>
<guid>https://arxiv.org/abs/2512.00723</guid>
<content:encoded><![CDATA[
arXiv:2512.00723v1 Announce Type: new 
Abstract: End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards</title>
<link>https://arxiv.org/abs/2512.00743</link>
<guid>https://arxiv.org/abs/2512.00743</guid>
<content:encoded><![CDATA[
arXiv:2512.00743v1 Announce Type: new 
Abstract: Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \emph{reward-based grouping} to compute advantages for each reward function \textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \textit{PickScore-25k} and multi-objective \textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Multi-scale Gated Transformer and Prior-guided Convolutional Network for Learned Image Compression</title>
<link>https://arxiv.org/abs/2512.00744</link>
<guid>https://arxiv.org/abs/2512.00744</guid>
<content:encoded><![CDATA[
arXiv:2512.00744v1 Announce Type: new 
Abstract: Recently, learned image compression methods have made remarkable achievements, some of which have outperformed the traditional image codec VVC. The advantages of learned image compression methods over traditional image codecs can be largely attributed to their powerful nonlinear transform coding. Convolutional layers and shifted window transformer (Swin-T) blocks are the basic units of neural networks, and their representation capabilities play an important role in nonlinear transform coding. In this paper, to improve the ability of the vanilla convolution to extract local features, we propose a novel prior-guided convolution (PGConv), where asymmetric convolutions (AConvs) and difference convolutions (DConvs) are introduced to strengthen skeleton elements and extract high-frequency information, respectively. A re-parameterization strategy is also used to reduce the computational complexity of PGConv. Moreover, to improve the ability of the Swin-T block to extract non-local features, we propose a novel multi-scale gated transformer (MGT), where dilated window-based multi-head self-attention blocks with different dilation rates and depth-wise convolution layers with different kernel sizes are used to extract multi-scale features, and a gate mechanism is introduced to enhance non-linearity. Finally, we propose a novel joint Multi-scale Gated Transformer and Prior-guided Convolutional Network (MGTPCN) for learned image compression. Experimental results show that our MGTPCN surpasses state-of-the-art algorithms with a better trade-off between performance and complexity.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization</title>
<link>https://arxiv.org/abs/2512.00748</link>
<guid>https://arxiv.org/abs/2512.00748</guid>
<content:encoded><![CDATA[
arXiv:2512.00748v1 Announce Type: new 
Abstract: Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and inter-observer variability in diagnosis. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generate diverse segmentation that lacks expert specificity or to produce personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Specifically, we introduce two latent variables to model expert annotation preferences and image boundary ambiguity. Their conditional probabilistic distributions are then obtained through variational inference, allowing segmentation outputs to be generated by sampling from these distributions. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized. Code can be found in https://github.com/AI4MOL/ProSeg.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Charts Are Not Images: On the Challenges of Scientific Chart Editing</title>
<link>https://arxiv.org/abs/2512.00752</link>
<guid>https://arxiv.org/abs/2512.00752</guid>
<content:encoded><![CDATA[
arXiv:2512.00752v1 Announce Type: new 
Abstract: Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing the Wind from a Falling Leaf</title>
<link>https://arxiv.org/abs/2512.00762</link>
<guid>https://arxiv.org/abs/2512.00762</guid>
<content:encoded><![CDATA[
arXiv:2512.00762v1 Announce Type: new 
Abstract: A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \href{https://chaoren2357.github.io/seeingthewind/}{project page}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches</title>
<link>https://arxiv.org/abs/2512.00765</link>
<guid>https://arxiv.org/abs/2512.00765</guid>
<content:encoded><![CDATA[
arXiv:2512.00765v1 Announce Type: new 
Abstract: Intelligent driving systems are vulnerable to physical adversarial attacks on traffic signs. These attacks can cause misclassification, leading to erroneous driving decisions that compromise road safety. Moreover, within V2X networks, such misinterpretations can propagate, inducing cascading failures that disrupt overall traffic flow and system stability. However, a key limitation of current physical attacks is their lack of stealth. Most methods apply perturbations to central regions of the sign, resulting in visually salient patterns that are easily detectable by human observers, thereby limiting their real-world practicality. This study proposes TESP-Attack, a novel stealth-aware adversarial patch method for traffic sign classification. Based on the observation that human visual attention primarily focuses on the central regions of traffic signs, we employ instance segmentation to generate edge-aligned masks that conform to the shape characteristics of the signs. A U-Net generator is utilized to craft adversarial patches, which are then optimized through color and texture constraints along with frequency domain analysis to achieve seamless integration with the background environment, resulting in highly effective visual concealment. The proposed method demonstrates outstanding attack success rates across traffic sign classification models with varied architectures, achieving over 90% under limited query budgets. It also exhibits strong cross-model transferability and maintains robust real-world performance that remains stable under varying angles and distances.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes</title>
<link>https://arxiv.org/abs/2512.00771</link>
<guid>https://arxiv.org/abs/2512.00771</guid>
<content:encoded><![CDATA[
arXiv:2512.00771v1 Announce Type: new 
Abstract: Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering</title>
<link>https://arxiv.org/abs/2512.00773</link>
<guid>https://arxiv.org/abs/2512.00773</guid>
<content:encoded><![CDATA[
arXiv:2512.00773v1 Announce Type: new 
Abstract: This work addresses the scarcity of high-quality, large-scale resources for Japanese Vision-and-Language (V&amp;L) modeling. We present a scalable and reproducible pipeline that integrates large-scale web collection with rigorous filtering/deduplication, object-detection-driven evidence extraction, and Large Language Model (LLM)-based refinement under grounding constraints. Using this pipeline, we build two resources: an image-caption dataset (DEJIMA-Cap) and a VQA dataset (DEJIMA-VQA), each containing 3.88M image-text pairs, far exceeding the size of existing Japanese V&amp;L datasets. Human evaluations demonstrate that DEJIMA achieves substantially higher Japaneseness and linguistic naturalness than datasets constructed via translation or manual annotation, while maintaining factual correctness at a level comparable to human-annotated corpora. Quantitative analyses of image feature distributions further confirm that DEJIMA broadly covers diverse visual domains characteristic of Japan, complementing its linguistic and cultural representativeness. Models trained on DEJIMA exhibit consistent improvements across multiple Japanese multimodal benchmarks, confirming that culturally grounded, large-scale resources play a key role in enhancing model performance. All data sources and modules in our pipeline are licensed for commercial use, and we publicly release the resulting dataset and metadata to encourage further research and industrial applications in Japanese V&amp;L modeling.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery</title>
<link>https://arxiv.org/abs/2512.00794</link>
<guid>https://arxiv.org/abs/2512.00794</guid>
<content:encoded><![CDATA[
arXiv:2512.00794v1 Announce Type: new 
Abstract: Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target</title>
<link>https://arxiv.org/abs/2512.00796</link>
<guid>https://arxiv.org/abs/2512.00796</guid>
<content:encoded><![CDATA[
arXiv:2512.00796v1 Announce Type: new 
Abstract: The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.00805</link>
<guid>https://arxiv.org/abs/2512.00805</guid>
<content:encoded><![CDATA[
arXiv:2512.00805v1 Announce Type: new 
Abstract: Long video understanding is essential for human-like intelligence, enabling coherent perception and reasoning over extended temporal contexts. While the emerging thinking-with-frames paradigm, which alternates between global temporal reasoning and local frame examination, has advanced the reasoning capabilities of video multi-modal large language models (MLLMs), it suffers from a significant efficiency bottleneck due to the progressively growing and redundant multi-modal context. To address this, we propose SpecTemp, a reinforcement learning-based Speculative Temporal reasoning framework that decouples temporal perception from reasoning via a cooperative dual-model design. In SpecTemp, a lightweight draft MLLM rapidly explores and proposes salient frames from densely sampled temporal regions, while a powerful target MLLM focuses on temporal reasoning and verifies the draft's proposals, iteratively refining its attention until convergence. This design mirrors the collaborative pathways of the human brain, balancing efficiency with accuracy. To support training, we construct the SpecTemp-80K dataset, featuring synchronized dual-level annotations for coarse evidence spans and fine-grained frame-level evidence. Experiments across multiple video understanding benchmarks demonstrate that SpecTemp not only maintains competitive accuracy but also significantly accelerates inference compared with existing thinking-with-frames methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IRPO: Boosting Image Restoration via Post-training GRPO</title>
<link>https://arxiv.org/abs/2512.00814</link>
<guid>https://arxiv.org/abs/2512.00814</guid>
<content:encoded><![CDATA[
arXiv:2512.00814v1 Announce Type: new 
Abstract: Recent advances in post-training paradigms have achieved remarkable success in high-level generation tasks, yet their potential for low-level vision remains rarely explored. Existing image restoration (IR) methods rely on pixel-level hard-fitting to ground-truth images, struggling with over-smoothing and poor generalization. To address these limitations, we propose IRPO, a low-level GRPO-based post-training paradigm that systematically explores both data formulation and reward modeling. We first explore a data formulation principle for low-level post-training paradigm, in which selecting underperforming samples from the pre-training stage yields optimal performance and improved efficiency. Furthermore, we model a reward-level criteria system that balances objective accuracy and human perceptual preference through three complementary components: a General Reward for structural fidelity, an Expert Reward leveraging Qwen-VL for perceptual alignment, and a Restoration Reward for task-specific low-level quality. Comprehensive experiments on six in-domain and five out-of-domain (OOD) low-level benchmarks demonstrate that IRPO achieves state-of-the-art results across diverse degradation types, surpassing the AdaIR baseline by 0.83 dB on in-domain tasks and 3.43 dB on OOD settings. Our code can be shown in https://github.com/HaoxuanXU1024/IRPO.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PanFlow: Decoupled Motion Control for Panoramic Video Generation</title>
<link>https://arxiv.org/abs/2512.00832</link>
<guid>https://arxiv.org/abs/2512.00832</guid>
<content:encoded><![CDATA[
arXiv:2512.00832v1 Announce Type: new 
Abstract: Panoramic video generation has attracted growing attention due to its applications in virtual reality and immersive media. However, existing methods lack explicit motion control and struggle to generate scenes with large and complex motions. We propose PanFlow, a novel approach that exploits the spherical nature of panoramas to decouple the highly dynamic camera rotation from the input optical flow condition, enabling more precise control over large and dynamic motions. We further introduce a spherical noise warping strategy to promote loop consistency in motion across panorama boundaries. To support effective training, we curate a large-scale, motion-rich panoramic video dataset with frame-level pose and flow annotations. We also showcase the effectiveness of our method in various applications, including motion transfer and video editing. Extensive experiments demonstrate that PanFlow significantly outperforms prior methods in motion fidelity, visual quality, and temporal coherence. Our code, dataset, and models are available at https://github.com/chengzhag/PanFlow.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent</title>
<link>https://arxiv.org/abs/2512.00846</link>
<guid>https://arxiv.org/abs/2512.00846</guid>
<content:encoded><![CDATA[
arXiv:2512.00846v1 Announce Type: new 
Abstract: There is a growing demand for mobile user interface (UI) automation, driven by its broad applications across industries. With the advent of visual language models (VLMs), GUI automation has progressed from generating text-based instructions for humans to autonomously executing tasks, thus optimizing automation workflows. Recent approaches leverage VLMs for this problem due to their ability to 1) process on-screen content directly, 2) remain independent of device-specific APIs by utilizing human actions (e.g., clicks, typing), and 3) apply real-world contextual knowledge for task understanding. However, these models often have trouble accurately identifying widgets and determining actions due to limited spatial information in vision encoder features. Additionally, top-performing models are often large, requiring extensive training and resulting in inference delays. In this work, we introduce AFRAgent, an instruct-BLIP-based multimodal architecture that achieves superior performance in GUI automation while being less than one-fourth the size of its nearest competitor. To enhance image embeddings in the large language model (LLM) pipeline, we propose an adaptive feature renormalization-based (a token-level affine transformation) technique that effectively enriches low-resolution image embeddings and fuses high-resolution details. We evaluate AFRAgent on Meta-GUI and AITW benchmarks, establishing a new state-of-the-art baseline for smartphone automation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smol-GS: Compact Representations for Abstract 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.00850</link>
<guid>https://arxiv.org/abs/2512.00850</guid>
<content:encoded><![CDATA[
arXiv:2512.00850v1 Announce Type: new 
Abstract: We present Smol-GS, a novel method for learning compact representations for 3D Gaussian Splatting (3DGS). Our approach learns highly efficient encodings in 3D space that integrate both spatial and semantic information. The model captures the coordinates of the splats through a recursive voxel hierarchy, while splat-wise features store abstracted cues, including color, opacity, transformation, and material properties. This design allows the model to compress 3D scenes by orders of magnitude without loss of flexibility. Smol-GS achieves state-of-the-art compression on standard benchmarks while maintaining high rendering quality. Beyond visual fidelity, the discrete representations could potentially serve as a foundation for downstream tasks such as navigation, planning, and broader 3D scene understanding.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAP-CT: 3D Task-Agnostic Pretraining of Computed Tomography Foundation Models</title>
<link>https://arxiv.org/abs/2512.00872</link>
<guid>https://arxiv.org/abs/2512.00872</guid>
<content:encoded><![CDATA[
arXiv:2512.00872v1 Announce Type: new 
Abstract: Existing foundation models (FMs) in the medical domain often require extensive fine-tuning or rely on training resource-intensive decoders, while many existing encoders are pretrained with objectives biased toward specific tasks. This illustrates a need for a strong, task-agnostic foundation model that requires minimal fine-tuning beyond feature extraction. In this work, we introduce a suite of task-agnostic pretraining of CT foundation models (TAP-CT): a simple yet effective adaptation of Vision Transformers (ViTs) and DINOv2 for volumetric data, enabling scalable self-supervised pretraining directly on 3D CT volumes. Our approach incorporates targeted modifications to patch embeddings, positional encodings, and volumetric augmentations, making the architecture depth-aware while preserving the simplicity of the underlying architectures. We show that large-scale 3D pretraining on an extensive in-house CT dataset (105K volumes) yields stable, robust frozen representations that generalize strongly across downstream tasks. To promote transparency and reproducibility, and to establish a powerful, low-resource baseline for future research in medical imaging, we will release all pretrained models, experimental configurations, and downstream benchmark code at https://huggingface.co/fomofo/tap-ct-b-3d.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Discrete Representation Learning for Sparse-View CBCT Reconstruction: From Algorithm Design to Prospective Multicenter Clinical Evaluation</title>
<link>https://arxiv.org/abs/2512.00873</link>
<guid>https://arxiv.org/abs/2512.00873</guid>
<content:encoded><![CDATA[
arXiv:2512.00873v1 Announce Type: new 
Abstract: Cone beam computed tomography (CBCT)-guided puncture has become an established approach for diagnosing and treating early- to mid-stage thoracic tumours, yet the associated radiation exposure substantially elevates the risk of secondary malignancies. Although multiple low-dose CBCT strategies have been introduced, none have undergone validation using large-scale multicenter retrospective datasets, and prospective clinical evaluation remains lacking. Here, we propose DeepPriorCBCT - a three-stage deep learning framework that achieves diagnostic-grade reconstruction using only one-sixth of the conventional radiation dose. 4102 patients with 8675 CBCT scans from 12 centers were included to develop and validate DeepPriorCBCT. Additionally, a prospective cross-over trial (Registry number: NCT07035977) which recruited 138 patients scheduled for percutaneous thoracic puncture was conducted to assess the model's clinical applicability. Assessment by 11 physicians confirmed that reconstructed images were indistinguishable from original scans. Moreover, diagnostic performance and overall image quality were comparable to those generated by standard reconstruction algorithms. In the prospective trial, five radiologists reported no significant differences in image quality or lesion assessment between DeepPriorCBCT and the clinical standard (all P>0.05). Likewise, 25 interventionalists expressed no preference between model-based and full-sampling images for surgical guidance (Kappa<0.2). Radiation exposure with DeepPriorCBCT was reduced to approximately one-sixth of that with the conventional approach, and collectively, the findings confirm that it enables high-quality CBCT reconstruction under sparse sampling conditions while markedly decreasing intraoperative radiation risk.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feed-Forward 3D Gaussian Splatting Compression with Long-Context Modeling</title>
<link>https://arxiv.org/abs/2512.00877</link>
<guid>https://arxiv.org/abs/2512.00877</guid>
<content:encoded><![CDATA[
arXiv:2512.00877v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a revolutionary 3D representation. However, its substantial data size poses a major barrier to widespread adoption. While feed-forward 3DGS compression offers a practical alternative to costly per-scene per-train compressors, existing methods struggle to model long-range spatial dependencies, due to the limited receptive field of transform coding networks and the inadequate context capacity in entropy models. In this work, we propose a novel feed-forward 3DGS compression framework that effectively models long-range correlations to enable highly compact and generalizable 3D representations. Central to our approach is a large-scale context structure that comprises thousands of Gaussians based on Morton serialization. We then design a fine-grained space-channel auto-regressive entropy model to fully leverage this expansive context. Furthermore, we develop an attention-based transform coding model to extract informative latent priors by aggregating features from a wide range of neighboring Gaussians. Our method yields a $20\times$ compression ratio for 3DGS in a feed-forward inference and achieves state-of-the-art performance among generalizable codecs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Inspired Spectral Geometry for Neural Operator Equivalence and Structured Pruning</title>
<link>https://arxiv.org/abs/2512.00880</link>
<guid>https://arxiv.org/abs/2512.00880</guid>
<content:encoded><![CDATA[
arXiv:2512.00880v1 Announce Type: new 
Abstract: The rapid growth of multimodal intelligence on resource-constrained and heterogeneous domestic hardware exposes critical bottlenecks: multimodal feature heterogeneity, real-time requirements in dynamic scenarios, and hardware-specific operator redundancy. This work introduces a quantum-inspired geometric framework for neural operators that represents each operator by its normalized singular value spectrum on the Bloch hypersphere. We prove a tight spectral-to-functional equivalence theorem showing that vanishing Fubini--Study/Wasserstein-2 distance implies provable functional closeness, establishing the first rigorous foundation for cross-modal and cross-architecture operator substitutability. Based on this metric, we propose Quantum Metric-Driven Functional Redundancy Graphs (QM-FRG) and one-shot structured pruning. Controlled simulation validates the superiority of the proposed metric over magnitude and random baselines. An extensive experimental validation on large-scale multimodal transformers and domestic heterogeneous hardware (Huawei Ascend, Cambricon MLU, Kunlunxin) hardware is deferred to an extended journal version currently in preparation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints</title>
<link>https://arxiv.org/abs/2512.00882</link>
<guid>https://arxiv.org/abs/2512.00882</guid>
<content:encoded><![CDATA[
arXiv:2512.00882v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to "Reasoning-Driven Hallucination" where linguistic priors override visual perception. A key bottleneck is the "Modality Gap": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose "Look, Recite, Then Answer," a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.6% over Qwen-VL and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HanDyVQA: A Video QA Benchmark for Fine-Grained Hand-Object Interaction Dynamics</title>
<link>https://arxiv.org/abs/2512.00885</link>
<guid>https://arxiv.org/abs/2512.00885</guid>
<content:encoded><![CDATA[
arXiv:2512.00885v1 Announce Type: new 
Abstract: Hand-object interaction (HOI) inherently involves dynamics where human manipulations produce distinct spatio-temporal effects on objects. However, existing semantic HOI benchmarks focused either on manipulation or on the resulting effects at a coarse level, lacking fine-grained spatio-temporal reasoning to capture the underlying dynamics in HOI. We introduce HanDyVQA, a fine-grained video question-answering benchmark that comprehensively covers both the manipulation and effect aspects of HOI. HanDyVQA comprises six complementary question types (Action, Process, Objects, Location, State Change, and Object Parts), totalling 11.1K multiple-choice QA pairs. Collected QA pairs recognizing manipulation styles, hand/object motions, and part-level state changes. HanDyVQA also includes 10.3K segmentation masks for Objects and Object Parts questions, enabling the evaluation of object/part-level reasoning in video object segmentation. We evaluated recent video foundation models on our benchmark and found that even the best-performing model, Gemini-2.5-Pro, reached only 73% average accuracy, which is far from human performance (97%). Further analysis shows the remaining challenges in spatial relationship, motion, and part-level geometric understanding. We also found that integrating explicit HOI-related cues into visual features improves performance, offering insights for developing future models with a deeper understanding of HOI dynamics.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual Training-Free Remote Sensing Image Captioning</title>
<link>https://arxiv.org/abs/2512.00887</link>
<guid>https://arxiv.org/abs/2512.00887</guid>
<content:encoded><![CDATA[
arXiv:2512.00887v1 Announce Type: new 
Abstract: Remote sensing image captioning has advanced rapidly through encoder--decoder models, although the reliance on large annotated datasets and the focus on English restricts global applicability. To address these limitations, we propose the first training-free multilingual approach, based on retrieval-augmented prompting. For a given aerial image, we employ a domain-adapted SigLIP2 encoder to retrieve related captions and few-shot examples from a datastore, which are then provided to a language model. We explore two variants: an image-blind setup, where a multilingual Large Language Model (LLM) generates the caption from textual prompts alone, and an image-aware setup, where a Vision--Language Model (VLM) jointly processes the prompt and the input image. To improve the coherence of the retrieved content, we introduce a graph-based re-ranking strategy using PageRank on a graph of images and captions. Experiments on four benchmark datasets across ten languages demonstrate that our approach is competitive with fully supervised English-only systems and generalizes to other languages. Results also highlight the importance of re-ranking with PageRank, yielding up to 35% improvements in performance metrics. Additionally, it was observed that while VLMs tend to generate visually grounded but lexically diverse captions, LLMs can achieve stronger BLEU and CIDEr scores. Lastly, directly generating captions in the target language consistently outperforms other translation-based strategies. Overall, our work delivers one of the first systematic evaluations of multilingual, training-free captioning for remote sensing imagery, advancing toward more inclusive and scalable multimodal Earth observation systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Streaming Video Large Language Models via Hierarchical Token Compression</title>
<link>https://arxiv.org/abs/2512.00891</link>
<guid>https://arxiv.org/abs/2512.00891</guid>
<content:encoded><![CDATA[
arXiv:2512.00891v1 Announce Type: new 
Abstract: Streaming Video Large Language Models (VideoLLMs) have demonstrated impressive performance across various video understanding tasks, but they face significant challenges in real-time deployment due to the high computational cost of processing dense visual tokens from continuous video streams. In streaming video scenarios, the primary bottleneck lies in the Vision Transformer (ViT) encoding stage, where redundant processing of temporally similar frames leads to inefficiency. Additionally, inflated token sequences during LLM pre-filling further exacerbate latency and memory overhead. To address these challenges, we propose \textbf{S}treaming \textbf{T}oken \textbf{C}ompression (\textbf{STC}), a plug-and-play hierarchical framework that seamlessly integrates into existing streaming VideoLLMs, optimizing both ViT encoding and LLM pre-filling stages to accelerate processing. STC introduces two token-level accelerators: \textbf{STC-Cacher}, which reduces ViT encoding overhead by caching and reusing features from temporally similar frames, and \textbf{STC-Pruner}, which compresses the visual token sequence before it enters the LLM, preserving only the most salient tokens based on both spatial and temporal relevance. Extensive experiments on four baseline streaming VideoLLMs across five benchmarks demonstrate that STC outperforms other compression methods. Notably, STC retains up to \textbf{99\%} of accuracy on the ReKV framework while reducing ViT encoding latency and LLM pre-filling latency by \textbf{24.5\%} and \textbf{45.3\%}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead</title>
<link>https://arxiv.org/abs/2512.00903</link>
<guid>https://arxiv.org/abs/2512.00903</guid>
<content:encoded><![CDATA[
arXiv:2512.00903v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Semantic Alignment for Image Clustering</title>
<link>https://arxiv.org/abs/2512.00904</link>
<guid>https://arxiv.org/abs/2512.00904</guid>
<content:encoded><![CDATA[
arXiv:2512.00904v1 Announce Type: new 
Abstract: Image clustering is a classic problem in computer vision, which categorizes images into different groups. Recent studies utilize nouns as external semantic knowledge to improve clus- tering performance. However, these methods often overlook the inherent ambiguity of nouns, which can distort semantic representations and degrade clustering quality. To address this issue, we propose a hierarChical semAntic alignmEnt method for image clustering, dubbed CAE, which improves cluster- ing performance in a training-free manner. In our approach, we incorporate two complementary types of textual seman- tics: caption-level descriptions, which convey fine-grained attributes of image content, and noun-level concepts, which represent high-level object categories. We first select relevant nouns from WordNet and descriptions from caption datasets to construct a semantic space aligned with image features. Then, we align image features with selected nouns and captions via optimal transport to obtain a more discriminative semantic space. Finally, we combine the enhanced semantic and image features to perform clustering. Extensive experiments across 8 datasets demonstrate the effectiveness of our method, notably surpassing the state-of-the-art training-free approach with a 4.2% improvement in accuracy and a 2.9% improvement in adjusted rand index (ARI) on the ImageNet-1K dataset.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TalkingPose: Efficient Face and Gesture Animation with Feedback-guided Diffusion Model</title>
<link>https://arxiv.org/abs/2512.00909</link>
<guid>https://arxiv.org/abs/2512.00909</guid>
<content:encoded><![CDATA[
arXiv:2512.00909v1 Announce Type: new 
Abstract: Recent advancements in diffusion models have significantly improved the realism and generalizability of character-driven animation, enabling the synthesis of high-quality motion from just a single RGB image and a set of driving poses. Nevertheless, generating temporally coherent long-form content remains challenging. Existing approaches are constrained by computational and memory limitations, as they are typically trained on short video segments, thus performing effectively only over limited frame lengths and hindering their potential for extended coherent generation. To address these constraints, we propose TalkingPose, a novel diffusion-based framework specifically designed for producing long-form, temporally consistent human upper-body animations. TalkingPose leverages driving frames to precisely capture expressive facial and hand movements, transferring these seamlessly to a target actor through a stable diffusion backbone. To ensure continuous motion and enhance temporal coherence, we introduce a feedback-driven mechanism built upon image-based diffusion models. Notably, this mechanism does not incur additional computational costs or require secondary training stages, enabling the generation of animations with unlimited duration. Additionally, we introduce a comprehensive, large-scale dataset to serve as a new benchmark for human upper-body animation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision</title>
<link>https://arxiv.org/abs/2512.00911</link>
<guid>https://arxiv.org/abs/2512.00911</guid>
<content:encoded><![CDATA[
arXiv:2512.00911v1 Announce Type: new 
Abstract: Panoramic cameras, capable of capturing a 360-degree field of view, are crucial in robotic vision, particularly in environments with sparse features. However, non-upright panoramas due to unstable robot postures hinder downstream tasks. Traditional IMU-based correction methods suffer from drift and external disturbances, while vision-based approaches offer a promising alternative. This study presents a dual-stream angle-aware generation network that jointly estimates camera inclination angles and reconstructs upright panoramic images. The network comprises a CNN branch that extracts local geometric structures from equirectangular projections and a ViT branch that captures global contextual cues from cubemap projections. These are integrated through a dual-projection adaptive fusion module that aligns spatial features across both domains. To further enhance performance, we introduce a high-frequency enhancement block, circular padding, and channel attention mechanisms to preserve 360{\deg} continuity and improve geometric sensitivity. Experiments on the SUN360 and M3D datasets demonstrate that our method outperforms existing approaches in both inclination estimation and upright panorama generation. Ablation studies further validate the contribution of each module and highlight the synergy between the two tasks. The code and related datasets can be found at: https://github.com/YuhaoShine/DualProjectionFusion.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForamDeepSlice: A High-Accuracy Deep Learning Framework for Foraminifera Species Classification from 2D Micro-CT Slices</title>
<link>https://arxiv.org/abs/2512.00912</link>
<guid>https://arxiv.org/abs/2512.00912</guid>
<content:encoded><![CDATA[
arXiv:2512.00912v1 Announce Type: new 
Abstract: This study presents a comprehensive deep learning pipeline for the automated classification of 12 foraminifera species using 2D micro-CT slices derived from 3D scans. We curated a scientifically rigorous dataset comprising 97 micro-CT scanned specimens across 27 species, selecting 12 species with sufficient representation for robust machine learning. To ensure methodological integrity and prevent data leakage, we employed specimen-level data splitting, resulting in 109,617 high-quality 2D slices (44,103 for training, 14,046 for validation, and 51,468 for testing). We evaluated seven state-of-the-art 2D convolutional neural network (CNN) architectures using transfer learning. Our final ensemble model, combining ConvNeXt-Large and EfficientNetV2-Small, achieved a test accuracy of 95.64%, with a top-3 accuracy of 99.6% and an area under the ROC curve (AUC) of 0.998 across all species. To facilitate practical deployment, we developed an interactive advanced dashboard that supports real-time slice classification and 3D slice matching using advanced similarity metrics, including SSIM, NCC, and the Dice coefficient. This work establishes new benchmarks for AI-assisted micropaleontological identification and provides a fully reproducible framework for foraminifera classification research, bridging the gap between deep learning and applied geosciences.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAHNet: Local Attentive Hashing Network for Point Cloud Registration</title>
<link>https://arxiv.org/abs/2512.00927</link>
<guid>https://arxiv.org/abs/2512.00927</guid>
<content:encoded><![CDATA[
arXiv:2512.00927v1 Announce Type: new 
Abstract: Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneProp: Combining Neural Network and Markov Random Field for Scene-Graph Grounding</title>
<link>https://arxiv.org/abs/2512.00936</link>
<guid>https://arxiv.org/abs/2512.00936</guid>
<content:encoded><![CDATA[
arXiv:2512.00936v1 Announce Type: new 
Abstract: Grounding complex, compositional visual queries with multiple objects and relationships is a fundamental challenge for vision-language models. While standard phrase grounding methods excel at localizing single objects, they lack the structural inductive bias to parse intricate relational descriptions, often failing as queries become more descriptive. To address this structural deficit, we focus on scene-graph grounding, a powerful but less-explored formulation where the query is an explicit graph of objects and their relationships. However, existing methods for this task also struggle, paradoxically showing decreased performance as the query graph grows -- failing to leverage the very information that should make grounding easier. We introduce SceneProp, a novel method that resolves this issue by reformulating scene-graph grounding as a Maximum a Posteriori (MAP) inference problem in a Markov Random Field (MRF). By performing global inference over the entire query graph, SceneProp finds the optimal assignment of image regions to nodes that jointly satisfies all constraints. This is achieved within an end-to-end framework via a differentiable implementation of the Belief Propagation algorithm. Experiments on four benchmarks show that our dedicated focus on the scene-graph grounding formulation allows SceneProp to significantly outperform prior work. Critically, its accuracy consistently improves with the size and complexity of the query graph, demonstrating for the first time that more relational context can, and should, lead to better grounding. Codes are available at https://github.com/keitaotani/SceneProp.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation</title>
<link>https://arxiv.org/abs/2512.00944</link>
<guid>https://arxiv.org/abs/2512.00944</guid>
<content:encoded><![CDATA[
arXiv:2512.00944v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Evidential Learning for Temporal-Semantic Robustness in Moment Retrieval</title>
<link>https://arxiv.org/abs/2512.00953</link>
<guid>https://arxiv.org/abs/2512.00953</guid>
<content:encoded><![CDATA[
arXiv:2512.00953v1 Announce Type: new 
Abstract: In the domain of moment retrieval, accurately identifying temporal segments within videos based on natural language queries remains challenging. Traditional methods often employ pre-trained models that struggle with fine-grained information and deterministic reasoning, leading to difficulties in aligning with complex or ambiguous moments. To overcome these limitations, we explore Deep Evidential Regression (DER) to construct a vanilla Evidential baseline. However, this approach encounters two major issues: the inability to effectively handle modality imbalance and the structural differences in DER's heuristic uncertainty regularizer, which adversely affect uncertainty estimation. This misalignment results in high uncertainty being incorrectly associated with accurate samples rather than challenging ones. Our observations indicate that existing methods lack the adaptability required for complex video scenarios. In response, we propose Debiased Evidential Learning for Moment Retrieval (DEMR), a novel framework that incorporates a Reflective Flipped Fusion (RFF) block for cross-modal alignment and a query reconstruction task to enhance text sensitivity, thereby reducing bias in uncertainty estimation. Additionally, we introduce a Geom-regularizer to refine uncertainty predictions, enabling adaptive alignment with difficult moments and improving retrieval accuracy. Extensive testing on standard datasets and debiased datasets ActivityNet-CD and Charades-CD demonstrates significant enhancements in effectiveness, robustness, and interpretability, positioning our approach as a promising solution for temporal-semantic robustness in moment retrieval. The code is publicly available at https://github.com/KaijingOfficial/DEMR.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction</title>
<link>https://arxiv.org/abs/2512.00960</link>
<guid>https://arxiv.org/abs/2512.00960</guid>
<content:encoded><![CDATA[
arXiv:2512.00960v1 Announce Type: new 
Abstract: Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-ACT: Learn from Multimodal Parallel Generation to Act</title>
<link>https://arxiv.org/abs/2512.00975</link>
<guid>https://arxiv.org/abs/2512.00975</guid>
<content:encoded><![CDATA[
arXiv:2512.00975v1 Announce Type: new 
Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhotoFramer: Multi-modal Image Composition Instruction</title>
<link>https://arxiv.org/abs/2512.00993</link>
<guid>https://arxiv.org/abs/2512.00993</guid>
<content:encoded><![CDATA[
arXiv:2512.00993v1 Announce Type: new 
Abstract: Composition matters during the photo-taking process, yet many casual users struggle to frame well-composed images. To provide composition guidance, we introduce PhotoFramer, a multi-modal composition instruction framework. Given a poorly composed image, PhotoFramer first describes how to improve the composition in natural language and then generates a well-composed example image. To train such a model, we curate a large-scale dataset. Inspired by how humans take photos, we organize composition guidance into a hierarchy of sub-tasks: shift, zoom-in, and view-change tasks. Shift and zoom-in data are sampled from existing cropping datasets, while view-change data are obtained via a two-stage pipeline. First, we sample pairs with varying viewpoints from multi-view datasets, and train a degradation model to transform well-composed photos into poorly composed ones. Second, we apply this degradation model to expert-taken photos to synthesize poor images to form training pairs. Using this dataset, we finetune a model that jointly processes and generates both text and images, enabling actionable textual guidance with illustrative examples. Extensive experiments demonstrate that textual instructions effectively steer image composition, and coupling them with exemplars yields consistent improvements over exemplar-only baselines. PhotoFramer offers a practical step toward composition assistants that make expert photographic priors accessible to everyday users. Codes, model weights, and datasets have been released in https://zhiyuanyou.github.io/photoframer.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud</title>
<link>https://arxiv.org/abs/2512.00995</link>
<guid>https://arxiv.org/abs/2512.00995</guid>
<content:encoded><![CDATA[
arXiv:2512.00995v1 Announce Type: new 
Abstract: Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints</title>
<link>https://arxiv.org/abs/2512.00999</link>
<guid>https://arxiv.org/abs/2512.00999</guid>
<content:encoded><![CDATA[
arXiv:2512.00999v1 Announce Type: new 
Abstract: Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency</title>
<link>https://arxiv.org/abs/2512.01008</link>
<guid>https://arxiv.org/abs/2512.01008</guid>
<content:encoded><![CDATA[
arXiv:2512.01008v1 Announce Type: new 
Abstract: Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model</title>
<link>https://arxiv.org/abs/2512.01030</link>
<guid>https://arxiv.org/abs/2512.01030</guid>
<content:encoded><![CDATA[
arXiv:2512.01030v1 Announce Type: new 
Abstract: Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.01048</link>
<guid>https://arxiv.org/abs/2512.01048</guid>
<content:encoded><![CDATA[
arXiv:2512.01048v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have made great strides in addressing temporal understanding tasks, which involve characterizing visual changes across a sequence of images. However, recent works have suggested that when making predictions, VLMs may rely on static feature biases, such as background or object features, rather than dynamic visual changes. Static feature biases are a type of shortcut and can contribute to systematic prediction errors on downstream tasks; as a result, identifying and characterizing error-inducing static feature biases is critical prior to real-world model deployment. In this work, we introduce TRoVe, an automated approach for discovering error-inducing static feature biases learned by temporal VLMs. Given a trained VLM and an annotated validation dataset associated with a downstream classification task, TRoVe extracts candidate static features from the dataset and scores each feature by (i) the effect of the feature on classification errors as well as (ii) the extent to which the VLM relies on the feature when making predictions. In order to quantitatively evaluate TRoVe, we introduce an evaluation framework consisting of 101 trained temporal VLMs paired with ground-truth annotations for learned static feature biases. We use this framework to demonstrate that TRoVe can accurately identify error-inducing static feature biases in VLMs, achieving a 28.6% improvement over the closest baseline. Finally, we apply TRoVe to 7 off-the-shelf VLMs and 2 temporal understanding tasks, surfacing previously-unknown static feature biases and demonstrating that knowledge of learned biases can aid in improving model performance at test time. Our code is available at https://github.com/Stanford-AIMI/TRoVe.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction</title>
<link>https://arxiv.org/abs/2512.01059</link>
<guid>https://arxiv.org/abs/2512.01059</guid>
<content:encoded><![CDATA[
arXiv:2512.01059v1 Announce Type: new 
Abstract: Although scaling laws and many empirical results suggest that increasing the size of Vision Transformers often improves performance, model accuracy and training behavior are not always monotonically increasing with scale. Focusing on ViT-B/16 trained on ImageNet-1K, we study two simple parameter-reduction strategies applied to the MLP blocks, each removing 32.7\% of the baseline parameters. Our \emph{GroupedMLP} variant shares MLP weights between adjacent transformer blocks and achieves 81.47\% top-1 accuracy while maintaining the baseline computational cost. Our \emph{ShallowMLP} variant halves the MLP hidden dimension and reaches 81.25\% top-1 accuracy with a 38\% increase in inference throughput. Both models outperform the 86.6M-parameter baseline (81.05\%) and exhibit substantially improved training stability, reducing peak-to-final accuracy degradation from 0.47\% to the range 0.03\% to 0.06\%. These results suggest that, for ViT-B/16 on ImageNet-1K with a standard training recipe, the model operates in an overparameterized regime in which MLP capacity can be reduced without harming performance and can even slightly improve it. More broadly, our findings suggest that architectural constraints such as parameter sharing and reduced width may act as useful inductive biases, and highlight the importance of how parameters are allocated when designing Vision Transformers. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Medical Phrase Grounding</title>
<link>https://arxiv.org/abs/2512.01085</link>
<guid>https://arxiv.org/abs/2512.01085</guid>
<content:encoded><![CDATA[
arXiv:2512.01085v1 Announce Type: new 
Abstract: Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Inference of Masked Image Generators via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.01094</link>
<guid>https://arxiv.org/abs/2512.01094</guid>
<content:encoded><![CDATA[
arXiv:2512.01094v1 Announce Type: new 
Abstract: Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions</title>
<link>https://arxiv.org/abs/2512.01095</link>
<guid>https://arxiv.org/abs/2512.01095</guid>
<content:encoded><![CDATA[
arXiv:2512.01095v1 Announce Type: new 
Abstract: We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions. CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes. CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition. We conduct extensive experiments with current state-of-the-art VLMs, both open-source and proprietary, and reveal their limitations in generalizing to cyclical dynamics such as linear and orbital motion, as well as time-dependent changes in visual attributes like color and scale. Our results demonstrate that present-day VLMs struggle to reliably detect and exploit cyclic patterns, lack a notion of temporal understanding, and are unable to extract quantitative insights from scenes, such as the number of objects in motion, highlighting a significant technical gap that needs to be addressed. More specifically, we find no single model consistently leads in performance: neither size nor architecture correlates strongly with outcomes, and no model succeeds equally well across all tasks. By providing a targeted challenge and a comprehensive evaluation framework, CycliST paves the way for visual reasoning models that surpass the state-of-the-art in understanding periodic patterns.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Eigenstructures of Unstructured Data Manifolds</title>
<link>https://arxiv.org/abs/2512.01103</link>
<guid>https://arxiv.org/abs/2512.01103</guid>
<content:encoded><![CDATA[
arXiv:2512.01103v1 Announce Type: new 
Abstract: We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Prognostic Event Modeling for Multimodal Cancer Survival Analysis</title>
<link>https://arxiv.org/abs/2512.01116</link>
<guid>https://arxiv.org/abs/2512.01116</guid>
<content:encoded><![CDATA[
arXiv:2512.01116v1 Announce Type: new 
Abstract: The integration of histology images and gene profiles has shown great promise for improving survival prediction in cancer. However, current approaches often struggle to model intra- and inter-modal interactions efficiently and effectively due to the high dimensionality and complexity of the inputs. A major challenge is capturing critical prognostic events that, though few, underlie the complexity of the observed inputs and largely determine patient outcomes. These events, manifested as high-level structural signals such as spatial histologic patterns or pathway co-activations, are typically sparse, patient-specific, and unannotated, making them inherently difficult to uncover. To address this, we propose SlotSPE, a slot-based framework for structural prognostic event modeling. Specifically, inspired by the principle of factorial coding, we compress each patient's multimodal inputs into compact, modality-specific sets of mutually distinctive slots using slot attention. By leveraging these slot representations as encodings for prognostic events, our framework enables both efficient and effective modeling of complex intra- and inter-modal interactions, while also facilitating seamless incorporation of biological priors that enhance prognostic relevance. Extensive experiments on ten cancer benchmarks show that SlotSPE outperforms existing methods in 8 out of 10 cohorts, achieving an overall improvement of 2.9%. It remains robust under missing genomic data and delivers markedly improved interpretability through structured event decomposition.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniFD: A Unified Model for Versatile Face Forgery Detection</title>
<link>https://arxiv.org/abs/2512.01128</link>
<guid>https://arxiv.org/abs/2512.01128</guid>
<content:encoded><![CDATA[
arXiv:2512.01128v1 Announce Type: new 
Abstract: Face forgery detection encompasses multiple critical tasks, including identifying forged images and videos and localizing manipulated regions and temporal segments. Current approaches typically employ task-specific models with independent architectures, leading to computational redundancy and ignoring potential correlations across related tasks. We introduce OmniFD, a unified framework that jointly addresses four core face forgery detection tasks within a single model, i.e., image and video classification, spatial localization, and temporal localization. Our architecture consists of three principal components: (1) a shared Swin Transformer encoder that extracts unified 4D spatiotemporal representations from both images and video inputs, (2) a cross-task interaction module with learnable queries that dynamically captures inter-task dependencies through attention-based reasoning, and (3) lightweight decoding heads that transform refined representations into corresponding predictions for all FFD tasks. Extensive experiments demonstrate OmniFD's advantage over task-specific models. Its unified design leverages multi-task learning to capture generalized representations across tasks, especially enabling fine-grained knowledge transfer that facilitates other tasks. For example, video classification accuracy improves by 4.63% when image data are incorporated. Furthermore, by unifying images, videos and the four tasks within one framework, OmniFD achieves superior performance across diverse benchmarks with high efficiency and scalability, e.g., reducing 63% model parameters and 50% training time. It establishes a practical and generalizable solution for comprehensive face forgery detection in real-world applications. The source code is made available at https://github.com/haotianll/OmniFD.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Continuous Micro-Expression Intensity Estimation Using Temporal Deep Neural Network</title>
<link>https://arxiv.org/abs/2512.01145</link>
<guid>https://arxiv.org/abs/2512.01145</guid>
<content:encoded><![CDATA[
arXiv:2512.01145v1 Announce Type: new 
Abstract: Micro-facial expressions are brief and involuntary facial movements that reflect genuine emotional states. While most prior work focuses on classifying discrete micro-expression categories, far fewer studies address the continuous evolution of intensity over time. Progress in this direction is limited by the lack of frame-level intensity labels, which makes fully supervised regression impractical.
  We propose a unified framework for continuous micro-expression intensity estimation using only weak temporal labels (onset, apex, offset). A simple triangular prior converts sparse temporal landmarks into dense pseudo-intensity trajectories, and a lightweight temporal regression model that combines a ResNet18 encoder with a bidirectional GRU predicts frame-wise intensity directly from image sequences. The method requires no frame-level annotation effort and is applied consistently across datasets through a single preprocessing and temporal alignment pipeline.
  Experiments on SAMM and CASME II show strong temporal agreement with the pseudo-intensity trajectories. On SAMM, the model reaches a Spearman correlation of 0.9014 and a Kendall correlation of 0.7999, outperforming a frame-wise baseline. On CASME II, it achieves up to 0.9116 and 0.8168, respectively, when trained without the apex-ranking term. Ablation studies confirm that temporal modeling and structured pseudo labels are central to capturing the rise-apex-fall dynamics of micro-facial movements.
  To our knowledge, this is the first unified approach for continuous micro-expression intensity estimation using only sparse temporal annotations.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SocialFusion: Addressing Social Degradation in Pre-trained Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.01148</link>
<guid>https://arxiv.org/abs/2512.01148</guid>
<content:encoded><![CDATA[
arXiv:2512.01148v1 Announce Type: new 
Abstract: Understanding social interactions from visual cues is a fundamental challenge for a socially competent AI. While powerful pre-trained vision-language models (VLMs) have shown remarkable general capabilities, they surprisingly struggle to unify and learn multiple social perception tasks simultaneously, often exhibiting negative transfer. We identify that this negative transfer stems from a critical issue we term "social degradation," whereby the general visual-linguistic pre-training process of VLMs impairs the visual encoder's ability to represent nuanced social information. We investigate this behavior further under two lenses: decodability through linear representation probing and compatibility through gradient conflict analysis, revealing that both play a role in the degradation, especially the former, which is significantly compromised in the VLM pre-training process. To address these issues, we propose SocialFusion, a unified framework that learns a minimal connection between a frozen visual encoder and a language model. Compared with existing VLMs, it exhibits positive transfer across all five social tasks, leveraging synergies between them to enhance overall performance and achieves comparable performance to task-specific state-of-the-art models on various benchmarks. Our findings suggest that current VLM pre-training strategies may be detrimental to acquiring general social competence and highlight the need for more socially-aware training paradigms.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPAC: Distribution-Preserving Adversarial Control for Diffusion Sampling</title>
<link>https://arxiv.org/abs/2512.01153</link>
<guid>https://arxiv.org/abs/2512.01153</guid>
<content:encoded><![CDATA[
arXiv:2512.01153v1 Announce Type: new 
Abstract: Adversarially guided diffusion sampling often achieves the target class, but sample quality degrades as deviations between the adversarially controlled and nominal trajectories accumulate. We formalize this degradation as a path-space Kullback-Leibler divergence(path-KL) between controlled and nominal (uncontrolled) diffusion processes, thereby showing via Girsanov's theorem that it exactly equals the control energy. Building on this stochastic optimal control (SOC) view, we theoretically establish that minimizing this path-KL simultaneously tightens upper bounds on both the 2-Wasserstein distance and Fr\'echet Inception Distance (FID), revealing a principled connection between adversarial control energy and perceptual fidelity. From a variational perspective, we derive a first-order optimality condition for the control: among all directions that yield the same classification gain, the component tangent to iso-(log-)density surfaces (i.e., orthogonal to the score) minimizes path-KL, whereas the normal component directly increases distributional drift. This leads to DPAC (Distribution-Preserving Adversarial Control), a diffusion guidance rule that projects adversarial gradients onto the tangent space defined by the generative score geometry. We further show that in discrete solvers, the tangent projection cancels the O({\Delta}t) leading error term in the Wasserstein distance, achieving an O({\Delta}t^2) quality gap; moreover, it remains second-order robust to score or metric approximation. Empirical studies on ImageNet-100 validate the theoretical predictions, confirming that DPAC achieves lower FID and estimated path-KL at matched attack success rates.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time On-the-Go Annotation Framework Using YOLO for Automated Dataset Generation</title>
<link>https://arxiv.org/abs/2512.01165</link>
<guid>https://arxiv.org/abs/2512.01165</guid>
<content:encoded><![CDATA[
arXiv:2512.01165v1 Announce Type: new 
Abstract: Efficient and accurate annotation of datasets remains a significant challenge for deploying object detection models such as You Only Look Once (YOLO) in real-world applications, particularly in agriculture where rapid decision-making is critical. Traditional annotation techniques are labor-intensive, requiring extensive manual labeling post data collection. This paper presents a novel real-time annotation approach leveraging YOLO models deployed on edge devices, enabling immediate labeling during image capture. To comprehensively evaluate the efficiency and accuracy of our proposed system, we conducted an extensive comparative analysis using three prominent YOLO architectures (YOLOv5, YOLOv8, YOLOv12) under various configurations: single-class versus multi-class annotation and pretrained versus scratch-based training. Our analysis includes detailed statistical tests and learning dynamics, demonstrating significant advantages of pretrained and single-class configurations in terms of model convergence, performance, and robustness. Results strongly validate the feasibility and effectiveness of our real-time annotation framework, highlighting its capability to drastically reduce dataset preparation time while maintaining high annotation quality.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VSRD++: Autolabeling for 3D Object Detection via Instance-Aware Volumetric Silhouette Rendering</title>
<link>https://arxiv.org/abs/2512.01178</link>
<guid>https://arxiv.org/abs/2512.01178</guid>
<content:encoded><![CDATA[
arXiv:2512.01178v1 Announce Type: new 
Abstract: Monocular 3D object detection is a fundamental yet challenging task in 3D scene understanding. Existing approaches heavily depend on supervised learning with extensive 3D annotations, which are often acquired from LiDAR point clouds through labor-intensive labeling processes. To tackle this problem, we propose VSRD++, a novel weakly supervised framework for monocular 3D object detection that eliminates the reliance on 3D annotations and leverages neural-field-based volumetric rendering with weak 2D supervision. VSRD++ consists of a two-stage pipeline: multi-view 3D autolabeling and subsequent monocular 3D detector training. In the multi-view autolabeling stage, object surfaces are represented as signed distance fields (SDFs) and rendered as instance masks via the proposed instance-aware volumetric silhouette rendering. To optimize 3D bounding boxes, we decompose each instance's SDF into a cuboid SDF and a residual distance field (RDF) that captures deviations from the cuboid. To address the geometry inconsistency commonly observed in volume rendering methods applied to dynamic objects, we model the dynamic objects by including velocity into bounding box attributes as well as assigning confidence to each pseudo-label. Moreover, we also employ a 3D attribute initialization module to initialize the dynamic bounding box parameters. In the monocular 3D object detection phase, the optimized 3D bounding boxes serve as pseudo labels for training monocular 3D object detectors. Extensive experiments on the KITTI-360 dataset demonstrate that VSRD++ significantly outperforms existing weakly supervised approaches for monocular 3D object detection on both static and dynamic scenes. Code is available at https://github.com/Magicboomliu/VSRD_plus_plus
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image</title>
<link>https://arxiv.org/abs/2512.01204</link>
<guid>https://arxiv.org/abs/2512.01204</guid>
<content:encoded><![CDATA[
arXiv:2512.01204v1 Announce Type: new 
Abstract: Generating high-fidelity, physically interactive 3D simulated tabletop scenes is essential for embodied AI--especially for robotic manipulation policy learning and data synthesis. However, current text- or image-driven 3D scene generation methods mainly focus on large-scale scenes, struggling to capture the high-density layouts and complex spatial relations that characterize tabletop scenes. To address these challenges, we propose TabletopGen, a training-free, fully automatic framework that generates diverse, instance-level interactive 3D tabletop scenes. TabletopGen accepts a reference image as input, which can be synthesized by a text-to-image model to enhance scene diversity. We then perform instance segmentation and completion on the reference to obtain per-instance images. Each instance is reconstructed into a 3D model followed by canonical coordinate alignment. The aligned 3D models then undergo pose and scale estimation before being assembled into a collision-free, simulation-ready tabletop scene. A key component of our framework is a novel pose and scale alignment approach that decouples the complex spatial reasoning into two stages: a Differentiable Rotation Optimizer for precise rotation recovery and a Top-view Spatial Alignment mechanism for robust translation and scale estimation, enabling accurate 3D reconstruction from 2D reference. Extensive experiments and user studies show that TabletopGen achieves state-of-the-art performance, markedly surpassing existing methods in visual fidelity, layout accuracy, and physical plausibility, capable of generating realistic tabletop scenes with rich stylistic and spatial diversity. Our code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Approximation Gap of Partial AUC Optimization: A Tale of Two Formulations</title>
<link>https://arxiv.org/abs/2512.01213</link>
<guid>https://arxiv.org/abs/2512.01213</guid>
<content:encoded><![CDATA[
arXiv:2512.01213v1 Announce Type: new 
Abstract: As a variant of the Area Under the ROC Curve (AUC), the partial AUC (PAUC) focuses on a specific range of false positive rate (FPR) and/or true positive rate (TPR) in the ROC curve. It is a pivotal evaluation metric in real-world scenarios with both class imbalance and decision constraints. However, selecting instances within these constrained intervals during its calculation is NP-hard, and thus typically requires approximation techniques for practical resolution. Despite the progress made in PAUC optimization over the last few years, most existing methods still suffer from uncontrollable approximation errors or a limited scalability when optimizing the approximate PAUC objectives. In this paper, we close the approximation gap of PAUC optimization by presenting two simple instance-wise minimax reformulations: one with an asymptotically vanishing gap, the other with the unbiasedness at the cost of more variables. Our key idea is to first establish an equivalent instance-wise problem to lower the time complexity, simplify the complicated sample selection procedure by threshold learning, and then apply different smoothing techniques. Equipped with an efficient solver, the resulting algorithms enjoy a linear per-iteration computational complexity w.r.t. the sample size and a convergence rate of $O(\epsilon^{-1/3})$ for typical one-way and two-way PAUCs. Moreover, we provide a tight generalization bound of our minimax reformulations. The result explicitly demonstrates the impact of the TPR/FPR constraints $\alpha$/$\beta$ on the generalization and exhibits a sharp order of $\tilde{O}(\alpha^{-1}\n_+^{-1} + \beta^{-1}\n_-^{-1})$. Finally, extensive experiments on several benchmark datasets validate the strength of our proposed methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M4-BLIP: Advancing Multi-Modal Media Manipulation Detection through Face-Enhanced Local Analysis</title>
<link>https://arxiv.org/abs/2512.01214</link>
<guid>https://arxiv.org/abs/2512.01214</guid>
<content:encoded><![CDATA[
arXiv:2512.01214v1 Announce Type: new 
Abstract: In the contemporary digital landscape, multi-modal media manipulation has emerged as a significant societal threat, impacting the reliability and integrity of information dissemination. Current detection methodologies in this domain often overlook the crucial aspect of localized information, despite the fact that manipulations frequently occur in specific areas, particularly in facial regions. In response to this critical observation, we propose the M4-BLIP framework. This innovative framework utilizes the BLIP-2 model, renowned for its ability to extract local features, as the cornerstone for feature extraction. Complementing this, we incorporate local facial information as prior knowledge. A specially designed alignment and fusion module within M4-BLIP meticulously integrates these local and global features, creating a harmonious blend that enhances detection accuracy. Furthermore, our approach seamlessly integrates with Large Language Models (LLM), significantly improving the interpretability of the detection outcomes. Extensive quantitative and visualization experiments validate the effectiveness of our framework against the state-of-the-art competitors.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance</title>
<link>https://arxiv.org/abs/2512.01223</link>
<guid>https://arxiv.org/abs/2512.01223</guid>
<content:encoded><![CDATA[
arXiv:2512.01223v1 Announce Type: new 
Abstract: 3D Visual Grounding (3DVG) focuses on locating objects in 3D scenes based on natural language descriptions, serving as a fundamental task for embodied AI and robotics. Recent advances in Multi-modal Large Language Models (MLLMs) have motivated research into extending them to 3DVG. However, MLLMs primarily process 2D visual inputs and struggle with understanding 3D spatial structure of scenes solely from these limited perspectives. Existing methods mainly utilize viewpoint-dependent rendering of reconstructed point clouds to provide explicit structural guidance for MLLMs in 3DVG tasks, leading to inefficiency and limited spatial reasoning. To address this issue, we propose S$^2$-MLLM, an efficient framework that enhances spatial reasoning in MLLMs through implicit spatial reasoning. We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction. By acquiring 3D structural understanding during training, our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction. Moreover, we propose a structure-enhanced module (SE), which first employs intra-view and inter-view attention mechanisms to capture dependencies within views and correspondences across views. The module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information, enabling more accurate structural understanding. Extensive experiments demonstrate that S$^2$-MLLM unifies superior performance, generalization, and efficiency, achieving significant performance over existing methods across the ScanRefer, Nr3D, and Sr3D datasets. Code will be available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSR: Scaling Multi-Subject Personalized Image Generation with Pairwise Subject-Consistency Rewards</title>
<link>https://arxiv.org/abs/2512.01236</link>
<guid>https://arxiv.org/abs/2512.01236</guid>
<content:encoded><![CDATA[
arXiv:2512.01236v1 Announce Type: new 
Abstract: Personalized generation models for a single subject have demonstrated remarkable effectiveness, highlighting their significant potential. However, when extended to multiple subjects, existing models often exhibit degraded performance, particularly in maintaining subject consistency and adhering to textual prompts. We attribute these limitations to the absence of high-quality multi-subject datasets and refined post-training strategies. To address these challenges, we propose a scalable multi-subject data generation pipeline that leverages powerful single-subject generation models to construct diverse and high-quality multi-subject training data. Through this dataset, we first enable single-subject personalization models to acquire knowledge of synthesizing multi-image and multi-subject scenarios. Furthermore, to enhance both subject consistency and text controllability, we design a set of Pairwise Subject-Consistency Rewards and general-purpose rewards, which are incorporated into a refined reinforcement learning stage. To comprehensively evaluate multi-subject personalization, we introduce a new benchmark that assesses model performance using seven subsets across three dimensions. Extensive experiments demonstrate the effectiveness of our approach in advancing multi-subject personalized image generation. Github Link: https://github.com/wang-shulei/PSR
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation</title>
<link>https://arxiv.org/abs/2512.01242</link>
<guid>https://arxiv.org/abs/2512.01242</guid>
<content:encoded><![CDATA[
arXiv:2512.01242v1 Announce Type: new 
Abstract: We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRivia: Self-supervised Fine-tuning of Vision-Language Models for Table Recognition</title>
<link>https://arxiv.org/abs/2512.01248</link>
<guid>https://arxiv.org/abs/2512.01248</guid>
<content:encoded><![CDATA[
arXiv:2512.01248v1 Announce Type: new 
Abstract: Table recognition (TR) aims to transform table images into semi-structured representations such as HTML or Markdown. As a core component of document parsing, TR has long relied on supervised learning, with recent efforts dominated by fine-tuning vision-language models (VLMs) using labeled data. While VLMs have brought TR to the next level, pushing performance further demands large-scale labeled data that is costly to obtain. Consequently, although proprietary models have continuously pushed the performance boundary, open-source models, often trained with limited resources and, in practice, the only viable option for many due to privacy regulations, still lag far behind. To bridge this gap, we introduce TRivia, a self-supervised fine-tuning method that enables pretrained VLMs to learn TR directly from unlabeled table images in the wild. Built upon Group Relative Policy Optimization, TRivia automatically identifies unlabeled samples that most effectively facilitate learning and eliminates the need for human annotations through a question-answering-based reward mechanism. An attention-guided module generates diverse questions for each table image, and the ability to interpret the recognition results and answer them correctly provides feedback to optimize the TR model. This closed-loop process allows the TR model to autonomously learn to recognize, structure, and reason over tables without labeled data. Leveraging this pipeline, we present TRivia-3B, an open-sourced, compact, and state-of-the-art TR model that surpasses existing systems (e.g., Gemini 2.5 Pro, MinerU2.5) on three popular benchmarks. Model and code are released at: https://github.com/opendatalab/TRivia
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViscNet: Vision-Based In-line Viscometry for Fluid Mixing Process</title>
<link>https://arxiv.org/abs/2512.01268</link>
<guid>https://arxiv.org/abs/2512.01268</guid>
<content:encoded><![CDATA[
arXiv:2512.01268v1 Announce Type: new 
Abstract: Viscosity measurement is essential for process monitoring and autonomous laboratory operation, yet conventional viscometers remain invasive and require controlled laboratory environments that differ substantially from real process conditions. We present a computer-vision-based viscometer that infers viscosity by exploiting how a fixed background pattern becomes optically distorted as light refracts through the mixing-driven, continuously deforming free surface. Under diverse lighting conditions, the system achieves a mean absolute error of 0.113 in log m2 s^-1 units for regression and reaches up to 81% accuracy in viscosity-class prediction. Although performance declines for classes with closely clustered viscosity values, a multi-pattern strategy improves robustness by providing enriched visual cues. To ensure sensor reliability, we incorporate uncertainty quantification, enabling viscosity predictions with confidence estimates. This stand-off viscometer offers a practical, automation-ready alternative to existing viscometry methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>nnMobileNet++: Towards Efficient Hybrid Networks for Retinal Image Analysis</title>
<link>https://arxiv.org/abs/2512.01273</link>
<guid>https://arxiv.org/abs/2512.01273</guid>
<content:encoded><![CDATA[
arXiv:2512.01273v1 Announce Type: new 
Abstract: Retinal imaging is a critical, non-invasive modality for the early detection and monitoring of ocular and systemic diseases. Deep learning, particularly convolutional neural networks (CNNs), has significant progress in automated retinal analysis, supporting tasks such as fundus image classification, lesion detection, and vessel segmentation. As a representative lightweight network, nnMobileNet has demonstrated strong performance across multiple retinal benchmarks while remaining computationally efficient. However, purely convolutional architectures inherently struggle to capture long-range dependencies and model the irregular lesions and elongated vascular patterns that characterize on retinal images, despite the critical importance of vascular features for reliable clinical diagnosis. To further advance this line of work and extend the original vision of nnMobileNet, we propose nnMobileNet++, a hybrid architecture that progressively bridges convolutional and transformer representations. The framework integrates three key components: (i) dynamic snake convolution for boundary-aware feature extraction, (ii) stage-specific transformer blocks introduced after the second down-sampling stage for global context modeling, and (iii) retinal image pretraining to improve generalization. Experiments on multiple public retinal datasets for classification, together with ablation studies, demonstrate that nnMobileNet++ achieves state-of-the-art or highly competitive accuracy while maintaining low computational cost, underscoring its potential as a lightweight yet effective framework for retinal image analysis.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised Contrastive Machine Unlearning of Background Bias in Sonar Image Classification with Fine-Grained Explainable AI</title>
<link>https://arxiv.org/abs/2512.01291</link>
<guid>https://arxiv.org/abs/2512.01291</guid>
<content:encoded><![CDATA[
arXiv:2512.01291v1 Announce Type: new 
Abstract: Acoustic sonar image analysis plays a critical role in object detection and classification, with applications in both civilian and defense domains. Despite the availability of real and synthetic datasets, existing AI models that achieve high accuracy often over-rely on seafloor features, leading to poor generalization. To mitigate this issue, we propose a novel framework that integrates two key modules: (i) a Targeted Contrastive Unlearning (TCU) module, which extends the traditional triplet loss to reduce seafloor-induced background bias and improve generalization, and (ii) the Unlearn to Explain Sonar Framework (UESF), which provides visual insights into what the model has deliberately forgotten while adapting the LIME explainer to generate more faithful and localized attributions for unlearning evaluation. Extensive experiments across both real and synthetic sonar datasets validate our approach, demonstrating significant improvements in unlearning effectiveness, model robustness, and interpretability.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Model in Latent Space for Medical Image Segmentation Task</title>
<link>https://arxiv.org/abs/2512.01292</link>
<guid>https://arxiv.org/abs/2512.01292</guid>
<content:encoded><![CDATA[
arXiv:2512.01292v1 Announce Type: new 
Abstract: Medical image segmentation is crucial for clinical diagnosis and treatment planning. Traditional methods typically produce a single segmentation mask, failing to capture inherent uncertainty. Recent generative models enable the creation of multiple plausible masks per image, mimicking the collaborative interpretation of several clinicians. However, these approaches remain computationally heavy. We propose MedSegLatDiff, a diffusion based framework that combines a variational autoencoder (VAE) with a latent diffusion model for efficient medical image segmentation. The VAE compresses the input into a low dimensional latent space, reducing noise and accelerating training, while the diffusion process operates directly in this compact representation. We further replace the conventional MSE loss with weighted cross entropy in the VAE mask reconstruction path to better preserve tiny structures such as small nodules. MedSegLatDiff is evaluated on ISIC-2018 (skin lesions), CVC-Clinic (polyps), and LIDC-IDRI (lung nodules). It achieves state of the art or highly competitive Dice and IoU scores while simultaneously generating diverse segmentation hypotheses and confidence maps. This provides enhanced interpretability and reliability compared to deterministic baselines, making the model particularly suitable for clinical deployment.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EGG-Fusion: Efficient 3D Reconstruction with Geometry-aware Gaussian Surfel on the Fly</title>
<link>https://arxiv.org/abs/2512.01296</link>
<guid>https://arxiv.org/abs/2512.01296</guid>
<content:encoded><![CDATA[
arXiv:2512.01296v1 Announce Type: new 
Abstract: Real-time 3D reconstruction is a fundamental task in computer graphics. Recently, differentiable-rendering-based SLAM system has demonstrated significant potential, enabling photorealistic scene rendering through learnable scene representations such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Current differentiable rendering methods face dual challenges in real-time computation and sensor noise sensitivity, leading to degraded geometric fidelity in scene reconstruction and limited practicality. To address these challenges, we propose a novel real-time system EGG-Fusion, featuring robust sparse-to-dense camera tracking and a geometry-aware Gaussian surfel mapping module, introducing an information filter-based fusion method that explicitly accounts for sensor noise to achieve high-precision surface reconstruction. The proposed differentiable Gaussian surfel mapping effectively models multi-view consistent surfaces while enabling efficient parameter optimization. Extensive experimental results demonstrate that the proposed system achieves a surface reconstruction error of 0.6\textit{cm} on standardized benchmark datasets including Replica and ScanNet++, representing over 20\% improvement in accuracy compared to state-of-the-art (SOTA) GS-based methods. Notably, the system maintains real-time processing capabilities at 24 FPS, establishing it as one of the most accurate differentiable-rendering-based real-time reconstruction systems. Project Page: https://zju3dv.github.io/eggfusion/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TBT-Former: Learning Temporal Boundary Distributions for Action Localization</title>
<link>https://arxiv.org/abs/2512.01298</link>
<guid>https://arxiv.org/abs/2512.01298</guid>
<content:encoded><![CDATA[
arXiv:2512.01298v1 Announce Type: new 
Abstract: Temporal Action Localization (TAL) remains a fundamental challenge in video understanding, aiming to identify the start time, end time, and category of all action instances within untrimmed videos. While recent single-stage, anchor-free models like ActionFormer have set a high standard by leveraging Transformers for temporal reasoning, they often struggle with two persistent issues: the precise localization of actions with ambiguous or "fuzzy" temporal boundaries and the effective fusion of multi-scale contextual information. In this paper, we introduce the Temporal Boundary Transformer (TBT-Former), a new architecture that directly addresses these limitations. TBT-Former enhances the strong ActionFormer baseline with three core contributions: (1) a higher-capacity scaled Transformer backbone with an increased number of attention heads and an expanded Multi-Layer Perceptron (MLP) dimension for more powerful temporal feature extraction; (2) a cross-scale feature pyramid network (FPN) that integrates a top-down pathway with lateral connections, enabling richer fusion of high-level semantics and low-level temporal details; and (3) a novel boundary distribution regression head. Inspired by the principles of Generalized Focal Loss (GFL), this new head recasts the challenging task of boundary regression as a more flexible probability distribution learning problem, allowing the model to explicitly represent and reason about boundary uncertainty. Within the paradigm of Transformer-based architectures, TBT-Former advances the formidable benchmark set by its predecessors, establishing a new level of performance on the highly competitive THUMOS14 and EPIC-Kitchens 100 datasets, while remaining competitive on the large-scale ActivityNet-1.3. Our code is available at https://github.com/aaivu/In21-S7-CS4681-AML-Research-Projects/tree/main/projects/210536K-Multi-Modal-Learning_Video-Understanding
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DCText: Scheduled Attention Masking for Visual Text Generation via Divide-and-Conquer Strategy</title>
<link>https://arxiv.org/abs/2512.01302</link>
<guid>https://arxiv.org/abs/2512.01302</guid>
<content:encoded><![CDATA[
arXiv:2512.01302v1 Announce Type: new 
Abstract: Despite recent text-to-image models achieving highfidelity text rendering, they still struggle with long or multiple texts due to diluted global attention. We propose DCText, a training-free visual text generation method that adopts a divide-and-conquer strategy, leveraging the reliable short-text generation of Multi-Modal Diffusion Transformers. Our method first decomposes a prompt by extracting and dividing the target text, then assigns each to a designated region. To accurately render each segment within their regions while preserving overall image coherence, we introduce two attention masks - Text-Focus and Context-Expansion - applied sequentially during denoising. Additionally, Localized Noise Initialization further improves text accuracy and region alignment without increasing computational cost. Extensive experiments on single- and multisentence benchmarks show that DCText achieves the best text accuracy without compromising image quality while also delivering the lowest generation latency.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Swaying: Surface-Based Framework for Aerodynamic Simulation with 3D Gaussians</title>
<link>https://arxiv.org/abs/2512.01306</link>
<guid>https://arxiv.org/abs/2512.01306</guid>
<content:encoded><![CDATA[
arXiv:2512.01306v1 Announce Type: new 
Abstract: Branches swaying in the breeze, flags rippling in the wind, and boats rocking on the water all show how aerodynamics shape natural motion -- an effect crucial for realism in vision and graphics. In this paper, we present Gaussian Swaying, a surface-based framework for aerodynamic simulation using 3D Gaussians. Unlike mesh-based methods that require costly meshing, or particle-based approaches that rely on discrete positional data, Gaussian Swaying models surfaces continuously with 3D Gaussians, enabling efficient and fine-grained aerodynamic interaction. Our framework unifies simulation and rendering on the same representation: Gaussian patches, which support force computation for dynamics while simultaneously providing normals for lightweight shading. Comprehensive experiments on both synthetic and real-world datasets across multiple metrics demonstrate that Gaussian Swaying achieves state-of-the-art performance and efficiency, offering a scalable approach for realistic aerodynamic scene simulation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in Distortion: Uncovering the Domain Gap Between Computer Vision and Brain Imaging - A Study on Pretraining for Age Prediction</title>
<link>https://arxiv.org/abs/2512.01310</link>
<guid>https://arxiv.org/abs/2512.01310</guid>
<content:encoded><![CDATA[
arXiv:2512.01310v1 Announce Type: new 
Abstract: Large-scale brain imaging datasets provide unprecedented opportunities for developing domain foundation models through pretraining. However, unlike natural image datasets in computer vision, these neuroimaging data often exhibit high heterogeneity in quality, ranging from well-structured scans to severely distorted or incomplete brain volumes. This raises a fundamental question: can noise or low-quality scans contribute meaningfully to pretraining, or do they instead hinder model learning? In this study, we systematically explore the role of data quality level in pretraining and its impact on downstream tasks. Specifically, we perform pretraining on datasets with different quality levels and perform fine-tuning for brain age prediction on external cohorts. Our results show significant performance differences across quality levels, revealing both opportunities and limitations. We further discuss the gap between computer vision practices and clinical neuroimaging standards, emphasizing the necessity of domain-aware curation to ensure trusted and generalizable domain-specific foundation models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IVCR-200K: A Large-Scale Multi-turn Dialogue Benchmark for Interactive Video Corpus Retrieval</title>
<link>https://arxiv.org/abs/2512.01312</link>
<guid>https://arxiv.org/abs/2512.01312</guid>
<content:encoded><![CDATA[
arXiv:2512.01312v1 Announce Type: new 
Abstract: In recent years, significant developments have been made in both video retrieval and video moment retrieval tasks, which respectively retrieve complete videos or moments for a given text query. These advancements have greatly improved user satisfaction during the search process. However, previous work has failed to establish meaningful "interaction" between the retrieval system and the user, and its one-way retrieval paradigm can no longer fully meet the personalization and dynamic needs of at least 80.8\% of users. In this paper, we introduce the Interactive Video Corpus Retrieval (IVCR) task, a more realistic setting that enables multi-turn, conversational, and realistic interactions between the user and the retrieval system. To facilitate research on this challenging task, we introduce IVCR-200K, a high-quality, bilingual, multi-turn, conversational, and abstract semantic dataset that supports video retrieval and even moment retrieval. Furthermore, we propose a comprehensive framework based on multi-modal large language models (MLLMs) to help users interact in several modes with more explainable solutions. The extensive experiments demonstrate the effectiveness of our dataset and framework.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance</title>
<link>https://arxiv.org/abs/2512.01314</link>
<guid>https://arxiv.org/abs/2512.01314</guid>
<content:encoded><![CDATA[
arXiv:2512.01314v1 Announce Type: new 
Abstract: In the digital economy era, digital watermarking serves as a critical basis for ownership proof of massive replicable content, including AI-generated and other virtual assets. Designing robust watermarks capable of withstanding various attacks and processing operations is even more paramount. We introduce TokenPure, a novel Diffusion Transformer-based framework designed for effective and consistent watermark removal. TokenPure solves the trade-off between thorough watermark destruction and content consistency by leveraging token-based conditional reconstruction. It reframes the task as conditional generation, entirely bypassing the initial watermark-carrying noise. We achieve this by decomposing the watermarked image into two complementary token sets: visual tokens for texture and structural tokens for geometry. These tokens jointly condition the diffusion process, enabling the framework to synthesize watermark-free images with fine-grained consistency and structural integrity. Comprehensive experiments show that TokenPure achieves state-of-the-art watermark removal and reconstruction fidelity, substantially outperforming existing baselines in both perceptual quality and consistency.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOD-S2R: A FOD Dataset for Sim2Real Transfer Learning based Object Detection</title>
<link>https://arxiv.org/abs/2512.01315</link>
<guid>https://arxiv.org/abs/2512.01315</guid>
<content:encoded><![CDATA[
arXiv:2512.01315v1 Announce Type: new 
Abstract: Foreign Object Debris (FOD) within aircraft fuel tanks presents critical safety hazards including fuel contamination, system malfunctions, and increased maintenance costs. Despite the severity of these risks, there is a notable lack of dedicated datasets for the complex, enclosed environments found inside fuel tanks. To bridge this gap, we present a novel dataset, FOD-S2R, composed of real and synthetic images of the FOD within a simulated aircraft fuel tank. Unlike existing datasets that focus on external or open-air environments, our dataset is the first to systematically evaluate the effectiveness of synthetic data in enhancing the real-world FOD detection performance in confined, closed structures. The real-world subset consists of 3,114 high-resolution HD images captured in a controlled fuel tank replica, while the synthetic subset includes 3,137 images generated using Unreal Engine. The dataset is composed of various Field of views (FOV), object distances, lighting conditions, color, and object size. Prior research has demonstrated that synthetic data can reduce reliance on extensive real-world annotations and improve the generalizability of vision models. Thus, we benchmark several state-of-the-art object detection models and demonstrate that introducing synthetic data improves the detection accuracy and generalization to real-world conditions. These experiments demonstrate the effectiveness of synthetic data in enhancing the model performance and narrowing the Sim2Real gap, providing a valuable foundation for developing automated FOD detection systems for aviation maintenance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Intracranial Aneurysm Vessel Segmentation: A Perspective from Computational Fluid Dynamics Applications</title>
<link>https://arxiv.org/abs/2512.01319</link>
<guid>https://arxiv.org/abs/2512.01319</guid>
<content:encoded><![CDATA[
arXiv:2512.01319v1 Announce Type: new 
Abstract: The precise segmentation of intracranial aneurysms and their parent vessels (IA-Vessel) is a critical step for hemodynamic analyses, which mainly depends on computational fluid dynamics (CFD). However, current segmentation methods predominantly focus on image-based evaluation metrics, often neglecting their practical effectiveness in subsequent CFD applications. To address this deficiency, we present the Intracranial Aneurysm Vessel Segmentation (IAVS) dataset, the first comprehensive, multi-center collection comprising 641 3D MRA images with 587 annotations of aneurysms and IA-Vessels. In addition to image-mask pairs, IAVS dataset includes detailed hemodynamic analysis outcomes, addressing the limitations of existing datasets that neglect topological integrity and CFD applicability. To facilitate the development and evaluation of clinically relevant techniques, we construct two evaluation benchmarks including global localization of aneurysms (Stage I) and fine-grained segmentation of IA-Vessel (Stage II) and develop a simple and effective two-stage framework, which can be used as a out-of-the-box method and strong baseline. For comprehensive evaluation of applicability of segmentation results, we establish a standardized CFD applicability evaluation system that enables the automated and consistent conversion of segmentation masks into CFD models, offering an applicability-focused assessment of segmentation outcomes. The dataset, code, and model will be public available at https://github.com/AbsoluteResonance/IAVS.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Stroke Risk Prediction: A Machine Learning Pipeline Combining ROS-Balanced Ensembles and XAI</title>
<link>https://arxiv.org/abs/2512.01333</link>
<guid>https://arxiv.org/abs/2512.01333</guid>
<content:encoded><![CDATA[
arXiv:2512.01333v1 Announce Type: new 
Abstract: Stroke is a major cause of death and permanent impairment, making it a major worldwide health concern. For prompt intervention and successful preventative tactics, early risk assessment is essential. To address this challenge, we used ensemble modeling and explainable AI (XAI) techniques to create an interpretable machine learning framework for stroke risk prediction. A thorough evaluation of 10 different machine learning models using 5-fold cross-validation across several datasets was part of our all-inclusive strategy, which also included feature engineering and data pretreatment (using Random Over-Sampling (ROS) to solve class imbalance). Our optimized ensemble model (Random Forest + ExtraTrees + XGBoost) performed exceptionally well, obtaining a strong 99.09% accuracy on the Stroke Prediction Dataset (SPD). We improved the model's transparency and clinical applicability by identifying three important clinical variables using LIME-based interpretability analysis: age, hypertension, and glucose levels. Through early prediction, this study highlights how combining ensemble learning with explainable AI (XAI) can deliver highly accurate and interpretable stroke risk assessment. By enabling data-driven prevention and personalized clinical decisions, our framework has the potential to transform stroke prediction and cardiovascular risk management.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignVid: Training-Free Attention Scaling for Semantic Fidelity in Text-Guided Image-to-Video Generation</title>
<link>https://arxiv.org/abs/2512.01334</link>
<guid>https://arxiv.org/abs/2512.01334</guid>
<content:encoded><![CDATA[
arXiv:2512.01334v1 Announce Type: new 
Abstract: Text-guided image-to-video (TI2V) generation has recently achieved remarkable progress, particularly in maintaining subject consistency and temporal coherence. However, existing methods still struggle to adhere to fine-grained prompt semantics, especially when prompts entail substantial transformations of the input image (e.g., object addition, deletion, or modification), a shortcoming we term semantic negligence. In a pilot study, we find that applying a Gaussian blur to the input image improves semantic adherence. Analyzing attention maps, we observe clearer foreground-background separation. From an energy perspective, this corresponds to a lower-entropy cross-attention distribution. Motivated by this, we introduce AlignVid, a training-free framework with two components: (i) Attention Scaling Modulation (ASM), which directly reweights attention via lightweight Q or K scaling, and (ii) Guidance Scheduling (GS), which applies ASM selectively across transformer blocks and denoising steps to reduce visual quality degradation. This minimal intervention improves prompt adherence while limiting aesthetic degradation. In addition, we introduce OmitI2V to evaluate semantic negligence in TI2V generation, comprising 367 human-annotated samples that span addition, deletion, and modification scenarios. Extensive experiments demonstrate that AlignVid can enhance semantic fidelity.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvalTalker: Learning to Evaluate Real-Portrait-Driven Multi-Subject Talking Humans</title>
<link>https://arxiv.org/abs/2512.01340</link>
<guid>https://arxiv.org/abs/2512.01340</guid>
<content:encoded><![CDATA[
arXiv:2512.01340v1 Announce Type: new 
Abstract: Speech-driven Talking Human (TH) generation, commonly known as "Talker," currently faces limitations in multi-subject driving capabilities. Extending this paradigm to "Multi-Talker," capable of animating multiple subjects simultaneously, introduces richer interactivity and stronger immersion in audiovisual communication. However, current Multi-Talkers still exhibit noticeable quality degradation caused by technical limitations, resulting in suboptimal user experiences. To address this challenge, we construct THQA-MT, the first large-scale Multi-Talker-generated Talking Human Quality Assessment dataset, consisting of 5,492 Multi-Talker-generated THs (MTHs) from 15 representative Multi-Talkers using 400 real portraits collected online. Through subjective experiments, we analyze perceptual discrepancies among different Multi-Talkers and identify 12 common types of distortion. Furthermore, we introduce EvalTalker, a novel TH quality assessment framework. This framework possesses the ability to perceive global quality, human characteristics, and identity consistency, while integrating Qwen-Sync to perceive multimodal synchrony. Experimental results demonstrate that EvalTalker achieves superior correlation with subjective scores, providing a robust foundation for future research on high-quality Multi-Talker generation and evaluation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision</title>
<link>https://arxiv.org/abs/2512.01342</link>
<guid>https://arxiv.org/abs/2512.01342</guid>
<content:encoded><![CDATA[
arXiv:2512.01342v1 Announce Type: new 
Abstract: Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Handwritten Text Recognition for Low Resource Languages</title>
<link>https://arxiv.org/abs/2512.01348</link>
<guid>https://arxiv.org/abs/2512.01348</guid>
<content:encoded><![CDATA[
arXiv:2512.01348v1 Announce Type: new 
Abstract: Despite considerable progress in handwritten text recognition, paragraph-level handwritten text recognition, especially in low-resource languages, such as Hindi, Urdu and similar scripts, remains a challenging problem. These languages, often lacking comprehensive linguistic resources, require special attention to develop robust systems for accurate optical character recognition (OCR). This paper introduces BharatOCR, a novel segmentation-free paragraph-level handwritten Hindi and Urdu text recognition. We propose a ViT-Transformer Decoder-LM architecture for handwritten text recognition, where a Vision Transformer (ViT) extracts visual features, a Transformer decoder generates text sequences, and a pre-trained language model (LM) refines the output to improve accuracy, fluency, and coherence. Our model utilizes a Data-efficient Image Transformer (DeiT) model proposed for masked image modeling in this research work. In addition, we adopt a RoBERTa architecture optimized for masked language modeling (MLM) to enhance the linguistic comprehension and generative capabilities of the proposed model. The transformer decoder generates text sequences from visual embeddings. This model is designed to iteratively process a paragraph image line by line, called implicit line segmentation. The proposed model was evaluated using our custom dataset ('Parimal Urdu') and ('Parimal Hindi'), introduced in this research work, as well as two public datasets. The proposed model achieved benchmark results in the NUST-UHWR, PUCIT-OUHL, and Parimal-Urdu datasets, achieving character recognition rates of 96.24%, 92.05%, and 94.80%, respectively. The model also provided benchmark results using the Hindi dataset achieving a character recognition rate of 80.64%. The results obtained from our proposed model indicated that it outperformed several state-of-the-art Urdu text recognition methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenBox: Annotate Any Bounding Boxes in 3D</title>
<link>https://arxiv.org/abs/2512.01352</link>
<guid>https://arxiv.org/abs/2512.01352</guid>
<content:encoded><![CDATA[
arXiv:2512.01352v1 Announce Type: new 
Abstract: Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects' physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via cross-modal instance alignment. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset, the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlinkBud: Detecting Hazards from Behind via Sampled Monocular 3D Detection on a Single Earbud</title>
<link>https://arxiv.org/abs/2512.01366</link>
<guid>https://arxiv.org/abs/2512.01366</guid>
<content:encoded><![CDATA[
arXiv:2512.01366v1 Announce Type: new 
Abstract: Failing to be aware of speeding vehicles approaching from behind poses a huge threat to the road safety of pedestrians and cyclists. In this paper, we propose BlinkBud, which utilizes a single earbud and a paired phone to online detect hazardous objects approaching from behind of a user. The core idea is to accurately track visually identified objects utilizing a small number of sampled camera images taken from the earbud. To minimize the power consumption of the earbud and the phone while guaranteeing the best tracking accuracy, a novel 3D object tracking algorithm is devised, integrating both a Kalman filter based trajectory estimation scheme and an optimal image sampling strategy based on reinforcement learning. Moreover, the impact of constant user head movements on the tracking accuracy is significantly eliminated by leveraging the estimated pitch and yaw angles to correct the object depth estimation and align the camera coordinate system to the user's body coordinate system, respectively. We implement a prototype BlinkBud system and conduct extensive real-world experiments. Results show that BlinkBud is lightweight with ultra-low mean power consumptions of 29.8 mW and 702.6 mW on the earbud and smartphone, respectively, and can accurately detect hazards with a low average false positive ratio (FPR) and false negative ratio (FNR) of 4.90% and 1.47%, respectively.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRAM: Shape-Realism Alignment Metric for No Reference 3D Shape Evaluation</title>
<link>https://arxiv.org/abs/2512.01373</link>
<guid>https://arxiv.org/abs/2512.01373</guid>
<content:encoded><![CDATA[
arXiv:2512.01373v1 Announce Type: new 
Abstract: 3D generation and reconstruction techniques have been widely used in computer games, film, and other content creation areas. As the application grows, there is a growing demand for 3D shapes that look truly realistic. Traditional evaluation methods rely on a ground truth to measure mesh fidelity. However, in many practical cases, a shape's realism does not depend on having a ground truth reference. In this work, we propose a Shape-Realism Alignment Metric that leverages a large language model (LLM) as a bridge between mesh shape information and realism evaluation. To achieve this, we adopt a mesh encoding approach that converts 3D shapes into the language token space. A dedicated realism decoder is designed to align the language model's output with human perception of realism. Additionally, we introduce a new dataset, RealismGrading, which provides human-annotated realism scores without the need for ground truth shapes. Our dataset includes shapes generated by 16 different algorithms on over a dozen objects, making it more representative of practical 3D shape distributions. We validate our metric's performance and generalizability through k-fold cross-validation across different objects. Experimental results show that our metric correlates well with human perceptions and outperforms existing methods, and has good generalizability.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textured Geometry Evaluation: Perceptual 3D Textured Shape Metric via 3D Latent-Geometry Network</title>
<link>https://arxiv.org/abs/2512.01380</link>
<guid>https://arxiv.org/abs/2512.01380</guid>
<content:encoded><![CDATA[
arXiv:2512.01380v1 Announce Type: new 
Abstract: Textured high-fidelity 3D models are crucial for games, AR/VR, and film, but human-aligned evaluation methods still fall behind despite recent advances in 3D reconstruction and generation. Existing metrics, such as Chamfer Distance, often fail to align with how humans evaluate the fidelity of 3D shapes. Recent learning-based metrics attempt to improve this by relying on rendered images and 2D image quality metrics. However, these approaches face limitations due to incomplete structural coverage and sensitivity to viewpoint choices. Moreover, most methods are trained on synthetic distortions, which differ significantly from real-world distortions, resulting in a domain gap. To address these challenges, we propose a new fidelity evaluation method that is based directly on 3D meshes with texture, without relying on rendering. Our method, named Textured Geometry Evaluation TGE, jointly uses the geometry and color information to calculate the fidelity of the input textured mesh with comparison to a reference colored shape. To train and evaluate our metric, we design a human-annotated dataset with real-world distortions. Experiments show that TGE outperforms rendering-based and geometry-only methods on real-world distortion dataset.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reversible Inversion for Training-Free Exemplar-guided Image Editing</title>
<link>https://arxiv.org/abs/2512.01382</link>
<guid>https://arxiv.org/abs/2512.01382</guid>
<content:encoded><![CDATA[
arXiv:2512.01382v1 Announce Type: new 
Abstract: Exemplar-guided Image Editing (EIE) aims to modify a source image according to a visual reference. Existing approaches often require large-scale pre-training to learn relationships between the source and reference images, incurring high computational costs. As a training-free alternative, inversion techniques can be used to map the source image into a latent space for manipulation. However, our empirical study reveals that standard inversion is sub-optimal for EIE, leading to poor quality and inefficiency. To tackle this challenge, we introduce \textbf{Reversible Inversion ({ReInversion})} for effective and efficient EIE. Specifically, ReInversion operates as a two-stage denoising process, which is first conditioned on the source image and subsequently on the reference. Besides, we introduce a Mask-Guided Selective Denoising (MSD) strategy to constrain edits to target regions, preserving the structural consistency of the background. Both qualitative and quantitative comparisons demonstrate that our ReInversion method achieves state-of-the-art EIE performance with the lowest computational overhead.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications</title>
<link>https://arxiv.org/abs/2512.01383</link>
<guid>https://arxiv.org/abs/2512.01383</guid>
<content:encoded><![CDATA[
arXiv:2512.01383v1 Announce Type: new 
Abstract: Understanding dynamic 4D environments-3D space evolving over time-is critical for robotic and interactive systems. These applications demand systems that can process streaming point cloud video in real-time, often under resource constraints, while also benefiting from past and present observations when available. However, current 4D backbone networks rely heavily on spatiotemporal convolutions and Transformers, which are often computationally intensive and poorly suited to real-time applications. We propose PointNet4D, a lightweight 4D backbone optimized for both online and offline settings. At its core is a Hybrid Mamba-Transformer temporal fusion block, which integrates the efficient state-space modeling of Mamba and the bidirectional modeling power of Transformers. This enables PointNet4D to handle variable-length online sequences efficiently across different deployment scenarios. To enhance temporal understanding, we introduce 4DMAP, a frame-wise masked auto-regressive pretraining strategy that captures motion cues across frames. Our extensive evaluations across 9 tasks on 7 datasets, demonstrating consistent improvements across diverse domains. We further demonstrate PointNet4D's utility by building two robotic application systems: 4D Diffusion Policy and 4D Imitation Learning, achieving substantial gains on the RoboTwin and HandoverSim benchmarks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRAMER: Frequency-Aligned Self-Distillation with Adaptive Modulation Leveraging Diffusion Priors for Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.01390</link>
<guid>https://arxiv.org/abs/2512.01390</guid>
<content:encoded><![CDATA[
arXiv:2512.01390v1 Announce Type: new 
Abstract: Real-image super-resolution (Real-ISR) seeks to recover HR images from LR inputs with mixed, unknown degradations. While diffusion models surpass GANs in perceptual quality, they under-reconstruct high-frequency (HF) details due to a low-frequency (LF) bias and a depth-wise "low-first, high-later" hierarchy. We introduce FRAMER, a plug-and-play training scheme that exploits diffusion priors without changing the backbone or inference. At each denoising step, the final-layer feature map teaches all intermediate layers. Teacher and student feature maps are decomposed into LF/HF bands via FFT masks to align supervision with the model's internal frequency hierarchy. For LF, an Intra Contrastive Loss (IntraCL) stabilizes globally shared structure. For HF, an Inter Contrastive Loss (InterCL) sharpens instance-specific details using random-layer and in-batch negatives. Two adaptive modulators, Frequency-based Adaptive Weight (FAW) and Frequency-based Alignment Modulation (FAM), reweight per-layer LF/HF signals and gate distillation by current similarity. Across U-Net and DiT backbones (e.g., Stable Diffusion 2, 3), FRAMER consistently improves PSNR/SSIM and perceptual metrics (LPIPS, NIQE, MANIQA, MUSIQ). Ablations validate the final-layer teacher and random-layer negatives.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rice-VL: Evaluating Vision-Language Models for Cultural Understanding Across ASEAN Countries</title>
<link>https://arxiv.org/abs/2512.01419</link>
<guid>https://arxiv.org/abs/2512.01419</guid>
<content:encoded><![CDATA[
arXiv:2512.01419v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) excel in multimodal tasks but often exhibit Western-centric biases, limiting their effectiveness in culturally diverse regions like Southeast Asia (SEA). To address this, we introduce RICE-VL, a novel benchmark evaluating VLM cultural understanding across 11 ASEAN countries. RICE-VL includes over 28,000 human-curated Visual Question Answering (VQA) samples -- covering True or False, Fill-in-the-Blank, and open-ended formats -- and 1,000 image-bounding box pairs for Visual Grounding, annotated by culturally informed experts across 14 sub-ground categories. We propose SEA-LAVE, an extension of the LAVE metric, assessing textual accuracy, cultural alignment, and country identification. Evaluations of six open- and closed-source VLMs reveal significant performance gaps in low-resource countries and abstract cultural domains. The Visual Grounding task tests models' ability to localize culturally significant elements in complex scenes, probing spatial and contextual accuracy. RICE-VL exposes limitations in VLMs' cultural comprehension and highlights the need for inclusive model development to better serve diverse global populations.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MDiff4STR: Mask Diffusion Model for Scene Text Recognition</title>
<link>https://arxiv.org/abs/2512.01422</link>
<guid>https://arxiv.org/abs/2512.01422</guid>
<content:encoded><![CDATA[
arXiv:2512.01422v1 Announce Type: new 
Abstract: Mask Diffusion Models (MDMs) have recently emerged as a promising alternative to auto-regressive models (ARMs) for vision-language tasks, owing to their flexible balance of efficiency and accuracy. In this paper, for the first time, we introduce MDMs into the Scene Text Recognition (STR) task. We show that vanilla MDM lags behind ARMs in terms of accuracy, although it improves recognition efficiency. To bridge this gap, we propose MDiff4STR, a Mask Diffusion model enhanced with two key improvement strategies tailored for STR. Specifically, we identify two key challenges in applying MDMs to STR: noising gap between training and inference, and overconfident predictions during inference. Both significantly hinder the performance of MDMs. To mitigate the first issue, we develop six noising strategies that better align training with inference behavior. For the second, we propose a token-replacement noise mechanism that provides a non-mask noise type, encouraging the model to reconsider and revise overly confident but incorrect predictions. We conduct extensive evaluations of MDiff4STR on both standard and challenging STR benchmarks, covering diverse scenarios including irregular, artistic, occluded, and Chinese text, as well as whether the use of pretraining. Across these settings, MDiff4STR consistently outperforms popular STR models, surpassing state-of-the-art ARMs in accuracy, while maintaining fast inference with only three denoising steps. Code: https://github.com/Topdu/OpenOCR.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>\textit{ViRectify}: A Challenging Benchmark for Video Reasoning Correction with Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.01424</link>
<guid>https://arxiv.org/abs/2512.01424</guid>
<content:encoded><![CDATA[
arXiv:2512.01424v1 Announce Type: new 
Abstract: As multimodal large language models (MLLMs) frequently exhibit errors in complex video reasoning scenarios, correcting these errors is critical for uncovering their weaknesses and improving performance. However, existing benchmarks lack systematic evaluation of MLLMs' ability to identify and correct these video reasoning errors. To bridge this gap, we propose \textit{ViRectify}, a comprehensive benchmark to evaluate their fine-grained correction capability. Through an AI-assisted annotation pipeline with human verification, we construct a dataset of over 30\textit{K} instances spanning dynamic perception, scientific reasoning, and embodied decision-making domains. In \textit{ViRectify}, we challenge MLLMs to perform step-wise error identification and generate rationales with key video evidence grounding. In addition, we further propose the trajectory evidence-driven correction framework, comprising step-wise error trajectory and reward modeling on visual evidence-grounded correction. It encourages the model to explicitly concentrate on error propagation and key timestamps for correction. Extensive evaluation across 16 advanced MLLMs demonstrates that our \textit{ViRectify} serves as a challenging testbed, where GPT-5 achieves only 31.94\% correction accuracy. Our framework enables a Qwen2.5-VL-7B to consistently outperform the variants of 72B on \textit{ViRectify}, showing the effectiveness of our approach. Further analysis uncovers systematic asymmetries in error correction across models, and our dataset is also a valuable data resource to perform reflection learning. We believe \textit{ViRectify} provides a new direction for comprehensively evaluating the advanced MLLMs in video reasoning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResDiT: Evoking the Intrinsic Resolution Scalability in Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.01426</link>
<guid>https://arxiv.org/abs/2512.01426</guid>
<content:encoded><![CDATA[
arXiv:2512.01426v1 Announce Type: new 
Abstract: Leveraging pre-trained Diffusion Transformers (DiTs) for high-resolution (HR) image synthesis often leads to spatial layout collapse and degraded texture fidelity. Prior work mitigates these issues with complex pipelines that first perform a base-resolution (i.e., training-resolution) denoising process to guide HR generation. We instead explore the intrinsic generative mechanisms of DiTs and propose ResDiT, a training-free method that scales resolution efficiently. We identify the core factor governing spatial layout, position embeddings (PEs), and show that the original PEs encode incorrect positional information when extrapolated to HR, which triggers layout collapse. To address this, we introduce a PE scaling technique that rectifies positional encoding under resolution changes. To further remedy low-fidelity details, we develop a local-enhancement mechanism grounded in base-resolution local attention. We design a patch-level fusion module that aggregates global and local cues, together with a Gaussian-weighted splicing strategy that eliminates grid artifacts. Comprehensive evaluations demonstrate that ResDiT consistently delivers high-fidelity, high-resolution image synthesis and integrates seamlessly with downstream tasks, including spatially controlled generation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Guided Open-World Anomaly Segmentation</title>
<link>https://arxiv.org/abs/2512.01427</link>
<guid>https://arxiv.org/abs/2512.01427</guid>
<content:encoded><![CDATA[
arXiv:2512.01427v1 Announce Type: new 
Abstract: Open-world and anomaly segmentation methods seek to enable autonomous driving systems to detect and segment both known and unknown objects in real-world scenes. However, existing methods do not assign semantically meaningful labels to unknown regions, and distinguishing and learning representations for unknown classes remains difficult. While open-vocabulary segmentation methods show promise in generalizing to novel classes, they require a fixed inference vocabulary and thus cannot be directly applied to anomaly segmentation where unknown classes are unconstrained. We propose Clipomaly, the first CLIP-based open-world and anomaly segmentation method for autonomous driving. Our zero-shot approach requires no anomaly-specific training data and leverages CLIP's shared image-text embedding space to both segment unknown objects and assign human-interpretable names to them. Unlike open-vocabulary methods, our model dynamically extends its vocabulary at inference time without retraining, enabling robust detection and naming of anomalies beyond common class definitions such as those in Cityscapes. Clipomaly achieves state-of-the-art performance on established anomaly segmentation benchmarks while providing interpretability and flexibility essential for practical deployment.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastAnimate: Towards Learnable Template Construction and Pose Deformation for Fast 3D Human Avatar Animation</title>
<link>https://arxiv.org/abs/2512.01444</link>
<guid>https://arxiv.org/abs/2512.01444</guid>
<content:encoded><![CDATA[
arXiv:2512.01444v1 Announce Type: new 
Abstract: 3D human avatar animation aims at transforming a human avatar from an arbitrary initial pose to a specified target pose using deformation algorithms. Existing approaches typically divide this task into two stages: canonical template construction and target pose deformation. However, current template construction methods demand extensive skeletal rigging and often produce artifacts for specific poses. Moreover, target pose deformation suffers from structural distortions caused by Linear Blend Skinning (LBS), which significantly undermines animation realism. To address these problems, we propose a unified learning-based framework to address both challenges in two phases. For the former phase, to overcome the inefficiencies and artifacts during template construction, we leverage a U-Net architecture that decouples texture and pose information in a feed-forward process, enabling fast generation of a human template. For the latter phase, we propose a data-driven refinement technique that enhances structural integrity. Extensive experiments show that our model delivers consistent performance across diverse poses with an optimal balance between efficiency and quality,surpassing state-of-the-art (SOTA) methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CourtMotion: Learning Event-Driven Motion Representations from Skeletal Data for Basketball</title>
<link>https://arxiv.org/abs/2512.01478</link>
<guid>https://arxiv.org/abs/2512.01478</guid>
<content:encoded><![CDATA[
arXiv:2512.01478v1 Announce Type: new 
Abstract: This paper presents CourtMotion, a spatiotemporal modeling framework for analyzing and predicting game events and plays as they develop in professional basketball. Anticipating basketball events requires understanding both physical motion patterns and their semantic significance in the context of the game. Traditional approaches that use only player positions fail to capture crucial indicators such as body orientation, defensive stance, or shooting preparation motions. Our two-stage approach first processes skeletal tracking data through Graph Neural Networks to capture nuanced motion patterns, then employs a Transformer architecture with specialized attention mechanisms to model player interactions. We introduce event projection heads that explicitly connect player movements to basketball events like passes, shots, and steals, training the model to associate physical motion patterns with their tactical purposes. Experiments on NBA tracking data demonstrate significant improvements over position-only baselines: 35% reduction in trajectory prediction error compared to state-of-the-art position-based models and consistent performance gains across key basketball analytics tasks. The resulting pretrained model serves as a powerful foundation for multiple downstream tasks, with pick detection, shot taker identification, assist prediction, shot location classification, and shot type recognition demonstrating substantial improvements over existing methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling</title>
<link>https://arxiv.org/abs/2512.01481</link>
<guid>https://arxiv.org/abs/2512.01481</guid>
<content:encoded><![CDATA[
arXiv:2512.01481v1 Announce Type: new 
Abstract: Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A variational method for curve extraction with curvature-dependent energies</title>
<link>https://arxiv.org/abs/2512.01494</link>
<guid>https://arxiv.org/abs/2512.01494</guid>
<content:encoded><![CDATA[
arXiv:2512.01494v1 Announce Type: new 
Abstract: We introduce a variational approach for extracting curves between a list of possible endpoints, based on the discretization of an energy and Smirnov's decomposition theorem for vector fields. It is used to design a bi-level minimization approach to automatically extract curves and 1D structures from an image, which is mostly unsupervised. We extend then the method to curvature-dependent energies, using a now classical lifting of the curves in the space of positions and orientations equipped with an appropriate sub-Riemanian or Finslerian metric.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark</title>
<link>https://arxiv.org/abs/2512.01495</link>
<guid>https://arxiv.org/abs/2512.01495</guid>
<content:encoded><![CDATA[
arXiv:2512.01495v1 Announce Type: new 
Abstract: Video instance segmentation (VIS) for low-light content remains highly challenging for both humans and machines alike, due to adverse imaging conditions including noise, blur and low-contrast. The lack of large-scale annotated datasets and the limitations of current synthetic pipelines, particularly in modeling temporal degradations, further hinder progress. Moreover, existing VIS methods are not robust to the degradations found in low-light videos and, as a result, perform poorly even when finetuned on low-light data. In this paper, we introduce \textbf{ELVIS} (\textbf{E}nhance \textbf{L}ow-light for \textbf{V}ideo \textbf{I}nstance \textbf{S}egmentation), a novel framework that enables effective domain adaptation of state-of-the-art VIS models to low-light scenarios. ELVIS comprises an unsupervised synthetic low-light video pipeline that models both spatial and temporal degradations, a calibration-free degradation profile synthesis network (VDP-Net) and an enhancement decoder head that disentangles degradations from content features. ELVIS improves performances by up to \textbf{+3.7AP} on the synthetic low-light YouTube-VIS 2019 dataset. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-aware Random Convolution and Source Matching for Domain Generalization in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.01510</link>
<guid>https://arxiv.org/abs/2512.01510</guid>
<content:encoded><![CDATA[
arXiv:2512.01510v1 Announce Type: new 
Abstract: We tackle the challenging problem of single-source domain generalization (DG) for medical image segmentation. To this end, we aim for training a network on one domain (e.g., CT) and directly apply it to a different domain (e.g., MR) without adapting the model and without requiring images or annotations from the new domain during training. We propose a novel method for promoting DG when training deep segmentation networks, which we call SRCSM. During training, our method diversifies the source domain through semantic-aware random convolution, where different regions of a source image are augmented differently, based on their annotation labels. At test-time, we complement the randomization of the training domain via mapping the intensity of target domain images, making them similar to source domain data. We perform a comprehensive evaluation on a variety of cross-modality and cross-center generalization settings for abdominal, whole-heart and prostate segmentation, where we outperform previous DG techniques in a vast majority of experiments. Additionally, we also investigate our method when training on whole-heart CT or MR data and testing on the diastolic and systolic phase of cine MR data captured with different scanner hardware, where we make a step towards closing the domain gap in this even more challenging setting. Overall, our evaluation shows that SRCSM can be considered a new state-of-the-art in DG for medical image segmentation and, moreover, even achieves a segmentation performance that matches the performance of the in-domain baseline in several settings.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuantumCanvas: A Multimodal Benchmark for Visual Learning of Atomic Interactions</title>
<link>https://arxiv.org/abs/2512.01519</link>
<guid>https://arxiv.org/abs/2512.01519</guid>
<content:encoded><![CDATA[
arXiv:2512.01519v1 Announce Type: new 
Abstract: Despite rapid advances in molecular and materials machine learning, most models still lack physical transferability: they fit correlations across whole molecules or crystals rather than learning the quantum interactions between atomic pairs. Yet bonding, charge redistribution, orbital hybridization, and electronic coupling all emerge from these two-body interactions that define local quantum fields in many-body systems. We introduce QuantumCanvas, a large-scale multimodal benchmark that treats two-body quantum systems as foundational units of matter. The dataset spans 2,850 element-element pairs, each annotated with 18 electronic, thermodynamic, and geometric properties and paired with ten-channel image representations derived from l- and m-resolved orbital densities, angular field transforms, co-occupancy maps, and charge-density projections. These physically grounded images encode spatial, angular, and electrostatic symmetries without explicit coordinates, providing an interpretable visual modality for quantum learning. Benchmarking eight architectures across 18 targets, we report mean absolute errors of 0.201 eV on energy gap using GATv2, 0.265 eV on HOMO and 0.274 eV on LUMO using EGNN. For energy-related quantities, DimeNet attains 2.27 eV total-energy MAE and 0.132 eV repulsive-energy MAE, while a multimodal fusion model achieves a 2.15 eV Mermin free-energy MAE. Pretraining on QuantumCanvas further improves convergence stability and generalization when fine-tuned on larger datasets such as QM9, MD17, and CrysMTM. By unifying orbital physics with vision-based representation learning, QuantumCanvas provides a principled and interpretable basis for learning transferable quantum interactions through coupled visual and numerical modalities. Dataset and model implementations are available at https://github.com/KurbanIntelligenceLab/QuantumCanvas.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Fuzzy System: Fuzzy Rule Guided Latent Multi-Path Diffusion Modeling</title>
<link>https://arxiv.org/abs/2512.01533</link>
<guid>https://arxiv.org/abs/2512.01533</guid>
<content:encoded><![CDATA[
arXiv:2512.01533v1 Announce Type: new 
Abstract: Diffusion models have emerged as a leading technique for generating images due to their ability to create high-resolution and realistic images. Despite their strong performance, diffusion models still struggle in managing image collections with significant feature differences. They often fail to capture complex features and produce conflicting results. Research has attempted to address this issue by learning different regions of an image through multiple diffusion paths and then combining them. However, this approach leads to inefficient coordination among multiple paths and high computational costs. To tackle these issues, this paper presents a Diffusion Fuzzy System (DFS), a latent-space multi-path diffusion model guided by fuzzy rules. DFS offers several advantages. First, unlike traditional multi-path diffusion methods, DFS uses multiple diffusion paths, each dedicated to learning a specific class of image features. By assigning each path to a different feature type, DFS overcomes the limitations of multi-path models in capturing heterogeneous image features. Second, DFS employs rule-chain-based reasoning to dynamically steer the diffusion process and enable efficient coordination among multiple paths. Finally, DFS introduces a fuzzy membership-based latent-space compression mechanism to reduce the computational costs of multi-path diffusion effectively. We tested our method on three public datasets: LSUN Bedroom, LSUN Church, and MS COCO. The results show that DFS achieves more stable training and faster convergence than existing single-path and multi-path diffusion models. Additionally, DFS surpasses baseline models in both image quality and alignment between text and images, and also shows improved accuracy when comparing generated images to target references.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Unsupervised Anomaly Detection in Brain Imaging: Large-Scale Benchmarking and Bias Analysis</title>
<link>https://arxiv.org/abs/2512.01534</link>
<guid>https://arxiv.org/abs/2512.01534</guid>
<content:encoded><![CDATA[
arXiv:2512.01534v1 Announce Type: new 
Abstract: Deep unsupervised anomaly detection in brain magnetic resonance imaging offers a promising route to identify pathological deviations without requiring lesion-specific annotations. Yet, fragmented evaluations, heterogeneous datasets, and inconsistent metrics have hindered progress toward clinical translation. Here, we present a large-scale, multi-center benchmark of deep unsupervised anomaly detection for brain imaging. The training cohort comprised 2,976 T1 and 2,972 T2-weighted scans from healthy individuals across six scanners, with ages ranging from 6 to 89 years. Validation used 92 scans to tune hyperparameters and estimate unbiased thresholds. Testing encompassed 2,221 T1w and 1,262 T2w scans spanning healthy datasets and diverse clinical cohorts. Across all algorithms, the Dice-based segmentation performance varied between 0.03 and 0.65, indicating substantial variability. To assess robustness, we systematically evaluated the impact of different scanners, lesion types and sizes, as well as demographics (age, sex). Reconstruction-based methods, particularly diffusion-inspired approaches, achieved the strongest lesion segmentation performance, while feature-based methods showed greater robustness under distributional shifts. However, systematic biases, such as scanner-related effects, were observed for the majority of algorithms, including that small and low-contrast lesions were missed more often, and that false positives varied with age and sex. Increasing healthy training data yields only modest gains, underscoring that current unsupervised anomaly detection frameworks are limited algorithmically rather than by data availability. Our benchmark establishes a transparent foundation for future research and highlights priorities for clinical translation, including image native pretraining, principled deviation measures, fairness-aware modeling, and robust domain adaptation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention</title>
<link>https://arxiv.org/abs/2512.01540</link>
<guid>https://arxiv.org/abs/2512.01540</guid>
<content:encoded><![CDATA[
arXiv:2512.01540v1 Announce Type: new 
Abstract: 3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MasHeNe: A Benchmark for Head and Neck CT Mass Segmentation using Window-Enhanced Mamba with Frequency-Domain Integration</title>
<link>https://arxiv.org/abs/2512.01563</link>
<guid>https://arxiv.org/abs/2512.01563</guid>
<content:encoded><![CDATA[
arXiv:2512.01563v1 Announce Type: new 
Abstract: Head and neck masses are space-occupying lesions that can compress the airway and esophagus and may affect nerves and blood vessels. Available public datasets primarily focus on malignant lesions and often overlook other space-occupying conditions in this region. To address this gap, we introduce MasHeNe, an initial dataset of 3,779 contrast-enhanced CT slices that includes both tumors and cysts with pixel-level annotations. We also establish a benchmark using standard segmentation baselines and report common metrics to enable fair comparison. In addition, we propose the Windowing-Enhanced Mamba with Frequency integration (WEMF) model. WEMF applies tri-window enhancement to enrich the input appearance before feature extraction. It further uses multi-frequency attention to fuse information across skip connections within a U-shaped Mamba backbone. On MasHeNe, WEMF attains the best performance among evaluated methods, with a Dice of 70.45%, IoU of 66.89%, NSD of 72.33%, and HD95 of 5.12 mm. This model indicates stable and strong results on this challenging task. MasHeNe provides a benchmark for head-and-neck mass segmentation beyond malignancy-only datasets. The observed error patterns also suggest that this task remains challenging and requires further research. Our dataset and code are available at https://github.com/drthaodao3101/MasHeNe.git.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoleMotion: A Large-Scale Dataset towards Robust Scene-Specific Role-Playing Motion Synthesis with Fine-grained Descriptions</title>
<link>https://arxiv.org/abs/2512.01582</link>
<guid>https://arxiv.org/abs/2512.01582</guid>
<content:encoded><![CDATA[
arXiv:2512.01582v1 Announce Type: new 
Abstract: In this paper, we introduce RoleMotion, a large-scale human motion dataset that encompasses a wealth of role-playing and functional motion data tailored to fit various specific scenes. Existing text datasets are mainly constructed decentrally as amalgamation of assorted subsets that their data are nonfunctional and isolated to work together to cover social activities in various scenes. Also, the quality of motion data is inconsistent, and textual annotation lacks fine-grained details in these datasets. In contrast, RoleMotion is meticulously designed and collected with a particular focus on scenes and roles. The dataset features 25 classic scenes, 110 functional roles, over 500 behaviors, and 10296 high-quality human motion sequences of body and hands, annotated with 27831 fine-grained text descriptions. We build an evaluator stronger than existing counterparts, prove its reliability, and evaluate various text-to-motion methods on our dataset. Finally, we explore the interplay of motion generation of body and hands. Experimental results demonstrate the high-quality and functionality of our dataset on text-driven whole-body generation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation</title>
<link>https://arxiv.org/abs/2512.01589</link>
<guid>https://arxiv.org/abs/2512.01589</guid>
<content:encoded><![CDATA[
arXiv:2512.01589v1 Announce Type: new 
Abstract: Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at https://github.com/drthaodao3101/AbscessHeNe.git.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth Matching Method Based on ShapeDTW for Oil-Based Mud Imager</title>
<link>https://arxiv.org/abs/2512.01611</link>
<guid>https://arxiv.org/abs/2512.01611</guid>
<content:encoded><![CDATA[
arXiv:2512.01611v1 Announce Type: new 
Abstract: In well logging operations using the oil-based mud (OBM) microresistivity imager, which employs an interleaved design with upper and lower pad sets, depth misalignment issues persist between the pad images even after velocity correction. This paper presents a depth matching method for borehole images based on the Shape Dynamic Time Warping (ShapeDTW) algorithm. The method extracts local shape features to construct a morphologically sensitive distance matrix, better preserving structural similarity between sequences during alignment. We implement this by employing a combined feature set of the one-dimensional Histogram of Oriented Gradients (HOG1D) and the original signal as the shape descriptor. Field test examples demonstrate that our method achieves precise alignment for images with complex textures, depth shifts, or local scaling. Furthermore, it provides a flexible framework for feature extension, allowing the integration of other descriptors tailored to specific geological features.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge</title>
<link>https://arxiv.org/abs/2512.01629</link>
<guid>https://arxiv.org/abs/2512.01629</guid>
<content:encoded><![CDATA[
arXiv:2512.01629v1 Announce Type: new 
Abstract: Articulated 3D objects are critical for embodied AI, robotics, and interactive scene understanding, yet creating simulation-ready assets remains labor-intensive and requires expert modeling of part hierarchies and motion structures. We introduce SPARK, a framework for reconstructing physically consistent, kinematic part-level articulated objects from a single RGB image. Given an input image, we first leverage VLMs to extract coarse URDF parameters and generate part-level reference images. We then integrate the part-image guidance and the inferred structure graph into a generative diffusion transformer to synthesize consistent part and complete shapes of articulated objects. To further refine the URDF parameters, we incorporate differentiable forward kinematics and differentiable rendering to optimize joint types, axes, and origins under VLM-generated open-state supervision. Extensive experiments show that SPARK produces high-quality, simulation-ready articulated assets across diverse categories, enabling downstream applications such as robotic manipulation and interaction modeling.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Editing in the Joint Vision-Language Space for Zero-Shot Composed Image Retrieval</title>
<link>https://arxiv.org/abs/2512.01636</link>
<guid>https://arxiv.org/abs/2512.01636</guid>
<content:encoded><![CDATA[
arXiv:2512.01636v1 Announce Type: new 
Abstract: Composed Image Retrieval (CIR) enables fine-grained visual search by combining a reference image with a textual modification. While supervised CIR methods achieve high accuracy, their reliance on costly triplet annotations motivates zero-shot solutions. The core challenge in zero-shot CIR (ZS-CIR) stems from a fundamental dilemma: existing text-centric or diffusion-based approaches struggle to effectively bridge the vision-language modality gap. To address this, we propose Fusion-Diff, a novel generative editing framework with high effectiveness and data efficiency designed for multimodal alignment. First, it introduces a multimodal fusion feature editing strategy within a joint vision-language (VL) space, substantially narrowing the modality gap. Second, to maximize data efficiency, the framework incorporates a lightweight Control-Adapter, enabling state-of-the-art performance through fine-tuning on only a limited-scale synthetic dataset of 200K samples. Extensive experiments on standard CIR benchmarks (CIRR, FashionIQ, and CIRCO) demonstrate that Fusion-Diff significantly outperforms prior zero-shot approaches. We further enhance the interpretability of our model by visualizing the fused multimodal representations.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViT$^3$: Unlocking Test-Time Training in Vision</title>
<link>https://arxiv.org/abs/2512.01643</link>
<guid>https://arxiv.org/abs/2512.01643</guid>
<content:encoded><![CDATA[
arXiv:2512.01643v1 Announce Type: new 
Abstract: Test-Time Training (TTT) has recently emerged as a promising direction for efficient sequence modeling. TTT reformulates attention operation as an online learning problem, constructing a compact inner model from key-value pairs at test time. This reformulation opens a rich and flexible design space while achieving linear computational complexity. However, crafting a powerful visual TTT design remains challenging: fundamental choices for the inner module and inner training lack comprehensive understanding and practical guidelines. To bridge this critical gap, in this paper, we present a systematic empirical study of TTT designs for visual sequence modeling. From a series of experiments and analyses, we distill six practical insights that establish design principles for effective visual TTT and illuminate paths for future improvement. These findings culminate in the Vision Test-Time Training (ViT$^3$) model, a pure TTT architecture that achieves linear complexity and parallelizable computation. We evaluate ViT$^3$ across diverse visual tasks, including image classification, image generation, object detection, and semantic segmentation. Results show that ViT$^3$ consistently matches or outperforms advanced linear-complexity models (e.g., Mamba and linear attention variants) and effectively narrows the gap to highly optimized vision Transformers. We hope this study and the ViT$^3$ baseline can facilitate future work on visual TTT models. Code is available at https://github.com/LeapLabTHU/ViTTT.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DB-KAUNet: An Adaptive Dual Branch Kolmogorov-Arnold UNet for Retinal Vessel Segmentation</title>
<link>https://arxiv.org/abs/2512.01657</link>
<guid>https://arxiv.org/abs/2512.01657</guid>
<content:encoded><![CDATA[
arXiv:2512.01657v1 Announce Type: new 
Abstract: Accurate segmentation of retinal vessels is crucial for the clinical diagnosis of numerous ophthalmic and systemic diseases. However, traditional Convolutional Neural Network (CNN) methods exhibit inherent limitations, struggling to capture long-range dependencies and complex nonlinear relationships. To address the above limitations, an Adaptive Dual Branch Kolmogorov-Arnold UNet (DB-KAUNet) is proposed for retinal vessel segmentation. In DB-KAUNet, we design a Heterogeneous Dual-Branch Encoder (HDBE) that features parallel CNN and Transformer pathways. The HDBE strategically interleaves standard CNN and Transformer blocks with novel KANConv and KAT blocks, enabling the model to form a comprehensive feature representation. To optimize feature processing, we integrate several critical components into the HDBE. First, a Cross-Branch Channel Interaction (CCI) module is embedded to facilitate efficient interaction of channel features between the parallel pathways. Second, an attention-based Spatial Feature Enhancement (SFE) module is employed to enhance spatial features and fuse the outputs from both branches. Building upon the SFE module, an advanced Spatial Feature Enhancement with Geometrically Adaptive Fusion (SFE-GAF) module is subsequently developed. In the SFE-GAF module, adaptive sampling is utilized to focus on true vessel morphology precisely. The adaptive process strengthens salient vascular features while significantly reducing background noise and computational overhead. Extensive experiments on the DRIVE, STARE, and CHASE_DB1 datasets validate that DB-KAUNet achieves leading segmentation performance and demonstrates exceptional robustness.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Scale Gap: Balanced Tiny and General Object Detection in Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2512.01665</link>
<guid>https://arxiv.org/abs/2512.01665</guid>
<content:encoded><![CDATA[
arXiv:2512.01665v1 Announce Type: new 
Abstract: Tiny object detection in remote sensing imagery has attracted significant research interest in recent years. Despite recent progress, achieving balanced detection performance across diverse object scales remains a formidable challenge, particularly in scenarios where dense tiny objects and large objects coexist. Although large foundation models have revolutionized general vision tasks, their application to tiny object detection remains unexplored due to the extreme scale variation and density distribution inherent to remote sensing imagery. To bridge this scale gap, we propose ScaleBridge-Det, to the best of our knowledge, the first large detection framework designed for tiny objects, which could achieve balanced performance across diverse scales through scale-adaptive expert routing and density-guided query allocation. Specifically, we introduce a Routing-Enhanced Mixture Attention (REM) module that dynamically selects and fuses scale-specific expert features via adaptive routing to address the tendency of standard MoE models to favor dominant scales. REM generates complementary and discriminative multi-scale representations suitable for both tiny and large objects. Furthermore, we present a Density-Guided Dynamic Query (DGQ) module that predicts object density to adaptively adjust query positions and numbers, enabling efficient resource allocation for objects of varying scales. The proposed framework allows ScaleBridge-Det to simultaneously optimize performance for both dense tiny and general objects without trade-offs. Extensive experiments on benchmark and cross-domain datasets demonstrate that ScaleBridge-Det achieves state-of-the-art performance on AI-TOD-V2 and DTOD, while exhibiting superior cross-domain robustness on VisDrone.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRASP: Guided Residual Adapters with Sample-wise Partitioning</title>
<link>https://arxiv.org/abs/2512.01675</link>
<guid>https://arxiv.org/abs/2512.01675</guid>
<content:encoded><![CDATA[
arXiv:2512.01675v1 Announce Type: new 
Abstract: Recent advances in text-to-image diffusion models enable high-fidelity generation across diverse prompts. However, these models falter in long-tail settings, such as medical imaging, where rare pathologies comprise a small fraction of data. This results in mode collapse: tail-class outputs lack quality and diversity, undermining the goal of synthetic data augmentation for underrepresented conditions. We pinpoint gradient conflicts between frequent head and rare tail classes as the primary culprit, a factor unaddressed by existing sampling or conditioning methods that mainly steer inference without altering the learned distribution. To resolve this, we propose GRASP: Guided Residual Adapters with Sample-wise Partitioning. GRASP uses external priors to statically partition samples into clusters that minimize intra-group gradient clashes. It then fine-tunes pre-trained models by injecting cluster-specific residual adapters into transformer feedforward layers, bypassing learned gating for stability and efficiency. On the long-tail MIMIC-CXR-LT dataset, GRASP yields superior FID and diversity metrics, especially for rare classes, outperforming baselines like vanilla fine-tuning and Mixture of Experts variants. Downstream classification on NIH-CXR-LT improves considerably for tail labels. Generalization to ImageNet-LT confirms broad applicability. Our method is lightweight, scalable, and readily integrates with diffusion pipelines.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-world Hand-Object Interaction Video Generation Based on Structure and Contact-aware Representation</title>
<link>https://arxiv.org/abs/2512.01677</link>
<guid>https://arxiv.org/abs/2512.01677</guid>
<content:encoded><![CDATA[
arXiv:2512.01677v1 Announce Type: new 
Abstract: Generating realistic hand-object interactions (HOI) videos is a significant challenge due to the difficulty of modeling physical constraints (e.g., contact and occlusion between hands and manipulated objects). Current methods utilize HOI representation as an auxiliary generative objective to guide video synthesis. However, there is a dilemma between 2D and 3D representations that cannot simultaneously guarantee scalability and interaction fidelity. To address this limitation, we propose a structure and contact-aware representation that captures hand-object contact, hand-object occlusion, and holistic structure context without 3D annotations. This interaction-oriented and scalable supervision signal enables the model to learn fine-grained interaction physics and generalize to open-world scenarios. To fully exploit the proposed representation, we introduce a joint-generation paradigm with a share-and-specialization strategy that generates interaction-oriented representations and videos. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on two real-world datasets in generating physics-realistic and temporally coherent HOI videos. Furthermore, our approach exhibits strong generalization to challenging open-world scenarios, highlighting the benefit of our scalable design. Our project page is https://hgzn258.github.io/SCAR/.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Domain Validation of a Resection-Trained Self-Supervised Model on Multicentre Mesothelioma Biopsies</title>
<link>https://arxiv.org/abs/2512.01681</link>
<guid>https://arxiv.org/abs/2512.01681</guid>
<content:encoded><![CDATA[
arXiv:2512.01681v1 Announce Type: new 
Abstract: Accurate subtype classification and outcome prediction in mesothelioma are essential for guiding therapy and patient care. Most computational pathology models are trained on large tissue images from resection specimens, limiting their use in real-world settings where small biopsies are common. We show that a self-supervised encoder trained on resection tissue can be applied to biopsy material, capturing meaningful morphological patterns. Using these patterns, the model can predict patient survival and classify tumor subtypes. This approach demonstrates the potential of AI-driven tools to support diagnosis and treatment planning in mesothelioma.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models</title>
<link>https://arxiv.org/abs/2512.01686</link>
<guid>https://arxiv.org/abs/2512.01686</guid>
<content:encoded><![CDATA[
arXiv:2512.01686v1 Announce Type: new 
Abstract: Current story visualization methods tend to position subjects solely by text and face challenges in maintaining artistic consistency. To address these limitations, we introduce DreamingComics, a layout-aware story visualization framework. We build upon a pretrained video diffusion-transformer (DiT) model, leveraging its spatiotemporal priors to enhance identity and style consistency. For layout-based position control, we propose RegionalRoPE, a region-aware positional encoding scheme that re-indexes embeddings based on the target layout. Additionally, we introduce a masked condition loss to further constrain each subject's visual features to their designated region. To infer layouts from natural language scripts, we integrate an LLM-based layout generator trained to produce comic-style layouts, enabling flexible and controllable layout conditioning. We present a comprehensive evaluation of our approach, showing a 29.2% increase in character consistency and a 36.2% increase in style similarity compared to previous methods, while displaying high spatial accuracy. Our project page is available at https://yj7082126.github.io/dreamingcomics/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSR: Semantic and Spatial Rectification for CLIP-based Weakly Supervised Segmentation</title>
<link>https://arxiv.org/abs/2512.01701</link>
<guid>https://arxiv.org/abs/2512.01701</guid>
<content:encoded><![CDATA[
arXiv:2512.01701v1 Announce Type: new 
Abstract: In recent years, Contrastive Language-Image Pretraining (CLIP) has been widely applied to Weakly Supervised Semantic Segmentation (WSSS) tasks due to its powerful cross-modal semantic understanding capabilities. This paper proposes a novel Semantic and Spatial Rectification (SSR) method to address the limitations of existing CLIP-based weakly supervised semantic segmentation approaches: over-activation in non-target foreground regions and background areas. Specifically, at the semantic level, the Cross-Modal Prototype Alignment (CMPA) establishes a contrastive learning mechanism to enforce feature space alignment across modalities, reducing inter-class overlap while enhancing semantic correlations, to rectify over-activation in non-target foreground regions effectively; at the spatial level, the Superpixel-Guided Correction (SGC) leverages superpixel-based spatial priors to precisely filter out interference from non-target regions during affinity propagation, significantly rectifying background over-activation. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate that our method outperforms all single-stage approaches, as well as more complex multi-stage approaches, achieving mIoU scores of 79.5% and 50.6%, respectively.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos</title>
<link>https://arxiv.org/abs/2512.01707</link>
<guid>https://arxiv.org/abs/2512.01707</guid>
<content:encoded><![CDATA[
arXiv:2512.01707v1 Announce Type: new 
Abstract: Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing</title>
<link>https://arxiv.org/abs/2512.01755</link>
<guid>https://arxiv.org/abs/2512.01755</guid>
<content:encoded><![CDATA[
arXiv:2512.01755v1 Announce Type: new 
Abstract: Instruction-based image editing through natural language has emerged as a powerful paradigm for intuitive visual manipulation. While recent models achieve impressive results on single edits, they suffer from severe quality degradation under multi-turn editing. Through systematic analysis, we identify progressive loss of high-frequency information as the primary cause of this quality degradation. We present FreqEdit, a training-free framework that enables stable editing across 10+ consecutive iterations. Our approach comprises three synergistic components: (1) high-frequency feature injection from reference velocity fields to preserve fine-grained details, (2) an adaptive injection strategy that spatially modulates injection strength for precise region-specific control, and (3) a path compensation mechanism that periodically recalibrates the editing trajectory to prevent over-constraint. Extensive experiments demonstrate that FreqEdit achieves superior performance in both identity preservation and instruction following compared to seven state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiconAgent: History Context-aware Policy Optimization for GUI Agents</title>
<link>https://arxiv.org/abs/2512.01763</link>
<guid>https://arxiv.org/abs/2512.01763</guid>
<content:encoded><![CDATA[
arXiv:2512.01763v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents require effective use of historical context to perform sequential navigation tasks. While incorporating past actions and observations can improve decision making, naive use of full history leads to excessive computational overhead and distraction from irrelevant information. To address this, we introduce HiconAgent, a GUI agent trained with History Context-aware Policy Optimization (HCPO) for efficient and effective utilization of historical information. HCPO optimizes history usage in both sampling and policy updates through two complementary components: (1) Dynamic Context Sampling (DCS) presents the agent with variable length histories during sampling, enabling adaptive use of the most relevant context; (2) Anchor-guided History Compression (AHC) refines the policy update phase with a dual branch strategy where the compressed branch removes history observations while keeping history actions as information flow anchors. The compressed and uncompressed branches are coupled through a history-enhanced alignment loss to enforce consistent history usage while maintaining efficiency. Experiments on mainstream GUI navigation benchmarks demonstrate strong performance. Despite being smaller, HiconAgent-3B outperforms GUI-R1-7B by +8.46 percent grounding accuracy and +11.32 percent step success rate on GUI-Odyssey, while achieving comparable results on AndroidControl and AITW with up to 2.47x computational speedup and 60 percent FLOPs reduction.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis</title>
<link>https://arxiv.org/abs/2512.01769</link>
<guid>https://arxiv.org/abs/2512.01769</guid>
<content:encoded><![CDATA[
arXiv:2512.01769v1 Announce Type: new 
Abstract: Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.
  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Rigid and Non-Rigid Medical Image Registration Using Learnable Edge Kernels</title>
<link>https://arxiv.org/abs/2512.01771</link>
<guid>https://arxiv.org/abs/2512.01771</guid>
<content:encoded><![CDATA[
arXiv:2512.01771v1 Announce Type: new 
Abstract: Medical image registration is crucial for various clinical and research applications including disease diagnosis or treatment planning which require alignment of images from different modalities, time points, or subjects. Traditional registration techniques often struggle with challenges such as contrast differences, spatial distortions, and modality-specific variations. To address these limitations, we propose a method that integrates learnable edge kernels with learning-based rigid and non-rigid registration techniques. Unlike conventional layers that learn all features without specific bias, our approach begins with a predefined edge detection kernel, which is then perturbed with random noise. These kernels are learned during training to extract optimal edge features tailored to the task. This adaptive edge detection enhances the registration process by capturing diverse structural features critical in medical imaging. To provide clearer insight into the contribution of each component in our design, we introduce four variant models for rigid registration and four variant models for non-rigid registration. We evaluated our approach using a dataset provided by the Medical University across three setups: rigid registration without skull removal, with skull removal, and non-rigid registration. Additionally, we assessed performance on two publicly available datasets. Across all experiments, our method consistently outperformed state-of-the-art techniques, demonstrating its potential to improve multi-modal image alignment and anatomical structure analysis.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating SAM2 for Video Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.01774</link>
<guid>https://arxiv.org/abs/2512.01774</guid>
<content:encoded><![CDATA[
arXiv:2512.01774v1 Announce Type: new 
Abstract: The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learned Image Compression for Earth Observation: Implications for Downstream Segmentation Tasks</title>
<link>https://arxiv.org/abs/2512.01788</link>
<guid>https://arxiv.org/abs/2512.01788</guid>
<content:encoded><![CDATA[
arXiv:2512.01788v1 Announce Type: new 
Abstract: The rapid growth of data from satellite-based Earth observation (EO) systems poses significant challenges in data transmission and storage. We evaluate the potential of task-specific learned compression algorithms in this context to reduce data volumes while retaining crucial information. In detail, we compare traditional compression (JPEG 2000) versus a learned compression approach (Discretized Mixed Gaussian Likelihood) on three EO segmentation tasks: Fire, cloud, and building detection. Learned compression notably outperforms JPEG 2000 for large-scale, multi-channel optical imagery in both reconstruction quality (PSNR) and segmentation accuracy. However, traditional codecs remain competitive on smaller, single-channel thermal infrared datasets due to limited data and architectural constraints. Additionally, joint end-to-end optimization of compression and segmentation models does not improve performance over standalone optimization.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM3-UNet: Simplified Adaptation of Segment Anything Model 3</title>
<link>https://arxiv.org/abs/2512.01789</link>
<guid>https://arxiv.org/abs/2512.01789</guid>
<content:encoded><![CDATA[
arXiv:2512.01789v1 Announce Type: new 
Abstract: In this paper, we introduce SAM3-UNet, a simplified variant of Segment Anything Model 3 (SAM3), designed to adapt SAM3 for downstream tasks at a low cost. Our SAM3-UNet consists of three components: a SAM3 image encoder, a simple adapter for parameter-efficient fine-tuning, and a lightweight U-Net-style decoder. Preliminary experiments on multiple tasks, such as mirror detection and salient object detection, demonstrate that the proposed SAM3-UNet outperforms the prior SAM2-UNet and other state-of-the-art methods, while requiring less than 6 GB of GPU memory during training with a batch size of 12. The code is publicly available at https://github.com/WZH0120/SAM3-UNet.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos</title>
<link>https://arxiv.org/abs/2512.01803</link>
<guid>https://arxiv.org/abs/2512.01803</guid>
<content:encoded><![CDATA[
arXiv:2512.01803v1 Announce Type: new 
Abstract: Despite rapid advances in video generative models, robust metrics for evaluating visual and temporal correctness of complex human actions remain elusive. Critically, existing pure-vision encoders and Multimodal Large Language Models (MLLMs) are strongly appearance-biased, lack temporal understanding, and thus struggle to discern intricate motion dynamics and anatomical implausibilities in generated videos. We tackle this gap by introducing a novel evaluation metric derived from a learned latent space of real-world human actions. Our method first captures the nuances, constraints, and temporal smoothness of real-world motion by fusing appearance-agnostic human skeletal geometry features with appearance-based features. We posit that this combined feature space provides a robust representation of action plausibility. Given a generated video, our metric quantifies its action quality by measuring the distance between its underlying representations and this learned real-world action distribution. For rigorous validation, we develop a new multi-faceted benchmark specifically designed to probe temporally challenging aspects of human action fidelity. Through extensive experiments, we show that our metric achieves substantial improvement of more than 68% compared to existing state-of-the-art methods on our benchmark, performs competitively on established external benchmarks, and has a stronger correlation with human perception. Our in-depth analysis reveals critical limitations in current video generative models and establishes a new standard for advanced research in video generation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Envision: Benchmarking Unified Understanding &amp; Generation for Causal World Process Insights</title>
<link>https://arxiv.org/abs/2512.01816</link>
<guid>https://arxiv.org/abs/2512.01816</guid>
<content:encoded><![CDATA[
arXiv:2512.01816v1 Announce Type: new 
Abstract: Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling</title>
<link>https://arxiv.org/abs/2512.01821</link>
<guid>https://arxiv.org/abs/2512.01821</guid>
<content:encoded><![CDATA[
arXiv:2512.01821v1 Announce Type: new 
Abstract: Spatial reasoning, the ability to understand and interpret the 3D structure of the world, is a critical yet underdeveloped capability in Multimodal Large Language Models (MLLMs). Current methods predominantly rely on verbal descriptive tuning, which suffers from visual illiteracy, i.e., they learn spatial concepts through textual symbols alone, devoid of connection to their visual manifestations. To bridge this gap, this paper introduces MILO, an Implicit spatIaL wOrld modeling paradigm that simulates human-like spatial imagination. MILO integrates a visual generator to provide geometry-aware feedback, thereby implicitly grounding the MLLM's symbolic reasoning in perceptual experience. Complementing this paradigm, we propose RePE (Relative Positional Encoding), a novel encoding scheme that captures relative camera-pose transformations, offering superior performance over absolute coordinate systems. To support the training, we construct GeoGen, a large-scale Geometry-aware Generative dataset with approximately 2,241 videos and 67,827 observation-action-outcome triplets. Experiments demonstrate that our approach significantly enhances spatial reasoning capabilities across multiple baselines and benchmarks, offering a more holistic understanding of 3D space.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CauSight: Learning to Supersense for Visual Causal Discovery</title>
<link>https://arxiv.org/abs/2512.01827</link>
<guid>https://arxiv.org/abs/2512.01827</guid>
<content:encoded><![CDATA[
arXiv:2512.01827v1 Announce Type: new 
Abstract: Causal thinking enables humans to understand not just what is seen, but why it happens. To replicate this capability in modern AI systems, we introduce the task of visual causal discovery. It requires models to infer cause-and-effect relations among visual entities across diverse scenarios instead of merely perceiving their presence. To this end, we first construct the Visual Causal Graph dataset (VCG-32K), a large-scale collection of over 32,000 images annotated with entity-level causal graphs, and further develop CauSight, a novel vision-language model to perform visual causal discovery through causally aware reasoning. Our training recipe integrates three components: (1) training data curation from VCG-32K, (2) Tree-of-Causal-Thought (ToCT) for synthesizing reasoning trajectories, and (3) reinforcement learning with a designed causal reward to refine the reasoning policy. Experiments show that CauSight outperforms GPT-4.1 on visual causal discovery, achieving over a threefold performance boost (21% absolute gain). Our code, model, and dataset are fully open-sourced at project page: https://github.com/OpenCausaLab/CauSight.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic</title>
<link>https://arxiv.org/abs/2512.01830</link>
<guid>https://arxiv.org/abs/2512.01830</guid>
<content:encoded><![CDATA[
arXiv:2512.01830v1 Announce Type: new 
Abstract: Recently, two-stage fine-tuning strategies, e.g., acquiring essential driving knowledge through supervised fine-tuning (SFT) and further enhancing decision-making and planning via reinforcement fine-tuning (RFT), have shown strong potential in advancing the knowledge-driven autonomous driving (AD) paradigm. However, the learning nature of SFT still limits the generalization of reasoning, thereby constraining the full potential of driving performance. Meanwhile, current RFT approaches are primarily applied to downstream tasks, since scene understanding is an open-ended problem where corresponding rewards are difficult to quantify. To address these limitations, we propose OpenREAD, an OPEN-ended REasoning reinforced vision-language model (VLM)-based autonomous driving (AD) framework that enables end-to-end RFT across the full spectrum from high-level reasoning to low-level trajectory planning. Specifically, we begin by constructing large-scale Chain-of-Thought (CoT) annotations on open-source driving-related knowledge datasets, and employ the powerful Qwen3 large language model (LLM) as the critic in RFT to quantify reasoning quality for open-ended questions during reward modeling. Extensive experiments confirm that joint end-to-end RFT yields substantial improvements in both upstream and downstream tasks, enabling OpenREAD to achieve state-of-the-art performance on reasoning and planning benchmarks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models</title>
<link>https://arxiv.org/abs/2512.01843</link>
<guid>https://arxiv.org/abs/2512.01843</guid>
<content:encoded><![CDATA[
arXiv:2512.01843v1 Announce Type: new 
Abstract: Driven by the growing capacity and training scale, Text-to-Video (T2V) generation models have recently achieved substantial progress in video quality, length, and instruction-following capability. However, whether these models can understand physics and generate physically plausible videos remains a question. While Vision-Language Models (VLMs) have been widely used as general-purpose evaluators in various applications, they struggle to identify the physically impossible content from generated videos. To investigate this issue, we construct a \textbf{PID} (\textbf{P}hysical \textbf{I}mplausibility \textbf{D}etection) dataset, which consists of a \textit{test split} of 500 manually annotated videos and a \textit{train split} of 2,588 paired videos, where each implausible video is generated by carefully rewriting the caption of its corresponding real-world video to induce T2V models producing physically implausible content. With the constructed dataset, we introduce a lightweight fine-tuning approach, enabling VLMs to not only detect physically implausible events but also generate textual explanations on the violated physical principles. Taking the fine-tuned VLM as a physical plausibility detector and explainer, namely \textbf{PhyDetEx}, we benchmark a series of state-of-the-art T2V models to assess their adherence to physical laws. Our findings show that although recent T2V models have made notable progress toward generating physically plausible content, understanding and adhering to physical laws remains a challenging issue, especially for open-source models. Our dataset, training code, and checkpoints are available at \href{https://github.com/Zeqing-Wang/PhyDetEx}{https://github.com/Zeqing-Wang/PhyDetEx}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching</title>
<link>https://arxiv.org/abs/2512.01850</link>
<guid>https://arxiv.org/abs/2512.01850</guid>
<content:encoded><![CDATA[
arXiv:2512.01850v1 Announce Type: new 
Abstract: Point cloud registration aligns multiple unposed point clouds into a common frame, and is a core step for 3D reconstruction and robot localization. In this work, we cast registration as conditional generation: a learned continuous, point-wise velocity field transports noisy points to a registered scene, from which the pose of each view is recovered. Unlike previous methods that conduct correspondence matching to estimate the transformation between a pair of point clouds and then optimize the pairwise transformations to realize multi-view registration, our model directly generates the registered point cloud. With a lightweight local feature extractor and test-time rigidity enforcement, our approach achieves state-of-the-art results on pairwise and multi-view registration benchmarks, particularly with low overlap, and generalizes across scales and sensor modalities. It further supports downstream tasks including relocalization, multi-robot SLAM, and multi-session map merging. Source code available at: https://github.com/PRBonn/RAP.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COACH: Collaborative Agents for Contextual Highlighting - A Multi-Agent Framework for Sports Video Analysis</title>
<link>https://arxiv.org/abs/2512.01853</link>
<guid>https://arxiv.org/abs/2512.01853</guid>
<content:encoded><![CDATA[
arXiv:2512.01853v1 Announce Type: new 
Abstract: Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct "cognitive tool" specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video intelligence.The project homepage is available at https://aiden1020.github.io/COACH-project-page
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</title>
<link>https://arxiv.org/abs/2512.01885</link>
<guid>https://arxiv.org/abs/2512.01885</guid>
<content:encoded><![CDATA[
arXiv:2512.01885v1 Announce Type: new 
Abstract: Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM</title>
<link>https://arxiv.org/abs/2512.01889</link>
<guid>https://arxiv.org/abs/2512.01889</guid>
<content:encoded><![CDATA[
arXiv:2512.01889v1 Announce Type: new 
Abstract: We present KM-ViPE (Knowledge Mapping Video Pose Engine), a real-time open-vocabulary SLAM framework for uncalibrated monocular cameras in dynamic environments. Unlike systems requiring depth sensors and offline calibration, KM-ViPE operates directly on raw RGB streams, making it ideal for ego-centric applications and harvesting internet-scale video data for training. KM-ViPE tightly couples DINO visual features with geometric constraints through a high-level features based adaptive robust kernel that handles both moving objects and movable static objects (e.g., moving furniture in ego-centric views). The system performs simultaneous online localization and open-vocabulary semantic mapping by fusing geometric and deep visual features aligned with language embeddings. Our results are competitive with state-of-the-art approaches, while existing solutions either operate offline, need depth data and/or odometry estimation, or lack dynamic scene robustness. KM-ViPE benefits from internet-scale training and uniquely combines online operation, uncalibrated monocular input, and robust handling of dynamic scenes, which makes it a good fit for autonomous robotics and AR/VR applications and advances practical spatial intelligence capabilities for embodied AI.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StyleYourSmile: Cross-Domain Face Retargeting Without Paired Multi-Style Data</title>
<link>https://arxiv.org/abs/2512.01895</link>
<guid>https://arxiv.org/abs/2512.01895</guid>
<content:encoded><![CDATA[
arXiv:2512.01895v1 Announce Type: new 
Abstract: Cross-domain face retargeting requires disentangled control over identity, expressions, and domain-specific stylistic attributes. Existing methods, typically trained on real-world faces, either fail to generalize across domains, need test-time optimizations, or require fine-tuning with carefully curated multi-style datasets to achieve domain-invariant identity representations. In this work, we introduce \textit{StyleYourSmile}, a novel one-shot cross-domain face retargeting method that eliminates the need for curated multi-style paired data. We propose an efficient data augmentation strategy alongside a dual-encoder framework, for extracting domain-invariant identity cues and capturing domain-specific stylistic variations. Leveraging these disentangled control signals, we condition a diffusion model to retarget facial expressions across domains. Extensive experiments demonstrate that \textit{StyleYourSmile} achieves superior identity preservation and retargeting fidelity across a wide range of visual domains.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception</title>
<link>https://arxiv.org/abs/2512.01908</link>
<guid>https://arxiv.org/abs/2512.01908</guid>
<content:encoded><![CDATA[
arXiv:2512.01908v1 Announce Type: new 
Abstract: Contact-rich robotic manipulation requires representations that encode local geometry. Vision provides global context but lacks direct measurements of properties such as texture and hardness, whereas touch supplies these cues. Modern visuo-tactile sensors capture both modalities in a single fused image, yielding intrinsically aligned inputs that are well suited to manipulation tasks requiring visual and tactile information. Most self-supervised learning (SSL) frameworks, however, compress feature maps into a global vector, discarding spatial structure and misaligning with the needs of manipulation. To address this, we propose SARL, a spatially-aware SSL framework that augments the Bootstrap Your Own Latent (BYOL) architecture with three map-level objectives, including Saliency Alignment (SAL), Patch-Prototype Distribution Alignment (PPDA), and Region Affinity Matching (RAM), to keep attentional focus, part composition, and geometric relations consistent across views. These losses act on intermediate feature maps, complementing the global objective. SARL consistently outperforms nine SSL baselines across six downstream tasks with fused visual-tactile data. On the geometry-sensitive edge-pose regression task, SARL achieves a Mean Absolute Error (MAE) of 0.3955, a 30% relative improvement over the next-best SSL method (0.5682 MAE) and approaching the supervised upper bound. These findings indicate that, for fused visual-tactile data, the most effective signal is structured spatial equivariance, in which features vary predictably with object geometry, which enables more capable robotic perception.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding</title>
<link>https://arxiv.org/abs/2512.01922</link>
<guid>https://arxiv.org/abs/2512.01922</guid>
<content:encoded><![CDATA[
arXiv:2512.01922v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13\% and improves hallucination accuracy by 6\% relative to baseline medical LVLMs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory</title>
<link>https://arxiv.org/abs/2512.01934</link>
<guid>https://arxiv.org/abs/2512.01934</guid>
<content:encoded><![CDATA[
arXiv:2512.01934v1 Announce Type: new 
Abstract: Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.01949</link>
<guid>https://arxiv.org/abs/2512.01949</guid>
<content:encoded><![CDATA[
arXiv:2512.01949v1 Announce Type: new 
Abstract: The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment</title>
<link>https://arxiv.org/abs/2512.01952</link>
<guid>https://arxiv.org/abs/2512.01952</guid>
<content:encoded><![CDATA[
arXiv:2512.01952v1 Announce Type: new 
Abstract: Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation</title>
<link>https://arxiv.org/abs/2512.01960</link>
<guid>https://arxiv.org/abs/2512.01960</guid>
<content:encoded><![CDATA[
arXiv:2512.01960v1 Announce Type: new 
Abstract: Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning</title>
<link>https://arxiv.org/abs/2512.01975</link>
<guid>https://arxiv.org/abs/2512.01975</guid>
<content:encoded><![CDATA[
arXiv:2512.01975v1 Announce Type: new 
Abstract: Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artemis: Structured Visual Reasoning for Perception Policy Learning</title>
<link>https://arxiv.org/abs/2512.01988</link>
<guid>https://arxiv.org/abs/2512.01988</guid>
<content:encoded><![CDATA[
arXiv:2512.01988v1 Announce Type: new 
Abstract: Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAI-Bench: A Comprehensive Benchmark For Physical AI</title>
<link>https://arxiv.org/abs/2512.01989</link>
<guid>https://arxiv.org/abs/2512.01989</guid>
<content:encoded><![CDATA[
arXiv:2512.01989v1 Announce Type: new 
Abstract: Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Visual Affordance from Audio</title>
<link>https://arxiv.org/abs/2512.02005</link>
<guid>https://arxiv.org/abs/2512.02005</guid>
<content:encoded><![CDATA[
arXiv:2512.02005v1 Announce Type: new 
Abstract: We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MV-TAP: Tracking Any Point in Multi-View Videos</title>
<link>https://arxiv.org/abs/2512.02006</link>
<guid>https://arxiv.org/abs/2512.02006</guid>
<content:encoded><![CDATA[
arXiv:2512.02006v1 Announce Type: new 
Abstract: Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AirSim360: A Panoramic Simulation Platform within Drone View</title>
<link>https://arxiv.org/abs/2512.02009</link>
<guid>https://arxiv.org/abs/2512.02009</guid>
<content:encoded><![CDATA[
arXiv:2512.02009v1 Announce Type: new 
Abstract: The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Mean Flows: On the Challenges of Fastforward Generative Models</title>
<link>https://arxiv.org/abs/2512.02012</link>
<guid>https://arxiv.org/abs/2512.02012</guid>
<content:encoded><![CDATA[
arXiv:2512.02012v1 Announce Type: new 
Abstract: MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\textbf{improved MeanFlow}$ ($\textbf{iMF}$) method, trained entirely from scratch, achieves $\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2512.02014</link>
<guid>https://arxiv.org/abs/2512.02014</guid>
<content:encoded><![CDATA[
arXiv:2512.02014v1 Announce Type: new 
Abstract: Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Video Motion Editing with 3D Point Tracks</title>
<link>https://arxiv.org/abs/2512.02015</link>
<guid>https://arxiv.org/abs/2512.02015</guid>
<content:encoded><![CDATA[
arXiv:2512.02015v1 Announce Type: new 
Abstract: Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now</title>
<link>https://arxiv.org/abs/2512.02016</link>
<guid>https://arxiv.org/abs/2512.02016</guid>
<content:encoded><![CDATA[
arXiv:2512.02016v1 Announce Type: new 
Abstract: Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\mathrm{eff}}$ from $1.81\,\mathrm{m/s^2}$ to $6.43\,\mathrm{m/s^2}$ (reaching $65\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion</title>
<link>https://arxiv.org/abs/2512.02017</link>
<guid>https://arxiv.org/abs/2512.02017</guid>
<content:encoded><![CDATA[
arXiv:2512.02017v1 Announce Type: new 
Abstract: Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Centric Visual Development for Self-Driving Labs</title>
<link>https://arxiv.org/abs/2512.02018</link>
<guid>https://arxiv.org/abs/2512.02018</guid>
<content:encoded><![CDATA[
arXiv:2512.02018v1 Announce Type: new 
Abstract: Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Survey on Surgical Digital Twin</title>
<link>https://arxiv.org/abs/2512.00019</link>
<guid>https://arxiv.org/abs/2512.00019</guid>
<content:encoded><![CDATA[
arXiv:2512.00019v1 Announce Type: cross 
Abstract: With the accelerating availability of multimodal surgical data and real-time computation, Surgical Digital Twins (SDTs) have emerged as virtual counterparts that mirror, predict, and inform decisions across pre-, intra-, and postoperative care. Despite promising demonstrations, SDTs face persistent challenges: fusing heterogeneous imaging, kinematics, and physiology under strict latency budgets; balancing model fidelity with computational efficiency; ensuring robustness, interpretability, and calibrated uncertainty; and achieving interoperability, privacy, and regulatory compliance in clinical environments. This survey offers a critical, structured review of SDTs. We clarify terminology and scope, propose a taxonomy by purpose, model fidelity, and data sources, and synthesize state-of-the-art achievements in deformable registration and tracking, real-time simulation and co-simulation, AR/VR guidance, edge-cloud orchestration, and AI for scene understanding and prediction. We contrast non-robotic twins with robot-in-the-loop architectures for shared control and autonomy, and identify open problems in validation and benchmarking, safety assurance and human factors, lifecycle "digital thread" integration, and scalable data governance. We conclude with a research agenda toward trustworthy, standards-aligned SDTs that deliver measurable clinical benefit. By unifying vocabulary, organizing capabilities, and highlighting gaps, this work aims to guide SDT design and deployment and catalyze translation from laboratory prototypes to routine surgical care.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Models for Trajectory Planning in Autonomous Driving: A Review of Progress and Open Challenges</title>
<link>https://arxiv.org/abs/2512.00021</link>
<guid>https://arxiv.org/abs/2512.00021</guid>
<content:encoded><![CDATA[
arXiv:2512.00021v1 Announce Type: cross 
Abstract: The emergence of multi-modal foundation models has markedly transformed the technology for autonomous driving, shifting away from conventional and mostly hand-crafted design choices towards unified, foundation-model-based approaches, capable of directly inferring motion trajectories from raw sensory inputs. This new class of methods can also incorporate natural language as an additional modality, with Vision-Language-Action (VLA) models serving as a representative example. In this review, we provide a comprehensive examination of such methods through a unifying taxonomy to critically evaluate their architectural design choices, methodological strengths, and their inherent capabilities and limitations. Our survey covers 37 recently proposed approaches that span the landscape of trajectory planning with foundation models. Furthermore, we assess these approaches with respect to the openness of their source code and datasets, offering valuable information to practitioners and researchers. We provide an accompanying webpage that catalogs the methods based on our taxonomy, available at: https://github.com/fiveai/FMs-for-driving-trajectories
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from Watching: Scalable Extraction of Manipulation Trajectories from Human Videos</title>
<link>https://arxiv.org/abs/2512.00024</link>
<guid>https://arxiv.org/abs/2512.00024</guid>
<content:encoded><![CDATA[
arXiv:2512.00024v1 Announce Type: cross 
Abstract: Collecting high-quality data for training large-scale robotic models typically relies on real robot platforms, which is labor-intensive and costly, whether via teleoperation or scripted demonstrations. To scale data collection, many researchers have turned to leveraging human manipulation videos available online. However, current methods predominantly focus on hand detection or object pose estimation, failing to fully exploit the rich interaction cues embedded in these videos. In this work, we propose a novel approach that combines large foundation models for video understanding with point tracking techniques to extract dense trajectories of all task-relevant keypoints during manipulation. This enables more comprehensive utilization of Internet-scale human demonstration videos. Experimental results demonstrate that our method can accurately track keypoints throughout the entire manipulation process, paving the way for more scalable and data-efficient robot learning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Improving Human Robot Collaboration through Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2512.00027</link>
<guid>https://arxiv.org/abs/2512.00027</guid>
<content:encoded><![CDATA[
arXiv:2512.00027v1 Announce Type: cross 
Abstract: Vision-and-Language Navigation (VLN) is a multi-modal, cooperative task requiring agents to interpret human instructions, navigate 3D environments, and communicate effectively under ambiguity. This paper presents a comprehensive review of recent VLN advancements in robotics and outlines promising directions to improve multi-robot coordination. Despite progress, current models struggle with bidirectional communication, ambiguity resolution, and collaborative decision-making in the multi-agent systems. We review approximately 200 relevant articles to provide an in-depth understanding of the current landscape. Through this survey, we aim to provide a thorough resource that inspires further research at the intersection of VLN and robotics. We advocate that the future VLN systems should support proactive clarification, real-time feedback, and contextual reasoning through advanced natural language understanding (NLU) techniques. Additionally, decentralized decision-making frameworks with dynamic role assignment are essential for scalable, efficient multi-robot collaboration. These innovations can significantly enhance human-robot interaction (HRI) and enable real-world deployment in domains such as healthcare, logistics, and disaster response.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICD-Net: Inertial Covariance Displacement Network for Drone Visual-Inertial SLAM</title>
<link>https://arxiv.org/abs/2512.00037</link>
<guid>https://arxiv.org/abs/2512.00037</guid>
<content:encoded><![CDATA[
arXiv:2512.00037v1 Announce Type: cross 
Abstract: Visual-inertial SLAM systems often exhibit suboptimal performance due to multiple confounding factors including imperfect sensor calibration, noisy measurements, rapid motion dynamics, low illumination, and the inherent limitations of traditional inertial navigation integration methods. These issues are particularly problematic in drone applications where robust and accurate state estimation is critical for safe autonomous operation. In this work, we present ICD-Net, a novel framework that enhances visual-inertial SLAM performance by learning to process raw inertial measurements and generating displacement estimates with associated uncertainty quantification. Rather than relying on analytical inertial sensor models that struggle with real-world sensor imperfections, our method directly extracts displacement maps from sensor data while simultaneously predicting measurement covariances that reflect estimation confidence. We integrate ICD-Net outputs as additional residual constraints into the VINS-Fusion optimization framework, where the predicted uncertainties appropriately weight the neural network contributions relative to traditional visual and inertial terms. The learned displacement constraints provide complementary information that compensates for various error sources in the SLAM pipeline. Our approach can be used under both normal operating conditions and in situations of camera inconsistency or visual degradation. Experimental evaluation on challenging high-speed drone sequences demonstrated that our approach significantly improved trajectory estimation accuracy compared to standard VINS-Fusion, with more than 38% improvement in mean APE and uncertainty estimates proving crucial for maintaining system robustness. Our method shows that neural network enhancement can effectively address multiple sources of SLAM degradation while maintaining real-time performance requirements.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VISTAv2: World Imagination for Indoor Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2512.00041</link>
<guid>https://arxiv.org/abs/2512.00041</guid>
<content:encoded><![CDATA[
arXiv:2512.00041v1 Announce Type: cross 
Abstract: Vision-and-Language Navigation (VLN) requires agents to follow language instructions while acting in continuous real-world spaces. Prior image imagination based VLN work shows benefits for discrete panoramas but lacks online, action-conditioned predictions and does not produce explicit planning values; moreover, many methods replace the planner with long-horizon objectives that are brittle and slow. To bridge this gap, we propose VISTAv2, a generative world model that rolls out egocentric future views conditioned on past observations, candidate action sequences, and instructions, and projects them into an online value map for planning. Unlike prior approaches, VISTAv2 does not replace the planner. The online value map is fused at score level with the base objective, providing reachability and risk-aware guidance. Concretely, we employ an action-aware Conditional Diffusion Transformer video predictor to synthesize short-horizon futures, align them with the natural language instruction via a vision-language scorer, and fuse multiple rollouts in a differentiable imagination-to-value head to output an imagined egocentric value map. For efficiency, rollouts occur in VAE latent space with a distilled sampler and sparse decoding, enabling inference on a single consumer GPU. Evaluated on MP3D and RoboTHOR, VISTAv2 improves over strong baselines, and ablations show that action-conditioned imagination, instruction-guided value fusion, and the online value-map planner are all critical, suggesting that VISTAv2 offers a practical and interpretable route to robust VLN.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coarse-to-Fine Non-Rigid Registration for Side-Scan Sonar Mosaicking</title>
<link>https://arxiv.org/abs/2512.00052</link>
<guid>https://arxiv.org/abs/2512.00052</guid>
<content:encoded><![CDATA[
arXiv:2512.00052v1 Announce Type: cross 
Abstract: Side-scan sonar mosaicking plays a crucial role in large-scale seabed mapping but is challenged by complex non-linear, spatially varying distortions due to diverse sonar acquisition conditions. Existing rigid or affine registration methods fail to model such complex deformations, whereas traditional non-rigid techniques tend to overfit and lack robustness in sparse-texture sonar data. To address these challenges, we propose a coarse-to-fine hierarchical non-rigid registration framework tailored for large-scale side-scan sonar images. Our method begins with a global Thin Plate Spline initialization from sparse correspondences, followed by superpixel-guided segmentation that partitions the image into structurally consistent patches preserving terrain integrity. Each patch is then refined by a pretrained SynthMorph network in an unsupervised manner, enabling dense and flexible alignment without task-specific training. Finally, a fusion strategy integrates both global and local deformations into a smooth, unified deformation field. Extensive quantitative and visual evaluations demonstrate that our approach significantly outperforms state-of-the-art rigid, classical non-rigid, and learning-based methods in accuracy, structural consistency, and deformation smoothness on the challenging sonar dataset.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bootstrap Dynamic-Aware 3D Visual Representation for Scalable Robot Learning</title>
<link>https://arxiv.org/abs/2512.00074</link>
<guid>https://arxiv.org/abs/2512.00074</guid>
<content:encoded><![CDATA[
arXiv:2512.00074v1 Announce Type: cross 
Abstract: Despite strong results on recognition and segmentation, current 3D visual pre-training methods often underperform on robotic manipulation. We attribute this gap to two factors: the lack of state-action-state dynamics modeling and the unnecessary redundancy of explicit geometric reconstruction. We introduce AFRO, a self-supervised framework that learns dynamics-aware 3D representations without action or reconstruction supervision. AFRO casts state prediction as a generative diffusion process and jointly models forward and inverse dynamics in a shared latent space to capture causal transition structure. To prevent feature leakage in action learning, we employ feature differencing and inverse-consistency supervision, improving the quality and stability of visual features. When combined with Diffusion Policy, AFRO substantially increases manipulation success rates across 16 simulated and 4 real-world tasks, outperforming existing pre-training approaches. The framework also scales favorably with data volume and task complexity. Qualitative visualizations indicate that AFRO learns semantically rich, discriminative features, offering an effective pre-training solution for 3D representation learning in robotics. Project page: https://kolakivy.github.io/AFRO/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arcadia: Toward a Full-Lifecycle Framework for Embodied Lifelong Learning</title>
<link>https://arxiv.org/abs/2512.00076</link>
<guid>https://arxiv.org/abs/2512.00076</guid>
<content:encoded><![CDATA[
arXiv:2512.00076v1 Announce Type: cross 
Abstract: We contend that embodied learning is fundamentally a lifecycle problem rather than a single-stage optimization. Systems that optimize only one link (data collection, simulation, learning, or deployment) rarely sustain improvement or generalize beyond narrow settings. We introduce Arcadia, a closed-loop framework that operationalizes embodied lifelong learning by tightly coupling four stages: (1) Self-evolving exploration and grounding for autonomous data acquisition in physical environments, (2) Generative scene reconstruction and augmentation for realistic and extensible scene creation, (3) a Shared embodied representation architecture that unifies navigation and manipulation within a single multimodal backbone, and (4) Sim-from-real evaluation and evolution that closes the feedback loop through simulation-based adaptation. This coupling is non-decomposable: removing any stage breaks the improvement loop and reverts to one-shot training. Arcadia delivers consistent gains on navigation and manipulation benchmarks and transfers robustly to physical robots, indicating that a tightly coupled lifecycle: continuous real-world data acquisition, generative simulation update, and shared-representation learning, supports lifelong improvement and end-to-end generalization. We release standardized interfaces enabling reproducible evaluation and cross-model comparison in reusable environments, positioning Arcadia as a scalable foundation for general-purpose embodied agents.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HMARK: Radioactive Multi-Bit Semantic-Latent Watermarking for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.00094</link>
<guid>https://arxiv.org/abs/2512.00094</guid>
<content:encoded><![CDATA[
arXiv:2512.00094v1 Announce Type: cross 
Abstract: Modern generative diffusion models rely on vast training datasets, often including images with uncertain ownership or usage rights. Radioactive watermarks -- marks that transfer to a model's outputs -- can help detect when such unauthorized data has been used for training. Moreover, aside from being radioactive, an effective watermark for protecting images from unauthorized training also needs to meet other existing requirements, such as imperceptibility, robustness, and multi-bit capacity. To overcome these challenges, we propose HMARK, a novel multi-bit watermarking scheme, which encodes ownership information as secret bits in the semantic-latent space (h-space) for image diffusion models. By leveraging the interpretability and semantic significance of h-space, ensuring that watermark signals correspond to meaningful semantic attributes, the watermarks embedded by HMARK exhibit radioactivity, robustness to distortions, and minimal impact on perceptual quality. Experimental results demonstrate that HMARK achieves 98.57% watermark detection accuracy, 95.07% bit-level recovery accuracy, 100% recall rate, and 1.0 AUC on images produced by the downstream adversarial model finetuned with LoRA on watermarked data across various types of distortions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoLT: Mixture of Layer-Wise Tokens for Efficient Audio-Visual Learning</title>
<link>https://arxiv.org/abs/2512.00115</link>
<guid>https://arxiv.org/abs/2512.00115</guid>
<content:encoded><![CDATA[
arXiv:2512.00115v1 Announce Type: cross 
Abstract: In this paper, we propose Mixture of Layer-Wise Tokens (MoLT), a parameter- and memory-efficient adaptation framework for audio-visual learning. The key idea of MoLT is to replace conventional, computationally heavy sequential adaptation at every transformer layer with a parallel, lightweight scheme that extracts and fuses layer-wise tokens only from the late layers. We adopt two types of adapters to distill modality-specific information and cross-modal interaction into compact latent tokens in a layer-wise manner. A token fusion module then dynamically fuses these layer-wise tokens by taking into account their relative significance. To prevent the redundancy of latent tokens, we apply an orthogonality regularization between latent tokens during training. Through the systematic analysis of the position of adaptation in the pre-trained transformers, we extract latent tokens only from the late layers of the transformers. This strategic adaptation approach avoids error propagation from the volatile early-layer features, thereby maximizing the adaptation performance while maintaining parameter and memory efficiency. Through extensive experiments, we demonstrate that MoLT outperforms existing methods on diverse audio-visual benchmarks, including Audio-Visual Question Answering, Audio-Visual Segmentation, and Audio-Visual Event Localization.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Art2Music: Generating Music for Art Images with Multi-modal Feeling Alignment</title>
<link>https://arxiv.org/abs/2512.00120</link>
<guid>https://arxiv.org/abs/2512.00120</guid>
<content:encoded><![CDATA[
arXiv:2512.00120v1 Announce Type: cross 
Abstract: With the rise of AI-generated content (AIGC), generating perceptually natural and feeling-aligned music from multimodal inputs has become a central challenge. Existing approaches often rely on explicit emotion labels that require costly annotation, underscoring the need for more flexible feeling-aligned methods. To support multimodal music generation, we construct ArtiCaps, a pseudo feeling-aligned image-music-text dataset created by semantically matching descriptions from ArtEmis and MusicCaps. We further propose Art2Music, a lightweight cross-modal framework that synthesizes music from artistic images and user comments. In the first stage, images and text are encoded with OpenCLIP and fused using a gated residual module; the fused representation is decoded by a bidirectional LSTM into Mel-spectrograms with a frequency-weighted L1 loss to enhance high-frequency fidelity. In the second stage, a fine-tuned HiFi-GAN vocoder reconstructs high-quality audio waveforms. Experiments on ArtiCaps show clear improvements in Mel-Cepstral Distortion, Frechet Audio Distance, Log-Spectral Distance, and cosine similarity. A small LLM-based rating study further verifies consistent cross-modal feeling alignment and offers interpretable explanations of matches and mismatches across modalities. These results demonstrate improved perceptual naturalness, spectral fidelity, and semantic consistency. Art2Music also maintains robust performance with only 50k training samples, providing a scalable solution for feeling-aligned creative audio generation in interactive art, personalized soundscapes, and digital art exhibitions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ternary-Input Binary-Weight CNN Accelerator Design for Miniature Object Classification System with Query-Driven Spatial DVS</title>
<link>https://arxiv.org/abs/2512.00138</link>
<guid>https://arxiv.org/abs/2512.00138</guid>
<content:encoded><![CDATA[
arXiv:2512.00138v1 Announce Type: cross 
Abstract: Miniature imaging systems are essential for space-constrained applications but are limited by memory and power constraints. While machine learning can reduce data size by extracting key features, its high energy demands often exceed the capacity of small batteries. This paper presents a CNN hardware accelerator optimized for object classification in miniature imaging systems. It processes data from a spatial Dynamic Vision Sensor (DVS), reconfigurable to a temporal DVS via pixel sharing, minimizing sensor area. By using ternary DVS outputs and a ternary-input, binary-weight neural network, the design reduces computation and memory needs. Fabricated in 28 nm CMOS, the accelerator cuts data size by 81% and MAC operations by 27%. It achieves 440 ms inference time at just 1.6 mW power consumption, improving the Figure-of-Merit (FoM) by 7.3x over prior CNN accelerators for miniature systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2512.00229</link>
<guid>https://arxiv.org/abs/2512.00229</guid>
<content:encoded><![CDATA[
arXiv:2512.00229v1 Announce Type: cross 
Abstract: Deep neural networks often struggle to recognize when an input lies outside their training experience, leading to unreliable and overconfident predictions. Building dependable machine learning systems therefore requires methods that can both estimate predictive \textit{uncertainty} and detect \textit{out-of-distribution (OOD)} samples in a unified manner. In this paper, we propose \textbf{TIE: a Training--Inversion--Exclusion} framework for visually interpretable and uncertainty-guided anomaly detection that jointly addresses these challenges through iterative refinement. TIE extends a standard $n$-class classifier to an $(n+1)$-class model by introducing a garbage class initialized with Gaussian noise to represent outlier inputs. Within each epoch, TIE performs a closed-loop process of \textit{training, inversion, and exclusion}, where highly uncertain inverted samples reconstructed from the just-trained classifier are excluded into the garbage class. Over successive iterations, the inverted samples transition from noisy artifacts into visually coherent class prototypes, providing transparent insight into how the model organizes its learned manifolds. During inference, TIE rejects OOD inputs by either directly mapping them to the garbage class or producing low-confidence, uncertain misclassifications within the in-distribution classes that are easily separable, all without relying on external OOD datasets. A comprehensive threshold-based evaluation using multiple OOD metrics and performance measures such as \textit{AUROC}, \textit{AUPR}, and \textit{FPR@95\%TPR} demonstrates that TIE offers a unified and interpretable framework for robust anomaly detection and calibrated uncertainty estimation (UE) achieving near-perfect OOD detection with \textbf{\(\!\approx\!\) 0 FPR@95\%TPR} when trained on MNIST or FashionMNIST and tested against diverse unseen datasets.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealAppliance: Let High-fidelity Appliance Assets Controllable and Workable as Aligned Real Manuals</title>
<link>https://arxiv.org/abs/2512.00287</link>
<guid>https://arxiv.org/abs/2512.00287</guid>
<content:encoded><![CDATA[
arXiv:2512.00287v1 Announce Type: cross 
Abstract: Existing appliance assets suffer from poor rendering, incomplete mechanisms, and misalignment with manuals, leading to simulation-reality gaps that hinder appliance manipulation development. In this work, we introduce the RealAppliance dataset, comprising 100 high-fidelity appliances with complete physical, electronic mechanisms, and program logic aligned with their manuals. Based on these assets, we propose the RealAppliance-Bench benchmark, which evaluates multimodal large language models and embodied manipulation planning models across key tasks in appliance manipulation planning: manual page retrieval, appliance part grounding, open-loop manipulation planning, and closed-loop planning adjustment. Our analysis of model performances on RealAppliance-Bench provides insights for advancing appliance manipulation research
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2512.00324</link>
<guid>https://arxiv.org/abs/2512.00324</guid>
<content:encoded><![CDATA[
arXiv:2512.00324v1 Announce Type: cross 
Abstract: Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedCondDiff: Lightweight, Robust, Semantically Guided Diffusion for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.00350</link>
<guid>https://arxiv.org/abs/2512.00350</guid>
<content:encoded><![CDATA[
arXiv:2512.00350v1 Announce Type: cross 
Abstract: We introduce MedCondDiff, a diffusion-based framework for multi-organ medical image segmentation that is efficient and anatomically grounded. The model conditions the denoising process on semantic priors extracted by a Pyramid Vision Transformer (PVT) backbone, yielding a semantically guided and lightweight diffusion architecture. This design improves robustness while reducing both inference time and VRAM usage compared to conventional diffusion models. Experiments on multi-organ, multi-modality datasets demonstrate that MedCondDiff delivers competitive performance across anatomical regions and imaging modalities, underscoring the potential of semantically guided diffusion models as an effective class of architectures for medical imaging tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement</title>
<link>https://arxiv.org/abs/2512.00396</link>
<guid>https://arxiv.org/abs/2512.00396</guid>
<content:encoded><![CDATA[
arXiv:2512.00396v1 Announce Type: cross 
Abstract: We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SelfAI: Building a Self-Training AI System with LLM Agents</title>
<link>https://arxiv.org/abs/2512.00403</link>
<guid>https://arxiv.org/abs/2512.00403</guid>
<content:encoded><![CDATA[
arXiv:2512.00403v1 Announce Type: cross 
Abstract: Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast, Robust, Permutation-and-Sign Invariant SO(3) Pattern Alignment</title>
<link>https://arxiv.org/abs/2512.00659</link>
<guid>https://arxiv.org/abs/2512.00659</guid>
<content:encoded><![CDATA[
arXiv:2512.00659v1 Announce Type: cross 
Abstract: We address the correspondence-free alignment of two rotation sets on \(SO(3)\), a core task in calibration and registration that is often impeded by missing time alignment, outliers, and unknown axis conventions. Our key idea is to decompose each rotation into its \emph{Transformed Basis Vectors} (TBVs)-three unit vectors on \(S^2\)-and align the resulting spherical point sets per axis using fast, robust matchers (SPMC, FRS, and a hybrid). To handle axis relabels and sign flips, we introduce a \emph{Permutation-and-Sign Invariant} (PASI) wrapper that enumerates the 24 proper signed permutations, scores them via summed correlations, and fuses the per-axis estimates into a single rotation by projection/Karcher mean. The overall complexity remains linear in the number of rotations (\(\mathcal{O}(n)\)), contrasting with \(\mathcal{O}(N_r^3\log N_r)\) for spherical/\(SO(3)\) correlation. Experiments on EuRoC Machine Hall simulations
  (axis-consistent) and the ETH Hand-Eye benchmark (\texttt{robot\_arm\_real})
  (axis-ambiguous) show that our methods are accurate, 6-60x faster than traditional methods, and robust under extreme outlier ratios (up to 90\%), all without correspondence search.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories</title>
<link>https://arxiv.org/abs/2512.00736</link>
<guid>https://arxiv.org/abs/2512.00736</guid>
<content:encoded><![CDATA[
arXiv:2512.00736v1 Announce Type: cross 
Abstract: Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sign Language Recognition using Bidirectional Reservoir Computing</title>
<link>https://arxiv.org/abs/2512.00777</link>
<guid>https://arxiv.org/abs/2512.00777</guid>
<content:encoded><![CDATA[
arXiv:2512.00777v1 Announce Type: cross 
Abstract: Sign language recognition (SLR) facilitates communication between deaf and hearing individuals. Deep learning is widely used to develop SLR-based systems; however, it is computationally intensive and requires substantial computational resources, making it unsuitable for resource-constrained devices. To address this, we propose an efficient sign language recognition system using MediaPipe and an echo state network (ESN)-based bidirectional reservoir computing (BRC) architecture. MediaPipe extracts hand joint coordinates, which serve as inputs to the ESN-based BRC architecture. The BRC processes these features in both forward and backward directions, efficiently capturing temporal dependencies. The resulting states of BRC are concatenated to form a robust representation for classification. We evaluated our method on the Word-Level American Sign Language (WLASL) video dataset, achieving a competitive accuracy of 57.71% and a significantly lower training time of only 9 seconds, in contrast to the 55 minutes and $38$ seconds required by the deep learning-based Bi-GRU approach. Consequently, the BRC-based SLR system is well-suited for edge devices.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.00818</link>
<guid>https://arxiv.org/abs/2512.00818</guid>
<content:encoded><![CDATA[
arXiv:2512.00818v1 Announce Type: cross 
Abstract: MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Audio-Visual World Models: Towards Multisensory Imagination in Sight and Sound</title>
<link>https://arxiv.org/abs/2512.00883</link>
<guid>https://arxiv.org/abs/2512.00883</guid>
<content:encoded><![CDATA[
arXiv:2512.00883v1 Announce Type: cross 
Abstract: World models simulate environmental dynamics to enable agents to plan and reason about future states. While existing approaches have primarily focused on visual observations, real-world perception inherently involves multiple sensory modalities. Audio provides crucial spatial and temporal cues such as sound source localization and acoustic scene properties, yet its integration into world models remains largely unexplored. No prior work has formally defined what constitutes an audio-visual world model or how to jointly capture binaural spatial audio and visual dynamics under precise action control with task reward prediction. This work presents the first formal framework for Audio-Visual World Models (AVWM), formulating multimodal environment simulation as a partially observable Markov decision process with synchronized audio-visual observations, fine-grained actions, and task rewards. To address the lack of suitable training data, we construct AVW-4k, a dataset comprising 30 hours of binaural audio-visual trajectories with action annotations and reward signals across 76 indoor environments. We propose AV-CDiT, an Audio-Visual Conditional Diffusion Transformer with a novel modality expert architecture that balances visual and auditory learning, optimized through a three-stage training strategy for effective multimodal integration. Extensive experiments demonstrate that AV-CDiT achieves high-fidelity multimodal prediction across visual and auditory modalities with reward. Furthermore, we validate its practical utility in continuous audio-visual navigation tasks, where AVWM significantly enhances the agent's performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOM-Nav: Frontier-Object Maps for Object Goal Navigation</title>
<link>https://arxiv.org/abs/2512.01009</link>
<guid>https://arxiv.org/abs/2512.01009</guid>
<content:encoded><![CDATA[
arXiv:2512.01009v1 Announce Type: cross 
Abstract: This paper addresses the Object Goal Navigation problem, where a robot must efficiently find a target object in an unknown environment. Existing implicit memory-based methods struggle with long-term memory retention and planning, while explicit map-based approaches lack rich semantic information. To address these challenges, we propose FOM-Nav, a modular framework that enhances exploration efficiency through Frontier-Object Maps and vision-language models. Our Frontier-Object Maps are built online and jointly encode spatial frontiers and fine-grained object information. Using this representation, a vision-language model performs multimodal scene understanding and high-level goal prediction, which is executed by a low-level planner for efficient trajectory generation. To train FOM-Nav, we automatically construct large-scale navigation datasets from real-world scanned environments. Extensive experiments validate the effectiveness of our model design and constructed dataset. FOM-Nav achieves state-of-the-art performance on the MP3D and HM3D benchmarks, particularly in navigation efficiency metric SPL, and yields promising results on a real robot.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer</title>
<link>https://arxiv.org/abs/2512.01061</link>
<guid>https://arxiv.org/abs/2512.01061</guid>
<content:encoded><![CDATA[
arXiv:2512.01061v1 Announce Type: cross 
Abstract: Recent progress in GPU-accelerated, photorealistic simulation has opened a scalable data-generation path for robot learning, where massive physics and visual randomization allow policies to generalize beyond curated environments. Building on these advances, we develop a teacher-student-bootstrap learning framework for vision-based humanoid loco-manipulation, using articulated-object interaction as a representative high-difficulty benchmark. Our approach introduces a staged-reset exploration strategy that stabilizes long-horizon privileged-policy training, and a GRPO-based fine-tuning procedure that mitigates partial observability and improves closed-loop consistency in sim-to-real RL. Trained entirely on simulation data, the resulting policy achieves robust zero-shot performance across diverse door types and outperforms human teleoperators by up to 31.7% in task completion time under the same whole-body control stack. This represents the first humanoid sim-to-real policy capable of diverse articulated loco-manipulation using pure RGB perception.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimation of Kinematic Motion from Dashcam Footage</title>
<link>https://arxiv.org/abs/2512.01104</link>
<guid>https://arxiv.org/abs/2512.01104</guid>
<content:encoded><![CDATA[
arXiv:2512.01104v1 Announce Type: cross 
Abstract: The goal of this paper is to explore the accuracy of dashcam footage to predict the actual kinematic motion of a car-like vehicle. Our approach uses ground truth information from the vehicle's on-board data stream, through the controller area network, and a time-synchronized dashboard camera, mounted to a consumer-grade vehicle, for 18 hours of footage and driving. The contributions of the paper include neural network models that allow us to quantify the accuracy of predicting the vehicle speed and yaw, as well as the presence of a lead vehicle, and its relative distance and speed. In addition, the paper describes how other researchers can gather their own data to perform similar experiments, using open-source tools and off-the-shelf technology.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution</title>
<link>https://arxiv.org/abs/2512.01152</link>
<guid>https://arxiv.org/abs/2512.01152</guid>
<content:encoded><![CDATA[
arXiv:2512.01152v1 Announce Type: cross 
Abstract: As we deploy machine learning systems in the real world, a core challenge is to maintain a model that is performant even as the data shifts. Such shifts can take many forms: new classes may emerge that were absent during training, a problem known as open-set recognition, and the distribution of known categories may change. Guarantees on open-set recognition are mostly derived under the assumption that the distribution of known classes, which we call \emph{the background distribution}, is fixed. In this paper we develop \ours{}, a method that is guaranteed to solve open-set recognition even in the challenging case where the background distribution shifts. We prove that the method works under benign assumptions that the novel class is separable from the non-novel classes, and provide theoretical guarantees that it outperforms a representative baseline in a simplified overparameterized setting. We develop techniques to make \ours{} scalable and robust, and perform comprehensive empirical evaluations on image and text data. The results show that \ours{} significantly outperforms existing open-set recognition methods under background shift. Moreover, we provide new insights into how factors such as the size of the novel class influences performance, an aspect that has not been extensively explored in prior work.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First On-Orbit Demonstration of a Geospatial Foundation Model</title>
<link>https://arxiv.org/abs/2512.01181</link>
<guid>https://arxiv.org/abs/2512.01181</guid>
<content:encoded><![CDATA[
arXiv:2512.01181v1 Announce Type: cross 
Abstract: Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe</title>
<link>https://arxiv.org/abs/2512.01252</link>
<guid>https://arxiv.org/abs/2512.01252</guid>
<content:encoded><![CDATA[
arXiv:2512.01252v1 Announce Type: cross 
Abstract: Recent efforts on Diffusion Mixture-of-Experts (MoE) models have primarily focused on developing more sophisticated routing mechanisms. However, we observe that the underlying architectural configuration space remains markedly under-explored. Inspired by the MoE design paradigms established in large language models (LLMs), we identify a set of crucial architectural factors for building effective Diffusion MoE models--including DeepSeek-style expert modules, alternative intermediate widths, varying expert counts, and enhanced attention positional encodings. Our systematic study reveals that carefully tuning these configurations is essential for unlocking the full potential of Diffusion MoE models, often yielding gains that exceed those achieved by routing innovations alone. Through extensive experiments, we present novel architectures that can be efficiently applied to both latent and pixel-space diffusion frameworks, which provide a practical and efficient training recipe that enables Diffusion MoE models to surpass strong baselines while using equal or fewer activated parameters. All code and models are publicly available at: https://github.com/yhlleo/EfficientMoE.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Panda: Self-distillation of Reusable Sensor-level Representations for High Energy Physics</title>
<link>https://arxiv.org/abs/2512.01324</link>
<guid>https://arxiv.org/abs/2512.01324</guid>
<content:encoded><![CDATA[
arXiv:2512.01324v1 Announce Type: cross 
Abstract: Liquid argon time projection chambers (LArTPCs) provide dense, high-fidelity 3D measurements of particle interactions and underpin current and future neutrino and rare-event experiments. Physics reconstruction typically relies on complex detector-specific pipelines that use tens of hand-engineered pattern recognition algorithms or cascades of task-specific neural networks that require extensive, labeled simulation that requires a careful, time-consuming calibration process. We introduce \textbf{Panda}, a model that learns reusable sensor-level representations directly from raw unlabeled LArTPC data. Panda couples a hierarchical sparse 3D encoder with a multi-view, prototype-based self-distillation objective. On a simulated dataset, Panda substantially improves label efficiency and reconstruction quality, beating the previous state-of-the-art semantic segmentation model with 1,000$\times$ fewer labels. We also show that a single set-prediction head 1/20th the size of the backbone with no physical priors trained on frozen outputs from Panda can result in particle identification that is comparable with state-of-the-art (SOTA) reconstruction tools. Full fine-tuning further improves performance across all tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TagSplat: Topology-Aware Gaussian Splatting for Dynamic Mesh Modeling and Tracking</title>
<link>https://arxiv.org/abs/2512.01329</link>
<guid>https://arxiv.org/abs/2512.01329</guid>
<content:encoded><![CDATA[
arXiv:2512.01329v1 Announce Type: cross 
Abstract: Topology-consistent dynamic model sequences are essential for applications such as animation and model editing. However, existing 4D reconstruction methods face challenges in generating high-quality topology-consistent meshes. To address this, we propose a topology-aware dynamic reconstruction framework based on Gaussian Splatting. We introduce a Gaussian topological structure that explicitly encodes spatial connectivity. This structure enables topology-aware densification and pruning, preserving the manifold consistency of the Gaussian representation. Temporal regularization terms further ensure topological coherence over time, while differentiable mesh rasterization improves mesh quality. Experimental results demonstrate that our method reconstructs topology-consistent mesh sequences with significantly higher accuracy than existing approaches. Moreover, the resulting meshes enable precise 3D keypoint tracking. Project page: https://haza628.github.io/tagSplat/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stay Unique, Stay Efficient: Preserving Model Personality in Multi-Task Merging</title>
<link>https://arxiv.org/abs/2512.01461</link>
<guid>https://arxiv.org/abs/2512.01461</guid>
<content:encoded><![CDATA[
arXiv:2512.01461v1 Announce Type: cross 
Abstract: Model merging has emerged as a promising paradigm for enabling multi-task capabilities without additional training. However, existing methods often experience substantial performance degradation compared with individually fine-tuned models, even on similar tasks, underscoring the need to preserve task-specific information. This paper proposes Decomposition, Thresholding, and Scaling (DTS), an approximation-based personalized merging framework that preserves task-specific information with minimal storage overhead. DTS first applies singular value decomposition to the task-specific information and retains only a small subset of singular values and vectors. It then introduces a novel thresholding strategy that partitions singular vector elements into groups and assigns a scaling factor to each group. To enable generalization to unseen tasks, we further extend DTS with a variant that fuses task-specific information in a data-free manner based on the semantic similarity of task characteristics. Extensive experiments demonstrate that DTS consistently outperforms state-of-the-art baselines while requiring only 1\% additional storage per task. Furthermore, experiments on unseen tasks show that the DTS variant achieves significantly better generalization performance. Our code is available at https://github.com/krumpguo/DTS.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction</title>
<link>https://arxiv.org/abs/2512.01550</link>
<guid>https://arxiv.org/abs/2512.01550</guid>
<content:encoded><![CDATA[
arXiv:2512.01550v1 Announce Type: cross 
Abstract: Embodied navigation for long-horizon tasks, guided by complex natural language instructions, remains a formidable challenge in artificial intelligence. Existing agents often struggle with robust long-term planning about unseen environments, leading to high failure rates. To address these limitations, we introduce NavForesee, a novel Vision-Language Model (VLM) that unifies high-level language planning and predictive world model imagination within a single, unified framework. Our approach empowers a single VLM to concurrently perform planning and predictive foresight. Conditioned on the full instruction and historical observations, the model is trained to understand the navigation instructions by decomposing the task, tracking its progress, and formulating the subsequent sub-goal. Simultaneously, it functions as a generative world model, providing crucial foresight by predicting short-term environmental dynamics and long-term navigation milestones. The VLM's structured plan guides its targeted prediction, while the imagined future provides rich context to inform the navigation actions, creating a powerful internal feedback loop of perception-planning/prediction-action. We demonstrate through extensive experiments on the R2R-CE and RxR-CE benchmark that NavForesee achieves highly competitive performance in complex scenarios. Our work highlights the immense potential of fusing explicit language planning with implicit spatiotemporal prediction, paving the way for more intelligent and capable embodied agents.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Direct Encoding: Learnable Temporal Dynamics for Static Image Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2512.01687</link>
<guid>https://arxiv.org/abs/2512.01687</guid>
<content:encoded><![CDATA[
arXiv:2512.01687v1 Announce Type: cross 
Abstract: Handling static images that lack inherent temporal dynamics remains a fundamental challenge for spiking neural networks (SNNs). In directly trained SNNs, static inputs are typically repeated across time steps, causing the temporal dimension to collapse into a rate like representation and preventing meaningful temporal modeling. This work revisits the reported performance gap between direct and rate based encodings and shows that it primarily stems from convolutional learnability and surrogate gradient formulations rather than the encoding schemes themselves. To illustrate this mechanism level clarification, we introduce a minimal learnable temporal encoding that adds adaptive phase shifts to induce meaningful temporal variation from static inputs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning</title>
<link>https://arxiv.org/abs/2512.01818</link>
<guid>https://arxiv.org/abs/2512.01818</guid>
<content:encoded><![CDATA[
arXiv:2512.01818v1 Announce Type: cross 
Abstract: Deep neural networks suffer from catastrophic forgetting, where performance on previous tasks degrades after training on a new task. This issue arises due to the model's tendency to overwrite previously acquired knowledge with new information. We present a novel approach to address this challenge, focusing on the intersection of memory-based methods and regularization approaches. We formulate a regularization strategy, termed Information Maximization (IM) regularizer, for memory-based continual learning methods, which is based exclusively on the expected label distribution, thus making it class-agnostic. As a consequence, IM regularizer can be directly integrated into various rehearsal-based continual learning methods, reducing forgetting and favoring faster convergence. Our empirical validation shows that, across datasets and regardless of the number of tasks, our proposed regularization strategy consistently improves baseline performance at the expense of a minimal computational overhead. The lightweight nature of IM ensures that it remains a practical and scalable solution, making it applicable to real-world continual learning scenarios where efficiency is paramount. Finally, we demonstrate the data-agnostic nature of our regularizer by applying it to video data, which presents additional challenges due to its temporal structure and higher memory requirements. Despite the significant domain gap, our experiments show that IM regularizer also improves the performance of video continual learning methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InnoGym: Benchmarking the Innovation Potential of AI Agents</title>
<link>https://arxiv.org/abs/2512.01822</link>
<guid>https://arxiv.org/abs/2512.01822</guid>
<content:encoded><![CDATA[
arXiv:2512.01822v1 Announce Type: cross 
Abstract: LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangling Progress in Medical Image Registration: Beyond Trend-Driven Architectures towards Domain-Specific Strategies</title>
<link>https://arxiv.org/abs/2512.01913</link>
<guid>https://arxiv.org/abs/2512.01913</guid>
<content:encoded><![CDATA[
arXiv:2512.01913v1 Announce Type: cross 
Abstract: Medical image registration drives quantitative analysis across organs, modalities, and patient populations. Recent deep learning methods often combine low-level "trend-driven" computational blocks from computer vision, such as large-kernel CNNs, Transformers, and state-space models, with high-level registration-specific designs like motion pyramids, correlation layers, and iterative refinement. Yet, their relative contributions remain unclear and entangled. This raises a central question: should future advances in registration focus on importing generic architectural trends or on refining domain-specific design principles? Through a modular framework spanning brain, lung, cardiac, and abdominal registration, we systematically disentangle the influence of these two paradigms. Our evaluation reveals that low-level "trend-driven" computational blocks offer only marginal or inconsistent gains, while high-level registration-specific designs consistently deliver more accurate, smoother, and more robust deformations. These domain priors significantly elevate the performance of a standard U-Net baseline, far more than variants incorporating "trend-driven" blocks, achieving an average relative improvement of $\sim3\%$. All models and experiments are released within a transparent, modular benchmark that enables plug-and-play comparison for new architectures and registration tasks (https://github.com/BailiangJ/rethink-reg). This dynamic and extensible platform establishes a common ground for reproducible and fair evaluation, inviting the community to isolate genuine methodological contributions from domain priors. Our findings advocate a shift in research emphasis: from following architectural trends to embracing domain-specific design principles as the true drivers of progress in learning-based medical image registration.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.01946</link>
<guid>https://arxiv.org/abs/2512.01946</guid>
<content:encoded><![CDATA[
arXiv:2512.01946v1 Announce Type: cross 
Abstract: Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback</title>
<link>https://arxiv.org/abs/2512.01979</link>
<guid>https://arxiv.org/abs/2512.01979</guid>
<content:encoded><![CDATA[
arXiv:2512.01979v1 Announce Type: cross 
Abstract: GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies</title>
<link>https://arxiv.org/abs/2512.01993</link>
<guid>https://arxiv.org/abs/2512.01993</guid>
<content:encoded><![CDATA[
arXiv:2512.01993v1 Announce Type: cross 
Abstract: Autonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors. We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data. During rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning. This approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving. We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\% and reduces collisions by 54\%.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI</title>
<link>https://arxiv.org/abs/2512.02020</link>
<guid>https://arxiv.org/abs/2512.02020</guid>
<content:encoded><![CDATA[
arXiv:2512.02020v1 Announce Type: cross 
Abstract: Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial Privacy Protection</title>
<link>https://arxiv.org/abs/2305.13625</link>
<guid>https://arxiv.org/abs/2305.13625</guid>
<content:encoded><![CDATA[
arXiv:2305.13625v4 Announce Type: replace 
Abstract: The increasingly pervasive facial recognition (FR) systems raise serious concerns about personal privacy, especially for billions of users who have publicly shared their photos on social media. Several attempts have been made to protect individuals from being identified by unauthorized FR systems utilizing adversarial attacks to generate encrypted face images. However, existing methods suffer from poor visual quality or low attack success rates, which limit their utility. Recently, diffusion models have achieved tremendous success in image generation. In this work, we ask: can diffusion models be used to generate adversarial examples to improve both visual quality and attack performance? We propose DiffProtect, which utilizes a diffusion autoencoder to generate semantically meaningful perturbations on FR systems. Extensive experiments demonstrate that DiffProtect produces more natural-looking encrypted images than state-of-the-art methods while achieving significantly higher attack success rates, e.g., 24.5% and 25.1% absolute improvements on the CelebA-HQ and FFHQ datasets.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CraftSVG: Multi-Object Text-to-SVG Synthesis via Layout Guided Diffusion</title>
<link>https://arxiv.org/abs/2404.00412</link>
<guid>https://arxiv.org/abs/2404.00412</guid>
<content:encoded><![CDATA[
arXiv:2404.00412v3 Announce Type: replace 
Abstract: Generating VectorArt from text prompts is a challenging vision task, requiring diverse yet realistic depictions of the seen as well as unseen entities. However, existing research has been mostly limited to the generation of single objects, rather than comprehensive scenes comprising multiple elements. In response, this work introduces SVGCraft, a novel end-to-end framework for the creation of vector graphics depicting entire scenes from textual descriptions. Utilizing a pre-trained LLM for layout generation from text prompts, this framework introduces a technique for producing masked latents in specified bounding boxes for accurate object placement. It introduces a fusion mechanism for integrating attention maps and employs a diffusion U-Net for coherent composition, speeding up the drawing process. The resulting SVG is optimized using a pre-trained encoder and LPIPS loss with opacity modulation to maximize similarity. Additionally, this work explores the potential of primitive shapes in facilitating canvas completion in constrained environments. Through both qualitative and quantitative assessments, SVGCraft is demonstrated to surpass prior works in abstraction, recognizability, and detail, as evidenced by its performance metrics (CLIP-T: 0.4563, Cosine Similarity: 0.6342, Confusion: 0.66, Aesthetic: 6.7832). The code will be available at https://github.com/ayanban011/SVGCraft.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Generative Adversarial Networks for Color Document Image Enhancement and Binarization Using Multi-scale Feature Extraction</title>
<link>https://arxiv.org/abs/2407.04231</link>
<guid>https://arxiv.org/abs/2407.04231</guid>
<content:encoded><![CDATA[
arXiv:2407.04231v2 Announce Type: replace 
Abstract: The outcome of text recognition for degraded color documents is often unsatisfactory due to interference from various contaminants. To extract information more efficiently for text recognition, document image enhancement and binarization are often employed as preliminary steps in document analysis. Training independent generative adversarial networks (GANs) for each color channel can generate images where shadows and noise are effectively removed, which subsequently allows for efficient text information extraction. However, employing multiple GANs for different color channels requires long training and inference times. To reduce both the training and inference times of these preliminary steps, we propose an efficient method based on multi-scale feature extraction, which incorporates Haar wavelet transformation and normalization to process document images before submitting them to GANs for training. Experiment results show that our proposed method significantly reduces both the training and inference times while maintaining comparable performances when benchmarked against the state-of-the-art methods. In the best case scenario, a reduction of 10% and 26% are observed for training and inference times, respectively, while maintaining the model performance at 73.79 of Average-Score metric. The implementation of this work is available at https://github.com/RuiyangJu/Efficient_Document_Image_Binarization.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hi-EF: Benchmarking Emotion Forecasting in Human-interaction</title>
<link>https://arxiv.org/abs/2407.16406</link>
<guid>https://arxiv.org/abs/2407.16406</guid>
<content:encoded><![CDATA[
arXiv:2407.16406v2 Announce Type: replace 
Abstract: Affective Forecasting is an psychology task that involves predicting an individual's future emotional responses, often hampered by reliance on external factors leading to inaccuracies, and typically remains at a qualitative analysis stage. To address these challenges, we narrows the scope of Affective Forecasting by introducing the concept of Human-interaction-based Emotion Forecasting (EF). This task is set within the context of a two-party interaction, positing that an individual's emotions are significantly influenced by their interaction partner's emotional expressions and informational cues. This dynamic provides a structured perspective for exploring the patterns of emotional change, thereby enhancing the feasibility of emotion forecasting.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Perception Matters: Diagnosing Temporal Integration Failures in Multimodal Models</title>
<link>https://arxiv.org/abs/2408.07867</link>
<guid>https://arxiv.org/abs/2408.07867</guid>
<content:encoded><![CDATA[
arXiv:2408.07867v2 Announce Type: replace 
Abstract: Continuous perception, the ability to integrate visual observations over time in a continuous stream fashion, is essential for robust real-world understanding, yet remains largely untested in current multimodal models. We introduce CP-Bench, a minimal and fully controlled benchmark designed to isolate this capability using an extremely simple task: counting identical cubes in a synthetic scene while the camera moves and only reveals subsets of objects at any moment. Despite the simplicity of the setting, we find that state-of-the-art open-source and commercial models, including Qwen-3-VL, InternVL3, GPT-5, and Gemini-3-Pro, fail dramatically. A static-camera control variant confirms that the failure arises not from object recognition but from an inability to accumulate evidence across time. Further experiments show that neither higher sampling FPS, perception- or spatial-enhanced models, nor finetuning with additional videos leads to meaningful cross-temporal generalization. Our results reveal a fundamental limitation in modern multimodal architectures and training paradigms. CP-Bench provides a simple yet powerful diagnostic tool and establishes a clean testbed for developing models capable of genuine time-consistent visual reasoning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynPlay: Large-Scale Synthetic Human Data with Real-World Diversity for Aerial-View Perception</title>
<link>https://arxiv.org/abs/2408.11814</link>
<guid>https://arxiv.org/abs/2408.11814</guid>
<content:encoded><![CDATA[
arXiv:2408.11814v2 Announce Type: replace 
Abstract: We introduce SynPlay, a large-scale synthetic human dataset purpose-built for advancing multi-perspective human localization, with a predominant focus on aerial-view perception. SynPlay departs from traditional synthetic datasets by addressing a critical but underexplored challenge: localizing humans in aerial scenes where subjects often occupy only tens of pixels in the image. In such scenarios, fine-grained details like facial features or textures become irrelevant, shifting the burden of recognition to human motion, behavior, and interactions. To meet this need, SynPlay implements a novel rule-guided motion generation framework that combines real-world motion capture with motion evolution graphs. This design enables human actions to evolve dynamically through high-level game rules rather than predefined scripts, resulting in effectively uncountable motion variations. Unlike existing synthetic datasets-which either focus on static visual traits or reuse a limited set of mocap-driven actions-SynPlay captures a wide spectrum of spontaneous behaviors, including complex interactions that naturally emerge from unscripted gameplay scenarios. SynPlay also introduces an extensive multi-camera setup that spans UAVs at random altitudes, CCTVs, and a freely roaming UGV, achieving true near-to-far perspective coverage in a single dataset. The majority of instances are captured from aerial viewpoints at varying scales, directly supporting the development of models for long-range human analysis-a setting where existing datasets fall short. Our data contains over 73k images and 6.5M human instances, with detailed annotations for detection, segmentation, and keypoint tasks. Extensive experiments demonstrate that training with SynPlay significantly improves human localization performance, especially in few-shot and data-scarce scenarios.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSR-KAN: Efficient Hyperspectral Image Super-Resolution via Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2409.06705</link>
<guid>https://arxiv.org/abs/2409.06705</guid>
<content:encoded><![CDATA[
arXiv:2409.06705v3 Announce Type: replace 
Abstract: Hyperspectral images (HSIs) have great potential in various visual tasks due to their rich spectral information. However, obtaining high-resolution hyperspectral images remains challenging due to limitations of physical imaging. Inspired by Kolmogorov-Arnold Networks (KANs), we propose an efficient HSI super-resolution (HSI-SR) model to fuse a low-resolution HSI (LR-HSI) and a high-resolution multispectral image (HR-MSI), yielding a high-resolution HSI (HR-HSI). To achieve the effective integration of spatial information from HR-MSI, we design a fusion module based on KANs, called KAN-Fusion. Further inspired by the channel attention mechanism, we design a spectral channel attention module called KAN Channel Attention Block (KAN-CAB) for post-fusion feature extraction. As a channel attention module integrated with KANs, KAN-CAB not only enhances the fine-grained adjustment ability of deep networks, enabling networks to accurately simulate details of spectral sequences and spatial textures, but also effectively avoid Curse of Dimensionality. Extensive experiments show that, compared to current state-of-the-art HSI-SR methods, proposed HSR-KAN achieves the best performance in terms of both qualitative and quantitative assessments. Our code is available at: https://github.com/Baisonm-Li/HSR-KAN.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoH: Multi-Head Attention as Mixture-of-Head Attention</title>
<link>https://arxiv.org/abs/2410.11842</link>
<guid>https://arxiv.org/abs/2410.11842</guid>
<content:encoded><![CDATA[
arXiv:2410.11842v3 Announce Type: replace 
Abstract: In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%-90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sketch-guided Cage-based 3D Gaussian Splatting Deformation</title>
<link>https://arxiv.org/abs/2411.12168</link>
<guid>https://arxiv.org/abs/2411.12168</guid>
<content:encoded><![CDATA[
arXiv:2411.12168v3 Announce Type: replace 
Abstract: 3D Gaussian Splatting (GS) is one of the most promising novel 3D representations that has received great interest in computer graphics and computer vision. While various systems have introduced editing capabilities for 3D GS, such as those guided by text prompts, fine-grained control over deformation remains an open challenge. In this work, we present a novel sketch-guided 3D GS deformation system that allows users to intuitively modify the geometry of a 3D GS model by drawing a silhouette sketch from a single viewpoint. Our approach introduces a new deformation method that combines cage-based deformations with a variant of Neural Jacobian Fields, enabling precise, fine-grained control. Additionally, it leverages large-scale 2D diffusion priors and ControlNet to ensure the generated deformations are semantically plausible. Through a series of experiments, we demonstrate the effectiveness of our method and showcase its ability to animate static 3D GS models as one of its key applications.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushing the Limits of Sparsity: A Bag of Tricks for Extreme Pruning</title>
<link>https://arxiv.org/abs/2411.13545</link>
<guid>https://arxiv.org/abs/2411.13545</guid>
<content:encoded><![CDATA[
arXiv:2411.13545v4 Announce Type: replace 
Abstract: Pruning of deep neural networks has been an effective technique for reducing model size while preserving most of the performance of dense networks, crucial for deploying models on memory and power-constrained devices. While recent sparse learning methods have shown promising performance up to moderate sparsity levels such as 95% and 98%, accuracy quickly deteriorates when pushing sparsities to extreme levels due to unique challenges such as fragile gradient flow. In this work, we explore network performance beyond the commonly studied sparsities, and develop techniques that encourage stable training without accuracy collapse even at extreme sparsities, including 99.90%, 99.95\% and 99.99% on ResNet architectures. We propose three complementary techniques that enhance sparse training through different mechanisms: 1) Dynamic ReLU phasing, where DyReLU initially allows for richer parameter exploration before being gradually replaced by standard ReLU, 2) weight sharing which reuses parameters within a residual layer while maintaining the same number of learnable parameters, and 3) cyclic sparsity, where both sparsity levels and sparsity patterns evolve dynamically throughout training to better encourage parameter exploration. We evaluate our method, which we term Extreme Adaptive Sparse Training (EAST) at extreme sparsities using ResNet-34 and ResNet-50 on CIFAR-10, CIFAR-100, and ImageNet, achieving competitive or improved performance compared to existing methods, with notable gains at extreme sparsity levels.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuperMat: Physically Consistent PBR Material Estimation at Interactive Rates</title>
<link>https://arxiv.org/abs/2411.17515</link>
<guid>https://arxiv.org/abs/2411.17515</guid>
<content:encoded><![CDATA[
arXiv:2411.17515v4 Announce Type: replace 
Abstract: Decomposing physically-based materials from images into their constituent properties remains challenging, particularly when maintaining both computational efficiency and physical consistency. While recent diffusion-based approaches have shown promise, they face substantial computational overhead due to multiple denoising steps and separate models for different material properties. We present SuperMat, a single-step framework that achieves high-quality material decomposition with one-step inference. This enables end-to-end training with perceptual and re-render losses while decomposing albedo, metallic, and roughness maps at millisecond-scale speeds. We further extend our framework to 3D objects through a UV refinement network, enabling consistent material estimation across viewpoints while maintaining efficiency. Experiments demonstrate that SuperMat achieves state-of-the-art PBR material decomposition quality while reducing inference time from seconds to milliseconds per image, and completes PBR material estimation for 3D objects in approximately 3 seconds. The project page is at https://hyj542682306.github.io/SuperMat/.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Manual-PA: Learning 3D Part Assembly from Instruction Diagrams</title>
<link>https://arxiv.org/abs/2411.18011</link>
<guid>https://arxiv.org/abs/2411.18011</guid>
<content:encoded><![CDATA[
arXiv:2411.18011v2 Announce Type: replace 
Abstract: Assembling furniture amounts to solving the discrete-continuous optimization task of selecting the furniture parts to assemble and estimating their connecting poses in a physically realistic manner. The problem is hampered by its combinatorially large yet sparse solution space thus making learning to assemble a challenging task for current machine learning models. In this paper, we attempt to solve this task by leveraging the assembly instructions provided in diagrammatic manuals that typically accompany the furniture parts. Our key insight is to use the cues in these diagrams to split the problem into discrete and continuous phases. Specifically, we present Manual-PA, a transformer-based instruction Manual-guided 3D Part Assembly framework that learns to semantically align 3D parts with their illustrations in the manuals using a contrastive learning backbone towards predicting the assembly order and infers the 6D pose of each part via relating it to the final furniture depicted in the manual. To validate the efficacy of our method, we conduct experiments on the benchmark PartNet dataset. Our results show that using the diagrams and the order of the parts lead to significant improvements in assembly performance against the state of the art. Further, Manual-PA demonstrates strong generalization to real-world IKEA furniture assembly on the IKEA-Manual dataset.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Exploitation of Data Diversity Improves Visual Localization</title>
<link>https://arxiv.org/abs/2412.00138</link>
<guid>https://arxiv.org/abs/2412.00138</guid>
<content:encoded><![CDATA[
arXiv:2412.00138v2 Announce Type: replace 
Abstract: Visual localization, which estimates a camera's pose within a known scene, is a fundamental capability for autonomous systems. While absolute pose regression (APR) methods have shown promise for efficient inference, they often struggle with generalization. Recent approaches attempt to address this through data augmentation with varied viewpoints, yet they overlook a critical factor: appearance diversity. In this work, we identify appearance variation as the key to robust localization. Specifically, we first lift real 2D images into 3D Gaussian Splats with varying appearance and deblurring ability, enabling the synthesis of diverse training data that varies not just in poses but also in environmental conditions such as lighting and weather. To fully unleash the potential of the appearance-diverse data, we build a two-branch joint training pipeline with an adversarial discriminator to bridge the syn-to-real gap. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, reducing translation and rotation errors by 50\% and 41\% on indoor datasets, and 38\% and 44\% on outdoor datasets. Most notably, our method shows remarkable robustness in dynamic driving scenarios under varying weather conditions and in day-to-night scenarios, where previous APR methods fail. Project Page: https://ai4ce.github.io/RAP/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blind Inverse Problem Solving Made Easy by Text-to-Image Latent Diffusion</title>
<link>https://arxiv.org/abs/2412.00557</link>
<guid>https://arxiv.org/abs/2412.00557</guid>
<content:encoded><![CDATA[
arXiv:2412.00557v2 Announce Type: replace 
Abstract: This paper considers blind inverse image restoration, the task of predicting a target image from a degraded source when the degradation (i.e. the forward operator) is unknown. Existing solutions typically rely on restrictive assumptions such as operator linearity, curated training data or narrow image distributions limiting their practicality. We introduce LADiBI, a training-free method leveraging large-scale text-to-image diffusion to solve diverse blind inverse problems with minimal assumptions. Within a Bayesian framework, LADiBI uses text prompts to jointly encode priors for both target images and operators, unlocking unprecedented flexibility compared to existing methods. Additionally, we propose a novel diffusion posterior sampling algorithm that combines strategic operator initialization with iterative refinement of image and operator parameters, eliminating the need for highly constrained operator forms. Experiments show that LADiBI effectively handles both linear and challenging nonlinear image restoration problems across various image distributions, all without task-specific assumptions or retraining.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SizeGS: Size-aware Compression of 3D Gaussian Splatting via Mixed Integer Programming</title>
<link>https://arxiv.org/abs/2412.05808</link>
<guid>https://arxiv.org/abs/2412.05808</guid>
<content:encoded><![CDATA[
arXiv:2412.05808v2 Announce Type: replace 
Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have greatly improved 3D reconstruction. However, its substantial data size poses a significant challenge for transmission and storage. While many compression techniques have been proposed, they fail to efficiently adapt to fluctuating network bandwidth, leading to resource wastage. We address this issue from the perspective of size-aware compression, where we aim to compress 3DGS to a desired size by quickly searching for suitable hyperparameters. Through a measurement study, we identify key hyperparameters that affect the size -- namely, the reserve ratio of Gaussians and bit-width settings for Gaussian attributes. Then, we formulate this hyperparameter optimization problem as a mixed-integer nonlinear programming (MINLP) problem, with the goal of maximizing visual quality while respecting the size budget constraint. To solve the MINLP, we decouple this problem into two parts: discretely sampling the reserve ratio and determining the bit-width settings using integer linear programming (ILP). To solve the ILP more quickly and accurately, we design a quality loss estimator and a calibrated size estimator, as well as implement a CUDA kernel. Extensive experiments on multiple 3DGS variants demonstrate that our method achieves state-of-the-art performance in post-training compression. Furthermore, our method can achieve comparable quality to leading training-required methods after fine-tuning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U-FaceBP: Uncertainty-aware Bayesian Ensemble Deep Learning for Face Video-based Blood Pressure Measurement</title>
<link>https://arxiv.org/abs/2412.10679</link>
<guid>https://arxiv.org/abs/2412.10679</guid>
<content:encoded><![CDATA[
arXiv:2412.10679v2 Announce Type: replace 
Abstract: Blood pressure (BP) measurement is crucial for daily health assessment. Remote photoplethysmography (rPPG), which extracts pulse waves from face videos captured by a camera, has the potential to enable convenient BP measurement without specialized medical devices. However, there are various uncertainties in BP estimation using rPPG, leading to limited estimation performance and reliability. In this paper, we propose U-FaceBP, an uncertainty-aware Bayesian ensemble deep learning method for face video-based BP measurement. U-FaceBP models aleatoric and epistemic uncertainties in face video-based BP estimation with a Bayesian neural network (BNN). Additionally, we design U-FaceBP as an ensemble method, estimating BP from rPPG signals, PPG signals derived from face videos, and face images using multiple BNNs. Large-scale experiments on two datasets involving 1197 subjects from diverse racial groups demonstrate that U-FaceBP outperforms state-of-the-art BP estimation methods. Furthermore, we show that the uncertainty estimates provided by U-FaceBP are informative and useful for guiding modality fusion, assessing prediction reliability, and analyzing performance across racial groups.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation</title>
<link>https://arxiv.org/abs/2412.15209</link>
<guid>https://arxiv.org/abs/2412.15209</guid>
<content:encoded><![CDATA[
arXiv:2412.15209v2 Announce Type: replace 
Abstract: Despite significant advancements in Large Vision-Language Models (LVLMs)' capabilities, existing pixel-grounding models operate in single-image settings, limiting their ability to perform detailed, fine-grained comparisons across multiple images. Conversely, current multi-image understanding models lack pixel-level grounding. Our work addresses this gap by introducing the task of multi-image pixel-grounded reasoning alongside PRIMA, an LVLM that integrates pixel-level grounding with robust multi-image reasoning to produce contextually rich, pixel-grounded explanations. Central to PRIMA is SQuARE, a vision module that injects cross-image relational context into compact query-based visual tokens before fusing them with the language backbone. To support training and evaluation, we curate M4SEG, a new multi-image reasoning segmentation benchmark consisting of $\sim$744K question-answer pairs that require fine-grained visual understanding across multiple images. PRIMA outperforms state-of-the-art baselines with $7.83\%$ and $11.25\%$ improvements in Recall and S-IoU, respectively. Ablation studies further demonstrate the effectiveness of the proposed SQuARE module in capturing cross-image relationships.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Minimal Subset Approach for Informed Keyframe Sampling in Large-Scale SLAM</title>
<link>https://arxiv.org/abs/2501.01791</link>
<guid>https://arxiv.org/abs/2501.01791</guid>
<content:encoded><![CDATA[
arXiv:2501.01791v3 Announce Type: replace 
Abstract: Typical LiDAR SLAM architectures feature a front-end for odometry estimation and a back-end for refining and optimizing the trajectory and map, commonly through loop closures. However, loop closure detection in large-scale missions presents significant computational challenges due to the need to identify, verify, and process numerous candidate pairs for pose graph optimization. Keyframe sampling bridges the front-end and back-end by selecting frames for storing and processing during global optimization. This article proposes an online keyframe sampling approach that constructs the pose graph using the most impactful keyframes for loop closure. We introduce the Minimal Subset Approach (MSA), which optimizes two key objectives: redundancy minimization and information preservation, implemented within a sliding window framework. By operating in the feature space rather than 3-D space, MSA efficiently reduces redundant keyframes while retaining essential information. Evaluations on diverse public datasets show that the proposed approach outperforms naive methods in reducing false positive rates in place recognition, while delivering superior ATE and RPE in metric localization, without the need for manual parameter tuning. Additionally, MSA demonstrates efficiency and scalability by reducing memory usage and computational overhead during loop closure detection and pose graph optimization.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Balanced Multi-Modal Learning in 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2501.05264</link>
<guid>https://arxiv.org/abs/2501.05264</guid>
<content:encoded><![CDATA[
arXiv:2501.05264v4 Announce Type: replace 
Abstract: 3D human pose estimation (3D HPE) has emerged as a prominent research topic, particularly in the realm of RGB-based methods. However, the use of RGB images is often limited by issues such as occlusion and privacy constraints. Consequently, multi-modal sensing, which leverages non-intrusive sensors, is gaining increasing attention. Nevertheless, multi-modal 3D HPE still faces challenges, including modality imbalance. In this work, we introduce a novel balanced multi-modal learning method for 3D HPE, which harnesses the power of RGB, LiDAR, mmWave, and WiFi. Specifically, we propose a Shapley value-based contribution algorithm to assess the contribution of each modality and detect modality imbalance. To address this imbalance, we design a modality learning regulation strategy that decelerates the learning process during the early stages of training. We conduct extensive experiments on the widely adopted multi-modal dataset, MM-Fi, demonstrating the superiority of our approach in enhancing 3D pose estimation under complex conditions. We will release our codes soon.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Partially Observed Trajectories Forecasting by Target-driven Self-Distillation</title>
<link>https://arxiv.org/abs/2501.16767</link>
<guid>https://arxiv.org/abs/2501.16767</guid>
<content:encoded><![CDATA[
arXiv:2501.16767v2 Announce Type: replace 
Abstract: Accurate prediction of future trajectories of traffic agents is essential for ensuring safe autonomous driving. However, partially observed trajectories can significantly degrade the performance of even state-of-the-art models. Previous approaches often rely on knowledge distillation to transfer features from fully observed trajectories to partially observed ones. This involves firstly training a fully observed model and then using a distillation process to create the final model. While effective, they require multi-stage training, making the training process very expensive. Moreover, knowledge distillation can lead to a performance degradation of the model. In this paper, we introduce a Target-drivenSelf-Distillation method (TSD) for motion forecasting. Our method leverages predicted accurate targets to guide the model in making predictions under partial observation conditions. By employing self-distillation, the model learns from the feature distributions of both fully observed and partially observed trajectories during a single end-to-end training process. This enhances the model's ability to predict motion accurately in both fully observed and partially observed scenarios. We evaluate our method on multiple datasets and state-of-the-art motion forecasting models. Extensive experimental results demonstrate that our approach achieves significant performance improvements in both settings. To facilitate further research, we will release our code and model checkpoints.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Forward-Forward: A Training Algorithm of Vision Transformer</title>
<link>https://arxiv.org/abs/2502.00571</link>
<guid>https://arxiv.org/abs/2502.00571</guid>
<content:encoded><![CDATA[
arXiv:2502.00571v2 Announce Type: replace 
Abstract: Although backpropagation is widely accepted as a training algorithm for artificial neural networks, researchers are always looking for inspiration from the brain to find ways with potentially better performance. Forward-Forward is a novel training algorithm that is more similar to what occurs in the brain, although there is a significant performance gap compared to backpropagation. In the Forward-Forward algorithm, the loss functions are placed after each layer, and the updating of a layer is done using two local forward passes and one local backward pass. Forward-Forward is in its early stages and has been designed and evaluated on simple multi-layer perceptron networks to solve image classification tasks. In this work, we have extended the use of this algorithm to a more complex and modern network, namely the Vision Transformer. Inspired by insights from contrastive learning, we have attempted to revise this algorithm, leading to the introduction of Contrastive Forward-Forward. Experimental results show that our proposed algorithm performs significantly better than the baseline Forward-Forward leading to an increase of up to 10% in accuracy and accelerating the convergence speed by 5 to 20 times. Furthermore, if we take Cross Entropy as the baseline loss function in backpropagation, it will be demonstrated that the proposed modifications to the baseline Forward-Forward reduce its performance gap compared to backpropagation on Vision Transformer, and even outperforms it in certain conditions, such as inaccurate supervision.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiMo: High-Speed Objects Motion Compensation in Point Clouds</title>
<link>https://arxiv.org/abs/2503.00803</link>
<guid>https://arxiv.org/abs/2503.00803</guid>
<content:encoded><![CDATA[
arXiv:2503.00803v3 Announce Type: replace 
Abstract: LiDAR point cloud is essential for autonomous vehicles, but motion distortions from dynamic objects degrade the data quality. While previous work has considered distortions caused by ego motion, distortions caused by other moving objects remain largely overlooked, leading to errors in object shape and position. This distortion is particularly pronounced in high-speed environments such as highways and in multi-LiDAR configurations, a common setup for heavy vehicles. To address this challenge, we introduce HiMo, a pipeline that repurposes scene flow estimation for non-ego motion compensation, correcting the representation of dynamic objects in point clouds. During the development of HiMo, we observed that existing self-supervised scene flow estimators often produce degenerate or inconsistent estimates under high-speed distortion. We further propose SeFlow++, a real-time scene flow estimator that achieves state-of-the-art performance on both scene flow and motion compensation. Since well-established motion distortion metrics are absent in the literature, we introduce two evaluation metrics: compensation accuracy at a point level and shape similarity of objects. We validate HiMo through extensive experiments on Argoverse 2, ZOD, and a newly collected real-world dataset featuring highway driving and multi-LiDAR-equipped heavy vehicles. Our findings show that HiMo improves the geometric consistency and visual fidelity of dynamic objects in LiDAR point clouds, benefiting downstream tasks such as semantic segmentation and 3D detection. See https://kin-zhang.github.io/HiMo for more details.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation</title>
<link>https://arxiv.org/abs/2503.08906</link>
<guid>https://arxiv.org/abs/2503.08906</guid>
<content:encoded><![CDATA[
arXiv:2503.08906v3 Announce Type: replace 
Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong performance but struggle when adapted to downstream tasks. Prompt learning has emerged as an efficient and effective strategy to adapt VLMs while preserving their pre-trained knowledge. However, existing methods still lead to overfitting and degrade zero-shot generalization. To address this challenge, we propose an optimal transport (OT)-guided prompt learning framework that mitigates forgetting by preserving the structural consistency of feature distributions between pre-trained and fine-tuned models. Unlike conventional point-wise constraints, OT naturally captures cross-instance relationships and expands the feasible parameter space for prompt tuning, allowing a better trade-off between adaptation and generalization. Our approach enforces joint constraints on both vision and text representations, ensuring a holistic feature alignment. Extensive experiments on benchmark datasets demonstrate that our simple yet effective method can outperform existing prompt learning strategies in base-to-novel generalization, cross-dataset evaluation, and domain generalization without additional augmentation or ensemble techniques. The code is available at https://github.com/ChongQingNoSubway/Prompt-OT
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proxy-Tuning: Tailoring Multimodal Autoregressive Models for Subject-Driven Image Generation</title>
<link>https://arxiv.org/abs/2503.10125</link>
<guid>https://arxiv.org/abs/2503.10125</guid>
<content:encoded><![CDATA[
arXiv:2503.10125v2 Announce Type: replace 
Abstract: Multimodal autoregressive (AR) models, based on next-token prediction and transformer architecture, have demonstrated remarkable capabilities in various multimodal tasks including text-to-image (T2I) generation. Despite their strong performance in general T2I tasks, our research reveals that these models initially struggle with subject-driven image generation compared to dominant diffusion models. To address this limitation, we introduce Proxy-Tuning, leveraging diffusion models to enhance AR models' capabilities in subject-specific image generation. Our method reveals a striking weak-to-strong phenomenon: fine-tuned AR models consistently outperform their diffusion model supervisors in both subject fidelity and prompt adherence. We analyze this performance shift and identify scenarios where AR models excel, particularly in multi-subject compositions and contextual understanding. This work not only demonstrates impressive results in subject-driven AR image generation, but also unveils the potential of weak-to-strong generalization in the image generation domain, contributing to a deeper understanding of different architectures' strengths and limitations.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP-Free, Label-Free, Zero-Shot Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2503.10981</link>
<guid>https://arxiv.org/abs/2503.10981</guid>
<content:encoded><![CDATA[
arXiv:2503.10981v3 Announce Type: replace 
Abstract: Concept Bottleneck Models (CBMs) map dense, high-dimensional feature representations into a set of human-interpretable concepts which are then combined linearly to make a prediction. However, modern CBMs rely on the CLIP model to establish a mapping from dense feature representations to textual concepts, and it remains unclear how to design CBMs for models other than CLIP. Methods that do not use CLIP instead require manual, labor intensive annotation to associate feature representations with concepts. Furthermore, all CBMs necessitate training a linear classifier to map the extracted concepts to class labels. In this work, we lift all three limitations simultaneously by proposing a method that converts any frozen visual classifier into a CBM without requiring image-concept labels (label-free), without relying on the CLIP model (CLIP-free), and by deriving the linear classifier in a zero-shot manner. Our method is formulated by aligning the original classifier's distribution (over discrete class indices) with its corresponding vision-language counterpart distribution derived from textual class names, while preserving the classifier's performance. The approach requires no ground-truth image-class annotations, and is highly data-efficient and preserves the classifier's reasoning process. Applied and tested on over 40 visual classifiers, our resulting CLIP-free, zero-shot CBM sets a new state of the art, surpassing even supervised CLIP-based CBMs. Finally, we also show that our method can be used for zero-shot image captioning, outperforming existing methods based on CLIP, and achieving state of the art results.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InsightDrive: Insight Scene Representation for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2503.13047</link>
<guid>https://arxiv.org/abs/2503.13047</guid>
<content:encoded><![CDATA[
arXiv:2503.13047v2 Announce Type: replace 
Abstract: Conventional end-to-end autonomous driving methods often rely on explicit global scene representations, which typically consist of 3D object detection, online mapping, and motion prediction. In contrast, human drivers selectively attend to task-relevant regions and implicitly reason over the broader traffic context. Motivated by this observation, we introduce a lightweight end-to-end autonomous driving framework, InsightDrive. Unlike approaches that directly embed large language models (LLMs), InsightDrive introduces an Insight scene representation that jointly models attention-centric explicit scene representation and reasoning-centric implicit scene representation, so that scene understanding aligns more closely with human cognitive patterns for trajectory planning. To this end, we employ Chain-of-Thought (CoT) instructions to model human driving cognition and design a task-level Mixture-of-Experts (MoE) adapter that injects this knowledge into the autonomous driving model at negligible parameter cost. We further condition the planner on both explicit and implicit scene representations and employ a diffusion-based generative policy, which produces robust trajectory predictions and decisions. The overall framework establishes a knowledge distillation pipeline that transfers human driving knowledge to LLMs and subsequently to onboard models. Extensive experiments on the nuScenes and Navsim benchmarks demonstrate that InsightDrive achieves significant improvements over conventional scene representation approaches.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GARF: Learning Generalizable 3D Reassembly for Real-World Fractures</title>
<link>https://arxiv.org/abs/2504.05400</link>
<guid>https://arxiv.org/abs/2504.05400</guid>
<content:encoded><![CDATA[
arXiv:2504.05400v2 Announce Type: replace 
Abstract: 3D reassembly is a challenging spatial intelligence task with broad applications across scientific domains. While large-scale synthetic datasets have fueled promising learning-based approaches, their generalizability to different domains is limited. Critically, it remains uncertain whether models trained on synthetic datasets can generalize to real-world fractures where breakage patterns are more complex. To bridge this gap, we propose GARF, a generalizable 3D reassembly framework for real-world fractures. GARF leverages fracture-aware pretraining to learn fracture features from individual fragments, with flow matching enabling precise 6-DoF alignments. At inference time, we introduce one-step preassembly, improving robustness to unseen objects and varying numbers of fractures. In collaboration with archaeologists, paleoanthropologists, and ornithologists, we curate Fractura, a diverse dataset for vision and learning communities, featuring real-world fracture types across ceramics, bones, eggshells, and lithics. Comprehensive experiments have shown our approach consistently outperforms state-of-the-art methods on both synthetic and real-world datasets, achieving 82.87\% lower rotation error and 25.15\% higher part accuracy. This sheds light on training on synthetic data to advance real-world 3D puzzle solving, demonstrating its strong generalization across unseen object shapes and diverse fracture types. GARF's code, data and demo are available at https://ai4ce.github.io/GARF/.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniSVG: A Unified Scalable Vector Graphics Generation Model</title>
<link>https://arxiv.org/abs/2504.06263</link>
<guid>https://arxiv.org/abs/2504.06263</guid>
<content:encoded><![CDATA[
arXiv:2504.06263v3 Announce Type: replace 
Abstract: Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions</title>
<link>https://arxiv.org/abs/2504.11967</link>
<guid>https://arxiv.org/abs/2504.11967</guid>
<content:encoded><![CDATA[
arXiv:2504.11967v3 Announce Type: replace 
Abstract: Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure inspection, surveillance, and related tasks, yet they also introduce critical security challenges. This survey provides a wide-ranging examination of the anti-UAV domain, centering on three core objectives-classification, detection, and tracking-while detailing emerging methodologies such as diffusion-based data synthesis, multi-modal fusion, vision-language modeling, self-supervised learning, and reinforcement learning. We systematically evaluate state-of-the-art solutions across both single-modality and multi-sensor pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss large-scale as well as adversarially oriented benchmarks. Our analysis reveals persistent gaps in real-time performance, stealth detection, and swarm-based scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems. By highlighting open research directions, we aim to foster innovation and guide the development of next-generation defense strategies in an era marked by the extensive use of UAVs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2504.20518</link>
<guid>https://arxiv.org/abs/2504.20518</guid>
<content:encoded><![CDATA[
arXiv:2504.20518v3 Announce Type: replace 
Abstract: Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the $<$EOS$>$ token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens' attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across six representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.27% and an AUC of 86.27%. The code is available at https://github.com/Robin-WZQ/DAA.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoQA: Visual-only Question Answering</title>
<link>https://arxiv.org/abs/2505.14227</link>
<guid>https://arxiv.org/abs/2505.14227</guid>
<content:encoded><![CDATA[
arXiv:2505.14227v2 Announce Type: replace 
Abstract: Visual understanding requires interpreting both natural scenes and the textual information that appears within them, motivating tasks such as Visual Question Answering (VQA). However, current VQA benchmarks overlook scenarios with visually embedded questions, whereas advanced agents should be able to see the question without separate text input as humans. We introduce Visual-only Question Answering (VoQA), where both the scene and the question appear within a single image, requiring models to perceive and reason purely through vision. This setting supports more realistic visual understanding and interaction in scenarios where questions or instructions are embedded directly in the visual scene. Evaluations under pure visual-only zero-shot, prompt-guided and OCR-assisted settings show that current models exhibit a clear performance drop compared to traditional VQA. To address this, we investigate question-alignment fine-tuning strategies designed to guide models toward interpreting the visual question prior to reasoning. Leveraging VoQA dataset together with these strategies yields robust vision-only reasoning while preserving cross-task generalization to traditional VQA, reflecting the complementary visual and textual reasoning capabilities fostered through VoQA training. The code and data are publicly available.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rendering-Aware Reinforcement Learning for Vector Graphics Generation</title>
<link>https://arxiv.org/abs/2505.20793</link>
<guid>https://arxiv.org/abs/2505.20793</guid>
<content:encoded><![CDATA[
arXiv:2505.20793v2 Announce Type: replace 
Abstract: Scalable Vector Graphics (SVG) offer a powerful format for representing visual designs as interpretable code. Recent advances in vision-language models (VLMs) have enabled high-quality SVG generation by framing the problem as a code generation task and leveraging large-scale pretraining. VLMs are particularly suitable for this task as they capture both global semantics and fine-grained visual patterns, while transferring knowledge across vision, natural language, and code domains. However, existing VLM approaches often struggle to produce faithful and efficient SVGs because they never observe the rendered images during training. Although differentiable rendering for autoregressive SVG code generation remains unavailable, rendered outputs can still be compared to original inputs, enabling evaluative feedback suitable for reinforcement learning (RL). We introduce RLRF (Reinforcement Learning from Rendering Feedback), an RL method that enhances SVG generation in autoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an input image, the model generates SVG roll-outs that are rendered and compared to the original image to compute a reward. This visual fidelity feedback guides the model toward producing more accurate, efficient, and semantically coherent SVGs. RLRF significantly outperforms supervised fine-tuning, addressing common failure modes and enabling precise, high-quality SVG generation with strong structural understanding and generalization.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation and Understanding</title>
<link>https://arxiv.org/abs/2505.22643</link>
<guid>https://arxiv.org/abs/2505.22643</guid>
<content:encoded><![CDATA[
arXiv:2505.22643v3 Announce Type: replace 
Abstract: Leveraging recent diffusion models, LiDAR-based large-scale 3D scene generation has achieved great success. While recent voxel-based approaches can generate both geometric structures and semantic labels, existing range-view methods are limited to producing unlabeled LiDAR scenes. Relying on pretrained segmentation models to predict the semantic maps often results in suboptimal cross-modal consistency. To address this limitation while preserving the advantages of range-view representations, such as computational efficiency and simplified network design, we propose Spiral, a novel range-view LiDAR diffusion model that simultaneously generates depth, reflectance images, and semantic maps. Furthermore, we introduce novel semantic-aware metrics to evaluate the quality of the generated labeled range-view data. Experiments on the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves state-of-the-art performance with the smallest parameter size, outperforming two-step methods that combine the generative and segmentation models. Additionally, we validate that range images generated by Spiral can be effectively used for synthetic data augmentation in the downstream segmentation training, significantly reducing the labeling effort on LiDAR data.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2505.23525</link>
<guid>https://arxiv.org/abs/2505.23525</guid>
<content:encoded><![CDATA[
arXiv:2505.23525v4 Announce Type: replace 
Abstract: Generating highly dynamic and photorealistic portrait animations driven by audio and skeletal motion remains challenging due to the need for precise lip synchronization, natural facial expressions, and high-fidelity body motion dynamics. We propose a human-preference-aligned diffusion framework that addresses these challenges through two key innovations. First, we introduce direct preference optimization tailored for human-centric animation, leveraging a curated dataset of human preferences to align generated outputs with perceptual metrics for portrait motion-video alignment and naturalness of expression. Second, the proposed temporal motion modulation resolves spatiotemporal resolution mismatches by reshaping motion conditions into dimensionally aligned latent features through temporal channel redistribution and proportional feature expansion, preserving the fidelity of high-frequency motion details in diffusion-based synthesis. The proposed mechanism is complementary to existing UNet and DiT-based portrait diffusion approaches, and experiments demonstrate obvious improvements in lip-audio synchronization, expression vividness, body motion coherence over baseline methods, alongside notable gains in human preference metrics. Our model and source code can be found at: https://github.com/fudan-generative-vision/hallo4.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Fit Check Videos with a Handheld Camera</title>
<link>https://arxiv.org/abs/2505.23886</link>
<guid>https://arxiv.org/abs/2505.23886</guid>
<content:encoded><![CDATA[
arXiv:2505.23886v2 Announce Type: replace 
Abstract: Self-captured full-body videos are popular, but most deployments require mounted cameras, carefully-framed shots, and repeated practice. We propose a more convenient solution that enables full-body video capture using handheld mobile devices. Our approach takes as input two static photos (front and back) of you in a mirror, along with an IMU motion reference that you perform while holding your mobile phone, and synthesizes a realistic video of you performing a similar target motion. We enable rendering into a new scene, with consistent illumination and shadows. We propose a novel video diffusion-based model to achieve this. Specifically, we propose a parameter-free frame generation strategy and a multi-reference attention mechanism to effectively integrate appearance information from both the front and back selfies into the video diffusion model. Further, we introduce an image-based fine-tuning strategy to enhance frame sharpness and improve shadows and reflections generation for more realistic human-scene composition.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Anomaly Detection with Semantics-Aware Information Bottleneck</title>
<link>https://arxiv.org/abs/2506.02535</link>
<guid>https://arxiv.org/abs/2506.02535</guid>
<content:encoded><![CDATA[
arXiv:2506.02535v3 Announce Type: replace 
Abstract: Semi-supervised video anomaly detection methods face two critical challenges: (1) Strong generalization blurs the boundary between normal and abnormal patterns. Although existing approaches attempt to alleviate this issue using memory modules, their rigid prototype-matching process limits adaptability to diverse scenarios; (2) Relying solely on low-level appearance and motion cues makes it difficult to perceive high-level semantic anomalies in complex scenes. To address these limitations, we propose SIB-VAD, a novel framework based on adaptive information bottleneck filtering and semantic-aware enhancement. We propose the Sparse Feature Filtering Module (SFFM) to replace traditional memory modules. It compresses normal features directly into a low-dimensional manifold based on the information bottleneck principle and uses an adaptive routing mechanism to dynamically select the most suitable normal bottleneck subspace. Trained only on normal data, SFFMs only learn normal low-dimensional manifolds, while abnormal features deviate and are effectively filtered. Unlike memory modules, SFFM directly removes abnormal information and adaptively handles scene variations. To improve semantic awareness, we further design a multimodal prediction framework that jointly models appearance, motion, and semantics. Through multimodal consistency constraints and joint error computation, it achieves more robust VAD performance. Experimental results validate the effectiveness of our feature filtering paradigm based on semantics-aware information bottleneck. Project page at https://qzfm.github.io/sib_vad_project_page/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIVAT: Virtuous Improving VAE Training through Artifact Mitigation</title>
<link>https://arxiv.org/abs/2506.07863</link>
<guid>https://arxiv.org/abs/2506.07863</guid>
<content:encoded><![CDATA[
arXiv:2506.07863v2 Announce Type: replace 
Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer vision, yet their training is often plagued by artifacts that degrade reconstruction and generation quality. This paper introduces VIVAT, a systematic approach to mitigating common artifacts in KL-VAE training without requiring radical architectural changes. We present a detailed taxonomy of five prevalent artifacts - color shift, grid patterns, blur, corner and droplet artifacts - and analyze their root causes. Through straightforward modifications, including adjustments to loss weights, padding strategies, and the integration of Spatially Conditional Normalization, we demonstrate significant improvements in VAE performance. Our method achieves state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhances text-to-image generation quality, as evidenced by superior CLIP scores. By preserving the simplicity of the KL-VAE framework while addressing its practical challenges, VIVAT offers actionable insights for researchers and practitioners aiming to optimize VAE training.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos</title>
<link>https://arxiv.org/abs/2506.08015</link>
<guid>https://arxiv.org/abs/2506.08015</guid>
<content:encoded><![CDATA[
arXiv:2506.08015v2 Announce Type: replace 
Abstract: We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene reconstruction, trained entirely on real-world monocular posed videos. Using 4D Gaussian as an inductive bias, 4DGT unifies static and dynamic components, enabling the modeling of complex, time-varying environments with varying object lifespans. We proposed a novel density control strategy in training, which enables our 4DGT to handle longer space-time input and remain efficient rendering at runtime. Our model processes 64 consecutive posed frames in a rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike optimization-based methods, 4DGT performs purely feed-forward inference, reducing reconstruction time from hours to seconds and scaling effectively to long video sequences. Trained only on large-scale monocular posed video datasets, 4DGT can outperform prior Gaussian-based networks significantly in real-world videos and achieve on-par accuracy with optimization-based methods on cross-domain videos. Project page: https://4dgt.github.io
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAMMA: Markerless &amp; Automatic Multi-Person Motion Action Capture</title>
<link>https://arxiv.org/abs/2506.13040</link>
<guid>https://arxiv.org/abs/2506.13040</guid>
<content:encoded><![CDATA[
arXiv:2506.13040v3 Announce Type: replace 
Abstract: We present MAMMA, a markerless motion-capture pipeline that accurately recovers SMPL-X parameters from multi-view video of two-person interaction sequences. Traditional motion-capture systems rely on physical markers. Although they offer high accuracy, their requirements of specialized hardware, manual marker placement, and extensive post-processing make them costly and time-consuming. Recent learning-based methods attempt to overcome these limitations, but most are designed for single-person capture, rely on sparse keypoints, or struggle with occlusions and physical interactions. In this work, we introduce a method that predicts dense 2D contact-aware surface landmarks conditioned on segmentation masks, enabling person-specific correspondence estimation even under heavy occlusion. We employ a novel architecture that exploits learnable queries for each landmark. We demonstrate that our approach can handle complex person--person interaction and offers greater accuracy than existing methods. To train our network, we construct a large, synthetic multi-view dataset combining human motions from diverse sources, including extreme poses, hand motions, and close interactions. Our dataset yields high-variability synthetic sequences with rich body contact and occlusion, and includes SMPL-X ground-truth annotations with dense 2D landmarks. The result is a system capable of capturing human motion without the need for markers. Our approach offers competitive reconstruction quality compared to commercial marker-based motion-capture solutions, without the extensive manual cleanup. Finally, we address the absence of common benchmarks for dense-landmark prediction and markerless motion capture by introducing two evaluation settings built from real multi-view sequences. We will release our dataset, benchmark, method, training code, and pre-trained model weights for research purposes.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-grained Image Retrieval via Dual-Vision Adaptation</title>
<link>https://arxiv.org/abs/2506.16273</link>
<guid>https://arxiv.org/abs/2506.16273</guid>
<content:encoded><![CDATA[
arXiv:2506.16273v4 Announce Type: replace 
Abstract: Fine-Grained Image Retrieval~(FGIR) faces challenges in learning discriminative visual representations to retrieve images with similar fine-grained features. Current leading FGIR solutions typically follow two regimes: enforce pairwise similarity constraints in the semantic embedding space, or incorporate a localization sub-network to fine-tune the entire model. However, such two regimes tend to overfit the training data while forgetting the knowledge gained from large-scale pre-training, thus reducing their generalization ability. In this paper, we propose a Dual-Vision Adaptation (DVA) approach for FGIR, which guides the frozen pre-trained model to perform FGIR through collaborative sample and feature adaptation. Specifically, we design Object-Perceptual Adaptation, which modifies input samples to help the pre-trained model perceive critical objects and elements within objects that are helpful for category prediction. Meanwhile, we propose In-Context Adaptation, which introduces a small set of parameters for feature adaptation without modifying the pre-trained parameters. This makes the FGIR task using these adjusted features closer to the task solved during the pre-training. Additionally, to balance retrieval efficiency and performance, we propose Discrimination Perception Transfer to transfer the discriminative knowledge in the object-perceptual adaptation to the image encoder using the knowledge distillation mechanism. Extensive experiments show that DVA has fewer learnable parameters and performs well on three in-distribution and three out-of-distribution fine-grained datasets.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Multimodal Prototype Learning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.03657</link>
<guid>https://arxiv.org/abs/2507.03657</guid>
<content:encoded><![CDATA[
arXiv:2507.03657v2 Announce Type: replace 
Abstract: With the increasing attention to pre-trained vision-language models (VLMs), \eg, CLIP, substantial efforts have been devoted to many downstream tasks, especially in test-time adaptation (TTA). However, previous works focus on learning prototypes only in the textual modality while overlooking the ambiguous semantics in class names. These ambiguities lead to textual prototypes that are insufficient to capture visual concepts, resulting in limited performance. To address this issue, we introduce \textbf{ProtoMM}, a training-free framework that constructs multimodal prototypes to adapt VLMs during the test time. By viewing the prototype as a discrete distribution over the textual descriptions and visual particles, ProtoMM has the ability to combine the multimodal features for comprehensive prototype learning. More importantly, the visual particles are dynamically updated as the testing stream flows. This allows our multimodal prototypes to continually learn from the data, enhancing their generalizability in unseen scenarios. In addition, we quantify the importance of the prototypes and test images by formulating their semantic distance as an optimal transport problem. Extensive experiments on 15 zero-shot benchmarks demonstrate the effectiveness of our method, achieving a 1.03\% average accuracy improvement over state-of-the-art methods on ImageNet and its variant datasets.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICAS: Detecting Training Data from Autoregressive Image Generative Models</title>
<link>https://arxiv.org/abs/2507.05068</link>
<guid>https://arxiv.org/abs/2507.05068</guid>
<content:encoded><![CDATA[
arXiv:2507.05068v2 Announce Type: replace 
Abstract: Autoregressive image generation has witnessed rapid advancements, with prominent models such as scale-wise visual auto-regression pushing the boundaries of visual synthesis. However, these developments also raise significant concerns regarding data privacy and copyright. In response, training data detection has emerged as a critical task for identifying unauthorized data usage in model training. To better understand the vulnerability of autoregressive image generative models to such detection, we conduct the first study applying membership inference to this domain. Our approach comprises two key components: implicit classification and an adaptive score aggregation strategy. First, we compute the implicit token-wise classification score within the query image. Then we propose an adaptive score aggregation strategy to acquire a final score, which places greater emphasis on the tokens with lower scores. A higher final score indicates that the sample is more likely to be involved in the training set. To validate the effectiveness of our method, we adapt existing detection algorithms originally designed for LLMs to visual autoregressive models. Extensive experiments demonstrate the superiority of our method in both class-conditional and text-to-image scenarios. Moreover, our approach exhibits strong robustness and generalization under various data transformations. Furthermore, sufficient experiments suggest two novel key findings: (1) A linear scaling law on membership inference, exposing the vulnerability of large foundation models. (2) Training data from scale-wise visual autoregressive models is easier to detect than other autoregressive paradigms. Our code is available at https://github.com/Chrisqcwx/ImageAR-MIA.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multigranular Evaluation for Brain Visual Decoding</title>
<link>https://arxiv.org/abs/2507.07993</link>
<guid>https://arxiv.org/abs/2507.07993</guid>
<content:encoded><![CDATA[
arXiv:2507.07993v2 Announce Type: replace 
Abstract: Existing evaluation protocols for brain visual decoding predominantly rely on coarse metrics that obscure inter-model differences, lack neuroscientific foundation, and fail to capture fine-grained visual distinctions. To address these limitations, we introduce BASIC, a unified, multigranular evaluation framework that jointly quantifies structural fidelity, inferential alignment, and contextual coherence between decoded and ground-truth images. For the structural level, we introduce a hierarchical suite of segmentation-based metrics, including foreground, semantic, instance, and component masks, anchored in granularity-aware correspondence across mask structures. For the semantic level, we extract structured scene representations encompassing objects, attributes, and relationships using multimodal large language models, enabling detailed, scalable, and context-rich comparisons with ground-truth stimuli. We benchmark a diverse set of visual decoding methods across multiple stimulus-neuroimaging datasets within this unified evaluation framework. Together, these criteria provide a more discriminative, interpretable, and comprehensive foundation for evaluating brain visual decoding methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Phase-Shifting Profilometry for Arbitrary Motion</title>
<link>https://arxiv.org/abs/2507.10009</link>
<guid>https://arxiv.org/abs/2507.10009</guid>
<content:encoded><![CDATA[
arXiv:2507.10009v2 Announce Type: replace 
Abstract: Phase-shifting profilometry (PSP) enables high-accuracy 3D reconstruction but remains highly susceptible to object motion. Although numerous studies have explored compensation for motion-induced errors, residual inaccuracies still persist, particularly in complex motion scenarios. In this paper, we propose a robust phase-shifting profilometry for arbitrary motion (RPSP-AM), including six-degrees-of-freedom (6-DoF) motion (translation and rotation in any direction), non-rigid deformations, and multi-target movements, achieving high-fidelity motion-error-free 3D reconstruction. We categorize motion errors into two components: 1) ghosting artifacts induced by image misalignment, and 2) ripple-like distortions induced by phase deviation. To eliminate the ghosting artifacts, we perform pixel-wise image alignment based on dense optical flow tracking. To correct ripple-like distortions, we propose a high-accuracy, low-complexity image-sequential binomial self-compensation (I-BSC) method, which performs a summation of the homogeneous fringe images weighted by binomial coefficients, exponentially reducing the ripple-like distortions with a competitive computational speed compared with the traditional four-step phase-shifting method. Extensive experimental results demonstrate that, under challenging conditions such as 6-DoF motion, non-rigid deformations, and multi-target movements, the proposed RPSP-AM outperforms state-of-the-art (SoTA) methods in compensating for both ghosting artifacts and ripple-like distortions. Our approach extends the applicability of PSP to arbitrary motion scenarios, endowing it with potential for widespread adoption in fields such as robotics, industrial inspection, and medical reconstruction.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VITA: Vision-to-Action Flow Matching Policy</title>
<link>https://arxiv.org/abs/2507.13231</link>
<guid>https://arxiv.org/abs/2507.13231</guid>
<content:encoded><![CDATA[
arXiv:2507.13231v3 Announce Type: replace 
Abstract: Conventional flow matching and diffusion-based policies sample through iterative denoising from standard noise distributions (e.g., Gaussian), and require conditioning modules to repeatedly incorporate visual information during the generative process, incurring substantial time and memory overhead. To reduce the complexity, we develop VITA(VIsion-To-Action policy), a noise-free and conditioning-free flow matching policy learning framework that directly flows from visual representations to latent actions. Since the source of the flow is visually grounded, VITA eliminates the need of visual conditioning during generation. As expected, bridging vision and action is challenging, because actions are lower-dimensional, less structured, and sparser than visual representations; moreover, flow matching requires the source and target to have the same dimensionality. To overcome this, we introduce an action autoencoder that maps raw actions into a structured latent space aligned with visual latents, trained jointly with flow matching. To further prevent latent space collapse, we propose flow latent decoding, which anchors the latent generation process by backpropagating the action reconstruction loss through the flow matching ODE (ordinary differential equation) solving steps. We evaluate VITA on 9 simulation and 5 real-world tasks from ALOHA and Robomimic. VITA achieves 1.5x-2x faster inference compared to conventional methods with conditioning modules, while outperforming or matching state-of-the-art policies. Codes, datasets, and demos are available at our project page: https://ucd-dare.github.io/VITA/.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking pig detection and tracking under diverse and challenging conditions</title>
<link>https://arxiv.org/abs/2507.16639</link>
<guid>https://arxiv.org/abs/2507.16639</guid>
<content:encoded><![CDATA[
arXiv:2507.16639v2 Announce Type: replace 
Abstract: To ensure animal welfare and effective management in pig farming, monitoring individual behavior is a crucial prerequisite. While monitoring tasks have traditionally been carried out manually, advances in machine learning have made it possible to collect individualized information in an increasingly automated way. Central to these methods is the localization of animals across space (object detection) and time (multi-object tracking). Despite extensive research of these two tasks in pig farming, a systematic benchmarking study has not yet been conducted. In this work, we address this gap by curating two datasets: PigDetect for object detection and PigTrack for multi-object tracking. The datasets are based on diverse image and video material from realistic barn conditions, and include challenging scenarios such as occlusions or bad visibility. For object detection, we show that challenging training images improve detection performance beyond what is achievable with randomly sampled images alone. Comparing different approaches, we found that state-of-the-art models offer substantial improvements in detection quality over real-time alternatives. For multi-object tracking, we observed that SORT-based methods achieve superior detection performance compared to end-to-end trainable models. However, end-to-end models show better association performance, suggesting they could become strong alternatives in the future. We also investigate characteristic failure cases of end-to-end models, providing guidance for future improvements. The detection and tracking models trained on our datasets perform well in unseen pens, suggesting good generalization capabilities. This highlights the importance of high-quality training data. The datasets and research code are made publicly available to facilitate reproducibility, re-use and further development.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA</title>
<link>https://arxiv.org/abs/2508.00453</link>
<guid>https://arxiv.org/abs/2508.00453</guid>
<content:encoded><![CDATA[
arXiv:2508.00453v2 Announce Type: replace 
Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to generate high-quality images that simultaneously possess rich spectral information and fine spatial details. However, due to the inherent trade-off between spectral and spatial information and the limited availability of observations, this task is fundamentally ill-posed. Previous studies have not effectively addressed the ill-posed nature caused by data misalignment. To tackle this challenge, we propose a fusion framework named PIF-Net, which explicitly incorporates ill-posed priors to effectively fuse multispectral images and hyperspectral images. To balance global spectral modeling with computational efficiency, we design a method based on an invertible Mamba architecture that maintains information consistency during feature transformation and fusion, ensuring stable gradient flow and process reversibility. Furthermore, we introduce a novel fusion module called the Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral and spatial features while keeping the model lightweight. Extensive experiments on multiple benchmark datasets demonstrate that PIF-Net achieves significantly better image restoration performance than current state-of-the-art methods while maintaining model efficiency.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatSSC: Decoupled Depth-Guided Gaussian Splatting for Semantic Scene Completion</title>
<link>https://arxiv.org/abs/2508.02261</link>
<guid>https://arxiv.org/abs/2508.02261</guid>
<content:encoded><![CDATA[
arXiv:2508.02261v2 Announce Type: replace 
Abstract: Monocular 3D Semantic Scene Completion (SSC) is a challenging yet promising task that aims to infer dense geometric and semantic descriptions of a scene from a single image. While recent object-centric paradigms significantly improve efficiency by leveraging flexible 3D Gaussian primitives, they still rely heavily on a large number of randomly initialized primitives, which inevitably leads to 1) inefficient primitive initialization and 2) outlier primitives that introduce erroneous artifacts. In this paper, we propose SplatSSC, a novel framework that resolves these limitations with a depth-guided initialization strategy and a principled Gaussian aggregator. Instead of random initialization, SplatSSC utilizes a dedicated depth branch composed of a Group-wise Multi-scale Fusion (GMF) module, which integrates multi-scale image and depth features to generate a sparse yet representative set of initial Gaussian primitives. To mitigate noise from outlier primitives, we develop the Decoupled Gaussian Aggregator (DGA), which enhances robustness by decomposing geometric and semantic predictions during the Gaussian-to-voxel splatting process. Complemented with a specialized Probability Scale Loss, our method achieves state-of-the-art performance on the Occ-ScanNet dataset, outperforming prior approaches by over 6.3% in IoU and 4.1% in mIoU, while reducing both latency and memory cost by more than 9.3%.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Rapid Contextual Learning in the Visual Cortex with Fast-Weight Deep Autoencoder Networks</title>
<link>https://arxiv.org/abs/2508.04988</link>
<guid>https://arxiv.org/abs/2508.04988</guid>
<content:encoded><![CDATA[
arXiv:2508.04988v2 Announce Type: replace 
Abstract: Recent neurophysiological studies have revealed that the early visual cortex can rapidly learn global image context, as evidenced by a sparsification of population responses and a reduction in mean activity when exposed to familiar versus novel image contexts. This phenomenon has been attributed primarily to local recurrent interactions, rather than changes in feedforward or feedback pathways, supported by both empirical findings and circuit-level modeling. Recurrent neural circuits capable of simulating these effects have been shown to reshape the geometry of neural manifolds, enhancing robustness and invariance to irrelevant variations. In this study, we employ a Vision Transformer (ViT)-based autoencoder to investigate, from a functional perspective, how familiarity training can induce sensitivity to global context in the early layers of a deep neural network. We hypothesize that rapid learning operates via fast weights, which encode transient or short-term memory traces, and we explore the use of Low-Rank Adaptation (LoRA) to implement such fast weights within each Transformer layer. Our results show that (1) The proposed ViT-based autoencoder's self-attention circuit performs a manifold transform similar to a neural circuit model of the familiarity effect. (2) Familiarity training aligns latent representations in early layers with those in the top layer that contains global context information. (3) Familiarity training broadens the self-attention scope within the remembered image context. (4) These effects are significantly amplified by LoRA-based fast weights. Together, these findings suggest that familiarity training introduces global sensitivity to earlier layers in a hierarchical network, and that a hybrid fast-and-slow weight architecture may provide a viable computational model for studying rapid global context learning in the brain.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion</title>
<link>https://arxiv.org/abs/2508.08679</link>
<guid>https://arxiv.org/abs/2508.08679</guid>
<content:encoded><![CDATA[
arXiv:2508.08679v2 Announce Type: replace 
Abstract: Multimodal medical image fusion (MMIF) aims to integrate images from different modalities to produce a comprehensive image that enhances medical diagnosis by accurately depicting organ structures, tissue textures, and metabolic information. Capturing both the unique and complementary information across multiple modalities simultaneously is a key research challenge in MMIF. To address this challenge, this paper proposes a novel image fusion method, MMIF-AMIN, which features a new architecture that can effectively extract these unique and complementary features. Specifically, an Invertible Dense Network (IDN) is employed for lossless feature extraction from individual modalities. To extract complementary information between modalities, a Multi-scale Complementary Feature Extraction Module (MCFEM) is designed, which incorporates a hybrid attention mechanism, convolutional layers of varying sizes, and Transformers. An adaptive loss function is introduced to guide model learning, addressing the limitations of traditional manually-designed loss functions and enhancing the depth of data mining. Extensive experiments demonstrate that MMIF-AMIN outperforms nine state-of-the-art MMIF methods, delivering superior results in both quantitative and qualitative analyses. Ablation experiments confirm the effectiveness of each component of the proposed method. Additionally, extending MMIF-AMIN to other image fusion tasks also achieves promising performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding</title>
<link>https://arxiv.org/abs/2508.11999</link>
<guid>https://arxiv.org/abs/2508.11999</guid>
<content:encoded><![CDATA[
arXiv:2508.11999v4 Announce Type: replace 
Abstract: With the rapid advancement of e-commerce, exploring general representations rather than task-specific ones has attracted increasing research attention. For product understanding, although existing discriminative dual-flow architectures drive progress in this field, they inherently struggle to model the many-to-one alignment between multiple images and texts of products. Therefore, we argue that generative Multimodal Large Language Models (MLLMs) hold significant potential for improving product representation learning. Nevertheless, achieving this goal still remains non-trivial due to several key challenges: the lack of multimodal and aspect-aware modeling modules in typical LLMs; the common presence of background noise in product images; and the absence of a standard benchmark for evaluation. To address these issues, we propose the first generative MLLM-based model named MOON for product representation learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for targeted modeling of multimodal and aspect-specific product content; (2) effectively detects core semantic regions in product images to mitigate the distraction and interference caused by background noise; and (3) introduces the specialized negative sampling strategy to increase the difficulty and diversity of negative samples. In addition, we release a large-scale multimodal benchmark MBE for various product understanding tasks. Experimentally, our model demonstrates competitive zero-shot performance on both our benchmark and the public dataset, showcasing strong generalization across various downstream tasks, including cross-modal retrieval, product classification, and attribute prediction. Furthermore, the case study and visualization illustrate the effectiveness of MOON for product understanding.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring</title>
<link>https://arxiv.org/abs/2508.12346</link>
<guid>https://arxiv.org/abs/2508.12346</guid>
<content:encoded><![CDATA[
arXiv:2508.12346v2 Announce Type: replace 
Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and Transformers for image deblurring. However, its flatten-and-scan strategy often results in local pixel forgetting and channel redundancy, limiting its ability to effectively aggregate 2D spatial information. Although existing methods mitigate this by modifying the scan strategy or incorporating local feature modules, it increase computational complexity and hinder real-time performance. In this paper, we propose a structure-aware image deblurring network without changing the original Mamba architecture. Specifically, we design a memory buffer mechanism to preserve historical information for later fusion, enabling reliable modeling of relevance between adjacent features. Additionally, we introduce an Ising-inspired regularization loss that simulates the energy minimization of the physical system's "mutual attraction" between pixels, helping to maintain image structure and coherence. Building on this, we develop MBMamba. Experimental results show that our method outperforms state-of-the-art approaches on widely used benchmarks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising</title>
<link>https://arxiv.org/abs/2509.06591</link>
<guid>https://arxiv.org/abs/2509.06591</guid>
<content:encoded><![CDATA[
arXiv:2509.06591v5 Announce Type: replace 
Abstract: Low-dose computed tomography (LDCT) and positron emission tomography (PET) have emerged as safer alternatives to conventional imaging modalities by significantly reducing radiation exposure. However, this reduction often results in increased noise and artifacts, which can compromise diagnostic accuracy. Consequently, denoising for LDCT/PET has become a vital area of research aimed at enhancing image quality while maintaining radiation safety. In this study, we introduce a novel Hybrid Swin Attention Network (HSANet), which incorporates Efficient Global Attention (EGA) modules and a hybrid upsampling module. The EGA modules enhance both spatial and channel-wise interaction, improving the network's capacity to capture relevant features, while the hybrid upsampling module mitigates the risk of overfitting to noise. We validate the proposed approach using a publicly available LDCT/PET dataset. Experimental results demonstrate that HSANet achieves superior denoising performance compared to existing methods, while maintaining a lightweight model size suitable for deployment on GPUs with standard memory configurations. This makes our approach highly practical for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Cross-Hierarchical Difference Feature Fusion Network Based on Multiscale Encoder-Decoder for Hyperspectral Change Detection</title>
<link>https://arxiv.org/abs/2509.16988</link>
<guid>https://arxiv.org/abs/2509.16988</guid>
<content:encoded><![CDATA[
arXiv:2509.16988v2 Announce Type: replace 
Abstract: Hyperspectral change detection (HCD) is one of the core applications of remote sensing images, holding significant research value in fields like environmental monitoring and disaster assessment. However, existing methods often suffer from incomplete capture of multiscale spatial-spectral features and insufficient fusion of differential feature information. To address these challenges, this paper proposes a Cross-Hierarchical Differential Feature Fusion Network (CHDFFN) based on a multiscale encoder-decoder. Firstly, a multiscale feature extraction subnetwork is designed, taking the customized encoder-decoder as the backbone, combined with residual connections and the proposed dual-core channel-spatial attention module to achieve multi-level extraction and initial integration of spatial-spectral features. The encoder embeds convolutional blocks with different receptive field sizes to capture multiscale representations from shallow details to deep semantics. The decoder fuses the encoder's output via skip connections to gradually restore spatial resolution while suppressing background noise and redundancy. To enhance the model's ability to capture differential features between bi-temporal hyperspectral images, a spatial-spectral change feature learning module is designed to learn hierarchical change representations. Additionally, an adaptive high-level feature fusion module is proposed, dynamically balancing the contribution of hierarchical differential features by adaptively assigning weights, which effectively strengthens the model's capability to characterize complex change patterns. Finally, experiments on four public hyperspectral datasets show that compared with some state-of-the-art methods, the average maximum improvements of OA, KC, and F1 are 4.61%, 19.79%, and 18.90% respectively, verifying the model's effectiveness.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting ResNet-based CLIP via Neuron-Attention Decomposition</title>
<link>https://arxiv.org/abs/2509.19943</link>
<guid>https://arxiv.org/abs/2509.19943</guid>
<content:encoded><![CDATA[
arXiv:2509.19943v3 Announce Type: replace 
Abstract: We present a novel technique for interpreting the neurons in CLIP-ResNet by decomposing their contributions to the output into individual computation paths. More specifically, we analyze all pairwise combinations of neurons and the following attention heads of CLIP's attention-pooling layer. We find that these neuron-head pairs can be approximated by a single direction in CLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret each neuron-head pair by associating it with text. Additionally, we find that only a sparse set of the neuron-head pairs have a significant contribution to the output value, and that some neuron-head pairs, while polysemantic, represent sub-concepts of their corresponding neurons. We use these observations for two applications. First, we employ the pairs for training-free semantic segmentation, outperforming previous methods for CLIP-ResNet. Second, we utilize the contributions of neuron-head pairs to monitor dataset distribution shifts. Our results demonstrate that examining individual computation paths in neural networks uncovers interpretable units, and that such units can be utilized for downstream tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2509.22544</link>
<guid>https://arxiv.org/abs/2509.22544</guid>
<content:encoded><![CDATA[
arXiv:2509.22544v3 Announce Type: replace 
Abstract: Video anomaly detection (VAD) is crucial for intelligent surveillance, but a significant challenge lies in identifying complex anomalies, which are events defined by intricate relationships and temporal dependencies among multiple entities rather than by isolated actions. While self-supervised learning (SSL) methods effectively model low-level spatiotemporal patterns, they often struggle to grasp the semantic meaning of these interactions. Conversely, large language models (LLMs) offer powerful contextual reasoning but are computationally expensive for frame-by-frame analysis and lack fine-grained spatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly Detection, a hybrid SSL-LLM model that combines a multi-task SSL temporal analyzer with LLM validator. The SSL module is built upon an nnFormer backbone which is a transformer-based model for image segmentation. It is trained with multiple proxy tasks, learns from video frames to identify those suspected of anomaly. The selected frames are then forwarded to the LLM, which enriches the analysis with semantic context by applying structured, rule-based reasoning to validate the presence of anomalies. Experiments on the challenging ComplexVAD dataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming existing baselines by 12.5% while reducing LLM computation. We release our interaction anomaly taxonomy, adaptive thresholding protocol, and code to facilitate future research in complex VAD scenarios.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3DLayout: A Multi-Source Dataset of 3D Indoor Layouts and Structured Descriptions for 3D Generation</title>
<link>https://arxiv.org/abs/2509.23728</link>
<guid>https://arxiv.org/abs/2509.23728</guid>
<content:encoded><![CDATA[
arXiv:2509.23728v2 Announce Type: replace 
Abstract: In text-driven 3D scene generation, object layout serves as a crucial intermediate representation that bridges high-level language instructions with detailed geometric output. It not only provides a structural blueprint for ensuring physical plausibility but also supports semantic controllability and interactive editing. However, the learning capabilities of current 3D indoor layout generation models are constrained by the limited scale, diversity, and annotation quality of existing datasets. To address this, we introduce M3DLayout, a large-scale, multi-source dataset for 3D indoor layout generation. M3DLayout comprises 21,367 layouts and over 433k object instances, integrating three distinct sources: real-world scans, professional CAD designs, and procedurally generated scenes. Each layout is paired with detailed structured text describing global scene summaries, relational placements of large furniture, and fine-grained arrangements of smaller items. This diverse and richly annotated resource enables models to learn complex spatial and semantic patterns across a wide variety of indoor environments. To assess the potential of M3DLayout, we establish a benchmark using both a text-conditioned diffusion model and a text-conditioned autoregressive model. Experimental results demonstrate that our dataset provides a solid foundation for training layout generation models. Its multi-source composition enhances diversity, notably through the Inf3DLayout subset which provides rich small-object information, enabling the generation of more complex and detailed scenes. We hope that M3DLayout can serve as a valuable resource for advancing research in text-driven 3D scene synthesis. All dataset and code will be made public upon acceptance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2509.24365</link>
<guid>https://arxiv.org/abs/2509.24365</guid>
<content:encoded><![CDATA[
arXiv:2509.24365v2 Announce Type: replace 
Abstract: Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at https://github.com/CURRENTF/Uni-X
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Generate Rigid Body Interactions with Video Diffusion Models</title>
<link>https://arxiv.org/abs/2510.02284</link>
<guid>https://arxiv.org/abs/2510.02284</guid>
<content:encoded><![CDATA[
arXiv:2510.02284v2 Announce Type: replace 
Abstract: Recent video generation models have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack object-level control mechanisms. To address these limitations, we introduce KineMask, an approach for video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predicted scene descriptions, leading to support for synthesis of complex dynamical phenomena. Our experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available. Project Page: https://daromog.github.io/KineMask/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training</title>
<link>https://arxiv.org/abs/2510.03189</link>
<guid>https://arxiv.org/abs/2510.03189</guid>
<content:encoded><![CDATA[
arXiv:2510.03189v2 Announce Type: replace 
Abstract: Interactive 3D biomedical image segmentation requires efficient models that can iteratively refine predictions based on user prompts. Current foundation models either lack volumetric awareness or suffer from limited interactive capabilities. We propose a training strategy that combines dynamic volumetric prompt generation with content-aware adaptive cropping to optimize the use of the image encoder. Our method simulates realistic user interaction patterns during training while addressing the computational challenges of learning from sequential refinement feedback on a single GPU. For efficient training, we initialize our network using the publicly available weights from the nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models for Interactive 3D Biomedical Image Segmentation} competition demonstrates strong performance with an average final Dice score of 0.6385, normalized surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice) and 2.5671 (NSD).
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5</title>
<link>https://arxiv.org/abs/2510.04003</link>
<guid>https://arxiv.org/abs/2510.04003</guid>
<content:encoded><![CDATA[
arXiv:2510.04003v2 Announce Type: replace 
Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital role in digitizing Vietnamese historical documents and enabling cross-lingual semantic research. However, existing OCR systems struggle with degraded scans, non-standard glyphs, and handwriting variations common in ancient sources. In this work, we propose a fine-tuning approach for PaddleOCRv5 to improve character recognition on Han-Nom texts. We retrain the text recognition module using a curated subset of ancient Vietnamese Chinese manuscripts, supported by a full training pipeline covering preprocessing, LMDB conversion, evaluation, and visualization. Experimental results show a significant improvement over the base model, with exact accuracy increasing from 37.5 percent to 50.0 percent, particularly under noisy image conditions. Furthermore, we develop an interactive demo that visually compares pre- and post-fine-tuning recognition results, facilitating downstream applications such as Han-Vietnamese semantic alignment, machine translation, and historical linguistics research. The demo is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global-to-local image quality assessment in optical microscopy via fast and robust deep learning predictions</title>
<link>https://arxiv.org/abs/2510.04859</link>
<guid>https://arxiv.org/abs/2510.04859</guid>
<content:encoded><![CDATA[
arXiv:2510.04859v2 Announce Type: replace 
Abstract: Optical microscopy is one of the most widely used techniques in research studies for life sciences and biomedicine. These applications require reliable experimental pipelines to extract valuable knowledge from the measured samples and must be supported by image quality assessment (IQA) to ensure correct processing and analysis of the image data. IQA methods are implemented with variable complexity. However, while most quality metrics have a straightforward implementation, they might be time consuming and computationally expensive when evaluating a large dataset. In addition, quality metrics are often designed for well-defined image features and may be unstable for images out of the ideal domain. To overcome these limitations, recent works have proposed deep learning-based IQA methods, which can provide superior performance, increased generalizability and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by previous studies and applies a deep convolutional neural network designed for IQA on natural images to optical microscopy measurements. We retrained the same architecture to predict individual quality metrics and global quality scores for optical microscopy data. The resulting models provide fast and stable predictions of image quality by generalizing quality estimation even outside the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA provides patch-wise prediction of image quality and can be used to visualize spatially varying quality in a single image. Our study demonstrates that optical microscopy-based studies can benefit from the generalizability of deep learning models due to their stable performance in the presence of outliers, the ability to assess small image patches, and rapid predictions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Have We Scene It All? Scene Graph-Aware Deep Point Cloud Compression</title>
<link>https://arxiv.org/abs/2510.08512</link>
<guid>https://arxiv.org/abs/2510.08512</guid>
<content:encoded><![CDATA[
arXiv:2510.08512v2 Announce Type: replace 
Abstract: Efficient transmission of 3D point cloud data is critical for advanced perception in centralized and decentralized multi-agent robotic systems, especially nowadays with the growing reliance on edge and cloud-based processing. However, the large and complex nature of point clouds creates challenges under bandwidth constraints and intermittent connectivity, often degrading system performance. We propose a deep compression framework based on semantic scene graphs. The method decomposes point clouds into semantically coherent patches and encodes them into compact latent representations with semantic-aware encoders conditioned by Feature-wise Linear Modulation (FiLM). A folding-based decoder, guided by latent features and graph node attributes, enables structurally accurate reconstruction. Experiments on the SemanticKITTI and nuScenes datasets show that the framework achieves state-of-the-art compression rates, reducing data size by up to 98% while preserving both structural and semantic fidelity. In addition, it supports downstream applications such as multi-robot pose graph optimization and map merging, achieving trajectory accuracy and map alignment comparable to those obtained with raw LiDAR scans.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding</title>
<link>https://arxiv.org/abs/2510.10194</link>
<guid>https://arxiv.org/abs/2510.10194</guid>
<content:encoded><![CDATA[
arXiv:2510.10194v2 Announce Type: replace 
Abstract: Localizing 3D objects using natural language is essential for robotic scene understanding. The descriptions often involve multiple spatial relationships to distinguish similar objects, making 3D-language alignment difficult. Current methods only model relationships for pairwise objects, ignoring the global perceptual significance of n-ary combinations in multi-modal relational understanding. To address this, we propose a novel progressive relational learning framework for 3D object grounding. We extend relational learning from binary to n-ary to identify visual relations that match the referential description globally. Given the absence of specific annotations for referred objects in the training data, we design a grouped supervision loss to facilitate n-ary relational learning. In the scene graph created with n-ary relationships, we use a multi-modal network with hybrid attention mechanisms to further localize the target within the n-ary combinations. Experiments and ablation studies on the ReferIt3D and ScanRefer benchmarks demonstrate that our method outperforms the state-of-the-art, and proves the advantages of the n-ary relational perception in 3D localization.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fast and Scalable Normal Integration using Continuous Components</title>
<link>https://arxiv.org/abs/2510.11508</link>
<guid>https://arxiv.org/abs/2510.11508</guid>
<content:encoded><![CDATA[
arXiv:2510.11508v2 Announce Type: replace 
Abstract: Surface normal integration is a fundamental problem in computer vision, dealing with the objective of reconstructing a surface from its corresponding normal map. Existing approaches require an iterative global optimization to jointly estimate the depth of each pixel, which scales poorly to larger normal maps. In this paper, we address this problem by recasting normal integration as the estimation of relative scales of continuous components. By constraining pixels belonging to the same component to jointly vary their scale, we drastically reduce the number of optimization variables. Our framework includes a heuristic to accurately estimate continuous components from the start, a strategy to rebalance optimization terms, and a technique to iteratively merge components to further reduce the size of the problem. Our method achieves state-of-the-art results on the standard normal integration benchmark in as little as a few seconds and achieves one-order-of-magnitude speedup over pixel-level approaches on large-resolution normal maps.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Capturing Context-Aware Route Choice Semantics for Trajectory Representation Learning</title>
<link>https://arxiv.org/abs/2510.14819</link>
<guid>https://arxiv.org/abs/2510.14819</guid>
<content:encoded><![CDATA[
arXiv:2510.14819v2 Announce Type: replace 
Abstract: Trajectory representation learning (TRL) aims to encode raw trajectory data into low-dimensional embeddings for downstream tasks such as travel time estimation, mobility prediction, and trajectory similarity analysis. From a behavioral perspective, a trajectory reflects a sequence of route choices within an urban environment. However, most existing TRL methods ignore this underlying decision-making process and instead treat trajectories as static, passive spatiotemporal sequences, thereby limiting the semantic richness of the learned representations. To bridge this gap, we propose CORE, a TRL framework that integrates context-aware route choice semantics into trajectory embeddings. CORE first incorporates a multi-granular Environment Perception Module, which leverages large language models (LLMs) to distill environmental semantics from point of interest (POI) distributions, thereby constructing a context-enriched road network. Building upon this backbone, CORE employs a Route Choice Encoder with a mixture-of-experts (MoE) architecture, which captures route choice patterns by jointly leveraging the context-enriched road network and navigational factors. Finally, a Transformer encoder aggregates the route-choice-aware representations into a global trajectory embedding. Extensive experiments on 4 real-world datasets across 6 downstream tasks demonstrate that CORE consistently outperforms 12 state-of-the-art TRL methods, achieving an average improvement of 9.79% over the best-performing baseline. Our code is available at https://github.com/caoji2001/CORE.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Survey on World Models for Embodied AI</title>
<link>https://arxiv.org/abs/2510.16732</link>
<guid>https://arxiv.org/abs/2510.16732</guid>
<content:encoded><![CDATA[
arXiv:2510.16732v2 Announce Type: replace 
Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions reshape future world states. World models serve as internal simulators that capture environment dynamics, enabling forward and counterfactual rollouts to support perception, prediction, and decision making. This survey presents a unified framework for world models in embodied AI. Specifically, we formalize the problem setting and learning objectives, and propose a three-axis taxonomy encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2) Temporal Modeling, Sequential Simulation and Inference vs. Global Difference Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We systematize data resources and metrics across robotics, autonomous driving, and general video settings, covering pixel prediction quality, state-level understanding, and task performance. Furthermore, we offer a quantitative comparison of state-of-the-art models and distill key open challenges, including the scarcity of unified datasets and the need for evaluation metrics that assess physical consistency over pixel fidelity, the trade-off between model performance and the computational efficiency required for real-time control, and the core modeling difficulty of achieving long-horizon temporal consistency while mitigating error accumulation. Finally, we maintain a curated bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning</title>
<link>https://arxiv.org/abs/2510.19622</link>
<guid>https://arxiv.org/abs/2510.19622</guid>
<content:encoded><![CDATA[
arXiv:2510.19622v2 Announce Type: replace 
Abstract: Existing Moment Retrieval methods face three critical bottlenecks: (1) data scarcity forces models into shallow keyword-feature associations; (2) boundary ambiguity in transition regions between adjacent events; (3) insufficient discrimination of fine-grained semantics (e.g., distinguishing ``kicking" vs. ``throwing" a ball). In this paper, we propose a zero-external-dependency Augmented Moment Retrieval framework, AMR, designed to overcome local optima caused by insufficient data annotations and the lack of robust boundary and semantic discrimination capabilities. AMR is built upon two key insights: (1) it resolves ambiguous boundary information and semantic confusion in existing annotations without additional data (avoiding costly manual labeling), and (2) it preserves boundary and semantic discriminative capabilities enhanced by training while generalizing to real-world scenarios, significantly improving performance. Furthermore, we propose a two-stage training framework with cold-start and distillation adaptation. The cold-start stage employs curriculum learning on augmented data to build foundational boundary/semantic awareness. The distillation stage introduces dual query sets: Original Queries maintain DETR-based localization using frozen Base Queries from the cold-start model, while Active Queries dynamically adapt to real-data distributions. A cross-stage distillation loss enforces consistency between Original and Base Queries, preventing knowledge forgetting while enabling real-world generalization. Experiments on multiple benchmarks show that AMR achieves improved performance over prior state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrainPuzzle: Hybrid Physics and Data-Driven Reconstruction for Transcranial Ultrasound Tomography</title>
<link>https://arxiv.org/abs/2510.20029</link>
<guid>https://arxiv.org/abs/2510.20029</guid>
<content:encoded><![CDATA[
arXiv:2510.20029v2 Announce Type: replace 
Abstract: Ultrasound brain imaging remains challenging due to the large difference in sound speed between the skull and brain tissues and the difficulty of coupling large probes to the skull. This work aims to achieve quantitative transcranial ultrasound by reconstructing an accurate speed-of-sound (SoS) map of the brain. Traditional physics-based full-waveform inversion (FWI) is limited by weak signals caused by skull-induced attenuation, mode conversion, and phase aberration, as well as incomplete spatial coverage since full-aperture arrays are clinically impractical. In contrast, purely data-driven methods that learn directly from raw ultrasound data often fail to model the complex nonlinear and nonlocal wave propagation through bone, leading to anatomically plausible but quantitatively biased SoS maps under low signal-to-noise and sparse-aperture conditions. To address these issues, we propose BrainPuzzle, a hybrid two-stage framework that combines physical modeling with machine learning. In the first stage, reverse time migration (time-reversal acoustics) is applied to multi-angle acquisitions to produce migration fragments that preserve structural details even under low SNR. In the second stage, a transformer-based super-resolution encoder-decoder with a graph-based attention unit (GAU) fuses these fragments into a coherent and quantitatively accurate SoS image. A partial-array acquisition strategy using a movable low-count transducer set improves feasibility and coupling, while the hybrid algorithm compensates for the missing aperture. Experiments on two synthetic datasets show that BrainPuzzle achieves superior SoS reconstruction accuracy and image completeness, demonstrating its potential for advancing quantitative ultrasound brain imaging.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering</title>
<link>https://arxiv.org/abs/2510.20285</link>
<guid>https://arxiv.org/abs/2510.20285</guid>
<content:encoded><![CDATA[
arXiv:2510.20285v2 Announce Type: replace 
Abstract: Egocentric Video Question Answering (Egocentric VideoQA) plays an important role in egocentric video understanding, which refers to answering questions based on first-person videos. Although existing methods have made progress through the paradigm of pre-training and fine-tuning, they ignore the unique challenges posed by the first-person perspective, such as understanding multiple events and recognizing hand-object interactions. To deal with these challenges, we propose a Dual-Modal Counterfactual Contrastive Construction (DMC$^3$) framework, which contains an egocentric videoqa baseline, a counterfactual sample construction module and a counterfactual sample-involved contrastive optimization. Specifically, We first develop a counterfactual sample construction module to generate positive and negative samples for textual and visual modalities through event description paraphrasing and core interaction mining, respectively. Then, We feed these samples together with the original samples into the baseline. Finally, in the counterfactual sample-involved contrastive optimization module, we apply contrastive loss to minimize the distance between the original sample features and the positive sample features, while maximizing the distance from the negative samples. Experiments show that our method achieve 52.51\% and 46.04\% on the \textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% on QAEGO4D, both reaching the state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</title>
<link>https://arxiv.org/abs/2510.20812</link>
<guid>https://arxiv.org/abs/2510.20812</guid>
<content:encoded><![CDATA[
arXiv:2510.20812v2 Announce Type: replace 
Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Face-MakeUpV2: Facial Consistency Learning for Controllable Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2510.21775</link>
<guid>https://arxiv.org/abs/2510.21775</guid>
<content:encoded><![CDATA[
arXiv:2510.21775v2 Announce Type: replace 
Abstract: In facial image generation, current text-to-image models often suffer from facial attribute leakage and insufficient physical consistency when responding to local semantic instructions. In this study, we propose Face-MakeUpV2, a facial image generation model that aims to maintain the consistency of face ID and physical characteristics with the reference image. First, we constructed a large-scale dataset FaceCaptionMask-1M comprising approximately one million image-text-masks pairs that provide precise spatial supervision for the local semantic instructions. Second, we employed a general text-to-image pretrained model as the backbone and introduced two complementary facial information injection channels: a 3D facial rendering channel to incorporate the physical characteristics of the image and a global facial feature channel. Third, we formulated two optimization objectives for the supervised learning of our model: semantic alignment in the model's embedding space to mitigate the attribute leakage problem and perceptual loss on facial images to preserve ID consistency. Extensive experiments demonstrated that our Face-MakeUpV2 achieves best overall performance in terms of preserving face ID and maintaining physical consistency of the reference images. These results highlight the practical potential of Face-MakeUpV2 for reliable and controllable facial editing in diverse applications.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HARMONY: Hidden Activation Representations and Model Output-Aware Uncertainty Estimation for Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.22171</link>
<guid>https://arxiv.org/abs/2510.22171</guid>
<content:encoded><![CDATA[
arXiv:2510.22171v2 Announce Type: replace 
Abstract: Uncertainty Estimation (UE) plays a central role in quantifying the reliability of model outputs and reducing unsafe generations via selective prediction. In this regard, most existing probability-based UE approaches rely on predefined functions, aggregating token probabilities into a single UE score using heuristics such as length-normalization. However, these methods often fail to capture the complex relationships between generated tokens and struggle to identify biased probabilities often influenced by \textbf{language priors}. Another line of research uses hidden representations of the model and trains simple MLP architectures to predict uncertainty. However, such functions often lose the intricate \textbf{ inter-token dependencies}. While prior works show that hidden representations encode multimodal alignment signals, our work demonstrates that how these signals are processed has a significant impact on the UE performance. To effectively leverage these signals to identify inter-token dependencies, and vision-text alignment, we propose \textbf{HARMONY} (Hidden Activation Representations and Model Output-Aware Uncertainty Estimation for Vision-Language Models), a novel UE framework that integrates generated tokens ('text'), model's uncertainty score at the output ('MaxProb'), and its internal belief on the visual understanding of the image and the generated token (captured by 'hidden representations') at token level via appropriate input mapping design and suitable architecture choice. Our experimental experiments across two open-ended VQA benchmarks (A-OKVQA, and VizWiz) and four state-of-the-art VLMs (LLaVA-7B, LLaVA-13B, InstructBLIP, and Qwen-VL) show that HARMONY consistently matches or surpasses existing approaches, achieving up to 5\% improvement in AUROC and 9\% in PRR.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynamicTree: Interactive Real Tree Animation via Sparse Voxel Spectrum</title>
<link>https://arxiv.org/abs/2510.22213</link>
<guid>https://arxiv.org/abs/2510.22213</guid>
<content:encoded><![CDATA[
arXiv:2510.22213v2 Announce Type: replace 
Abstract: Generating dynamic and interactive 3D trees has wide applications in virtual reality, games, and world simulation. However, existing methods still face various challenges in generating structurally consistent and realistic 4D motion for complex real trees. In this paper, we propose DynamicTree, the first framework that can generate long-term, interactive 3D motion for 3DGS reconstructions of real trees. Unlike prior optimization-based methods, our approach generates dynamics in a fast feed-forward manner. The key success of our approach is the use of a compact sparse voxel spectrum to represent the tree movement. Given a 3D tree from Gaussian Splatting reconstruction, our pipeline first generates mesh motion using the sparse voxel spectrum and then binds Gaussians to deform the mesh. Additionally, the proposed sparse voxel spectrum can also serve as a basis for fast modal analysis under external forces, allowing real-time interactive responses. To train our model, we also introduce 4DTree, the first large-scale synthetic 4D tree dataset containing 8,786 animated tree meshes with 100-frame motion sequences. Extensive experiments demonstrate that our method achieves realistic and responsive tree animations, significantly outperforming existing approaches in both visual quality and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error Detection</title>
<link>https://arxiv.org/abs/2510.23594</link>
<guid>https://arxiv.org/abs/2510.23594</guid>
<content:encoded><![CDATA[
arXiv:2510.23594v4 Announce Type: replace 
Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress on vision-language tasks, yet their reasoning processes remain sometimes unreliable. We introduce PRISM-Bench, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning</title>
<link>https://arxiv.org/abs/2510.23907</link>
<guid>https://arxiv.org/abs/2510.23907</guid>
<content:encoded><![CDATA[
arXiv:2510.23907v2 Announce Type: replace 
Abstract: Scene-level captioning in instructional videos can enhance learning by requiring an understanding of both visual cues and temporal structure. By aligning visual cues with textual guidance, this understanding supports procedural learning and multimodal reasoning, providing a richer context for skill acquisition. However, captions that fail to capture this structure may lack coherence and quality, which can create confusion and undermine the video's educational intent. To address this gap, we introduce DynaStride, a pipeline to generate coherent, scene-level captions without requiring manual scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride performs adaptive frame sampling and multimodal windowing to capture key transitions within each scene. It then employs a multimodal chain-of-thought process to produce multiple action-object pairs, which are refined and fused using a dynamic stride window selection algorithm that adaptively balances temporal context and redundancy. The final scene-level caption integrates visual semantics and temporal reasoning in a single instructional caption. Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o, demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses further show that DynaStride produces captions that are more temporally coherent and informative, suggesting a promising direction for improving AI-powered instructional content generation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting</title>
<link>https://arxiv.org/abs/2510.27680</link>
<guid>https://arxiv.org/abs/2510.27680</guid>
<content:encoded><![CDATA[
arXiv:2510.27680v2 Announce Type: replace 
Abstract: Generating automated reports for 3D positron emission tomography (PET) is an important and challenging task in medical imaging. PET plays a vital role in oncology, but automating report generation is difficult due to the complexity of whole-body 3D volumes, the wide range of potential clinical findings, and the limited availability of annotated datasets. To address these challenges, we introduce PETARSeg-11K, the first large-scale, publicly available dataset that provides lesion-level correspondence between 3D PET/CT volumes and free-text radiological findings. It comprises 11,356 lesion descriptions paired with 3D segmentations. Second, we propose PETAR-4B, a 3D vision-language model designed for mask-aware, spatially grounded PET/CT reporting. PETAR-4B jointly encodes PET, CT, and 3D lesion segmentation masks, using a 3D focal prompt to capture fine-grained details of lesions that normally comprise less than 0.1% of the volume. Evaluations using automated metrics show PETAR-4B substantially outperforming all 2D and 3D baselines. A human study involving five physicians -- the first of its kind for automated PET reporting -- confirms the model's clinical utility and establishes correlations between automated metrics and expert judgment. This work provides a foundational dataset and a novel architecture, advancing 3D medical vision-language understanding in PET.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3EED: Ground Everything Everywhere in 3D</title>
<link>https://arxiv.org/abs/2511.01755</link>
<guid>https://arxiv.org/abs/2511.01755</guid>
<content:encoded><![CDATA[
arXiv:2511.01755v2 Announce Type: replace 
Abstract: Visual grounding in 3D is the key for embodied agents to localize language-referred objects in open-world environments. However, existing benchmarks are limited to indoor focus, single-platform constraints, and small scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We provide over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes -- 10x larger than existing datasets. We develop a scalable annotation pipeline combining vision-language model prompting with human verification to ensure high-quality spatial grounding. To support cross-platform learning, we propose platform-aware normalization and cross-modal alignment techniques, and establish benchmark protocols for in-domain and cross-platform evaluations. Our findings reveal significant performance gaps, highlighting the challenges and opportunities of generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released to advance future research in language-driven 3D embodied perception.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Image Restoration via Progressive PDE Integration</title>
<link>https://arxiv.org/abs/2511.06244</link>
<guid>https://arxiv.org/abs/2511.06244</guid>
<content:encoded><![CDATA[
arXiv:2511.06244v2 Announce Type: replace 
Abstract: Motion blur, caused by relative movement between camera and scene during exposure, significantly degrades image quality and impairs downstream computer vision tasks such as object detection, tracking, and recognition in dynamic environments. While deep learning-based motion deblurring methods have achieved remarkable progress, existing approaches face fundamental challenges in capturing the long-range spatial dependencies inherent in motion blur patterns. Traditional convolutional methods rely on limited receptive fields and require extremely deep networks to model global spatial relationships. These limitations motivate the need for alternative approaches that incorporate physical priors to guide feature evolution during restoration. In this paper, we propose a progressive training framework that integrates physics-informed PDE dynamics into state-of-the-art restoration architectures. By leveraging advection-diffusion equations to model feature evolution, our approach naturally captures the directional flow characteristics of motion blur while enabling principled global spatial modeling. Our PDE-enhanced deblurring models achieve superior restoration quality with minimal overhead, adding only approximately 1\% to inference GMACs while providing consistent improvements in perceptual quality across multiple state-of-the-art architectures. Comprehensive experiments on standard motion deblurring benchmarks demonstrate that our physics-informed approach improves PSNR and SSIM significantly across four diverse architectures, including FFTformer, NAFNet, Restormer, and Stripformer. These results validate that incorporating mathematical physics principles through PDE-based global layers can enhance deep learning-based image restoration, establishing a promising direction for physics-informed neural network design in computer vision applications.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross Modal Fine-Grained Alignment via Granularity-Aware and Region-Uncertain Modeling</title>
<link>https://arxiv.org/abs/2511.07710</link>
<guid>https://arxiv.org/abs/2511.07710</guid>
<content:encoded><![CDATA[
arXiv:2511.07710v3 Announce Type: replace 
Abstract: Fine-grained image-text alignment is a pivotal challenge in multimodal learning, underpinning key applications such as visual question answering, image captioning, and vision-language navigation. Unlike global alignment, fine-grained alignment requires precise correspondence between localized visual regions and textual tokens, often hindered by noisy attention mechanisms and oversimplified modeling of cross-modal relationships. In this work, we identify two fundamental limitations of existing approaches: the lack of robust intra-modal mechanisms to assess the significance of visual and textual tokens, leading to poor generalization in complex scenes; and the absence of fine-grained uncertainty modeling, which fails to capture the one-to-many and many-to-one nature of region-word correspondences. To address these issues, we propose a unified approach that incorporates significance-aware and granularity-aware modeling and region-level uncertainty modeling. Our method leverages modality-specific biases to identify salient features without relying on brittle cross-modal attention, and represents region features as a mixture of Gaussian distributions to capture fine-grained uncertainty. Extensive experiments on Flickr30K and MS-COCO demonstrate that our approach achieves state-of-the-art performance across various backbone architectures, significantly enhancing the robustness and interpretability of fine-grained image-text alignment.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Randomness: Understand the Order of the Noise in Diffusion</title>
<link>https://arxiv.org/abs/2511.07756</link>
<guid>https://arxiv.org/abs/2511.07756</guid>
<content:encoded><![CDATA[
arXiv:2511.07756v2 Announce Type: replace 
Abstract: In text-driven content generation (T2C) diffusion model, semantic of generated content is mostly attributed to the process of text embedding and attention mechanism interaction. The initial noise of the generation process is typically characterized as a random element that contributes to the diversity of the generated content. Contrary to this view, this paper reveals that beneath the random surface of noise lies strong analyzable patterns. Specifically, this paper first conducts a comprehensive analysis of the impact of random noise on the model's generation. We found that noise not only contains rich semantic information, but also allows for the erasure of unwanted semantics from it in an extremely simple way based on information theory, and using the equivalence between the generation process of diffusion model and semantic injection to inject semantics into the cleaned noise. Then, we mathematically decipher these observations and propose a simple but efficient training-free and universal two-step "Semantic Erasure-Injection" process to modulate the initial noise in T2C diffusion model. Experimental results demonstrate that our method is consistently effective across various T2C models based on both DiT and UNet architectures and presents a novel perspective for optimizing the generation of diffusion model, providing a universal tool for consistent generation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification</title>
<link>https://arxiv.org/abs/2511.08711</link>
<guid>https://arxiv.org/abs/2511.08711</guid>
<content:encoded><![CDATA[
arXiv:2511.08711v2 Announce Type: replace 
Abstract: Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STORM: Segment, Track, and Object Re-Localization from a Single Image</title>
<link>https://arxiv.org/abs/2511.09771</link>
<guid>https://arxiv.org/abs/2511.09771</guid>
<content:encoded><![CDATA[
arXiv:2511.09771v2 Announce Type: replace 
Abstract: Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots. However, existing approaches typically require a pre-defined 3D model of the target and rely on a manually annotated segmentation mask in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement. To address these limitations, we propose STORM (Segment, Track, and Object Re-localization from a single iMage), an open-source robust real-time 6D pose estimation system that requires no manual annotation. STORM employs a novel three-stage pipeline combining vision-language understanding with feature matching: contextual object descriptions guide localization, self-cross-attention mechanisms identify candidate regions, and produce precise masks and 3D models for accurate pose estimation. Another key innovation is our automatic re-registration mechanism that detects tracking failures through feature similarity monitoring and recovers from severe occlusions or rapid motion. STORM achieves state-of-the-art accuracy on challenging industrial datasets featuring multi-object occlusions, high-speed motion, and varying illumination, while operating at real-time speeds without additional training. This annotation-free approach significantly reduces deployment overhead, providing a practical solution for modern applications, such as flexible manufacturing and intelligent quality control.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PANDA -- Patch And Distribution-Aware Augmentation for Long-Tailed Exemplar-Free Continual Learning</title>
<link>https://arxiv.org/abs/2511.09791</link>
<guid>https://arxiv.org/abs/2511.09791</guid>
<content:encoded><![CDATA[
arXiv:2511.09791v2 Announce Type: replace 
Abstract: Exemplar-Free Continual Learning (EFCL) restricts the storage of previous task data and is highly susceptible to catastrophic forgetting. While pre-trained models (PTMs) are increasingly leveraged for EFCL, existing methods often overlook the inherent imbalance of real-world data distributions. We discovered that real-world data streams commonly exhibit dual-level imbalances, dataset-level distributions combined with extreme or reversed skews within individual tasks, creating both intra-task and inter-task disparities that hinder effective learning and generalization. To address these challenges, we propose PANDA, a Patch-and-Distribution-Aware Augmentation framework that integrates seamlessly with existing PTM-based EFCL methods. PANDA amplifies low-frequency classes by using a CLIP encoder to identify representative regions and transplanting those into frequent-class samples within each task. Furthermore, PANDA incorporates an adaptive balancing strategy that leverages prior task distributions to smooth inter-task imbalances, reducing the overall gap between average samples across tasks and enabling fairer learning with frozen PTMs. Extensive experiments and ablation studies demonstrate PANDA's capability to work with existing PTM-based CL methods, improving accuracy and reducing catastrophic forgetting.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Gaussian Representation Learning for Medical Action Evaluation</title>
<link>https://arxiv.org/abs/2511.10060</link>
<guid>https://arxiv.org/abs/2511.10060</guid>
<content:encoded><![CDATA[
arXiv:2511.10060v2 Announce Type: replace 
Abstract: Fine-grained action evaluation in medical vision faces unique challenges due to the unavailability of comprehensive datasets, stringent precision requirements, and insufficient spatiotemporal dynamic modeling of very rapid actions. To support development and evaluation, we introduce CPREval-6k, a multi-view, multi-label medical action benchmark containing 6,372 expert-annotated videos with 22 clinical labels. Using this dataset, we present GaussMedAct, a multivariate Gaussian encoding framework, to advance medical motion analysis through adaptive spatiotemporal representation learning. Multivariate Gaussian Representation projects the joint motions to a temporally scaled multi-dimensional space, and decomposes actions into adaptive 3D Gaussians that serve as tokens. These tokens preserve motion semantics through anisotropic covariance modeling while maintaining robustness to spatiotemporal noise. Hybrid Spatial Encoding, employing a Cartesian and Vector dual-stream strategy, effectively utilizes skeletal information in the form of joint and bone features. The proposed method achieves 92.1% Top-1 accuracy with real-time inference on the benchmark, outperforming baseline by +5.9% accuracy with only 10% FLOPs. Cross-dataset experiments confirm the superiority of our method in robustness.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiNeXt: Revisiting LiDAR Completion with Efficient Non-Diffusion Architectures</title>
<link>https://arxiv.org/abs/2511.10209</link>
<guid>https://arxiv.org/abs/2511.10209</guid>
<content:encoded><![CDATA[
arXiv:2511.10209v2 Announce Type: replace 
Abstract: 3D LiDAR scene completion from point clouds is a fundamental component of perception systems in autonomous vehicles. Previous methods have predominantly employed diffusion models for high-fidelity reconstruction. However, their multi-step iterative sampling incurs significant computational overhead, limiting its real-time applicability. To address this, we propose LiNeXt-a lightweight, non-diffusion network optimized for rapid and accurate point cloud completion. Specifically, LiNeXt first applies the Noise-to-Coarse (N2C) Module to denoise the input noisy point cloud in a single pass, thereby obviating the multi-step iterative sampling of diffusion-based methods. The Refine Module then takes the coarse point cloud and its intermediate features from the N2C Module to perform more precise refinement, further enhancing structural completeness. Furthermore, we observe that LiDAR point clouds exhibit a distance-dependent spatial distribution, being densely sampled at proximal ranges and sparsely sampled at distal ranges. Accordingly, we propose the Distance-aware Selected Repeat strategy to generate a more uniformly distributed noisy point cloud. On the SemanticKITTI dataset, LiNeXt achieves a 199.8x speedup in inference, reduces Chamfer Distance by 50.7%, and uses only 6.1% of the parameters compared with LiDiff. These results demonstrate the superior efficiency and effectiveness of LiNeXt for real-time scene completion.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GFT: Graph Feature Tuning for Efficient Point Cloud Analysis</title>
<link>https://arxiv.org/abs/2511.10799</link>
<guid>https://arxiv.org/abs/2511.10799</guid>
<content:encoded><![CDATA[
arXiv:2511.10799v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) significantly reduces computational and memory costs by updating only a small subset of the model's parameters, enabling faster adaptation to new tasks with minimal loss in performance. Previous studies have introduced PEFTs tailored for point cloud data, as general approaches are suboptimal. To further reduce the number of trainable parameters, we propose a point-cloud-specific PEFT, termed Graph Features Tuning (GFT), which learns a dynamic graph from initial tokenized inputs of the transformer using a lightweight graph convolution network and passes these graph features to deeper layers via skip connections and efficient cross-attention modules. Extensive experiments on object classification and segmentation tasks show that GFT operates in the same domain, rivalling existing methods, while reducing the trainable parameters. Code is available at https://github.com/manishdhakal/GFT.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Deep Convolutional Multi-Type Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.11165</link>
<guid>https://arxiv.org/abs/2511.11165</guid>
<content:encoded><![CDATA[
arXiv:2511.11165v2 Announce Type: replace 
Abstract: Explainable anomaly detection methods often have the capability to identify and spatially localise anomalies within an image but lack the capability to differentiate the type of anomaly. Furthermore, they often require the costly training and maintenance of separate models for each object category. The lack of specificity is a significant research gap because identifying the type of anomaly (e.g., "Crack" vs. "Scratch") is crucial for accurate diagnosis that facilitates cost-saving operational decisions across diverse application domains. While some recent large-scale Vision-Language Models (VLMs) have begun to address this, they are computationally intensive and memory-heavy, restricting their use in real-time or embedded systems. We propose MultiTypeFCDD, a simple and lightweight convolutional framework designed as a practical alternative for explainable multi-type anomaly detection. MultiTypeFCDD uses only image-level labels to learn and produce multi-channel heatmaps, where each channel is trained to correspond to a specific anomaly type. The model functions as a single, unified framework capable of differentiating anomaly types across multiple object categories, eliminating the need to train and manage separate models for each object category. We evaluated our proposed method on the Real-IAD dataset and it delivers competitive results (96.4% I-AUROC) at just over 1% the size of state-of-the-art VLM models used for similar tasks. This makes it a highly practical and viable solution for real-world applications where computational resources are tightly constrained.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Efficient Mixture-of-Experts for Remote Sensing Modality-Missing Classification</title>
<link>https://arxiv.org/abs/2511.11460</link>
<guid>https://arxiv.org/abs/2511.11460</guid>
<content:encoded><![CDATA[
arXiv:2511.11460v2 Announce Type: replace 
Abstract: Multimodal classification in remote sensing often suffers from missing modalities caused by environmental interference, sensor failures, or atmospheric effects, which severely degrade classification performance. Existing two-stage adaptation methods are computationally expensive and assume complete multimodal data during training, limiting their generalization to real-world incompleteness. To overcome these issues, we propose a Missing-aware Mixture-of-Loras (MaMOL) framework that reformulates modality missing as a multi-task learning problem. MaMOL introduces a dual-routing mechanism: a task-oriented dynamic router that adaptively activates experts for different missing patterns, and a modality-specific-shared static router that maintains stable cross-modal knowledge sharing. Unlike prior methods that train separate networks for each missing configuration, MaMOL achieves parameter-efficient adaptation via lightweight expert updates and shared expert reuse. Experiments on multiple remote sensing benchmarks demonstrate superior robustness and generalization under varying missing rates, with minimal computational overhead. Moreover, transfer experiments on natural image datasets validate its scalability and cross-domain applicability, highlighting MaMOL as a general and efficient solution for incomplete multimodal learning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound</title>
<link>https://arxiv.org/abs/2511.12077</link>
<guid>https://arxiv.org/abs/2511.12077</guid>
<content:encoded><![CDATA[
arXiv:2511.12077v2 Announce Type: replace 
Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective</title>
<link>https://arxiv.org/abs/2511.12170</link>
<guid>https://arxiv.org/abs/2511.12170</guid>
<content:encoded><![CDATA[
arXiv:2511.12170v2 Announce Type: replace 
Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation</title>
<link>https://arxiv.org/abs/2511.12200</link>
<guid>https://arxiv.org/abs/2511.12200</guid>
<content:encoded><![CDATA[
arXiv:2511.12200v2 Announce Type: replace 
Abstract: Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.12263</link>
<guid>https://arxiv.org/abs/2511.12263</guid>
<content:encoded><![CDATA[
arXiv:2511.12263v2 Announce Type: replace 
Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction</title>
<link>https://arxiv.org/abs/2511.12578</link>
<guid>https://arxiv.org/abs/2511.12578</guid>
<content:encoded><![CDATA[
arXiv:2511.12578v2 Announce Type: replace 
Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2511.13853</link>
<guid>https://arxiv.org/abs/2511.13853</guid>
<content:encoded><![CDATA[
arXiv:2511.13853v2 Announce Type: replace 
Abstract: While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation</title>
<link>https://arxiv.org/abs/2511.14712</link>
<guid>https://arxiv.org/abs/2511.14712</guid>
<content:encoded><![CDATA[
arXiv:2511.14712v2 Announce Type: replace 
Abstract: The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Ranks with Degradation-Aware Routing for One-Step Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2511.16024</link>
<guid>https://arxiv.org/abs/2511.16024</guid>
<content:encoded><![CDATA[
arXiv:2511.16024v2 Announce Type: replace 
Abstract: The demonstrated success of sparsely-gated Mixture-of-Experts (MoE) architectures, exemplified by models such as DeepSeek and Grok, has motivated researchers to investigate their adaptation to diverse domains. In real-world image super-resolution (Real-ISR), existing approaches mainly rely on fine-tuning pre-trained diffusion models through Low-Rank Adaptation (LoRA) module to reconstruct high-resolution (HR) images. However, these dense Real-ISR models are limited in their ability to adaptively capture the heterogeneous characteristics of complex real-world degraded samples or enable knowledge sharing between inputs under equivalent computational budgets. To address this, we investigate the integration of sparse MoE into Real-ISR and propose a Mixture-of-Ranks (MoR) architecture for single-step image super-resolution. We introduce a fine-grained expert partitioning strategy that treats each rank in LoRA as an independent expert. This design enables flexible knowledge recombination while isolating fixed-position ranks as shared experts to preserve common-sense features and minimize routing redundancy. Furthermore, we develop a degradation estimation module leveraging CLIP embeddings and predefined positive-negative text pairs to compute relative degradation scores, dynamically guiding expert activation. To better accommodate varying sample complexities, we incorporate zero-expert slots and propose a degradation-aware load-balancing loss, which dynamically adjusts the number of active experts based on degradation severity, ensuring optimal computational resource allocation. Comprehensive experiments validate our framework's effectiveness and state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Off the Planckian Locus: Using 2D Chromaticity to Improve In-Camera Color</title>
<link>https://arxiv.org/abs/2511.17133</link>
<guid>https://arxiv.org/abs/2511.17133</guid>
<content:encoded><![CDATA[
arXiv:2511.17133v2 Announce Type: replace 
Abstract: Traditional in-camera colorimetric mapping relies on correlated color temperature (CCT)-based interpolation between pre-calibrated transforms optimized for Planckian illuminants such as CIE A and D65. However, modern lighting technologies such as LEDs can deviate substantially from the Planckian locus, exposing the limitations of relying on conventional one-dimensional CCT for illumination characterization. This paper demonstrates that transitioning from 1D CCT (on the Planckian locus) to a 2D chromaticity space (off the Planckian locus) improves colorimetric accuracy across various mapping approaches. In addition, we replace conventional CCT interpolation with a lightweight multi-layer perceptron (MLP) that leverages 2D chromaticity features for robust colorimetric mapping under non-Planckian illuminants. A lightbox-based calibration procedure incorporating representative LED sources is used to train our MLP. Validated across diverse LED lighting, our method reduces angular reproduction error by 22% on average in LED-lit scenes, maintains backward compatibility with traditional illuminants, accommodates multi-illuminant scenes, and supports real-time in-camera deployment with negligible additional computational cost.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AttnRegDeepLab: A Two-Stage Decoupled Framework for Interpretable Embryo Fragmentation Grading</title>
<link>https://arxiv.org/abs/2511.18454</link>
<guid>https://arxiv.org/abs/2511.18454</guid>
<content:encoded><![CDATA[
arXiv:2511.18454v2 Announce Type: replace 
Abstract: Embryo fragmentation is a morphological indicator critical for evaluating developmental potential in In Vitro Fertilization (IVF). However, manual grading is subjective and inefficient, while existing deep learning solutions often lack clinical explainability or suffer from accumulated errors in segmentation area estimation. To address these issues, this study proposes AttnRegDeepLab (Attention-Guided Regression DeepLab), a framework characterized by dual-branch Multi-Task Learning (MTL). A vanilla DeepLabV3+ decoder is modified by integrating Attention Gates into its skip connections, explicitly suppressing cytoplasmic noise to preserve contour details. Furthermore, a Multi-Scale Regression Head is introduced with a Feature Injection mechanism to propagate global grading priors into the segmentation task, rectifying systematic quantification errors. A 2-stage decoupled training strategy is proposed to address the gradient conflict in MTL. Also, a range-based loss is designed to leverage weakly labeled data. Our method achieves robust grading precision while maintaining excellent segmentation accuracy (Dice coefficient =0.729), in contrast to the end-to-end counterpart that might minimize grading error at the expense of contour integrity. This work provides a clinically interpretable solution that balances visual fidelity and quantitative precision.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</title>
<link>https://arxiv.org/abs/2511.19418</link>
<guid>https://arxiv.org/abs/2511.19418</guid>
<content:encoded><![CDATA[
arXiv:2511.19418v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Subspace Isolation: Many-to-Many Transformer for Light Field Image Super-resolution</title>
<link>https://arxiv.org/abs/2401.00740</link>
<guid>https://arxiv.org/abs/2401.00740</guid>
<content:encoded><![CDATA[
arXiv:2401.00740v4 Announce Type: replace-cross 
Abstract: The effective extraction of spatial-angular features plays a crucial role in light field image super-resolution (LFSR) tasks, and the introduction of convolution and Transformers leads to significant improvement in this area. Nevertheless, due to the large 4D data volume of light field images, many existing methods opted to decompose the data into a number of lower-dimensional subspaces and perform Transformers in each sub-space individually. As a side effect, these methods inadvertently restrict the self-attention mechanisms to a One-to-One scheme accessing only a limited subset of LF data, explicitly preventing comprehensive optimization on all spatial and angular cues. In this paper, we identify this limitation as subspace isolation and introduce a novel Many-to-Many Transformer (M2MT) to address it. M2MT aggregates angular information in the spatial subspace before performing the self-attention mechanism. It enables complete access to all information across all sub-aperture images (SAIs) in a light field image. Consequently, M2MT is enabled to comprehensively capture long-range correlation dependencies. With M2MT as the foundational component, we develop a simple yet effective M2MT network for LFSR. Our experimental results demonstrate that M2MT achieves state-of-the-art performance across various public datasets, and it offers a favorable balance between model performance and efficiency, yielding higher-quality LFSR results with substantially lower demand for memory and computation. We further conduct in-depth analysis using local attribution maps (LAM) to obtain visual interpretability, and the results validate that M2MT is empowered with a truly non-local context in both spatial and angular subspaces to mitigate subspace isolation and acquire effective spatial-angular representation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuideGen: A Text-Guided Framework for Paired Full-torso Anatomy and CT Volume Generation</title>
<link>https://arxiv.org/abs/2403.07247</link>
<guid>https://arxiv.org/abs/2403.07247</guid>
<content:encoded><![CDATA[
arXiv:2403.07247v3 Announce Type: replace-cross 
Abstract: The recently emerging conditional diffusion models seem promising for mitigating the labor and expenses in building large 3D medical imaging datasets. However, previous studies on 3D CT generation primarily focus on specific organs characterized by a local structure and fixed contrast and have yet to fully capitalize on the benefits of both semantic and textual conditions. In this paper, we present GuideGen, a controllable framework based on easily-acquired text prompts to generate anatomical masks and corresponding CT volumes for the entire torso-from chest to pelvis. Our approach includes three core components: a text-conditional semantic synthesizer for creating realistic full-torso anatomies; an anatomy-aware high-dynamic-range (HDR) autoencoder for high-fidelity feature extraction across varying intensity levels; and a latent feature generator that ensures alignment between CT images, anatomical semantics and input prompts. Combined, these components enable data synthesis for segmentation tasks from only textual instructions. To train and evaluate GuideGen, we compile a multi-modality cancer imaging dataset with paired CT and clinical descriptions from 12 public TCIA datasets and one private real-world dataset. Comprehensive evaluations across generation quality, cross-modality alignment, and data usability on multi-organ and tumor segmentation tasks demonstrate GuideGen's superiority over existing CT generation methods. Relevant materials are available at https://github.com/OvO1111/GuideGen.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised One-Step Diffusion Refinement for Snapshot Compressive Imaging</title>
<link>https://arxiv.org/abs/2409.07417</link>
<guid>https://arxiv.org/abs/2409.07417</guid>
<content:encoded><![CDATA[
arXiv:2409.07417v2 Announce Type: replace-cross 
Abstract: Snapshot compressive imaging (SCI) captures multispectral images (MSIs) using a single coded two-dimensional (2-D) measurement, but reconstructing high-fidelity MSIs from these compressed inputs remains a fundamentally ill-posed challenge. While diffusion-based reconstruction methods have recently raised the bar for quality, they face critical limitations: a lack of large-scale MSI training data, adverse domain shifts from RGB-pretrained models, and inference inefficiencies due to multi-step sampling. These drawbacks restrict their practicality in real-world applications. In contrast to existing methods, which either follow costly iterative refinement or adapt subspace-based embeddings for diffusion models (e.g. DiffSCI, PSR-SCI), we introduce a fundamentally different paradigm: a self-supervised One-Step Diffusion (OSD) framework specifically designed for SCI. The key novelty lies in using a single-step diffusion refiner to correct an initial reconstruction, eliminating iterative denoising entirely while preserving generative quality. Moreover, we adopt a self-supervised equivariant learning strategy to train both the predictor and refiner directly from raw 2-D measurements, enabling generalization to unseen domains without the need for ground-truth MSI. To further address the challenge of limited MSI data, we design a band-selection-driven distillation strategy that transfers core generative priors from large-scale RGB datasets, effectively bridging the domain gap. Extensive experiments confirm that our approach sets a new benchmark, yielding PSNR gains of 3.44 dB, 1.61 dB, and 0.28 dB on the Harvard, NTIRE, and ICVL datasets, respectively, while reducing reconstruction time by 97.5%. This remarkable improvement in efficiency and adaptability makes our method a significant advancement in SCI reconstruction, combining both accuracy and practicality for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rank Matters: Understanding and Defending Model Inversion Attacks via Low-Rank Feature Filtering</title>
<link>https://arxiv.org/abs/2410.05814</link>
<guid>https://arxiv.org/abs/2410.05814</guid>
<content:encoded><![CDATA[
arXiv:2410.05814v3 Announce Type: replace-cross 
Abstract: Model Inversion Attacks (MIAs) pose a significant threat to data privacy by reconstructing sensitive training samples from the knowledge embedded in trained machine learning models. Despite recent progress in enhancing the effectiveness of MIAs across diverse settings, defense strategies have lagged behind -- struggling to balance model utility with robustness against increasingly sophisticated attacks. In this work, we propose the ideal inversion error to measure the privacy leakage, and our theoretical and empirical investigations reveals that higher-rank features are inherently more prone to privacy leakage. Motivated by this insight, we propose a lightweight and effective defense strategy based on low-rank feature filtering, which explicitly reduces the attack surface by constraining the dimension of intermediate representations. Extensive experiments across various model architectures and datasets demonstrate that our method consistently outperforms existing defenses, achieving state-of-the-art performance against a wide range of MIAs. Notably, our approach remains effective even in challenging regimes involving high-resolution data and high-capacity models, where prior defenses fail to provide adequate protection.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration</title>
<link>https://arxiv.org/abs/2412.10349</link>
<guid>https://arxiv.org/abs/2412.10349</guid>
<content:encoded><![CDATA[
arXiv:2412.10349v2 Announce Type: replace-cross 
Abstract: In dynamic environments, robots often encounter constrained movement trajectories when manipulating objects with specific properties, such as doors. Therefore, applying the appropriate force is crucial to prevent damage to both the robots and the objects. However, current vision-guided robot state generation methods often falter in this regard, as they lack the integration of tactile perception. To tackle this issue, this paper introduces a novel state diffusion framework termed SafeDiff. It generates a prospective state sequence from the current robot state and visual context observation while incorporating real-time tactile feedback to refine the sequence. As far as we know, this is the first study specifically focused on ensuring force safety in robotic manipulation. It significantly enhances the rationality of state planning, and the safe action trajectory is derived from inverse dynamics based on this refined planning. In practice, unlike previous approaches that concatenate visual and tactile data to generate future robot state sequences, our method employs tactile data as a calibration signal to adjust the robot's state within the state space implicitly. Additionally, we've developed a large-scale simulation dataset called SafeDoorManip50k, offering extensive multimodal data to train and evaluate the proposed method. Extensive experiments show that our visual-tactile model substantially mitigates the risk of harmful forces in the door opening, across both simulated and real-world settings.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D MedDiffusion: A 3D Medical Latent Diffusion Model for Controllable and High-quality Medical Image Generation</title>
<link>https://arxiv.org/abs/2412.13059</link>
<guid>https://arxiv.org/abs/2412.13059</guid>
<content:encoded><![CDATA[
arXiv:2412.13059v2 Announce Type: replace-cross 
Abstract: The generation of medical images presents significant challenges due to their high-resolution and three-dimensional nature. Existing methods often yield suboptimal performance in generating high-quality 3D medical images, and there is currently no universal generative framework for medical imaging. In this paper, we introduce a 3D Medical Latent Diffusion (3D MedDiffusion) model for controllable, high-quality 3D medical image generation. 3D MedDiffusion incorporates a novel, highly efficient Patch-Volume Autoencoder that compresses medical images into latent space through patch-wise encoding and recovers back into image space through volume-wise decoding. Additionally, we design a new noise estimator to capture both local details and global structural information during diffusion denoising process. 3D MedDiffusion can generate fine-detailed, high-resolution images (up to 512x512x512) and effectively adapt to various downstream tasks as it is trained on large-scale datasets covering CT and MRI modalities and different anatomical regions (from head to leg). Experimental results demonstrate that 3D MedDiffusion surpasses state-of-the-art methods in generative quality and exhibits strong generalizability across tasks such as sparse-view CT reconstruction, fast MRI reconstruction, and data augmentation for segmentation and classification. Source code and checkpoints are available at https://github.com/ShanghaiTech-IMPACT/3D-MedDiffusion.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Full-scale Representation Guided Network for Retinal Vessel Segmentation</title>
<link>https://arxiv.org/abs/2501.18921</link>
<guid>https://arxiv.org/abs/2501.18921</guid>
<content:encoded><![CDATA[
arXiv:2501.18921v3 Announce Type: replace-cross 
Abstract: The U-Net architecture and its variants have remained state-of-the-art (SOTA) for retinal vessel segmentation over the past decade. In this study, we introduce a Full-Scale Guided Network (FSG-Net), where a novel feature representation module using modernized convolution blocks effectively captures full-scale structural information, while a guided convolution block subsequently refines this information. Specifically, we introduce an attention-guided filter within the guided convolution block, leveraging its similarity to unsharp masking to enhance fine vascular structures. Passing full-scale information to the attention block facilitates the generation of more contextually relevant attention maps, which are then passed to the attention-guided filter, providing further refinement to the segmentation performance. The structure preceding the guided convolution block can be replaced by any U-Net variant, ensuring flexibility and scalability across various segmentation tasks. For a fair comparison, we re-implemented recent studies available in public repositories to evaluate their scalability and reproducibility. Our experiments demonstrate that, despite its compact architecture, FSG-Net delivers performance competitive with SOTA methods across multiple public datasets. Ablation studies further demonstrate that each proposed component meaningfully contributes to this competitive performance. Our code is available on https://github.com/ZombaSY/FSG-Net-pytorch.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models</title>
<link>https://arxiv.org/abs/2502.16671</link>
<guid>https://arxiv.org/abs/2502.16671</guid>
<content:encoded><![CDATA[
arXiv:2502.16671v3 Announce Type: replace-cross 
Abstract: As AI becomes more closely integrated with peoples' daily activities, socially intelligent AI that can understand and interact seamlessly with humans in daily lives is increasingly important. However, current works in AI social reasoning all rely on language-only or language-dominant approaches to benchmark and training models, resulting in systems that are improving in verbal communication but struggle with nonverbal social understanding. To address this limitation, we tap into a novel data source rich in nonverbal social interactions -- mime videos. Mimes refer to the art of expression through gesture and movement without spoken words, which presents unique challenges and opportunities in interpreting nonverbal social communication. We contribute a new dataset called MimeQA, obtained by sourcing ~8 hours of videos clips from YouTube and developing a comprehensive video question-answering benchmark comprising 806 carefully annotated and verified question-answer pairs, designed to probe nonverbal social reasoning capabilities. Using MimeQA, we evaluate state-of-the-art video large language models (VideoLLMs) and find that they achieve low accuracy, generally ranging from 20-30%, while humans score 86%. Our analysis reveals that VideoLLMs often fail to ground imagined objects and over-rely on the text prompt while ignoring subtle nonverbal interactions. We hope to inspire future work in AI models that embody true social intelligence capable of interpreting non-verbal human interactions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain tumour Segmentation on mp-MRI</title>
<link>https://arxiv.org/abs/2503.04325</link>
<guid>https://arxiv.org/abs/2503.04325</guid>
<content:encoded><![CDATA[
arXiv:2503.04325v4 Announce Type: replace-cross 
Abstract: Gliomas are aggressive brain tumors that require accurate imaging-based diagnosis, with segmentation playing a critical role in evaluating morphology and treatment decisions. Manual delineation of gliomas is time-consuming and prone to variability, motivating the use of deep learning to improve consistency and alleviate clinical workload. However, existing methods often fail to fully exploit the information available in multi-parametric MRI (mp-MRI), particularly inter-slice contextual features, and typically require considerable computational resources while lacking robustness across tumor type variations. We present GBT-SAM, a parameter-efficient deep learning framework that adapts the Segment Anything Model (SAM), a large-scale vision model, to volumetric mp-MRI data. GBT-SAM reduces input complexity by selecting fewer than 2.6\% of slices per scan while incorporating all four MRI modalities, preserving essential tumor-related information with minimal cost. Furthermore, our model is trained by a two-step fine-tuning strategy that incorporates a depth-aware module to capture inter-slice correlations and lightweight adaptation layers, resulting in just 6.5M trainable parameters, which is the lowest among SAM-based approaches. GBT-SAM achieves a Dice Score of 93.54 on the BraTS Adult Glioma dataset and demonstrates robust performance on Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. These results highlight GBT-SAM's potential as a computationally efficient and domain-robust framework for brain tumor segmentation using mp-MRI. Our code and models are available at https://github.com/vpulab/med-sam-brain .
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech Audio Generation from dynamic MRI via a Knowledge Enhanced Conditional Variational Autoencoder</title>
<link>https://arxiv.org/abs/2503.06588</link>
<guid>https://arxiv.org/abs/2503.06588</guid>
<content:encoded><![CDATA[
arXiv:2503.06588v2 Announce Type: replace-cross 
Abstract: Dynamic Magnetic Resonance Imaging (MRI) of the vocal tract has become an increasingly adopted imaging modality for speech motor studies. Beyond image signals, systematic data loss, noise pollution, and audio file corruption can occur due to the unpredictability of the MRI acquisition environment. In such cases, generating audio from images is critical for data recovery in both clinical and research applications. However, this remains challenging due to hardware constraints, acoustic interference, and data corruption. Existing solutions, such as denoising and multi-stage synthesis methods, face limitations in audio fidelity and generalizability. To address these challenges, we propose a Knowledge Enhanced Conditional Variational Autoencoder (KE-CVAE), a novel two-step "knowledge enhancement + variational inference" framework for generating speech audio signals from cine dynamic MRI sequences. This approach introduces two key innovations: (1) integration of unlabeled MRI data for knowledge enhancement, and (2) a variational inference architecture to improve generative modeling capacity. To the best of our knowledge, this is one of the first attempts at synthesizing speech audio directly from dynamic MRI video sequences. The proposed method was trained and evaluated on an open-source dynamic vocal tract MRI dataset recorded during speech. Experimental results demonstrate its effectiveness in generating natural speech waveforms while addressing MRI-specific acoustic challenges, outperforming conventional deep learning-based synthesis approaches.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering</title>
<link>https://arxiv.org/abs/2503.16177</link>
<guid>https://arxiv.org/abs/2503.16177</guid>
<content:encoded><![CDATA[
arXiv:2503.16177v2 Announce Type: replace-cross 
Abstract: In large-scale scene reconstruction using 3D Gaussian splatting, it is common to partition the scene into multiple smaller regions and reconstruct them individually. However, existing division methods are occlusion-agnostic, meaning that each region may contain areas with severe occlusions. As a result, the cameras within those regions are less correlated, leading to a low average contribution to the overall reconstruction. In this paper, we propose an occlusion-aware scene division strategy that clusters training cameras based on their positions and co-visibilities to acquire multiple regions. Cameras in such regions exhibit stronger correlations and a higher average contribution, facilitating high-quality scene reconstruction. We further propose a region-based rendering technique to accelerate large scene rendering, which culls Gaussians invisible to the region where the viewpoint is located. Such a technique significantly speeds up the rendering without compromising quality. Extensive experiments on multiple large scenes show that our method achieves superior reconstruction results with faster rendering speed compared to existing state-of-the-art approaches. Project page: https://occlugaussian.github.io.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame Animated Sticker Generation</title>
<link>https://arxiv.org/abs/2503.17735</link>
<guid>https://arxiv.org/abs/2503.17735</guid>
<content:encoded><![CDATA[
arXiv:2503.17735v2 Announce Type: replace-cross 
Abstract: Recently, significant advancements have been achieved in video generation technology, but applying it to resource-constrained downstream tasks like multi-frame animated sticker generation (ASG) characterized by low frame rates, abstract semantics, and long tail frame length distribution-remains challenging. Parameter-efficient fine-tuning (PEFT) techniques (e.g., Adapter, LoRA) for large pre-trained models suffer from insufficient fitting ability and source-domain knowledge interference. In this paper, we propose Resource-Efficient Dual-Mask Training Framework (RDTF), a dedicated solution for multi-frame ASG task under resource constraints. We argue that training a compact model from scratch with million-level samples outperforms PEFT on large models, with RDTF realizing this via three core designs: 1) a Discrete Frame Generation Network (DFGN) optimized for low-frame-rate ASG, ensuring parameter efficiency; 2) a dual-mask based data utilization strategy to enhance the availability and diversity of limited data; 3) a difficulty-adaptive curriculum learning method that decomposes sample entropy into static and adaptive components, enabling easy-to-difficult training convergence. To provide high-quality data support for RDTFs training from scratch, we construct VSD2M-a million-level multi-modal animated sticker dataset with rich annotations (static and animated stickers, action-focused text descriptions)-filling the gap of dedicated animated data for ASG task. Experiments demonstrate that RDTF is quantitatively and qualitatively superior to state-of-the-art PEFT methods (e.g., I2V-Adapter, SimDA) on ASG tasks, verifying the feasibility of our framework under resource constraints.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldScore: A Unified Evaluation Benchmark for World Generation</title>
<link>https://arxiv.org/abs/2504.00983</link>
<guid>https://arxiv.org/abs/2504.00983</guid>
<content:encoded><![CDATA[
arXiv:2504.00983v2 Announce Type: replace-cross 
Abstract: We introduce the WorldScore benchmark, the first unified benchmark for world generation. We decompose world generation into a sequence of next-scene generation tasks with explicit camera trajectory-based layout specifications, enabling unified evaluation of diverse approaches from 3D and 4D scene generation to video generation models. The WorldScore benchmark encompasses a curated dataset of 3,000 test examples that span diverse worlds: static and dynamic, indoor and outdoor, photorealistic and stylized. The WorldScore metrics evaluate generated worlds through three key aspects: controllability, quality, and dynamics. Through extensive evaluation of 19 representative models, including both open-source and closed-source ones, we reveal key insights and challenges for each category of models. Our dataset, evaluation code, and leaderboard can be found at https://haoyi-duan.github.io/WorldScore/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users</title>
<link>https://arxiv.org/abs/2504.10445</link>
<guid>https://arxiv.org/abs/2504.10445</guid>
<content:encoded><![CDATA[
arXiv:2504.10445v2 Announce Type: replace-cross 
Abstract: To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond performing a single, clearly defined task. For instance, real-world human instructions can be ambiguous, require different levels of AI assistance, and may evolve over time, reflecting changes in the user's mental state. To address this gap, we introduce RealWebAssist, a novel benchmark designed to evaluate sequential instruction-following in realistic scenarios involving long-horizon interactions with the web, visual GUI grounding, and understanding ambiguous real-world user instructions. RealWebAssist includes a dataset of sequential instructions collected from real-world human users. Each user instructs a web-based assistant to perform a series of tasks on multiple websites. A successful agent must reason about the true intent behind each instruction, keep track of the mental state of the user, understand user-specific routines, and ground the intended tasks to actions on the correct GUI elements. Our experimental results show that state-of-the-art models struggle to understand and ground user instructions, posing critical challenges in following real-world user instructions for long-horizon web assistance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-Conditional Distribution Balancing for Group Robust Classification</title>
<link>https://arxiv.org/abs/2504.17314</link>
<guid>https://arxiv.org/abs/2504.17314</guid>
<content:encoded><![CDATA[
arXiv:2504.17314v3 Announce Type: replace-cross 
Abstract: Spurious correlations that lead models to correct predictions for the wrong reasons pose a critical challenge for robust real-world generalization. Existing research attributes this issue to group imbalance and addresses it by maximizing group-balanced or worst-group accuracy, which heavily relies on expensive bias annotations. A compromise approach involves predicting bias information using extensively pretrained foundation models, which requires large-scale data and becomes impractical for resource-limited rare domains. To address these challenges, we offer a novel perspective by reframing the spurious correlations as imbalances or mismatches in class-conditional distributions, and propose a simple yet effective robust learning method that eliminates the need for both bias annotations and predictions. With the goal of maximizing the conditional entropy (uncertainty) of the label given spurious factors, our method leverages a sample reweighting strategy to achieve class-conditional distribution balancing, which automatically highlights minority groups and classes, effectively dismantling spurious correlations and producing a debiased data distribution for classification. Extensive experiments and analysis demonstrate that our approach consistently delivers state-of-the-art performance, rivaling methods that rely on bias supervision.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses</title>
<link>https://arxiv.org/abs/2504.20405</link>
<guid>https://arxiv.org/abs/2504.20405</guid>
<content:encoded><![CDATA[
arXiv:2504.20405v2 Announce Type: replace-cross 
Abstract: Deep learning has shown strong performance in musculoskeletal imaging, but prior work has largely targeted conditions where diagnosis is relatively straightforward. More challenging problems remain underexplored, such as detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard MRIs. These lesions are difficult to diagnose due to subtle imaging features, often necessitating invasive MRI arthrograms (MRAs). We introduce ScopeMRI, the first publicly available, expert-annotated dataset for shoulder pathologies, and present a deep learning framework for Bankart lesion detection on both standard MRIs and MRAs. ScopeMRI contains shoulder MRIs from patients who underwent arthroscopy, providing ground-truth labels from intraoperative findings, the diagnostic gold standard. Separate models were trained for MRIs and MRAs using CNN- and transformer-based architectures, with predictions ensembled across multiple imaging planes. Our models achieved radiologist-level performance, with accuracy on standard MRIs surpassing radiologists interpreting MRAs. External validation on independent hospital data demonstrated initial generalizability across imaging protocols. By releasing ScopeMRI and a modular codebase for training and evaluation, we aim to accelerate research in musculoskeletal imaging and foster development of datasets and models that address clinically challenging diagnostic tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions</title>
<link>https://arxiv.org/abs/2505.18151</link>
<guid>https://arxiv.org/abs/2505.18151</guid>
<content:encoded><![CDATA[
arXiv:2505.18151v2 Announce Type: replace-cross 
Abstract: WonderPlay is a novel framework integrating physics simulation with video generation for generating action-conditioned dynamic 3D scenes from a single image. While prior works are restricted to rigid body or simple elastic dynamics, WonderPlay features a hybrid generative simulator to synthesize a wide range of 3D dynamics. The hybrid generative simulator first uses a physics solver to simulate coarse 3D dynamics, which subsequently conditions a video generator to produce a video with finer, more realistic motion. The generated video is then used to update the simulated dynamic 3D scene, closing the loop between the physics solver and the video generator. This approach enables intuitive user control to be combined with the accurate dynamics of physics-based simulators and the expressivity of diffusion-based video generators. Experimental results demonstrate that WonderPlay enables users to interact with various scenes of diverse content, including cloth, sand, snow, liquid, smoke, elastic, and rigid bodies -- all using a single image input. Code will be made public. Project website: https://kyleleey.github.io/WonderPlay/
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders</title>
<link>https://arxiv.org/abs/2505.18884</link>
<guid>https://arxiv.org/abs/2505.18884</guid>
<content:encoded><![CDATA[
arXiv:2505.18884v2 Announce Type: replace-cross 
Abstract: Visual encoders have become fundamental components in modern computer vision pipelines. However, ensuring robustness against adversarial perturbations remains a critical challenge. Recent efforts have explored both supervised and unsupervised adversarial fine-tuning strategies. We identify two key limitations in these approaches: (i) they often suffer from instability, especially during the early stages of fine-tuning, resulting in suboptimal convergence and degraded performance on clean data, and (ii) they exhibit a suboptimal trade-off between robustness and clean data accuracy, hindering the simultaneous optimization of both objectives. To overcome these challenges, we propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised adversarial fine-tuning framework. LORE utilizes constrained optimization, which offers a principled approach to balancing competing goals, such as improving robustness while preserving nominal performance. By enforcing embedding-space proximity constraints, LORE effectively maintains clean data performance throughout adversarial fine-tuning. Extensive experiments show that LORE significantly improves zero-shot adversarial robustness with minimal degradation in clean data accuracy. Furthermore, we demonstrate the effectiveness of the adversarially fine-tuned CLIP image encoder in out-of-distribution generalization and enhancing the interpretability of image embeddings.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Language Models are Biased</title>
<link>https://arxiv.org/abs/2505.23941</link>
<guid>https://arxiv.org/abs/2505.23941</guid>
<content:encoded><![CDATA[
arXiv:2505.23941v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that helps them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g., unable to recognize the 4th stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Removing image backgrounds nearly doubles accuracy (21.09 percentage points), revealing that contextual visual cues trigger these biased responses. Further analysis of VLMs' reasoning patterns shows that counting accuracy initially rises with thinking tokens, reaching ~40%, before declining with excessive reasoning. Our work presents an interesting failure mode in VLMs and a human-supervised automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.00727</link>
<guid>https://arxiv.org/abs/2506.00727</guid>
<content:encoded><![CDATA[
arXiv:2506.00727v2 Announce Type: replace-cross 
Abstract: Background and Objective: Plane reformatting for four-dimensional phase contrast MRI (4D flow MRI) is time-consuming and prone to inter-observer variability, which limits fast cardiovascular flow assessment. Deep reinforcement learning (DRL) trains agents to iteratively adjust plane position and orientation, enabling accurate plane reformatting without the need for detailed landmarks, making it suitable for images with limited contrast and resolution such as 4D flow MRI. However, current DRL methods assume that test volumes share the same spatial alignment as the training data, limiting generalization across scanners and institutions. To address this limitation, we introduce AdaPR (Adaptive Plane Reformatting), a DRL framework that uses a local coordinate system to navigate volumes with arbitrary positions and orientations.
  Methods: We implemented AdaPR using the Asynchronous Advantage Actor-Critic (A3C) algorithm and validated it on 88 4D flow MRI datasets acquired from multiple vendors, including patients with congenital heart disease.
  Results: AdaPR achieved a mean angular error of 6.32 +/- 4.15 degrees and a distance error of 3.40 +/- 2.75 mm, outperforming global-coordinate DRL methods and alternative non-DRL methods. AdaPR maintained consistent accuracy under different volume orientations and positions. Flow measurements from AdaPR planes showed no significant differences compared to two manual observers, with excellent correlation (R^2 = 0.972 and R^2 = 0.968), comparable to inter-observer agreement (R^2 = 0.969).
  Conclusion: AdaPR provides robust, orientation-independent plane reformatting for 4D flow MRI, achieving flow quantification comparable to expert observers. Its adaptability across datasets and scanners makes it a promising candidate for medical imaging applications beyond 4D flow MRI.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot Denoising via Neural Compression: Theoretical and algorithmic framework</title>
<link>https://arxiv.org/abs/2506.12693</link>
<guid>https://arxiv.org/abs/2506.12693</guid>
<content:encoded><![CDATA[
arXiv:2506.12693v2 Announce Type: replace-cross 
Abstract: Zero-shot denoising aims to denoise observations without access to training samples or clean reference images. This setting is particularly relevant in practical imaging scenarios involving specialized domains such as medical imaging or biology. In this work, we propose the Zero-Shot Neural Compression Denoiser (ZS-NCD), a novel denoising framework based on neural compression. ZS-NCD treats a neural compression network as an untrained model, optimized directly on patches extracted from a single noisy image. The final reconstruction is then obtained by aggregating the outputs of the trained model over overlapping patches. Thanks to the built-in entropy constraints of compression architectures, our method naturally avoids overfitting and does not require manual regularization or early stopping. Through extensive experiments, we show that ZS-NCD achieves state-of-the-art performance among zero-shot denoisers for both Gaussian and Poisson noise, and generalizes well to both natural and non-natural images. Additionally, we provide new finite-sample theoretical results that characterize upper bounds on the achievable reconstruction error of general maximum-likelihood compression-based denoisers. These results further establish the theoretical foundations of compression-based denoising. Our code is available at: https://github.com/Computational-Imaging-RU/ZS-NCDenoiser.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation</title>
<link>https://arxiv.org/abs/2506.23717</link>
<guid>https://arxiv.org/abs/2506.23717</guid>
<content:encoded><![CDATA[
arXiv:2506.23717v4 Announce Type: replace-cross 
Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated research spot, pursuing energy-efficient and high-accurate AI. However, with more bits involved, the associated memory and computation demands escalate to the point where the performance improvements become disproportionate. Based on the insight that different layers demonstrate different importance and extra bits could be wasted and interfering, this paper presents an adaptive bit allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise allocation of memory and computation resources. Thus, SNN's efficiency and accuracy can be improved. Specifically, we parametrize the temporal lengths and the bit widths of weights and spikes, and make them learnable and controllable through gradients. To address the challenges caused by changeable bit widths and temporal lengths, we propose the refined spiking neuron, which can handle different temporal lengths, enable the derivation of gradients for temporal lengths, and suit spike quantization better. In addition, we theoretically formulate the step-size mismatch problem of learnable bit widths, which may incur severe quantization errors to SNN, and accordingly propose the step-size renewal mechanism to alleviate this issue. Experiments on various datasets, including the static CIFAR and ImageNet datasets and the dynamic CIFAR-DVS and DVS-GESTURE datasets, demonstrate that our methods can reduce the overall memory and computation cost while achieving higher accuracy. Particularly, our SEWResNet-34 can achieve a 2.69\% accuracy gain and 4.16$\times$ lower bit budgets over the advanced baseline work on ImageNet. This work is open-sourced at \href{https://github.com/Ikarosy/Towards-Efficient-and-Accurate-Spiking-Neural-Networks-via-Adaptive-Bit-Allocation}{this link}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow Equivariant Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2507.14793</link>
<guid>https://arxiv.org/abs/2507.14793</guid>
<content:encoded><![CDATA[
arXiv:2507.14793v2 Announce Type: replace-cross 
Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of 'flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands</title>
<link>https://arxiv.org/abs/2508.03339</link>
<guid>https://arxiv.org/abs/2508.03339</guid>
<content:encoded><![CDATA[
arXiv:2508.03339v2 Announce Type: replace-cross 
Abstract: Dexterous grasp datasets are vital for embodied intelligence, but mostly emphasize grasp stability, ignoring functional grasps needed for tasks like opening bottle caps or holding cup handles. Most rely on bulky, costly, and hard-to-control high-DOF Shadow Hands. Inspired by the human hand's underactuated mechanism, we establish UniFucGrasp, a universal functional grasp annotation strategy and dataset for multiple dexterous hand types. Based on biomimicry, it maps natural human motions to diverse hand structures and uses geometry-based force closure to ensure functional, stable, human-like grasps. This method supports low-cost, efficient collection of diverse, high-quality functional grasps. Finally, we establish the first multi-hand functional grasp dataset and provide a synthesis model to validate its effectiveness. Experiments on the UFG dataset, IsaacSim, and complex robotic tasks show that our method improves functional manipulation accuracy and grasp stability, demonstrates improved adaptability across multiple robotic hands, helping to alleviate annotation cost and generalization challenges in dexterous grasping. The project page is at https://haochen611.github.io/UFG.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Cancer Knowledge Transfer in WSI-based Prognosis Prediction</title>
<link>https://arxiv.org/abs/2508.13482</link>
<guid>https://arxiv.org/abs/2508.13482</guid>
<content:encoded><![CDATA[
arXiv:2508.13482v3 Announce Type: replace-cross 
Abstract: Whole-Slide Image (WSI) is an important tool for estimating cancer prognosis. Current studies generally follow a conventional cancer-specific paradigm in which each cancer corresponds to a single model. However, this paradigm naturally struggles to scale to rare tumors and cannot leverage knowledge from other cancers. While multi-task learning frameworks have been explored recently, they often place high demands on computational resources and require extensive training on ultra-large, multi-cancer WSI datasets. To this end, this paper shifts the paradigm to knowledge transfer and presents the first preliminary yet systematic study on cross-cancer prognosis knowledge transfer in WSIs, called CROPKT. It comprises three major parts. (1) We curate a large dataset (UNI2-h-DSS) with 26 cancers and use it to measure the transferability of WSI-based prognostic knowledge across different cancers (including rare tumors). (2) Beyond a simple evaluation merely for benchmarking, we design a range of experiments to gain deeper insights into the underlying mechanism behind transferability. (3) We further show the utility of cross-cancer knowledge transfer, by proposing a routing-based baseline approach (ROUPKT) that could often efficiently utilize the knowledge transferred from off-the-shelf models of other cancers. CROPKT could serve as an inception that lays the foundation for this nascent paradigm, i.e., WSI-based prognosis prediction with cross-cancer knowledge transfer. Our source code is available at https://github.com/liupei101/CROPKT.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction of Distant Metastasis in Head and Neck Cancer Patients Using Tumor and Peritumoral Multi-Modal Deep Learning</title>
<link>https://arxiv.org/abs/2508.20469</link>
<guid>https://arxiv.org/abs/2508.20469</guid>
<content:encoded><![CDATA[
arXiv:2508.20469v2 Announce Type: replace-cross 
Abstract: Although the combined treatment of surgery, radiotherapy, chemotherapy, and emerging target therapy has significantly improved the outcomes of patients with head and neck cancer, distant metastasis remains the leading cause of treatment failure. In this study, we propose a deep learning-based multimodal framework integrating CT imaging, radiomics, and clinical data to predict metastasis risk in HNSCC. A total of 1497 patients were retrospectively analyzed. Tumor and organ masks were generated from pretreatment CT scans, from which a 3D Swin Transformer extracted deep imaging features, while 1562 radiomics features were reduced to 36 via correlation filtering and random forest selection. Clinical data (age, sex, smoking, and alcohol status) were encoded and fused with imaging features, and the multimodal representation was fed into a fully connected network for prediction. Five-fold cross-validation was used to assess performance via AUC, accuracy, sensitivity, and specificity. The multimodal model outperformed all single-modality baselines. The deep learning module alone achieved an AUC of 0.715, whereas multimodal fusion significantly improved performance (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758). Stratified analyses confirmed good generalizability across tumor subtypes. Ablation experiments demonstrated complementary contributions from each modality, and the 3D Swin Transformer provided more robust representations than conventional architectures. This multimodal deep learning model enables accurate, non-invasive metastasis prediction in HNSCC and shows strong potential for individualized treatment planning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.01944</link>
<guid>https://arxiv.org/abs/2509.01944</guid>
<content:encoded><![CDATA[
arXiv:2509.01944v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models in autonomous driving systems have recently demonstrated transformative potential by integrating multimodal perception with decision-making capabilities. However, the interpretability and coherence of the decision process and the plausibility of action sequences remain largely underexplored. To address these issues, we propose AutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and self-reflection capabilities of autonomous driving systems through chain-of-thought (CoT) processing and reinforcement learning (RL). Specifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K for supervised fine-tuning, which effectively builds cognitive bridges between input information and output trajectories through a four-step logical chain with self-reflection for validation. Moreover, to maximize both reasoning and self-reflection during the RL stage, we further employ the Group Relative Policy Optimization (GRPO) algorithm within a physics-grounded reward framework that incorporates spatial alignment, vehicle dynamic, and temporal smoothness criteria to ensure reliable and realistic trajectory planning. Extensive evaluation results across both nuScenes and Waymo datasets demonstrates the state-of-the-art performance and robust generalization capacity of our proposed method.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedHK-MVFC: Federated Heat Kernel Multi-View Clustering</title>
<link>https://arxiv.org/abs/2509.15844</link>
<guid>https://arxiv.org/abs/2509.15844</guid>
<content:encoded><![CDATA[
arXiv:2509.15844v2 Announce Type: replace-cross 
Abstract: In the realm of distributed artificial intelligence (AI) and privacy-focused medical applications, this paper proposes a multi-view clustering framework that links quantum field theory with federated healthcare analytics. The method uses heat kernel coefficients from spectral analysis to convert Euclidean distances into geometry-aware similarity measures that capture the structure of diverse medical data. The framework is presented through the heat kernel distance (HKD) transformation, which has convergence guarantees. Two algorithms have been developed: The first, Heat Kernel-Enhanced Multi-View Fuzzy Clustering (HK-MVFC), is used for central analysis. The second, Federated Heat Kernel Multi-View Fuzzy Clustering (FedHK-MVFC), is used for secure, privacy-preserving learning across hospitals. FedHK-MVFC uses differential privacy and secure aggregation to enable HIPAA-compliant collaboration. Tests on synthetic cardiovascular patient datasets demonstrate increased clustering accuracy, reduced communication, and retained efficiency compared to centralized methods. After being validated on 10,000 synthetic patient records across two hospitals, the methods proved useful for collaborative phenotyping involving electrocardiogram (ECG) data, cardiac imaging data, and behavioral data. The proposed methods' theoretical contributions include update rules with proven convergence, adaptive view weighting, and privacy-preserving protocols. These contributions establish a new standard for geometry-aware federated learning in healthcare, translating advanced mathematics into practical solutions for analyzing sensitive medical data while ensuring rigor and clinical relevance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2509.21526</link>
<guid>https://arxiv.org/abs/2509.21526</guid>
<content:encoded><![CDATA[
arXiv:2509.21526v2 Announce Type: replace-cross 
Abstract: We introduce TRiCo, a novel triadic game-theoretic co-training framework that rethinks the structure of semi-supervised learning by incorporating a teacher, two students, and an adversarial generator into a unified training paradigm. Unlike existing co-training or teacher-student approaches, TRiCo formulates SSL as a structured interaction among three roles: (i) two student classifiers trained on frozen, complementary representations, (ii) a meta-learned teacher that adaptively regulates pseudo-label selection and loss balancing via validation-based feedback, and (iii) a non-parametric generator that perturbs embeddings to uncover decision boundary weaknesses. Pseudo-labels are selected based on mutual information rather than confidence, providing a more robust measure of epistemic uncertainty. This triadic interaction is formalized as a Stackelberg game, where the teacher leads strategy optimization and students follow under adversarial perturbations. By addressing key limitations in existing SSL frameworks, such as static view interactions, unreliable pseudo-labels, and lack of hard sample modeling, TRiCo provides a principled and generalizable solution. Extensive experiments on CIFAR-10, SVHN, STL-10, and ImageNet demonstrate that TRiCo consistently achieves state-of-the-art performance in low-label regimes, while remaining architecture-agnostic and compatible with frozen vision backbones.Code:https://github.com/HoHongYeung/NeurIPS25-TRiCo.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Variational Autoencoder</title>
<link>https://arxiv.org/abs/2511.07472</link>
<guid>https://arxiv.org/abs/2511.07472</guid>
<content:encoded><![CDATA[
arXiv:2511.07472v2 Announce Type: replace-cross 
Abstract: Learning latent representations that are simultaneously expressive, geometrically well-structured, and reliably calibrated remains a central challenge for Variational Autoencoders (VAEs). Standard VAEs typically assume a diagonal Gaussian posterior, which simplifies optimization but rules out correlated uncertainty and often yields entangled or redundant latent dimensions. We introduce the Multivariate Variational Autoencoder (MVAE), a tractable full-covariance extension of the VAE that augments the encoder with sample-specific diagonal scales and a global coupling matrix. This induces a multivariate Gaussian posterior of the form $N(\mu_\phi(x), C \operatorname{diag}(\sigma_\phi^2(x)) C^\top)$, enabling correlated latent factors while preserving a closed-form KL divergence and a simple reparameterization path. Beyond likelihood, we propose a multi-criterion evaluation protocol that jointly assesses reconstruction quality (MSE, ELBO), downstream discrimination (linear probes), probabilistic calibration (NLL, Brier, ECE), and unsupervised structure (NMI, ARI). Across Larochelle-style MNIST variants, Fashion-MNIST, and CIFAR-10/100, MVAE consistently matches or outperforms diagonal-covariance VAEs of comparable capacity, with particularly notable gains in calibration and clustering metrics at both low and high latent dimensions. Qualitative analyses further show smoother, more semantically coherent latent traversals and sharper reconstructions. All code, dataset splits, and evaluation utilities are released to facilitate reproducible comparison and future extensions of multivariate posterior models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Diversifying Dataset Distillation by Decoupling Architectural Priors</title>
<link>https://arxiv.org/abs/2511.09905</link>
<guid>https://arxiv.org/abs/2511.09905</guid>
<content:encoded><![CDATA[
arXiv:2511.09905v2 Announce Type: replace-cross 
Abstract: Dataset distillation (DD) promises compact yet faithful synthetic data, but existing approaches often inherit the inductive bias of a single teacher model. As dataset size increases, this bias drives generation toward overly smooth, homogeneous samples, reducing intra-class diversity and limiting generalization. We present PRISM (PRIors from diverse Source Models), a framework that disentangles architectural priors during synthesis. PRISM decouples the logit-matching and regularization objectives, supervising them with different teacher architectures: a primary model for logits and a stochastic subset for batch-normalization (BN) alignment. On ImageNet-1K, PRISM consistently and reproducibly outperforms single-teacher methods (e.g., SRe2L) and recent multi-teacher variants (e.g., G-VBSM) at low- and mid-IPC regimes. The generated data also show significantly richer intra-class diversity, as reflected by a notable drop in cosine similarity between features. We further analyze teacher selection strategies (pre- vs. intra-distillation) and introduce a scalable cross-class batch formation scheme for fast parallel synthesis. Code will be released after the review period.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention</title>
<link>https://arxiv.org/abs/2511.14515</link>
<guid>https://arxiv.org/abs/2511.14515</guid>
<content:encoded><![CDATA[
arXiv:2511.14515v2 Announce Type: replace-cross 
Abstract: Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex "approximate-compensate" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the "amplitude-ignoring" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.15605</link>
<guid>https://arxiv.org/abs/2511.15605</guid>
<content:encoded><![CDATA[
arXiv:2511.15605v2 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRI Super-Resolution with Deep Learning: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2511.16854</link>
<guid>https://arxiv.org/abs/2511.16854</guid>
<content:encoded><![CDATA[
arXiv:2511.16854v2 Announce Type: replace-cross 
Abstract: High-resolution (HR) magnetic resonance imaging (MRI) is crucial for many clinical and research applications. However, achieving it remains costly and constrained by technical trade-offs and experimental limitations. Super-resolution (SR) presents a promising computational approach to overcome these challenges by generating HR images from more affordable low-resolution (LR) scans, potentially improving diagnostic accuracy and efficiency without requiring additional hardware. This survey reviews recent advances in MRI SR techniques, with a focus on deep learning (DL) approaches. It examines DL-based MRI SR methods from the perspectives of computer vision, computational imaging, inverse problems, and MR physics, covering theoretical foundations, architectural designs, learning strategies, benchmark datasets, and performance metrics. We propose a systematic taxonomy to categorize these methods and present an in-depth study of both established and emerging SR techniques applicable to MRI, considering unique challenges in clinical and research contexts. We also highlight open challenges and directions that the community needs to address. Additionally, we provide a collection of essential open-access resources, tools, and tutorials, available on our GitHub: https://github.com/mkhateri/Awesome-MRI-Super-Resolution.
  IEEE keywords: MRI, Super-Resolution, Deep Learning, Computational Imaging, Inverse Problem, Survey.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization</title>
<link>https://arxiv.org/abs/2511.18192</link>
<guid>https://arxiv.org/abs/2511.18192</guid>
<content:encoded><![CDATA[
<div> Keywords: Document Visual Question Answering, spatial grounding, ARIAL, modular framework, interpretability<br /><br />Summary: Document Visual Question Answering (VQA) demands systems to accurately extract textual answers and localize them precisely within document images to ensure interpretability, especially in critical applications. Existing methods either focus on textual accuracy with insufficient spatial grounding or compromise performance for better explainability. ARIAL (Agentic Reasoning for Interpretable Answer Localization) addresses this by using an LLM-based planning agent to coordinate specialized tools, enabling both high-accuracy answer extraction and reliable spatial localization. The framework decomposes Document VQA into four key subtasks: OCR text extraction using TrOCR, context selection via retrieval-augmented semantic search, answer generation through a fine-tuned Gemma 3-27B model, and precise bounding-box localization by aligning extracted text to image regions. ARIAL’s modular design facilitates transparent reasoning, auditability at the tool level, and independent optimization of components. Evaluated on four benchmarks—DocVQA, FUNSD, CORD, and SROIE—ARIAL achieves state-of-the-art textual accuracy (ANLS) and spatial precision (mAP), exceeding previous best results such as those by DLaVA. These results underscore how agentic orchestration of specialized tools enhances both performance and interpretability, advancing trustworthy and explainable document AI systems. <div>
arXiv:2511.18192v2 Announce Type: replace 
Abstract: Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing What Matters: Visual Preference Policy Optimization for Visual Generation</title>
<link>https://arxiv.org/abs/2511.18719</link>
<guid>https://arxiv.org/abs/2511.18719</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Visual Generative Models, Group Relative Policy Optimization, Pixel-Level Advantages, Perceptual Structuring Module<br /><br />Summary:<br /><br />Reinforcement learning (RL) has proven effective for improving visual generative models by aligning them with human preferences using Group Relative Policy Optimization (GRPO). However, conventional GRPO methods rely solely on a single scalar reward per sample, treating the entire image or video as a whole and neglecting detailed spatial and temporal information. This coarse feedback limits the ability to correct localized imperfections and to model subtle perceptual details. To address these limitations, the authors propose Visual Preference Policy Optimization (ViPO), a novel GRPO variant. ViPO transforms scalar rewards into structured pixel-level advantage maps, enabling finer and more informative supervision. At the core of ViPO is a Perceptual Structuring Module that leverages pretrained vision models to generate spatially and temporally aware advantage maps. This approach effectively reallocates the optimization focus towards perceptually significant regions while maintaining training stability comparable to standard GRPO. Extensive experiments on image and video benchmarks demonstrate that ViPO consistently outperforms vanilla GRPO by achieving better alignment with human-preference rewards within the training domain and improving generalization to out-of-domain scenarios. ViPO is architecture-agnostic, lightweight, and seamlessly integrates with existing GRPO pipelines, offering a more expressive learning signal for visual generation tasks. <div>
arXiv:2511.18719v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion</title>
<link>https://arxiv.org/abs/2511.18734</link>
<guid>https://arxiv.org/abs/2511.18734</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D city generation, large models, hierarchical planning, image-to-3D synthesis, city expansion<br /><br />Summary:<br /><br />Yo'City is a novel framework aimed at realistic 3D city generation that addresses the limitations of traditional single diffusion model approaches. It enables user-customized and infinitely expandable city-scale scenes by leveraging the reasoning and compositional abilities of large pre-trained models. The method uses a top-down hierarchical planning strategy that organizes the city into a "City-District-Grid" structure, where a Global Planner outlines the overall layout and functional districts, and a Local Designer provides detailed grid-level descriptions. The framework employs a "produce-refine-evaluate" loop for isometric image synthesis to generate detailed grid-level visuals, which are then converted into 3D models through image-to-3D generation techniques. To simulate ongoing urban evolution, Yo'City incorporates an interactive, relationship-driven expansion mechanism that performs scene graph-based layout optimization, taking into account spatial coherence, distances, and semantic relationships. The authors also construct a comprehensive benchmark dataset with six multi-dimensional metrics assessing semantics, geometry, texture, and city layout quality. Extensive experimental results show that Yo'City consistently outperforms current state-of-the-art methods across all evaluation criteria, demonstrating improvements in generation quality and scalability for practical applications such as virtual reality and digital twins. <div>
arXiv:2511.18734v2 Announce Type: replace 
Abstract: Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion</title>
<link>https://arxiv.org/abs/2511.18742</link>
<guid>https://arxiv.org/abs/2511.18742</guid>
<content:encoded><![CDATA[
<div> Diffusion models, text-to-image, proximal operators, reinforcement learning, LAION-Face-T2I-15M<br /><br />Summary:<br /><br />This work introduces ProxT2I, a novel text-to-image diffusion model that departs from the traditional forward discretization approach by using backward discretizations and conditional proximal operators learned from data instead of score functions. This method addresses key issues in existing samplers, such as slowness and instability, which require many sampling steps for generating high-quality images. The authors incorporate reinforcement learning and policy optimization techniques to directly optimize samplers for task-specific rewards, enhancing both efficiency and alignment with human preferences. A major contribution is the creation of LAION-Face-T2I-15M, a large-scale, open-source dataset consisting of 15 million high-quality human images with detailed captions, enabling robust training and evaluation of the model. ProxT2I consistently outperforms score-based baseline models in terms of sampling speed and preference alignment while matching state-of-the-art results in quality. Notably, it achieves this with a smaller model size and reduced computational requirements, offering a lightweight yet effective solution for human-centric text-to-image generation tasks. This work advances the field by combining novel modeling techniques with reinforcement learning and large-scale data to improve generative performance and practicality. <div>
arXiv:2511.18742v2 Announce Type: replace 
Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiP: Taming Diffusion Models in Pixel Space</title>
<link>https://arxiv.org/abs/2511.18822</link>
<guid>https://arxiv.org/abs/2511.18822</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, Latent Diffusion Models, pixel space diffusion, Diffusion Transformer, ImageNet FID score<br /><br />Summary:<br /><br />1. Diffusion models typically face a trade-off between generation quality and computational efficiency, with Latent Diffusion Models (LDMs) offering efficiency but potentially losing information and lacking end-to-end training. 2. Pixel space diffusion models avoid reliance on VAEs but are computationally expensive for high-resolution image synthesis. 3. The paper proposes DiP, an efficient pixel space diffusion framework that decouples image generation into two stages: a global stage using a Diffusion Transformer (DiT) operating on large patches to build global structure, and a local stage with a lightweight Patch Detailer Head that restores fine details using contextual information. 4. This two-stage, synergistic approach achieves computational efficiency on par with LDMs without using a VAE, improving inference speeds by up to 10× compared to previous methods while increasing parameters by only 0.3%. 5. DiP achieves state-of-the-art performance with a Fréchet Inception Distance (FID) score of 1.79 on ImageNet at 256×256 resolution, demonstrating both high-quality generation and efficient computation. <div>
arXiv:2511.18822v2 Announce Type: replace 
Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.79 FID score on ImageNet 256$\times$256.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2511.19145</link>
<guid>https://arxiv.org/abs/2511.19145</guid>
<content:encoded><![CDATA[
<div> Activation Boundary Matching, Low-Rank Adaptation, ABM-LoRA, convergence acceleration, VTAB-1K

<br /><br />Summary: This paper introduces Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a novel initialization method designed to accelerate the convergence of low-rank adapters in neural networks. Unlike the conventional LoRA approach that starts from random initialization, ABM-LoRA aligns the adapter's activation boundaries with those of a pretrained model, improving the transfer of gradient information. This alignment minimizes information loss at initialization by ensuring the projection of full-parameter gradients is maximized within the adapter's subspace. As a result, ABM-LoRA achieves a lower initial loss and faster convergence during training. The authors validate their approach on a diverse set of tasks and architectures, including language understanding with T5-Base on GLUE, dialogue generation using LLaMA2-7B on WizardLM, and vision recognition with ViT-B/16 on VTAB-1K. Notably, on VTAB-1K, ABM-LoRA attains the highest accuracy compared to existing methods. The method also demonstrates substantial improvements on structured reasoning tasks that require geometric understanding, highlighting its potential for broad applicability across domains. This principled initialization strategy represents a significant step forward in enhancing the efficiency and effectiveness of low-rank adaptation techniques. <div>
arXiv:2511.19145v3 Announce Type: replace 
Abstract: We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.19220</link>
<guid>https://arxiv.org/abs/2511.19220</guid>
<content:encoded><![CDATA[
<div> Keywords: vision language models, medical visual question answering, visual grounding, Italy, EuropeMedQA<br /><br />Summary:<br /><br />1. The study evaluates the visual grounding capabilities of four cutting-edge vision language models (VLMs)—Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp—on Italian medical visual question answering tasks.<br /><br />2. Using 60 image-dependent questions from the EuropeMedQA Italian dataset, the researchers replaced the original medical images with blank placeholders to assess whether these models genuinely rely on visual input when answering questions.<br /><br />3. The findings reveal substantial variability in the degree to which models depend on visual information. GPT-4o demonstrated the strongest visual dependency, with a 27.9 percentage point drop in accuracy when images were omitted, decreasing from 83.2% to 55.3%.<br /><br />4. In contrast, GPT-5-mini, Gemini, and Claude showed only modest accuracy drops of 8.5, 2.4, and 5.6 percentage points respectively, indicating they rely more on textual shortcuts or reasoning than actual image analysis.<br /><br />5. Further examination of explanations generated by the models indicated that all provided confident but sometimes fabricated justifications for visual interpretations, highlighting differences in robustness and the critical need for thorough evaluations before clinical application. <div>
arXiv:2511.19220v2 Announce Type: replace 
Abstract: Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Plug-and-play Memory for Guiding Video Diffusion Models</title>
<link>https://arxiv.org/abs/2511.19229</link>
<guid>https://arxiv.org/abs/2511.19229</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformer, Video Generation, Memory Encoder, Physical Consistency, Temporal Coherence  

<br /><br />Summary:  
This paper addresses the challenge of physical law violations and lack of commonsense dynamics in Diffusion Transformer (DiT) based video generation models, which, despite producing high-quality and temporally coherent videos, fail to incorporate explicit world knowledge. Inspired by in-context memory in Transformer-based large language models, the authors empirically demonstrate that DiT’s hidden states can be manipulated via simple interventions. They find that low-pass and high-pass filters in the embedding space disentangle low-level appearance features from high-level physical and semantic information, facilitating targeted guidance. Based on these insights, the paper proposes a learnable memory encoder, DiT-Mem, built with stacked 3D CNNs, spectral filters, and self-attention layers, which encodes reference videos into compact memory tokens. These tokens are integrated into DiT’s self-attention mechanism as a plug-and-play memory during inference. Training freezes the diffusion backbone and optimizes only the memory encoder, allowing efficient learning with 150M parameters and 10K data samples. Extensive experiments demonstrate that DiT-Mem improves adherence to physical rules and enhances video fidelity. The code and datasets are publicly available to foster further research in memory-augmented video generation. <div>
arXiv:2511.19229v2 Announce Type: replace 
Abstract: Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation</title>
<link>https://arxiv.org/abs/2511.18833</link>
<guid>https://arxiv.org/abs/2511.18833</guid>
<content:encoded><![CDATA[
<div> Keywords: Video-to-Audio generation, Reinforcement Learning, Chain-of-Thought, Fast-GRPO, AudioCanvas<br /><br />Summary:  
1. PrismAudio is a novel framework designed for Video-to-Audio (V2A) generation that addresses the challenge of balancing four key perceptual aspects: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy.  
2. The framework innovatively applies Reinforcement Learning (RL) with specialized Chain-of-Thought (CoT) planning to break down complex perceptual goals into four distinct CoT modules: Semantic, Temporal, Aesthetic, and Spatial, each with its own targeted reward function.  
3. This modular CoT-reward setup allows multidimensional RL optimization that resolves the objective entanglement problem by guiding the model to improve jointly across all perceptual dimensions while maintaining interpretability.  
4. To enhance computational efficiency, the authors propose Fast-GRPO, a hybrid ODE-SDE sampling technique that significantly reduces training overhead compared to traditional GRPO implementations.  
5. The work also introduces AudioCanvas, a new V2A benchmark dataset featuring 300 single-event classes and 501 multi-event samples, designed to be more diverse, balanced, and realistic than existing datasets.  
6. Experimental evaluations demonstrate that PrismAudio achieves state-of-the-art performance on both the in-domain VGGSound dataset and the out-of-domain AudioCanvas benchmark, excelling across all four perceptual metrics.  
7. The project and codebase details are publicly accessible at https://PrismAudio-Project.github.io. <div>
arXiv:2511.18833v3 Announce Type: replace-cross 
Abstract: Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce PrismAudio, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables multidimensional RL optimization that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose Fast-GRPO, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce AudioCanvas, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at https://PrismAudio-Project.github.io.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SO-Bench: A Structural Output Evaluation of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.21750</link>
<guid>https://arxiv.org/abs/2511.21750</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.21750v1  
Keywords: Multimodal Large Language Models, Schema-grounded Extraction, Visual Structured Output, SO-Bench Benchmark, Multimodal Reasoning  

<br /><br />Summary:  
This paper addresses the challenge of evaluating schema-grounded information extraction and reasoning within multimodal large language models (MLLMs), particularly focusing on their capacity to produce structured outputs that comply with predefined data formats. Unlike existing benchmarks primarily centered on textual data, the study introduces SO-Bench, a comprehensive benchmark designed to assess and improve MLLMs' structured output capabilities across four diverse visual domains: user interface (UI) screens, natural images, documents, and charts. SO-Bench is constructed using more than 6,500 diverse JSON schemas and 1,800 curated image-schema pairs, all validated for high quality by human annotators. The authors conduct systematic benchmarking experiments involving both open-source and cutting-edge proprietary MLLMs, revealing persistent deficiencies in generating accurate, schema-compliant responses. These findings highlight the current limitations in multimodal structured reasoning. To address this gap, the study additionally performs training interventions that significantly enhance the structured generation performance of MLLMs. The paper emphasizes the importance of improved benchmarks like SO-Bench for fostering progress in the field and expresses the intent to publicly release the benchmark to benefit the research community developing multimodal structured reasoning systems. <div>
arXiv:2511.21750v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saddle-Free Guidance: Improved On-Manifold Sampling without Labels or Additional Training</title>
<link>https://arxiv.org/abs/2511.21863</link>
<guid>https://arxiv.org/abs/2511.21863</guid>
<content:encoded><![CDATA[
<div> Keywords: score-based generative models, saddle-free guidance, classifier-free guidance, image generation, diffusion models<br /><br />Summary: Score-based generative models rely on guidance to produce plausible, on-manifold samples, but existing methods like Classifier-Free Guidance (CFG) and Auto-Guidance have limitations such as requiring labeled data or additional model training. The authors discover that the positive curvature of log density estimates in saddle regions can serve as an effective guidance signal for these models. They introduce a novel method called saddle-free guidance (SFG), which tracks the maximal positive curvature of the log density during generation to guide a single score-based model without the need for extra training or labeled datasets. SFG has computational costs comparable to CFG and is compatible with standard diffusion and flow matching models. Experimental results show that SFG achieves state-of-the-art performance in unconditional ImageNet-512 generation based on FID and FD-DINOv2 metrics. Furthermore, combining SFG with Auto-Guidance leads to even better FD-DINOv2 scores on unconditional samples. Additional tests with FLUX.1-dev and Stable Diffusion v3.5 demonstrate that SFG enhances the diversity of generated images while preserving strong prompt adherence and image fidelity, making it a versatile and efficient approach for score-based generative modeling. <div>
arXiv:2511.21863v1 Announce Type: new 
Abstract: Score-based generative models require guidance in order to generate plausible, on-manifold samples. The most popular guidance method, Classifier-Free Guidance (CFG), is only applicable in settings with labeled data and requires training an additional unconditional score-based model. More recently, Auto-Guidance adopts a smaller, less capable version of the original model to guide generation. While each method effectively promotes the fidelity of generated data, each requires labeled data or the training of additional models, making it challenging to guide score-based models when (labeled) training data are not available or training new models is not feasible.
  We make the surprising discovery that the positive curvature of log density estimates in saddle regions provides strong guidance for score-based models. Motivated by this, we develop saddle-free guidance (SFG) which maintains estimates of maximal positive curvature of the log density to guide individual score-based models. SFG has the same computational cost of classifier-free guidance, does not require additional training, and works with off-the-shelf diffusion and flow matching models. Our experiments indicate that SFG achieves state-of-the-art FID and FD-DINOv2 metrics in single-model unconditional ImageNet-512 generation. When SFG is combined with Auto-Guidance, its unconditional samples achieve general state-of-the-art in FD-DINOv2 score. Our experiments with FLUX.1-dev and Stable Diffusion v3.5 indicate that SFG boosts the diversity of output images compared to CFG while maintaining excellent prompt adherence and image fidelity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniArt: Unified 3D Representation for Generating 3D Articulated Objects with Open-Set Articulation</title>
<link>https://arxiv.org/abs/2511.21887</link>
<guid>https://arxiv.org/abs/2511.21887</guid>
<content:encoded><![CDATA[
<div> articulated 3D objects, diffusion framework, unified latent representation, articulation prediction, PartNet-Mobility benchmark  

<br /><br />Summary:  
The paper introduces UniArt, a diffusion-based framework designed to synthesize fully articulated 3D objects directly from a single input image in an end-to-end manner. This approach contrasts with prior multi-stage techniques by establishing a unified latent representation that simultaneously encodes geometry, texture, part segmentation, and kinematic parameters, enabling a more cohesive output. A novel reversible joint-to-voxel embedding is proposed, which spatially aligns articulation features with volumetric geometry, allowing the model to learn coherent motion patterns along with the structural formation of the object. Additionally, the authors reformulate the articulation type prediction task as an open-set problem, removing the constraint of fixed joint categories, and thereby improving generalization to novel joints and unseen object types. The effectiveness and versatility of UniArt are validated through experiments conducted on the PartNet-Mobility benchmark, where it achieves state-of-the-art performance in both mesh quality and articulation accuracy. This work addresses the challenge of scalable and automated creation of articulated 3D assets, which are crucial for applications in realistic simulation and embodied robotics. <div>
arXiv:2511.21887v1 Announce Type: new 
Abstract: Articulated 3D objects play a vital role in realistic simulation and embodied robotics, yet manually constructing such assets remains costly and difficult to scale. In this paper, we present UniArt, a diffusion-based framework that directly synthesizes fully articulated 3D objects from a single image in an end-to-end manner. Unlike prior multi-stage techniques, UniArt establishes a unified latent representation that jointly encodes geometry, texture, part segmentation, and kinematic parameters. We introduce a reversible joint-to-voxel embedding, which spatially aligns articulation features with volumetric geometry, enabling the model to learn coherent motion behaviors alongside structural formation. Furthermore, we formulate articulation type prediction as an open-set problem, removing the need for fixed joint semantics and allowing generalization to novel joint categories and unseen object types. Experiments on the PartNet-Mobility benchmark demonstrate that UniArt achieves state-of-the-art mesh quality and articulation accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images</title>
<link>https://arxiv.org/abs/2511.21902</link>
<guid>https://arxiv.org/abs/2511.21902</guid>
<content:encoded><![CDATA[
<div> Keywords: tumor microenvironment, Whole Slide Images, PathReasoning, multi-modal reasoning, digital pathology<br /><br />Summary:<br /><br />1. The paper addresses the challenge of analyzing Whole Slide Images (WSIs) in cancer diagnosis, prognosis, and treatment by deciphering the tumor microenvironment. WSIs are extremely large, containing over 10 billion pixels, making manual navigation difficult and time-consuming.<br /><br />2. Inspired by how pathologists navigate WSIs through sampling, reasoning, and self-reflection, the authors propose "PathReasoning," a multi-modal reasoning agent that iteratively navigates WSIs by multiple rounds of reasoning and refinement.<br /><br />3. PathReasoning starts with randomly sampled candidate regions and improves selections through self-reflection and reasoning that connects visual data with clinical questions, ultimately proposing new regions to explore.<br /><br />4. This iterative process forms a reasoning chain that guides attention to diagnostically relevant areas, converting each slide into a sequence of question-guided views enabling efficient identification of informative regions of interest (ROIs) without dense pixel-level annotations.<br /><br />5. The approach outperforms strong ROI-selection baselines by 6.7% and 3.1% AUROC on cancer subtyping and longitudinal analysis tasks, respectively. It also enhances breast cancer report generation accuracy by 10% over GPT-4o.<br /><br />6. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, facilitating efficient slide review, consistent diagnostics, comprehensive reporting, and evidence traceability in digital pathology workflows. <div>
arXiv:2511.21902v1 Announce Type: new 
Abstract: Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed "PathReasoning", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Parameter Optimization for Robust Remote Photoplethysmography</title>
<link>https://arxiv.org/abs/2511.21903</link>
<guid>https://arxiv.org/abs/2511.21903</guid>
<content:encoded><![CDATA[
<div> rPPG, PRISM, photometric detrending, online parameter adaptation, vital sign monitoring

<br /><br />Summary: This paper presents Projection-based Robust Signal Mixing (PRISM), a novel training-free algorithm for remote photoplethysmography (rPPG) that enables contactless vital sign monitoring using standard RGB cameras. Unlike existing methods that depend on fixed parameters tailored to specific lighting and camera setups, PRISM adapts parameters online through signal quality assessment, enhancing robustness across diverse environments. The method jointly optimizes photometric detrending and color mixing, improving signal quality without requiring any training phase. PRISM achieves state-of-the-art results among unsupervised rPPG methods, with mean absolute errors (MAE) of 0.77 bpm on the PURE dataset and 0.66 bpm on UBFC-rPPG, alongside accuracies of 97.3% and 97.5%, respectively, under a 5 bpm threshold. Statistical analysis confirms that PRISM performs on par with the best supervised methods (with p-values greater than 0.2), despite being training-free. The algorithm also operates efficiently in real-time on standard CPUs, supporting practical applications. Overall, PRISM validates that adaptive time series optimization significantly advances the accuracy and adaptability of rPPG vital sign monitoring across varying conditions without the need for supervised training. <div>
arXiv:2511.21903v1 Announce Type: new 
Abstract: Remote photoplethysmography (rPPG) enables contactless vital sign monitoring using standard RGB cameras. However, existing methods rely on fixed parameters optimized for particular lighting conditions and camera setups, limiting adaptability to diverse deployment environments. This paper introduces the Projection-based Robust Signal Mixing (PRISM) algorithm, a training-free method that jointly optimizes photometric detrending and color mixing through online parameter adaptation based on signal quality assessment. PRISM achieves state-of-the-art performance among unsupervised methods, with MAE of 0.77 bpm on PURE and 0.66 bpm on UBFC-rPPG, and accuracy of 97.3\% and 97.5\% respectively at a 5 bpm threshold. Statistical analysis confirms PRISM performs equivalently to leading supervised methods ($p > 0.2$), while maintaining real-time CPU performance without training. This validates that adaptive time series optimization significantly improves rPPG across diverse conditions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Multimodal Cancer Prototyping with Whole Slide Images and Incompletely Paired Genomics</title>
<link>https://arxiv.org/abs/2511.21937</link>
<guid>https://arxiv.org/abs/2511.21937</guid>
<content:encoded><![CDATA[
<div> Multimodal learning, Precision oncology, Genomics imputation, Whole slide images, Prototype-based fusion  

<br /><br />Summary:  
This paper introduces a flexible multimodal prototyping framework designed to integrate histology whole slide images (WSIs) and incomplete genomic data for precision oncology. The framework addresses challenges posed by phenotypic and genotypic heterogeneity that affect intra-modal feature quality and complicate cross-modal integration. The proposed method includes four key components: (1) Biological Prototyping leverages text prompting and prototype-wise weighting to enhance biological relevance in data representation; (2) Multiview Alignment aligns data across samples and distributions to improve consistency between modalities; (3) Bipartite Fusion captures both shared and modality-specific features to achieve effective multimodal fusion; (4) Semantic Genomics Imputation handles scenarios where genomic data is partially or completely missing, ensuring robustness in real-world clinical settings. Extensive experiments on multiple downstream tasks demonstrate that this approach consistently outperforms current state-of-the-art multimodal methods. The framework facilitates better integration of histological and genomic information for precision oncology, even when data are incomplete. The authors provide the code for reproducibility and further research at their GitHub repository, supporting transparency and accessibility. <div>
arXiv:2511.21937v1 Announce Type: new 
Abstract: Multimodal approaches that integrate histology and genomics hold strong potential for precision oncology. However, phenotypic and genotypic heterogeneity limits the quality of intra-modal representations and hinders effective inter-modal integration. Furthermore, most existing methods overlook real-world clinical scenarios where genomics may be partially missing or entirely unavailable. We propose a flexible multimodal prototyping framework to integrate whole slide images and incomplete genomics for precision oncology. Our approach has four key components: 1) Biological Prototyping using text prompting and prototype-wise weighting; 2) Multiview Alignment through sample- and distribution-wise alignments; 3) Bipartite Fusion to capture both shared and modality-specific information for multimodal fusion; and 4) Semantic Genomics Imputation to handle missing data. Extensive experiments demonstrate the consistent superiority of the proposed method compared to other state-of-the-art approaches on multiple downstream tasks. The code is available at https://github.com/helenypzhang/Interpretable-Multimodal-Prototyping.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AmodalGen3D: Generative Amodal 3D Object Reconstruction from Sparse Unposed Views</title>
<link>https://arxiv.org/abs/2511.21945</link>
<guid>https://arxiv.org/abs/2511.21945</guid>
<content:encoded><![CDATA[
<div> Keywords: AmodalGen3D, 3D reconstruction, occlusion, sparse views, cross attention<br /><br />Summary:<br /><br />1. AmodalGen3D is a novel generative framework designed for reconstructing complete 3D objects from a limited number of unposed and partially occluded views, addressing challenges posed by unseen object surfaces in real-world scenarios.<br /><br />2. Unlike traditional multi-view and inpainting-based methods, which often produce incomplete or inconsistent reconstructions under sparse and occluded conditions, AmodalGen3D infers full, occlusion-free geometry and appearance.<br /><br />3. The model uniquely integrates 2D amodal completion priors with multi-view stereo geometry and incorporates a View-Wise Cross Attention mechanism to effectively fuse sparse-view features, alongside a Stereo-Conditioned Cross Attention module for inferring unobserved structures.<br /><br />4. By jointly modeling visible and hidden parts of objects, AmodalGen3D generates reconstructions that comply with sparse-view constraints while plausibly hallucinating unseen regions, achieving higher fidelity and completeness.<br /><br />5. Extensive experiments on synthetic and real datasets confirm that AmodalGen3D outperforms existing methods under occlusion-heavy, sparse-view scenarios, offering significant benefits for applications in robotics, augmented/virtual reality, and embodied AI. <div>
arXiv:2511.21945v1 Announce Type: new 
Abstract: Reconstructing 3D objects from a few unposed and partially occluded views is a common yet challenging problem in real-world scenarios, where many object surfaces are never directly observed. Traditional multi-view or inpainting-based approaches struggle under such conditions, often yielding incomplete or geometrically inconsistent reconstructions. We introduce AmodalGen3D, a generative framework for amodal 3D object reconstruction that infers complete, occlusion-free geometry and appearance from arbitrary sparse inputs. The model integrates 2D amodal completion priors with multi-view stereo geometry conditioning, supported by a View-Wise Cross Attention mechanism for sparse-view feature fusion and a Stereo-Conditioned Cross Attention module for unobserved structure inference. By jointly modeling visible and hidden regions, AmodalGen3D faithfully reconstructs 3D objects that are consistent with sparse-view constraints while plausibly hallucinating unseen parts. Experiments on both synthetic and real-world datasets demonstrate that AmodalGen3D achieves superior fidelity and completeness under occlusion-heavy sparse-view settings, addressing a pressing need for object-level 3D scene reconstruction in robotics, AR/VR, and embodied AI applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAPVid-360: Tracking Any Point in 360 from Narrow Field of View Video</title>
<link>https://arxiv.org/abs/2511.21946</link>
<guid>https://arxiv.org/abs/2511.21946</guid>
<content:encoded><![CDATA[
<div> TAPVid-360, panoramic understanding, allocentric representation, 360 video, point tracking<br /><br />Summary:<br /><br />1. The paper addresses a key limitation in current artificial vision systems, which struggle to maintain persistent, panoramic, and allocentric understanding of scenes, especially beyond the visible field of view. This contrasts with human ability to create comprehensive mental models and maintain object permanence.<br /><br />2. The authors introduce TAPVid-360, a novel task designed to predict the 3D direction to queried scene points throughout a video sequence, including those far outside the narrow field of view, enabling allocentric scene representation.<br /><br />3. TAPVid-360 avoids the need for dynamic 4D ground truth scene models during training by leveraging 360-degree videos as supervision, which are resampled into narrow FOV perspectives. Ground truth directions for points are computed by tracking them across the full panorama using an efficient 2D pipeline.<br /><br />4. A new dataset and benchmark named TAPVid360-10k is presented, containing 10,000 perspective videos annotated with ground truth directional point tracking to facilitate research and evaluation on this task.<br /><br />5. The paper proposes a baseline method by adapting CoTracker v3 to predict per-point rotation updates for direction prediction, achieving superior performance compared to existing TAP and TAPVid 3D methods, thus demonstrating the effectiveness of the approach. <div>
arXiv:2511.21946v1 Announce Type: new 
Abstract: Humans excel at constructing panoramic mental models of their surroundings, maintaining object permanence and inferring scene structure beyond visible regions. In contrast, current artificial vision systems struggle with persistent, panoramic understanding, often processing scenes egocentrically on a frame-by-frame basis. This limitation is pronounced in the Track Any Point (TAP) task, where existing methods fail to track 2D points outside the field of view. To address this, we introduce TAPVid-360, a novel task that requires predicting the 3D direction to queried scene points across a video sequence, even when far outside the narrow field of view of the observed video. This task fosters learning allocentric scene representations without needing dynamic 4D ground truth scene models for training. Instead, we exploit 360 videos as a source of supervision, resampling them into narrow field-of-view perspectives while computing ground truth directions by tracking points across the full panorama using a 2D pipeline. We introduce a new dataset and benchmark, TAPVid360-10k comprising 10k perspective videos with ground truth directional point tracking. Our baseline adapts CoTracker v3 to predict per-point rotations for direction updates, outperforming existing TAP and TAPVid 3D methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WalkCLIP: Multimodal Learning for Urban Walkability Prediction</title>
<link>https://arxiv.org/abs/2511.21947</link>
<guid>https://arxiv.org/abs/2511.21947</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban walkability, multimodal framework, satellite imagery, GPT-4o captions, population dynamics<br /><br />Summary:<br /><br />1. Urban walkability is essential for public health, sustainability, and quality of life, but traditional assessment methods like surveys and field audits are costly and hard to scale.<br />2. Prior approaches using single data sources such as satellite imagery, street view imagery, or population indicators only capture limited dimensions of the walking environment.<br />3. Satellite data provides an overhead view of the built environment but lacks pedestrian-level details, street view imagery focuses on ground-level conditions but misses broader context, and population dynamics reflect activity patterns but not visual environment characteristics.<br />4. The introduced WalkCLIP framework integrates these complementary data sources — satellite images, GPT-4o-generated captions from street view images, and population dynamics — to learn rich, walkability-aware vision-language representations.<br />5. WalkCLIP incorporates a spatial aggregation module for neighborhood context and fuses visual with behavioral signals for accurate predictions.<br />6. When evaluated across 4,660 locations in Minneapolis-Saint Paul, WalkCLIP outperforms both unimodal and multimodal baseline models in predictive accuracy and spatial alignment, demonstrating the advantage of combining diverse data modalities to assess walkability effectively. <div>
arXiv:2511.21947v1 Announce Type: new 
Abstract: Urban walkability is a cornerstone of public health, sustainability, and quality of life. Traditional walkability assessments rely on surveys and field audits, which are costly and difficult to scale. Recent studies have used satellite imagery, street view imagery, or population indicators to estimate walkability, but these single-source approaches capture only one dimension of the walking environment. Satellite data describe the built environment from above, but overlook the pedestrian perspective. Street view imagery captures conditions at the ground level, but lacks broader spatial context. Population dynamics reveal patterns of human activity but not the visual form of the environment. We introduce WalkCLIP, a multimodal framework that integrates these complementary viewpoints to predict urban walkability. WalkCLIP learns walkability-aware vision-language representations from GPT-4o generated image captions, refines these representations with a spatial aggregation module that incorporates neighborhood context, and fuses the resulting features with representations from a population dynamics foundation model. Evaluated at 4,660 locations throughout Minneapolis-Saint Paul, WalkCLIP outperforms unimodal and multimodal baselines in both predictive accuracy and spatial alignment. These results show that the integration of visual and behavioral signals yields reliable predictions of the walking environment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepGI: Explainable Deep Learning for Gastrointestinal Image Classification</title>
<link>https://arxiv.org/abs/2511.21959</link>
<guid>https://arxiv.org/abs/2511.21959</guid>
<content:encoded><![CDATA[
<div> Gastrointestinal imaging, deep learning, disease classification, explainable AI, medical benchmarks<br /><br />Summary:<br /><br />This paper introduces a novel gastrointestinal medical imaging dataset containing 4,000 endoscopic images categorized into four disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging advanced deep learning models, the study addresses typical challenges in endoscopic imaging such as variable lighting, changing camera angles, and image artifacts. The evaluation shows that VGG16 and MobileNetV2 models achieved the highest test accuracy of 96.5%, while the Xception model scored 94.24%, setting strong benchmarks for automated disease classification in this domain. The work also integrates explainable AI techniques using Grad-CAM visualizations, which highlight the key image regions influencing the models' predictions, thereby improving the interpretability and potential clinical trustworthiness of the AI system. Through comprehensive experiments, the study demonstrates the feasibility of robust, accurate, and interpretable medical image analysis under realistic and challenging conditions. This contribution advances gastrointestinal computer-aided diagnosis by providing new comparative insights, original benchmark results, and visual explanation tools. It emphasizes the critical role of diverse, clinically relevant datasets and explainability in the development and deployment of medical AI solutions. <div>
arXiv:2511.21959v1 Announce Type: new 
Abstract: This paper presents a comprehensive comparative model analysis on a novel gastrointestinal medical imaging dataset, comprised of 4,000 endoscopic images spanning four critical disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging state-of-the-art deep learning techniques, the study confronts common endoscopic challenges such as variable lighting, fluctuating camera angles, and frequent imaging artifacts. The best performing models, VGG16 and MobileNetV2, each achieved a test accuracy of 96.5%, while Xception reached 94.24%, establishing robust benchmarks and baselines for automated disease classification. In addition to strong classification performance, the approach includes explainable AI via Grad-CAM visualization, enabling identification of image regions most influential to model predictions and enhancing clinical interpretability. Experimental results demonstrate the potential for robust, accurate, and interpretable medical image analysis even in complex real-world conditions. This work contributes original benchmarks, comparative insights, and visual explanations, advancing the landscape of gastrointestinal computer-aided diagnosis and underscoring the importance of diverse, clinically relevant datasets and model explainability in medical AI research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAT3D: Physics-Augmented Text-to-3D Scene Generation</title>
<link>https://arxiv.org/abs/2511.21978</link>
<guid>https://arxiv.org/abs/2511.21978</guid>
<content:encoded><![CDATA[
<div> Keywords: PAT3D, text-to-3D generation, physics simulation, rigid-body simulation, scene optimization<br /><br />Summary:<br />1. PAT3D is introduced as the first framework that combines vision-language models with physics-based simulation to generate 3D scenes directly from text prompts, ensuring physical plausibility and simulation readiness. <br />2. The system generates 3D objects and infers their spatial relationships, organizing them into a hierarchical scene tree which serves as initial conditions for a differentiable rigid-body simulator. <br />3. This simulator models object interactions under gravity and guides the scene toward a stable static equilibrium without object interpenetrations. <br />4. A simulation-in-the-loop optimization procedure is employed to refine the scene by enforcing physical stability, eliminating intersections, and improving semantic alignment with the input text. <br />5. Experimental results demonstrate that PAT3D significantly surpasses existing methods in terms of physical realism, semantic consistency with prompts, and visual quality, while also providing simulation-ready scenes suitable for downstream applications such as scene editing and robotic manipulation. Code and data release are planned upon acceptance. <div>
arXiv:2511.21978v1 Announce Type: new 
Abstract: We introduce PAT3D, the first physics-augmented text-to-3D scene generation framework that integrates vision-language models with physics-based simulation to produce physically plausible, simulation-ready, and intersection-free 3D scenes. Given a text prompt, PAT3D generates 3D objects, infers their spatial relations, and organizes them into a hierarchical scene tree, which is then converted into initial conditions for simulation. A differentiable rigid-body simulator ensures realistic object interactions under gravity, driving the scene toward static equilibrium without interpenetrations. To further enhance scene quality, we introduce a simulation-in-the-loop optimization procedure that guarantees physical stability and non-intersection, while improving semantic consistency with the input prompt. Experiments demonstrate that PAT3D substantially outperforms prior approaches in physical plausibility, semantic consistency, and visual quality. Beyond high-quality generation, PAT3D uniquely enables simulation-ready 3D scenes for downstream tasks such as scene editing and robotic manipulation. Code and data will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DialBench: Towards Accurate Reading Recognition of Pointer Meter using Large Foundation Models</title>
<link>https://arxiv.org/abs/2511.21982</link>
<guid>https://arxiv.org/abs/2511.21982</guid>
<content:encoded><![CDATA[
<div> pointer meters, dial reading, vision-language model, physical relation injection, RPM-10K dataset<br /><br />Summary:<br /><br />1. The paper addresses the challenge of precise reading recognition for pointer meters, which is critical for smart power systems yet faces difficulties from reflections, occlusions, dynamic viewing angles, and pointer-scale similarity.<br /><br />2. To overcome the lack of large-scale datasets for robust model development, the authors introduce RPM-10K, a new comprehensive benchmark dataset comprising 10,730 images of meter dials that capture these challenging conditions.<br /><br />3. Built on this dataset, they propose MRLM, a novel vision-language model that integrates physical relation injection to improve understanding by explicitly encoding the geometric and causal relationships between meter pointers and scale markings.<br /><br />4. MRLM departs from typical image-level correlation learning by aligning perception with physical reasoning, leveraging world-model perspectives to more accurately interpret dial configurations.<br /><br />5. The model utilizes cross-attentional fusion and adaptive expert selection mechanisms to generate precise numeric readings, and extensive experiments validate its effectiveness on the RPM-10K benchmark.<br /><br />6. The authors commit to releasing both the RPM-10K dataset and the MRLM source code, facilitating further progress in dial reading recognition research. <div>
arXiv:2511.21982v1 Announce Type: new 
Abstract: The precise reading recognition of pointer meters plays a key role in smart power systems, but existing approaches remain fragile due to challenges like reflections, occlusions, dynamic viewing angles, and overly between thin pointers and scale markings. Up to now, this area still lacks large-scale datasets to support the development of robust algorithms. To address these challenges, this paper first presents a new large-scale benchmark dataset for dial reading, termed RPM-10K, which contains 10730 meter images that fully reflect the aforementioned key challenges. Built upon the dataset, we propose a novel vision-language model for pointer meter reading recognition, termed MRLM, based on physical relation injection. Instead of exhaustively learning image-level correlations, MRLM explicitly encodes the geometric and causal relationships between the pointer and the scale, aligning perception with physical reasoning in the spirit of world-model perspectives. Through cross-attentional fusion and adaptive expert selection, the model learns to interpret dial configurations and generate precise numeric readings. Extensive experiments fully validated the effectiveness of our proposed framework on the newly proposed benchmark dataset. Both the dataset and source code will be released on https://github.com/Event-AHU/DialBench
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPBoost: Progressive Prompt Boosting for Text-Driven Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.21984</link>
<guid>https://arxiv.org/abs/2511.21984</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, text prompts, visual prompts, zero-shot learning, pseudo-bounding boxes<br /><br />Summary:<br /><br />1. The paper addresses the shortcomings of text-prompted medical image segmentation models, which struggle with spatial precision and domain shifts, and contrasts them with visual-prompted models that use precise bounding-box (bbox) prompts to improve segmentation accuracy.<br /><br />2. It highlights the challenge of obtaining precise visual prompts in clinical settings due to the high cost and difficulty.<br /><br />3. The authors propose PPBoost (Progressive Prompt-Boosting), a framework that converts weak text-derived signals into strong, spatially grounded visual prompts without using any image- or pixel-level segmentation labels, thus operating strictly in a zero-shot regime.<br /><br />4. PPBoost leverages a vision-language model to generate initial pseudo-bboxes from textual descriptions, applies an uncertainty-aware filtering to select reliable bboxes, and trains a pseudo-labeled detector on these bboxes to produce high-quality bounding boxes.<br /><br />5. During inference, PPBoost refines these bboxes by expanding them to tightly cover target anatomical structures, providing enhanced spatial cues that guide existing segmentation models to create more accurate dense masks.<br /><br />6. Empirical results on three datasets with diverse modalities and anatomies show PPBoost consistently improves metrics like Dice and Normalized Surface Distance, surpassing both text- and visual-prompt baselines and even outperforming few-shot segmentation methods without labeled data.<br /><br />7. The framework is compatible with multiple common visual segmentation model backbones, demonstrating its generalizability and practical utility in medical image segmentation tasks. <div>
arXiv:2511.21984v1 Announce Type: new 
Abstract: Text-prompted foundation models for medical image segmentation offer an intuitive way to delineate anatomical structures from natural language queries, but their predictions often lack spatial precision and degrade under domain shift. In contrast, visual-prompted models achieve strong segmentation performance across diverse modalities by leveraging spatial cues of precise bounding-box (bbox) prompts to guide the segmentation of target lesions. However, it is costly and challenging to obtain the precise visual prompts in clinical practice. We propose PPBoost (Progressive Prompt-Boosting), a framework that bridges these limitations by transforming weak text-derived signals into strong, spatially grounded visual prompts, operating under a strict zero-shot regime with no image- or pixel-level segmentation labels. PPBoost first uses a vision-language model to produce initial pseudo-bboxes conditioned on the textual object descriptions and applies an uncertainty-aware criterion to filter unreliable predictions. The retained image-bboxes pairs are then leveraged to train a pseudo-labeled detector, producing the high-quality bboxes for the query images. During inference, PPBoost further refines the generated bboxes by appropriately expanding them to tightly cover the target anatomical structures. The enhanced spatially-grounding bbox prompts guide existing segmentation models to generate final dense masks, effectively amplifying weak text cues into strong spatial guidance. Across three datasets spanning diverse modalities and anatomies, PPBoost consistently improves Dice and Normalized Surface Distance over text- and visual-prompted baselines and, notably, surpasses few-shot segmentation models without using labeled data. PPBoost can generalize to multiple typical visual segmentation model backbones.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?</title>
<link>https://arxiv.org/abs/2511.21998</link>
<guid>https://arxiv.org/abs/2511.21998</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-modal LLM, interactive guidance, live coaching, Qualcomm Interactive Cooking, mistake detection<br /><br />Summary:  
1. The paper addresses a key limitation in current multi-modal Large Language Models (LLMs), which excel in conversation but lack the capability to provide live, interactive, step-by-step guidance.  
2. Effective live guidance requires the model to not just give instructions but also detect whether those instructions have been successfully followed, identify user mistakes, and alert users immediately, all in real-time and asynchronously rather than turn-based interaction.  
3. To facilitate research in this area, the authors introduce the Qualcomm Interactive Cooking benchmark and dataset, based on CaptainCook4D, which uniquely includes annotations of user mistakes and their subsequent corrections during task execution.  
4. This dataset features densely timed annotations for both instructions and feedback messages, with mistake alerts precisely timestamped according to the visual occurrence in the video stream, enabling evaluation of models’ real-time interactive capabilities.  
5. The authors evaluate existing state-of-the-art multi-modal LLMs on this new benchmark and propose LiveMamba, a novel streaming multi-modal LLM designed specifically for interactive instructional guidance, establishing a strong baseline for future work in live, situated coaching scenarios. <div>
arXiv:2511.21998v1 Announce Type: new 
Abstract: Multi-modal Large Language Models (LLM) have advanced conversational abilities but struggle with providing live, interactive step-by-step guidance, a key capability for future AI assistants. Effective guidance requires not only delivering instructions but also detecting their successful execution, as well as identifying and alerting users to mistakes, all of which has to happen in real-time. This requires models that are not turn-based, but that can react asynchronously to a video stream, as well as video data showing users performing tasks including mistakes and their corrections. To this end, we introduce Qualcomm Interactive Cooking, a new benchmark and dataset built upon CaptainCook4D, which contains user mistakes during task execution. Our dataset and benchmark features densely annotated, timed instructions and feedback messages, specifically including mistake alerts precisely timestamped to their visual occurrence in the video. We evaluate state-of-the-art multi-modal LLMs on the Qualcomm Interactive Cooking benchmark and introduce LiveMamba, a streaming multi-modal LLM designed for interactive instructional guidance. This work provides the first dedicated benchmark and a strong baseline for developing and evaluating on live, situated coaching.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamFlow: Theory, Algorithm, and Implementation for High-Efficiency Rectified Flow Generation</title>
<link>https://arxiv.org/abs/2511.22009</link>
<guid>https://arxiv.org/abs/2511.22009</guid>
<content:encoded><![CDATA[
<div> Rectified Flow, Flow Matching, acceleration pipeline, dynamic TensorRT, image generation speed<br /><br />Summary:  
This article addresses the challenge of accelerating Rectified Flow generative models, which differ theoretically and structurally from existing diffusion models, making current acceleration techniques inapplicable. The authors present a comprehensive acceleration pipeline that innovates in theory, system design, and reasoning strategies to enhance generation speed significantly. Key methods include batch processing employing a newly designed velocity field, vectorization of heterogeneous time-step batch processing, and dynamic compilation using TensorRT tailored for these novel approaches. These combined techniques effectively optimize computational efficiency and throughput specifically for flow-based generative models. Experimentally, while current public acceleration methods only improve speed by about 18%, the proposed pipeline achieves an extraordinary acceleration of up to 611% for generating 512×512 images, demonstrating a marked advancement beyond existing methods. This work not only advances the state of acceleration in flow-based generative models but also provides a generalizable framework that can potentially be adapted to related models, thus significantly enhancing practical applicability in real-time and high-resolution image generation tasks. <div>
arXiv:2511.22009v1 Announce Type: new 
Abstract: New technologies such as Rectified Flow and Flow Matching have significantly improved the performance of generative models in the past two years, especially in terms of control accuracy, generation quality, and generation efficiency. However, due to some differences in its theory, design, and existing diffusion models, the existing acceleration methods cannot be directly applied to the Rectified Flow model. In this article, we have comprehensively implemented an overall acceleration pipeline from the aspects of theory, design, and reasoning strategies. This pipeline uses new methods such as batch processing with a new velocity field, vectorization of heterogeneous time-step batch processing, and dynamic TensorRT compilation for the new methods to comprehensively accelerate related models based on flow models. Currently, the existing public methods usually achieve an acceleration of 18%, while experiments have proved that our new method can accelerate the 512*512 image generation speed to up to 611%, which is far beyond the current non-generalized acceleration methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis</title>
<link>https://arxiv.org/abs/2511.22018</link>
<guid>https://arxiv.org/abs/2511.22018</guid>
<content:encoded><![CDATA[
<div> Keywords: medical diagnosis, reinforcement learning, visual reasoning, expert guidance, medical VQA<br /><br />Summary:<br /><br />1. The paper addresses the challenge of creating accurate medical diagnostic AI systems that mimic clinicians' progressive visual focusing and iterative reasoning during diagnosis.<br />2. Existing vision-language models using on-policy reinforcement learning with verifiable rewards (RLVR) often favor superficially coherent but clinically inaccurate reasoning paths.<br />3. MedEyes is introduced as a novel reinforcement learning framework that incorporates off-policy expert guidance by leveraging expert visual search trajectories as external behavioral signals, steering the model toward clinically aligned visual reasoning.<br />4. The Gaze-guided Reasoning Navigator (GRN) emulates clinical diagnostic strategies through a dual-mode exploration that both scans systematically for abnormalities and drills down for detailed regional analysis.<br />5. To encourage a balance between imitating expert behavior and autonomous exploration, the Confidence Value Sampler (CVS) applies nucleus sampling and adaptive termination to generate diverse yet credible reasoning paths.<br />6. The dual-stream GRPO optimization framework separates on-policy and off-policy learning signals, which reduces reward assimilation issues and entropy collapse.<br />7. Experimental results demonstrate MedEyes improves performance by +8.5% on multiple medical Visual Question Answering (VQA) benchmarks, showing its effectiveness in developing interpretable medical AI systems. <div>
arXiv:2511.22018v1 Announce Type: new 
Abstract: Accurate medical diagnosis often involves progressive visual focusing and iterative reasoning, characteristics commonly observed in clinical workflows. While recent vision-language models demonstrate promising chain-of-thought (CoT) reasoning capabilities via reinforcement learning with verifiable rewards (RLVR), their purely on-policy learning paradigm tends to reinforce superficially coherent but clinically inaccurate reasoning paths. We propose MedEyes, a novel reinforcement learning framework that dynamically models clinician-style diagnostic reasoning by progressively attending to and interpreting relevant medical image regions. By incorporating off-policy expert guidance, MedEyes converts expert visual search trajectories into structured external behavioral signals, guiding the model toward clinically aligned visual reasoning. We design the Gaze-guided Reasoning Navigator (GRN) to emulate the diagnostic process through a dual-mode exploration strategy, scanning for systematic abnormality localization and drilling for detailed regional analysis. To balance expert imitation and autonomous discovery, we introduce the Confidence Value Sampler (CVS), which employs nucleus sampling and adaptive termination to create diverse yet credible exploration paths. Finally, the dual-stream GRPO optimization framework decouples on-policy and off-policy learning signals, mitigating reward assimilation and entropy collapse. Experiments demonstrate that MedEyes achieves an average performance improvement of +8.5\% across multiple medical VQA benchmarks, validating MedEyes's potential in building interpretable medical AI systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22019</link>
<guid>https://arxiv.org/abs/2511.22019</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, uncertainty estimation, contrastive learning, feature projection, error detection<br /><br />Summary:<br /><br />This paper addresses the issue of overconfident misclassifications in vision-language models (VLMs) such as CLIP, which limits their reliability in safety-critical applications. The authors propose a training-free, post-hoc uncertainty estimation method designed specifically for contrastive VLMs to detect erroneous predictions effectively. The core of the approach involves measuring visual feature consistency within a class by using a combination of feature projection and multivariate Gaussian modeling to produce class-specific probabilistic embeddings. This method is VLM-agnostic and does not require any fine-tuning, making it adaptable and easy to apply to different models. Additionally, it demonstrates robustness to distribution shifts and is effective even when only a few (as few as 10) training images per class are available. To validate the approach, extensive experiments are conducted on several benchmark datasets, including ImageNet, Flowers102, Food101, EuroSAT, and DTD. The results show that the proposed method achieves state-of-the-art performance in error detection, significantly outperforming both deterministic and probabilistic baseline methods applied to VLMs. The implementation code is made publicly available, promoting reproducibility and further research. <div>
arXiv:2511.22019v1 Announce Type: new 
Abstract: Vision-language models (VLMs), such as CLIP, have gained popularity for their strong open vocabulary classification performance, but they are prone to assigning high confidence scores to misclassifications, limiting their reliability in safety-critical applications. We introduce a training-free, post-hoc uncertainty estimation method for contrastive VLMs that can be used to detect erroneous predictions. The key to our approach is to measure visual feature consistency within a class, using feature projection combined with multivariate Gaussians to create class-specific probabilistic embeddings. Our method is VLM-agnostic, requires no fine-tuning, demonstrates robustness to distribution shift, and works effectively with as few as 10 training images per class. Extensive experiments on ImageNet, Flowers102, Food101, EuroSAT and DTD show state-of-the-art error detection performance, significantly outperforming both deterministic and probabilistic VLM baselines. Code is available at https://github.com/zhenxianglin/ICPE.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layover or Direct Flight: Rethinking Audio-Guided Image Segmentation</title>
<link>https://arxiv.org/abs/2511.22025</link>
<guid>https://arxiv.org/abs/2511.22025</guid>
<content:encoded><![CDATA[
<div> audio-visual grounding, human-robot interaction, spoken instructions, transcription-free, robustness  

<br /><br />Summary:  
This paper addresses the task of object grounding, which involves localizing an object in an image based on spoken human instructions, a key step toward better human-robot interaction. Traditional methods rely heavily on transcribing speech into text, extracting keywords, and then grounding objects using pretrained text-vision models. The authors challenge this transcription-dependent approach, questioning its efficiency and robustness. They propose exploring direct alignment between audio and visual data without converting speech to text. To test this, they simplify the problem to grounding based on single-word spoken instructions and introduce a new dataset containing diverse objects spoken with various human accents. The study benchmarks several adapted audio-visual models and shows that direct audio grounding is not only possible but in some cases more effective than transcription-based pipelines, especially in handling linguistic variability. These results advocate for a shift toward transcription-free multimodal understanding systems, promising enhanced robustness and efficiency for future audio-visual applications in robotics and AI. <div>
arXiv:2511.22025v1 Announce Type: new 
Abstract: Understanding human instructions is essential for enabling smooth human-robot interaction. In this work, we focus on object grounding, i.e., localizing an object of interest in a visual scene (e.g., an image) based on verbal human instructions. Despite recent progress, a dominant research trend relies on using text as an intermediate representation. These approaches typically transcribe speech to text, extract relevant object keywords, and perform grounding using models pretrained on large text-vision datasets. However, we question both the efficiency and robustness of such transcription-based pipelines. Specifically, we ask: Can we achieve direct audio-visual alignment without relying on text? To explore this possibility, we simplify the task by focusing on grounding from single-word spoken instructions. We introduce a new audio-based grounding dataset that covers a wide variety of objects and diverse human accents. We then adapt and benchmark several models from the closely audio-visual field. Our results demonstrate that direct grounding from audio is not only feasible but, in some cases, even outperforms transcription-based methods, especially in terms of robustness to linguistic variability. Our findings encourage a renewed interest in direct audio grounding and pave the way for more robust and efficient multimodal understanding systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAGen: Phase-guided Amplitude Generation for Domain-adaptive Object Detection</title>
<link>https://arxiv.org/abs/2511.22029</link>
<guid>https://arxiv.org/abs/2511.22029</guid>
<content:encoded><![CDATA[
<div> Unsupervised domain adaptation, frequency domain, image style adaptation, domain-adaptive object detection, lightweight preprocessing  

<br /><br />Summary:  
This paper addresses the challenge of unsupervised domain adaptation (UDA) to improve neural network deployment across varying environments without requiring extensive labeled data in target domains. It critiques existing methods for their complexity, including adversarial training and auxiliary model components, which increase computational cost and implementation difficulty. The authors propose a novel and simple UDA technique that adapts image styles by operating in the frequency domain, effectively reducing the domain gap between source and target data. A key advantage is the use of a lightweight preprocessing module applied only during training, which is completely removed during inference, thereby avoiding any inference overhead. The method is evaluated on domain-adaptive object detection (DAOD) tasks, particularly where annotations are readily available in source domains but scarce or unavailable in more challenging target domains like adverse weather or low-light conditions. Experimental results demonstrate substantial and consistent improvements across multiple benchmarks, validating both the practicality and effectiveness of the approach. This work highlights the potential of frequency-based style adaptation as a streamlined solution for UDA tasks requiring robust performance without sacrificing computational efficiency. <div>
arXiv:2511.22029v1 Announce Type: new 
Abstract: Unsupervised domain adaptation (UDA) greatly facilitates the deployment of neural networks across diverse environments. However, most state-of-the-art approaches are overly complex, relying on challenging adversarial training strategies, or on elaborate architectural designs with auxiliary models for feature distillation and pseudo-label generation. In this work, we present a simple yet effective UDA method that learns to adapt image styles in the frequency domain to reduce the discrepancy between source and target domains. The proposed approach introduces only a lightweight pre-processing module during training and entirely discards it at inference time, thus incurring no additional computational overhead. We validate our method on domain-adaptive object detection (DAOD) tasks, where ground-truth annotations are easily accessible in source domains (e.g., normal-weather or synthetic conditions) but challenging to obtain in target domains (e.g., adverse weather or low-light scenes). Extensive experiments demonstrate that our method achieves substantial performance gains on multiple benchmarks, highlighting its practicality and effectiveness.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model</title>
<link>https://arxiv.org/abs/2511.22039</link>
<guid>https://arxiv.org/abs/2511.22039</guid>
<content:encoded><![CDATA[
<div> trajectory-conditioned forecasting, 3D scene occupancy, transformer architecture, sparse representation, nuScenes benchmark<br /><br />Summary:<br /><br />This paper presents a novel transformer-based architecture designed for trajectory-conditioned forecasting of future 3D scene occupancy. Unlike prior approaches relying on variational autoencoders (VAEs) to generate discrete occupancy tokens with limited representational capacity, the proposed method predicts multi-frame future occupancy directly from raw image features in an end-to-end manner. The approach uses a sparse occupancy representation that removes the need for intermediate bird's eye view (BEV) projection and its associated geometric priors, enabling the model to better capture spatiotemporal dependencies. By avoiding tokenization constraints and BEV-induced structural limitations, the method significantly improves performance. Experiments on the nuScenes benchmark demonstrate state-of-the-art results for 1 to 3 second occupancy forecasting horizons, surpassing existing methods by a notable margin. Additionally, the model exhibits robust understanding of dynamic scene elements and maintains high accuracy when conditioned on arbitrary future trajectories. The work draws inspiration from the success of attention-based transformer architectures in foundational vision and language tasks, adapting these principles to achieve enhanced occupancy forecasting in complex 3D environments. <div>
arXiv:2511.22039v1 Announce Type: new 
Abstract: This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion</title>
<link>https://arxiv.org/abs/2511.22048</link>
<guid>https://arxiv.org/abs/2511.22048</guid>
<content:encoded><![CDATA[
<div> Keywords: Real-world image super-resolution, diffusion models, manifold regularization, image-conditioned manifold, perceptual quality<br /><br />Summary:<br />1. Real-world image super-resolution (Real-ISR) methods commonly use text-to-image diffusion models to regularize outputs on a learned manifold; however, existing approaches typically use a text-conditioned manifold which is misaligned with Real-ISR goals.<br />2. This misalignment causes practical issues such as color distortions and blurred edges in the reconstructed high-quality images, indicating that text-conditioned generative priors are not optimal for Real-ISR.<br />3. To address these flaws, the study proposes image-conditioned manifold regularization (ICM), which regularizes outputs towards a manifold conditioned on sparse structural information derived from the input images, specifically a combination of colormap and Canny edges.<br />4. The ICM approach avoids the numerical instability caused by conditioning directly on dense raw input images and provides a more stable and task-aligned regularization signal.<br />5. Experimental results demonstrate that ICM significantly improves super-resolution performance, especially enhancing perceptual quality, confirming its effectiveness and suitability for real-world applications. The authors also plan to release the source code to support reproducibility. <div>
arXiv:2511.22048v1 Announce Type: new 
Abstract: Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TPCNet: Triple physical constraints for Low-light Image Enhancement</title>
<link>https://arxiv.org/abs/2511.22052</link>
<guid>https://arxiv.org/abs/2511.22052</guid>
<content:encoded><![CDATA[
<div> Low-light enhancement, Retinex theory, specular reflection, Kubelka-Munk theory, triple physical constraints<br /><br />Summary:<br /><br />1. The paper addresses the challenge of enhancing low-light images by improving image contrast and reducing color bias and noise, which are critical tasks in computer vision.<br />2. Existing deep-learning methods frequently use the Retinex theory but often assume surfaces reflect light ideally as Lambertian, ignoring the effects of specular reflection.<br />3. The authors propose preserving the specular reflection coefficient and reformulate physical constraints based on Kubelka-Munk theory, which allows modeling illumination, reflection, and detection via triple physical constraints (TPCs).<br />4. Unlike previous approaches that apply constraints in image space, this work embeds the TPCs into the feature space of the deep learning model, leading to better generalization and adaptability.<br />5. The resulting architecture, named TPCNet, improves both quantitative metrics and visual quality across 10 benchmark datasets, without increasing the number of parameters, and outperforms existing state-of-the-art methods according to extensive experiments and ablation studies. <div>
arXiv:2511.22052v1 Announce Type: new 
Abstract: Low-light image enhancement is an essential computer vision task to improve image contrast and to decrease the effects of color bias and noise. Many existing interpretable deep-learning algorithms exploit the Retinex theory as the basis of model design. However, previous Retinex-based algorithms, that consider reflected objects as ideal Lambertian ignore specular reflection in the modeling process and construct the physical constraints in image space, limiting generalization of the model. To address this issue, we preserve the specular reflection coefficient and reformulate the original physical constraints in the imaging process based on the Kubelka-Munk theory, thereby constructing constraint relationship between illumination, reflection, and detection, the so-called triple physical constraints (TPCs)theory. Based on this theory, the physical constraints are constructed in the feature space of the model to obtain the TPC network (TPCNet). Comprehensive quantitative and qualitative benchmark and ablation experiments confirm that these constraints effectively improve the performance metrics and visual quality without introducing new parameters, and demonstrate that our TPCNet outperforms other state-of-the-art methods on 10 datasets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OralGPT-Omni: A Versatile Dental Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2511.22055</link>
<guid>https://arxiv.org/abs/2511.22055</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Dentistry, TRACE-CoT, OralGPT-Omni, MMOral-Uni<br /><br />Summary:<br /><br />This paper introduces OralGPT-Omni, the first dental-specialized multimodal large language model (MLLM) designed to analyze diverse dental imaging modalities and clinical tasks with high reliability. The development of OralGPT-Omni addresses challenges in dentistry AI such as limited domain-specific data, lack of expert annotations, and insufficient modality-specific modeling. The authors created TRACE-CoT, a clinically grounded chain-of-thought dataset that simulates the diagnostic reasoning process of dental radiologists, which enhances the model’s interpretability and accuracy. A novel four-stage training paradigm is proposed to further improve the model’s performance in understanding dental images. The study also presents MMOral-Uni, the first unified multimodal benchmark for dental image analysis, featuring 2,809 open-ended question-answer pairs across five modalities and five clinical tasks. OralGPT-Omni achieves a superior overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, significantly outperforming GPT-5. By releasing all code, benchmarks, and models publicly, this work promotes intelligent dentistry and lays the foundation for future advances in dental image analysis and AI-assisted diagnostics. <div>
arXiv:2511.22055v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DNA: Dual-branch Network with Adaptation for Open-Set Online Handwriting Generation</title>
<link>https://arxiv.org/abs/2511.22064</link>
<guid>https://arxiv.org/abs/2511.22064</guid>
<content:encoded><![CDATA[
<div> Online handwriting generation, unseen characters, dual-branch network, style adaptation, content generalization<br /><br />Summary:<br /><br />This paper addresses the challenge of online handwriting generation (OHG) specifically focusing on the difficulty of generating unseen characters, a common problem in glyph-based languages such as Chinese. The authors propose a novel method named Dual-branch Network with Adaptation (DNA), which is designed to generate handwriting samples featuring both unseen writer styles and previously unseen characters during testing. The DNA model incorporates two adaptive branches: a style branch and a content branch. The style branch captures stroke-level attributes such as writing direction, spacing, placement, and flow to mimic realistic handwriting styles. Simultaneously, the content branch enhances the model's ability to generalize to unseen characters by decomposing character content into structural information and texture details via local and global encoders. This dual approach allows the model to adapt efficiently to new writing styles and unseen character forms, overcoming key limitations of existing OHG models. Extensive experimental evaluations demonstrate that the DNA model outperforms prior methods, setting new state-of-the-art results in OHG tasks particularly under conditions involving unseen styles and characters, thereby increasing the model’s practical applicability in real-world handwriting recognition enhancement. <div>
arXiv:2511.22064v1 Announce Type: new 
Abstract: Online handwriting generation (OHG) enhances handwriting recognition models by synthesizing diverse, human-like samples. However, existing OHG methods struggle to generate unseen characters, particularly in glyph-based languages like Chinese, limiting their real-world applicability. In this paper, we introduce our method for OHG, where the writer's style and the characters generated during testing are unseen during training. To tackle this challenge, we propose a Dual-branch Network with Adaptation (DNA), which comprises an adaptive style branch and an adaptive content branch. The style branch learns stroke attributes such as writing direction, spacing, placement, and flow to generate realistic handwriting. Meanwhile, the content branch is designed to generalize effectively to unseen characters by decomposing character content into structural information and texture details, extracted via local and global encoders, respectively. Extensive experiments demonstrate that our DNA model is well-suited for the unseen OHG setting, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation</title>
<link>https://arxiv.org/abs/2511.22098</link>
<guid>https://arxiv.org/abs/2511.22098</guid>
<content:encoded><![CDATA[
<div> Keywords: video diffusion models, egocentric-exocentric translation, WorldWander, in-context learning, EgoExo-8K<br /><br />Summary:<br /><br />1. The paper addresses the challenge of translating video content seamlessly between egocentric (first-person) and exocentric (third-person) perspectives, which is important for applications like filmmaking, embodied AI, and world modeling.  
2. The authors propose WorldWander, an in-context learning framework built upon advanced video diffusion transformers, specifically designed to handle cross-view synchronization in video generation.  
3. WorldWander introduces two key technical components: In-Context Perspective Alignment, which helps the model understand and align different viewpoints within context, and Collaborative Position Encoding, which enhances cross-view temporal and spatial consistency.  
4. To support training and evaluation, the authors curate EgoExo-8K, a large-scale dataset consisting of synchronized egocentric-exocentric video triplets from both synthetic environments and real-world scenes, enabling robust and diverse learning scenarios.  
5. Experimental results demonstrate that WorldWander significantly improves perspective synchronization and preserves character consistency between views, outperforming existing approaches and setting a new benchmark for egocentric-exocentric video translation tasks. <div>
arXiv:2511.22098v1 Announce Type: new 
Abstract: Video diffusion models have recently achieved remarkable progress in realism and controllability. However, achieving seamless video translation across different perspectives, such as first-person (egocentric) and third-person (exocentric), remains underexplored. Bridging these perspectives is crucial for filmmaking, embodied AI, and world models. Motivated by this, we present WorldWander, an in-context learning framework tailored for translating between egocentric and exocentric worlds in video generation. Building upon advanced video diffusion transformers, WorldWander integrates (i) In-Context Perspective Alignment and (ii) Collaborative Position Encoding to efficiently model cross-view synchronization. To further support our task, we curate EgoExo-8K, a large-scale dataset containing synchronized egocentric-exocentric triplets from both synthetic and real-world scenarios. Experiments demonstrate that WorldWander achieves superior perspective synchronization, character consistency, and generalization, setting a new benchmark for egocentric-exocentric video translation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation</title>
<link>https://arxiv.org/abs/2511.22102</link>
<guid>https://arxiv.org/abs/2511.22102</guid>
<content:encoded><![CDATA[
<div> Keywords: brain age estimation, supervised contrastive learning, Rank-N-Contrast loss, Grad-RAM, neurodegenerative diseases<br /><br />Summary:<br />1. The study focuses on MRI-based brain age estimation models, which aim to determine an individual's biological brain age through neuroanatomical features, relevant for identifying accelerated brain aging linked to neurodegenerative diseases.<br />2. Traditional deep learning regression methods often neglect the continuous nature of neuromorphological changes, limiting feature representation and accuracy.<br />3. To overcome these limitations, the authors propose a novel approach using supervised contrastive learning combined with the Rank-N-Contrast (RNC) loss function applied to T1-weighted structural MRI data for brain age estimation.<br />4. The method incorporates Grad-RAM visualization to provide more detailed and interpretable explanations of the regression outputs.<br />5. Experimental results demonstrate that the proposed method achieves a mean absolute error (MAE) of 4.27 years and an R² of 0.93 using a limited training set, outperforming conventional deep regression models with the same ResNet backbone and matching or surpassing state-of-the-art methods trained on larger datasets.<br />6. Grad-RAM visualizations reveal that the RNC loss captures more nuanced age-related neuromorphological features compared to standard regression.<br />7. As an exploratory application, the model estimates the biological-chronological brain age gap in Alzheimer's and Parkinson's patients, finding correlations between this gap and disease severity.<br />8. These findings support the potential utility of the approach as a biomarker for diagnosing and monitoring neurodegenerative disorders. <div>
arXiv:2511.22102v1 Announce Type: new 
Abstract: MRI-based brain age estimation models aim to assess a subject's biological brain age based on information, such as neuroanatomical features. Various factors, including neurodegenerative diseases, can accelerate brain aging and measuring this phenomena could serve as a potential biomarker for clinical applications. While deep learning (DL)-based regression has recently attracted major attention, existing approaches often fail to capture the continuous nature of neuromorphological changes, potentially resulting in sub-optimal feature representation and results. To address this, we propose to use supervised contrastive learning with the recent Rank-N-Contrast (RNC) loss to estimate brain age based on widely used T1w structural MRI for the first time and leverage Grad-RAM to visually explain regression results. Experiments show that our proposed method achieves a mean absolute error (MAE) of 4.27 years and an $R^2$ of 0.93 with a limited dataset of training samples, significantly outperforming conventional deep regression with the same ResNet backbone while performing better or comparably with the state-of-the-art methods with significantly larger training data. Furthermore, Grad-RAM revealed more nuanced features related to age regression with the RNC loss than conventional deep regression. As an exploratory study, we employed the proposed method to estimate the gap between the biological and chronological brain ages in Alzheimer's Disease and Parkinson's disease patients, and revealed the correlation between the brain age gap and disease severity, demonstrating its potential as a biomarker in neurodegenerative disorders.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoE3D: Mixture of Experts meets Multi-Modal 3D Understanding</title>
<link>https://arxiv.org/abs/2511.22103</link>
<guid>https://arxiv.org/abs/2511.22103</guid>
<content:encoded><![CDATA[
<div> Multi-modal learning, Mixture of Experts, 3D understanding, Transformer, Pre-training<br /><br />Summary:<br /><br />This paper addresses the challenges in multi-modal 3D understanding tasks within computer vision, where existing fusion methods with a single dense network struggle due to modality heterogeneity and complexity. The authors propose MoE3D, a novel framework that integrates Mixture of Experts (MoE) into multi-modal learning to better leverage complementary information across different data modalities. MoE3D features specialized "expert" networks tailored to specific modalities or cross-modal interactions, improving feature processing. A MoE-based transformer architecture is introduced, enhancing the utilization of visual features. An information aggregation module is designed to further boost fusion effectiveness. The approach employs top-1 gating, allowing only one expert or expert group to process inputs for greater computational efficiency. Additionally, a progressive pre-training strategy is developed to harness semantic and 2D prior knowledge, providing better initialization for the network. Experiments demonstrate MoE3D's competitive performance across four key 3D understanding tasks. Crucially, MoE3D outperforms state-of-the-art methods by a significant margin, notably surpassing the best previous model by 6.1 mIoU on the Multi3DRefer dataset, highlighting its effectiveness in multi-modal fusion for 3D vision problems. <div>
arXiv:2511.22103v1 Announce Type: new 
Abstract: Multi-modal 3D understanding is a fundamental task in computer vision. Previous multi-modal fusion methods typically employ a single, dense fusion network, struggling to handle the significant heterogeneity and complexity across modalities, leading to suboptimal performance. In this paper, we propose MoE3D, which integrates Mixture of Experts (MoE) into the multi-modal learning framework. The core is that we deploy a set of specialized "expert" networks, each adept at processing a specific modality or a mode of cross-modal interaction. Specifically, the MoE-based transformer is designed to better utilize the complementary information hidden in the visual features. Information aggregation module is put forward to further enhance the fusion performance. Top-1 gating is employed to make one expert process features with expert groups, ensuring high efficiency. We further propose a progressive pre-training strategy to better leverage the semantic and 2D prior, thus equipping the network with good initialization. Our MoE3D achieves competitive performance across four prevalent 3D understanding tasks. Notably, our MoE3D surpasses the top-performing counterpart by 6.1 mIoU on Multi3DRefer.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperST: Hierarchical Hyperbolic Learning for Spatial Transcriptomics Prediction</title>
<link>https://arxiv.org/abs/2511.22107</link>
<guid>https://arxiv.org/abs/2511.22107</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial Transcriptomics, gene expression prediction, hyperbolic space, hierarchical alignment, multi-level representation<br /><br />Summary:<br /><br />1. Spatial Transcriptomics (ST) integrates pathology images with gene expression data to analyze tissue function at spot level, but current methods struggle to fully utilize the hierarchical structure present in ST data, especially regarding gene expression.<br />2. Existing approaches mostly perform spot-level image-to-gene matching but overlook multi-level contextual information and the complexity of gene expression profiles, which contain molecular details that do not always have clear visual counterparts.<br />3. HyperST is proposed as a novel framework that captures multi-level representations from both image and gene data by embedding them into hyperbolic space, which naturally models hierarchical structures.<br />4. The framework includes a Multi-Level Representation Extractors module to obtain spot-level and niche-level features, providing enriched contextual information beyond individual spots.<br />5. A Hierarchical Hyperbolic Alignment module aligns these multi-level embeddings in a shared space, enhancing image features with molecular semantics and bridging the modality gap.<br />6. HyperST outperforms existing methods on four public datasets across various tissues, demonstrating its superior ability to predict spatial gene expression effectively and providing a scalable, accurate tool for ST prediction. <div>
arXiv:2511.22107v1 Announce Type: new 
Abstract: Spatial Transcriptomics (ST) merges the benefits of pathology images and gene expression, linking molecular profiles with tissue structure to analyze spot-level function comprehensively. Predicting gene expression from histology images is a cost-effective alternative to expensive ST technologies. However, existing methods mainly focus on spot-level image-to-gene matching but fail to leverage the full hierarchical structure of ST data, especially on the gene expression side, leading to incomplete image-gene alignment. Moreover, a challenge arises from the inherent information asymmetry: gene expression profiles contain more molecular details that may lack salient visual correlates in histological images, demanding a sophisticated representation learning approach to bridge this modality gap. We propose HyperST, a framework for ST prediction that learns multi-level image-gene representations by modeling the data's inherent hierarchy within hyperbolic space, a natural geometric setting for such structures. First, we design a Multi-Level Representation Extractors to capture both spot-level and niche-level representations from each modality, providing context-aware information beyond individual spot-level image-gene pairs. Second, a Hierarchical Hyperbolic Alignment module is introduced to unify these representations, performing spatial alignment while hierarchically structuring image and gene embeddings. This alignment strategy enriches the image representations with molecular semantics, significantly improving cross-modal prediction. HyperST achieves state-of-the-art performance on four public datasets from different tissues, paving the way for more scalable and accurate spatial transcriptomics prediction.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization</title>
<link>https://arxiv.org/abs/2511.22119</link>
<guid>https://arxiv.org/abs/2511.22119</guid>
<content:encoded><![CDATA[
<div> Text-to-image generation, prompt stealing, reinforcement learning, black-box attack, diffusion models<br /><br />Summary:<br /><br />Text-to-image generative models like Stable Diffusion produce high-quality images based on textual prompts, making these prompts valuable digital assets. This value raises security and intellectual property concerns, particularly regarding prompt stealing attacks which aim to recover the textual prompts from generated images. Such attacks not only risk unauthorized reuse but can also have positive uses like data attribution and model provenance. Existing methods for prompt stealing often need white-box access, large labeled datasets, or rely on simple captioning, limiting their practicality. To overcome these challenges, the authors propose PROMPTMINER, a black-box framework that separates prompt recovery into two phases: reinforcement learning-based optimization for reconstructing the primary subject, and fuzzing-based search to recover style-related modifiers. Extensive experiments on various datasets and diffusion backbones show PROMPTMINER achieves superior performance with high CLIP similarity (up to 0.958) and SBERT textual alignment (up to 0.751), outperforming all baselines. PROMPTMINER also generalizes well to real-world images with unknown generators, surpassing the strongest baseline by 7.5% in CLIP similarity. Lastly, it demonstrates robustness against defensive perturbations, making it a strong and practical approach to prompt stealing. The authors provide code for reproducibility. <div>
arXiv:2511.22119v1 Announce Type: new 
Abstract: Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GoPrune: Accelerated Structured Pruning with $\ell_{2,p}$-Norm Optimization</title>
<link>https://arxiv.org/abs/2511.22120</link>
<guid>https://arxiv.org/abs/2511.22120</guid>
<content:encoded><![CDATA[
<div> Keywords: convolutional neural networks, structured pruning, ℓ₂,ₚ-norm, proximal alternating minimization, network compression

<br /><br />Summary:  
The paper addresses the challenge of high storage and computational costs in deep convolutional neural networks (CNNs), which limit their deployment on edge devices with restricted resources. It focuses on pruning as a compression technique, emphasizing structured pruning due to its effectiveness in accelerating inference. Existing methods applying the ℓₚ-norm for pruning mainly handle unstructured pruning with \( p \in (0,1) \) and suffer from low computational efficiency. To overcome these issues, the authors propose GoPrune, an accelerated structured pruning method that leverages the ℓ₂,ₚ-norm for sparse network learning with \( p \in [0,1) \), extending the possible values of \( p \). The method includes an efficient optimization algorithm based on proximal alternating minimization (PAM), which simplifies subproblems into closed-form solutions, enhancing compression speed and efficiency. Experimental results on CIFAR datasets with ResNet and VGG architectures demonstrate that GoPrune achieves superior pruning performance compared to existing techniques. The authors also provide their implementation on GitHub, facilitating reproducibility and further research. Overall, GoPrune offers a computationally efficient structured pruning framework that balances network sparsity and performance for resource-limited environments. <div>
arXiv:2511.22120v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) suffer from rapidly increasing storage and computational costs as their depth grows, which severely hinders their deployment on resource-constrained edge devices. Pruning is a practical approach for network compression, among which structured pruning is the most effective for inference acceleration. Although existing work has applied the $\ell_p$-norm to pruning, it only considers unstructured pruning with $p\in (0, 1)$ and has low computational efficiency. To overcome these limitations, we propose an accelerated structured pruning method called GoPrune. Our method employs the $\ell_{2,p}$-norm for sparse network learning, where the value of $p$ is extended to $[0, 1)$. Moreover, we develop an efficient optimization algorithm based on the proximal alternating minimization (PAM), and the resulting subproblems enjoy closed-form solutions, thus improving compression efficiency. Experiments on the CIFAR datasets using ResNet and VGG models demonstrate the superior performance of the proposed method in network pruning. Our code is available at https://github.com/xianchaoxiu/GoPrune.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cue3D: Quantifying the Role of Image Cues in Single-Image 3D Generation</title>
<link>https://arxiv.org/abs/2511.22121</link>
<guid>https://arxiv.org/abs/2511.22121</guid>
<content:encoded><![CDATA[
<div> Keywords: single-image 3D generation, monocular cues, Cue3D framework, shading, silhouette<br /><br />Summary:<br /><br />This paper introduces Cue3D, the first comprehensive, model-agnostic framework designed to quantify the influence of individual monocular image cues in single-image 3D generation. The study evaluates seven state-of-the-art 3D generation methods, including regression-based, multi-view, and native 3D generative models, under a unified benchmark. By systematically perturbing image cues such as shading, texture, silhouette, perspective, edges, and local continuity, the work measures their specific impact on the quality of 3D outputs. The analysis reveals that shape meaningfulness, rather than texture, is the primary factor dictating a model's ability to generalize across inputs. It highlights geometric cues, especially shading, as critical for effective 3D reconstruction from single images. The research further identifies that many models rely heavily on provided silhouettes, which may limit robustness, and they exhibit varied sensitivities to other cues like perspective and local continuity depending on their architectural family. By dissecting how modern 3D networks exploit these classical vision cues, Cue3D not only deepens understanding of current methods but also provides valuable insights and directions to develop more transparent, robust, and controllable models for single-image 3D generation. <div>
arXiv:2511.22121v1 Announce Type: new 
Abstract: Humans and traditional computer vision methods rely on a diverse set of monocular cues to infer 3D structure from a single image, such as shading, texture, silhouette, etc. While recent deep generative models have dramatically advanced single-image 3D generation, it remains unclear which image cues these methods actually exploit. We introduce Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation. Our unified benchmark evaluates seven state-of-the-art methods, spanning regression-based, multi-view, and native 3D generative paradigms. By systematically perturbing cues such as shading, texture, silhouette, perspective, edges, and local continuity, we measure their impact on 3D output quality. Our analysis reveals that shape meaningfulness, not texture, dictates generalization. Geometric cues, particularly shading, are crucial for 3D generation. We further identify over-reliance on provided silhouettes and diverse sensitivities to cues such as perspective and local continuity across model families. By dissecting these dependencies, Cue3D advances our understanding of how modern 3D networks leverage classical vision cues, and offers directions for developing more transparent, robust, and controllable single-image 3D generation models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GA2-CLIP: Generic Attribute Anchor for Efficient Prompt Tuningin Video-Language Models</title>
<link>https://arxiv.org/abs/2511.22125</link>
<guid>https://arxiv.org/abs/2511.22125</guid>
<content:encoded><![CDATA[
<div> Visual-Language Models, soft prompt tuning, semantic space, video tasks, generalization<br /><br />Summary:<br /><br />This work addresses the challenge of maintaining the generalization ability of Vision-Language Models (VLMs) when fine-tuned on video tasks, as fine-tuning often narrows the semantic space, causing overfitting to supervised categories and impairing unseen class recognition. To tackle this, the authors propose a plug-and-play coupling prompt learning framework designed to optimize generalization in video tasks by mitigating semantic space narrowing. The core innovation involves integrating externally supervised prompts into the model: for textual prompts, pre-trained hard prompt tokens from other datasets are concatenated with soft prompt tokens and coupled through a learnable mapping layer, creating a competitive prompting mechanism that prevents overfitting. Additionally, they introduce irrelevant video sets and negative prompts as generic attribute anchors to help maintain generic attribute relevance within the pre-trained semantic space, further preserving generalization ability. Experimental results on video benchmarks demonstrate that this method significantly outperforms existing state-of-the-art soft prompt tuning approaches, especially regarding performance on base-to-new class prediction tasks, showcasing its effectiveness in enhancing generalization to unseen classes. <div>
arXiv:2511.22125v1 Announce Type: new 
Abstract: Visual and textual soft prompt tuning can effectively improve the adaptability of Vision-Language Models (VLMs) in downstream tasks. However, fine-tuning on video tasks impairs the model's generalization ability to unseen classes. Existing methods attempt to mitigate this forgetting effect by regularizing the gap between hand-crafted prompts and soft prompts, but this also weakens the learning ability of soft prompts. To address this challenge, we propose a plug-and-play coupling prompt learning framework to optimize the generalization performance of V-L models in video tasks, with the core motivation of mitigating semantic space narrowing during fine-tuning by introducing an externally supervised prompt. Specifically, for textual prompts, we introduce pre-trained prompts from other datasets as hard prompt tokens. These are concatenated with soft prompt tokens and coupled via a learnable mapping layer. This competitive prompting approach prevents the semantic space from overfitting to supervised categories. In addition, we introduce a set of well-designed irrelevant video sets and negative prompts as generic attribute anchors to maintain the generic relevance of the attributes in the pre-trained semantic space, thus preserving the generalization ability. Experiments on video tasks demonstrate that our method significantly outperforms state-of-the-art prompt tuning approaches across generalization benchmarks, particularly on base-to-new class prediction.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous labeling of surgical resection margins using a foundation model</title>
<link>https://arxiv.org/abs/2511.22131</link>
<guid>https://arxiv.org/abs/2511.22131</guid>
<content:encoded><![CDATA[
<div> resection margins, virtual inking network, digital pathology, cautery artifacts, whole-slide images<br /><br />Summary:<br /><br />1. Assessing resection margins is critical for evaluating pathological specimens and significantly impacts patient outcomes. Current methods rely on physical inking, which is inconsistently applied and can be obscured by cautery artifacts in histological slides.<br /><br />2. The study introduces a Virtual Inking Network (VIN) designed to autonomously localize surgical cut surfaces on whole-slide images, aiming to reduce dependence on physical inks and standardize margin assessment.<br /><br />3. VIN leverages a frozen foundation model as its feature extractor combined with a two-layer multilayer perceptron trained for patch-level classification to detect features consistent with cautery artifacts.<br /><br />4. The dataset includes 120 hematoxylin and eosin (H&amp;E) stained slides from 12 human tonsil tissue blocks, totaling about 2 TB of data, with margin boundaries annotated by a board-certified pathologist.<br /><br />5. In blind testing on 20 slides from unseen blocks, VIN produced margin overlays that qualitatively matched expert annotations, achieving a region-level accuracy of approximately 73.3%, with minor errors that did not disrupt margin continuity.<br /><br />6. These findings demonstrate VIN's capability to capture cautery-related histomorphology and provide reproducible, ink-free margin delineations, supporting its integration into routine digital pathology workflows and enabling downstream margin distance measurements. <div>
arXiv:2511.22131v1 Announce Type: new 
Abstract: Assessing resection margins is central to pathological specimen evaluation and has profound implications for patient outcomes. Current practice employs physical inking, which is applied variably, and cautery artifacts can obscure the true margin on histological sections. We present a virtual inking network (VIN) that autonomously localizes the surgical cut surface on whole-slide images, reducing reliance on inks and standardizing margin-focused review. VIN uses a frozen foundation model as the feature extractor and a compact two-layer multilayer perceptron trained for patch-level classification of cautery-consistent features. The dataset comprised 120 hematoxylin and eosin (H&amp;E) stained slides from 12 human tonsil tissue blocks, resulting in ~2 TB of uncompressed raw image data, where a board-certified pathologist provided boundary annotations. In blind testing with 20 slides from previously unseen blocks, VIN produced coherent margin overlays that qualitatively aligned with expert annotations across serial sections. Quantitatively, region-level accuracy was ~73.3% across the test set, with errors largely confined to limited areas that did not disrupt continuity of the whole-slide margin map. These results indicate that VIN captures cautery-related histomorphology and can provide a reproducible, ink-free margin delineation suitable for integration into routine digital pathology workflows and for downstream measurement of margin distances.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action</title>
<link>https://arxiv.org/abs/2511.22134</link>
<guid>https://arxiv.org/abs/2511.22134</guid>
<content:encoded><![CDATA[
<div> Vision-Language-Action, action degeneration, dual-layer data pruning, dual-teacher adaptive distillation, VLA Score<br /><br />Summary:<br /><br />1. The paper addresses the challenge of building generalizable Vision-Language-Action (VLA) models that combine strong reasoning abilities with precise manipulation skills for robotics. 2. A common approach trains a specialist VLA on robot demonstrations to learn reliable actions, then fine-tunes it with mixed annotated robot and multimodal data to enhance reasoning; however, this often leads to "action degeneration," where action performance degrades after fine-tuning. 3. To counter this, the authors propose DualVLA, a model that improves action performance post-training while preserving reasoning ability. 4. DualVLA introduces a dual-layer data pruning method to eliminate redundant embodied reasoning data that negatively impacts action learning. 5. Furthermore, a dual-teacher adaptive distillation strategy is designed to give tailored supervision signals for different data domains, maintaining a balance between action accuracy and reasoning. 6. To better evaluate generalist VLAs, the authors propose the VLA Score, which breaks down VLA capabilities into reasoning, intention, action, and alignment dimensions for detailed assessment. 7. Experimental results demonstrate DualVLA achieves an average success rate of 61.0 in SimplerEnv and scores 65.4 on eight multimodal benchmarks, showing a stronger balance between precise action execution and multimodal understanding. 8. The research underscores the effectiveness of targeted data pruning and adaptive distillation in resolving the trade-off between reasoning and action in VLA models. <div>
arXiv:2511.22134v1 Announce Type: new 
Abstract: To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EASL: Multi-Emotion Guided Semantic Disentanglement for Expressive Sign Language Generation</title>
<link>https://arxiv.org/abs/2511.22135</link>
<guid>https://arxiv.org/abs/2511.22135</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, sign language generation, emotional expression, emotion-semantic disentanglement, diffusion models<br /><br />Summary:  
1. Large language models (LLMs) have greatly advanced sign language generation by converting text into high-quality sign language videos, enhancing communication accessibility for the Deaf community.  
2. Current LLM-based methods focus mainly on semantic accuracy but fail to capture emotional expressions, leading to generated sign language that lacks naturalness and expressiveness.  
3. The proposed EASL (Emotion-Aware Sign Language) framework introduces a multi-emotion-guided generation architecture designed to integrate fine-grained emotional information into sign language synthesis.  
4. EASL employs emotion-semantic disentanglement modules with progressive training to separately extract semantic features and affective (emotional) features, allowing better emotional representation.  
5. During pose decoding, EASL uses the emotional representations to guide semantic interactions, generating sign poses accompanied by 7-class emotion confidence scores, which enables recognition of emotional expressions.  
6. Experimental results show that EASL outperforms existing baseline methods in pose accuracy by effectively incorporating multi-emotion information.  
7. Additionally, EASL adapts well to diffusion models, facilitating the generation of more expressive and emotionally rich sign language videos, enhancing naturalness and communication quality. <div>
arXiv:2511.22135v1 Announce Type: new 
Abstract: Large language models have revolutionized sign language generation by automatically transforming text into high-quality sign language videos, providing accessible communication for the Deaf community. However, existing LLM-based approaches prioritize semantic accuracy while overlooking emotional expressions, resulting in outputs that lack naturalness and expressiveness. We propose EASL (Emotion-Aware Sign Language), a multi-emotion-guided generation architecture for fine-grained emotional integration. We introduce emotion-semantic disentanglement modules with progressive training to separately extract semantic and affective features. During pose decoding, the emotional representations guide semantic interaction to generate sign poses with 7-class emotion confidence scores, enabling emotional expression recognition. Experimental results demonstrate that EASL achieves pose accuracy superior to all compared baselines by integrating multi-emotion information and effectively adapts to diffusion models to generate expressive sign language videos.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemOD: Semantic Enabled Object Detection Network under Various Weather Conditions</title>
<link>https://arxiv.org/abs/2511.22142</link>
<guid>https://arxiv.org/abs/2511.22142</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic-enabled network, object detection, all-weather conditions, image enhancement, YOLO

<br /><br />Summary: This paper addresses the challenge of object detection in autonomous driving systems under diverse weather conditions, where most existing camera-based perception models are trained on clear weather data and lack adaptability. The authors propose a semantic-enabled network composed of two main components: a Preprocessing Unit (PPU) and a Detection Unit (DTU). The PPU uses a U-shaped network enriched by semantic information to refine degraded images caused by adverse weather, enhancing visual coherence and realism by generating plausible content in missing or degraded areas. The DTU then incorporates this semantic data within a modified YOLO detection framework to improve object recognition accuracy. The inclusion of semantic information helps the model understand object boundaries better and supports effective image transformation across various weather scenarios. Experimental results demonstrate that this approach yields improvements in mean Average Precision (mAP) by 1.47% to 8.80% compared to existing methods on multiple benchmark datasets, showcasing the effectiveness of semantics for both image enhancement and object detection. The approach pioneers the integration of semantic information throughout the all-weather detection pipeline, providing a comprehensive and robust solution to enhance autonomous vehicle perception systems. The related code is made publicly available for further research and development. <div>
arXiv:2511.22142v1 Announce Type: new 
Abstract: In the field of autonomous driving, camera-based perception models are mostly trained on clear weather data. Models that focus on addressing specific weather challenges are unable to adapt to various weather changes and primarily prioritize their weather removal characteristics. Our study introduces a semantic-enabled network for object detection in diverse weather conditions. In our analysis, semantics information can enable the model to generate plausible content for missing areas, understand object boundaries, and preserve visual coherency and realism across both filled-in and existing portions of the image, which are conducive to image transformation and object recognition. Specific in implementation, our architecture consists of a Preprocessing Unit (PPU) and a Detection Unit (DTU), where the PPU utilizes a U-shaped net enriched by semantics to refine degraded images, and the DTU integrates this semantic information for object detection using a modified YOLO network. Our method pioneers the use of semantic data for all-weather transformations, resulting in an increase between 1.47\% to 8.80\% in mAP compared to existing methods across benchmark datasets of different weather. This highlights the potency of semantics in image enhancement and object detection, offering a comprehensive approach to improving object detection performance. Code will be available at https://github.com/EnisZuo/SemOD.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stacked Ensemble of Fine-Tuned CNNs for Knee Osteoarthritis Severity Grading</title>
<link>https://arxiv.org/abs/2511.22143</link>
<guid>https://arxiv.org/abs/2511.22143</guid>
<content:encoded><![CDATA[
<div> Knee Osteoarthritis, Kellgren-Lawrence grading, stacked ensemble model, convolutional neural networks, CatBoost<br /><br />Summary:<br /><br />Knee Osteoarthritis (KOA) is a musculoskeletal disorder impacting daily activities, particularly among elderly individuals. KOA severity is traditionally assessed by analyzing knee X-rays with the Kellgren-Lawrence (KL) grading system, which classifies severity from 0 to 4, though this method requires expertise and is prone to subjective errors. To improve diagnostic accuracy and efficiency, the authors developed a stacked ensemble model that combines several fine-tuned Convolutional Neural Networks (CNNs) for two tasks: binary classification to detect the presence of KOA and multiclass classification to assign the precise KL grade. The ensemble includes MobileNetV2, YOLOv8, and DenseNet201 as base learners, with Categorical Boosting (CatBoost) acting as the meta-learner. The model achieved a balanced test accuracy of 87.5% for binary classification and 73% for multiclass grading, outperforming previous studies reported in the literature. This approach reduces subjectivity and can potentially streamline KOA diagnosis and grading, supporting clinicians with more reliable and automated assessments based on X-ray imaging. <div>
arXiv:2511.22143v1 Announce Type: new 
Abstract: Knee Osteoarthritis (KOA) is a musculoskeletal condition that can cause significant limitations and impairments in daily activities, especially among older individuals. To evaluate the severity of KOA, typically, X-ray images of the affected knee are analyzed, and a grade is assigned based on the Kellgren-Lawrence (KL) grading system, which classifies KOA severity into five levels, ranging from 0 to 4. This approach requires a high level of expertise and time and is susceptible to subjective interpretation, thereby introducing potential diagnostic inaccuracies. To address this problem a stacked ensemble model of fine-tuned Convolutional Neural Networks (CNNs) was developed for two classification tasks: a binary classifier for detecting the presence of KOA, and a multiclass classifier for precise grading across the KL spectrum. The proposed stacked ensemble model consists of a diverse set of pre-trained architectures, including MobileNetV2, You Only Look Once (YOLOv8), and DenseNet201 as base learners and Categorical Boosting (CatBoost) as the meta-learner. This proposed model had a balanced test accuracy of 73% in multiclass classification and 87.5% in binary classification, which is higher than previous works in extant literature.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks</title>
<link>https://arxiv.org/abs/2511.22147</link>
<guid>https://arxiv.org/abs/2511.22147</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian splatting, computation cost attacks, black-box defense, adversarial training, 3D reconstruction<br /><br />Summary:<br /><br />This paper addresses vulnerabilities in 3D Gaussian splatting (3DGS), a prominent technique for 3D reconstruction widely used in various applications. The authors highlight recent discoveries of critical computation cost attacks that exploit 3DGS, resulting in malicious resource consumption and denial-of-service (DoS) situations, thus threatening the reliability of 3DGS systems. To counter these threats, the paper introduces RemedyGS, the first comprehensive black-box defense framework designed specifically for 3DGS pipelines. RemedyGS consists of two main components: a detector that identifies input images affected by poisoned textures and a purifier that restores these attacked images back to a benign state, mitigating the attacks’ harmful impact. The framework also integrates adversarial training within the purifier to ensure alignment of the distribution between recovered images and original natural images, which strengthens the defense mechanism's effectiveness. Experimental evaluations demonstrate that RemedyGS not only successfully defends against white-box, black-box, and adaptive attacks but also maintains high utility and safety standards for 3DGS systems. This work paves the way for more secure and dependable deployment of 3D Gaussian splatting in real-world applications by effectively mitigating computationally costly adversarial threats. <div>
arXiv:2511.22147v1 Announce Type: new 
Abstract: As a mainstream technique for 3D reconstruction, 3D Gaussian splatting (3DGS) has been applied in a wide range of applications and services. Recent studies have revealed critical vulnerabilities in this pipeline and introduced computation cost attacks that lead to malicious resource occupancies and even denial-of-service (DoS) conditions, thereby hindering the reliable deployment of 3DGS. In this paper, we propose the first effective and comprehensive black-box defense framework, named RemedyGS, against such computation cost attacks, safeguarding 3DGS reconstruction systems and services. Our pipeline comprises two key components: a detector to identify the attacked input images with poisoned textures and a purifier to recover the benign images from their attacked counterparts, mitigating the adverse effects of these attacks. Moreover, we incorporate adversarial training into the purifier to enforce distributional alignment between the recovered and original natural images, thereby enhancing the defense efficacy. Experimental results demonstrate that our framework effectively defends against white-box, black-box, and adaptive attacks in 3DGS systems, achieving state-of-the-art performance in both safety and utility.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMTalker: Efficient Audio-driven Talking Face Generation with Implicit Motion Transfer</title>
<link>https://arxiv.org/abs/2511.22167</link>
<guid>https://arxiv.org/abs/2511.22167</guid>
<content:encoded><![CDATA[
<div> Keywords: talking face generation, implicit motion transfer, identity preservation, cross-attention mechanism, audio-lip synchronization  

<br /><br />Summary:  
This paper introduces IMTalker, a new framework for talking face generation that improves upon existing methods by using implicit motion transfer instead of explicit optical flow and local warping techniques. The approach replaces traditional flow-based warping with a cross-attention mechanism to implicitly capture motion discrepancy and identity alignment within a unified latent space, enabling robust modeling of complex global motions and reducing identity drift. To better preserve speaker identity during cross-identity reenactment, an identity-adaptive module is integrated, which projects motion latents into personalized spaces, ensuring a clear separation between motion and identity features. Additionally, IMTalker employs a lightweight flow-matching motion generator that creates vivid and controllable implicit motion vectors from a combination of audio, pose, and gaze inputs. Experimental results demonstrate that IMTalker outperforms previous state-of-the-art methods in terms of motion accuracy, identity preservation, and audio-lip synchronization quality. The system is also highly efficient, operating at 40 frames per second (FPS) during video-driven generation and 42 FPS for audio-driven generation on an RTX 4090 GPU. The authors plan to release their code and pretrained models to support further applications and research in this domain. <div>
arXiv:2511.22167v1 Announce Type: new 
Abstract: Talking face generation aims to synthesize realistic speaking portraits from a single image, yet existing methods often rely on explicit optical flow and local warping, which fail to model complex global motions and cause identity drift. We present IMTalker, a novel framework that achieves efficient and high-fidelity talking face generation through implicit motion transfer. The core idea is to replace traditional flow-based warping with a cross-attention mechanism that implicitly models motion discrepancy and identity alignment within a unified latent space, enabling robust global motion rendering. To further preserve speaker identity during cross-identity reenactment, we introduce an identity-adaptive module that projects motion latents into personalized spaces, ensuring clear disentanglement between motion and identity. In addition, a lightweight flow-matching motion generator produces vivid and controllable implicit motion vectors from audio, pose, and gaze cues. Extensive experiments demonstrate that IMTalker surpasses prior methods in motion accuracy, identity preservation, and audio-lip synchronization, achieving state-of-the-art quality with superior efficiency, operating at 40 FPS for video-driven and 42 FPS for audio-driven generation on an RTX 4090 GPU. We will release our code and pre-trained models to facilitate applications and future research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2511.22169</link>
<guid>https://arxiv.org/abs/2511.22169</guid>
<content:encoded><![CDATA[
<div> Keywords: particulate matter forecasting, East Asia, CMAQ-OBS dataset, Group-Relative Policy Optimization, False Alarm Rate reduction<br /><br />Summary:<br /><br />Accurate long-term forecasting of particulate matter (PM) concentration is vital for public health decision-making, especially in regions with complex atmospheric conditions like East Asia. Existing foundation models such as Aurora provide wide applicability but often fail to capture region-specific dynamics and depend on non-real-time inputs, limiting their effectiveness for localized and timely warnings. To overcome these limitations, the authors developed and released the CMAQ-OBS dataset, which combines real-world observations and high-resolution model outputs for East Asia. This dataset reduces regional forecasting error by 59.5% and supports real-time forecasts ranging from 48 to 120 hours, crucial for enabling prompt public health alerts. The study highlights the inadequacy of standard point-wise training objectives, which overlook the asymmetric costs of false alarms versus missed severe pollution events. Standard supervised fine-tuning (SFT) models tend to over-predict, leading to elevated false alarm rates and undermining public trust. To better align the models with operational priorities, the authors introduce Group-Relative Policy Optimization (GRPO), a method employing class-wise rewards and curriculum rollout strategies. Experimental results demonstrate that their approach significantly reduces false alarm rates by 47.3% compared to SFT baselines, while maintaining a competitive F1-score. This framework offers a practical and reliable solution for long horizon air quality forecasting in real-world applications. <div>
arXiv:2511.22169v1 Announce Type: new 
Abstract: Accurate long horizon forecasting of particulate matter (PM) concentration fields is essential for operational public health decisions. However, achieving reliable forecasts remains challenging in regions with complex terrain and strong atmospheric dynamics such as East Asia. While foundation models such as Aurora offer global generality, they often miss region-specific dynamics and rely on non-real-time inputs, limiting their practical utility for localized warning systems. To address this gap, we construct and release the real-world observations and high-resolution CMAQ-OBS dataset for East Asia, reducing regional error by 59.5% and enabling real-time 48-120 hour forecasts critical for public health alerts. However, standard point-wise objectives cannot reflect asymmetric operational costs, where false alarms deteriorate public trust while missed severe events endanger populations. This cost mismatch causes SFT models to over-predict and yield high False Alarm Rates. We introduce Group-Relative Policy Optimization (GRPO) with class-wise rewards and curriculum rollout to align predictions with operational priorities. Experimental results demonstrate that our framework significantly improves the reliability of the forecast. Compared to the SFT-only baseline, our model reduces the False Alarm Rate by 47.3% while achieving a competitive F1-score, proving its effectiveness for practical, real-world air quality forecasting systems on long lead time scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partially Shared Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2511.22170</link>
<guid>https://arxiv.org/abs/2511.22170</guid>
<content:encoded><![CDATA[
<div> Concept Bottleneck Models, Large Language Models, Vision-Language Models, interpretability, concept compactness<br /><br />Summary:<br /><br />1. Concept Bottleneck Models (CBMs) improve interpretability by inserting a layer of human-understandable concepts between input data and model predictions. 2. Existing automated concept generation methods using Large Language Models (LLMs) and Vision-Language Models (VLMs) suffer from issues like poor visual grounding, redundant concepts, and lack of suitable metrics to balance accuracy and compactness. 3. The authors propose PS-CBM, a Partially Shared Concept Bottleneck Model framework that mitigates these challenges with three key components: a multimodal concept generator integrating LLM-based semantic information with exemplar visual cues; a Partially Shared Concept Strategy that merges concepts based on their activation patterns to balance specificity and reduce redundancy; and a new post-hoc metric named Concept-Efficient Accuracy (CEA) that jointly evaluates predictive accuracy and concept compactness. 4. PS-CBM was rigorously tested on eleven diverse datasets, consistently outperforming state-of-the-art CBMs by improving classification accuracy by 1.0% to 7.4% and increasing CEA by 2.0% to 9.5%. 5. The results demonstrate that PS-CBM achieves a better trade-off between high classification accuracy and strong interpretability while requiring fewer concepts, highlighting its effectiveness in enhancing concept-based model design. <div>
arXiv:2511.22170v1 Announce Type: new 
Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by introducing a layer of human-understandable concepts between inputs and predictions. While recent methods automate concept generation using Large Language Models (LLMs) and Vision-Language Models (VLMs), they still face three fundamental challenges: poor visual grounding, concept redundancy, and the absence of principled metrics to balance predictive accuracy and concept compactness. We introduce PS-CBM, a Partially Shared CBM framework that addresses these limitations through three core components: (1) a multimodal concept generator that integrates LLM-derived semantics with exemplar-based visual cues; (2) a Partially Shared Concept Strategy that merges concepts based on activation patterns to balance specificity and compactness; and (3) Concept-Efficient Accuracy (CEA), a post-hoc metric that jointly captures both predictive accuracy and concept compactness. Extensive experiments on eleven diverse datasets show that PS-CBM consistently outperforms state-of-the-art CBMs, improving classification accuracy by 1.0%-7.4% and CEA by 2.0%-9.5%, while requiring significantly fewer concepts. These results underscore PS-CBM's effectiveness in achieving both high accuracy and strong interpretability.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrepGPT: Autoregressive B-rep Generation with Voronoi Half-Patch</title>
<link>https://arxiv.org/abs/2511.22171</link>
<guid>https://arxiv.org/abs/2511.22171</guid>
<content:encoded><![CDATA[
<div> Boundary representation, B-rep, Voronoi Half-Patch, VQ-VAE, Transformer

<br /><br />Summary:  
Boundary representation (B-rep) is the standard format for CAD models but existing generative methods rely on complex multi-stage networks that cause errors and inefficiency. The authors introduce BrepGPT, a single-stage autoregressive model that generates B-rep structures more effectively. The core innovation is the Voronoi Half-Patch (VHP) representation, which breaks down B-reps into local units by assigning geometry to the closest half-edges and sampling their connections. This approach unifies geometric and topological data into a single coherent format, avoiding multiple hierarchical encodings. The method uses dual vector-quantized VAEs (VQ-VAEs) to encode vertex topology and VHP units into compact vertex-based tokens. These tokens are then autoregressively predicted by a decoder-only Transformer, enabling direct reconstruction of complete B-rep models. Experiments show that BrepGPT outperforms previous methods in unconditional B-rep generation. Additionally, the framework supports conditional generation based on category labels, point clouds, text, and images, as well as applications like B-rep autocompletion and interpolation. This demonstrates its versatility and potential for streamlined CAD model generation in diverse scenarios. <div>
arXiv:2511.22171v1 Announce Type: new 
Abstract: Boundary representation (B-rep) is the de facto standard for CAD model representation in modern industrial design. The intricate coupling between geometric and topological elements in B-rep structures has forced existing generative methods to rely on cascaded multi-stage networks, resulting in error accumulation and computational inefficiency. We present BrepGPT, a single-stage autoregressive framework for B-rep generation. Our key innovation lies in the Voronoi Half-Patch (VHP) representation, which decomposes B-reps into unified local units by assigning geometry to nearest half-edges and sampling their next pointers. Unlike hierarchical representations that require multiple distinct encodings for different structural levels, our VHP representation facilitates unifying geometric attributes and topological relations in a single, coherent format. We further leverage dual VQ-VAEs to encode both vertex topology and Voronoi Half-Patches into vertex-based tokens, achieving a more compact sequential encoding. A decoder-only Transformer is then trained to autoregressively predict these tokens, which are subsequently mapped to vertex-based features and decoded into complete B-rep models. Experiments demonstrate that BrepGPT achieves state-of-the-art performance in unconditional B-rep generation. The framework also exhibits versatility in various applications, including conditional generation from category labels, point clouds, text descriptions, and images, as well as B-rep autocompletion and interpolation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning</title>
<link>https://arxiv.org/abs/2511.22172</link>
<guid>https://arxiv.org/abs/2511.22172</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal AI, visual grounded reasoning, reinforcement learning, Salience-Weighted IoU Reward, cognitive flexibility<br /><br />Summary:  
The paper presents GRiP (Guided Reasoning and Perception), a novel two-stage training framework designed to enhance models' capabilities in visual grounded reasoning by explicitly guiding perceptual focus and logical reasoning pathways. It addresses limitations in current methods, which either suffer from instability in end-to-end reinforcement learning (RL) or lack flexibility due to supervised fine-tuning (SFT). GRiP introduces two main innovations during its cognitive-enhanced RL stage: (1) a Salience-Weighted Intersection over Union (IoU) Reward that encourages prioritizing mission-critical objects over irrelevant distractors, and (2) a Multi-Heuristic Reward that fosters cognitive flexibility by rewarding diverse yet logically valid reasoning strategies. Starting with the Qwen2.5-VL-7B model, GRiP significantly improves performance on multiple challenging benchmarks, achieving state-of-the-art results among open-source models on TreeBench and V* Bench, both known for their complex visual reasoning tasks. This demonstrates that going beyond simplistic reward functions and incorporating cognitively inspired signals about what to observe and how to reason is vital for advancing multimodal intelligence. The authors also commit to releasing the code publicly to support further research. <div>
arXiv:2511.22172v1 Announce Type: new 
Abstract: Models capable of "thinking with images" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Graph Convolutional Network with Chebyshev Spectral Graph and Graph Attention for Autism Spectrum Disorder Classification</title>
<link>https://arxiv.org/abs/2511.22178</link>
<guid>https://arxiv.org/abs/2511.22178</guid>
<content:encoded><![CDATA[
<div> Keywords: Autism Spectrum Disorder, Graph Convolutional Network, multimodal neuroimaging, Chebyshev Spectral Graph Convolution, Graph Attention Networks  

<br /><br />Summary:  
This paper addresses the challenges of early and objective diagnosis of Autism Spectrum Disorder (ASD), a neurodevelopmental disorder with diverse symptom manifestations and neurological bases. The authors propose a novel Graph Convolutional Network (GCN) model that integrates Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT) to improve classification accuracy of ASD by utilizing multimodal neuroimaging data and phenotypic information. The study leverages the ABIDE I dataset, consisting of resting-state functional MRI (rs-fMRI), structural MRI (sMRI), and phenotypic data from 870 individuals. The model employs a multi-branch architecture to process each data modality separately before merging them via concatenation. A population graph is constructed based on site similarity to capture inter-individual relationships. Chebyshev polynomial filters enable efficient localized spectral learning, while GAT layers enhance node representation through attention-weighted aggregation of neighboring nodes. The model is trained using stratified five-fold cross-validation with an input dimension of 5,206 features per subject. Experimental results demonstrate that the proposed model outperforms several state-of-the-art baselines, achieving a test accuracy of 74.82% along with an AUC of 0.82, highlighting its potential for more accurate and robust ASD diagnosis using multimodal data. <div>
arXiv:2511.22178v1 Announce Type: new 
Abstract: ASD is a complicated neurodevelopmental disorder marked by variation in symptom presentation and neurological underpinnings, making early and objective diagnosis extremely problematic. This paper presents a Graph Convolutional Network (GCN) model, incorporating Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT), to increase the classification accuracy of ASD utilizing multimodal neuroimaging and phenotypic data. Leveraging the ABIDE I dataset, which contains resting-state functional MRI (rs-fMRI), structural MRI (sMRI), and phenotypic variables from 870 patients, the model leverages a multi-branch architecture that processes each modality individually before merging them via concatenation. Graph structure is encoded using site-based similarity to generate a population graph, which helps in understanding relationship connections across individuals. Chebyshev polynomial filters provide localized spectral learning with lower computational complexity, whereas GAT layers increase node representations by attention-weighted aggregation of surrounding information. The proposed model is trained using stratified five-fold cross-validation with a total input dimension of 5,206 features per individual. Extensive trials demonstrate the enhanced model's superiority, achieving a test accuracy of 74.82\% and an AUC of 0.82 on the entire dataset, surpassing multiple state-of-the-art baselines, including conventional GCNs, autoencoder-based deep neural networks, and multimodal CNNs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image Encoding and Multiple Trajectory Prediction</title>
<link>https://arxiv.org/abs/2511.22181</link>
<guid>https://arxiv.org/abs/2511.22181</guid>
<content:encoded><![CDATA[
<div> trajectory planning, autonomous driving, Vision Transformer (ViT), motion prediction, multimodal trajectory

<br /><br />Summary:  
The paper introduces MTR-VP, a novel approach for trajectory planning in autonomous driving that relies on vision-based context embeddings instead of traditional map-based features. 1) The method uses a Vision Transformer (ViT) encoder to process raw camera images along with past vehicle kinematic states, producing context embeddings inspired by the Motion Transformer (MTR) framework. 2) MTR-VP replaces MTR’s learnable intention queries with a cross-attention mechanism that integrates intent information with scene and historical vehicle data embeddings. 3) The approach is evaluated on the Waymo End-to-End Driving Dataset, aiming to predict 5-second future trajectories in bird’s-eye-view coordinates by utilizing prior images, pose histories, and routing goals. 4) Ablation studies reveal that transformer-based fusion of visual and kinetic features alone struggles to create effective scene context embeddings, even when enhanced with foundation-model embeddings like CLIP and DINOv2. 5) However, forecasting a distribution over multiple future trajectories rather than a single one significantly improves planning quality, highlighting the benefit of multimodal prediction outputs for autonomous driving trajectory planning. <div>
arXiv:2511.22181v1 Announce Type: new 
Abstract: We present a method for trajectory planning for autonomous driving, learning image-based context embeddings that align with motion prediction frameworks and planning-based intention input. Within our method, a ViT encoder takes raw images and past kinematic state as input and is trained to produce context embeddings, drawing inspiration from those generated by the recent MTR (Motion Transformer) encoder, effectively substituting map-based features with learned visual representations. MTR provides a strong foundation for multimodal trajectory prediction by localizing agent intent and refining motion iteratively via motion query pairs; we name our approach MTR-VP (Motion Transformer for Vision-based Planning), and instead of the learnable intention queries used in the MTR decoder, we use cross attention on the intent and the context embeddings, which reflect a combination of information encoded from the driving scene and past vehicle states. We evaluate our methods on the Waymo End-to-End Driving Dataset, which requires predicting the agent's future 5-second trajectory in bird's-eye-view coordinates using prior camera images, agent pose history, and routing goals. We analyze our architecture using ablation studies, removing input images and multiple trajectory output. Our results suggest that transformer-based methods that are used to combine the visual features along with the kinetic features such as the past trajectory features are not effective at combining both modes to produce useful scene context embeddings, even when intention embeddings are augmented with foundation-model representations of scene context from CLIP and DINOv2, but that predicting a distribution over multiple futures instead of a single future trajectory boosts planning performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation</title>
<link>https://arxiv.org/abs/2511.22184</link>
<guid>https://arxiv.org/abs/2511.22184</guid>
<content:encoded><![CDATA[
<div> Keywords: foot contact estimation, shoe style-invariant, ground-aware learning, adversarial training, dense prediction<br /><br />Summary:<br /><br />Foot contact is a crucial aspect of human physical interaction and movement analysis, yet existing methods mostly rely on joint-level approximations using zero-velocity constraints which lack detailed interaction modeling between the foot and environment. This study addresses the challenge of predicting dense foot contact maps from a single RGB image, a largely underexplored area. Two primary difficulties are identified: (1) high variability in shoe appearances that hinder generalization across different shoe styles, and (2) the typically monotonous texture of the ground, which provides limited informative features for prediction. To overcome these challenges, the authors propose the FEet COntact estimation (FECO) framework, which learns dense foot contact by incorporating both shoe style-invariant and ground-aware learning components. The framework leverages adversarial training to enforce shoe style-invariant feature learning, allowing the model to generalize across diverse shoe types effectively. Additionally, a ground feature extractor is introduced to capture ground characteristics using spatial context, improving the utilization of ground information for contact estimation. Experimental results demonstrate that FECO attains robust and reliable foot contact predictions regardless of shoe appearance and effectively exploits ground cues. The authors plan to release their code to facilitate further research in this domain. <div>
arXiv:2511.22184v1 Announce Type: new 
Abstract: Foot contact plays a critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using a zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from a single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has a monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present a FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce a ground feature extractor that captures ground properties based on spatial context. As a result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridWorldSim: A Scalable and Controllable High-fidelity Simulator for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.22187</link>
<guid>https://arxiv.org/abs/2511.22187</guid>
<content:encoded><![CDATA[
<div> Keywords: HybridWorldSim, multi-traversal neural reconstruction, generative modeling, autonomous driving simulation, MIRROR dataset<br /><br />Summary:<br /><br />1. The paper presents HybridWorldSim, a novel hybrid simulation framework designed to enhance end-to-end autonomous driving by tackling limitations found in existing simulation methods.<br />2. HybridWorldSim combines multi-traversal neural reconstruction techniques for accurately rendering static backgrounds with generative modeling approaches to simulate dynamic agents, thus improving both visual fidelity and spatial consistency.<br />3. This unified approach allows for high-quality novel view synthesis even under large viewpoint changes, addressing key challenges in realistic driving scenario simulation.<br />4. To support robust benchmarking and research, the authors introduce the MIRROR dataset, which consists of extensive multi-traversal data capturing diverse routes and varying environmental conditions across multiple cities.<br />5. Extensive experiments demonstrate that HybridWorldSim outperforms previous state-of-the-art methods, providing a practical, scalable, and high-fidelity simulation solution that can accelerate research and development in autonomous driving systems. <div>
arXiv:2511.22187v1 Announce Type: new 
Abstract: Realistic and controllable simulation is critical for advancing end-to-end autonomous driving, yet existing approaches often struggle to support novel view synthesis under large viewpoint changes or to ensure geometric consistency. We introduce HybridWorldSim, a hybrid simulation framework that integrates multi-traversal neural reconstruction for static backgrounds with generative modeling for dynamic agents. This unified design addresses key limitations of previous methods, enabling the creation of diverse and high-fidelity driving scenarios with reliable visual and spatial consistency. To facilitate robust benchmarking, we further release a new multi-traversal dataset MIRROR that captures a wide range of routes and environmental conditions across different cities. Extensive experiments demonstrate that HybridWorldSim surpasses previous state-of-the-art methods, providing a practical and scalable solution for high-fidelity simulation and a valuable resource for research and development in autonomous driving.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARPGNet: Appearance- and Relation-aware Parallel Graph Attention Fusion Network for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2511.22188</link>
<guid>https://arxiv.org/abs/2511.22188</guid>
<content:encoded><![CDATA[
<div> Keywords: facial expression recognition, spatial-temporal representation, graph attention, CNN, facial region relations<br /><br />Summary: This paper addresses the challenge of learning discriminative spatial-temporal representations crucial for facial expression recognition by capturing facial expression dynamics more effectively. Unlike previous methods that primarily depend on pre-trained Convolutional Neural Networks (CNNs) focusing on facial appearance and often neglecting the interrelations between facial regions, the authors introduce the Appearance- and Relation-aware Parallel Graph attention fusion Network (ARPGNet). ARPGNet constructs a facial region relation graph and utilizes a graph attention mechanism to explicitly model relationships between different facial regions, enabling enhanced relational representation sequences. Alongside these, CNN-based appearance representation sequences are concurrently processed through a parallel graph attention fusion module. This module facilitates mutual interaction and enhancement between the appearance and relation representations while simultaneously capturing the complementarity across different sequences as well as their temporal dynamics. Experimental evaluation on three facial expression recognition datasets demonstrates that ARPGNet either outperforms or matches state-of-the-art methods, proving its effectiveness in integrating appearance and relational information for improved recognition accuracy. The proposed approach highlights the importance of considering facial region relationships and temporal information jointly to advance facial expression recognition performance. <div>
arXiv:2511.22188v1 Announce Type: new 
Abstract: The key to facial expression recognition is to learn discriminative spatial-temporal representations that embed facial expression dynamics. Previous studies predominantly rely on pre-trained Convolutional Neural Networks (CNNs) to learn facial appearance representations, overlooking the relationships between facial regions. To address this issue, this paper presents an Appearance- and Relation-aware Parallel Graph attention fusion Network (ARPGNet) to learn mutually enhanced spatial-temporal representations of appearance and relation information. Specifically, we construct a facial region relation graph and leverage the graph attention mechanism to model the relationships between facial regions. The resulting relational representation sequences, along with CNN-based appearance representation sequences, are then fed into a parallel graph attention fusion module for mutual interaction and enhancement. This module simultaneously explores the complementarity between different representation sequences and the temporal dynamics within each sequence. Experimental results on three facial expression recognition datasets demonstrate that the proposed ARPGNet outperforms or is comparable to state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable 3D Object Generation with Single Image Prompt</title>
<link>https://arxiv.org/abs/2511.22194</link>
<guid>https://arxiv.org/abs/2511.22194</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, 3D object generation, textual inversion, image adapter, depth-conditioned warmup<br /><br />Summary: The paper addresses limitations in current 3D object generation methods that rely heavily on text-to-image diffusion models with textual inversion, which require additional training time and offer limited control. To overcome these issues, the authors propose two novel approaches. First, they introduce an off-the-shelf image adapter to generate 3D objects without the need for textual inversion, enabling better control over depth, pose, and textual conditions. Second, they present a depth-conditioned warmup strategy designed to improve 3D consistency in generated models. Experimental results demonstrate that their methods achieve comparable or better performance both qualitatively and quantitatively compared to existing text-inversion-based approaches, with noticeable improvements in maintaining 3D consistency. Additionally, a user study was conducted to evaluate how well the generated 3D objects match the input images and maintain 3D consistency. The results from this study show that their proposed model outperforms existing alternatives, confirming the effectiveness of the approaches. The authors also provide their implementation code on GitHub, facilitating further research and use. <div>
arXiv:2511.22194v1 Announce Type: new 
Abstract: Recently, the impressive generative capabilities of diffusion models have been demonstrated, producing images with remarkable fidelity. Particularly, existing methods for the 3D object generation tasks, which is one of the fastest-growing segments in computer vision, pre-dominantly use text-to-image diffusion models with textual inversion which train a pseudo text prompt to describe the given image. In practice, various text-to-image generative models employ textual inversion to learn concepts or styles of target object in the pseudo text prompt embedding space, thereby generating sophisticated outputs. However, textual inversion requires additional training time and lacks control ability. To tackle this issues, we propose two innovative methods: (1) using an off-the-shelf image adapter that generates 3D objects without textual inversion, offering enhanced control over conditions such as depth, pose, and text. (2) a depth conditioned warmup strategy to enhance 3D consistency. In experimental results, ours show qualitatively and quantitatively comparable performance and improved 3D consistency to the existing text-inversion-based alternatives. Furthermore, we conduct a user study to assess (i) how well results match the input image and (ii) whether 3D consistency is maintained. User study results show that our model outperforms the alternatives, validating the effectiveness of our approaches. Our code is available at GitHub repository:https://github.com/Seooooooogi/Control3D_IP/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-Consistent Multi-View Editing by Diffusion Guidance</title>
<link>https://arxiv.org/abs/2511.22228</link>
<guid>https://arxiv.org/abs/2511.22228</guid>
<content:encoded><![CDATA[
<div> Diffusion models, multi-view consistency, 3D image editing, NeRF, Gaussian Splat<br /><br />Summary:<br /><br />Recent innovations in diffusion models have significantly enhanced text-based image editing capabilities. However, conventional approaches that edit images individually often result in geometric and photometric inconsistencies across different views of the same 3D scene, which is especially problematic for 3D representations like Neural Radiance Fields (NeRFs) and Gaussian Splat models. To overcome these challenges, the authors propose a novel, training-free diffusion framework designed to enforce multi-view consistency during the image editing process. The core idea relies on the assumption that corresponding points in unedited multi-view images should undergo similar transformations after the editing step. To realize this, a novel consistency loss is introduced that steers the diffusion sampling process toward producing coherent and consistent edits across views. This framework is flexible and compatible with various existing image editing techniques, supporting scenarios involving both dense and sparse multi-view setups. Extensive experiments demonstrate that their method significantly improves 3D consistency when compared to prior multi-view editing approaches. Furthermore, the approach enables superior quality editing within Gaussian Splat models, producing sharp details and ensuring fidelity to user-provided text prompts. Additional visual and video results are accessible on their project webpage for deeper insights. <div>
arXiv:2511.22228v1 Announce Type: new 
Abstract: Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene. Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models. We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process. The key assumption is that corresponding points in the unedited images should undergo similar transformations after editing. To achieve this, we introduce a consistency loss that guides the diffusion sampling toward coherent edits. The framework is flexible and can be combined with widely varying image editing methods, supporting both dense and sparse multi-view editing setups. Experimental results show that our approach significantly improves 3D consistency compared to existing multi-view editing methods. We also show that this increased consistency enables high-quality Gaussian Splat editing with sharp details and strong fidelity to user-specified text prompts. Please refer to our project page for video results: https://3d-consistent-editing.github.io/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation</title>
<link>https://arxiv.org/abs/2511.22232</link>
<guid>https://arxiv.org/abs/2511.22232</guid>
<content:encoded><![CDATA[
arXiv:2511.22232v1 Announce Type: new 
Abstract: Multi-modal large language models (MLLMs) have shown promise in advancing healthcare. However, most existing models remain confined to single-image understanding, which greatly limits their applicability in clinical workflows. In practice, medical diagnosis and progression often require synthesizing information across multiple images from different modalities or time points. The development of medical MLLMs capable of such multi-image understanding has been hindered by the lack of large-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that leverages license-permissive compound images in biomedical literature, as a rich yet underutilized data source for multi-image analysis. Specifically, we design a five-stage, context-aware instruction generation paradigm underpinned by a divide-and-conquer strategy. By decomposing multi-image analysis into manageable sub-tasks, this paradigm empowers MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the complex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over 237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical multi-image multi-modal large language model. For benchmarking, we construct PMC-MI-Bench for composite understanding, manually validated by medical experts. Extensive experiments show that M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization to longitudinal chest X-ray analysis using the MIMIC dataset. This work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning, bridging the gap between biomedical literature and real-world clinical applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IE-SRGS: An Internal-External Knowledge Fusion Framework for High-Fidelity 3D Gaussian Splatting Super-Resolution</title>
<link>https://arxiv.org/abs/2511.22233</link>
<guid>https://arxiv.org/abs/2511.22233</guid>
<content:encoded><![CDATA[
arXiv:2511.22233v1 Announce Type: new 
Abstract: Reconstructing high-resolution (HR) 3D Gaussian Splatting (3DGS) models from low-resolution (LR) inputs remains challenging due to the lack of fine-grained textures and geometry. Existing methods typically rely on pre-trained 2D super-resolution (2DSR) models to enhance textures, but suffer from 3D Gaussian ambiguity arising from cross-view inconsistencies and domain gaps inherent in 2DSR models. We propose IE-SRGS, a novel 3DGS SR paradigm that addresses this issue by jointly leveraging the complementary strengths of external 2DSR priors and internal 3DGS features. Specifically, we use 2DSR and depth estimation models to generate HR images and depth maps as external knowledge, and employ multi-scale 3DGS models to produce cross-view consistent, domain-adaptive counterparts as internal knowledge. A mask-guided fusion strategy is introduced to integrate these two sources and synergistically exploit their complementary strengths, effectively guiding the 3D Gaussian optimization toward high-fidelity reconstruction. Extensive experiments on both synthetic and real-world benchmarks show that IE-SRGS consistently outperforms state-of-the-art methods in both quantitative accuracy and visual fidelity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging 3D Deep Learning and Curation for Analysis and High-Quality Segmentation in Practice</title>
<link>https://arxiv.org/abs/2511.22236</link>
<guid>https://arxiv.org/abs/2511.22236</guid>
<content:encoded><![CDATA[
arXiv:2511.22236v1 Announce Type: new 
Abstract: Accurate 3D microscopy image segmentation is critical for quantitative bioimage analysis but even state-of-the-art foundation models yield error-prone results. Therefore, manual curation is still widely used for either preparing high-quality training data or fixing errors before analysis. We present VessQC, an open-source tool for uncertainty-guided curation of large 3D microscopy segmentations. By integrating uncertainty maps, VessQC directs user attention to regions most likely containing biologically meaningful errors. In a preliminary user study uncertainty-guided correction significantly improved error detection recall from 67% to 94.0% (p=0.007) without a significant increase in total curation time. VessQC thus enables efficient, human-in-the-loop refinement of volumetric segmentations and bridges a key gap in real-world applications between uncertainty estimation and practical human-computer interaction. The software is freely available at github.com/MMV-Lab/VessQC.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Creating Blank Canvas Against AI-enabled Image Forgery</title>
<link>https://arxiv.org/abs/2511.22237</link>
<guid>https://arxiv.org/abs/2511.22237</guid>
<content:encoded><![CDATA[
arXiv:2511.22237v1 Announce Type: new 
Abstract: AIGC-based image editing technology has greatly simplified the realistic-level image modification, causing serious potential risks of image forgery. This paper introduces a new approach to tampering detection using the Segment Anything Model (SAM). Instead of training SAM to identify tampered areas, we propose a novel strategy. The entire image is transformed into a blank canvas from the perspective of neural models. Any modifications to this blank canvas would be noticeable to the models. To achieve this idea, we introduce adversarial perturbations to prevent SAM from ``seeing anything'', allowing it to identify forged regions when the image is tampered with. Due to SAM's powerful perceiving capabilities, naive adversarial attacks cannot completely tame SAM. To thoroughly deceive SAM and make it blind to the image, we introduce a frequency-aware optimization strategy, which further enhances the capability of tamper localization. Extensive experimental results demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTSnap: Test-Time Scaling of Diffusion Models via Noise-Aware Pruning</title>
<link>https://arxiv.org/abs/2511.22242</link>
<guid>https://arxiv.org/abs/2511.22242</guid>
<content:encoded><![CDATA[
arXiv:2511.22242v1 Announce Type: new 
Abstract: A prominent approach to test-time scaling for text-to-image diffusion models formulates the problem as a search over multiple noise seeds, selecting the one that maximizes a certain image-reward function. The effectiveness of this strategy heavily depends on the number and diversity of noise seeds explored. However, verifying each candidate is computationally expensive, because each must be fully denoised before a reward can be computed. This severely limits the number of samples that can be explored under a fixed budget. We propose test-time scaling with noise-aware pruning (TTSnap), a framework that prunes low-quality candidates without fully denoising them. The key challenge is that reward models are learned in the clean image domain, and the ranking of rewards predicted for intermediate estimates are often inconsistent with those predicted for clean images. To overcome this, we train noise-aware reward models via self-distillation to align the reward for intermediate estimates with that of the final clean images. To stabilize learning across different noise levels, we adopt a curriculum training strategy that progressively shifts the data domain from clean images to noise images. In addition, we introduce a new metric that measures reward alignment and computational budget utilization. Experiments demonstrate that our approach improves performance by over 16\% compared with existing methods, enabling more efficient and effective test-time scaling. It also provides orthogonal gains when combined with post-training techniques and local test-time optimization. Code: https://github.com/TerrysLearning/TTSnap/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Anchoring for Robust Personalization in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2511.22245</link>
<guid>https://arxiv.org/abs/2511.22245</guid>
<content:encoded><![CDATA[
arXiv:2511.22245v1 Announce Type: new 
Abstract: Text-to-image diffusion models have achieved remarkable progress in generating diverse and realistic images from textual descriptions. However, they still struggle with personalization, which requires adapting a pretrained model to depict user-specific subjects from only a few reference images. The key challenge lies in learning a new visual concept from a limited number of reference images while preserving the pretrained semantic prior that maintains text-image alignment. When the model focuses on subject fidelity, it tends to overfit the limited reference images and fails to leverage the pretrained distribution. Conversely, emphasizing prior preservation maintains semantic consistency but prevents the model from learning new personalized attributes. Building on these observations, we propose the personalization process through a semantic anchoring that guides adaptation by grounding new concepts in their corresponding distributions. We therefore reformulate personalization as the process of learning a rare concept guided by its frequent counterpart through semantic anchoring. This anchoring encourages the model to adapt new concepts in a stable and controlled manner, expanding the pretrained distribution toward personalized regions while preserving its semantic structure. As a result, the proposed method achieves stable adaptation and consistent improvements in both subject fidelity and text-image alignment compared to baseline methods. Extensive experiments and ablation studies further demonstrate the robustness and effectiveness of the proposed anchoring strategy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Diffusible High-Dimensional Latent Spaces: A Frequency Perspective</title>
<link>https://arxiv.org/abs/2511.22249</link>
<guid>https://arxiv.org/abs/2511.22249</guid>
<content:encoded><![CDATA[
arXiv:2511.22249v1 Announce Type: new 
Abstract: Latent diffusion has become the default paradigm for visual generation, yet we observe a persistent reconstruction-generation trade-off as latent dimensionality increases: higher-capacity autoencoders improve reconstruction fidelity but generation quality eventually declines. We trace this gap to the different behaviors in high-frequency encoding and decoding. Through controlled perturbations in both RGB and latent domains, we analyze encoder/decoder behaviors and find that decoders depend strongly on high-frequency latent components to recover details, whereas encoders under-represent high-frequency contents, yielding insufficient exposure and underfitting in high-frequency bands for diffusion model training. To address this issue, we introduce FreqWarm, a plug-and-play frequency warm-up curriculum that increases early-stage exposure to high-frequency latent signals during diffusion or flow-matching training -- without modifying or retraining the autoencoder. Applied across several high-dimensional autoencoders, FreqWarm consistently improves generation quality: decreasing gFID by 14.11 on Wan2.2-VAE, 6.13 on LTX-VAE, and 4.42 on DC-AE-f32, while remaining architecture-agnostic and compatible with diverse backbones. Our study shows that explicitly managing frequency exposure can successfully turn high-dimensional latent spaces into more diffusible targets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UMind-VL: A Generalist Ultrasound Vision-Language Model for Unified Grounded Perception and Comprehensive Interpretation</title>
<link>https://arxiv.org/abs/2511.22256</link>
<guid>https://arxiv.org/abs/2511.22256</guid>
<content:encoded><![CDATA[
arXiv:2511.22256v1 Announce Type: new 
Abstract: Despite significant strides in medical foundation models, the ultrasound domain lacks a comprehensive solution capable of bridging low-level Ultrasound Grounded Perception (e.g., segmentation, localization) and high-level Ultrasound Comprehensive Interpretation (e.g., diagnosis, reasoning). To bridge this gap, we propose UMind-VL, a unified foundation model designed to synergize pixel-level structural understanding with complex clinical reasoning. We first introduce UMind-DS, a large-scale multimodal dataset comprising 1.2 million ultrasound image-text pairs across 16 anatomical regions, enriching standard data with pixel-level annotations and clinician-validated rationales. Architecturally, UMind-VL incorporates a lightweight Dynamic Convolutional Mask Decoder that generates masks via dynamic kernels conditioned on LLM outputs. This design, combined with task-specific tokens, unifies segmentation, detection, geometric measurement, and diagnosis tasks within a single framework. Extensive evaluations demonstrate that UMind-VL significantly outperforms existing generalist multimodal models and achieves performance on par with, or superior to, state-of-the-art specialist models across segmentation, detection, keypoint localization, and diagnostic reasoning benchmarks, while maintaining strong generalization ability. We demonstrate the capability of UMind-VL in Figure 1.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Protective Watermarking Safeguard the Copyright of 3D Gaussian Splatting?</title>
<link>https://arxiv.org/abs/2511.22262</link>
<guid>https://arxiv.org/abs/2511.22262</guid>
<content:encoded><![CDATA[
arXiv:2511.22262v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for 3D scenes, widely adopted due to its exceptional efficiency and high-fidelity visual quality. Given the significant value of 3DGS assets, recent works have introduced specialized watermarking schemes to ensure copyright protection and ownership verification. However, can existing 3D Gaussian watermarking approaches genuinely guarantee robust protection of the 3D assets? In this paper, for the first time, we systematically explore and validate possible vulnerabilities of 3DGS watermarking frameworks. We demonstrate that conventional watermark removal techniques designed for 2D images do not effectively generalize to the 3DGS scenario due to the specialized rendering pipeline and unique attributes of each gaussian primitives. Motivated by this insight, we propose GSPure, the first watermark purification framework specifically for 3DGS watermarking representations. By analyzing view-dependent rendering contributions and exploiting geometrically accurate feature clustering, GSPure precisely isolates and effectively removes watermark-related Gaussian primitives while preserving scene integrity. Extensive experiments demonstrate that our GSPure achieves the best watermark purification performance, reducing watermark PSNR by up to 16.34dB while minimizing degradation to original scene fidelity with less than 1dB PSNR loss. Moreover, it consistently outperforms existing methods in both effectiveness and generalization.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveVGGT: Visual Geometry Transformer for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.22264</link>
<guid>https://arxiv.org/abs/2511.22264</guid>
<content:encoded><![CDATA[
arXiv:2511.22264v1 Announce Type: new 
Abstract: Feed-forward reconstruction has recently gained significant attention, with VGGT being a notable example. However, directly applying VGGT to autonomous driving (AD) systems leads to sub-optimal results due to the different priors between the two tasks. In AD systems, several important new priors need to be considered: (i) The overlap between camera views is minimal, as autonomous driving sensor setups are designed to achieve coverage at a low cost. (ii) The camera intrinsics and extrinsics are known, which introduces more constraints on the output and also enables the estimation of absolute scale. (iii) Relative positions of all cameras remain fixed though the ego vehicle is in motion. To fully integrate these priors into a feed-forward framework, we propose DriveVGGT, a scale-aware 4D reconstruction framework specifically designed for autonomous driving data. Specifically, we propose a Temporal Video Attention (TVA) module to process multi-camera videos independently, which better leverages the spatiotemporal continuity within each single-camera sequence. Then, we propose a Multi-camera Consistency Attention (MCA) module to conduct window attention with normalized relative pose embeddings, aiming to establish consistency relationships across different cameras while restricting each token to attend only to nearby frames. Finally, we extend the standard VGGT heads by adding an absolute scale head and an ego vehicle pose head. Experiments show that DriveVGGT outperforms VGGT, StreamVGGT, fastVGGT on autonomous driving dataset while extensive ablation studies verify effectiveness of the proposed designs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Collapse of Patches</title>
<link>https://arxiv.org/abs/2511.22281</link>
<guid>https://arxiv.org/abs/2511.22281</guid>
<content:encoded><![CDATA[
arXiv:2511.22281v1 Announce Type: new 
Abstract: Observing certain patches in an image reduces the uncertainty of others. Their realization lowers the distribution entropy of each remaining patch feature, analogous to collapsing a particle's wave function in quantum mechanics. This phenomenon can intuitively be called patch collapse. To identify which patches are most relied on during a target region's collapse, we learn an autoencoder that softly selects a subset of patches to reconstruct each target patch. Graphing these learned dependencies for each patch's PageRank score reveals the optimal patch order to realize an image. We show that respecting this order benefits various masked image modeling methods. First, autoregressive image generation can be boosted by retraining the state-of-the-art model MAR. Next, we introduce a new setup for image classification by exposing Vision Transformers only to high-rank patches in the collapse order. Seeing 22\% of such patches is sufficient to achieve high accuracy. With these experiments, we propose patch collapse as a novel image modeling perspective that promotes vision efficiency. Our project is available at https://github.com/wguo-ai/CoP .
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Match-and-Fuse: Consistent Generation from Unstructured Image Sets</title>
<link>https://arxiv.org/abs/2511.22287</link>
<guid>https://arxiv.org/abs/2511.22287</guid>
<content:encoded><![CDATA[
arXiv:2511.22287v1 Announce Type: new 
Abstract: We present Match-and-Fuse - a zero-shot, training-free method for consistent controlled generation of unstructured image sets - collections that share a common visual element, yet differ in viewpoint, time of capture, and surrounding content. Unlike existing methods that operate on individual images or densely sampled videos, our framework performs set-to-set generation: given a source set and user prompts, it produces a new set that preserves cross-image consistency of shared content. Our key idea is to model the task as a graph, where each node corresponds to an image and each edge triggers a joint generation of image pairs. This formulation consolidates all pairwise generations into a unified framework, enforcing their local consistency while ensuring global coherence across the entire set. This is achieved by fusing internal features across image pairs, guided by dense input correspondences, without requiring masks or manual supervision. It also allows us to leverage an emergent prior in text-to-image models that encourages coherent generation when multiple views share a single canvas. Match-and-Fuse achieves state-of-the-art consistency and visual quality, and unlocks new capabilities for content creation from image collections.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure is Supervision: Multiview Masked Autoencoders for Radiology</title>
<link>https://arxiv.org/abs/2511.22294</link>
<guid>https://arxiv.org/abs/2511.22294</guid>
<content:encoded><![CDATA[
arXiv:2511.22294v1 Announce Type: new 
Abstract: Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Object Detection for Birds with Swin Transformer</title>
<link>https://arxiv.org/abs/2511.22310</link>
<guid>https://arxiv.org/abs/2511.22310</guid>
<content:encoded><![CDATA[
arXiv:2511.22310v1 Announce Type: new 
Abstract: Object detection is the task of detecting objects in an image. In this task, the detection of small objects is particularly difficult. Other than the small size, it is also accompanied by difficulties due to blur, occlusion, and so on. Current small object detection methods are tailored to small and dense situations, such as pedestrians in a crowd or far objects in remote sensing scenarios. However, when the target object is small and sparse, there is a lack of objects available for training, making it more difficult to learn effective features. In this paper, we propose a specialized method for detecting a specific category of small objects; birds. Particularly, we improve the features learned by the neck; the sub-network between the backbone and the prediction head, to learn more effective features with a hierarchical design. We employ Swin Transformer to upsample the image features. Moreover, we change the shifted window size for adapting to small objects. Experiments show that the proposed Swin Transformer-based neck combined with CenterNet can lead to good performance by changing the window sizes. We further find that smaller window sizes (default 2) benefit mAPs for small object detection.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-based Consistent Video Colorization</title>
<link>https://arxiv.org/abs/2511.22330</link>
<guid>https://arxiv.org/abs/2511.22330</guid>
<content:encoded><![CDATA[
arXiv:2511.22330v1 Announce Type: new 
Abstract: Existing video colorization methods struggle with temporal flickering or demand extensive manual input. We propose a novel approach automating high-fidelity video colorization using rich semantic guidance derived from language and segmentation. We employ a language-conditioned diffusion model to colorize grayscale frames. Guidance is provided via automatically generated object masks and textual prompts; our primary automatic method uses a generic prompt, achieving state-of-the-art results without specific color input. Temporal stability is achieved by warping color information from previous frames using optical flow (RAFT); a correction step detects and fixes inconsistencies introduced by warping. Evaluations on standard benchmarks (DAVIS30, VIDEVO20) show our method achieves state-of-the-art performance in colorization accuracy (PSNR) and visual realism (Colorfulness, CDC), demonstrating the efficacy of automated prompt-based guidance for consistent video colorization.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unexplored flaws in multiple-choice VQA evaluations</title>
<link>https://arxiv.org/abs/2511.22341</link>
<guid>https://arxiv.org/abs/2511.22341</guid>
<content:encoded><![CDATA[
arXiv:2511.22341v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate strong capabilities in handling image-text inputs. A common way to assess this ability is through multiple-choice Visual Question Answering (VQA). Earlier works have already revealed that these benchmarks are sensitive to answer choice order, a limitation that can be mitigated through careful design. Yet, we highlight additional, unexplored biases in prompt formatting that question the reliability of current MLLM evaluations. Specifically, we identify three key variation factors in prompt formatting and analyze their impact through a large-scale study involving $\mathbf{\text{seven}}$ MLLMs and $\mathbf{\text{five}}$ VQA datasets, spanning $\mathbf{48}$ distinct $\mathbf{\text{prompt format variations}}$. Our findings reveal that multiple-choice VQA is highly sensitive to minor prompt format changes, even when these changes are semantically neutral. We further demonstrate that these biases persist independently of known order biases or the MLLM's confidence in the correct answer. Finally, we demonstrate that existing bias mitigation strategies fail to address these newly identified biases.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment</title>
<link>https://arxiv.org/abs/2511.22345</link>
<guid>https://arxiv.org/abs/2511.22345</guid>
<content:encoded><![CDATA[
arXiv:2511.22345v1 Announce Type: new 
Abstract: Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3$\times$, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64$\times$64 and 256$\times$256. Our code is available at https://github.com/MCG-NJU/FlowBack.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INSIGHT: An Interpretable Neural Vision-Language Framework for Reasoning of Generative Artifacts</title>
<link>https://arxiv.org/abs/2511.22351</link>
<guid>https://arxiv.org/abs/2511.22351</guid>
<content:encoded><![CDATA[
arXiv:2511.22351v1 Announce Type: new 
Abstract: The growing realism of AI-generated images produced by recent GAN and diffusion models has intensified concerns over the reliability of visual media. Yet, despite notable progress in deepfake detection, current forensic systems degrade sharply under real-world conditions such as severe downsampling, compression, and cross-domain distribution shifts. Moreover, most detectors operate as opaque classifiers, offering little insight into why an image is flagged as synthetic, undermining trust and hindering adoption in high-stakes settings.
  We introduce INSIGHT (Interpretable Neural Semantic and Image-based Generative-forensic Hallucination Tracing), a unified multimodal framework for robust detection and transparent explanation of AI-generated images, even at extremely low resolutions (16x16 - 64x64). INSIGHT combines hierarchical super-resolution for amplifying subtle forensic cues without inducing misleading artifacts, Grad-CAM driven multi-scale localization to reveal spatial regions indicative of generative patterns, and CLIP-guided semantic alignment to map visual anomalies to human-interpretable descriptors. A vision-language model is then prompted using a structured ReAct + Chain-of-Thought protocol to produce consistent, fine-grained explanations, verified through a dual-stage G-Eval + LLM-as-a-judge pipeline to minimize hallucinations and ensure factuality.
  Across diverse domains, including animals, vehicles, and abstract synthetic scenes, INSIGHT substantially improves both detection robustness and explanation quality under extreme degradation, outperforming prior detectors and black-box VLM baselines. Our results highlight a practical path toward transparent, reliable AI-generated image forensics and establish INSIGHT as a step forward in trustworthy multimodal content verification.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorFlow: Training-Free 3D Editing via Latent Anchor-Aligned Flows</title>
<link>https://arxiv.org/abs/2511.22357</link>
<guid>https://arxiv.org/abs/2511.22357</guid>
<content:encoded><![CDATA[
arXiv:2511.22357v1 Announce Type: new 
Abstract: Training-free 3D editing aims to modify 3D shapes based on human instructions without model finetuning. It plays a crucial role in 3D content creation. However, existing approaches often struggle to produce strong or geometrically stable edits, largely due to inconsistent latent anchors introduced by timestep-dependent noise during diffusion sampling. To address these limitations, we introduce AnchorFlow, which is built upon the principle of latent anchor consistency. Specifically, AnchorFlow establishes a global latent anchor shared between the source and target trajectories, and enforces coherence using a relaxed anchor-alignment loss together with an anchor-aligned update rule. This design ensures that transformations remain stable and semantically faithful throughout the editing process. By stabilizing the latent reference space, AnchorFlow enables more pronounced semantic modifications. Moreover, AnchorFlow is mask-free. Without mask supervision, it effectively preserves geometric fidelity. Experiments on the Eval3DEdit benchmark show that AnchorFlow consistently delivers semantically aligned and structurally robust edits across diverse editing types. Code is at https://github.com/ZhenglinZhou/AnchorFlow.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asking like Socrates: Socrates helps VLMs understand remote sensing images</title>
<link>https://arxiv.org/abs/2511.22396</link>
<guid>https://arxiv.org/abs/2511.22396</guid>
<content:encoded><![CDATA[
arXiv:2511.22396v1 Announce Type: new 
Abstract: Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAV-MM3D: A Large-Scale Synthetic Benchmark for 3D Perception of Unmanned Aerial Vehicles with Multi-Modal Data</title>
<link>https://arxiv.org/abs/2511.22404</link>
<guid>https://arxiv.org/abs/2511.22404</guid>
<content:encoded><![CDATA[
arXiv:2511.22404v1 Announce Type: new 
Abstract: Accurate perception of UAVs in complex low-altitude environments is critical for airspace security and related intelligent systems. Developing reliable solutions requires large-scale, accurately annotated, and multimodal data. However, real-world UAV data collection faces inherent constraints due to airspace regulations, privacy concerns, and environmental variability, while manual annotation of 3D poses and cross-modal correspondences is time-consuming and costly. To overcome these challenges, we introduce UAV-MM3D, a high-fidelity multimodal synthetic dataset for low-altitude UAV perception and motion understanding. It comprises 400K synchronized frames across diverse scenes (urban areas, suburbs, forests, coastal regions) and weather conditions (clear, cloudy, rainy, foggy), featuring multiple UAV models (micro, small, medium-sized) and five modalities - RGB, IR, LiDAR, Radar, and DVS (Dynamic Vision Sensor). Each frame provides 2D/3D bounding boxes, 6-DoF poses, and instance-level annotations, enabling core tasks related to UAVs such as 3D detection, pose estimation, target tracking, and short-term trajectory forecasting. We further propose LGFusionNet, a LiDAR-guided multimodal fusion baseline, and a dedicated UAV trajectory prediction baseline to facilitate benchmarking. With its controllable simulation environment, comprehensive scenario coverage, and rich annotations, UAV3D offers a public benchmark for advancing 3D perception of UAVs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffStyle360: Diffusion-Based 360{\deg} Head Stylization via Style Fusion Attention</title>
<link>https://arxiv.org/abs/2511.22411</link>
<guid>https://arxiv.org/abs/2511.22411</guid>
<content:encoded><![CDATA[
arXiv:2511.22411v1 Announce Type: new 
Abstract: 3D head stylization has emerged as a key technique for reimagining realistic human heads in various artistic forms, enabling expressive character design and creative visual experiences in digital media. Despite the progress in 3D-aware generation, existing 3D head stylization methods often rely on computationally expensive optimization or domain-specific fine-tuning to adapt to new styles. To address these limitations, we propose DiffStyle360, a diffusion-based framework capable of producing multi-view consistent, identity-preserving 3D head stylizations across diverse artistic domains given a single style reference image, without requiring per-style training. Building upon the 3D-aware DiffPortrait360 architecture, our approach introduces two key components: the Style Appearance Module, which disentangles style from content, and the Style Fusion Attention mechanism, which adaptively balances structure preservation and stylization fidelity in the latent space. Furthermore, we employ a 3D GAN-generated multi-view dataset for robust fine-tuning and introduce a temperaturebased key scaling strategy to control stylization intensity during inference. Extensive experiments on FFHQ and RenderMe360 demonstrate that DiffStyle360 achieves superior style quality, outperforming state-of-the-art GAN- and diffusion-based stylization methods across challenging style domains.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models</title>
<link>https://arxiv.org/abs/2511.22425</link>
<guid>https://arxiv.org/abs/2511.22425</guid>
<content:encoded><![CDATA[
arXiv:2511.22425v1 Announce Type: new 
Abstract: We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This avoids common artifacts like oversmoothing while maintaining semantic fidelity. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fin3R: Fine-tuning Feed-forward 3D Reconstruction Models via Monocular Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.22429</link>
<guid>https://arxiv.org/abs/2511.22429</guid>
<content:encoded><![CDATA[
arXiv:2511.22429v1 Announce Type: new 
Abstract: We present Fin3R, a simple, effective, and general fine-tuning method for feed-forward 3D reconstruction models. The family of feed-forward reconstruction model regresses pointmap of all input images to a reference frame coordinate system, along with other auxiliary outputs, in a single forward pass. However, we find that current models struggle with fine geometry and robustness due to (\textit{i}) the scarcity of high-fidelity depth and pose supervision and (\textit{ii}) the inherent geometric misalignment from multi-view pointmap regression. Fin3R jointly tackles two issues with an extra lightweight fine-tuning step. We freeze the decoder, which handles view matching, and fine-tune only the image encoder-the component dedicated to feature extraction. The encoder is enriched with fine geometric details distilled from a strong monocular teacher model on large, unlabeled datasets, using a custom, lightweight LoRA adapter. We validate our method on a wide range of models, including DUSt3R, MASt3R, CUT3R, and VGGT. The fine-tuned models consistently deliver sharper boundaries, recover complex structures, and achieve higher geometric accuracy in both single- and multi-view settings, while adding only the tiny LoRA weights, which leave test-time memory and latency virtually unchanged. Project page: \href{http://visual-ai.github.io/fin3r}{https://visual-ai.github.io/fin3r}
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkeletonAgent: An Agentic Interaction Framework for Skeleton-based Action Recognition</title>
<link>https://arxiv.org/abs/2511.22433</link>
<guid>https://arxiv.org/abs/2511.22433</guid>
<content:encoded><![CDATA[
arXiv:2511.22433v1 Announce Type: new 
Abstract: Recent advances in skeleton-based action recognition increasingly leverage semantic priors from Large Language Models (LLMs) to enrich skeletal representations. However, the LLM is typically queried in isolation from the recognition model and receives no performance feedback. As a result, it often fails to deliver the targeted discriminative cues critical to distinguish similar actions. To overcome these limitations, we propose SkeletonAgent, a novel framework that bridges the recognition model and the LLM through two cooperative agents, i.e., Questioner and Selector. Specifically, the Questioner identifies the most frequently confused classes and supplies them to the LLM as context for more targeted guidance. Conversely, the Selector parses the LLM's response to extract precise joint-level constraints and feeds them back to the recognizer, enabling finer-grained cross-modal alignment. Comprehensive evaluations on five benchmarks, including NTU RGB+D, NTU RGB+D 120, Kinetics-Skeleton, FineGYM, and UAV-Human, demonstrate that SkeletonAgent consistently outperforms state-of-the-art benchmark methods. The code is available at https://github.com/firework8/SkeletonAgent.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABounD: Adversarial Boundary-Driven Few-Shot Learning for Multi-Class Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.22436</link>
<guid>https://arxiv.org/abs/2511.22436</guid>
<content:encoded><![CDATA[
arXiv:2511.22436v1 Announce Type: new 
Abstract: Few-shot multi-class industrial anomaly detection remains a challenging task. Vision-language models need to be both category-adaptive and sharply discriminative, yet data scarcity often blurs the boundary between normal and abnormal states. This ambiguity leads to missed subtle defects and the rejection of atypical normal samples. We propose ABounD, an Adversarial Boundary-Driven few-shot learning for multi-class anomaly detection, which is a unified learning framework that integrates semantic concept learning with decision boundary shaping. The Dynamic Concept Fusion (DCF) module produces class-adaptive prompts by fusing generalizable priors with class-specific cues, conditioned on image features. Meanwhile, Adversarial Boundary Forging (ABF) sculpts a more precise decision margin by generating boundary-level fence features via PGD-style perturbations. Training is conducted in a single stage under a Concept-Boundary Loss, where ABF provides the main supervisory signal and semantic-spatial regularizers stabilize the optimization. This synergy yields a decision boundary that closely follows normal data while preserving flexibility and robust semantic alignment. Experiments on MVTec-AD and VisA datasets demonstrate state-of-the-art performance in the task of few-shot multi-class anomaly detection.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do You See What I Say? Generalizable Deepfake Detection based on Visual Speech Recognition</title>
<link>https://arxiv.org/abs/2511.22443</link>
<guid>https://arxiv.org/abs/2511.22443</guid>
<content:encoded><![CDATA[
arXiv:2511.22443v1 Announce Type: new 
Abstract: Deepfake generation has witnessed remarkable progress, contributing to highly realistic generated images, videos, and audio. While technically intriguing, such progress has raised serious concerns related to the misuse of manipulated media. To mitigate such misuse, robust and reliable deepfake detection is urgently needed. Towards this, we propose a novel network FauxNet, which is based on pre-trained Visual Speech Recognition (VSR) features. By extracting temporal VSR features from videos, we identify and segregate real videos from manipulated ones. The holy grail in this context has to do with zero-shot detection, i.e., generalizable detection, which we focus on in this work. FauxNet consistently outperforms the state-of-the-art in this setting. In addition, FauxNet is able to attribute - distinguish between generation techniques from which the videos stem. Finally, we propose new datasets, referred to as Authentica-Vox and Authentica-HDTF, comprising about 38,000 real and fake videos in total, the latter created with six recent deepfake generation techniques. We provide extensive analysis and results on the Authentica datasets and FaceForensics++, demonstrating the superiority of FauxNet. The Authentica datasets will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking machine learning models for multi-class state recognition in double duantum dot data</title>
<link>https://arxiv.org/abs/2511.22451</link>
<guid>https://arxiv.org/abs/2511.22451</guid>
<content:encoded><![CDATA[
arXiv:2511.22451v1 Announce Type: new 
Abstract: Semiconductor quantum dots (QDs) are a leading platform for scalable quantum processors. However, scaling to large arrays requires reliable, automated tuning strategies for devices' bootstrapping, calibration, and operation, with many tuning aspects depending on accurately identifying QD device states from charge-stability diagrams (CSDs). In this work, we present a comprehensive benchmarking study of four modern machine learning (ML) architectures for multi-class state recognition in double-QD CSDs. We evaluate their performance across different data budgets and normalization schemes using both synthetic and experimental data. We find that the more resource-intensive models -- U-Nets and visual transformers (ViTs) -- achieve the highest MSE score (defined as $1-\mathrm{MSE}$) on synthetic data (over $0.98$) but fail to generalize to experimental data. MDNs are the most computationally efficient and exhibit highly stable training, but with substantially lower peak performance. CNNs offer the most favorable trade-off on experimental CSDs, achieving strong accuracy with two orders of magnitude fewer parameters than the U-Nets and ViTs. Normalization plays a nontrivial role: min-max scaling generally yields higher MSE scores but less stable convergence, whereas z-score normalization produces more predictable training dynamics but at reduced accuracy for most models. Overall, our study shows that CNNs with min-max normalization are a practical approach for QD CSDs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Real versus Fake Towards Intent-Aware Video Analysis</title>
<link>https://arxiv.org/abs/2511.22455</link>
<guid>https://arxiv.org/abs/2511.22455</guid>
<content:encoded><![CDATA[
arXiv:2511.22455v1 Announce Type: new 
Abstract: The rapid advancement of generative models has led to increasingly realistic deepfake videos, posing significant societal and security risks. While existing detection methods focus on distinguishing real from fake videos, such approaches fail to address a fundamental question: What is the intent behind a manipulated video? Towards addressing this question, we introduce IntentHQ: a new benchmark for human-centered intent analysis, shifting the paradigm from authenticity verification to contextual understanding of videos. IntentHQ consists of 5168 videos that have been meticulously collected and annotated with 23 fine-grained intent-categories, including "Financial fraud", "Indirect marketing", "Political propaganda", as well as "Fear mongering". We perform intent recognition with supervised and self-supervised multi-modality models that integrate spatio-temporal video features, audio processing, and text analysis to infer underlying motivations and goals behind videos. Our proposed model is streamlined to differentiate between a wide range of intent-categories.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ITS3D: Inference-Time Scaling for Text-Guided 3D Diffusion Models</title>
<link>https://arxiv.org/abs/2511.22456</link>
<guid>https://arxiv.org/abs/2511.22456</guid>
<content:encoded><![CDATA[
arXiv:2511.22456v1 Announce Type: new 
Abstract: We explore inference-time scaling in text-guided 3D diffusion models to enhance generative quality without additional training. To this end, we introduce ITS3D, a framework that formulates the task as an optimization problem to identify the most effective Gaussian noise input. The framework is driven by a verifier-guided search algorithm, where the search algorithm iteratively refines noise candidates based on verifier feedback. To address the inherent challenges of 3D generation, we introduce three techniques for improved stability, efficiency, and exploration capability. 1) Gaussian normalization is applied to stabilize the search process. It corrects distribution shifts when noise candidates deviate from a standard Gaussian distribution during iterative updates. 2) The high-dimensional nature of the 3D search space increases computational complexity. To mitigate this, a singular value decomposition-based compression technique is employed to reduce dimensionality while preserving effective search directions. 3) To further prevent convergence to suboptimal local minima, a singular space reset mechanism dynamically updates the search space based on diversity measures. Extensive experiments demonstrate that ITS3D enhances text-to-3D generation quality, which shows the potential of computationally efficient search methods in generative processes. The source code is available at https://github.com/ZhenglinZhou/ITS3D.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussians on Fire: High-Frequency Reconstruction of Flames</title>
<link>https://arxiv.org/abs/2511.22459</link>
<guid>https://arxiv.org/abs/2511.22459</guid>
<content:encoded><![CDATA[
arXiv:2511.22459v1 Announce Type: new 
Abstract: We propose a method to reconstruct dynamic fire in 3D from a limited set of camera views with a Gaussian-based spatiotemporal representation. Capturing and reconstructing fire and its dynamics is highly challenging due to its volatile nature, transparent quality, and multitude of high-frequency features. Despite these challenges, we aim to reconstruct fire from only three views, which consequently requires solving for under-constrained geometry. We solve this by separating the static background from the dynamic fire region by combining dense multi-view stereo images with monocular depth priors. The fire is initialized as a 3D flow field, obtained by fusing per-view dense optical flow projections. To capture the high frequency features of fire, each 3D Gaussian encodes a lifetime and linear velocity to match the dense optical flow. To ensure sub-frame temporal alignment across cameras we employ a custom hardware synchronization pattern -- allowing us to reconstruct fire with affordable commodity hardware. Our quantitative and qualitative validations across numerous reconstruction experiments demonstrate robust performance for diverse and challenging real fire scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoadSceneBench: A Lightweight Benchmark for Mid-Level Road Scene Understanding</title>
<link>https://arxiv.org/abs/2511.22466</link>
<guid>https://arxiv.org/abs/2511.22466</guid>
<content:encoded><![CDATA[
arXiv:2511.22466v1 Announce Type: new 
Abstract: Understanding mid-level road semantics, which capture the structural and contextual cues that link low-level perception to high-level planning, is essential for reliable autonomous driving and digital map construction. However, existing benchmarks primarily target perception tasks such as detection or segmentation, overlooking the reasoning capabilities required to infer road topology and dynamic scene structure. To address this gap, we present RoadSceneBench, a lightweight yet information-rich benchmark designed to evaluate and advance visual reasoning in complex road environments. Unlike large-scale perception datasets, RoadSceneBench emphasizes relational understanding and structural consistency, encouraging models to capture the underlying logic of real-world road scenes. Furthermore, to enhance reasoning reliability, we propose Hierarchical Relational Reward Propagation with Temporal Consistency (HRRP-T), a training framework for Vision-Language Models (VLMs) in which reward signals adaptively promote spatial coherence and semantic alignment throughout the reasoning process. This paradigm enables models to move beyond static recognition toward geometry-aware and temporally consistent reasoning. Extensive experiments demonstrate that our method achieves state-of-the-art performance across diverse road configurations. RoadSceneBench thus provides a compact yet powerful foundation for studying mid-level road semantics and fostering structure-aware autonomous perception. Our dataset is available at https://github.com/XiyanLiu/RoadSceneBench.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid, Unified and Iterative: A Novel Framework for Text-based Person Anomaly Retrieval</title>
<link>https://arxiv.org/abs/2511.22470</link>
<guid>https://arxiv.org/abs/2511.22470</guid>
<content:encoded><![CDATA[
arXiv:2511.22470v1 Announce Type: new 
Abstract: Text-based person anomaly retrieval has emerged as a challenging task, with most existing approaches relying on complex deep-learning techniques. This raises a research question: How can the model be optimized to achieve greater fine-grained features? To address this, we propose a Local-Global Hybrid Perspective (LHP) module integrated with a Vision-Language Model (VLM), designed to explore the effectiveness of incorporating both fine-grained features alongside coarse-grained features. Additionally, we investigate a Unified Image-Text (UIT) model that combines multiple objective loss functions, including Image-Text Contrastive (ITC), Image-Text Matching (ITM), Masked Language Modeling (MLM), and Masked Image Modeling (MIM) loss. Beyond this, we propose a novel iterative ensemble strategy, by combining iteratively instead of using model results simultaneously like other ensemble methods. To take advantage of the superior performance of the LHP model, we introduce a novel feature selection algorithm based on its guidance, which helps improve the model's performance. Extensive experiments demonstrate the effectiveness of our method in achieving state-of-the-art (SOTA) performance on PAB dataset, compared with previous work, with a 9.70\% improvement in R@1, 1.77\% improvement in R@5, and 1.01\% improvement in R@10.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Cross-Generator Image Forgery Detection through DINOv3</title>
<link>https://arxiv.org/abs/2511.22471</link>
<guid>https://arxiv.org/abs/2511.22471</guid>
<content:encoded><![CDATA[
arXiv:2511.22471v1 Announce Type: new 
Abstract: As generative models become increasingly diverse and powerful, cross-generator detection has emerged as a new challenge. Existing detection methods often memorize artifacts of specific generative models rather than learning transferable cues, leading to substantial failures on unseen generators. Surprisingly, this work finds that frozen visual foundation models, especially DINOv3, already exhibit strong cross-generator detection capability without any fine-tuning. Through systematic studies on frequency, spatial, and token perspectives, we observe that DINOv3 tends to rely on global, low-frequency structures as weak but transferable authenticity cues instead of high-frequency, generator-specific artifacts. Motivated by this insight, we introduce a simple, training-free token-ranking strategy followed by a lightweight linear probe to select a small subset of authenticity-relevant tokens. This token subset consistently improves detection accuracy across all evaluated datasets. Our study provides empirical evidence and a feasible hypothesis for understanding why foundation models generalize across diverse generators, offering a universal, efficient, and interpretable baseline for image forgery detection.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI killed the video star. Audio-driven diffusion model for expressive talking head generation</title>
<link>https://arxiv.org/abs/2511.22488</link>
<guid>https://arxiv.org/abs/2511.22488</guid>
<content:encoded><![CDATA[
arXiv:2511.22488v1 Announce Type: new 
Abstract: We propose Dimitra++, a novel framework for audio-driven talking head generation, streamlined to learn lip motion, facial expression, as well as head pose motion. Specifically, we propose a conditional Motion Diffusion Transformer (cMDT) to model facial motion sequences, employing a 3D representation. The cMDT is conditioned on two inputs: a reference facial image, which determines appearance, as well as an audio sequence, which drives the motion. Quantitative and qualitative experiments, as well as a user study on two widely employed datasets, i.e., VoxCeleb2 and CelebV-HQ, suggest that Dimitra++ is able to outperform existing approaches in generating realistic talking heads imparting lip motion, facial expression, and head pose.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciPostGen: Bridging the Gap between Scientific Papers and Poster Layouts</title>
<link>https://arxiv.org/abs/2511.22490</link>
<guid>https://arxiv.org/abs/2511.22490</guid>
<content:encoded><![CDATA[
arXiv:2511.22490v1 Announce Type: new 
Abstract: As the number of scientific papers continues to grow, there is a demand for approaches that can effectively convey research findings, with posters serving as a key medium for presenting paper contents. Poster layouts determine how effectively research is communicated and understood, highlighting their growing importance. In particular, a gap remains in understanding how papers correspond to the layouts that present them, which calls for datasets with paired annotations at scale. To bridge this gap, we introduce SciPostGen, a large-scale dataset for understanding and generating poster layouts from scientific papers. Our analyses based on SciPostGen show that paper structures are associated with the number of layout elements in posters. Based on this insight, we explore a framework, Retrieval-Augmented Poster Layout Generation, which retrieves layouts consistent with a given paper and uses them as guidance for layout generation. We conducted experiments under two conditions: with and without layout constraints typically specified by poster creators. The results show that the retriever estimates layouts aligned with paper structures, and our framework generates layouts that also satisfy given constraints.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Shape Is Optimal for Masks in Text Removal?</title>
<link>https://arxiv.org/abs/2511.22499</link>
<guid>https://arxiv.org/abs/2511.22499</guid>
<content:encoded><![CDATA[
arXiv:2511.22499v1 Announce Type: new 
Abstract: The advent of generative models has dramatically improved the accuracy of image inpainting. In particular, by removing specific text from document images, reconstructing original images is extremely important for industrial applications. However, most existing methods of text removal focus on deleting simple scene text which appears in images captured by a camera in an outdoor environment. There is little research dedicated to complex and practical images with dense text. Therefore, we created benchmark data for text removal from images including a large amount of text. From the data, we found that text-removal performance becomes vulnerable against mask profile perturbation. Thus, for practical text-removal tasks, precise tuning of the mask shape is essential. This study developed a method to model highly flexible mask profiles and learn their parameters using Bayesian optimization. The resulting profiles were found to be character-wise masks. It was also found that the minimum cover of a text region is not optimal. Our research is expected to pave the way for a user-friendly guideline for manual masking.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA</title>
<link>https://arxiv.org/abs/2511.22521</link>
<guid>https://arxiv.org/abs/2511.22521</guid>
<content:encoded><![CDATA[
arXiv:2511.22521v1 Announce Type: new 
Abstract: Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\% ANLS and 82.4\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.22532</link>
<guid>https://arxiv.org/abs/2511.22532</guid>
<content:encoded><![CDATA[
arXiv:2511.22532v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration</title>
<link>https://arxiv.org/abs/2511.22533</link>
<guid>https://arxiv.org/abs/2511.22533</guid>
<content:encoded><![CDATA[
arXiv:2511.22533v1 Announce Type: new 
Abstract: Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diff-ICMH: Harmonizing Machine and Human Vision in Image Compression with Generative Prior</title>
<link>https://arxiv.org/abs/2511.22549</link>
<guid>https://arxiv.org/abs/2511.22549</guid>
<content:encoded><![CDATA[
arXiv:2511.22549v1 Announce Type: new 
Abstract: Image compression methods are usually optimized isolatedly for human perception or machine analysis tasks. We reveal fundamental commonalities between these objectives: preserving accurate semantic information is paramount, as it directly dictates the integrity of critical information for intelligent tasks and aids human understanding. Concurrently, enhanced perceptual quality not only improves visual appeal but also, by ensuring realistic image distributions, benefits semantic feature extraction for machine tasks. Based on this insight, we propose Diff-ICMH, a generative image compression framework aiming for harmonizing machine and human vision in image compression. It ensures perceptual realism by leveraging generative priors and simultaneously guarantees semantic fidelity through the incorporation of Semantic Consistency loss (SC loss) during training. Additionally, we introduce the Tag Guidance Module (TGM) that leverages highly semantic image-level tags to stimulate the pre-trained diffusion model's generative capabilities, requiring minimal additional bit rates. Consequently, Diff-ICMH supports multiple intelligent tasks through a single codec and bitstream without any task-specific adaptation, while preserving high-quality visual experience for human perception. Extensive experimental results demonstrate Diff-ICMH's superiority and generalizability across diverse tasks, while maintaining visual appeal for human perception. Code is available at: https://github.com/RuoyuFeng/Diff-ICMH.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bringing Your Portrait to 3D Presence</title>
<link>https://arxiv.org/abs/2511.22553</link>
<guid>https://arxiv.org/abs/2511.22553</guid>
<content:encoded><![CDATA[
arXiv:2511.22553v1 Announce Type: new 
Abstract: We present a unified framework for reconstructing animatable 3D human avatars from a single portrait across head, half-body, and full-body inputs. Our method tackles three bottlenecks: pose- and framing-sensitive feature representations, limited scalable data, and unreliable proxy-mesh estimation. We introduce a Dual-UV representation that maps image features to a canonical UV space via Core-UV and Shell-UV branches, eliminating pose- and framing-induced token shifts. We also build a factorized synthetic data manifold combining 2D generative diversity with geometry-consistent 3D renderings, supported by a training scheme that improves realism and identity consistency. A robust proxy-mesh tracker maintains stability under partial visibility. Together, these components enable strong in-the-wild generalization. Trained only on half-body synthetic data, our model achieves state-of-the-art head and upper-body reconstruction and competitive full-body results. Extensive experiments and analyses further validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Condition Embedded Regression Network for Automated Dental Abutment Design</title>
<link>https://arxiv.org/abs/2511.22578</link>
<guid>https://arxiv.org/abs/2511.22578</guid>
<content:encoded><![CDATA[
arXiv:2511.22578v1 Announce Type: new 
Abstract: The abutment is an important part of artificial dental implants, whose design process is time-consuming and labor-intensive. Long-term use of inappropriate dental implant abutments may result in implant complications, including peri-implantitis. Using artificial intelligence to assist dental implant abutment design can quickly improve the efficiency of abutment design and enhance abutment adaptability. In this paper, we propose a text condition embedded abutment design framework (TCEAD), the novel automated abutment design solution available in literature. The proposed study extends the self-supervised learning framework of the mesh mask autoencoder (MeshMAE) by introducing a text-guided localization (TGL) module to facilitate abutment area localization. As the parameter determination of the abutment is heavily dependent on local fine-grained features (the width and height of the implant and the distance to the opposing tooth), we pre-train the encoder using oral scan data to improve the model's feature extraction ability. Moreover, considering that the abutment area is only a small part of the oral scan data, we designed a TGL module, which introduces the description of the abutment area through the text encoder of Contrastive Language-Image Pre-training (CLIP), enabling the network to quickly locate the abutment area. We validated the performance of TCEAD on a large abutment design dataset. Extensive experiments demonstrate that TCEAD achieves an Intersection over Union (IoU) improvement of 0.8%-12.85% over other mainstream methods, underscoring its potential in automated dental abutment design.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization</title>
<link>https://arxiv.org/abs/2511.22586</link>
<guid>https://arxiv.org/abs/2511.22586</guid>
<content:encoded><![CDATA[
arXiv:2511.22586v1 Announce Type: new 
Abstract: We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as "think with image", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a "short is long" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HarmoCLIP: Harmonizing Global and Regional Representations in Contrastive Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22594</link>
<guid>https://arxiv.org/abs/2511.22594</guid>
<content:encoded><![CDATA[
arXiv:2511.22594v1 Announce Type: new 
Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated remarkable generalization ability and strong performance across a wide range of vision-language tasks. However, due to the lack of region-level supervision, CLIP exhibits limited fine-grained semantic understanding. Although several methods attempt to mitigate this issue, they unintentionally disrupt the global alignment, resulting in a persistent trade-off where improving local perception simultaneously degrades global coherence. In this paper, we propose HarmoCLIP, a novel framework designed to harmonize global and region representations within CLIP. We first identify that the absence of direct alignment between local textual and visual semantics is the fundamental cause of the trade-off. To address this, HarmoCLIP introduces an explicit fine-grained semantic supervision term that directly aligns textual segments with their corresponding visual regions, effectively bridging the image region space and the textual space. To further strengthen the representation capability at the local level, our method introduces a novel Region-Language Alignment supervision strategy that promotes fine-grained semantic learning without compromising global semantic consistency. Extensive experiments demonstrate that HarmoCLIP achieves state-of-the-art (improvement up to 69.78%) performance on the global task of retrieval and yields a substantial 3.2% improvement in Top-1 accuracy on the region task of bounding-box classification, consistently outperforming prior approaches while providing a balanced, efficient, and plug-and-play solution to the global-local trade-off in CLIP. Code is available at https://github.com/Erosist/HarmoCLIP.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnoRefiner: Anomaly-Aware Group-Wise Refinement for Zero-Shot Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.22595</link>
<guid>https://arxiv.org/abs/2511.22595</guid>
<content:encoded><![CDATA[
arXiv:2511.22595v1 Announce Type: new 
Abstract: Zero-shot industrial anomaly detection (ZSAD) methods typically yield coarse anomaly maps as vision transformers (ViTs) extract patch-level features only. To solve this, recent solutions attempt to predict finer anomalies using features from ZSAD, but they still struggle to recover fine-grained anomalies without missed detections, mainly due to the gap between randomly synthesized training anomalies and real ones. We observe that anomaly score maps exactly provide complementary spatial cues that are largely absent from ZSAD's image features, a fact overlooked before.
  Inspired by this, we propose an anomaly-aware refiner (AnoRefiner) that can be plugged into most ZSAD models and improve patch-level anomaly maps to the pixel level. First, we design an anomaly refinement decoder (ARD) that progressively enhances image features using anomaly score maps, reducing the reliance on synthetic anomaly data. Second, motivated by the mass production paradigm, we propose a progressive group-wise test-time training (PGT) strategy that trains ARD in each product group for the refinement process in the next group, while staying compatible with any ZSAD method.
  Experiments on the MVTec AD and VisA datasets show that AnoRefiner boosts various ZSAD models by up to a 5.2\% gain in pixel-AP metrics, which can also be directly observed in many visualizations. The code will be available at https://github.com/HUST-SLOW/AnoRefiner.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing</title>
<link>https://arxiv.org/abs/2511.22607</link>
<guid>https://arxiv.org/abs/2511.22607</guid>
<content:encoded><![CDATA[
arXiv:2511.22607v1 Announce Type: new 
Abstract: Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory</title>
<link>https://arxiv.org/abs/2511.22609</link>
<guid>https://arxiv.org/abs/2511.22609</guid>
<content:encoded><![CDATA[
arXiv:2511.22609v1 Announce Type: new 
Abstract: We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable-Drift: A Patient-Aware Latent Drift Replay Method for Stabilizing Representations in Continual Learning</title>
<link>https://arxiv.org/abs/2511.22615</link>
<guid>https://arxiv.org/abs/2511.22615</guid>
<content:encoded><![CDATA[
arXiv:2511.22615v1 Announce Type: new 
Abstract: When deep learning models are sequentially trained on new data, they tend to abruptly lose performance on previously learned tasks, a critical failure known as catastrophic forgetting. This challenge severely limits the deployment of AI in medical imaging, where models must continually adapt to data from new hospitals without compromising established diagnostic knowledge. To address this, we introduce a latent drift-guided replay method that identifies and replays samples with high representational instability. Specifically, our method quantifies this instability via latent drift, the change in a sample internal feature representation after naive domain adaptation. To ensure diversity and clinical relevance, we aggregate drift at the patient level, our memory buffer stores the per patient slices exhibiting the greatest multi-layer representation shift. Evaluated on a cross-hospital COVID-19 CT classification task using state-of-the-art CNN and Vision Transformer backbones, our method substantially reduces forgetting compared to naive fine-tuning and random replay. This work highlights latent drift as a practical and interpretable replay signal for advancing robust continual learning in real world medical settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REASONEDIT: Towards Reasoning-Enhanced Image Editing Models</title>
<link>https://arxiv.org/abs/2511.22625</link>
<guid>https://arxiv.org/abs/2511.22625</guid>
<content:encoded><![CDATA[
arXiv:2511.22625v1 Announce Type: new 
Abstract: Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoZero: Incentivizing Reasoning from Scratch on Geospatial Scenes</title>
<link>https://arxiv.org/abs/2511.22645</link>
<guid>https://arxiv.org/abs/2511.22645</guid>
<content:encoded><![CDATA[
arXiv:2511.22645v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have undergone rapid development in advancing geospatial scene understanding. Recent studies have sought to enhance the reasoning capabilities of remote sensing MLLMs, typically through cold-start training with elaborately curated chain-of-thought (CoT) data. However, this approach not only incurs substantial annotation costs but also introduces human biases that may limit the diversity of model reasoning. To address these challenges, we propose GeoZero, a framework that enables MLLMs to perform geospatial reasoning without any predefined CoT supervision. Specifically, we construct two datasets, GeoZero-Instruct and GeoZero-Hard. GeoZero-Instruct allows the model to acquire preliminary geospatial knowledge through supervised fine-tuning, while GeoZero-Hard stimulates deep reasoning during the subsequent reinforcement learning stage. Furthermore, we introduce Answer-Anchored Group Relative Policy Optimization (A$^2$GRPO), where the reasoning process is regularized by the model's own answers, encouraging diverse yet accurate thinking. Extensive experiments on multiple remote sensing vision-language benchmarks demonstrate that GeoZero not only surpasses existing state-of-the-art methods but also fosters universal emergent reasoning capabilities across diverse geospatial tasks. Code,data,and models will be publicly available at https://github.com/MiliLab/GeoZero.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architecture Decoupling Is Not All You Need For Unified Multimodal Model</title>
<link>https://arxiv.org/abs/2511.22663</link>
<guid>https://arxiv.org/abs/2511.22663</guid>
<content:encoded><![CDATA[
arXiv:2511.22663v1 Announce Type: new 
Abstract: Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22664</link>
<guid>https://arxiv.org/abs/2511.22664</guid>
<content:encoded><![CDATA[
arXiv:2511.22664v1 Announce Type: new 
Abstract: Vision-language models (VLMs), such as CLIP, have shown strong generalization under zero-shot settings, yet adapting them to downstream tasks with limited supervision remains a significant challenge. Existing multi-modal prompt learning methods typically rely on fixed, shared prompts and deterministic parameters, which limits their ability to capture instance-level variation or model uncertainty across diverse tasks and domains. To tackle this issue, we propose a novel Variational Multi-Modal Prompt Learning (VaMP) framework that enables sample-specific, uncertainty-aware prompt tuning in multi-modal representation learning. VaMP generates instance-conditioned prompts by sampling from a learned posterior distribution, allowing the model to personalize its behavior based on input content. To further enhance the integration of local and global semantics, we introduce a class-aware prior derived from the instance representation and class prototype. Building upon these, we formulate prompt tuning as variational inference over latent prompt representations and train the entire framework end-to-end through reparameterized sampling. Experiments on few-shot and domain generalization benchmarks show that VaMP achieves state-of-the-art performance, highlighting the benefits of modeling both uncertainty and task structure in our method. Project page: https://visual-ai.github.io/vamp
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A deep learning perspective on Rubens' attribution</title>
<link>https://arxiv.org/abs/2511.22667</link>
<guid>https://arxiv.org/abs/2511.22667</guid>
<content:encoded><![CDATA[
arXiv:2511.22667v1 Announce Type: new 
Abstract: This study explores the use of deep learning for the authentication and attribution of paintings, focusing on the complex case of Peter Paul Rubens and his workshop. A convolutional neural network was trained on a curated dataset of verified and comparative artworks to identify micro-level stylistic features characteristic of the master s hand. The model achieved high classification accuracy and demonstrated the potential of computational analysis to complement traditional art historical expertise, offering new insights into authorship and workshop collaboration.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield</title>
<link>https://arxiv.org/abs/2511.22677</link>
<guid>https://arxiv.org/abs/2511.22677</guid>
<content:encoded><![CDATA[
arXiv:2511.22677v1 Announce Type: new 
Abstract: Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Extreme-View Geometry in 3D Foundation Models</title>
<link>https://arxiv.org/abs/2511.22686</link>
<guid>https://arxiv.org/abs/2511.22686</guid>
<content:encoded><![CDATA[
arXiv:2511.22686v1 Announce Type: new 
Abstract: 3D foundation models (3DFMs) have recently transformed 3D vision, enabling joint prediction of depths, poses, and point maps directly from images. Yet their ability to reason under extreme, non-overlapping views remains largely unexplored. In this work, we study their internal representations and find that 3DFMs exhibit an emergent understanding of extreme-view geometry, despite never being trained for such conditions. To further enhance these capabilities, we introduce a lightweight alignment scheme that refines their internal 3D representation by tuning only a small subset of backbone bias terms, leaving all decoder heads frozen. This targeted adaptation substantially improves relative pose estimation under extreme viewpoints without degrading per-image depth or point quality. Additionally, we contribute MegaUnScene, a new benchmark of Internet scenes unseen by existing 3DFMs, with dedicated test splits for both relative pose estimation and dense 3D reconstruction. All code and data will be released.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ar2Can: An Architect and an Artist Leveraging a Canvas for Multi-Human Generation</title>
<link>https://arxiv.org/abs/2511.22690</link>
<guid>https://arxiv.org/abs/2511.22690</guid>
<content:encoded><![CDATA[
arXiv:2511.22690v1 Announce Type: new 
Abstract: Despite recent advances in text-to-image generation, existing models consistently fail to produce reliable multi-human scenes, often duplicating faces, merging identities, or miscounting individuals. We present Ar2Can, a novel two-stage framework that disentangles spatial planning from identity rendering for multi-human generation. The Architect module predicts structured layouts, specifying where each person should appear. The Artist module then synthesizes photorealistic images, guided by a spatially-grounded face matching reward that combines Hungarian spatial alignment with ArcFace identity similarity. This approach ensures faces are rendered at correct locations and faithfully preserve reference identities. We develop two Architect variants, seamlessly integrated with our diffusion-based Artist model and optimized via Group Relative Policy Optimization (GRPO) using compositional rewards for count accuracy, image quality, and identity matching. Evaluated on the MultiHuman-Testbench, Ar2Can achieves substantial improvements in both count accuracy and identity preservation, while maintaining high perceptual quality. Notably, our method achieves these results using primarily synthetic data, without requiring real multi-human images.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</title>
<link>https://arxiv.org/abs/2511.22699</link>
<guid>https://arxiv.org/abs/2511.22699</guid>
<content:encoded><![CDATA[
arXiv:2511.22699v1 Announce Type: new 
Abstract: The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the "scale-at-all-costs" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Splat-SAP: Feed-Forward Gaussian Splatting for Human-Centered Scene with Scale-Aware Point Map Reconstruction</title>
<link>https://arxiv.org/abs/2511.22704</link>
<guid>https://arxiv.org/abs/2511.22704</guid>
<content:encoded><![CDATA[
arXiv:2511.22704v1 Announce Type: new 
Abstract: We present Splat-SAP, a feed-forward approach to render novel views of human-centered scenes from binocular cameras with large sparsity. Gaussian Splatting has shown its promising potential in rendering tasks, but it typically necessitates per-scene optimization with dense input views. Although some recent approaches achieve feed-forward Gaussian Splatting rendering through geometry priors obtained by multi-view stereo, such approaches still require largely overlapped input views to establish the geometry prior. To bridge this gap, we leverage pixel-wise point map reconstruction to represent geometry which is robust to large sparsity for its independent view modeling. In general, we propose a two-stage learning strategy. In stage 1, we transform the point map into real space via an iterative affinity learning process, which facilitates camera control in the following. In stage 2, we project point maps of two input views onto the target view plane and refine such geometry via stereo matching. Furthermore, we anchor Gaussian primitives on this refined plane in order to render high-quality images. As a metric representation, the scale-aware point map in stage 1 is trained in a self-supervised manner without 3D supervision and stage 2 is supervised with photo-metric loss. We collect multi-view human-centered data and demonstrate that our method improves both the stability of point map reconstruction and the visual quality of free-viewpoint rendering.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.22715</link>
<guid>https://arxiv.org/abs/2511.22715</guid>
<content:encoded><![CDATA[
arXiv:2511.22715v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All Centers Are at most a Few Tokens Apart: Knowledge Distillation with Domain Invariant Prompt Tuning</title>
<link>https://arxiv.org/abs/2511.22739</link>
<guid>https://arxiv.org/abs/2511.22739</guid>
<content:encoded><![CDATA[
arXiv:2511.22739v1 Announce Type: new 
Abstract: Domain generalization is critical in computational pathology (CPath) due to inherent domain shifts caused by variations in staining protocols, scanner devices, and imaging settings across clinical centers. Vision-language models (VLMs), such as PLIP-a pathology-tuned CLIP-trained on image-text pairs across diverse domains, serve as strong knowledge distillation sources. However, their zero-shot performance with predefined prompts remains limited due to sensitivity to prompt variations. Moreover, unlike natural images, histopathology centers lack semantic descriptors (e.g., 'sketch'), making it difficult to define domain-specific prompts for clinical centers. This requires a data-driven approach for learning domain-specific and ultimately class-generic continuous prompts. We propose Domain Invariant Prompt Tuning (DIPT) for knowledge distillation process, a novel step that learns multiple input tokens for each domain. These tokens are trained separately for each domain and are averaged across domains, leading to domain-invariant prompts. Our student model then distills knowledge from PLIP's text encoder by leveraging the prompts learned by DIPT. This leads to alignment of visual features with domain-invariant embeddings, enhancing generalization by training on multiple domains. Our method adds a significant improvement in average F1-score to existing state-of-the-art (SOTA) knowledge distillation approaches in domain generalization with histopathology datasets. This work helps the way of deploying robust CPath models in real-world clinical problems with heterogeneous data sources.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MammoRGB: Dual-View Mammogram Synthesis Using Denoising Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2511.22759</link>
<guid>https://arxiv.org/abs/2511.22759</guid>
<content:encoded><![CDATA[
arXiv:2511.22759v1 Announce Type: new 
Abstract: Purpose: This study aims to develop and evaluate a three channel denoising diffusion probabilistic model (DDPM) for synthesizing single breast dual view mammograms and to assess the impact of channel representations on image fidelity and cross view consistency. Materials and Methods: A pretrained three channel DDPM, sourced from Hugging Face, was fine tuned on a private dataset of 11020 screening mammograms to generate paired craniocaudal (CC) and mediolateral oblique (MLO) views. Three third channel encodings of the CC and MLO views were evaluated: sum, absolute difference, and zero channel. Each model produced 500 synthetic image pairs. Quantitative assessment involved breast mask segmentation using Intersection over Union (IoU) and Dice Similarity Coefficient (DSC), with distributional comparisons against 2500 real pairs using Earth Movers Distance (EMD) and Kolmogorov Smirnov (KS) tests. Qualitative evaluation included a visual Turing test by a non expert radiologist to assess cross view consistency and artifacts. Results: Synthetic mammograms showed IoU and DSC distributions comparable to real images, with EMD and KS values (0.020 and 0.077 respectively). Models using sum or absolute difference encodings outperformed others in IoU and DSC (p < 0.001), though distributions remained broadly similar. Generated CC and MLO views maintained cross view consistency, with 6 to 8 percent of synthetic images exhibiting artifacts consistent with those in the training data. Conclusion: Three channel DDPMs can generate realistic and anatomically consistent dual view mammograms with promising applications in dataset augmentation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusion or Confusion? Assessing the impact of visible-thermal image fusion for automated wildlife detection</title>
<link>https://arxiv.org/abs/2511.22768</link>
<guid>https://arxiv.org/abs/2511.22768</guid>
<content:encoded><![CDATA[
arXiv:2511.22768v1 Announce Type: new 
Abstract: Efficient wildlife monitoring methods are necessary for biodiversity conservation and management. The combination of remote sensing, aerial imagery and deep learning offer promising opportunities to renew or improve existing survey methods. The complementary use of visible (VIS) and thermal infrared (TIR) imagery can add information compared to a single-source image and improve results in an automated detection context. However, the alignment and fusion process can be challenging, especially since visible and thermal images usually have different fields of view (FOV) and spatial resolutions. This research presents a case study on the great blue heron (Ardea herodias) to evaluate the performances of synchronous aerial VIS and TIR imagery to automatically detect individuals and nests using a YOLO11n model. Two VIS-TIR fusion methods were tested and compared: an early fusion approach and a late fusion approach, to determine if the addition of the TIR image gives any added value compared to a VIS-only model. VIS and TIR images were automatically aligned using a deep learning model. A principal component analysis fusion method was applied to VIS-TIR image pairs to form the early fusion dataset. A classification and regression tree was used to process the late fusion dataset, based on the detection from the VIS-only and TIR-only trained models. Across all classes, both late and early fusion improved the F1 score compared to the VIS-only model. For the main class, occupied nest, the late fusion improved the F1 score from 90.2 (VIS-only) to 93.0%. This model was also able to identify false positives from both sources with 90% recall. Although fusion methods seem to give better results, this approach comes with a limiting TIR FOV and alignment constraints that eliminate data. Using an aircraft-mounted very high-resolution visible sensor could be an interesting option for operationalizing surveys.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alzheimer's Disease Prediction Using EffNetViTLoRA and BiLSTM with Multimodal Longitudinal MRI Data</title>
<link>https://arxiv.org/abs/2511.22774</link>
<guid>https://arxiv.org/abs/2511.22774</guid>
<content:encoded><![CDATA[
arXiv:2511.22774v1 Announce Type: new 
Abstract: Alzheimer's disease (AD) is a prevalent neurodegenerative disorder that progressively impairs memory, decision-making, and overall cognitive function. As AD is irreversible, early prediction is critical for timely intervention and management. Mild Cognitive Impairment (MCI), a transitional stage between cognitively normal (CN) aging and AD, plays a significant role in early AD diagnosis. However, predicting MCI progression remains a significant challenge, as not all individuals with MCI convert to AD. MCI subjects are categorized into stable MCI (sMCI) and progressive MCI (pMCI) based on conversion status. In this study, we propose a generalized, end-to-end deep learning model for AD prediction using MCI cases from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our hybrid architecture integrates Convolutional Neural Networks and Vision Transformers to capture both local spatial features and global contextual dependencies from Magnetic Resonance Imaging (MRI) scans. To incorporate temporal progression, we further employ Bidirectional Long Short-Term Memory (BiLSTM) networks to process features extracted from four consecutive MRI timepoints along with some other non-image biomarkers, predicting each subject's cognitive status at month 48. Our multimodal model achieved an average progression prediction accuracy of 95.05\% between sMCI and pMCI, outperforming existing studies in AD prediction. This work demonstrates state-of-the-art performance in longitudinal AD prediction and highlights the effectiveness of combining spatial and temporal modeling for the early detection of Alzheimer's disease.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22787</link>
<guid>https://arxiv.org/abs/2511.22787</guid>
<content:encoded><![CDATA[
arXiv:2511.22787v1 Announce Type: new 
Abstract: In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images</title>
<link>https://arxiv.org/abs/2511.22805</link>
<guid>https://arxiv.org/abs/2511.22805</guid>
<content:encoded><![CDATA[
arXiv:2511.22805v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LC4-DViT: Land-cover Creation for Land-cover Classification with Deformable Vision Transformer</title>
<link>https://arxiv.org/abs/2511.22812</link>
<guid>https://arxiv.org/abs/2511.22812</guid>
<content:encoded><![CDATA[
arXiv:2511.22812v1 Announce Type: new 
Abstract: Land-cover underpins ecosystem services, hydrologic regulation, disaster-risk reduction, and evidence-based land planning; timely, accurate land-cover maps are therefore critical for environmental stewardship. Remote sensing-based land-cover classification offers a scalable route to such maps but is hindered by scarce and imbalanced annotations and by geometric distortions in high-resolution scenes. We propose LC4-DViT (Land-cover Creation for Land-cover Classification with Deformable Vision Transformer), a framework that combines generative data creation with a deformation-aware Vision Transformer. A text-guided diffusion pipeline uses GPT-4o-generated scene descriptions and super-resolved exemplars to synthesize class-balanced, high-fidelity training images, while DViT couples a DCNv4 deformable convolutional backbone with a Vision Transformer encoder to jointly capture fine-scale geometry and global context. On eight classes from the Aerial Image Dataset (AID)-Beach, Bridge, Desert, Forest, Mountain, Pond, Port, and River-DViT achieves 0.9572 overall accuracy, 0.9576 macro F1-score, and 0.9510 Cohen' s Kappa, improving over a vanilla ViT baseline (0.9274 OA, 0.9300 macro F1, 0.9169 Kappa) and outperforming ResNet50, MobileNetV2, and FlashInternImage. Cross-dataset experiments on a three-class SIRI-WHU subset (Harbor, Pond, River) yield 0.9333 overall accuracy, 0.9316 macro F1, and 0.8989 Kappa, indicating good transferability. An LLM-based judge using GPT-4o to score Grad-CAM heatmaps further shows that DViT' s attention aligns best with hydrologically meaningful structures. These results suggest that description-driven generative augmentation combined with deformation-aware transformers is a promising approach for high-resolution land-cover mapping.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Captain Safari: A World Engine</title>
<link>https://arxiv.org/abs/2511.22815</link>
<guid>https://arxiv.org/abs/2511.22815</guid>
<content:encoded><![CDATA[
arXiv:2511.22815v1 Announce Type: new 
Abstract: World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs</title>
<link>https://arxiv.org/abs/2511.22826</link>
<guid>https://arxiv.org/abs/2511.22826</guid>
<content:encoded><![CDATA[
arXiv:2511.22826v1 Announce Type: new 
Abstract: Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Visual Shortcuts in Multimodal Knowledge-Based Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.22843</link>
<guid>https://arxiv.org/abs/2511.22843</guid>
<content:encoded><![CDATA[
arXiv:2511.22843v1 Announce Type: new 
Abstract: Existing Multimodal Knowledge-Based Visual Question Answering (MKB-VQA) benchmarks suffer from "visual shortcuts", as the query image typically matches the primary subject entity of the target document. We demonstrate that models can exploit these shortcuts, achieving comparable results using visual cues alone. To address this, we introduce Relational Entity Text-Image kNowledge Augmented (RETINA) benchmark, automatically constructed using an LLM-driven pipeline, consisting of 120k training and 2k human-curated test set. RETINA contains queries referencing secondary subjects (i.e. related entities) and pairs them with images of these related entities, removing the visual shortcut. When evaluated on RETINA existing models show significantly degraded performance, confirming their reliance on the shortcut. Furthermore, we propose Multi-Image MultImodal Retriever (MIMIR), which enriches document embeddings by augmenting images of multiple related entities, effectively handling RETINA, unlike prior work that uses only a single image per document. Our experiments validate the limitations of existing benchmarks and demonstrate the effectiveness of RETINA and MIMIR. Our project is available at: Project Page.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding</title>
<link>https://arxiv.org/abs/2511.22850</link>
<guid>https://arxiv.org/abs/2511.22850</guid>
<content:encoded><![CDATA[
arXiv:2511.22850v1 Announce Type: new 
Abstract: Document understanding is a long standing practical task. Vision Language Models (VLMs) have gradually become a primary approach in this domain, demonstrating effective performance on single page tasks. However, their effectiveness diminishes when handling long documents. In such scenarios, clues are often scattered across multiple pages and modalities, and redundancy from lengthy inputs can impair the models judgment. While retrieval augmented generation mitigates this issue by filtering for question relevant content, the retrieved results still contain substantial redundancy. To address these limitations, we propose SLEUTH, a multi agent framework. Concretely, SLEUTH orchestrates a retriever and four collaborative agents in a coarse to fine process. The framework identifies key textual and visual clues within the retrieved pages, filters for salient visual evidence such as tables and charts, and analyzes the query to devise a reasoning strategy. It ultimately synthesizes a distilled, evidence dense multimodal context to generate the final prediction. SLEUTH is model agnostic and scalable. When paired with advanced VLM backbones, it consistently improves performance on multiple long document benchmarks, achieving state of the art results. Ablation studies verify each modules effectiveness and confirm the benefits of our hierarchical refinement paradigm.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light &amp; Camera</title>
<link>https://arxiv.org/abs/2511.22857</link>
<guid>https://arxiv.org/abs/2511.22857</guid>
<content:encoded><![CDATA[
arXiv:2511.22857v1 Announce Type: new 
Abstract: Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoordSpeaker: Exploiting Gesture Captioning for Coordinated Caption-Empowered Co-Speech Gesture Generation</title>
<link>https://arxiv.org/abs/2511.22863</link>
<guid>https://arxiv.org/abs/2511.22863</guid>
<content:encoded><![CDATA[
arXiv:2511.22863v1 Announce Type: new 
Abstract: Co-speech gesture generation has significantly advanced human-computer interaction, yet speaker movements remain constrained due to the omission of text-driven non-spontaneous gestures (e.g., bowing while talking). Existing methods face two key challenges: 1) the semantic prior gap due to the lack of descriptive text annotations in gesture datasets, and 2) the difficulty in achieving coordinated multimodal control over gesture generation. To address these challenges, this paper introduces CoordSpeaker, a comprehensive framework that enables coordinated caption-empowered co-speech gesture synthesis. Our approach first bridges the semantic prior gap through a novel gesture captioning framework, leveraging a motion-language model to generate descriptive captions at multiple granularities. Building upon this, we propose a conditional latent diffusion model with unified cross-dataset motion representation and a hierarchically controlled denoiser to achieve highly controlled, coordinated gesture generation. CoordSpeaker pioneers the first exploration of gesture understanding and captioning to tackle the semantic gap in gesture generation while offering a novel perspective of bidirectional gesture-text mapping. Extensive experiments demonstrate that our method produces high-quality gestures that are both rhythmically synchronized with speeches and semantically coherent with arbitrary captions, achieving superior performance with higher efficiency compared to existing approaches.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Diffusion Transformer for Conditional 4D fMRI Synthesis</title>
<link>https://arxiv.org/abs/2511.22870</link>
<guid>https://arxiv.org/abs/2511.22870</guid>
<content:encoded><![CDATA[
arXiv:2511.22870v1 Announce Type: new 
Abstract: Generating whole-brain 4D fMRI sequences conditioned on cognitive tasks remains challenging due to the high-dimensional, heterogeneous BOLD dynamics across subjects/acquisitions and the lack of neuroscience-grounded validation. We introduce the first diffusion transformer for voxelwise 4D fMRI conditional generation, combining 3D VQ-GAN latent compression with a CNN-Transformer backbone and strong task conditioning via AdaLN-Zero and cross-attention. On HCP task fMRI, our model reproduces task-evoked activation maps, preserves the inter-task representational structure observed in real data (RSA), achieves perfect condition specificity, and aligns ROI time-courses with canonical hemodynamic responses. Performance improves predictably with scale, reaching task-evoked map correlation of 0.83 and RSA of 0.98, consistently surpassing a U-Net baseline on all metrics. By coupling latent diffusion with a scalable backbone and strong conditioning, this work establishes a practical path to conditional 4D fMRI synthesis, paving the way for future applications such as virtual experiments, cross-site harmonization, and principled augmentation for downstream neuroimaging models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNN-Based Framework for Pedestrian Age and Gender Classification Using Far-View Surveillance in Mixed-Traffic Intersections</title>
<link>https://arxiv.org/abs/2511.22873</link>
<guid>https://arxiv.org/abs/2511.22873</guid>
<content:encoded><![CDATA[
arXiv:2511.22873v1 Announce Type: new 
Abstract: Pedestrian safety remains a pressing concern in congested urban intersections, particularly in low- and middle-income countries where traffic is multimodal, and infrastructure often lacks formal control. Demographic factors like age and gender significantly influence pedestrian vulnerability, yet real-time monitoring systems rarely capture this information. To address this gap, this study proposes a deep learning framework that classifies pedestrian age group and gender from far-view intersection footage using convolutional neural networks (CNNs), without relying on facial recognition or high-resolution imagery. The classification is structured as a unified six-class problem, distinguishing adult, teenager, and child pedestrians for both males and females, based on full-body visual cues. Video data was collected from three high-risk intersections in Dhaka, Bangladesh. Two CNN architectures were implemented: ResNet50, a deep convolutional neural network pretrained on ImageNet, and a custom lightweight CNN optimized for computational efficiency. Eight model variants explored combinations of pooling strategies and optimizers. ResNet50 with Max Pooling and SGD achieved the highest accuracy (86.19%), while the custom CNN performed comparably (84.15%) with fewer parameters and faster training. The model's efficient design enables real-time inference on standard surveillance feeds. For practitioners, this system provides a scalable, cost-effective tool to monitor pedestrian demographics at intersections using existing camera infrastructure. Its outputs can shape intersection design, optimize signal timing, and enable targeted safety interventions for vulnerable groups such as children or the elderly. By offering demographic insights often missing in conventional traffic data, the framework supports more inclusive, data-driven planning in mixed-traffic environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClearGCD: Mitigating Shortcut Learning For Robust Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2511.22892</link>
<guid>https://arxiv.org/abs/2511.22892</guid>
<content:encoded><![CDATA[
arXiv:2511.22892v1 Announce Type: new 
Abstract: In open-world scenarios, Generalized Category Discovery (GCD) requires identifying both known and novel categories within unlabeled data. However, existing methods often suffer from prototype confusion caused by shortcut learning, which undermines generalization and leads to forgetting of known classes. We propose ClearGCD, a framework designed to mitigate reliance on non-semantic cues through two complementary mechanisms. First, Semantic View Alignment (SVA) generates strong augmentations via cross-class patch replacement and enforces semantic consistency using weak augmentations. Second, Shortcut Suppression Regularization (SSR) maintains an adaptive prototype bank that aligns known classes while encouraging separation of potential novel ones. ClearGCD can be seamlessly integrated into parametric GCD approaches and consistently outperforms state-of-the-art methods across multiple benchmarks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DM$^3$T: Harmonizing Modalities via Diffusion for Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2511.22896</link>
<guid>https://arxiv.org/abs/2511.22896</guid>
<content:encoded><![CDATA[
arXiv:2511.22896v1 Announce Type: new 
Abstract: Multi-object tracking (MOT) is a fundamental task in computer vision with critical applications in autonomous driving and robotics. Multimodal MOT that integrates visible light and thermal infrared information is particularly essential for robust autonomous driving systems. However, effectively fusing these heterogeneous modalities is challenging. Simple strategies like concatenation or addition often fail to bridge the significant non-linear distribution gap between their feature representations, which can lead to modality conflicts and degrade tracking accuracy. Drawing inspiration from the connection between multimodal MOT and the iterative refinement in diffusion models, this paper proposes DM$^3$T, a novel framework that reformulates multimodal fusion as an iterative feature alignment process to generate accurate and temporally coherent object trajectories. Our approach performs iterative cross-modal harmonization through a proposed Cross-Modal Diffusion Fusion (C-MDF) module. In this process, features from both modalities provide mutual guidance, iteratively projecting them onto a shared, consistent feature manifold. This enables the learning of complementary information and achieves deeper fusion compared to conventional methods. Additionally, we introduce a plug-and-play Diffusion Refiner (DR) to enhance and refine the unified feature representation. To further improve tracking robustness, we design a Hierarchical Tracker that adaptively handles confidence estimation. DM$^3$T unifies object detection, state estimation, and data association into a comprehensive online tracking framework without complex post-processing. Extensive experiments on the VT-MOT benchmark demonstrate that our method achieves 41.7 HOTA, representing a 1.54% relative improvement over existing state-of-the-art methods. The code and models are available at https://vranlee.github.io/DM-3-T/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Points to Clouds: Learning Robust Semantic Distributions for Multi-modal Prompts</title>
<link>https://arxiv.org/abs/2511.22897</link>
<guid>https://arxiv.org/abs/2511.22897</guid>
<content:encoded><![CDATA[
arXiv:2511.22897v1 Announce Type: new 
Abstract: Multimodal Prompt Learning (MPL) has emerged as a pivotal technique for adapting large-scale Visual Language Models (VLMs). However, current MPL methods are fundamentally limited by their optimization of a single, static point representation. This paradigm is inherently brittle, leads to overfitting on base classes, and generalizes poorly to novel or ambiguous categories. We challenge this point paradigm, proposing that robust generalization requires learning a semantic cloud (i.e., a distribution over the embedding space). To achieve this, we introduce Points-to-Clouds (P2C), a novel framework inspired by diffusion models that reframes prompt learning as a dynamic denoising task. At the core of P2C is a dual denoising mechanism: a Dynamic Prompt Denoising (DPD) mechanism perturbs text prompts with sophisticated, annealed noise to learn a smoother semantic landscape, while an auxiliary V-L Mapper denoising loss re-tasks the mapper as a denoising autoencoder. This forces the mapper to reconstruct clean visual prompts from noisy text inputs, ensuring robust cross-modal alignment. Extensive experiments across 11 datasets demonstrate that P2C consistently outperforms strong baselines. On the base-to-novel generalization benchmark, our method achieves a Harmonic Mean of 79.7%, representing a relative improvement of 1.4% over the baseline. The code and models are available at https://vranlee.github.io/P2C/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Textual Compositional Reasoning for Robust Change Captioning</title>
<link>https://arxiv.org/abs/2511.22903</link>
<guid>https://arxiv.org/abs/2511.22903</guid>
<content:encoded><![CDATA[
arXiv:2511.22903v1 Announce Type: new 
Abstract: Change captioning aims to describe changes between a pair of images. However, existing works rely on visual features alone, which often fail to capture subtle but meaningful changes because they lack the ability to represent explicitly structured information such as object relationships and compositional semantics. To alleviate this, we present CORTEX (COmpositional Reasoning-aware TEXt-guided), a novel framework that integrates complementary textual cues to enhance change understanding. In addition to capturing cues from pixel-level differences, CORTEX utilizes scene-level textual knowledge provided by Vision Language Models (VLMs) to extract richer image text signals that reveal underlying compositional reasoning. CORTEX consists of three key modules: (i) an Image-level Change Detector that identifies low-level visual differences between paired images, (ii) a Reasoning-aware Text Extraction (RTE) module that use VLMs to generate compositional reasoning descriptions implicit in visual features, and (iii) an Image-Text Dual Alignment (ITDA) module that aligns visual and textual features for fine-grained relational reasoning. This enables CORTEX to reason over visual and textual features and capture changes that are otherwise ambiguous in visual features alone.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See, Rank, and Filter: Important Word-Aware Clip Filtering via Scene Understanding for Moment Retrieval and Highlight Detection</title>
<link>https://arxiv.org/abs/2511.22906</link>
<guid>https://arxiv.org/abs/2511.22906</guid>
<content:encoded><![CDATA[
arXiv:2511.22906v1 Announce Type: new 
Abstract: Video moment retrieval (MR) and highlight detection (HD) with natural language queries aim to localize relevant moments and key highlights in a video clips. However, existing methods overlook the importance of individual words, treating the entire text query and video clips as a black-box, which hinders contextual understanding. In this paper, we propose a novel approach that enables fine-grained clip filtering by identifying and prioritizing important words in the query. Our method integrates image-text scene understanding through Multimodal Large Language Models (MLLMs) and enhances the semantic understanding of video clips. We introduce a feature enhancement module (FEM) to capture important words from the query and a ranking-based filtering module (RFM) to iteratively refine video clips based on their relevance to these important words. Extensive experiments demonstrate that our approach significantly outperforms existing state-of-the-art methods, achieving superior performance in both MR and HD tasks. Our code is available at: https://github.com/VisualAIKHU/SRF.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViGG: Robust RGB-D Point Cloud Registration using Visual-Geometric Mutual Guidance</title>
<link>https://arxiv.org/abs/2511.22908</link>
<guid>https://arxiv.org/abs/2511.22908</guid>
<content:encoded><![CDATA[
arXiv:2511.22908v1 Announce Type: new 
Abstract: Point cloud registration is a fundamental task in 3D vision. Most existing methods only use geometric information for registration. Recently proposed RGB-D registration methods primarily focus on feature fusion or improving feature learning, which limits their ability to exploit image information and hinders their practical applicability. In this paper, we propose ViGG, a robust RGB-D registration method using mutual guidance. First, we solve clique alignment in a visual-geometric combination form, employing a geometric guidance design to suppress ambiguous cliques. Second, to mitigate accuracy degradation caused by noise in visual matches, we propose a visual-guided geometric matching method that utilizes visual priors to determine the search space, enabling the extraction of high-quality, noise-insensitive correspondences. This mutual guidance strategy brings our method superior robustness, making it applicable for various RGB-D registration tasks. The experiments on 3DMatch, ScanNet and KITTI datasets show that our method outperforms recent state-of-the-art methods in both learning-free and learning-based settings. Code is available at https://github.com/ccjccjccj/ViGG.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artwork Interpretation with Vision Language Models: A Case Study on Emotions and Emotion Symbols</title>
<link>https://arxiv.org/abs/2511.22929</link>
<guid>https://arxiv.org/abs/2511.22929</guid>
<content:encoded><![CDATA[
arXiv:2511.22929v1 Announce Type: new 
Abstract: Emotions are a fundamental aspect of artistic expression. Due to their abstract nature, there is a broad spectrum of emotion realization in artworks. These are subject to historical change and their analysis requires expertise in art history. In this article, we investigate which aspects of emotional expression can be detected by current (2025) vision language models (VLMs). We present a case study of three VLMs (Llava-Llama and two Qwen models) in which we ask these models four sets of questions of increasing complexity about artworks (general content, emotional content, expression of emotions, and emotion symbols) and carry out a qualitative expert evaluation. We find that the VLMs recognize the content of the images surprisingly well and often also which emotions they depict and how they are expressed. The models perform best for concrete images but fail for highly abstract or highly symbolic images. Reliable recognition of symbols remains fundamentally difficult. Furthermore, the models continue to exhibit the well-known LLM weakness of providing inconsistent answers to related questions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuMatC: A General Neural Framework for Fast Parametric Matrix Operation</title>
<link>https://arxiv.org/abs/2511.22934</link>
<guid>https://arxiv.org/abs/2511.22934</guid>
<content:encoded><![CDATA[
arXiv:2511.22934v1 Announce Type: new 
Abstract: Matrix operations (e.g., inversion and singular value decomposition (SVD)) are fundamental in science and engineering. In many emerging real-world applications (such as wireless communication and signal processing), these operations must be performed repeatedly over matrices with parameters varying continuously. However, conventional methods tackle each matrix operation independently, underexploring the inherent low-rankness and continuity along the parameter dimension, resulting in significantly redundant computation. To address this challenge, we propose \textbf{\textit{Neural Matrix Computation Framework} (NeuMatC)}, which elegantly tackles general parametric matrix operation tasks by leveraging the underlying low-rankness and continuity along the parameter dimension. Specifically, NeuMatC unsupervisedly learns a low-rank and continuous mapping from parameters to their corresponding matrix operation results. Once trained, NeuMatC enables efficient computations at arbitrary parameters using only a few basic operations (e.g., matrix multiplications and nonlinear activations), significantly reducing redundant computations. Experimental results on both synthetic and real-world datasets demonstrate the promising performance of NeuMatC, exemplified by over $3\times$ speedup in parametric inversion and $10\times$ speedup in parametric SVD compared to the widely used NumPy baseline in wireless communication, while maintaining acceptable accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Image Self-Recovery against Tampering using Watermark Generation with Pixel Shuffling</title>
<link>https://arxiv.org/abs/2511.22936</link>
<guid>https://arxiv.org/abs/2511.22936</guid>
<content:encoded><![CDATA[
arXiv:2511.22936v1 Announce Type: new 
Abstract: The rapid growth of Artificial Intelligence-Generated Content (AIGC) raises concerns about the authenticity of digital media. In this context, image self-recovery, reconstructing original content from its manipulated version, offers a practical solution for understanding the attacker's intent and restoring trustworthy data. However, existing methods often fail to accurately recover tampered regions, falling short of the primary goal of self-recovery. To address this challenge, we propose ReImage, a neural watermarking-based self-recovery framework that embeds a shuffled version of the target image into itself as a watermark. We design a generator that produces watermarks optimized for neural watermarking and introduce an image enhancement module to refine the recovered image. We further analyze and resolve key limitations of shuffled watermarking, enabling its effective use in self-recovery. We demonstrate that ReImage achieves state-of-the-art performance across diverse tampering scenarios, consistently producing high-quality recovered images. The code and pretrained models will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Barcode and QR Code Object Detection: An Experimental Study on YOLOv8 Models</title>
<link>https://arxiv.org/abs/2511.22937</link>
<guid>https://arxiv.org/abs/2511.22937</guid>
<content:encoded><![CDATA[
arXiv:2511.22937v1 Announce Type: new 
Abstract: This research work dives into an in-depth evaluation of the YOLOv8 (You Only Look Once) algorithm's efficiency in object detection, specially focusing on Barcode and QR code recognition. Utilizing the real-time detection abilities of YOLOv8, we performed a study aimed at enhancing its talent in swiftly and correctly figuring out objects. Through large training and high-quality-tuning on Kaggle datasets tailored for Barcode and QR code detection, our goal became to optimize YOLOv8's overall performance throughout numerous situations and environments. The look encompasses the assessment of YOLOv8 throughout special version iterations: Nano, Small, and Medium, with a meticulous attention on precision, recall, and F1 assessment metrics. The consequences exhibit large improvements in object detection accuracy with every subsequent model refinement. Specifically, we achieved an accuracy of 88.95% for the nano model, 97.10% for the small model, and 94.10% for the medium version, showcasing the incremental improvements finished via model scaling. Our findings highlight the big strides made through YOLOv8 in pushing the limits of computer vision, ensuring its function as a milestone within the subject of object detection. This study sheds light on how model scaling affects object recognition, increasing the concept of deep learning-based computer creative and prescient techniques.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenoiseGS: Gaussian Reconstruction Model for Burst Denoising</title>
<link>https://arxiv.org/abs/2511.22939</link>
<guid>https://arxiv.org/abs/2511.22939</guid>
<content:encoded><![CDATA[
arXiv:2511.22939v1 Announce Type: new 
Abstract: Burst denoising methods are crucial for enhancing images captured on handheld devices, but they often struggle with large motion or suffer from prohibitive computational costs. In this paper, we propose DenoiseGS, the first framework to leverage the efficiency of 3D Gaussian Splatting for burst denoising. Our approach addresses two key challenges when applying feedforward Gaussian reconsturction model to noisy inputs: the degradation of Gaussian point clouds and the loss of fine details. To this end, we propose a Gaussian self-consistency (GSC) loss, which regularizes the geometry predicted from noisy inputs with high-quality Gaussian point clouds. These point clouds are generated from clean inputs by the same model that we are training, thereby alleviating potential bias or domain gaps. Additionally, we introduce a log-weighted frequency (LWF) loss to strengthen supervision within the spectral domain, effectively preserving fine-grained details. The LWF loss adaptively weights frequency discrepancies in a logarithmic manner, emphasizing challenging high-frequency details. Extensive experiments demonstrate that DenoiseGS significantly exceeds the state-of-the-art NeRF-based methods on both burst denoising and novel view synthesis under noisy conditions, while achieving \textbf{250$\times$} faster inference speed. Code and models are released at https://github.com/yscheng04/DenoiseGS.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfe</title>
<link>https://arxiv.org/abs/2511.22940</link>
<guid>https://arxiv.org/abs/2511.22940</guid>
<content:encoded><![CDATA[
arXiv:2511.22940v1 Announce Type: new 
Abstract: Recent advances in diffusion models have greatly improved pose-driven character animation. However, existing methods are limited to spatially aligned reference-pose pairs with matched skeletal structures. Handling reference-pose misalignment remains unsolved. To address this, we present One-to-All Animation, a unified framework for high-fidelity character animation and image pose transfer for references with arbitrary layouts. First, to handle spatially misaligned reference, we reformulate training as a self-supervised outpainting task that transforms diverse-layout reference into a unified occluded-input format. Second, to process partially visible reference, we design a reference extractor for comprehensive identity feature extraction. Further, we integrate hybrid reference fusion attention to handle varying resolutions and dynamic sequence lengths. Finally, from the perspective of generation quality, we introduce identity-robust pose control that decouples appearance from skeletal structure to mitigate pose overfitting, and a token replace strategy for coherent long-video generation. Extensive experiments show that our method outperforms existing approaches. The code and model will be available at https://github.com/ssj9596/One-to-All-Animation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do We Need Perfect Data? Leveraging Noise for Domain Generalized Segmentation</title>
<link>https://arxiv.org/abs/2511.22948</link>
<guid>https://arxiv.org/abs/2511.22948</guid>
<content:encoded><![CDATA[
arXiv:2511.22948v1 Announce Type: new 
Abstract: Domain generalization in semantic segmentation faces challenges from domain shifts, particularly under adverse conditions. While diffusion-based data generation methods show promise, they introduce inherent misalignment between generated images and semantic masks. This paper presents FLEX-Seg (FLexible Edge eXploitation for Segmentation), a framework that transforms this limitation into an opportunity for robust learning. FLEX-Seg comprises three key components: (1) Granular Adaptive Prototypes that captures boundary characteristics across multiple scales, (2) Uncertainty Boundary Emphasis that dynamically adjusts learning emphasis based on prediction entropy, and (3) Hardness-Aware Sampling that progressively focuses on challenging examples. By leveraging inherent misalignment rather than enforcing strict alignment, FLEX-Seg learns robust representations while capturing rich stylistic variations. Experiments across five real-world datasets demonstrate consistent improvements over state-of-the-art methods, achieving 2.44% and 2.63% mIoU gains on ACDC and Dark Zurich. Our findings validate that adaptive strategies for handling imperfect synthetic data lead to superior domain generalization. Code is available at https://github.com/VisualScienceLab-KHU/FLEX-Seg.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video</title>
<link>https://arxiv.org/abs/2511.22950</link>
<guid>https://arxiv.org/abs/2511.22950</guid>
<content:encoded><![CDATA[
arXiv:2511.22950v1 Announce Type: new 
Abstract: Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Heliophysical Image Pretraining for Solar Dynamics Observatory Records</title>
<link>https://arxiv.org/abs/2511.22958</link>
<guid>https://arxiv.org/abs/2511.22958</guid>
<content:encoded><![CDATA[
arXiv:2511.22958v1 Announce Type: new 
Abstract: Deep learning has revolutionized solar image analysis, yet most approaches train task-specific encoders from scratch or rely on natural-image pretraining that ignores the unique characteristics of Solar Dynamics Observatory (SDO) data. We introduce SolarCHIP, a family of contrastively pretrained visual backbones tailored to multi-instrument SDO observations. SolarCHIP addresses three key challenges in solar imaging: multimodal sensing across AIA and HMI instruments, weak inter-class separability due to slow temporal evolution, and strong intra-class variability with sparse activity signals. Our pretraining framework employs a multi-granularity contrastive objective that jointly aligns (1) global class tokens across co-temporal AIA-HMI pairs to enhance temporal discrimination, (2) local patch tokens at fixed spatial indices to enforce position-consistent, modality-invariant features, and (3) intra-sample patches across different spatial locations to preserve fine-grained spatial structure. We train both CNN- and Vision Transformer-based autoencoders and demonstrate their effectiveness on two downstream tasks: cross-modal translation between HMI and AIA passbands via ControlNet, and full-disk flare classification. Experimental results show that SolarCHIP achieves state-of-the-art performance across both tasks, with particularly strong gains in low-resource settings where labeled data is limited. Ablation studies confirm that each contrastive component contributes essential discriminative capacity at different granularities. By publicly releasing pretrained weights and training code, we provide the heliophysics community with a practical, plug-and-play feature extractor that reduces computational requirements, improves label efficiency, and establishes a reusable foundation for diverse solar imaging applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model</title>
<link>https://arxiv.org/abs/2511.22961</link>
<guid>https://arxiv.org/abs/2511.22961</guid>
<content:encoded><![CDATA[
arXiv:2511.22961v1 Announce Type: new 
Abstract: Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM's embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&amp;A and general 3D Q&amp;A benchmarks demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM</title>
<link>https://arxiv.org/abs/2511.22968</link>
<guid>https://arxiv.org/abs/2511.22968</guid>
<content:encoded><![CDATA[
arXiv:2511.22968v1 Announce Type: new 
Abstract: Extreme exposure degrades both the 3D map reconstruction and semantic segmentation accuracy, which is particularly detrimental to tightly-coupled systems. To achieve illumination invariance, we propose a novel semantic SLAM framework with two designs. First, the Intrinsic Appearance Normalization (IAN) module proactively disentangles the scene's intrinsic properties, such as albedo, from transient lighting. By learning a standardized, illumination-invariant appearance model, it assigns a stable and consistent color representation to each Gaussian primitive. Second, the Dynamic Radiance Balancing Loss (DRB-Loss) reactively handles frames with extreme exposure. It activates only when an image's exposure is poor, operating directly on the radiance field to guide targeted optimization. This prevents error accumulation from extreme lighting without compromising performance under normal conditions. The synergy between IAN's proactive invariance and DRB-Loss's reactive correction endows our system with unprecedented robustness. Evaluations on public datasets demonstrate state-of-the-art performance in camera tracking, map quality, and semantic and geometric accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation</title>
<link>https://arxiv.org/abs/2511.22973</link>
<guid>https://arxiv.org/abs/2511.22973</guid>
<content:encoded><![CDATA[
arXiv:2511.22973v1 Announce Type: new 
Abstract: Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning</title>
<link>https://arxiv.org/abs/2511.22974</link>
<guid>https://arxiv.org/abs/2511.22974</guid>
<content:encoded><![CDATA[
arXiv:2511.22974v1 Announce Type: new 
Abstract: Text-to-video (T2V) generation has achieved remarkable progress in producing high-quality videos aligned with textual prompts. However, aligning synthesized videos with nuanced human preference remains challenging due to the subjective and multifaceted nature of human judgment. Existing video preference alignment methods rely on costly human annotations or utilize proxy metrics to predict preference, which lacks the understanding of human preference logic. Moreover, they usually directly align T2V models with the overall preference distribution, ignoring potential conflict dimensions like motion dynamics and visual quality, which may bias models towards low-motion content. To address these issues, we present Motion-corrective alignment with Self-critic hierarchical Reasoning (McSc), a three-stage reinforcement learning framework for robust preference modeling and alignment. Firstly, Self-critic Dimensional Reasoning (ScDR) trains a generative reward model (RM) to decompose preferences into per-dimension assessments, using self-critic reasoning chains for reliable learning. Secondly, to achieve holistic video comparison, we introduce Hierarchical Comparative Reasoning (HCR) for structural multi-dimensional reasoning with hierarchical reward supervision. Finally, using RM-preferred videos, we propose Motion-corrective Direct Preference Optimization (McDPO) to optimize T2V models, while dynamically re-weighting alignment objective to mitigate bias towards low-motion content. Experiments show that McSc achieves superior performance in human preference alignment and generates videos with high-motion dynamic.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ovis-Image Technical Report</title>
<link>https://arxiv.org/abs/2511.22982</link>
<guid>https://arxiv.org/abs/2511.22982</guid>
<content:encoded><![CDATA[
arXiv:2511.22982v1 Announce Type: new 
Abstract: We introduce $\textbf{Ovis-Image}$, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convolutional Feature Noise Reduction for 2D Cardiac MR Image Segmentation</title>
<link>https://arxiv.org/abs/2511.22983</link>
<guid>https://arxiv.org/abs/2511.22983</guid>
<content:encoded><![CDATA[
arXiv:2511.22983v1 Announce Type: new 
Abstract: Noise reduction constitutes a crucial operation within Digital Signal Processing. Regrettably, it frequently remains neglected when dealing with the processing of convolutional features in segmentation networks. This oversight could trigger the butterfly effect, impairing the subsequent outcomes within the entire feature system. To complete this void, we consider convolutional features following Gaussian distributions as feature signal matrices and then present a simple and effective feature filter in this study. The proposed filter is fundamentally a low-amplitude pass filter primarily aimed at minimizing noise in feature signal inputs and is named Convolutional Feature Filter (CFF). We conducted experiments on two established 2D segmentation networks and two public cardiac MR image datasets to validate the effectiveness of the CFF, and the experimental findings demonstrated a decrease in noise within the feature signal matrices. To enable a numerical observation and analysis of this reduction, we developed a binarization equation to calculate the information entropy of feature signals.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2511.22989</link>
<guid>https://arxiv.org/abs/2511.22989</guid>
<content:encoded><![CDATA[
arXiv:2511.22989v1 Announce Type: new 
Abstract: Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. However, the existing benchmark datasets often focus on the generation with single or a few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as "what to edit" or "how many references are given", and therefore fail to capture the intrinsic difficulty of multi-reference settings. To address this gap, we introduce $\textbf{MultiBanana}$, which is carefully designed to assesses the edge of model capabilities by widely covering multi-reference-specific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., a red banana), and (5) multilingual textual references for rendering. Our analysis among a variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish a standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/multibanana .
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIMM-X: Disentangling Spurious Correlations for Medical Image Analysis</title>
<link>https://arxiv.org/abs/2511.22990</link>
<guid>https://arxiv.org/abs/2511.22990</guid>
<content:encoded><![CDATA[
arXiv:2511.22990v1 Announce Type: new 
Abstract: Deep learning models can excel on medical tasks, yet often experience spurious correlations, known as shortcut learning, leading to poor generalization in new environments. Particularly in medical imaging, where multiple spurious correlations can coexist, misclassifications can have severe consequences. We propose MIMM-X, a framework that disentangles causal features from multiple spurious correlations by minimizing their mutual information. It enables predictions based on true underlying causal relationships rather than dataset-specific shortcuts. We evaluate MIMM-X on three datasets (UK Biobank, NAKO, CheXpert) across two imaging modalities (MRI and X-ray). Results demonstrate that MIMM-X effectively mitigates shortcut learning of multiple spurious correlations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding Visual Autoregressive Models through Spectrum Weakening</title>
<link>https://arxiv.org/abs/2511.22991</link>
<guid>https://arxiv.org/abs/2511.22991</guid>
<content:encoded><![CDATA[
arXiv:2511.22991v1 Announce Type: new 
Abstract: Classifier-free guidance (CFG) has become a widely adopted and practical approach for enhancing generation quality and improving condition alignment. Recent studies have explored guidance mechanisms for unconditional generation, yet these approaches remain fundamentally tied to assumptions specific to diffusion models. In this work, we propose a spectrum-weakening framework for visual autoregressive (AR) models. This method works without the need for re-training, specific conditions, or any architectural modifications. It achieves this by constructing a controllable weak model in the spectral domain. We theoretically show that invertible spectral transformations preserve information, while selectively retaining only a subset of spectrum introduces controlled information reduction. Based on this insight, we perform spectrum selection along the channel dimension of internal representations, which avoids the structural constraints imposed by diffusion models. We further introduce two spectrum renormalization strategies that ensures numerical stability during the weakening process. Extensive experiments were conducted on both discrete and continuous AR models, with text or class conditioning. The results demonstrate that our method enables high-quality unconditional generation while maintaining strong prompt alignment for conditional generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizer Sensitivity In Vision Transformerbased Iris Recognition: Adamw Vs Sgd Vs Rmsprop</title>
<link>https://arxiv.org/abs/2511.22994</link>
<guid>https://arxiv.org/abs/2511.22994</guid>
<content:encoded><![CDATA[
arXiv:2511.22994v1 Announce Type: new 
Abstract: The security of biometric authentication is increasingly critical as digital identity systems expand. Iris recognition offers high reliability due to its distinctive and stable texture patterns. Recent progress in deep learning, especially Vision Transformers ViT, has improved visual recognition performance. Yet, the effect of optimizer choice on ViT-based biometric systems remains understudied. This work evaluates how different optimizers influence the accuracy and stability of ViT for iris recognition, providing insights to enhance the robustness of biometric identification models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MrGS: Multi-modal Radiance Fields with 3D Gaussian Splatting for RGB-Thermal Novel View Synthesis</title>
<link>https://arxiv.org/abs/2511.22997</link>
<guid>https://arxiv.org/abs/2511.22997</guid>
<content:encoded><![CDATA[
arXiv:2511.22997v1 Announce Type: new 
Abstract: Recent advances in Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved considerable performance in RGB scene reconstruction. However, multi-modal rendering that incorporates thermal infrared imagery remains largely underexplored. Existing approaches tend to neglect distinctive thermal characteristics, such as heat conduction and the Lambertian property. In this study, we introduce MrGS, a multi-modal radiance field based on 3DGS that simultaneously reconstructs both RGB and thermal 3D scenes. Specifically, MrGS derives RGB- and thermal-related information from a single appearance feature through orthogonal feature extraction and employs view-dependent or view-independent embedding strategies depending on the degree of Lambertian reflectance exhibited by each modality. Furthermore, we leverage two physics-based principles to effectively model thermal-domain phenomena. First, we integrate Fourier's law of heat conduction prior to alpha blending to model intensity interpolation caused by thermal conduction between neighboring Gaussians. Second, we apply the Stefan-Boltzmann law and the inverse-square law to formulate a depth-aware thermal radiation map that imposes additional geometric constraints on thermal rendering. Experimental results demonstrate that the proposed MrGS achieves high-fidelity RGB-T scene reconstruction while reducing the number of Gaussians.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JarvisEvo: Towards a Self-Evolving Photo Editing Agent with Synergistic Editor-Evaluator Optimization</title>
<link>https://arxiv.org/abs/2511.23002</link>
<guid>https://arxiv.org/abs/2511.23002</guid>
<content:encoded><![CDATA[
arXiv:2511.23002v1 Announce Type: new 
Abstract: Agent-based editing models have substantially advanced interactive experiences, processing quality, and creative flexibility. However, two critical challenges persist: (1) instruction hallucination, text-only chain-of-thought (CoT) reasoning cannot fully prevent factual errors due to inherent information bottlenecks; (2) reward hacking, dynamic policy optimization against static reward models allows agents to exploit flaws in reward functions. To address these issues, we propose JarvisEvo, a unified image editing agent that emulates an expert human designer by iteratively editing, selecting appropriate tools, evaluating results, and reflecting on its own decisions to refine outcomes. JarvisEvo offers three key advantages: (1) an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism that enhances instruction following and editing quality; (2) a synergistic editor-evaluator policy optimization (SEPO) framework that enables self-improvement without external rewards, effectively mitigating reward hacking; and (3) support for both global and local fine-grained editing through seamless integration of Adobe Lightroom. On ArtEdit-Bench, JarvisEvo outperforms Nano-Banana by an average of 18.95% on preservative editing metrics, including a substantial 44.96% improvement in pixel-level content fidelity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning</title>
<link>https://arxiv.org/abs/2511.23031</link>
<guid>https://arxiv.org/abs/2511.23031</guid>
<content:encoded><![CDATA[
arXiv:2511.23031v1 Announce Type: new 
Abstract: Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to "get the right answer for the right visual reason". Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry-Consistent 4D Gaussian Splatting for Sparse-Input Dynamic View Synthesis</title>
<link>https://arxiv.org/abs/2511.23044</link>
<guid>https://arxiv.org/abs/2511.23044</guid>
<content:encoded><![CDATA[
arXiv:2511.23044v1 Announce Type: new 
Abstract: Gaussian Splatting has been considered as a novel way for view synthesis of dynamic scenes, which shows great potential in AIoT applications such as digital twins. However, recent dynamic Gaussian Splatting methods significantly degrade when only sparse input views are available, limiting their applicability in practice. The issue arises from the incoherent learning of 4D geometry as input views decrease. This paper presents GC-4DGS, a novel framework that infuses geometric consistency into 4D Gaussian Splatting (4DGS), offering real-time and high-quality dynamic scene rendering from sparse input views. While learning-based Multi-View Stereo (MVS) and monocular depth estimators (MDEs) provide geometry priors, directly integrating these with 4DGS yields suboptimal results due to the ill-posed nature of sparse-input 4D geometric optimization. To address these problems, we introduce a dynamic consistency checking strategy to reduce estimation uncertainties of MVS across spacetime. Furthermore, we propose a global-local depth regularization approach to distill spatiotemporal-consistent geometric information from monocular depths, thereby enhancing the coherent geometry and appearance learning within the 4D volume. Extensive experiments on the popular N3DV and Technicolor datasets validate the effectiveness of GC-4DGS in rendering quality without sacrificing efficiency. Notably, our method outperforms RF-DeRF, the latest dynamic radiance field tailored for sparse-input dynamic view synthesis, and the original 4DGS by 2.62dB and 1.58dB in PSNR, respectively, with seamless deployability on resource-constrained IoT edge devices.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GOATex: Geometry &amp; Occlusion-Aware Texturing</title>
<link>https://arxiv.org/abs/2511.23051</link>
<guid>https://arxiv.org/abs/2511.23051</guid>
<content:encoded><![CDATA[
arXiv:2511.23051v1 Announce Type: new 
Abstract: We present GOATex, a diffusion-based method for 3D mesh texturing that generates high-quality textures for both exterior and interior surfaces. While existing methods perform well on visible regions, they inherently lack mechanisms to handle occluded interiors, resulting in incomplete textures and visible seams. To address this, we introduce an occlusion-aware texturing framework based on the concept of hit levels, which quantify the relative depth of mesh faces via multi-view ray casting. This allows us to partition mesh faces into ordered visibility layers, from outermost to innermost. We then apply a two-stage visibility control strategy that progressively reveals interior regions with structural coherence, followed by texturing each layer using a pretrained diffusion model. To seamlessly merge textures obtained across layers, we propose a soft UV-space blending technique that weighs each texture's contribution based on view-dependent visibility confidence. Empirical results demonstrate that GOATex consistently outperforms existing methods, producing seamless, high-fidelity textures across both visible and occluded surfaces. Unlike prior works, GOATex operates entirely without costly fine-tuning of a pretrained diffusion model and allows separate prompting for exterior and interior mesh regions, enabling fine-grained control over layered appearances. For more qualitative results, please visit our project page: https://goatex3d.github.io/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Valuation in NeRF-based 3D reconstruction</title>
<link>https://arxiv.org/abs/2511.23052</link>
<guid>https://arxiv.org/abs/2511.23052</guid>
<content:encoded><![CDATA[
arXiv:2511.23052v1 Announce Type: new 
Abstract: Data valuation and monetization are becoming increasingly important across domains such as eXtended Reality (XR) and digital media. In the context of 3D scene reconstruction from a set of images -- whether casually or professionally captured -- not all inputs contribute equally to the final output. Neural Radiance Fields (NeRFs) enable photorealistic 3D reconstruction of scenes by optimizing a volumetric radiance field given a set of images. However, in-the-wild scenes often include image captures of varying quality, occlusions, and transient objects, resulting in uneven utility across inputs. In this paper we propose a method to quantify the individual contribution of each image to NeRF-based reconstructions of in-the-wild image sets. Contribution is assessed through reconstruction quality metrics based on PSNR and MSE. We validate our approach by removing low-contributing images during training and measuring the resulting impact on reconstruction fidelity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Clinical Impact of Generative Inpainting on Bone Age Estimation</title>
<link>https://arxiv.org/abs/2511.23066</link>
<guid>https://arxiv.org/abs/2511.23066</guid>
<content:encoded><![CDATA[
arXiv:2511.23066v1 Announce Type: new 
Abstract: Generative foundation models can remove visual artifacts through realistic image inpainting, but their impact on medical AI performance remains uncertain. Pediatric hand radiographs often contain non-anatomical markers, and it is unclear whether inpainting these regions preserves features needed for bone age and gender prediction. To evaluate the clinical reliability of generative model-based inpainting for artifact removal, we used the RSNA Bone Age Challenge dataset, selecting 200 original radiographs and generating 600 inpainted versions with gpt-image-1 using natural language prompts to target non-anatomical artifacts. Downstream performance was assessed with deep learning ensembles for bone age estimation and gender classification, using mean absolute error (MAE) and area under the ROC curve (AUC) as metrics, and pixel intensity distributions to detect structural alterations. Inpainting markedly degraded model performance: bone age MAE increased from 6.26 to 30.11 months, and gender classification AUC decreased from 0.955 to 0.704. Inpainted images displayed pixel-intensity shifts and inconsistencies, indicating structural modifications not corrected by simple calibration. These findings show that, although visually realistic, foundation model-based inpainting can obscure subtle but clinically relevant features and introduce latent bias even when edits are confined to non-diagnostic regions, underscoring the need for rigorous, task-specific validation before integrating such generative tools into clinical AI workflows.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Buffer replay enhances the robustness of multimodal learning under missing-modality</title>
<link>https://arxiv.org/abs/2511.23070</link>
<guid>https://arxiv.org/abs/2511.23070</guid>
<content:encoded><![CDATA[
arXiv:2511.23070v1 Announce Type: new 
Abstract: Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding</title>
<link>https://arxiv.org/abs/2511.23071</link>
<guid>https://arxiv.org/abs/2511.23071</guid>
<content:encoded><![CDATA[
arXiv:2511.23071v1 Announce Type: new 
Abstract: Reading scene text, that is, text appearing in images, has numerous application areas, including assistive technology, search, and e-commerce. Although scene text recognition in English has advanced significantly and is often considered nearly a solved problem, Indian language scene text recognition remains an open challenge. This is due to script diversity, non-standard fonts, and varying writing styles, and, more importantly, the lack of high-quality datasets and open-source models. To address these gaps, we introduce the Bharat Scene Text Dataset (BSTD) - a large-scale and comprehensive benchmark for studying Indian Language Scene Text Recognition. It comprises more than 100K words that span 11 Indian languages and English, sourced from over 6,500 scene images captured across various linguistic regions of India. The dataset is meticulously annotated and supports multiple scene text tasks, including: (i) Scene Text Detection, (ii) Script Identification, (iii) Cropped Word Recognition, and (iv) End-to-End Scene Text Recognition. We evaluated state-of-the-art models originally developed for English by adapting (fine-tuning) them for Indian languages. Our results highlight the challenges and opportunities in Indian language scene text recognition. We believe that this dataset represents a significant step toward advancing research in this domain. All our models and data are open source.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.23075</link>
<guid>https://arxiv.org/abs/2511.23075</guid>
<content:encoded><![CDATA[
arXiv:2511.23075v1 Announce Type: new 
Abstract: Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implementation of a Skin Lesion Detection System for Managing Children with Atopic Dermatitis Based on Ensemble Learning</title>
<link>https://arxiv.org/abs/2511.23082</link>
<guid>https://arxiv.org/abs/2511.23082</guid>
<content:encoded><![CDATA[
arXiv:2511.23082v1 Announce Type: new 
Abstract: The amendments made to the Data 3 Act and impact of COVID-19 have fostered the growth of digital healthcare market and promoted the use of medical data in artificial intelligence in South Korea. Atopic dermatitis, a chronic inflammatory skin disease, is diagnosed via subjective evaluations without using objective diagnostic methods, thereby increasing the risk of misdiagnosis. It is also similar to psoriasis in appearance, further complicating its accurate diagnosis. Existing studies on skin diseases have used high-quality dermoscopic image datasets, but such high-quality images cannot be obtained in actual clinical settings. Moreover, existing systems must ensure accuracy and fast response times. To this end, an ensemble learning-based skin lesion detection system (ENSEL) was proposed herein. ENSEL enhanced diagnostic accuracy by integrating various deep learning models via an ensemble approach. Its performance was verified by conducting skin lesion detection experiments using images of skin lesions taken by actual users. Its accuracy and response time were measured using randomly sampled skin disease images. Results revealed that ENSEL achieved high recall in most images and less than 1s s processing speed. This study contributes to the objective diagnosis of skin lesions and promotes the advancement of digital healthcare.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NumeriKontrol: Adding Numeric Control to Diffusion Transformers for Instruction-based Image Editing</title>
<link>https://arxiv.org/abs/2511.23105</link>
<guid>https://arxiv.org/abs/2511.23105</guid>
<content:encoded><![CDATA[
arXiv:2511.23105v1 Announce Type: new 
Abstract: Instruction-based image editing enables intuitive manipulation through natural language commands. However, text instructions alone often lack the precision required for fine-grained control over edit intensity. We introduce NumeriKontrol, a framework that allows users to precisely adjust image attributes using continuous scalar values with common units. NumeriKontrol encodes numeric editing scales via an effective Numeric Adapter and injects them into diffusion models in a plug-and-play manner. Thanks to a task-separated design, our approach supports zero-shot multi-condition editing, allowing users to specify multiple instructions in any order. To provide high-quality supervision, we synthesize precise training data from reliable sources, including high-fidelity rendering engines and DSLR cameras. Our Common Attribute Transform (CAT) dataset covers diverse attribute manipulations with accurate ground-truth scales, enabling NumeriKontrol to function as a simple yet powerful interactive editing studio. Extensive experiments show that NumeriKontrol delivers accurate, continuous, and stable scale control across a wide range of attribute editing scenarios. These contributions advance instruction-based image editing by enabling precise, scalable, and user-controllable image manipulation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?</title>
<link>https://arxiv.org/abs/2511.23112</link>
<guid>https://arxiv.org/abs/2511.23112</guid>
<content:encoded><![CDATA[
arXiv:2511.23112v1 Announce Type: new 
Abstract: Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism</title>
<link>https://arxiv.org/abs/2511.23113</link>
<guid>https://arxiv.org/abs/2511.23113</guid>
<content:encoded><![CDATA[
arXiv:2511.23113v1 Announce Type: new 
Abstract: Scaling Diffusion Transformer (DiT) inference via sequence parallelism is critical for reducing latency in visual generation, but is severely hampered by workload imbalance when applied to models employing block-wise sparse attention. The imbalance stems from the inherent variation in sparsity across attention heads and the irregular distribution of dense blocks within the sparse mask, when sequence parallelism is applied along the head dimension (as in Ulysses) or the block dimension (as in Ring Attention). In this paper, we formalize a sparse imbalance ratio to quantify the imbalance, and propose db-SP, a sparsity-aware sequence parallelism technique that tackles the challenge. db-SP contains a dual-level partitioning approach that achieves near-perfect workload balance at both the head and block levels with negligible overhead. Furthermore, to handle the evolving sparsity patterns across denoising steps and layers, db-SP dynamically determines the parallel degrees for the head and block dimensions at runtime. Experimental results demonstrate that db-SP delivers an end-to-end speedup of 1.25x and an attention-specific speedup of 1.40x over state-of-the-art sequence parallel methods on average. Code is available at https://github.com/thu-nics/db-SP.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Image Beyond Visual Aspect: Image Emotion Classification via Multiple-Affective Captioning</title>
<link>https://arxiv.org/abs/2511.23115</link>
<guid>https://arxiv.org/abs/2511.23115</guid>
<content:encoded><![CDATA[
arXiv:2511.23115v1 Announce Type: new 
Abstract: Image emotion classification (IEC) is a longstanding research field that has received increasing attention with the rapid progress of deep learning. Although recent advances have leveraged the knowledge encoded in pre-trained visual models, their effectiveness is constrained by the "affective gap" , limits the applicability of pre-training knowledge for IEC tasks. It has been demonstrated in psychology that language exhibits high variability, encompasses diverse and abundant information, and can effectively eliminate the "affective gap". Inspired by this, we propose a novel Affective Captioning for Image Emotion Classification (ACIEC) to classify image emotion based on pure texts, which effectively capture the affective information in the image. In our method, a hierarchical multi-level contrastive loss is designed for detecting emotional concepts from images, while an emotional attribute chain-of-thought reasoning is proposed to generate affective sentences. Then, a pre-trained language model is leveraged to synthesize emotional concepts and affective sentences to conduct IEC. Additionally, a contrastive loss based on semantic similarity sampling is designed to solve the problem of large intra-class differences and small inter-class differences in affective datasets. Moreover, we also take the images with embedded texts into consideration, which were ignored by previous studies. Extensive experiments illustrate that our method can effectively bridge the affective gap and achieve superior results on multiple benchmarks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DNA-Prior: Unsupervised Denoise Anything via Dual-Domain Prior</title>
<link>https://arxiv.org/abs/2511.23124</link>
<guid>https://arxiv.org/abs/2511.23124</guid>
<content:encoded><![CDATA[
arXiv:2511.23124v1 Announce Type: new 
Abstract: Medical imaging pipelines critically rely on robust denoising to stabilise downstream tasks such as segmentation and reconstruction. However, many existing denoisers depend on large annotated datasets or supervised learning, which restricts their usability in clinical environments with heterogeneous modalities and limited ground-truth data. To address this limitation, we introduce DNA-Prior, a universal unsupervised denoising framework that reconstructs clean images directly from corrupted observations through a mathematically principled hybrid prior. DNA-Prior integrates (i) an implicit architectural prior, enforced through a deep network parameterisation, with (ii) an explicit spectral-spatial prior composed of a frequency-domain fidelity term and a spatial regularisation functional. This dual-domain formulation yields a well-structured optimisation problem that jointly preserves global frequency characteristics and local anatomical structure, without requiring any external training data or modality-specific tuning. Experiments across multiple modalities show that DNA achieves consistent noise suppression and structural preservation under diverse noise conditions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation</title>
<link>https://arxiv.org/abs/2511.23127</link>
<guid>https://arxiv.org/abs/2511.23127</guid>
<content:encoded><![CDATA[
arXiv:2511.23127v1 Announce Type: new 
Abstract: This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl\-page/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstanceV: Instance-Level Video Generation</title>
<link>https://arxiv.org/abs/2511.23146</link>
<guid>https://arxiv.org/abs/2511.23146</guid>
<content:encoded><![CDATA[
arXiv:2511.23146v1 Announce Type: new 
Abstract: Recent advances in text-to-video diffusion models have enabled the generation of high-quality videos conditioned on textual descriptions. However, most existing text-to-video models rely solely on textual conditions, lacking general fine-grained controllability over video generation. To address this challenge, we propose InstanceV, a video generation framework that enables i) instance-level control and ii) global semantic consistency. Specifically, with the aid of proposed Instance-aware Masked Cross-Attention mechanism, InstanceV maximizes the utilization of additional instance-level grounding information to generate correctly attributed instances at designated spatial locations. To improve overall consistency, We introduce the Shared Timestep-Adaptive Prompt Enhancement module, which connects local instances with global semantics in a parameter-efficient manner. Furthermore, we incorporate Spatially-Aware Unconditional Guidance during both training and inference to alleviate the disappearance of small instances. Finally, we propose a new benchmark, named InstanceBench, which combines general video quality metrics with instance-aware metrics for more comprehensive evaluation on instance-level video generation. Extensive experiments demonstrate that InstanceV not only achieves remarkable instance-level controllability in video generation, but also outperforms existing state-of-the-art models in both general quality and instance-aware metrics across qualitative and quantitative evaluations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cascaded Robust Rectification for Arbitrary Document Images</title>
<link>https://arxiv.org/abs/2511.23150</link>
<guid>https://arxiv.org/abs/2511.23150</guid>
<content:encoded><![CDATA[
arXiv:2511.23150v1 Announce Type: new 
Abstract: Document rectification in real-world scenarios poses significant challenges due to extreme variations in camera perspectives and physical distortions. Driven by the insight that complex transformations can be decomposed and resolved progressively, we introduce a novel multi-stage framework that progressively reverses distinct distortion types in a coarse-to-fine manner. Specifically, our framework first performs a global affine transformation to correct perspective distortions arising from the camera's viewpoint, then rectifies geometric deformations resulting from physical paper curling and folding, and finally employs a content-aware iterative process to eliminate fine-grained content distortions. To address limitations in existing evaluation protocols, we also propose two enhanced metrics: layout-aligned OCR metrics (AED/ACER) for a stable assessment that decouples geometric rectification quality from the layout analysis errors of OCR engines, and masked AD/AAD (AD-M/AAD-M) tailored for accurately evaluating geometric distortions in documents with incomplete boundaries. Extensive experiments show that our method establishes new state-of-the-art performance on multiple challenging benchmarks, yielding a substantial reduction of 14.1\%--34.7\% in the AAD metric and demonstrating superior efficacy in real-world applications. The code will be publicly available at https://github.com/chaoyunwang/ArbDR.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Refuse: Refusal-Aware Reinforcement Fine-Tuning for Hard-Irrelevant Queries in Video Temporal Grounding</title>
<link>https://arxiv.org/abs/2511.23151</link>
<guid>https://arxiv.org/abs/2511.23151</guid>
<content:encoded><![CDATA[
arXiv:2511.23151v1 Announce Type: new 
Abstract: Video Temporal Grounding (VTG) aims to localize a temporal segment in a video corresponding to a natural language query. However, existing VTG models assume that a relevant segment always exists, causing them to always predict a target segment even when the query is irrelevant to the video. While recent approaches attempt to handle irrelevant queries, they can only reject those that are entirely unrelated to the video and still fail to handle hard-irrelevant queries that are semantically similar but not actually relevant. To address this, we propose Refusal-Aware Reinforcement Fine-Tuning (RA-RFT) to effectively refuse hard-irrelevant queries in VTG. Our method is based on the Group Relative Policy Optimization (GRPO) framework and integrates four reward objectives-format, refuse-IoU, explain, and query correction-to improve both relevance discrimination and fine-grained semantic reasoning. In addition, to effectively support RA-RFT, we construct a Hard-Irrelevant VTG (HI-VTG) dataset, which includes hard-irrelevant queries and their refusal answers. We demonstrate the effectiveness of our method across various relevance-aware VTG scenarios, including hard-irrelevant VTG, simply-shuffled RA-VTG, and human-annotated RA-VTG settings. We also show that the proposed method is scalable by applying it to various LVLM-based VTG models. Our code is available at https://github.com/JINSUBY/RA-RFT.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection</title>
<link>https://arxiv.org/abs/2511.23158</link>
<guid>https://arxiv.org/abs/2511.23158</guid>
<content:encoded><![CDATA[
arXiv:2511.23158v1 Announce Type: new 
Abstract: With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity. Consequently, there is an urgent need for efficient and truly explainable image forensic methods. Recent detection paradigms have shifted towards explainable forensics. However, state-of-the-art approaches primarily rely on post-hoc rationalizations or visual discrimination, lacking a verifiable chain of evidence. This reliance on surface-level pattern matching limits the generation of causally grounded explanations and often results in poor generalization. To bridge this critical gap, we introduce \textbf{REVEAL-Bench}, the first reasoning-enhanced multimodal benchmark for AI-generated image detection that is explicitly structured around a chain-of-evidence derived from multiple lightweight expert models, then records step-by-step reasoning traces and evidential justifications. Building upon this dataset, we propose \textbf{REVEAL} (\underline{R}easoning-\underline{e}nhanced Forensic E\underline{v}id\underline{e}nce \underline{A}na\underline{l}ysis), an effective and explainable forensic framework that integrates detection with a novel expert-grounded reinforcement learning. Our reward mechanism is specially tailored to jointly optimize detection accuracy, explanation fidelity, and logical coherence grounded in explicit forensic evidence, enabling REVEAL to produce fine-grained, interpretable, and verifiable reasoning chains alongside its detection outcomes. Extensive experimental results demonstrate that REVEAL significantly enhances detection accuracy, explanation fidelity, and robust cross-model generalization, benchmarking a new state of the art for explainable image forensics.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PowerCLIP: Powerset Alignment for Contrastive Pre-Training</title>
<link>https://arxiv.org/abs/2511.23170</link>
<guid>https://arxiv.org/abs/2511.23170</guid>
<content:encoded><![CDATA[
arXiv:2511.23170v1 Announce Type: new 
Abstract: Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Multi-view Consistent 3D Editing with Video Priors</title>
<link>https://arxiv.org/abs/2511.23172</link>
<guid>https://arxiv.org/abs/2511.23172</guid>
<content:encoded><![CDATA[
arXiv:2511.23172v1 Announce Type: new 
Abstract: Text-driven 3D editing enables user-friendly 3D object or scene editing with text instructions. Due to the lack of multi-view consistency priors, existing methods typically resort to employing 2D generation or editing models to process each view individually, followed by iterative 2D-3D-2D updating. However, these methods are not only time-consuming but also prone to over-smoothed results because the different editing signals gathered from different views are averaged during the iterative process. In this paper, we propose generative Video Prior based 3D Editing (ViP3DE) to employ the temporal consistency priors from pre-trained video generation models for multi-view consistent 3D editing in a single forward pass. Our key insight is to condition the video generation model on a single edited view to generate other consistent edited views for 3D updating directly, thereby bypassing the iterative editing paradigm. Since 3D updating requires edited views to be paired with specific camera poses, we propose motion-preserved noise blending for the video model to generate edited views at predefined camera poses. In addition, we introduce geometry-aware denoising to further enhance multi-view consistency by integrating 3D geometric priors into video models. Extensive experiments demonstrate that our proposed ViP3DE can achieve high-quality 3D editing results even within a single forward pass, significantly outperforming existing methods in both editing quality and speed.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoWorld: Unlocking the Potential of Geometry Models to Facilitate High-Fidelity 3D Scene Generation</title>
<link>https://arxiv.org/abs/2511.23191</link>
<guid>https://arxiv.org/abs/2511.23191</guid>
<content:encoded><![CDATA[
arXiv:2511.23191v1 Announce Type: new 
Abstract: Previous works leveraging video models for image-to-3D scene generation tend to suffer from geometric distortions and blurry content. In this paper, we renovate the pipeline of image-to-3D scene generation by unlocking the potential of geometry models and present our GeoWorld. Instead of exploiting geometric information obtained from a single-frame input, we propose to first generate consecutive video frames and then take advantage of the geometry model to provide full-frame geometry features, which contain richer information than single-frame depth maps or camera embeddings used in previous methods, and use these geometry features as geometrical conditions to aid the video generation model. To enhance the consistency of geometric structures, we further propose a geometry alignment loss to provide the model with real-world geometric constraints and a geometry adaptation module to ensure the effective utilization of geometry features. Extensive experiments show that our GeoWorld can generate high-fidelity 3D scenes from a single image and a given camera trajectory, outperforming prior methods both qualitatively and quantitatively. Project Page: https://peaes.github.io/GeoWorld/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Bridge Transformer at Scale</title>
<link>https://arxiv.org/abs/2511.23199</link>
<guid>https://arxiv.org/abs/2511.23199</guid>
<content:encoded><![CDATA[
arXiv:2511.23199v1 Announce Type: new 
Abstract: We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pathryoshka: Compressing Pathology Foundation Models via Multi-Teacher Knowledge Distillation with Nested Embeddings</title>
<link>https://arxiv.org/abs/2511.23204</link>
<guid>https://arxiv.org/abs/2511.23204</guid>
<content:encoded><![CDATA[
arXiv:2511.23204v1 Announce Type: new 
Abstract: Pathology foundation models (FMs) have driven significant progress in computational pathology. However, these high-performing models can easily exceed a billion parameters and produce high-dimensional embeddings, thus limiting their applicability for research or clinical use when computing resources are tight. Here, we introduce Pathryoshka, a multi-teacher distillation framework inspired by RADIO distillation and Matryoshka Representation Learning to reduce pathology FM sizes while allowing for adaptable embedding dimensions. We evaluate our framework with a distilled model on ten public pathology benchmarks with varying downstream tasks. Compared to its much larger teachers, Pathryoshka reduces the model size by 86-92% at on-par performance. It outperforms state-of-the-art single-teacher distillation models of comparable size by a median margin of 7.0 in accuracy. By enabling efficient local deployment without sacrificing accuracy or representational richness, Pathryoshka democratizes access to state-of-the-art pathology FMs for the broader research and clinical community.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Multi-Criteria Visual Quality Inspection for Semi-Controlled Industrial Environments via Real-Time 3D Digital Twin Simulation</title>
<link>https://arxiv.org/abs/2511.23214</link>
<guid>https://arxiv.org/abs/2511.23214</guid>
<content:encoded><![CDATA[
arXiv:2511.23214v1 Announce Type: new 
Abstract: Early-stage visual quality inspection is vital for achieving Zero-Defect Manufacturing and minimizing production waste in modern industrial environments. However, the complexity of robust visual inspection systems and their extensive data requirements hinder widespread adoption in semi-controlled industrial settings. In this context, we propose a pose-agnostic, zero-shot quality inspection framework that compares real scenes against real-time Digital Twins (DT) in the RGB-D space. Our approach enables efficient real-time DT rendering by semantically describing industrial scenes through object detection and pose estimation of known Computer-Aided Design models. We benchmark tools for real-time, multimodal RGB-D DT creation while tracking consumption of computational resources. Additionally, we provide an extensible and hierarchical annotation strategy for multi-criteria defect detection, unifying pose labelling with logical and structural defect annotations. Based on an automotive use case featuring the quality inspection of an axial flux motor, we demonstrate the effectiveness of our framework. Our results demonstrate detection performace, achieving intersection-over-union (IoU) scores of up to 63.3% compared to ground-truth masks, even if using simple distance measurements under semi-controlled industrial conditions. Our findings lay the groundwork for future research on generalizable, low-data defect detection methods in dynamic manufacturing settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction Tuning of Large Language Models for Tabular Data Generation-in One Day</title>
<link>https://arxiv.org/abs/2511.23220</link>
<guid>https://arxiv.org/abs/2511.23220</guid>
<content:encoded><![CDATA[
arXiv:2511.23220v1 Announce Type: new 
Abstract: Tabular instruction tuning has emerged as a promising research direction for improving LLMs understanding of tabular data. However, the majority of existing works only consider question-answering and reasoning tasks over tabular data, leaving tabular data generation largely unnoticed. In this work, for the first time, we explore the efficacy of instruction tuning in improving LLMs tabular data generation capabilities. More specifically, given the high data and computation requirements of tabular instruction tuning, we aim to address the possibility of instruction tuning for tabular data generation with limited data and computational resources. To achieve this, we first create a high-quality instruction dataset for tabular data, enabling efficient LLM comprehension. We then instruction-tune an open-source LLM (Llama3.1-8B-Instruct) on the training set of this dataset to improve its tabular data generation performance. Our experimental results show that by using our high-quality dataset and instruction-tuning on only 7K instructions with an A100 GPU, for less than 6 hours, we achieve tabular data generation performance on par with the most capable commercial LLM, GPT-4o.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust 3DGS-based SLAM via Adaptive Kernel Smoothing</title>
<link>https://arxiv.org/abs/2511.23221</link>
<guid>https://arxiv.org/abs/2511.23221</guid>
<content:encoded><![CDATA[
arXiv:2511.23221v1 Announce Type: new 
Abstract: In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy. We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking. To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM. Unlike conventional methods that focus solely on minimizing rendering error, our core insight is to make the rasterization process more resilient to imperfections in the 3DGS parameters. We hypothesize that by allowing each Gaussian to influence a smoother, wider distribution of pixels during rendering, we can mitigate the detrimental effects of parameter noise from outlier Gaussians. This approach intentionally introduces a controlled blur to the rendered image, which acts as a regularization term, stabilizing the subsequent pose optimization. While a complete redesign of the rasterization pipeline is an ideal solution, we propose a practical and effective alternative that is readily integrated into existing 3DGS frameworks. Our method, termed Corrective Blurry KNN (CB-KNN), adaptively modifies the RGB values and locations of the K-nearest neighboring Gaussians within a local region. This dynamic adjustment generates a smoother local rendering, reducing the impact of erroneous GS parameters on the overall image. Experimental results demonstrate that our approach, while maintaining the overall quality of the scene reconstruction (mapping), significantly improves the robustness and accuracy of camera pose tracking.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAONet-YOLOv8: An Occlusion-Aware Dual-Attention Network for Tea Leaf Pest and Disease Detection</title>
<link>https://arxiv.org/abs/2511.23222</link>
<guid>https://arxiv.org/abs/2511.23222</guid>
<content:encoded><![CDATA[
arXiv:2511.23222v1 Announce Type: new 
Abstract: Accurate detection of tea leaf pests and diseases in real plantations remains challenging due to complex backgrounds, variable illumination, and frequent occlusions among dense branches and leaves. Existing detectors often suffer from missed detections and false positives in such scenarios. To address these issues, we propose DAONet-YOLOv8, an enhanced YOLOv8 variant with three key improvements: (1) a Dual-Attention Fusion Module (DAFM) that combines convolutional local feature extraction with self-attention based global context modeling to focus on subtle lesion regions while suppressing background noise; (2) an occlusion-aware detection head (Detect-OAHead) that learns the relationship between visible and occluded parts to compensate for missing lesion features; and (3) a C2f-DSConv module employing dynamic synthesis convolutions with multiple kernel shapes to better capture irregular lesion boundaries. Experiments on our real-world tea plantation dataset containing six pest and disease categories demonstrate that DAONet-YOLOv8 achieves 92.97% precision, 92.80% recall, 97.10% mAP@50 and 76.90% mAP@50:95, outperforming the YOLOv8n baseline by 2.34, 4.68, 1.40 and 1.80 percentage points respectively, while reducing parameters by 16.7%. Comparative experiments further confirm that DAONet-YOLOv8 achieves superior performance over mainstream detection models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointCNN++: Performant Convolution on Native Points</title>
<link>https://arxiv.org/abs/2511.23227</link>
<guid>https://arxiv.org/abs/2511.23227</guid>
<content:encoded><![CDATA[
arXiv:2511.23227v1 Announce Type: new 
Abstract: Existing convolutional learning methods for 3D point cloud data are divided into two paradigms: point-based methods that preserve geometric precision but often face performance challenges, and voxel-based methods that achieve high efficiency through quantization at the cost of geometric fidelity. This loss of precision is a critical bottleneck for tasks such as point cloud registration. We propose PointCNN++, a novel architectural design that fundamentally mitigates this precision-performance trade-off. It \textbf{generalizes sparse convolution from voxels to points}, treating voxel-based convolution as a specialized, degraded case of our more general point-based convolution. First, we introduce a point-centric convolution where the receptive field is centered on the original, high-precision point coordinates. Second, to make this high-fidelity operation performant, we design a computational strategy that operates \textbf{natively} on points. We formulate the convolution on native points as a Matrix-Vector Multiplication and Reduction (MVMR) problem, for which we develop a dedicated, highly-optimized GPU kernel. Experiments demonstrate that PointCNN++ \textbf{uses an order of magnitude less memory and is several times faster} than representative point-based methods. Furthermore, when used as a simple replacement for the voxel-based backbones it generalizes, it \textbf{significantly improves point cloud registration accuracies while proving both more memory-efficient and faster}. PointCNN++ shows that preserving geometric detail and achieving high performance are not mutually exclusive, paving the way for a new class of 3D learning with high fidelity and efficiency. Our code will be open sourced.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-guided 3D scene synthesis for fine-grained functionality understanding</title>
<link>https://arxiv.org/abs/2511.23230</link>
<guid>https://arxiv.org/abs/2511.23230</guid>
<content:encoded><![CDATA[
arXiv:2511.23230v1 Announce Type: new 
Abstract: Functionality understanding in 3D, which aims to identify the functional element in a 3D scene to complete an action (e.g., the correct handle to "Open the second drawer of the cabinet near the bed"), is hindered by the scarcity of real-world data due to the substantial effort needed for its collection and annotation. To address this, we introduce SynthFun3D, the first method for task-based 3D scene synthesis. Given the action description, SynthFun3D generates a 3D indoor environment using a furniture asset database with part-level annotation, ensuring the action can be accomplished. It reasons about the action to automatically identify and retrieve the 3D mask of the correct functional element, enabling the inexpensive and large-scale generation of high-quality annotated data. We validate SynthFun3D through user studies, which demonstrate improved scene-prompt coherence compared to other approaches. Our quantitative results further show that the generated data can either replace real data with minor performance loss or supplement real data for improved performance, thereby providing an inexpensive and scalable solution for data-hungry 3D applications. Project page: github.com/tev-fbk/synthfun3d.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering</title>
<link>https://arxiv.org/abs/2511.23231</link>
<guid>https://arxiv.org/abs/2511.23231</guid>
<content:encoded><![CDATA[
arXiv:2511.23231v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Industrial Object Detection: GenAI vs. Feature-Based Methods</title>
<link>https://arxiv.org/abs/2511.23241</link>
<guid>https://arxiv.org/abs/2511.23241</guid>
<content:encoded><![CDATA[
arXiv:2511.23241v1 Announce Type: new 
Abstract: Reducing the burden of data generation and annotation remains a major challenge for the cost-effective deployment of machine learning in industrial and robotics settings. While synthetic rendering is a promising solution, bridging the sim-to-real gap often requires expert intervention. In this work, we benchmark a range of domain randomization (DR) and domain adaptation (DA) techniques, including feature-based methods, generative AI (GenAI), and classical rendering approaches, for creating contextualized synthetic data without manual annotation. Our evaluation focuses on the effectiveness and efficiency of low-level and high-level feature alignment, as well as a controlled diffusion-based DA method guided by prompts generated from real-world contexts. We validate our methods on two datasets: a proprietary industrial dataset (automotive and logistics) and a public robotics dataset. Results show that if render-based data with enough variability is available as seed, simpler feature-based methods, such as brightness-based and perceptual hashing filtering, outperform more complex GenAI-based approaches in both accuracy and resource efficiency. Perceptual hashing consistently achieves the highest performance, with mAP50 scores of 98% and 67% on the industrial and robotics datasets, respectively. Additionally, GenAI methods present significant time overhead for data generation at no apparent improvement of sim-to-real mAP values compared to simpler methods. Our findings offer actionable insights for efficiently bridging the sim-to-real gap, enabling high real-world performance from models trained exclusively on synthetic data.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>