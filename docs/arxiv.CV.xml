<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>

<item>
<title>No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves</title>
<link>https://arxiv.org/abs/2505.02831</link>
<guid>https://arxiv.org/abs/2505.02831</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion transformers, representation guidance, Self-Representation Alignment, generative training, self-distillation  

<br><br>Summary: This study addresses the challenges in improving generative training by proposing a novel method called Self-Representation Alignment (SRA). Recent works have shown that meaningful internal representations can enhance both the training speed and output quality in diffusion transformers. Traditional methods either require complex external representation frameworks or rely on large pre-trained foundation models for guidance. In contrast, SRA leverages the inherent discriminative process of diffusion transformers, allowing them to generate representation guidance internally during generative training. The method works by aligning the output latent representations from earlier layers (which have higher noise) with those from later layers (with lower noise). This alignment process enhances overall representation learning without the need for external components. Experimental results demonstrate that applying SRA to Diffusion Transformers (DiTs) and Simple Transformers (SiTs) results in consistent performance improvements. Notably, SRA surpasses existing approaches that depend on auxiliary representation frameworks and achieves performance levels comparable to methods reliant on sophisticated external representation priors, highlighting its efficacy and simplicity in optimizing generative processes. <div>
arXiv:2505.02831v3 Announce Type: replace 
Abstract: Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance the generation quality of diffusion transformers. However, existing approaches necessitate to either introduce an external and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation Alignment (SRA), a simple yet straightforward method that obtains representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in the earlier layer with higher noise to that in the later layer with lower noise to progressively enhance the overall representation learning during only the generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that are heavily dependent on powerful external representation priors.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing</title>
<link>https://arxiv.org/abs/2505.07984</link>
<guid>https://arxiv.org/abs/2505.07984</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal language model, remote sensing imagery, fine-tuning, Group Relative Policy Optimization, military installations

Summary: 
MilChat is a lightweight multimodal language model designed for analyzing remote sensing imagery in secluded areas, particularly focusing on challenging missile launch sites. A new dataset, MilData, was compiled to train the model with expert-verified aerial images and detailed captions highlighting military installations. The model underwent supervised fine-tuning using CoT reasoning annotations and utilized Group Relative Policy Optimization to improve domain-specific cue detection while minimizing false positives. Empirical evaluations showed that MilChat outperformed larger, general-purpose multimodal models and existing remote sensing-adapted approaches on open-ended captioning and classification tasks. Achieving over 80% recall and 98% precision on the MilData benchmark, the study highlights the effectiveness of targeted fine-tuning and reinforcement learning in specialized domains. 

<br /><br />Summary: <div>
arXiv:2505.07984v1 Announce Type: new 
Abstract: Remarkable capabilities in understanding and generating text-image content have been demonstrated by recent advancements in multimodal large language models (MLLMs). However, their effectiveness in specialized domains-particularly those requiring resource-efficient and domain-specific adaptations-has remained limited. In this work, a lightweight multimodal language model termed MilChat is introduced, specifically adapted to analyze remote sensing imagery in secluded areas, including challenging missile launch sites. A new dataset, MilData, was compiled by verifying hundreds of aerial images through expert review, and subtle military installations were highlighted via detailed captions. Supervised fine-tuning on a 2B-parameter open-source MLLM with chain-of-thought (CoT) reasoning annotations was performed, enabling more accurate and interpretable explanations. Additionally, Group Relative Policy Optimization (GRPO) was leveraged to enhance the model's ability to detect critical domain-specific cues-such as defensive layouts and key military structures-while minimizing false positives on civilian scenes. Through empirical evaluations, it has been shown that MilChat significantly outperforms both larger, general-purpose multimodal models and existing remote sensing-adapted approaches on open-ended captioning and classification metrics. Over 80% recall and 98% precision were achieved on the newly proposed MilData benchmark, underscoring the potency of targeted fine-tuning and reinforcement learning in specialized real-world applications.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Foundation Model Embedding-Based Semantic Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.07998</link>
<guid>https://arxiv.org/abs/2505.07998</guid>
<content:encoded><![CDATA[
<div> semantic anomaly detection, visual embeddings, autonomous systems, instance segmentation, filtering

Summary: 
This paper investigates semantic anomaly detection in autonomous systems using state-of-the-art vision foundation models. The framework proposed compares local vision embeddings from runtime images to a database of safe scenarios to identify anomalies. Two variants of the framework are considered, one using raw grid-based embeddings and the other leveraging instance segmentation for object-centric representations. A filtering mechanism is introduced to reduce false positives and enhance robustness. Evaluation on CARLA-simulated anomalies shows that the instance-based method with filtering achieves performance comparable to GPT-4o, with precise anomaly localization. These results demonstrate the potential of vision embeddings from foundation models for real-time anomaly detection in autonomous systems. 

Summary: <div>
arXiv:2505.07998v1 Announce Type: new 
Abstract: Semantic anomalies are contextually invalid or unusual combinations of familiar visual elements that can cause undefined behavior and failures in system-level reasoning for autonomous systems. This work explores semantic anomaly detection by leveraging the semantic priors of state-of-the-art vision foundation models, operating directly on the image. We propose a framework that compares local vision embeddings from runtime images to a database of nominal scenarios in which the autonomous system is deemed safe and performant. In this work, we consider two variants of the proposed framework: one using raw grid-based embeddings, and another leveraging instance segmentation for object-centric representations. To further improve robustness, we introduce a simple filtering mechanism to suppress false positives. Our evaluations on CARLA-simulated anomalies show that the instance-based method with filtering achieves performance comparable to GPT-4o, while providing precise anomaly localization. These results highlight the potential utility of vision embeddings from foundation models for real-time anomaly detection in autonomous systems.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RDD: Robust Feature Detector and Descriptor using Deformable Transformer</title>
<link>https://arxiv.org/abs/2505.08013</link>
<guid>https://arxiv.org/abs/2505.08013</guid>
<content:encoded><![CDATA[
<div> Keywords: feature detection, structure-from-motion, SLAM, deformable transformer, keypoint detection

Summary:
Robust feature detection and description in structure-from-motion and SLAM applications remain challenging, particularly in cases of significant viewpoint changes. Existing methods focus on local features but fail to capture long-range relationships. The Robust Deformable Detector (RDD) introduced in this study leverages deformable transformers to incorporate global context and geometric invariance through deformable self-attention mechanisms. By focusing on key locations, the deformable attention effectively reduces search space complexity and models geometric invariance. The RDD outperforms state-of-the-art methods in keypoint detection/description tasks and can handle semi-dense matching. The study also introduces two new benchmarks, emphasizing large viewpoint and scale variations, as well as an Air-to-Ground benchmark, showcasing the method's effectiveness for 3D reconstruction across different altitudes.

<br /><br />Summary: <div>
arXiv:2505.08013v1 Announce Type: new 
Abstract: As a core step in structure-from-motion and SLAM, robust feature detection and description under challenging scenarios such as significant viewpoint changes remain unresolved despite their ubiquity. While recent works have identified the importance of local features in modeling geometric transformations, these methods fail to learn the visual cues present in long-range relationships. We present Robust Deformable Detector (RDD), a novel and robust keypoint detector/descriptor leveraging the deformable transformer, which captures global context and geometric invariance through deformable self-attention mechanisms. Specifically, we observed that deformable attention focuses on key locations, effectively reducing the search space complexity and modeling the geometric invariance. Furthermore, we collected an Air-to-Ground dataset for training in addition to the standard MegaDepth dataset. Our proposed method outperforms all state-of-the-art keypoint detection/description methods in sparse matching tasks and is also capable of semi-dense matching. To ensure comprehensive evaluation, we introduce two challenging benchmarks: one emphasizing large viewpoint and scale variations, and the other being an Air-to-Ground benchmark -- an evaluation setting that has recently gaining popularity for 3D reconstruction across different altitudes.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Interpretable Subtask Reasoning for Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.08084</link>
<guid>https://arxiv.org/abs/2505.08084</guid>
<content:encoded><![CDATA[
<div> Keywords: visual question answering, multimodal large language models, reasoning, interpretability, subtask-driven training

Summary:<br />
The article introduces VISTAR, a framework for enhancing interpretability and reasoning in visual question answering tasks. VISTAR utilizes subtask-driven training to generate structured reasoning sequences within multimodal large language models. By fine-tuning these models to produce step-by-step rationales, VISTAR improves reasoning accuracy while maintaining interpretability. The framework addresses the limitations of current methods by integrating both textual and visual explanations directly into the model, eliminating the need for external models. Experimental results on two benchmarks demonstrate the effectiveness of VISTAR in improving reasoning performance. The code and dataset for VISTAR will be made available for further research and development. 

Summary: <div>
arXiv:2505.08084v1 Announce Type: new 
Abstract: Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)</title>
<link>https://arxiv.org/abs/2505.08086</link>
<guid>https://arxiv.org/abs/2505.08086</guid>
<content:encoded><![CDATA[
<div> Keywords: wound classification, Artificial Intelligence, transfer learning, multi-modal network, medical image analysis <br />
Summary: <br />
- Effective diagnosis of acute and hard-to-heal wounds is crucial for providing proper patient care, especially in cases of infection, peripheral vascular disease, and increasing wound depth.
- Utilizing Artificial Intelligence (AI) in medical image interpretation can significantly improve early disease detection.
- A multi-modal AI model based on transfer learning (TL) was proposed, combining Xception and GMRNN architectures for wound classification with high accuracy.
- The model incorporated features extracted by TL algorithm and location features for classifying diabetic, pressure, surgical, and venous ulcers.
- The proposed methodology showcased exceptional accuracy in accurately classifying the most commonly occurring wound types using images and their locations. <div>
arXiv:2505.08086v1 Announce Type: new 
Abstract: The effective diagnosis of acute and hard-to-heal wounds is crucial for wound care practitioners to provide effective patient care. Poor clinical outcomes are often linked to infection, peripheral vascular disease, and increasing wound depth, which collectively exacerbate these comorbidities. However, diagnostic tools based on Artificial Intelligence (AI) speed up the interpretation of medical images and improve early detection of disease. In this article, we propose a multi-modal AI model based on transfer learning (TL), which combines two state-of-the-art architectures, Xception and GMRNN, for wound classification. The multi-modal network is developed by concatenating the features extracted by a transfer learning algorithm and location features to classify the wound types of diabetic, pressure, surgical, and venous ulcers. The proposed method is comprehensively compared with deep neural networks (DNN) for medical image analysis. The experimental results demonstrate a notable wound-class classifications (containing only diabetic, pressure, surgical, and venous) vary from 78.77 to 100\% in various experiments. The results presented in this study showcase the exceptional accuracy of the proposed methodology in accurately classifying the most commonly occurring wound types using wound images and their corresponding locations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing</title>
<link>https://arxiv.org/abs/2505.08101</link>
<guid>https://arxiv.org/abs/2505.08101</guid>
<content:encoded><![CDATA[
<div> Point cloud processing, autonomous driving, 3D object recognition, Point Transformer V3, distillation framework<br />
<br />
Summary: 
This paper introduces a novel distillation framework for transferring knowledge from a high-capacity teacher model to a lightweight student model in point cloud processing applications. The framework leverages topology-aware representations and gradient-guided knowledge distillation to capture geometric structures and guide the learning process effectively. Experimental results on Nuscenes, SemanticKITTI, and Waymo datasets show competitive performance with a 16x reduction in model size and nearly 1.9x decrease in inference time compared to the teacher model. The proposed method achieves state-of-the-art performance in segmentation on NuScenes, surpassing prior knowledge distillation baselines in LiDAR data training. Available publicly on GitHub at: https://github.com/HySonLab/PointDistill<br /><br /> <div>
arXiv:2505.08101v1 Announce Type: new 
Abstract: Point cloud processing has gained significant attention due to its critical role in applications such as autonomous driving and 3D object recognition. However, deploying high-performance models like Point Transformer V3 in resource-constrained environments remains challenging due to their high computational and memory demands. This work introduces a novel distillation framework that leverages topology-aware representations and gradient-guided knowledge distillation to effectively transfer knowledge from a high-capacity teacher to a lightweight student model. Our approach captures the underlying geometric structures of point clouds while selectively guiding the student model's learning process through gradient-based feature alignment. Experimental results in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the proposed method achieves competitive performance, with an approximately 16x reduction in model size and a nearly 1.9x decrease in inference time compared to its teacher model. Notably, on NuScenes, our method achieves state-of-the-art performance among knowledge distillation techniques trained solely on LiDAR data, surpassing prior knowledge distillation baselines in segmentation performance. Our implementation is available publicly at:
  https://github.com/HySonLab/PointDistill
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors</title>
<link>https://arxiv.org/abs/2505.08111</link>
<guid>https://arxiv.org/abs/2505.08111</guid>
<content:encoded><![CDATA[
<div> sleep position classification, pressure-sensitive mats, deep learning, transfer learning, polysomnography

Summary:
The study focuses on classifying four-way sleep positions using data from pressure-sensitive mats (PSMs) placed under mattresses in a sleep clinic. Sleep positions can impact sleep quality and the prevalence of sleep disorders like apnea. The researchers used transfer learning techniques to adapt pre-trained deep learning models for accurate sleep position estimation from a low-resolution PSM dataset collected in a polysomnography sleep lab. They leveraged Vision Transformer models pre-trained on ImageNet and a model for human pose estimation. Their approach surpassed previous methods using deep learning and traditional machine learning models. The performance was evaluated on 112 nights of patient recordings and validated on a higher-resolution dataset from 13 patients. Despite challenges with low-resolution data, the approach shows promise for real-world application in clinical settings.<br /><br />Summary: <div>
arXiv:2505.08111v1 Announce Type: new 
Abstract: Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way of monitoring patients during sleep. We focus on four-way sleep position classification using data collected from a PSM placed under a mattress in a sleep clinic. Sleep positions can affect sleep quality and the prevalence of sleep disorders, such as apnea. Measurements were performed on patients with suspected sleep disorders referred for assessments at a sleep clinic. Training deep learning models can be challenging in clinical settings due to the need for large amounts of labeled data. To overcome the shortage of labeled training data, we utilize transfer learning to adapt pre-trained deep learning models to accurately estimate sleep positions from a low-resolution PSM dataset collected in a polysomnography sleep lab. Our approach leverages Vision Transformer models pre-trained on ImageNet using masked autoencoding (ViTMAE) and a pre-trained model for human pose estimation (ViTPose). These approaches outperform previous work from PSM-based sleep pose classification using deep learning (TCN) as well as traditional machine learning models (SVM, XGBoost, Random Forest) that use engineered features. We evaluate the performance of sleep position classification from 112 nights of patient recordings and validate it on a higher resolution 13-patient dataset. Despite the challenges of differentiating between sleep positions from low-resolution PSM data, our approach shows promise for real-world deployment in clinical settings
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Now you see it, Now you don't: Damage Label Agreement in Drone &amp; Satellite Post-Disaster Imagery</title>
<link>https://arxiv.org/abs/2505.08117</link>
<guid>https://arxiv.org/abs/2505.08117</guid>
<content:encoded><![CDATA[
<div> Keywords: damage labels, satellite imagery, drone imagery, machine learning, building damage assessment

Summary: 
This paper examines discrepancies in damage labels from satellite and drone aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey. The analysis reveals a significant 29.02% label disagreement between the sources, highlighting potential risks in deploying machine learning damage assessment systems. Satellite-derived labels were found to under-report damage by at least 20.43% compared to drone-derived labels, indicating a misrepresentation of actual conditions. The differing distributions of satellite and drone-derived labels suggest that computer vision and machine learning models trained on either dataset may not accurately represent real-world scenarios. To address these issues and prevent potential societal harm, the paper offers four recommendations for enhancing reliability and transparency in deploying CV/ML damage assessment systems. 

<br /><br />Summary: <div>
arXiv:2505.08117v1 Announce Type: new 
Abstract: This paper audits damage labels derived from coincident satellite and drone aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey, finding 29.02% label disagreement and significantly different distributions between the two sources, which presents risks and potential harms during the deployment of machine learning damage assessment systems. Currently, there is no known study of label agreement between drone and satellite imagery for building damage assessment. The only prior work that could be used to infer if such imagery-derived labels agree is limited by differing damage label schemas, misaligned building locations, and low data quantities. This work overcomes these limitations by comparing damage labels using the same damage label schemas and building locations from three hurricanes, with the 15,814 buildings representing 19.05 times more buildings considered than the most relevant prior work. The analysis finds satellite-derived labels significantly under-report damage by at least 20.43% compared to drone-derived labels (p<1.2x10^-117), and satellite- and drone-derived labels represent significantly different distributions (p<5.1x10^-175). This indicates that computer vision and machine learning (CV/ML) models trained on at least one of these distributions will misrepresent actual conditions, as the differing satellite and drone-derived distributions cannot simultaneously represent the distribution of actual conditions in a scene. This potential misrepresentation poses ethical risks and potential societal harm if not managed. To reduce the risk of future societal harms, this paper offers four recommendations to improve reliability and transparency to decisio-makers when deploying CV/ML damage assessment systems in practice
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections</title>
<link>https://arxiv.org/abs/2505.08123</link>
<guid>https://arxiv.org/abs/2505.08123</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-material decomposition, SEMMD, JSover, spectral CT, implicit neural representation

Summary: 
JSover is a new one-step framework for multi-material decomposition (MMD) in CT imaging that improves the accuracy and efficiency of traditional methods. Unlike previous two-step approaches, JSover simultaneously reconstructs tissue compositions and estimates the energy spectrum directly from single-energy CT projections. By incorporating physics-informed spectral priors, it creates a virtual spectral CT system from SE acquisitions, reducing beam hardening artifacts. Additionally, JSover leverages implicit neural representation (INR) to enhance material map estimation using unsupervised deep learning. Experimental results demonstrate that JSover surpasses existing SEMMD techniques in accuracy and computational performance on both simulated and real CT datasets. This advancement in MMD technology holds potential for a wide range of clinical applications by enabling more reliable quantitative tissue composition reconstructions from single-energy CT scans.<br /><br />Summary: <div>
arXiv:2505.08123v1 Announce Type: new 
Abstract: Multi-material decomposition (MMD) enables quantitative reconstruction of tissue compositions in the human body, supporting a wide range of clinical applications. However, traditional MMD typically requires spectral CT scanners and pre-measured X-ray energy spectra, significantly limiting clinical applicability. To this end, various methods have been developed to perform MMD using conventional (i.e., single-energy, SE) CT systems, commonly referred to as SEMMD. Despite promising progress, most SEMMD methods follow a two-step image decomposition pipeline, which first reconstructs monochromatic CT images using algorithms such as FBP, and then performs decomposition on these images. The initial reconstruction step, however, neglects the energy-dependent attenuation of human tissues, introducing severe nonlinear beam hardening artifacts and noise into the subsequent decomposition. This paper proposes JSover, a fundamentally reformulated one-step SEMMD framework that jointly reconstructs multi-material compositions and estimates the energy spectrum directly from SECT projections. By explicitly incorporating physics-informed spectral priors into the SEMMD process, JSover accurately simulates a virtual spectral CT system from SE acquisitions, thereby improving the reliability and accuracy of decomposition. Furthermore, we introduce implicit neural representation (INR) as an unsupervised deep learning solver for representing the underlying material maps. The inductive bias of INR toward continuous image patterns constrains the solution space and further enhances estimation quality. Extensive experiments on both simulated and real CT datasets show that JSover outperforms state-of-the-art SEMMD methods in accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLAG: Scalable Language-Augmented Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.08124</link>
<guid>https://arxiv.org/abs/2505.08124</guid>
<content:encoded><![CDATA[
<div> framework, language-augmented, scene representations, Gaussian splatting, scalability
Summary:
SLAG is a multi-GPU framework that accelerates the encoding of language-augmented scene representations for robotics applications. By integrating 2D visual-language model features into 3D scenes using SAM and CLIP, SLAG eliminates the need for loss functions and enables highly parallelized scene encoding. The method achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while maintaining embedding quality on datasets like ScanNet and LERF. Additionally, SLAG introduces a vector database for efficient embedding storage and retrieval, making it suitable for time-sensitive and data-intensive scenarios. The framework holds promise for use in search-and-rescue operations, smart cities, mining, and other large-scale robotics applications. Visit the project website for more information: https://slag-project.github.io/. 

<br /><br />Summary: <div>
arXiv:2505.08124v1 Announce Type: new 
Abstract: Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asynchronous Multi-Object Tracking with an Event Camera</title>
<link>https://arxiv.org/abs/2505.08126</link>
<guid>https://arxiv.org/abs/2505.08126</guid>
<content:encoded><![CDATA[
<div> Event cameras, Asynchronous Event Multi-Object Tracking, AEMOT, object detection, object tracking<br />
Summary:<br />
Events cameras with their low latency output, high temporal resolution, and dynamic range, are ideal for detecting and tracking objects in dynamic environments. The Asynchronous Event Multi-Object Tracking (AEMOT) algorithm processes individual raw events asynchronously to detect and track multiple objects. By identifying salient event blob features through optical flow analysis, AEMOT tracks candidate objects using the Asynchronous Event Blob (AEB) tracker. A novel validation stage distinguishes and estimates object characteristics such as position, velocity, size, and orientation in real time. Tested on the Bee Swarm Dataset, AEMOT outperforms other event-based detection and tracking algorithms with precision and recall rates exceeding 37%. The algorithm and the labeled event dataset will be made open source. <br />Summary: <div>
arXiv:2505.08126v1 Announce Type: new 
Abstract: Events cameras are ideal sensors for enabling robots to detect and track objects in highly dynamic environments due to their low latency output, high temporal resolution, and high dynamic range. In this paper, we present the Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and tracking multiple objects by processing individual raw events asynchronously. AEMOT detects salient event blob features by identifying regions of consistent optical flow using a novel Field of Active Flow Directions built from the Surface of Active Events. Detected features are tracked as candidate objects using the recently proposed Asynchronous Event Blob (AEB) tracker in order to construct small intensity patches of each candidate object. A novel learnt validation stage promotes or discards candidate objects based on classification of their intensity patches, with promoted objects having their position, velocity, size, and orientation estimated at their event rate. We evaluate AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with precision and recall performance exceeding that of alternative event-based detection and tracking algorithms by over 37%. Source code and the labelled event Bee Swarm Dataset will be open sourced
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoKD: Multi-Task Optimization for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.08170</link>
<guid>https://arxiv.org/abs/2505.08170</guid>
<content:encoded><![CDATA[
<div> Knowledge Distillation, Multi-Task Optimization, Gradient Conflicts, Gradient Dominance, Image Classification, Object Detection <br />
<br />Summary: <br />Compact models can be effectively trained through Knowledge Distillation (KD), with two main challenges being balancing guidance from the teacher model and the task objective, and handling knowledge representation disparities between teacher and student models. To address these challenges, Multi-Task Optimization for Knowledge Distillation (MoKD) is proposed. MoKD reformulates KD as a multi-objective optimization problem to balance objectives and introduces a subspace learning framework to improve knowledge transfer by projecting feature representations into a high-dimensional space. Results from experiments on ImageNet-1K dataset and COCO dataset show that MoKD outperforms existing methods in image classification and object detection, achieving state-of-the-art performance with greater efficiency. Additionally, MoKD models achieve state-of-the-art performance compared to models trained from scratch. <div>
arXiv:2505.08170v1 Announce Type: new 
Abstract: Compact models can be effectively trained through Knowledge Distillation (KD), a technique that transfers knowledge from larger, high-performing teacher models. Two key challenges in Knowledge Distillation (KD) are: 1) balancing learning from the teacher's guidance and the task objective, and 2) handling the disparity in knowledge representation between teacher and student models. To address these, we propose Multi-Task Optimization for Knowledge Distillation (MoKD). MoKD tackles two main gradient issues: a) Gradient Conflicts, where task-specific and distillation gradients are misaligned, and b) Gradient Dominance, where one objective's gradient dominates, causing imbalance. MoKD reformulates KD as a multi-objective optimization problem, enabling better balance between objectives. Additionally, it introduces a subspace learning framework to project feature representations into a high-dimensional space, improving knowledge transfer. Our MoKD is demonstrated to outperform existing methods through extensive experiments on image classification using the ImageNet-1K dataset and object detection using the COCO dataset, achieving state-of-the-art performance with greater efficiency. To the best of our knowledge, MoKD models also achieve state-of-the-art performance compared to models trained from scratch.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification</title>
<link>https://arxiv.org/abs/2505.08173</link>
<guid>https://arxiv.org/abs/2505.08173</guid>
<content:encoded><![CDATA[
<div> causal inference, long-tail classification, Visual Transformers, TSCNet, multi-scale causal interventions <br />
<br />
Summary: 
This paper addresses the challenge of handling biases introduced by class imbalance in long-tail classification by investigating the influence of existing causal models on CNNs and Visual Transformers. The study shows that Visual Transformers' global feature representation makes it difficult for causal methods to model associations between fine-grained features and predictions, especially for tail classes with similar visual appearance. To tackle this issue, the paper proposes TSCNet, a two-stage causal modeling method that employs multi-scale causal interventions. The first stage focuses on hierarchical causal representation learning by decoupling background and objects and applying backdoor interventions. The second stage, counterfactual logits bias calibration, refines the model's decision boundary by constructing a counterfactual balanced data distribution. Experimental results demonstrate that TSCNet outperforms existing methods in eliminating biases caused by data imbalance. <br /><br /> <div>
arXiv:2505.08173v1 Announce Type: new 
Abstract: Causal inference has emerged as a promising approach to mitigate long-tail classification by handling the biases introduced by class imbalance. However, along with the change of advanced backbone models from Convolutional Neural Networks (CNNs) to Visual Transformers (ViT), existing causal models may not achieve an expected performance gain. This paper investigates the influence of existing causal models on CNNs and ViT variants, highlighting that ViT's global feature representation makes it hard for causal methods to model associations between fine-grained features and predictions, which leads to difficulties in classifying tail classes with similar visual appearance. To address these issues, this paper proposes TSCNet, a two-stage causal modeling method to discover fine-grained causal associations through multi-scale causal interventions. Specifically, in the hierarchical causal representation learning stage (HCRL), it decouples the background and objects, applying backdoor interventions at both the patch and feature level to prevent model from using class-irrelevant areas to infer labels which enhances fine-grained causal representation. In the counterfactual logits bias calibration stage (CLBC), it refines the optimization of model's decision boundary by adaptive constructing counterfactual balanced data distribution to remove the spurious associations in the logits caused by data distribution. Extensive experiments conducted on various long-tail benchmarks demonstrate that the proposed TSCNet can eliminate multiple biases introduced by data imbalance, which outperforms existing methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images</title>
<link>https://arxiv.org/abs/2505.08178</link>
<guid>https://arxiv.org/abs/2505.08178</guid>
<content:encoded><![CDATA[
<div> DGORNet, Disparity Estimation, Surgical Images, Occlusion, Monocular Depth<br />
<br />
Summary: <br />
The study introduces DGORNet, a novel Depth Guided Occlusion-Aware Disparity Refinement Network for improving stereo laparoscopic image disparity estimation. It addresses challenges such as occlusion and limited labeled data by leveraging monocular depth information and introducing a Position Embedding module for enhanced spatial context. The network also incorporates an Optical Flow Difference Loss for unlabeled data to enhance robustness in dynamic surgical scenes. Experimental results on the SCARED dataset demonstrate that DGORNet outperforms existing methods in terms of End-Point Error and Root Mean Squared Error, particularly in occlusion and texture-less regions. Ablation studies validate the significant contributions of the Position Embedding and Optical Flow Difference Loss in improving spatial and temporal consistency. DGORNet proves to be an effective solution for enhancing disparity estimation in laparoscopic surgery, providing practical insights for addressing challenges in disparity estimation and data scarcity. <div>
arXiv:2505.08178v1 Announce Type: new 
Abstract: Occlusion and the scarcity of labeled surgical data are significant challenges in disparity estimation for stereo laparoscopic images. To address these issues, this study proposes a Depth Guided Occlusion-Aware Disparity Refinement Network (DGORNet), which refines disparity maps by leveraging monocular depth information unaffected by occlusion. A Position Embedding (PE) module is introduced to provide explicit spatial context, enhancing the network's ability to localize and refine features. Furthermore, we introduce an Optical Flow Difference Loss (OFDLoss) for unlabeled data, leveraging temporal continuity across video frames to improve robustness in dynamic surgical scenes. Experiments on the SCARED dataset demonstrate that DGORNet outperforms state-of-the-art methods in terms of End-Point Error (EPE) and Root Mean Squared Error (RMSE), particularly in occlusion and texture-less regions. Ablation studies confirm the contributions of the Position Embedding and Optical Flow Difference Loss, highlighting their roles in improving spatial and temporal consistency. These results underscore DGORNet's effectiveness in enhancing disparity estimation for laparoscopic surgery, offering a practical solution to challenges in disparity estimation and data limitations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models</title>
<link>https://arxiv.org/abs/2505.08190</link>
<guid>https://arxiv.org/abs/2505.08190</guid>
<content:encoded><![CDATA[
<div> diffusion models, image inpainting, raindrop removal, single image, Generative Adversarial Network <br />
<br />
Summary:
Raindrop removal from images is a challenging task, especially when relying on a single image. Common approaches involve detecting raindrop regions and then restoring the background based on these detections. The use of Generative Adversarial Networks (GANs) is prevalent for background restoration. However, recent advancements in diffusion models have improved image inpainting techniques significantly. This paper introduces a novel method for raindrop removal using diffusion-based image inpainting from a single image. By leveraging the capabilities of diffusion models, this technique offers a promising approach for effectively removing raindrops from images in a single-step process. <div>
arXiv:2505.08190v1 Announce Type: new 
Abstract: Raindrop removal is a challenging task in image processing. Removing raindrops while relying solely on a single image further increases the difficulty of the task. Common approaches include the detection of raindrop regions in the image, followed by performing a background restoration process conditioned on those regions. While various methods can be applied for the detection step, the most common architecture used for background restoration is the Generative Adversarial Network (GAN). Recent advances in the use of diffusion models have led to state-of-the-art image inpainting techniques. In this paper, we introduce a novel technique for raindrop removal from a single image using diffusion-based image inpainting.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction</title>
<link>https://arxiv.org/abs/2505.08196</link>
<guid>https://arxiv.org/abs/2505.08196</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian Splatting, Dynamic Scene Reconstruction, Anchor-Based Structure, Rate-Distortion Optimization, Hierarchical Pipeline

Summary: 
ADC-GS introduces a novel approach for dynamic scene reconstruction by organizing Gaussian primitives into an anchor-based structure within a canonical space, improving performance by reducing redundancy and optimizing representation. It utilizes a hierarchical pipeline to capture motions at varying granularities and adopts rate-distortion optimization to achieve an optimal balance between bitrate consumption and representation fidelity. The method outperforms existing approaches in rendering speed by 300%-800% while maintaining state-of-the-art storage efficiency and rendering quality. The code for ADC-GS is available on GitHub at https://github.com/H-Huang774/ADC-GS.git. <div>
arXiv:2505.08196v1 Announce Type: new 
Abstract: Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from a canonical space to target frames, which overlooks redundancy among adjacent Gaussian primitives and results in suboptimal performance. To address this limitation, we propose Anchor-Driven Deformable and Compressed Gaussian Splatting (ADC-GS), a compact and efficient representation for dynamic scene reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an anchor-based structure within the canonical space, enhanced by a temporal significance-based anchor refinement strategy. To reduce deformation redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that captures motions at varying granularities. Moreover, a rate-distortion optimization is adopted to achieve an optimal balance between bitrate consumption and representation fidelity. Experimental results demonstrate that ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed by 300%-800% while achieving state-of-the-art storage efficiency without compromising rendering quality. The code is released at https://github.com/H-Huang774/ADC-GS.git.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Watermarking in the Era of Diffusion Models: Advances and Challenges</title>
<link>https://arxiv.org/abs/2505.08197</link>
<guid>https://arxiv.org/abs/2505.08197</guid>
<content:encoded><![CDATA[
<div> Keywords: generative artificial intelligence, stable diffusion, visual watermarks, deepfake detection, diffusion models

Summary: 
This paper discusses the implications of advanced generative artificial intelligence technologies like Stable Diffusion on copyright infringement and the need for robust protection mechanisms such as visual watermarks. Traditional deepfake detection methods are ineffective against sophisticated manipulations, but diffusion models offer enhanced detection accuracy by learning features and embedding imperceptible watermarks. The integration of diffusion models and watermarking security is crucial in preserving ownership rights in the face of evolving forgery threats. The analysis focuses on the strengths and challenges of watermark techniques in the context of diffusion models, highlighting the importance of innovative solutions to protect digital content. The research aims to advance the discourse on safeguarding visual content and ensuring the integrity of ownership in the age of generative AI. 

<br /><br />Summary: <div>
arXiv:2505.08197v1 Announce Type: new 
Abstract: As generative artificial intelligence technologies like Stable Diffusion advance, visual content becomes more vulnerable to misuse, raising concerns about copyright infringement. Visual watermarks serve as effective protection mechanisms, asserting ownership and deterring unauthorized use. Traditional deepfake detection methods often rely on passive techniques that struggle with sophisticated manipulations. In contrast, diffusion models enhance detection accuracy by allowing for the effective learning of features, enabling the embedding of imperceptible and robust watermarks. We analyze the strengths and challenges of watermark techniques related to diffusion models, focusing on their robustness and application in watermark generation. By exploring the integration of advanced diffusion models and watermarking security, we aim to advance the discourse on preserving watermark robustness against evolving forgery threats. It emphasizes the critical importance of developing innovative solutions to protect digital content and ensure the preservation of ownership rights in the era of generative AI.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix</title>
<link>https://arxiv.org/abs/2505.08228</link>
<guid>https://arxiv.org/abs/2505.08228</guid>
<content:encoded><![CDATA[
<div> Keywords: object detection, adverse weather conditions, data augmentation, autonomous driving, robustness

Summary:<br /><br />Enhancing the robustness of object detection systems in adverse weather conditions is crucial for autonomous driving technology. The study introduces a novel approach using the diffusion model Instruct Pix2Pix to develop prompting methodologies for generating realistic datasets with weather-based augmentations. This aims to mitigate the impact of challenging weather on state-of-the-art object detection models like Faster R-CNN and YOLOv10. Experiments conducted in the CARLA simulator and real-world datasets BDD100K and ACDC demonstrate the effectiveness of the approach. The key contributions include quantifying the performance gap in object detection models in difficult weather conditions and showcasing how tailored data augmentation strategies can enhance model robustness. This research lays a solid foundation for enhancing perception systems in demanding environmental scenarios, offering pathways for future advancements in autonomous driving technology. 

Summary: <div>
arXiv:2505.08228v1 Announce Type: new 
Abstract: Enhancing the robustness of object detection systems under adverse weather conditions is crucial for the advancement of autonomous driving technology. This study presents a novel approach leveraging the diffusion model Instruct Pix2Pix to develop prompting methodologies that generate realistic datasets with weather-based augmentations aiming to mitigate the impact of adverse weather on the perception capabilities of state-of-the-art object detection models, including Faster R-CNN and YOLOv10. Experiments were conducted in two environments, in the CARLA simulator where an initial evaluation of the proposed data augmentation was provided, and then on the real-world image data sets BDD100K and ACDC demonstrating the effectiveness of the approach in real environments.
  The key contributions of this work are twofold: (1) identifying and quantifying the performance gap in object detection models under challenging weather conditions, and (2) demonstrating how tailored data augmentation strategies can significantly enhance the robustness of these models. This research establishes a solid foundation for improving the reliability of perception systems in demanding environmental scenarios, and provides a pathway for future advancements in autonomous driving.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HMPNet: A Feature Aggregation Architecture for Maritime Object Detection from a Shipborne Perspective</title>
<link>https://arxiv.org/abs/2505.08231</link>
<guid>https://arxiv.org/abs/2505.08231</guid>
<content:encoded><![CDATA[
<div> Object detection, maritime navigation, dataset, HMPNet, shipborne object detection <br />
Summary: 
- The article introduces Navigation12, a dataset for object detection in maritime environments, addressing the lack of maritime-specific data.
- HMPNet, a lightweight architecture, is proposed for shipborne object detection, with improved accuracy and computational efficiency compared to existing methods.
- HMPNet incorporates a hierarchical dynamic modulation backbone for feature aggregation, a matrix cascading poly-scale neck, and a polymerization weight sharing detector for efficient multi-scale feature aggregation.
- Empirical evaluations show HMPNet outperforms the current state-of-the-art model YOLOv11n, with a 3.3% improvement in mean Average Precision and 23% reduction in parameters.
- The proposed dataset and architecture aim to enhance visual perception techniques in autonomous maritime navigation systems. 
<br /><br />Summary: <div>
arXiv:2505.08231v1 Announce Type: new 
Abstract: In the realm of intelligent maritime navigation, object detection from a shipborne perspective is paramount. Despite the criticality, the paucity of maritime-specific data impedes the deployment of sophisticated visual perception techniques, akin to those utilized in autonomous vehicular systems, within the maritime context. To bridge this gap, we introduce Navigation12, a novel dataset annotated for 12 object categories under diverse maritime environments and weather conditions. Based upon this dataset, we propose HMPNet, a lightweight architecture tailored for shipborne object detection. HMPNet incorporates a hierarchical dynamic modulation backbone to bolster feature aggregation and expression, complemented by a matrix cascading poly-scale neck and a polymerization weight sharing detector, facilitating efficient multi-scale feature aggregation. Empirical evaluations indicate that HMPNet surpasses current state-of-the-art methods in terms of both accuracy and computational efficiency, realizing a 3.3% improvement in mean Average Precision over YOLOv11n, the prevailing model, and reducing parameters by 23%.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition</title>
<link>https://arxiv.org/abs/2505.08233</link>
<guid>https://arxiv.org/abs/2505.08233</guid>
<content:encoded><![CDATA[
<div> Keyword: contactless fingerprint recognition, G-MSGINet, minutiae localization, identity embedding, robust

Summary:
G-MSGINet is a new framework for contactless fingerprint recognition that integrates minutiae localization and identity embedding directly from raw input images. It introduces the GMSGI layer, which combines pixel-level involution, dynamic multi-scale kernel generation, and graph-based relational modeling. This approach eliminates the need for orientation supervision and adapts graph connectivity from learned kernel descriptors. Experimental results on benchmark datasets show that G-MSGINet achieves high minutiae F1-scores and Rank-1 identification accuracies, with an Equal Error Rate as low as 0.5%. It outperforms previous methods in terms of F1-score and accuracy while using fewer parameters and floating-point operations. The results demonstrate the scalability and effectiveness of G-MSGINet in real-world contactless biometric recognition scenarios.

Summary:<br /><br />Keyword: contactless fingerprint recognition, G-MSGINet, minutiae localization, identity embedding, robust <div>
arXiv:2505.08233v1 Announce Type: new 
Abstract: This paper presents G-MSGINet, a unified and efficient framework for robust contactless fingerprint recognition that jointly performs minutiae localization and identity embedding directly from raw input images. Existing approaches rely on multi-branch architectures, orientation labels, or complex preprocessing steps, which limit scalability and generalization across real-world acquisition scenarios. In contrast, the proposed architecture introduces the GMSGI layer, a novel computational module that integrates grouped pixel-level involution, dynamic multi-scale kernel generation, and graph-based relational modelling into a single processing unit. Stacked GMSGI layers progressively refine both local minutiae-sensitive features and global topological representations through end-to-end optimization. The architecture eliminates explicit orientation supervision and adapts graph connectivity directly from learned kernel descriptors, thereby capturing meaningful structural relationships among fingerprint regions without fixed heuristics. Extensive experiments on three benchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that G-MSGINet consistently achieves minutiae F1-scores in the range of $0.83\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%, while maintaining an Equal Error Rate (EER) as low as 0.5%. These results correspond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1 accuracy when compared to prior methods, using only 0.38 million parameters and 6.63 giga floating-point operations, which represents up to ten times fewer parameters than competitive baselines. This highlights the scalability and effectiveness of G-MSGINet in real-world contactless biometric recognition scenarios.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Removing Watermarks with Partial Regeneration using Semantic Information</title>
<link>https://arxiv.org/abs/2505.08234</link>
<guid>https://arxiv.org/abs/2505.08234</guid>
<content:encoded><![CDATA[
<div> Vulnerability, SemanticRegen, Watermarking, Adversarial Attack, Image Manipulation

Summary:
SemanticRegen is a three-stage, label-free attack that targets state-of-the-art semantic and invisible watermarks by erasing them while preserving an image's apparent meaning. The pipeline utilizes a vision-language model to obtain captions, zero-shot segmentation for foreground masks, and inpainting with an LLM-guided diffusion model to maintain salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems, SemanticRegen successfully defeats the semantic TreeRing watermark and reduces bit-accuracy for the remaining schemes while maintaining high perceptual quality. The attack achieves up to 12 percent higher masked SSIM in foreground regions compared to prior diffusion-based attackers. These results underscore the gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, emphasizing the need for watermarking algorithms resilient to content-preserving regenerative attacks.<br /><br />Summary: <div>
arXiv:2505.08234v1 Announce Type: new 
Abstract: As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged as a primary line of defense for copyright and provenance. The newest watermarking schemes embed semantic signals - content-aware patterns that are designed to survive common image manipulations - yet their true robustness against adaptive adversaries remains under-explored. We expose a previously unreported vulnerability and introduce SemanticRegen, a three-stage, label-free attack that erases state-of-the-art semantic and invisible watermarks while leaving an image's apparent meaning intact. Our pipeline (i) uses a vision-language model to obtain fine-grained captions, (ii) extracts foreground masks with zero-shot segmentation, and (iii) inpaints only the background via an LLM-guided diffusion model, thereby preserving salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing, StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy below 0.75 for the remaining schemes, all while maintaining high perceptual quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM) to quantify fidelity within foreground regions, showing that our attack achieves up to 12 percent higher mSSIM than prior diffusion-based attackers. These results highlight an urgent gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, underscoring the need for watermarking algorithms that are resilient to content-preserving regenerative attacks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation</title>
<link>https://arxiv.org/abs/2505.08235</link>
<guid>https://arxiv.org/abs/2505.08235</guid>
<content:encoded><![CDATA[
<div> diffusion model, event camera, video frame interpolation, denoising, event-based

Summary:
EventDiff is a novel event-based diffusion model framework for Video Frame Interpolation (VFI). It incorporates a lightweight Spatial-Temporal Cross Attention (STCA) module to fuse dynamic event streams with static frames, allowing for direct interpolation in the latent space through a denoising diffusion process. The method surpasses existing event-based VFI approaches by up to 1.98dB in PSNR on Vimeo90K-Triplet dataset and demonstrates superior performance on SNU-FILM tasks. In comparison to diffusion-based VFI methods, EventDiff achieves up to 5.72dB PSNR improvement on Vimeo90K-Triplet and is 4.24 times faster during inference. Through a two-stage training strategy involving pretraining the Event-Frame Hybrid AutoEncoder (HAE) and then jointly optimizing it with the diffusion model, EventDiff achieves state-of-the-art performance on multiple synthetic and real-world event VFI datasets. <br /><br />Summary: <div>
arXiv:2505.08235v1 Announce Type: new 
Abstract: Video Frame Interpolation (VFI) is a fundamental yet challenging task in computer vision, particularly under conditions involving large motion, occlusion, and lighting variation. Recent advancements in event cameras have opened up new opportunities for addressing these challenges. While existing event-based VFI methods have succeeded in recovering large and complex motions by leveraging handcrafted intermediate representations such as optical flow, these designs often compromise high-fidelity image reconstruction under subtle motion scenarios due to their reliance on explicit motion modeling. Meanwhile, diffusion models provide a promising alternative for VFI by reconstructing frames through a denoising process, eliminating the need for explicit motion estimation or warping operations. In this work, we propose EventDiff, a unified and efficient event-based diffusion model framework for VFI. EventDiff features a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic event streams with static frames. Unlike previous event-based VFI methods, EventDiff performs interpolation directly in the latent space via a denoising diffusion process, making it more robust across diverse and challenging VFI scenarios. Through a two-stage training strategy that first pretrains the HAE and then jointly optimizes it with the diffusion model, our method achieves state-of-the-art performance across multiple synthetic and real-world event VFI datasets. The proposed method outperforms existing state-of-the-art event-based VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior performance in SNU-FILM tasks with multiple difficulty levels. Compared to the emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR gain on Vimeo90K-Triplet and 4.24X faster inference.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Congenital Heart Disease recognition using Deep Learning/Transformer models</title>
<link>https://arxiv.org/abs/2505.08242</link>
<guid>https://arxiv.org/abs/2505.08242</guid>
<content:encoded><![CDATA[
<div> Keywords: Congenital Heart Disease, deep learning, dual-modality, diagnosis, accuracy 

Summary:
Deep learning models are investigated for their effectiveness in diagnosing Congenital Heart Disease (CHD), a major cause of infant morbidity and mortality. The study focuses on utilizing dual-modality (sound and image) deep learning methods for CHD screening. The research achieves 73.9% accuracy on the ZCHSound dataset and 80.72% accuracy on the DICOM Chest X-ray dataset. By leveraging the automatic feature extraction abilities of deep learning, doctors can improve the accuracy of CHD detection, reducing the risk of false negatives commonly seen in current non-invasive screening methods. This innovative approach showcases the potential of deep learning in assisting healthcare professionals in diagnosing CHD more effectively and accurately. <div>
arXiv:2505.08242v1 Announce Type: new 
Abstract: Congenital Heart Disease (CHD) remains a leading cause of infant morbidity and mortality, yet non-invasive screening methods often yield false negatives. Deep learning models, with their ability to automatically extract features, can assist doctors in detecting CHD more effectively. In this work, we investigate the use of dual-modality (sound and image) deep learning methods for CHD diagnosis. We achieve 73.9% accuracy on the ZCHSound dataset and 80.72% accuracy on the DICOM Chest X-ray dataset.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Memorization of Diffusion Models through p-Laplace Analysis</title>
<link>https://arxiv.org/abs/2505.08246</link>
<guid>https://arxiv.org/abs/2505.08246</guid>
<content:encoded><![CDATA[
<div> score function, gradient, diffusion models, p-Laplace operators, memorization identification

Summary:
This study explores the use of diffusion models in estimating score functions to compute higher-order differentials known as p-Laplace operators. By utilizing the estimated score functions, researchers were able to accurately identify memorized training data. A numerical p-Laplace approximation was proposed based on the learned score functions, proving effective in detecting important features of the probability landscape. The analysis focused on Gaussian mixture models but also extended to image generative models, marking the first instance of memorization identification using the p-Laplace operator in image generation. This research sheds light on the potential of leveraging score functions in diffusion models for advanced data analysis applications. <br /><br />Summary: <div>
arXiv:2505.08246v1 Announce Type: new 
Abstract: Diffusion models, today's leading image generative models, estimate the score function, i.e. the gradient of the log probability of (perturbed) data samples, without direct access to the underlying probability distribution. This work investigates whether the estimated score function can be leveraged to compute higher-order differentials, namely p-Laplace operators. We show here these operators can be employed to identify memorized training data. We propose a numerical p-Laplace approximation based on the learned score functions, showing its effectiveness in identifying key features of the probability landscape. We analyze the structured case of Gaussian mixture models, and demonstrate the results carry-over to image generative models, where memorization identification based on the p-Laplace operator is performed for the first time.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets</title>
<link>https://arxiv.org/abs/2505.08259</link>
<guid>https://arxiv.org/abs/2505.08259</guid>
<content:encoded><![CDATA[
<div> Convolutional, transformer, image classification, ResNet-18, Vision Transformer <br />
Summary: <br />
This study compares convolutional and transformer-based architectures for image classification, focusing on reducing inference latency and model complexity while maintaining accuracy. Using ResNet-18 as a baseline, four Vision Transformer variants (Tiny, Small, Base, Large) were fine-tuned on DermatologyMNIST and TinyImageNet datasets. Through hyperparameter tuning, the study shows that optimally fine-tuned Vision Transformers can match or exceed baseline performance, achieve faster inference, and operate with fewer parameters. This highlights the potential of Vision Transformers for applications in resource-constrained environments. <div>
arXiv:2505.08259v1 Announce Type: new 
Abstract: This study evaluates the trade-offs between convolutional and transformer-based architectures on both medical and general-purpose image classification benchmarks. We use ResNet-18 as our baseline and introduce a fine-tuning strategy applied to four Vision Transformer variants (Tiny, Small, Base, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce inference latency and model complexity with acceptable accuracy degradation. Through systematic hyperparameter variations, we demonstrate that appropriately fine-tuned Vision Transformers can match or exceed the baseline's performance, achieve faster inference, and operate with fewer parameters, highlighting their viability for deployment in resource-constrained environments.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Novel Category Discovery</title>
<link>https://arxiv.org/abs/2505.08260</link>
<guid>https://arxiv.org/abs/2505.08260</guid>
<content:encoded><![CDATA[
<div> discovery, transductive learning, few-shot learning, clustering, model adaptability
Summary: 
This paper introduces the Few-Shot Novel Category Discovery (FSNCD) setting, where a trained agent can switch between identifying known classes and clustering novel classes with only a few support examples. Two methods, Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means Clustering (UKC), are proposed to enhance the model's reasoning capabilities. The framework extends prior-based clustering algorithms to real-world open set scenarios, improving model adaptability in few-shot learning. Extensive experiments on five datasets show that the methods achieve high performance across various task settings and scenarios. <div>
arXiv:2505.08260v1 Announce Type: new 
Abstract: The recently proposed Novel Category Discovery (NCD) adapt paradigm of transductive learning hinders its application in more real-world scenarios. In fact, few labeled data in part of new categories can well alleviate this burden, which coincides with the ease that people can label few of new category data. Therefore, this paper presents a new setting in which a trained agent is able to flexibly switch between the tasks of identifying examples of known (labelled) classes and clustering novel (completely unlabeled) classes as the number of query examples increases by leveraging knowledge learned from only a few (handful) support examples. Drawing inspiration from the discovery of novel categories using prior-based clustering algorithms, we introduce a novel framework that further relaxes its assumptions to the real-world open set level by unifying the concept of model adaptability in few-shot learning. We refer to this setting as Few-Shot Novel Category Discovery (FSNCD) and propose Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means Clustering (UKC) to examine the model's reasoning capabilities. Extensive experiments and detailed analysis on five commonly used datasets demonstrate that our methods can achieve leading performance levels across different task settings and scenarios.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction</title>
<link>https://arxiv.org/abs/2505.08266</link>
<guid>https://arxiv.org/abs/2505.08266</guid>
<content:encoded><![CDATA[
<div> MPNN, SFs, link prediction, Graph Vision Network, visual perception <br />
<br />
Summary: 
Message-passing graph neural networks (MPNNs) and structural features (SFs) are crucial for link prediction. However, the potential of visual perception has been overlooked in MPNNs. The Graph Vision Network (GVN) framework integrates vision structural awareness into MPNNs for the first time. An efficient variant, E-GVN, is also proposed. Empirical results show that GVN consistently benefits from visual enhancement across various link prediction datasets, including large-scale graphs. GVN outperforms existing state-of-the-art methods, indicating a promising direction for link prediction. <div>
arXiv:2505.08266v1 Announce Type: new 
Abstract: Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called Graph Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping</title>
<link>https://arxiv.org/abs/2505.08273</link>
<guid>https://arxiv.org/abs/2505.08273</guid>
<content:encoded><![CDATA[
<div> satellite imagery, irrigation mapping, dataset, deep learning, agriculture <br />
Summary: 
IrrMap is a large-scale dataset comprising 1.1 million patches for irrigation method mapping across regions. The dataset includes multi-resolution satellite imagery from LandSat and Sentinel, along with auxiliary data such as crop type and vegetation indices. With standardized GeoTIFF patches and multiple input modalities, IrrMap is ML-ready for seamless deep learning model training. The dataset covers 1,687,899 farms and 14,117,330 acres in multiple western U.S. states from 2013 to 2023, enabling comprehensive irrigation analysis. The dataset's complete pipeline allows for easy extension to new regions for irrigation data collection. Analysis of irrigation method distribution, spatial patterns, and irrigated area variations provides insights into regional and resolution-based differences. IrrMap is openly released with benchmark models and pipeline code on GitHub and a data repository for further exploration in agricultural and geospatial analysis. <br /> <div>
arXiv:2505.08273v1 Announce Type: new 
Abstract: We introduce IrrMap, the first large-scale dataset (1.1 million patches) for irrigation method mapping across regions. IrrMap consists of multi-resolution satellite imagery from LandSat and Sentinel, along with key auxiliary data such as crop type, land use, and vegetation indices. The dataset spans 1,687,899 farms and 14,117,330 acres across multiple western U.S. states from 2013 to 2023, providing a rich and diverse foundation for irrigation analysis and ensuring geospatial alignment and quality control. The dataset is ML-ready, with standardized 224x224 GeoTIFF patches, the multiple input modalities, carefully chosen train-test-split data, and accompanying dataloaders for seamless deep learning model training andbenchmarking in irrigation mapping. The dataset is also accompanied by a complete pipeline for dataset generation, enabling researchers to extend IrrMap to new regions for irrigation data collection or adapt it with minimal effort for other similar applications in agricultural and geospatial analysis. We also analyze the irrigation method distribution across crop groups, spatial irrigation patterns (using Shannon diversity indices), and irrigated area variations for both LandSat and Sentinel, providing insights into regional and resolution-based differences. To promote further exploration, we openly release IrrMap, along with the derived datasets, benchmark models, and pipeline code, through a GitHub repository: https://github.com/Nibir088/IrrMap and Data repository: https://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and implementation details.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion</title>
<link>https://arxiv.org/abs/2505.08281</link>
<guid>https://arxiv.org/abs/2505.08281</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal image compression, residual-guided, semantic retrieval, diffusion-based generation, compression-aware diffusion model

Summary:
ResULIC is a new residual-guided ultra-low-rate image compression framework that integrates semantic retrieval, residual signals, and a compression-aware diffusion model for improved reconstruction fidelity and coding efficiency. It introduces Semantic Residual Coding (SRC) to capture semantic disparities and uses a perceptual fidelity optimizer for high-quality reconstruction. The Compression-aware Diffusion Model (CDM) aligns bitrates with diffusion time steps to enhance compression-reconstruction synergy. Experimental results show ResULIC outperforms state-of-the-art methods, achieving significant improvements in LPIPS and FID metrics. This approach offers a comprehensive solution for high-quality image compression by incorporating residual signals, semantic retrieval, and a compression-aware diffusion model. <br /><br />Summary: <div>
arXiv:2505.08281v1 Announce Type: new 
Abstract: Existing multimodal large model-based image compression frameworks often rely on a fragmented integration of semantic retrieval, latent compression, and generative models, resulting in suboptimal performance in both reconstruction fidelity and coding efficiency. To address these challenges, we propose a residual-guided ultra lowrate image compression named ResULIC, which incorporates residual signals into both semantic retrieval and the diffusion-based generation process. Specifically, we introduce Semantic Residual Coding (SRC) to capture the semantic disparity between the original image and its compressed latent representation. A perceptual fidelity optimizer is further applied for superior reconstruction quality. Additionally, we present the Compression-aware Diffusion Model (CDM), which establishes an optimal alignment between bitrates and diffusion time steps, improving compression-reconstruction synergy. Extensive experiments demonstrate the effectiveness of ResULIC, achieving superior objective and subjective performance compared to state-of-the-art diffusion-based methods with - 80.7%, -66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at https: //njuvision.github.io/ResULIC/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disruptive Transformation of Artworks in Master-Disciple Relationships: The Case of Ukiyo-e Artworks</title>
<link>https://arxiv.org/abs/2505.08284</link>
<guid>https://arxiv.org/abs/2505.08284</guid>
<content:encoded><![CDATA[
<div> quantitative analysis, Ukiyo-e, creativity, machine learning, cultural evolution
Summary: 
- The research focuses on using machine learning to conduct a quantitative analysis of creativity in Ukiyo-e, a traditional Japanese art form, using a large database of high-resolution images.
- The study reveals that the overall creativity of Ukiyo-e has decreased with cultural maturation, but the style has become more segmented and maintained a high level of creativity.
- The analysis provides new insights into the study of Ukiyo-e and demonstrates its role in the ongoing cultural history of Eastern art.
<br /><br />Summary: <div>
arXiv:2505.08284v1 Announce Type: new 
Abstract: Artwork research has long relied on human sensibility and subjective judgment, but recent developments in machine learning have enabled the quantitative assessment of features that humans could not discover. In Western paintings, comprehensive analyses have been conducted from various perspectives in conjunction with large databases, but such extensive analysis has not been sufficiently conducted for Eastern paintings. Then, we focus on Ukiyo-e, a traditional Japanese art form, as a case study of Eastern paintings, and conduct a quantitative analysis of creativity in works of art using 11,000 high-resolution images. This involves using the concept of calculating creativity from networks to analyze both the creativity of the artwork and that of the artists. As a result, In terms of Ukiyo-e as a whole, it was found that the creativity of its appearance has declined with the maturation of culture, but in terms of style, it has become more segmented with the maturation of culture and has maintained a high level of creativity. This not only provides new insights into the study of Ukiyo-e but also shows how Ukiyo-e has evolved within the ongoing cultural history, playing a culturally significant role in the analysis of Eastern art.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units</title>
<link>https://arxiv.org/abs/2505.08294</link>
<guid>https://arxiv.org/abs/2505.08294</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, deepfakes, facial action units, multimodal manipulations, cross-modal fusion<br />
Summary: 
The article introduces FauForensics, a novel framework for detecting realistic audio-visual deepfakes by using biologically invariant facial action units (FAUs) to capture subtle dynamics disrupted in synthetic content. This approach reduces domain dependency and addresses multi-modal forgery issues. FauForensics computes frame-wise audiovisual similarities through a fusion module with learnable cross-modal queries, effectively aligning temporal-spatial lip-audio relationships. The experiments on FakeAVCeleb and LAV-DF datasets demonstrate state-of-the-art performance and superior cross-dataset generalizability, outperforming existing methods by up to an average of 4.83%. This innovative framework provides robust detection capabilities for multimodal manipulations in the evolving landscape of generative AI and deepfake threats.<br /><br />Summary: <div>
arXiv:2505.08294v1 Announce Type: new 
Abstract: The rapid evolution of generative AI has increased the threat of realistic audio-visual deepfakes, demanding robust detection methods. Existing solutions primarily address unimodal (audio or visual) forgeries but struggle with multimodal manipulations due to inadequate handling of heterogeneous modality features and poor generalization across datasets. To this end, we propose a novel framework called FauForensics by introducing biologically invariant facial action units (FAUs), which is a quantitative descriptor of facial muscle activity linked to emotion physiology. It serves as forgery-resistant representations that reduce domain dependency while capturing subtle dynamics often disrupted in synthetic content. Besides, instead of comparing entire video clips as in prior works, our method computes fine-grained frame-wise audiovisual similarities via a dedicated fusion module augmented with learnable cross-modal queries. It dynamically aligns temporal-spatial lip-audio relationships while mitigating multi-modal feature heterogeneity issues. Experiments on FakeAVCeleb and LAV-DF show state-of-the-art (SOTA) performance and superior cross-dataset generalizability with up to an average of 4.83\% than existing methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing</title>
<link>https://arxiv.org/abs/2505.08302</link>
<guid>https://arxiv.org/abs/2505.08302</guid>
<content:encoded><![CDATA[
<div> Swin-Transformer, irrigation mapping, agricultural landscapes, satellite imagery, sustainable practices 
Summary:
- The article introduces Knowledge-Informed Irrigation Mapping (KIIM), a novel approach using Swin-Transformer for accurate irrigation mapping in complex agricultural landscapes.
- KIIM incorporates a specialized projection matrix for crop to irrigation probability encoding, a spatial attention map for land identification, bi-directional cross-attention for information integration, and a weighted ensemble for prediction combination.
- Experimentation in five US states demonstrates significant improvement over baseline methods, particularly in identifying drip irrigation.
- A two-phase transfer learning approach enhances cross-state mapping, resulting in a substantial boost in performance in states with limited labeled data.
- KIIM shows efficiency by achieving baseline performance with only 40% of the training data, reducing the need for extensive manual labeling efforts and promoting cost-effective large-scale automated irrigation mapping. 

<br /><br />Summary: <div>
arXiv:2505.08302v1 Announce Type: new 
Abstract: Accurate mapping of irrigation methods is crucial for sustainable agricultural practices and food systems. However, existing models that rely solely on spectral features from satellite imagery are ineffective due to the complexity of agricultural landscapes and limited training data, making this a challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a novel Swin-Transformer based approach that uses (i) a specialized projection matrix to encode crop to irrigation probability, (ii) a spatial attention map to identify agricultural lands from non-agricultural lands, (iii) bi-directional cross-attention to focus complementary information from different modalities, and (iv) a weighted ensemble for combining predictions from images and crop information. Our experimentation on five states in the US shows up to 22.9\% (IoU) improvement over baseline with a 71.4% (IoU) improvement for hard-to-classify drip irrigation. In addition, we propose a two-phase transfer learning approach to enhance cross-state irrigation mapping, achieving a 51% IoU boost in a state with limited labeled data. The ability to achieve baseline performance with only 40% of the training data highlights its efficiency, reducing the dependency on extensive manual labeling efforts and making large-scale, automated irrigation mapping more feasible and cost-effective.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An incremental algorithm for non-convex AI-enhanced medical image processing</title>
<link>https://arxiv.org/abs/2505.08324</link>
<guid>https://arxiv.org/abs/2505.08324</guid>
<content:encoded><![CDATA[
<div> deep learning, non-convex optimization, medical imaging, image reconstruction, incremental model-based optimization

Summary:
The article introduces incDG, a hybrid framework that combines deep learning with incremental model-based optimization to efficiently solve non-convex regularized inverse problems in medical imaging. By leveraging the Deep Guess strategy, incDG uses a deep neural network to generate initializations for a non-convex variational solver, improving reconstruction through incremental iterations. The integration of AI tools and model-based optimization enhances robustness and stability in solving imaging inverse problems. Validation on TpV-regularized optimization tasks, including medical image deblurring and tomographic reconstruction, shows superior accuracy and stability compared to conventional solvers and deep learning-based methods. Furthermore, training incDG without ground truth minimally affects performance, making it a practical and powerful tool for addressing non-convex inverse problems in various imaging applications and beyond. 

<br /><br />Summary: <div>
arXiv:2505.08324v1 Announce Type: new 
Abstract: Solving non-convex regularized inverse problems is challenging due to their complex optimization landscapes and multiple local minima. However, these models remain widely studied as they often yield high-quality, task-oriented solutions, particularly in medical imaging, where the goal is to enhance clinically relevant features rather than merely minimizing global error. We propose incDG, a hybrid framework that integrates deep learning with incremental model-based optimization to efficiently approximate the $\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess strategy, incDG exploits a deep neural network to generate effective initializations for a non-convex variational solver, which refines the reconstruction through regularized incremental iterations. This design combines the efficiency of Artificial Intelligence (AI) tools with the theoretical guarantees of model-based optimization, ensuring robustness and stability. We validate incDG on TpV-regularized optimization tasks, demonstrating its effectiveness in medical image deblurring and tomographic reconstruction across diverse datasets, including synthetic images, brain CT slices, and chest-abdomen scans. Results show that incDG outperforms both conventional iterative solvers and deep learning-based methods, achieving superior accuracy and stability. Moreover, we confirm that training incDG without ground truth does not significantly degrade performance, making it a practical and powerful tool for solving non-convex inverse problems in imaging and beyond.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A computer vision-based model for occupancy detection using low-resolution thermal images</title>
<link>https://arxiv.org/abs/2505.08336</link>
<guid>https://arxiv.org/abs/2505.08336</guid>
<content:encoded><![CDATA[
<div> occupancy detection, thermal images, computer vision techniques, YOLOv5 model, privacy concerns <br />
Summary: <br />
Occupancy detection is crucial for efficient HVAC system operation, with Advanced occupant-centric control (OCC) being a more personalized approach. While traditional methods use RGB images for occupancy detection, privacy concerns arise due to captured facial and body features. This study presents a model using low-resolution thermal images and computer vision techniques, reducing privacy risks and computational resources. The model, incorporating transfer learning with YOLOv5, exhibited high performance with precision and recall values nearing perfection. By utilizing thermal images, this novel approach not only ensures privacy protection but also delivers accurate occupancy information for HVAC system optimization. <div>
arXiv:2505.08336v1 Announce Type: new 
Abstract: Occupancy plays an essential role in influencing the energy consumption and operation of heating, ventilation, and air conditioning (HVAC) systems. Traditional HVAC typically operate on fixed schedules without considering occupancy. Advanced occupant-centric control (OCC) adopted occupancy status in regulating HVAC operations. RGB images combined with computer vision (CV) techniques are widely used for occupancy detection, however, the detailed facial and body features they capture raise significant privacy concerns. Low-resolution thermal images offer a non-invasive solution that mitigates privacy issues. The study developed an occupancy detection model utilizing low-resolution thermal images and CV techniques, where transfer learning was applied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The developed model ultimately achieved satisfactory performance, with precision, recall, mAP50, and mAP50 values approaching 1.000. The contributions of this model lie not only in mitigating privacy concerns but also in reducing computing resource demands.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning</title>
<link>https://arxiv.org/abs/2505.08349</link>
<guid>https://arxiv.org/abs/2505.08349</guid>
<content:encoded><![CDATA[
<div> Frequency Adaptation, Diversion, Cross-domain few-shot learning, spectral representations, generalization <br />
Summary: <br />
The paper introduces Frequency Adaptation and Diversion (FAD) for Cross-domain few-shot learning (CD-FSL), which tackles the issue of spectral variations in spatially similar images across domains. FAD utilizes a Frequency Diversion Adapter to transform features into the frequency domain, partition them into low, mid, and high-frequency bands, and adapt each band separately to capture different levels of semantic information. Experiment results on the Meta-Dataset benchmark show that FAD outperforms existing methods in both seen and unseen domains, highlighting the effectiveness of frequency-aware frameworks and band-wise adaptation for enhancing generalization in CD-FSL. <div>
arXiv:2505.08349v1 Announce Type: new 
Abstract: Cross-domain few-shot learning (CD-FSL) requires models to generalize from limited labeled samples under significant distribution shifts. While recent methods enhance adaptability through lightweight task-specific modules, they operate solely in the spatial domain and overlook frequency-specific variations that are often critical for robust transfer. We observe that spatially similar images across domains can differ substantially in their spectral representations, with low and high frequencies capturing complementary semantic information at coarse and fine levels. This indicates that uniform spatial adaptation may overlook these spectral distinctions, thus constraining generalization. To address this, we introduce Frequency Adaptation and Diversion (FAD), a frequency-aware framework that explicitly models and modulates spectral components. At its core is the Frequency Diversion Adapter, which transforms intermediate features into the frequency domain using the discrete Fourier transform (DFT), partitions them into low, mid, and high-frequency bands via radial masks, and reconstructs each band using inverse DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional branch with a kernel size tailored to its spectral scale, enabling targeted and disentangled adaptation across frequencies. Extensive experiments on the Meta-Dataset benchmark demonstrate that FAD consistently outperforms state-of-the-art methods on both seen and unseen domains, validating the utility of frequency-domain representations and band-wise adaptation for improving generalization in CD-FSL.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives</title>
<link>https://arxiv.org/abs/2505.08350</link>
<guid>https://arxiv.org/abs/2505.08350</guid>
<content:encoded><![CDATA[
<div> Keywords: StoryAnchors, story frame generation, temporal consistency, scene diversity, narrative richness

Summary:
StoryAnchors introduces a framework for generating high-quality, multi-scene story frames with strong temporal consistency. The bidirectional story generator integrates past and future contexts to ensure temporal consistency, character continuity, and smooth scene transitions. Specific conditions are introduced to enhance scene diversity and narrative richness. The framework incorporates Multi-Event Story Frame Labeling and Progressive Story Frame Training to capture overarching narrative flow and event-level dynamics, supporting editable and expandable story frames. Extensive experiments demonstrate that StoryAnchors surpasses existing models in consistency, coherence, and scene diversity. Its performance in narrative consistency and story richness rivals that of GPT-4o. StoryAnchors provides a scalable, flexible, and highly editable foundation for advancing story-driven frame generation. 

<br /><br />Summary: <div>
arXiv:2505.08350v1 Announce Type: new 
Abstract: This paper introduces StoryAnchors, a unified framework for generating high-quality, multi-scene story frames with strong temporal consistency. The framework employs a bidirectional story generator that integrates both past and future contexts to ensure temporal consistency, character continuity, and smooth scene transitions throughout the narrative. Specific conditions are introduced to distinguish story frame generation from standard video synthesis, facilitating greater scene diversity and enhancing narrative richness. To further improve generation quality, StoryAnchors integrates Multi-Event Story Frame Labeling and Progressive Story Frame Training, enabling the model to capture both overarching narrative flow and event-level dynamics. This approach supports the creation of editable and expandable story frames, allowing for manual modifications and the generation of longer, more complex sequences. Extensive experiments show that StoryAnchors outperforms existing open-source models in key areas such as consistency, narrative coherence, and scene diversity. Its performance in narrative consistency and story richness is also on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of story-driven frame generation, offering a scalable, flexible, and highly editable foundation for future research.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DArFace: Deformation Aware Robustness for Low Quality Face Recognition</title>
<link>https://arxiv.org/abs/2505.08423</link>
<guid>https://arxiv.org/abs/2505.08423</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial recognition, deep neural networks, low-quality images, deformation modeling, robustness

Summary: 
DArFace is a new framework for robust face recognition that addresses the challenges posed by low-quality facial images in real-world scenarios. It incorporates global transformations and local elastic deformations during training to simulate realistic degraded conditions without the need for paired high- and low-quality samples. The method also includes a contrastive objective to enforce identity consistency across different deformed views. Extensive evaluations on benchmarks such as TinyFace, IJB-B, and IJB-C showcase DArFace's superior performance compared to existing methods, with significant improvements attributed to its incorporation of local deformation modeling. The framework leverages deep neural networks, advanced loss functions, and large-scale datasets to overcome the domain gap between high-quality training data and low-quality real-world images. <div>
arXiv:2505.08423v1 Announce Type: new 
Abstract: Facial recognition systems have achieved remarkable success by leveraging deep neural networks, advanced loss functions, and large-scale datasets. However, their performance often deteriorates in real-world scenarios involving low-quality facial images. Such degradations, common in surveillance footage or standoff imaging include low resolution, motion blur, and various distortions, resulting in a substantial domain gap from the high-quality data typically used during training. While existing approaches attempt to address robustness by modifying network architectures or modeling global spatial transformations, they frequently overlook local, non-rigid deformations that are inherently present in real-world settings. In this work, we introduce DArFace, a Deformation-Aware robust Face recognition framework that enhances robustness to such degradations without requiring paired high- and low-quality training samples. Our method adversarially integrates both global transformations (e.g., rotation, translation) and local elastic deformations during training to simulate realistic low-quality conditions. Moreover, we introduce a contrastive objective to enforce identity consistency across different deformed views. Extensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and IJB-C demonstrate that DArFace surpasses state-of-the-art methods, with significant gains attributed to the inclusion of local deformation modeling.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation</title>
<link>https://arxiv.org/abs/2505.08426</link>
<guid>https://arxiv.org/abs/2505.08426</guid>
<content:encoded><![CDATA[
<div> Keywords: gaze estimation, deep learning, super-resolution, cross-attention, dataset correction

Summary: 
- The paper introduces DHECA-SuperGaze, a deep learning-based method for improved gaze estimation in unconstrained environments.
- The method incorporates super-resolution and a dual head-eye cross-attention module to enhance gaze prediction accuracy.
- Errors in the Gaze360 dataset annotations were identified and corrected, leading to improved performance evaluation results.
- DHECA-SuperGaze outperforms state-of-the-art methods in both within-dataset and cross-dataset testing scenarios.
- The proposed approach demonstrates superior angular error reduction in static and temporal gaze estimation configurations, showcasing its robust generalization capabilities.

<br /><br />Summary: <div>
arXiv:2505.08426v1 Announce Type: new 
Abstract: Unconstrained gaze estimation is the process of determining where a subject is directing their visual attention in uncontrolled environments. Gaze estimation systems are important for a myriad of tasks such as driver distraction monitoring, exam proctoring, accessibility features in modern software, etc. However, these systems face challenges in real-world scenarios, partially due to the low resolution of in-the-wild images and partially due to insufficient modeling of head-eye interactions in current state-of-the-art (SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based method that advances gaze prediction through super-resolution (SR) and a dual head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone processes eye and multiscale SR head images, while the proposed DHECA module enables bidirectional feature refinement between the extracted visual features through cross-attention mechanisms. Furthermore, we identified critical annotation errors in one of the most diverse and widely used gaze estimation datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on Gaze360 and GFIE datasets demonstrates superior within-dataset performance of the proposed method, reducing angular error (AE) by 0.48{\deg} (Gaze360) and 2.95{\deg} (GFIE) in static configurations, and 0.59{\deg} (Gaze360) and 3.00{\deg} (GFIE) in temporal settings compared to prior SOTA methods. Cross-dataset testing shows improvements in AE of more than 1.53{\deg} (Gaze360) and 3.99{\deg} (GFIE) in both static and temporal settings, validating the robust generalization properties of our approach.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Image Reconstruction from Brain Activity via Latent Representation</title>
<link>https://arxiv.org/abs/2505.08429</link>
<guid>https://arxiv.org/abs/2505.08429</guid>
<content:encoded><![CDATA[
<div> Keywords: visual image reconstruction, deep neural networks, generative models, compositional strategies, ethical considerations

Summary: 
Visual image reconstruction has progressed with the integration of deep neural networks and generative models. The evolution of the field from early classification approaches to detailed reconstructions emphasizes hierarchical latent representations, compositional strategies, and modular architectures. Challenges include achieving zero-shot generalization for unseen images and modeling subjective aspects of perception accurately. Diverse datasets, refined evaluation metrics aligned with human judgments, and compositional representations are needed for model robustness and generalizability. Ethical considerations such as privacy, consent, and potential misuse must be addressed responsibly. Visual image reconstruction provides insights into neural coding, enables new psychological measurements of visual experiences, and has applications in clinical diagnostics and brain-machine interfaces. 

<br /><br />Summary: <div>
arXiv:2505.08429v1 Announce Type: new 
Abstract: Visual image reconstruction, the decoding of perceptual content from brain activity into images, has advanced significantly with the integration of deep neural networks (DNNs) and generative models. This review traces the field's evolution from early classification approaches to sophisticated reconstructions that capture detailed, subjective visual experiences, emphasizing the roles of hierarchical latent representations, compositional strategies, and modular architectures. Despite notable progress, challenges remain, such as achieving true zero-shot generalization for unseen images and accurately modeling the complex, subjective aspects of perception. We discuss the need for diverse datasets, refined evaluation metrics aligned with human perceptual judgments, and compositional representations that strengthen model robustness and generalizability. Ethical issues, including privacy, consent, and potential misuse, are underscored as critical considerations for responsible development. Visual image reconstruction offers promising insights into neural coding and enables new psychological measurements of visual experiences, with applications spanning clinical diagnostics and brain-machine interfaces.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection</title>
<link>https://arxiv.org/abs/2505.08437</link>
<guid>https://arxiv.org/abs/2505.08437</guid>
<content:encoded><![CDATA[
<div> Dataset, Forgery Detection, Deepfake, Human Body, TikTok-DeepFake<br />
<br />
Summary:<br />
The article introduces the TikTok-DeepFake (TT-DF) dataset, which focuses on body forgery detection, addressing the lack of datasets and detection methods in this area. TT-DF consists of 6,120 forged videos and 1,378,857 synthetic frames, offering a wide range of forgery methods using advanced human image animation models. The dataset aims to simulate various forged data scenarios comprehensively. Additionally, a new body forgery detection model, Temporal Optical Flow Network (TOF-Net), is proposed, leveraging spatiotemporal inconsistencies and optical flow distribution differences between natural and forged data. Experimental results show that TOF-Net outperforms existing facial forgery detection models on the TT-DF dataset. The work provides a significant contribution to advancing research in human body forgery detection and offers the TT-DF dataset for further exploration in this domain. <br /> <div>
arXiv:2505.08437v1 Announce Type: new 
Abstract: The emergence and popularity of facial deepfake methods spur the vigorous development of deepfake datasets and facial forgery detection, which to some extent alleviates the security concerns about facial-related artificial intelligence technologies. However, when it comes to human body forgery, there has been a persistent lack of datasets and detection methods, due to the later inception and complexity of human body generation methods. To mitigate this issue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale diffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic frames, specifically tailored for body forgery detection. TT-DF offers a wide variety of forgery methods, involving multiple advanced human image animation models utilized for manipulation, two generative configurations based on the disentanglement of identity and pose information, as well as different compressed versions. The aim is to simulate any potential unseen forged data in the wild as comprehensively as possible, and we also furnish a benchmark on TT-DF. Additionally, we propose an adapted body forgery detection model, Temporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal inconsistencies and optical flow distribution differences between natural data and forged data. Our experiments demonstrate that TOF-Net achieves favorable performance on TT-DF, outperforming current state-of-the-art extendable facial forgery detection models. For our TT-DF dataset, please refer to https://github.com/HashTAG00002/TT-DF.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering</title>
<link>https://arxiv.org/abs/2505.08438</link>
<guid>https://arxiv.org/abs/2505.08438</guid>
<content:encoded><![CDATA[
<div> Event cameras, 3D reconstruction, stereo, monocular, multimodal systems<br />
Summary:<br />
Event cameras are a promising sensor for 3D reconstruction due to their ability to capture per-pixel brightness changes asynchronously. This survey categorizes existing works into three types based on input modality and further classifies them by reconstruction approach. Methods are organized chronologically and summarized public datasets relevant to event-based 3D reconstruction. Current research limitations in data availability, evaluation, representation, and dynamic scene handling are highlighted. Future research directions in event-driven 3D reconstruction are outlined. <br /> <div>
arXiv:2505.08438v1 Announce Type: new 
Abstract: Event cameras have emerged as promising sensors for 3D reconstruction due to their ability to capture per-pixel brightness changes asynchronously. Unlike conventional frame-based cameras, they produce sparse and temporally rich data streams, which enable more accurate 3D reconstruction and open up the possibility of performing reconstruction in extreme environments such as high-speed motion, low light, or high dynamic range scenes. In this survey, we provide the first comprehensive review focused exclusively on 3D reconstruction using event cameras. The survey categorises existing works into three major types based on input modality - stereo, monocular, and multimodal systems, and further classifies them by reconstruction approach, including geometry-based, deep learning-based, and recent neural rendering techniques such as Neural Radiance Fields and 3D Gaussian Splatting. Methods with a similar research focus were organised chronologically into the most subdivided groups. We also summarise public datasets relevant to event-based 3D reconstruction. Finally, we highlight current research limitations in data availability, evaluation, representation, and dynamic scene handling, and outline promising future research directions. This survey aims to serve as a comprehensive reference and a roadmap for future developments in event-driven 3D reconstruction.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models</title>
<link>https://arxiv.org/abs/2505.08455</link>
<guid>https://arxiv.org/abs/2505.08455</guid>
<content:encoded><![CDATA[
<div> benchmark, Video-based long-form Causal Reasoning, LVLM, procedural videos, Recognition-Reasoning Decomposition<br />
<br />
Summary: Large Video Language Models (LVLMs) lack the ability to perform video-based causal reasoning due to the absence of benchmarks. To address this, a new benchmark called Video-based long-form Causal Reasoning (VCRBench) is introduced. VCRBench tests LVLMs on their capability to identify, reason about, and sequence events in videos to achieve a specific goal. State-of-the-art LVLMs struggle with long-form causal reasoning in videos due to difficulty in modeling long-range causal dependencies from visual observations. To address this issue, a modular approach called Recognition-Reasoning Decomposition (RRD) is proposed, breaking video-based causal reasoning into video recognition and causal reasoning sub-tasks. Experiments show that RRD significantly improves accuracy on VCRBench, with gains up to 25.2%. Analysis reveals that LVLMs heavily rely on language knowledge for complex video-based causal reasoning tasks. <br /><br /> <div>
arXiv:2505.08455v1 Announce Type: new 
Abstract: Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images</title>
<link>https://arxiv.org/abs/2505.08517</link>
<guid>https://arxiv.org/abs/2505.08517</guid>
<content:encoded><![CDATA[
<div> deep learning, inhalation injuries, bronchoscopy images, enhanced StarGAN, classification accuracy

Summary:
This study introduces a novel deep learning-based framework for grading inhalation injuries using bronchoscopy images. The traditional methods for diagnosing and grading inhalation injuries have limitations, leading to subjective assessments and weak correlations with clinical outcomes. To overcome the scarcity of medical imaging data, enhanced StarGAN, a generative model, was used to improve the quality and clinical relevance of synthetic images. The augmented dataset generated by enhanced StarGAN significantly improved classification performance, achieving an accuracy of 77.78%. The image quality was assessed using the Fr\'echet Inception Distance, with enhanced StarGAN achieving the lowest FID of 30.06. Burn surgeons confirmed the realism and clinical relevance of the generated images, particularly in preserving bronchial structures and color distribution. These results highlight the potential of enhanced StarGAN in addressing data limitations and improving classification accuracy for inhalation injury grading.<br /><br />Summary: <div>
arXiv:2505.08517v1 Announce Type: new 
Abstract: Inhalation injuries face a challenge in clinical diagnosis and grading due to the limitations of traditional methods, such as Abbreviated Injury Score (AIS), which rely on subjective assessments and show weak correlations with clinical outcomes. This study introduces a novel deep learning-based framework for grading inhalation injuries using bronchoscopy images with the duration of mechanical ventilation as an objective metric. To address the scarcity of medical imaging data, we propose enhanced StarGAN, a generative model that integrates Patch Loss and SSIM Loss to improve synthetic images' quality and clinical relevance. The augmented dataset generated by enhanced StarGAN significantly improved classification performance when evaluated using the Swin Transformer, achieving an accuracy of 77.78%, an 11.11% improvement over the original dataset. Image quality was assessed using the Fr\'echet Inception Distance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06, outperforming baseline models. Burn surgeons confirmed the realism and clinical relevance of the generated images, particularly the preservation of bronchial structures and color distribution. These results highlight the potential of enhanced StarGAN in addressing data limitations and improving classification accuracy for inhalation injury grading.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis</title>
<link>https://arxiv.org/abs/2505.08524</link>
<guid>https://arxiv.org/abs/2505.08524</guid>
<content:encoded><![CDATA[
<div> Keywords: Whole slide image classification, domain shift, continual learning, generative latent replay, attention mechanism

Summary: 
The article introduces a novel framework, Attention-based Generative Latent Replay Continual Learning (AGLR-CL), for Whole Slide Image (WSI) classification in the presence of domain shifts. By utilizing Gaussian Mixture Models (GMMs) to synthesize WSI representations and patch count distributions, the method preserves past domain knowledge without storing original data. An attention-based filtering step focuses on salient patch embeddings, generating high-quality synthetic samples. This privacy-aware approach eliminates the need for replay buffers and outperforms buffer-free methods while matching buffer-based solutions' performance. The AGLR-CL framework is validated on tasks such as biomarker detection and molecular status prediction across diverse datasets, demonstrating its ability to retain previous knowledge and adapt to new domains effectively. The framework provides a privacy-preserving solution for domain incremental continual learning in WSI classification. 

Summary: <div>
arXiv:2505.08524v1 Announce Type: new 
Abstract: Whole slide image (WSI) classification has emerged as a powerful tool in computational pathology, but remains constrained by domain shifts, e.g., due to different organs, diseases, or institution-specific variations. To address this challenge, we propose an Attention-based Generative Latent Replay Continual Learning framework (AGLR-CL), in a multiple instance learning (MIL) setup for domain incremental WSI classification. Our method employs Gaussian Mixture Models (GMMs) to synthesize WSI representations and patch count distributions, preserving knowledge of past domains without explicitly storing original data. A novel attention-based filtering step focuses on the most salient patch embeddings, ensuring high-quality synthetic samples. This privacy-aware strategy obviates the need for replay buffers and outperforms other buffer-free counterparts while matching the performance of buffer-based solutions. We validate AGLR-CL on clinically relevant biomarker detection and molecular status prediction across multiple public datasets with diverse centers, organs, and patient cohorts. Experimental results confirm its ability to retain prior knowledge and adapt to new domains, offering an effective, privacy-preserving avenue for domain incremental continual learning in WSI classification.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation</title>
<link>https://arxiv.org/abs/2505.08525</link>
<guid>https://arxiv.org/abs/2505.08525</guid>
<content:encoded><![CDATA[
<div> Keywords: tubular structures, semantic segmentation, super-resolution, dynamic snake upsampling, boundary-skeleton weighted loss

Summary:
This paper introduces a novel approach for accurate segmentation of tubular topological structures, addressing the challenges posed by the slenderness and curvature of such structures. The proposed method combines dynamic snake upsampling operators with a boundary-skeleton weighted loss to improve subpixel-level feature recovery and topological continuity. The snake upsampling operators adaptively adjust the sampling stride and select subpixel sampling points along the serpentine path, enabling more precise feature recovery for tubular structures. The boundary-skeleton weighted loss prioritizes boundary alignment precision and topological consistency by adjusting weight allocation based on the mask class ratio and distance field. Experimental results demonstrate that the proposed method significantly enhances both pixel-wise segmentation accuracy and topological consistency across various datasets and backbone networks. Overall, this dynamic snake upsampling operator and boundary-skeleton weighted loss offer a promising solution for accurate segmentation of tubular structures in various applications. 

<br /><br />Summary: <div>
arXiv:2505.08525v1 Announce Type: new 
Abstract: Accurate segmentation of tubular topological structures (e.g., fissures and vasculature) is critical in various fields to guarantee dependable downstream quantitative analysis and modeling. However, in dense prediction tasks such as semantic segmentation and super-resolution, conventional upsampling operators cannot accommodate the slenderness of tubular structures and the curvature of morphology. This paper introduces a dynamic snake upsampling operators and a boundary-skeleton weighted loss tailored for topological tubular structures. Specifically, we design a snake upsampling operators based on an adaptive sampling domain, which dynamically adjusts the sampling stride according to the feature map and selects a set of subpixel sampling points along the serpentine path, enabling more accurate subpixel-level feature recovery for tubular structures. Meanwhile, we propose a skeleton-to-boundary increasing weighted loss that trades off main body and boundary weight allocation based on mask class ratio and distance field, preserving main body overlap while enhancing focus on target topological continuity and boundary alignment precision. Experiments across various domain datasets and backbone networks show that this plug-and-play dynamic snake upsampling operator and boundary-skeleton weighted loss boost both pixel-wise segmentation accuracy and topological consistency of results.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting</title>
<link>https://arxiv.org/abs/2505.08527</link>
<guid>https://arxiv.org/abs/2505.08527</guid>
<content:encoded><![CDATA[
<div> SFDA, segmentation, Segment Anything Model, Dual Feature Guided, box prompt search<br />
Summary:<br />
The article discusses source-free domain adaptation (SFDA) for segmentation using the Segment Anything Model (SAM). Existing SFDA approaches generate defective bounding box prompts due to the domain gap. To address this, a Dual Feature Guided (DFG) auto-prompting approach is proposed. The approach involves training the source model in a feature aggregation phase to adapt to the target domain and build a suitable feature distribution for box prompt search. The box prompt is gradually expanded based on target model features and SAM features to handle different target feature distributions. Postprocessing the pseudo-labels produced by SAM further refines the results. Experimental results on 3D and 2D datasets show superior performance compared to conventional methods. The code for the approach is available on GitHub. <div>
arXiv:2505.08527v1 Announce Type: new 
Abstract: Source-free domain adaptation (SFDA) for segmentation aims at adapting a model trained in the source domain to perform well in the target domain with only the source model and unlabeled target data.Inspired by the recent success of Segment Anything Model (SAM) which exhibits the generality of segmenting images of various modalities and in different domains given human-annotated prompts like bounding boxes or points, we for the first time explore the potentials of Segment Anything Model for SFDA via automatedly finding an accurate bounding box prompt. We find that the bounding boxes directly generated with existing SFDA approaches are defective due to the domain gap.To tackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting approach to search for the box prompt. Specifically, the source model is first trained in a feature aggregation phase, which not only preliminarily adapts the source model to the target domain but also builds a feature distribution well-prepared for box prompt search. In the second phase, based on two feature distribution observations, we gradually expand the box prompt with the guidance of the target model feature and the SAM feature to handle the class-wise clustered target features and the class-wise dispersed target features, respectively. To remove the potentially enlarged false positive regions caused by the over-confident prediction of the target model, the refined pseudo-labels produced by SAM are further postprocessed based on connectivity analysis. Experiments on 3D and 2D datasets indicate that our approach yields superior performance compared to conventional methods. Code is available at https://github.com/zheangh/DFG.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning</title>
<link>https://arxiv.org/abs/2505.08537</link>
<guid>https://arxiv.org/abs/2505.08537</guid>
<content:encoded><![CDATA[
<div> Keywords: computer vision, food quality assessment, raspberry grading, instance segmentation, industrial environment<br />
<br />
Summary: <br />
This research explores the use of computer vision for assessing food quality rapidly and accurately in an industrial setting, specifically focusing on real-time raspberry grading. A dataset called RaspGrade was created and meticulously annotated for this purpose. The study found that while accurate fruit-level masks can be generated through instance segmentation, classifying certain raspberry grades poses challenges due to color similarities and occlusion, while others are more easily distinguishable based on color. The RaspGrade dataset is now available for access on HuggingFace. This research sheds light on the potential of computer vision for non-invasive and efficient food quality assessment in industrial processes, highlighting the complexities and opportunities in real-time fruit grading. <br /> <div>
arXiv:2505.08537v1 Announce Type: new 
Abstract: This research investigates the application of computer vision for rapid, accurate, and non-invasive food quality assessment, focusing on the novel challenge of real-time raspberry grading into five distinct classes within an industrial environment as the fruits move along a conveyor belt. To address this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and meticulously annotated. Instance segmentation experiments revealed that accurate fruit-level masks can be obtained; however, the classification of certain raspberry grades presents challenges due to color similarities and occlusion, while others are more readily distinguishable based on color. The acquired and annotated RaspGrade dataset is accessible on HuggingFace at: https://huggingface.co/datasets/FBK-TeV/RaspGrade.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art</title>
<link>https://arxiv.org/abs/2505.08552</link>
<guid>https://arxiv.org/abs/2505.08552</guid>
<content:encoded><![CDATA[
<div> detect, copyright infringement, generative AI, visual artworks, contrastive learning 

Summary: 
The article discusses the issue of copyright infringement and forgery in AI-generated visual artworks due to the inherent memorization capabilities of generative models. The proposed DFA-CON framework utilizes contrastive learning to detect copyright violations in AI-generated art by creating a discriminative representation space that distinguishes between original artworks and their forged counterparts. The model is trained across various attack types, such as inpainting, style transfer, adversarial perturbation, and cutmix, to improve detection performance. Evaluation results show that DFA-CON outperforms existing pretrained foundation models in detecting copyright infringement. The code and model checkpoints will be released publicly upon acceptance. <div>
arXiv:2505.08552v1 Announce Type: new 
Abstract: Recent proliferation of generative AI tools for visual content creation-particularly in the context of visual artworks-has raised serious concerns about copyright infringement and forgery. The large-scale datasets used to train these models often contain a mixture of copyrighted and non-copyrighted artworks. Given the tendency of generative models to memorize training patterns, they are susceptible to varying degrees of copyright violation. Building on the recently proposed DeepfakeArt Challenge benchmark, this work introduces DFA-CON, a contrastive learning framework designed to detect copyright-infringing or forged AI-generated art. DFA-CON learns a discriminative representation space, posing affinity among original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. Code and model checkpoints will be released publicly upon acceptance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection</title>
<link>https://arxiv.org/abs/2505.08561</link>
<guid>https://arxiv.org/abs/2505.08561</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked video modeling, Trajectory-Aware Adaptive Token Sampler, MAE, Proximal Policy Optimization, action recognition

Summary: 
Masked video modeling (MVM) has been proven effective for pre-training visual foundation models by reconstructing masked spatiotemporal tokens. However, the challenge lies in selecting the appropriate masking strategy. This study introduces the Trajectory-Aware Adaptive Token Sampler (TATS), which models motion dynamics to select motion-centric tokens, integrated into the masked autoencoder framework. A unified training strategy enables joint optimization of MAE and TATS using Proximal Policy Optimization (PPO) from scratch. The model allows aggressive masking without compromising action recognition performance, maintaining memory efficiency in pre-training. Experiments across benchmarks demonstrate the effectiveness, transferability, generalization, and efficiency of the proposed approach compared to state-of-the-art methods. <div>
arXiv:2505.08561v1 Announce Type: new 
Abstract: Masked video modeling~(MVM) has emerged as a highly effective pre-training strategy for visual foundation models, whereby the model reconstructs masked spatiotemporal tokens using information from visible tokens. However, a key challenge in such approaches lies in selecting an appropriate masking strategy. Previous studies have explored predefined masking techniques, including random and tube-based masking, as well as approaches that leverage key motion priors, optical flow and semantic cues from externally pre-trained models. In this work, we introduce a novel and generalizable Trajectory-Aware Adaptive Token Sampler (TATS), which models the motion dynamics of tokens and can be seamlessly integrated into the masked autoencoder (MAE) framework to select motion-centric tokens in videos. Additionally, we propose a unified training strategy that enables joint optimization of both MAE and TATS from scratch using Proximal Policy Optimization (PPO). We show that our model allows for aggressive masking without compromising performance on the downstream task of action recognition while also ensuring that the pre-training remains memory efficient. Extensive experiments of the proposed approach across four benchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51, demonstrate the effectiveness, transferability, generalization, and efficiency of our work compared to other state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections</title>
<link>https://arxiv.org/abs/2505.08568</link>
<guid>https://arxiv.org/abs/2505.08568</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, thermal detector, traffic light system, mobility restrictions, pedestrian safety

Summary:
The article introduces a new thermal detector-based traffic light system designed to cater to the needs of individuals with mobility restrictions and visual impairments. Traditional RGB camera-based systems often overlook these users, but the proposed system dynamically adjusts signal durations for those with walking impairments and triggers auditory signals for the visually impaired. To address the challenges of thermal imaging, a new YOLO-Thermal detector is developed, enhancing detection accuracy and robustness in thermal imagery. The system also includes the creation of a thermal dataset for people with mobility restrictions, capturing diverse pedestrian scenarios in various environmental conditions. Experimental results show that the proposed system outperforms existing detectors and effectively enhances barrier-free intersection for all users. The source codes and dataset are available for further research and development. <br /><br />Summary: The new thermal detector-based traffic light system addresses the needs of individuals with mobility restrictions and visual impairments by dynamically adjusting signal durations and triggering auditory signals. Developed with a focus on thermal imaging, the YOLO-Thermal detector improves detection accuracy, and the created thermal dataset captures diverse pedestrian scenarios. Experimental results show superior performance compared to existing detectors, highlighting its effectiveness in enhancing barrier-free intersection for all users. <div>
arXiv:2505.08568v1 Announce Type: new 
Abstract: Rapid advances in deep learning for computer vision have driven the adoption of RGB camera-based adaptive traffic light systems to improve traffic safety and pedestrian comfort. However, these systems often overlook the needs of people with mobility restrictions. Moreover, the use of RGB cameras presents significant challenges, including limited detection performance under adverse weather or low-visibility conditions, as well as heightened privacy concerns. To address these issues, we propose a fully automated, thermal detector-based traffic light system that dynamically adjusts signal durations for individuals with walking impairments or mobility burden and triggers the auditory signal for visually impaired individuals, thereby advancing towards barrier-free intersection for all users. To this end, we build the thermal dataset for people with mobility restrictions (TD4PWMR), designed to capture diverse pedestrian scenarios, particularly focusing on individuals with mobility aids or mobility burden under varying environmental conditions, such as different lighting, weather, and crowded urban settings. While thermal imaging offers advantages in terms of privacy and robustness to adverse conditions, it also introduces inherent hurdles for object detection due to its lack of color and fine texture details and generally lower resolution of thermal images. To overcome these limitations, we develop YOLO-Thermal, a novel variant of the YOLO architecture that integrates advanced feature extraction and attention mechanisms for enhanced detection accuracy and robustness in thermal imaging. Experiments demonstrate that the proposed thermal detector outperforms existing detectors, while the proposed traffic light system effectively enhances barrier-free intersection. The source codes and dataset are available at https://github.com/leon2014dresden/YOLO-THERMAL.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking</title>
<link>https://arxiv.org/abs/2505.08581</link>
<guid>https://arxiv.org/abs/2505.08581</guid>
<content:encoded><![CDATA[
arXiv:2505.08581v1 Announce Type: new 
Abstract: Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex real-world surgical scenarios. In this paper, we introduce ReSurgSAM2, a two-stage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose a cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. Upon selecting the initial frame, our method transitions to the tracking stage, where it incorporates a diversity-driven memory mechanism that maintains a credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets will be available at https://github.com/jinlab-imvr/ReSurgSAM2.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large-scale Benchmark on Geological Fault Delineation Models: Domain Shift, Training Dynamics, Generalizability, Evaluation and Inferential Behavior</title>
<link>https://arxiv.org/abs/2505.08585</link>
<guid>https://arxiv.org/abs/2505.08585</guid>
<content:encoded><![CDATA[
arXiv:2505.08585v1 Announce Type: new 
Abstract: Machine learning has taken a critical role in seismic interpretation workflows, especially in fault delineation tasks. However, despite the recent proliferation of pretrained models and synthetic datasets, the field still lacks a systematic understanding of the generalizability limits of these models across seismic data representing a variety of geologic, acquisition and processing settings. Distributional shifts between different data sources, limitations in fine-tuning strategies and labeled data accessibility, and inconsistent evaluation protocols all represent major roadblocks in the deployment of reliable and robust models in real-world exploration settings. In this paper, we present the first large-scale benchmarking study explicitly designed to provide answers and guidelines for domain shift strategies in seismic interpretation. Our benchmark encompasses over $200$ models trained and evaluated on three heterogeneous datasets (synthetic and real data) including FaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining, fine-tuning, and joint training strategies under varying degrees of domain shift. Our analysis highlights the fragility of current fine-tuning practices, the emergence of catastrophic forgetting, and the challenges of interpreting performance in a systematic manner. We establish a robust experimental baseline to provide insights into the tradeoffs inherent to current fault delineation workflows, and shed light on directions for developing more generalizable, interpretable and effective machine learning models for seismic interpretation. The insights and analyses reported provide a set of guidelines on the deployment of fault delineation models within seismic interpretation workflows.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrePrompt: Predictive prompting for class incremental learning</title>
<link>https://arxiv.org/abs/2505.08586</link>
<guid>https://arxiv.org/abs/2505.08586</guid>
<content:encoded><![CDATA[
arXiv:2505.08586v1 Announce Type: new 
Abstract: Class Incremental Learning (CIL) based on pre-trained models offers a promising direction for open-world continual learning. Existing methods typically rely on correlation-based strategies, where an image's classification feature is used as a query to retrieve the most related key prompts and select the corresponding value prompts for training. However, these approaches face an inherent limitation: fitting the entire feature space of all tasks with only a few trainable prompts is fundamentally challenging. We propose Predictive Prompting (PrePrompt), a novel CIL framework that circumvents correlation-based limitations by leveraging pre-trained models' natural classification ability to predict task-specific prompts. Specifically, PrePrompt decomposes CIL into a two-stage prediction framework: task-specific prompt prediction followed by label prediction. While theoretically appealing, this framework risks bias toward recent classes due to missing historical data for older classifier calibration. PrePrompt then mitigates this by incorporating feature translation, dynamically balancing stability and plasticity. Experiments across multiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art prompt-based CIL methods. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment</title>
<link>https://arxiv.org/abs/2505.08589</link>
<guid>https://arxiv.org/abs/2505.08589</guid>
<content:encoded><![CDATA[
arXiv:2505.08589v1 Announce Type: new 
Abstract: This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI) dataset comprising 2525 images taken by a drone flying over dense urban environments. MESSI is unique in two main features. First, it contains images from various altitudes, allowing us to investigate the effect of depth on semantic segmentation. Second, it includes images taken from several different urban regions (at different altitudes). This is important since the variety covers the visual richness captured by a drone's 3D flight, performing horizontal and vertical maneuvers. MESSI contains images annotated with location, orientation, and the camera's intrinsic parameters and can be used to train a deep neural network for semantic segmentation or other applications of interest (e.g., localization, navigation, and tracking). This paper describes the dataset and provides annotation details. It also explains how semantic segmentation was performed using several neural network models and shows several relevant statistics. MESSI will be published in the public domain to serve as an evaluation benchmark for semantic segmentation using images captured by a drone or similar vehicle flying over a dense urban environment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rejoining fragmented ancient bamboo slips with physics-driven deep learning</title>
<link>https://arxiv.org/abs/2505.08601</link>
<guid>https://arxiv.org/abs/2505.08601</guid>
<content:encoded><![CDATA[
arXiv:2505.08601v1 Announce Type: new 
Abstract: Bamboo slips are a crucial medium for recording ancient civilizations in East Asia, and offers invaluable archaeological insights for reconstructing the Silk Road, studying material culture exchanges, and global history. However, many excavated bamboo slips have been fragmented into thousands of irregular pieces, making their rejoining a vital yet challenging step for understanding their content. Here we introduce WisePanda, a physics-driven deep learning framework designed to rejoin fragmented bamboo slips. Based on the physics of fracture and material deterioration, WisePanda automatically generates synthetic training data that captures the physical properties of bamboo fragmentations. This approach enables the training of a matching network without requiring manually paired samples, providing ranked suggestions to facilitate the rejoining process. Compared to the leading curve matching method, WisePanda increases Top-50 matching accuracy from 36\% to 52\%. Archaeologists using WisePanda have experienced substantial efficiency improvements (approximately 20 times faster) when rejoining fragmented bamboo slips. This research demonstrates that incorporating physical principles into deep learning models can significantly enhance their performance, transforming how archaeologists restore and study fragmented artifacts. WisePanda provides a new paradigm for addressing data scarcity in ancient artifact restoration through physics-driven machine learning.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Out-of-Distribution Detection in Medical Imaging Using Multi-Exit Class Activation Maps and Feature Masking</title>
<link>https://arxiv.org/abs/2505.08604</link>
<guid>https://arxiv.org/abs/2505.08604</guid>
<content:encoded><![CDATA[
arXiv:2505.08604v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability of deep learning models in medical imaging applications. This work is motivated by the observation that class activation maps (CAMs) for in-distribution (ID) data typically emphasize regions that are highly relevant to the model's predictions, whereas OOD data often lacks such focused activations. By masking input images with inverted CAMs, the feature representations of ID data undergo more substantial changes compared to those of OOD data, offering a robust criterion for differentiation. In this paper, we introduce a novel unsupervised OOD detection framework, Multi-Exit Class Activation Map (MECAM), which leverages multi-exit CAMs and feature masking. By utilizing mult-exit networks that combine CAMs from varying resolutions and depths, our method captures both global and local feature representations, thereby enhancing the robustness of OOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and PathMNIST, and test its performance against three medical OOD datasets, RSNA Pneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN. Comprehensive comparisons with state-of-the-art OOD detection methods validate the effectiveness of our approach. Our findings emphasize the potential of multi-exit networks and feature masking for advancing unsupervised OOD detection in medical imaging, paving the way for more reliable and interpretable models in clinical practice.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Multi-Modal Information to Enhance Dataset Distillation</title>
<link>https://arxiv.org/abs/2505.08605</link>
<guid>https://arxiv.org/abs/2505.08605</guid>
<content:encoded><![CDATA[
arXiv:2505.08605v1 Announce Type: new 
Abstract: Dataset distillation aims to create a compact and highly representative synthetic dataset that preserves the knowledge of a larger real dataset. While existing methods primarily focus on optimizing visual representations, incorporating additional modalities and refining object-level information can significantly improve the quality of distilled datasets. In this work, we introduce two key enhancements to dataset distillation: caption-guided supervision and object-centric masking. To integrate textual information, we propose two strategies for leveraging caption features: the feature concatenation, where caption embeddings are fused with visual features at the classification stage, and caption matching, which introduces a caption-based alignment loss during training to ensure semantic coherence between real and synthetic data. Additionally, we apply segmentation masks to isolate target objects and remove background distractions, introducing two loss functions designed for object-centric learning: masked feature alignment loss and masked gradient matching loss. Comprehensive evaluations demonstrate that integrating caption-based guidance and object-centric masking enhances dataset distillation, leading to synthetic datasets that achieve superior performance on downstream tasks.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World</title>
<link>https://arxiv.org/abs/2505.08607</link>
<guid>https://arxiv.org/abs/2505.08607</guid>
<content:encoded><![CDATA[
arXiv:2505.08607v1 Announce Type: new 
Abstract: Stereo matching methods rely on dense pixel-wise ground truth labels, which are laborious to obtain, especially for real-world datasets. The scarcity of labeled data and domain gaps between synthetic and real-world images also pose notable challenges. In this paper, we propose a novel framework, \textbf{BooSTer}, that leverages both vision foundation models and large-scale mixed image sources, including synthetic, real, and single-view images. First, to fully unleash the potential of large-scale single-view images, we design a data generation strategy combining monocular depth estimation and diffusion models to generate dense stereo matching data from single-view images. Second, to tackle sparse labels in real-world datasets, we transfer knowledge from monocular depth estimation models, using pseudo-mono depth labels and a dynamic scale- and shift-invariant loss for additional supervision. Furthermore, we incorporate vision foundation model as an encoder to extract robust and transferable features, boosting accuracy and generalization. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, achieving significant improvements in accuracy over existing methods, particularly in scenarios with limited labeled data and domain shifts.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.08614</link>
<guid>https://arxiv.org/abs/2505.08614</guid>
<content:encoded><![CDATA[
arXiv:2505.08614v1 Announce Type: new 
Abstract: Deepfake technology poses increasing risks such as privacy invasion and identity theft. To address these threats, we propose WaveGuard, a proactive watermarking framework that enhances robustness and imperceptibility via frequency-domain embedding and graph-based structural consistency. Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph Neural Network (SC-GNN) to preserve visual quality. We also design an attention module to refine embedding precision. Experimental results on face swap and reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art methods in both robustness and visual quality. Code is available at https://github.com/vpsg-research/WaveGuard.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.08617</link>
<guid>https://arxiv.org/abs/2505.08617</guid>
<content:encoded><![CDATA[
arXiv:2505.08617v1 Announce Type: new 
Abstract: While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely "think with images".
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.08644</link>
<guid>https://arxiv.org/abs/2505.08644</guid>
<content:encoded><![CDATA[
arXiv:2505.08644v1 Announce Type: new 
Abstract: This work presents DLO-Splatting, an algorithm for estimating the 3D shape of Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state information through prediction-update filtering. The DLO-Splatting algorithm uses a position-based dynamics model with shape smoothness and rigidity dampening corrections to predict the object shape. Optimization with a 3D Gaussian Splatting-based rendering loss iteratively renders and refines the prediction to align it with the visual observations in the update step. Initial experiments demonstrate promising results in a knot tying scenario, which is challenging for existing vision-only methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation</title>
<link>https://arxiv.org/abs/2505.08665</link>
<guid>https://arxiv.org/abs/2505.08665</guid>
<content:encoded><![CDATA[
arXiv:2505.08665v1 Announce Type: new 
Abstract: Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS) challenge results</title>
<link>https://arxiv.org/abs/2505.08685</link>
<guid>https://arxiv.org/abs/2505.08685</guid>
<content:encoded><![CDATA[
arXiv:2505.08685v1 Announce Type: new 
Abstract: Deep learning (DL) has become the dominant approach for medical image segmentation, yet ensuring the reliability and clinical applicability of these models requires addressing key challenges such as annotation variability, calibration, and uncertainty estimation. This is why we created the Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS), which highlights the critical role of multiple annotators in establishing a more comprehensive ground truth, emphasizing that segmentation is inherently subjective and that leveraging inter-annotator variability is essential for robust model evaluation. Seven teams participated in the challenge, submitting a variety of DL models evaluated using metrics such as Dice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and Continuous Ranked Probability Score (CRPS). By incorporating consensus and dissensus ground truth, we assess how DL models handle uncertainty and whether their confidence estimates align with true segmentation performance. Our findings reinforce the importance of well-calibrated models, as better calibration is strongly correlated with the quality of the results. Furthermore, we demonstrate that segmentation models trained on diverse datasets and enriched with pre-trained knowledge exhibit greater robustness, particularly in cases deviating from standard anatomical structures. Notably, the best-performing models achieved high DSC and well-calibrated uncertainty estimates. This work underscores the need for multi-annotator ground truth, thorough calibration assessments, and uncertainty-aware evaluations to develop trustworthy and clinically reliable DL-based medical image segmentation models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model</title>
<link>https://arxiv.org/abs/2505.08695</link>
<guid>https://arxiv.org/abs/2505.08695</guid>
<content:encoded><![CDATA[
arXiv:2505.08695v1 Announce Type: new 
Abstract: Given an arbitrary content and style image, arbitrary style transfer aims to render a new stylized
  image which preserves the content image's structure and possesses the style image's style. Existing
  arbitrary style transfer methods are based on either small models or pre-trained large-scale models.
  The small model-based methods fail to generate high-quality stylized images, bringing artifacts and
  disharmonious patterns. The pre-trained large-scale model-based methods can generate high-quality
  stylized images but struggle to preserve the content structure and cost long inference time. To this
  end, we propose a new framework, called SPAST, to generate high-quality stylized images with
  less inference time. Specifically, we design a novel Local-global Window Size Stylization Module
  (LGWSSM)tofuse style features into content features. Besides, we introduce a novel style prior loss,
  which can dig out the style priors from a pre-trained large-scale model into the SPAST and motivate
  the SPAST to generate high-quality stylized images with short inference time.We conduct abundant
  experiments to verify that our proposed method can generate high-quality stylized images and less
  inference time compared with the SOTA arbitrary style transfer methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Image Colorization with Instance-aware Texts and Masks</title>
<link>https://arxiv.org/abs/2505.08705</link>
<guid>https://arxiv.org/abs/2505.08705</guid>
<content:encoded><![CDATA[
arXiv:2505.08705v1 Announce Type: new 
Abstract: Recently, the application of deep learning in image colorization has received widespread attention. The maturation of diffusion models has further advanced the development of image colorization models. However, current mainstream image colorization models still face issues such as color bleeding and color binding errors, and cannot colorize images at the instance level. In this paper, we propose a diffusion-based colorization method MT-Color to achieve precise instance-aware colorization with use-provided guidance. To tackle color bleeding issue, we design a pixel-level mask attention mechanism that integrates latent features and conditional gray image features through cross-attention. We use segmentation masks to construct cross-attention masks, preventing pixel information from exchanging between different instances. We also introduce an instance mask and text guidance module that extracts instance masks and text representations of each instance, which are then fused with latent features through self-attention, utilizing instance masks to form self-attention masks to prevent instance texts from guiding the colorization of other areas, thus mitigating color binding errors. Furthermore, we apply a multi-instance sampling strategy, which involves sampling each instance region separately and then fusing the results. Additionally, we have created a specialized dataset for instance-level colorization tasks, GPT-color, by leveraging large visual language models on existing image datasets. Qualitative and quantitative experiments show that our model and dataset outperform previous methods and datasets.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series</title>
<link>https://arxiv.org/abs/2505.08723</link>
<guid>https://arxiv.org/abs/2505.08723</guid>
<content:encoded><![CDATA[
arXiv:2505.08723v1 Announce Type: new 
Abstract: Satellite image time series (SITS) provide continuous observations of the Earth's surface, making them essential for applications such as environmental management and disaster assessment. However, existing spatiotemporal foundation models rely on plain vision transformers, which encode entire temporal sequences without explicitly capturing multiscale spatiotemporal relationships between land objects. This limitation hinders their effectiveness in downstream tasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision transformer foundation model tailored for SITS analysis. At its core, we introduce a spatiotemporal gyroscope attention mechanism that dynamically captures evolving multiscale patterns across both time and space. For pre-training, we curate MillionST, a large-scale dataset of one million images from 100,000 geographic locations, each captured across 10 temporal phases over five years, encompassing diverse geospatial changes and seasonal variations. Leveraging this dataset, we adapt masked image modeling to pre-train TiMo, enabling it to effectively learn and encode generalizable spatiotemporal representations.Extensive experiments across multiple spatiotemporal tasks-including deforestation monitoring, land cover segmentation, crop type classification, and flood detection-demonstrate TiMo's superiority over state-of-the-art methods. Code, model, and dataset will be released at https://github.com/MiliLab/TiMo.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.08725</link>
<guid>https://arxiv.org/abs/2505.08725</guid>
<content:encoded><![CDATA[
arXiv:2505.08725v1 Announce Type: new 
Abstract: The Large Visual-Language Models (LVLMs) have significantly advanced image understanding. Their comprehension and reasoning capabilities enable promising applications in autonomous driving scenarios. However, existing research typically focuses on front-view perspectives and partial objects within scenes, struggling to achieve comprehensive scene understanding. Meanwhile, existing LVLMs suffer from the lack of mapping relationship between 2D and 3D and insufficient integration of 3D object localization and instruction understanding. To tackle these limitations, we first introduce NuInteract, a large-scale dataset with over 1.5M multi-view image language pairs spanning dense scene captions and diverse interactive tasks. Furthermore, we propose DriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs with a spatial processor using a series of learnable queries. The spatial processor, designed as a plug-and-play component, can be initialized with pre-trained 3D detectors to improve 3D perception. Our experiments show that DriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable improvement on the 3D visual grounding task. The dataset and code will be released at https://github.com/zc-zhao/DriveMonkey.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion</title>
<link>https://arxiv.org/abs/2505.08747</link>
<guid>https://arxiv.org/abs/2505.08747</guid>
<content:encoded><![CDATA[
arXiv:2505.08747v1 Announce Type: new 
Abstract: Nutrition estimation is an important component of promoting healthy eating and mitigating diet-related health risks. Despite advances in tasks such as food classification and ingredient recognition, progress in nutrition estimation is limited due to the lack of datasets with nutritional annotations. To address this issue, we introduce FastFood, a dataset with 84,446 images across 908 fast food categories, featuring ingredient and nutritional annotations. In addition, we propose a new model-agnostic Visual-Ingredient Feature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating visual and ingredient features. Ingredient robustness is improved through synonym replacement and resampling strategies during training. The ingredient-aware visual feature fusion module combines ingredient features and visual representation to achieve accurate nutritional prediction. During testing, ingredient predictions are refined using large multimodal models by data augmentation and majority voting. Our experiments on both FastFood and Nutrition5k datasets validate the effectiveness of our proposed method built in different backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the importance of ingredient information in nutrition estimation. https://huiyanqi.github.io/fastfood-nutrition-estimation/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology</title>
<link>https://arxiv.org/abs/2505.08765</link>
<guid>https://arxiv.org/abs/2505.08765</guid>
<content:encoded><![CDATA[
arXiv:2505.08765v1 Announce Type: new 
Abstract: Aerial Visual Object Search (AVOS) tasks in urban environments require Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target objects using visual and textual cues without external guidance. Existing approaches struggle in complex urban environments due to redundant semantic processing, similar object distinction, and the exploration-exploitation dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS, the first benchmark dataset for autonomous search of common urban objects. This dataset comprises 2,420 tasks across six object categories with varying difficulty levels, enabling comprehensive evaluation of UAV agents' search capabilities. To solve the AVOS tasks, we also propose PRPSearcher (Perception-Reasoning-Planning Searcher), a novel agentic method powered by multi-modal large language models (MLLMs) that mimics human three-tier cognition. Specifically, PRPSearcher constructs three specialized maps: an object-centric dynamic semantic map enhancing spatial perception, a 3D cognitive map based on semantic attraction values for target reasoning, and a 3D uncertainty map for balanced exploration-exploitation search. Also, our approach incorporates a denoising mechanism to mitigate interference from similar objects and utilizes an Inspiration Promote Thought (IPT) prompting mechanism for adaptive action planning. Experimental results on CityAVOS demonstrate that PRPSearcher surpasses existing baselines in both success rate and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and -46.40% NE). While promising, the performance gap compared to humans highlights the need for better semantic reasoning and spatial exploration capabilities in AVOS tasks. This work establishes a foundation for future advances in embodied target search. Dataset and source code are available at https://anonymous.4open.science/r/CityAVOS-3DF8.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of UAV-Based RGB and Multispectral Vegetation Indices for Precision Agriculture in Palm Tree Cultivation</title>
<link>https://arxiv.org/abs/2505.07840</link>
<guid>https://arxiv.org/abs/2505.07840</guid>
<content:encoded><![CDATA[
arXiv:2505.07840v1 Announce Type: cross 
Abstract: Precision farming relies on accurate vegetation monitoring to enhance crop productivity and promote sustainable agricultural practices. This study presents a comprehensive evaluation of UAV-based imaging for vegetation health assessment in a palm tree cultivation region in Dubai. By comparing multispectral and RGB image data, we demonstrate that RGBbased vegetation indices offer performance comparable to more expensive multispectral indices, providing a cost-effective alternative for large-scale agricultural monitoring. Using UAVs equipped with multispectral sensors, indices such as NDVI and SAVI were computed to categorize vegetation into healthy, moderate, and stressed conditions. Simultaneously, RGB-based indices like VARI and MGRVI delivered similar results in vegetation classification and stress detection. Our findings highlight the practical benefits of integrating RGB imagery into precision farming, reducing operational costs while maintaining accuracy in plant health monitoring. This research underscores the potential of UAVbased RGB imaging as a powerful tool for precision agriculture, enabling broader adoption of data-driven decision-making in crop management. By leveraging the strengths of both multispectral and RGB imaging, this work advances the state of UAV applications in agriculture, paving the way for more efficient and scalable farming solutions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding</title>
<link>https://arxiv.org/abs/2505.07851</link>
<guid>https://arxiv.org/abs/2505.07851</guid>
<content:encoded><![CDATA[
arXiv:2505.07851v1 Announce Type: cross 
Abstract: Intra-cardiac Echocardiography (ICE) plays a crucial role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing high-resolution, real-time imaging of cardiac structures. However, existing navigation methods rely on electromagnetic (EM) tracking, which is susceptible to interference and position drift, or require manual adjustments based on operator expertise. To overcome these limitations, we propose a novel anatomy-aware pose estimation system that determines the ICE catheter position and orientation solely from ICE images, eliminating the need for external tracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep learning model, which captures spatial relationships between ICE images and anatomical structures. The model is trained on a clinically acquired dataset of 851 subjects, including ICE images paired with position and orientation labels normalized to the left atrium (LA) mesh. ICE images are patchified into 16x16 embeddings and processed through a transformer network, where a [CLS] token independently predicts position and orientation via separate linear layers. The model is optimized using a Mean Squared Error (MSE) loss function, balancing positional and orientational accuracy. Experimental results demonstrate an average positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98 deg, 10.47 deg) across x, y, and z axes, confirming the model accuracy. Qualitative assessments further validate alignment between predicted and target views within 3D cardiac meshes. This AI-driven system enhances procedural efficiency, reduces operator workload, and enables real-time ICE catheter localization for tracking-free procedures. The proposed method can function independently or complement existing mapping systems like CARTO, offering a transformative approach to ICE-guided interventions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding</title>
<link>https://arxiv.org/abs/2505.07864</link>
<guid>https://arxiv.org/abs/2505.07864</guid>
<content:encoded><![CDATA[
arXiv:2505.07864v1 Announce Type: cross 
Abstract: Flowcharts are indispensable tools in software design and business-process analysis, yet current vision-language models (VLMs) frequently misinterpret the directional arrows and graph topology that set these diagrams apart from natural images. We introduce a seven-stage pipeline grouped into three broader processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical character recognition (OCR) to extract node text; and (3) construction of a structured prompt that guides the VLMs. Tested on a 90-question benchmark distilled from 30 annotated flowcharts, the method raises overall accuracy from 80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp); branch-result questions improve more modestly, and before-step questions remain difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same trends, reinforcing the advantage of explicit arrow encoding. Limitations include dependence on detector and OCR precision, the small evaluation set, and residual errors at nodes with multiple incoming edges. Future work will enlarge the benchmark with synthetic and handwritten flowcharts and assess the approach on Business Process Model and Notation (BPMN) and Unified Modeling Language (UML).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2505.07866</link>
<guid>https://arxiv.org/abs/2505.07866</guid>
<content:encoded><![CDATA[
arXiv:2505.07866v1 Announce Type: cross 
Abstract: The diffusion model has recently emerged as a potent approach in computer vision, demonstrating remarkable performances in the field of generative artificial intelligence. Capable of producing high-quality synthetic images, diffusion models have been successfully applied across a range of applications. However, a significant challenge remains with the high computational cost associated with training and generating these models. This study focuses on the efficiency and inference time of diffusion-based generative models, highlighting their applications in both natural and medical imaging. We present the most recent advances in diffusion models by categorizing them into three key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play a crucial role in medical imaging, where producing fast, reliable, and high-quality medical images is essential for accurate analysis of abnormalities and disease diagnosis. We first investigate the general framework of DDPM, LDM, and WDM and discuss the computational complexity gap filled by these models in natural and medical imaging. We then discuss the current limitations of these models as well as the opportunities and future research directions in medical imaging.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval</title>
<link>https://arxiv.org/abs/2505.07879</link>
<guid>https://arxiv.org/abs/2505.07879</guid>
<content:encoded><![CDATA[
arXiv:2505.07879v1 Announce Type: cross 
Abstract: Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monocular Online Reconstruction with Enhanced Detail Preservation</title>
<link>https://arxiv.org/abs/2505.07887</link>
<guid>https://arxiv.org/abs/2505.07887</guid>
<content:encoded><![CDATA[
arXiv:2505.07887v1 Announce Type: cross 
Abstract: We propose an online 3D Gaussian-based dense mapping framework for photorealistic details reconstruction from a monocular image stream. Our approach addresses two key challenges in monocular online reconstruction: distributing Gaussians without relying on depth maps and ensuring both local and global consistency in the reconstructed maps. To achieve this, we introduce two key modules: the Hierarchical Gaussian Management Module for effective Gaussian distribution and the Global Consistency Optimization Module for maintaining alignment and coherence at all scales. In addition, we present the Multi-level Occupancy Hash Voxels (MOHV), a structure that regularizes Gaussians for capturing details across multiple levels of granularity. MOHV ensures accurate reconstruction of both fine and coarse geometries and textures, preserving intricate details while maintaining overall structural integrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our framework achieves superior reconstruction quality with high computational efficiency. Moreover, it integrates seamlessly with various tracking systems, ensuring generality and scalability.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image-Guided Microstructure Optimization using Diffusion Models: Validated with Li-Mn-rich Cathode Precursors</title>
<link>https://arxiv.org/abs/2505.07906</link>
<guid>https://arxiv.org/abs/2505.07906</guid>
<content:encoded><![CDATA[
arXiv:2505.07906v1 Announce Type: cross 
Abstract: Microstructure often dictates materials performance, yet it is rarely treated as an explicit design variable because microstructure is hard to quantify, predict, and optimize. Here, we introduce an image centric, closed-loop framework that makes microstructural morphology into a controllable objective and demonstrate its use case with Li- and Mn-rich layered oxide cathode precursors. This work presents an integrated, AI driven framework for the predictive design and optimization of lithium-ion battery cathode precursor synthesis. This framework integrates a diffusion-based image generation model, a quantitative image analysis pipeline, and a particle swarm optimization (PSO) algorithm. By extracting key morphological descriptors such as texture, sphericity, and median particle size (D50) from SEM images, the platform accurately predicts SEM like morphologies resulting from specific coprecipitation conditions, including reaction time-, solution concentration-, and pH-dependent structural changes. Optimization then pinpoints synthesis parameters that yield user defined target morphologies, as experimentally validated by the close agreement between predicted and synthesized structures. This framework offers a practical strategy for data driven materials design, enabling both forward prediction and inverse design of synthesis conditions and paving the way toward autonomous, image guided microstructure engineering.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny</title>
<link>https://arxiv.org/abs/2505.07908</link>
<guid>https://arxiv.org/abs/2505.07908</guid>
<content:encoded><![CDATA[
arXiv:2505.07908v1 Announce Type: cross 
Abstract: In this reproduction study, we revisit recent claims that self-attention implements kernel principal component analysis (KPCA) (Teo et al., 2024), positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix of the keys, and (ii) that self-attention projects queries onto the principal component axes of the key matrix $K$ in a feature space. Our analysis reveals three critical inconsistencies: (1) No alignment exists between learned self-attention value vectors and what is proposed in the KPCA perspective, with average similarity metrics (optimal cosine similarity $\leq 0.32$, linear CKA (Centered Kernel Alignment) $\leq 0.11$, kernel CKA $\leq 0.32$) indicating negligible correspondence; (2) Reported decreases in reconstruction loss $J_\text{proj}$, arguably justifying the claim that the self-attention minimizes the projection error of KPCA, are misinterpreted, as the quantities involved differ by orders of magnitude ($\sim\!10^3$); (3) Gram matrix eigenvalue statistics, introduced to justify that $V$ captures the eigenvector of the gram matrix, are irreproducible without undocumented implementation-specific adjustments. Across 10 transformer architectures, we conclude that the KPCA interpretation of self-attention lacks empirical support.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fr\'{e}chet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids</title>
<link>https://arxiv.org/abs/2505.08082</link>
<guid>https://arxiv.org/abs/2505.08082</guid>
<content:encoded><![CDATA[
arXiv:2505.08082v1 Announce Type: cross 
Abstract: Generative artificial intelligence (AI) models in smart grids have advanced significantly in recent years due to their ability to generate large amounts of synthetic data, which would otherwise be difficult to obtain in the real world due to confidentiality constraints. A key challenge in utilizing such synthetic data is how to assess the data quality produced from such generative models. Traditional Euclidean distance-based metrics only reflect pair-wise relations between two individual samples, and could fail in evaluating quality differences between groups of synthetic datasets. In this work, we propose a novel metric based on the Fr\'{e}chet Distance (FD) estimated between two datasets in a learned feature space. The proposed method evaluates the quality of generation from a distributional perspective. Empirical results demonstrate the superiority of the proposed metric across timescales and models, enhancing the reliability of data-driven decision-making in smart grid operations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Neighborhood Environments with Large Language Models</title>
<link>https://arxiv.org/abs/2505.08163</link>
<guid>https://arxiv.org/abs/2505.08163</guid>
<content:encoded><![CDATA[
arXiv:2505.08163v1 Announce Type: cross 
Abstract: Neighborhood environments include physical and environmental conditions such as housing quality, roads, and sidewalks, which significantly influence human health and well-being. Traditional methods for assessing these environments, including field surveys and geographic information systems (GIS), are resource-intensive and challenging to evaluate neighborhood environments at scale. Although machine learning offers potential for automated analysis, the laborious process of labeling training data and the lack of accessible models hinder scalability. This study explores the feasibility of large language models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood environments (e.g., sidewalk and powerline) at scale. We train a robust YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting six environmental indicators, including streetlight, sidewalk, powerline, apartment, single-lane road, and multilane road. We then evaluate four LLMs, including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility, robustness, and limitations in identifying these indicators, with a focus on the impact of prompting strategies and fine-tuning. We apply majority voting with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs could be a useful tool to decode the neighborhood environment without any training effort.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpNeRF: Memory Efficient Sparse Volumetric Neural Rendering Accelerator for Edge Devices</title>
<link>https://arxiv.org/abs/2505.08191</link>
<guid>https://arxiv.org/abs/2505.08191</guid>
<content:encoded><![CDATA[
arXiv:2505.08191v1 Announce Type: cross 
Abstract: Neural rendering has gained prominence for its high-quality output, which is crucial for AR/VR applications. However, its large voxel grid data size and irregular access patterns challenge real-time processing on edge devices. While previous works have focused on improving data locality, they have not adequately addressed the issue of large voxel grid sizes, which necessitate frequent off-chip memory access and substantial on-chip memory. This paper introduces SpNeRF, a software-hardware co-design solution tailored for sparse volumetric neural rendering. We first identify memory-bound rendering inefficiencies and analyze the inherent sparsity in the voxel grid data of neural rendering. To enhance efficiency, we propose novel preprocessing and online decoding steps, reducing the memory size for voxel grid. The preprocessing step employs hash mapping to support irregular data access while maintaining a minimal memory size. The online decoding step enables efficient on-chip sparse voxel grid processing, incorporating bitmap masking to mitigate PSNR loss caused by hash collisions. To further optimize performance, we design a dedicated hardware architecture supporting our sparse voxel grid processing technique. Experimental results demonstrate that SpNeRF achieves an average 21.07$\times$ reduction in memory size while maintaining comparable PSNR levels. When benchmarked against Jetson XNX, Jetson ONX, RT-NeRF.Edge and NeuRex.Edge, our design achieves speedups of 95.1$\times$, 63.5$\times$, 1.5$\times$ and 10.3$\times$, and improves energy efficiency by 625.6$\times$, 529.1$\times$, 4$\times$, and 4.4$\times$, respectively.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image</title>
<link>https://arxiv.org/abs/2505.08239</link>
<guid>https://arxiv.org/abs/2505.08239</guid>
<content:encoded><![CDATA[
arXiv:2505.08239v1 Announce Type: cross 
Abstract: We introduce adaptive view planning to multi-view synthesis, aiming to improve both occlusion revelation and 3D consistency for single-view 3D reconstruction. Instead of generating an unordered set of views independently or simultaneously, we generate a sequence of views, leveraging temporal consistency to enhance 3D coherence. Most importantly, our view sequence is not determined by a pre-determined camera setup. Instead, we compute an adaptive camera trajectory (ACT), specifically, an orbit of camera views, which maximizes the visibility of occluded regions of the 3D object to be reconstructed. Once the best orbit is found, we feed it to a video diffusion model to generate novel views around the orbit, which in turn, are passed to a multi-view 3D reconstruction model to obtain the final reconstruction. Our multi-view synthesis pipeline is quite efficient since it involves no run-time training/optimization, only forward inferences by applying the pre-trained models for occlusion analysis and multi-view synthesis. Our method predicts camera trajectories that reveal occlusions effectively and produce consistent novel views, significantly improving 3D reconstruction over SOTA on the unseen GSO dataset, both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in Hallux Valgus Diagnosis</title>
<link>https://arxiv.org/abs/2505.08247</link>
<guid>https://arxiv.org/abs/2505.08247</guid>
<content:encoded><![CDATA[
arXiv:2505.08247v1 Announce Type: cross 
Abstract: Medical image synthesis plays a crucial role in providing anatomically accurate images for diagnosis and treatment. Hallux valgus, which affects approximately 19% of the global population, requires frequent weight-bearing X-rays for assessment, placing additional strain on both patients and healthcare providers. Existing X-ray models often struggle to balance image fidelity, skeletal consistency, and physical constraints, particularly in diffusion-based methods that lack skeletal guidance. We propose the Skeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a foot evaluation method utilizing skeletal landmarks. SCCDM incorporates multi-scale feature extraction and attention mechanisms, improving the Structural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise Ratio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves an average score of 0.85, demonstrating strong clinical applicability. The code is available at https://github.com/midisec/SCCDM.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where the Devil Hides: Deepfake Detectors Can No Longer Be Trusted</title>
<link>https://arxiv.org/abs/2505.08255</link>
<guid>https://arxiv.org/abs/2505.08255</guid>
<content:encoded><![CDATA[
arXiv:2505.08255v1 Announce Type: cross 
Abstract: With the advancement of AI generative techniques, Deepfake faces have become incredibly realistic and nearly indistinguishable to the human eye. To counter this, Deepfake detectors have been developed as reliable tools for assessing face authenticity. These detectors are typically developed on Deep Neural Networks (DNNs) and trained using third-party datasets. However, this protocol raises a new security risk that can seriously undermine the trustfulness of Deepfake detectors: Once the third-party data providers insert poisoned (corrupted) data maliciously, Deepfake detectors trained on these datasets will be injected ``backdoors'' that cause abnormal behavior when presented with samples containing specific triggers. This is a practical concern, as third-party providers may distribute or sell these triggers to malicious users, allowing them to manipulate detector performance and escape accountability.
  This paper investigates this risk in depth and describes a solution to stealthily infect Deepfake detectors. Specifically, we develop a trigger generator, that can synthesize passcode-controlled, semantic-suppression, adaptive, and invisible trigger patterns, ensuring both the stealthiness and effectiveness of these triggers. Then we discuss two poisoning scenarios, dirty-label poisoning and clean-label poisoning, to accomplish the injection of backdoors. Extensive experiments demonstrate the effectiveness, stealthiness, and practicality of our method compared to several baselines.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Multimodal Prototypes for Visual Recognition with Missing Modalities</title>
<link>https://arxiv.org/abs/2505.08283</link>
<guid>https://arxiv.org/abs/2505.08283</guid>
<content:encoded><![CDATA[
arXiv:2505.08283v1 Announce Type: cross 
Abstract: Multimodal learning enhances deep learning models by enabling them to perceive and understand information from multiple data modalities, such as visual and textual inputs. However, most existing approaches assume the availability of all modalities, an assumption that often fails in real-world applications. Recent works have introduced learnable missing-case-aware prompts to mitigate performance degradation caused by missing modalities while reducing the need for extensive model fine-tuning. Building upon the effectiveness of missing-case-aware handling for missing modalities, we propose a novel decoupled prototype-based output head, which leverages missing-case-aware class-wise prototypes tailored for each individual modality. This approach dynamically adapts to different missing modality scenarios and can be seamlessly integrated with existing prompt-based methods. Extensive experiments demonstrate that our proposed output head significantly improves performance across a wide range of missing-modality scenarios and varying missing rates.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</title>
<link>https://arxiv.org/abs/2505.08293</link>
<guid>https://arxiv.org/abs/2505.08293</guid>
<content:encoded><![CDATA[
arXiv:2505.08293v1 Announce Type: cross 
Abstract: Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments</title>
<link>https://arxiv.org/abs/2505.08299</link>
<guid>https://arxiv.org/abs/2505.08299</guid>
<content:encoded><![CDATA[
arXiv:2505.08299v1 Announce Type: cross 
Abstract: State-space models (SSMs), particularly the Mamba architecture, have emerged as powerful alternatives to Transformers for sequence modeling, offering linear-time complexity and competitive performance across diverse tasks. However, their large parameter counts pose significant challenges for deployment in resource-constrained environments. We propose a novel unstructured pruning framework tailored for Mamba models that achieves up to 70\% parameter reduction while retaining over 95\% of the original performance. Our approach integrates three key innovations: (1) a gradient-aware magnitude pruning technique that combines weight magnitude and gradient information to identify less critical parameters, (2) an iterative pruning schedule that gradually increases sparsity to maintain model stability, and (3) a global pruning strategy that optimizes parameter allocation across the entire model. Through extensive experiments on WikiText-103, Long Range Arena, and ETT time-series benchmarks, we demonstrate significant efficiency gains with minimal performance degradation. Our analysis of pruning effects on Mamba's components reveals critical insights into the architecture's redundancy and robustness, enabling practical deployment in resource-constrained settings while broadening Mamba's applicability.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity</title>
<link>https://arxiv.org/abs/2505.08316</link>
<guid>https://arxiv.org/abs/2505.08316</guid>
<content:encoded><![CDATA[
arXiv:2505.08316v1 Announce Type: cross 
Abstract: Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care</title>
<link>https://arxiv.org/abs/2505.08414</link>
<guid>https://arxiv.org/abs/2505.08414</guid>
<content:encoded><![CDATA[
arXiv:2505.08414v1 Announce Type: cross 
Abstract: Current deep learning models are mostly task specific and lack a user-friendly interface to operate. We present Meta-EyeFM, a multi-function foundation model that integrates a large language model (LLM) with vision foundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a routing mechanism to enable accurate task-specific analysis based on text queries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and systemic diseases, differentiate ocular disease severity, and identify common ocular signs. The model achieved 100% accuracy in routing fundus images to appropriate VFMs, which achieved $\ge$ 82.2% accuracy in disease detection, $\ge$ 89% in severity differentiation, $\ge$ 76% in sign identification. Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o LMMs in detecting various eye diseases and comparable to an ophthalmologist. This system offers enhanced usability and diagnostic performance, making it a valuable decision support tool for primary eye care or an online LLM for fundus evaluation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI</title>
<link>https://arxiv.org/abs/2505.08430</link>
<guid>https://arxiv.org/abs/2505.08430</guid>
<content:encoded><![CDATA[
arXiv:2505.08430v1 Announce Type: cross 
Abstract: Tertiary lymphoid structures (TLS) are organized clusters of immune cells, whose maturity and area can be quantified in whole slide image (WSI) for various prognostic tasks. Existing methods for assessing these characteristics typically rely on cell proxy tasks and require additional post-processing steps. In this work, We focus on a novel task-TLS Semantic Segmentation (TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in an end-to-end manner. Due to the extensive scale of WSI and patch-based segmentation strategies, TLS-SS necessitates integrating from neighboring patches to guide target patch (target) segmentation. Previous techniques often employ on multi-resolution approaches, constraining the capacity to leverage the broader neighboring context while tend to preserve coarse-grained information. To address this, we propose a GNN-based Neighboring Context Aggregation Framework (GNCAF), which progressively aggregates multi-hop neighboring context from the target and employs a self-attention mechanism to guide the segmentation of the target. GNCAF can be integrated with various segmentation models to enhance their ability to perceive contextual information outside of the patch. We build two TLS-SS datasets, called TCGA-COAD and INHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly available. Experiments on these datasets demonstrate the superiority of GNCAF, achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU, respectively. Additionally, we also validate the task scalability of GNCAF on segmentation of lymph node metastases.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?</title>
<link>https://arxiv.org/abs/2505.08468</link>
<guid>https://arxiv.org/abs/2505.08468</guid>
<content:encoded><![CDATA[
arXiv:2505.08468v1 Announce Type: cross 
Abstract: Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (<10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judge's accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2505.08528</link>
<guid>https://arxiv.org/abs/2505.08528</guid>
<content:encoded><![CDATA[
arXiv:2505.08528v1 Announce Type: cross 
Abstract: In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A portable diagnosis model for Keratoconus using a smartphone</title>
<link>https://arxiv.org/abs/2505.08616</link>
<guid>https://arxiv.org/abs/2505.08616</guid>
<content:encoded><![CDATA[
arXiv:2505.08616v1 Announce Type: cross 
Abstract: Keratoconus (KC) is a progressive corneal disorder characterized by localized thinning and protrusion, leading to visual distortion. While Placido disc-based topography remains a standard in clinical diagnostics, its dependence on specialized equipment limits accessibility. In this paper, we propose a portable, smartphone-based diagnostic framework that captures corneal reflections of a Placido disc displayed on a phone screen and applies a two-stage detection pipeline, then validate on 3D-printed emulated eyeball models that simulate normal, moderate, and severe KC stages based on anterior chamber depth (ACD). The first step of the two-stage detection pipeline is classifying different stages of KC with features including height and width of extracted reflections using weighted support vector machine (WSVM). It achieves a maximum accuracy of 92.93%, and maintains over 90% accuracy across multiple smartphone models, including the Galaxy Z Flip 3, iPhone 15 Pro, and iPhone 16 Pro. For the second step, we visualize the KC-affected protrusion regions on the corneas with color maps based on inter-disc distance, that provides an intuitive representation of disease severity and localization. Moreover, we validate the ability of the extracted features to differentiate between KC stages with ANOVA and Omega Squared, with significant p-values (e.g., $p < 10^{-6}$) and large effect sizes ($\\omega^2$ up to 0.8398) among classes.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models</title>
<link>https://arxiv.org/abs/2505.08622</link>
<guid>https://arxiv.org/abs/2505.08622</guid>
<content:encoded><![CDATA[
arXiv:2505.08622v1 Announce Type: cross 
Abstract: Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. However, crafting effective textual prompts to guide these models remains challenging, often requiring extensive trial and error. Existing prompt inversion approaches, such as soft and hard prompt techniques, are not so effective due to the limited interpretability and incoherent prompt generation. To address these issues, we propose Visually Guided Decoding (VGD), a gradient-free approach that leverages large language models (LLMs) and CLIP-based guidance to generate coherent and semantically aligned prompts. In essence, VGD utilizes the robust text generation capabilities of LLMs to produce human-readable prompts. Further, by employing CLIP scores to ensure alignment with user-specified visual concepts, VGD enhances the interpretability, generalization, and flexibility of prompt generation without the need for additional training. Our experiments demonstrate that VGD outperforms existing prompt inversion techniques in generating understandable and contextually relevant prompts, facilitating more intuitive and controllable interactions with text-to-image models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Claycode: Stylable and Deformable 2D Scannable Codes</title>
<link>https://arxiv.org/abs/2505.08666</link>
<guid>https://arxiv.org/abs/2505.08666</guid>
<content:encoded><![CDATA[
arXiv:2505.08666v1 Announce Type: cross 
Abstract: This paper introduces Claycode, a novel 2D scannable code designed for extensive stylization and deformation. Unlike traditional matrix-based codes (e.g., QR codes), Claycodes encode their message in a tree structure. During the encoding process, bits are mapped into a topology tree, which is then depicted as a nesting of color regions drawn within the boundaries of a target polygon shape. When decoding, Claycodes are extracted and interpreted in real-time from a camera stream. We detail the end-to-end pipeline and show that Claycodes allow for extensive stylization without compromising their functionality. We then empirically demonstrate Claycode's high tolerance to heavy deformations, outperforming traditional 2D scannable codes in scenarios where they typically fail.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Coder:Text-Guided CAD Files Code Generation</title>
<link>https://arxiv.org/abs/2505.08686</link>
<guid>https://arxiv.org/abs/2505.08686</guid>
<content:encoded><![CDATA[
arXiv:2505.08686v1 Announce Type: cross 
Abstract: Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D models of real-world products. Traditional CAD typically relies on hand-drawing by experts or modifications of existing library files, which doesn't allow for rapid personalization. With the emergence of generative artificial intelligence, convenient and efficient personalized CAD generation has become possible. However, existing generative methods typically produce outputs that lack interactive editability and geometric annotations, limiting their practical applications in manufacturing. To enable interactive generative CAD, we propose CAD-Coder, a framework that transforms natural language instructions into CAD script codes, which can be executed in Python environments to generate human-editable CAD files (.Dxf). To facilitate the generation of editable CAD sketches with annotation information, we construct a comprehensive dataset comprising 29,130 Dxf files with their corresponding script codes, where each sketch preserves both editability and geometric annotations. We evaluate CAD-Coder on various 2D/3D CAD generation tasks against existing methods, demonstrating superior interactive capabilities while uniquely providing editable sketches with geometric annotations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIViT: Variable-Input Vision Transformer Framework for 3D MR Image Segmentation</title>
<link>https://arxiv.org/abs/2505.08693</link>
<guid>https://arxiv.org/abs/2505.08693</guid>
<content:encoded><![CDATA[
arXiv:2505.08693v1 Announce Type: cross 
Abstract: Self-supervised pretrain techniques have been widely used to improve the downstream tasks' performance. However, real-world magnetic resonance (MR) studies usually consist of different sets of contrasts due to different acquisition protocols, which poses challenges for the current deep learning methods on large-scale pretrain and different downstream tasks with different input requirements, since these methods typically require a fixed set of input modalities or, contrasts. To address this challenge, we propose variable-input ViT (VIViT), a transformer-based framework designed for self-supervised pretraining and segmentation finetuning for variable contrasts in each study. With this ability, our approach can maximize the data availability in pretrain, and can transfer the learned knowledge from pretrain to downstream tasks despite variations in input requirements. We validate our method on brain infarct and brain tumor segmentation, where our method outperforms current CNN and ViT-based models with a mean Dice score of 0.624 and 0.883 respectively. These results highlight the efficacy of our design for better adaptability and performance on tasks with real-world heterogeneous MR data.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aya Vision: Advancing the Frontier of Multilingual Multimodality</title>
<link>https://arxiv.org/abs/2505.08751</link>
<guid>https://arxiv.org/abs/2505.08751</guid>
<content:encoded><![CDATA[
arXiv:2505.08751v1 Announce Type: cross 
Abstract: Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations</title>
<link>https://arxiv.org/abs/2505.08787</link>
<guid>https://arxiv.org/abs/2505.08787</guid>
<content:encoded><![CDATA[
arXiv:2505.08787v1 Announce Type: cross 
Abstract: Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Anytime Optical Flow Estimation with Event Cameras</title>
<link>https://arxiv.org/abs/2307.05033</link>
<guid>https://arxiv.org/abs/2307.05033</guid>
<content:encoded><![CDATA[
arXiv:2307.05033v3 Announce Type: replace 
Abstract: Event cameras respond to changes in log-brightness at the millisecond level, making them ideal for optical flow estimation. However, existing datasets from event cameras provide only low frame rate ground truth for optical flow, limiting the research potential of event-driven optical flow. To address this challenge, we introduce a low-latency event representation, Unified Voxel Grid, and propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce high-frame-rate event optical flow with only low-frame-rate optical flow ground truth for supervision. Furthermore, we propose the Rectified Flow Warp Loss (RFWL) for the unsupervised assessment of intermediate optical flow. A comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow achieves competitive performance, super-low-latency (5ms), time-dense motion estimation (200Hz), and strong generalization. Our code will be available at https://github.com/Yaozhuwa/EVA-Flow.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning-based interactive segmentation in remote sensing</title>
<link>https://arxiv.org/abs/2308.13174</link>
<guid>https://arxiv.org/abs/2308.13174</guid>
<content:encoded><![CDATA[
arXiv:2308.13174v3 Announce Type: replace 
Abstract: Interactive segmentation, a computer vision technique where a user provides guidance to help an algorithm segment a feature of interest in an image, has achieved outstanding accuracy and efficient human-computer interaction. However, few studies have discussed its application to remote sensing imagery, where click-based interactive segmentation could greatly facilitate the analysis of complicated landscapes. This study aims to bridge the gap between click-based interactive segmentation and remote sensing image analysis by conducting a benchmark study on various click-based interactive segmentation models. We assessed the performance of five state-of-the-art interactive segmentation methods (Reviving Iterative Training with Mask Guidance for Interactive Segmentation (RITM), FocalClick, SimpleClick, Iterative Click Loss (ICL), and Segment Anything (SAM)) on two high-resolution aerial imagery datasets. The Cascade-Forward Refinement (CFR) approach, an innovative inference strategy for interactive segmentation, was also introduced to enhance the segmentation results without requiring manual efforts. We further integrated CFR into all models for comparison. The performance of these methods on various land cover types, different object sizes, and multiple band combinations in the datasets was evaluated. The SimpleClick-CFR model consistently outperformed the other methods in our experiments. Building upon these findings, we developed a dedicated online tool called SegMap for interactive segmentation of remote sensing data. SegMap incorporates a well-performing interactive model that is fine-tuned with remote sensing data. Unlike existing interactive segmentation tools, SegMap offers robust interactivity, modifiability, and adaptability to analyze remote sensing imagery.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal Consistency to Frame-Based Domain Translation Approaches</title>
<link>https://arxiv.org/abs/2310.00868</link>
<guid>https://arxiv.org/abs/2310.00868</guid>
<content:encoded><![CDATA[
arXiv:2310.00868v2 Announce Type: replace 
Abstract: Fourteen million colonoscopies are performed annually just in the U.S. However, the videos from these colonoscopies are not saved due to storage constraints (each video from a high-definition colonoscope camera can be in tens of gigabytes). Instead, a few relevant individual frames are saved for documentation/reporting purposes and these are the frames on which most current colonoscopy AI models are trained on. While developing new unsupervised domain translation methods for colonoscopy (e.g. to translate between real optical and virtual/CT colonoscopy), it is thus typical to start with approaches that initially work for individual frames without temporal consistency. Once an individual-frame model has been finalized, additional contiguous frames are added with a modified deep learning architecture to train a new model from scratch for temporal consistency. This transition to temporally-consistent deep learning models, however, requires significantly more computational and memory resources for training. In this paper, we present a lightweight solution with a tunable temporal parameter, RT-GAN (Recurrent Temporal GAN), for adding temporal consistency to individual frame-based approaches that reduces training requirements by a factor of 5. We demonstrate the effectiveness of our approach on two challenging use cases in colonoscopy: haustral fold segmentation (indicative of missed surface) and realistic colonoscopy simulator video generation. We also release a first-of-its kind temporal dataset for colonoscopy for the above use cases. The datasets, accompanying code, and pretrained models will be made available on our Computational Endoscopy Platform GitHub (https://github.com/nadeemlab/CEP). The supplementary video is available at https://youtu.be/UMVP-uIXwWk.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimized View and Geometry Distillation from Multi-view Diffuser</title>
<link>https://arxiv.org/abs/2312.06198</link>
<guid>https://arxiv.org/abs/2312.06198</guid>
<content:encoded><![CDATA[
arXiv:2312.06198v4 Announce Type: replace 
Abstract: Generating multi-view images from a single input view using image-conditioned diffusion models is a recent advancement and has shown considerable potential. However, issues such as the lack of consistency in synthesized views and over-smoothing in extracted geometry persist. Previous methods integrate multi-view consistency modules or impose additional supervisory to enhance view consistency while compromising on the flexibility of camera positioning and limiting the versatility of view synthesis. In this study, we consider the radiance field optimized during geometry extraction as a more rigid consistency prior, compared to volume and ray aggregation used in previous works. We further identify and rectify a critical bias in the traditional radiance field optimization process through score distillation from a multi-view diffuser. We introduce an Unbiased Score Distillation (USD) that utilizes unconditioned noises from a 2D diffusion model, greatly refining the radiance field fidelity. We leverage the rendered views from the optimized radiance field as the basis and develop a two-step specialization process of a 2D diffusion model, which is adept at conducting object-specific denoising and generating high-quality multi-view images. Finally, we recover faithful geometry and texture directly from the refined multi-view images. Empirical evaluations demonstrate that our optimized geometry and view distillation technique generates comparable results to the state-of-the-art models trained on extensive datasets, all while maintaining freedom in camera positioning. Please see our project page at https://youjiazhang.github.io/USD/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation</title>
<link>https://arxiv.org/abs/2403.17525</link>
<guid>https://arxiv.org/abs/2403.17525</guid>
<content:encoded><![CDATA[
arXiv:2403.17525v2 Announce Type: replace 
Abstract: When benefiting graphic sketch representation with sketch drawing orders, recent studies have linked sketch patches as graph edges by drawing orders in accordance to a temporal-based nearest neighboring strategy. However, such constructed graph edges may be unreliable, since the contextual relationships between patches may be inconsistent with the sequential positions in drawing orders, due to variants of sketch drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for sketch learning. We introduce a sinusoidal absolute PE to embed the sequential positions in drawing orders, and a learnable relative PE to encode the unseen contextual relationships between patches. Both types of PEs never attend the construction of graph edges, but are injected into graph nodes to cooperate with the visual patterns captured from patches. After linking nodes by semantic proximity, during message aggregation via graph convolutional networks, each node receives both semantic features from patches and contextual information from PEs from its neighbors, which equips local patch patterns with global contextual information, further obtaining drawing-order-enhanced sketch representations. Experimental results indicate that our method significantly improves sketch healing and controllable sketch synthesis. The source codes could be found at https://github.com/SCZang/DC-gra2seq.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2404.11824</link>
<guid>https://arxiv.org/abs/2404.11824</guid>
<content:encoded><![CDATA[
arXiv:2404.11824v5 Announce Type: replace 
Abstract: Text-to-image (T2I) generation has made remarkable progress in producing high-quality images, but a fundamental challenge remains: creating backgrounds that naturally accommodate text placement without compromising image quality. This capability is non-trivial for real-world applications like graphic design, where clear visual hierarchy between content and text is essential. Prior work has primarily focused on arranging layouts within existing static images, leaving unexplored the potential of T2I models for generating text-friendly backgrounds. We present TextCenGen, a training-free dynamic background adaptation in the blank region for text-friendly image generation. Instead of directly reducing attention in text areas, which degrades image quality, we relocate conflicting objects before background optimization. Our method analyzes cross-attention maps to identify conflicting objects overlapping with text regions and uses a force-directed graph approach to guide their relocation, followed by attention excluding constraints to ensure smooth backgrounds. Our method is plug-and-play, requiring no additional training while well balancing both semantic fidelity and visual quality. Evaluated on our proposed text-friendly T2I benchmark of 27,000 images across four seed datasets, TextCenGen outperforms existing methods by achieving 23% lower saliency overlap in text regions while maintaining 98% of the semantic fidelity measured by CLIP score and our proposed Visual-Textual Concordance Metric (VTCM).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discriminative and Consistent Representation Distillation</title>
<link>https://arxiv.org/abs/2407.11802</link>
<guid>https://arxiv.org/abs/2407.11802</guid>
<content:encoded><![CDATA[
arXiv:2407.11802v5 Announce Type: replace 
Abstract: Knowledge Distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. While contrastive learning has shown promise in self-supervised learning by creating discriminative representations, its application in knowledge distillation remains limited and focuses primarily on discrimination, neglecting the structural relationships captured by the teacher model. To address this limitation, we propose Discriminative and Consistent Distillation (DCD), which employs a contrastive loss along with a consistency regularization to minimize the discrepancy between the distributions of teacher and student representations. Our method introduces learnable temperature and bias parameters that adapt during training to balance these complementary objectives, replacing the fixed hyperparameters commonly used in contrastive learning approaches. Through extensive experiments on CIFAR-100 and ImageNet ILSVRC-2012, we demonstrate that DCD achieves state-of-the-art performance, with the student model sometimes surpassing the teacher's accuracy. Furthermore, we show that DCD's learned representations exhibit superior cross-dataset generalization when transferred to Tiny ImageNet and STL-10.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relational Representation Distillation</title>
<link>https://arxiv.org/abs/2407.12073</link>
<guid>https://arxiv.org/abs/2407.12073</guid>
<content:encoded><![CDATA[
arXiv:2407.12073v5 Announce Type: replace 
Abstract: Knowledge distillation involves transferring knowledge from large, cumbersome teacher models to more compact student models. The standard approach minimizes the Kullback-Leibler (KL) divergence between the probabilistic outputs of a teacher and student network. However, this approach fails to capture important structural relationships in the teacher's internal representations. Recent advances have turned to contrastive learning objectives, but these methods impose overly strict constraints through instance-discrimination, forcing apart semantically similar samples even when they should maintain similarity. This motivates an alternative objective by which we preserve relative relationships between instances. Our method employs separate temperature parameters for teacher and student distributions, with sharper student outputs, enabling precise learning of primary relationships while preserving secondary similarities. We show theoretical connections between our objective and both InfoNCE loss and KL divergence. Experiments demonstrate that our method significantly outperforms existing knowledge distillation methods across diverse knowledge transfer tasks, achieving better alignment with teacher models, and sometimes even outperforms the teacher network.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Feature Matching for Large-Scale Structure from Motion</title>
<link>https://arxiv.org/abs/2409.02310</link>
<guid>https://arxiv.org/abs/2409.02310</guid>
<content:encoded><![CDATA[
arXiv:2409.02310v4 Announce Type: replace 
Abstract: Establishing consistent and dense correspondences across multiple images is crucial for Structure from Motion (SfM) systems. Significant view changes, such as air-to-ground with very sparse view overlap, pose an even greater challenge to the correspondence solvers. We present a novel optimization-based approach that significantly enhances existing feature matching methods by introducing geometry cues in addition to color cues. This helps fill gaps when there is less overlap in large-scale scenarios. Our method formulates geometric verification as an optimization problem, guiding feature matching within detector-free methods and using sparse correspondences from detector-based methods as anchor points. By enforcing geometric constraints via the Sampson Distance, our approach ensures that the denser correspondences from detector-free methods are geometrically consistent and more accurate. This hybrid strategy significantly improves correspondence density and accuracy, mitigates multi-view inconsistencies, and leads to notable advancements in camera pose accuracy and point cloud density. It outperforms state-of-the-art feature matching methods on benchmark datasets and enables feature matching in challenging extreme large-scale settings.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration</title>
<link>https://arxiv.org/abs/2410.01723</link>
<guid>https://arxiv.org/abs/2410.01723</guid>
<content:encoded><![CDATA[
arXiv:2410.01723v5 Announce Type: replace 
Abstract: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrated and Efficient Sampling-Free Confidence Estimation for LiDAR Scene Semantic Segmentation</title>
<link>https://arxiv.org/abs/2411.11935</link>
<guid>https://arxiv.org/abs/2411.11935</guid>
<content:encoded><![CDATA[
arXiv:2411.11935v2 Announce Type: replace 
Abstract: Reliable deep learning models require not only accurate predictions but also well-calibrated confidence estimates to ensure dependable uncertainty estimation. This is crucial in safety-critical applications like autonomous driving, which depend on rapid and precise semantic segmentation of LiDAR point clouds for real-time 3D scene understanding. In this work, we introduce a sampling-free approach for estimating well-calibrated confidence values for classification tasks, achieving alignment with true classification accuracy and significantly reducing inference time compared to sampling-based methods. Our evaluation using the Adaptive Calibration Error (ACE) metric for LiDAR semantic segmentation shows that our approach maintains well-calibrated confidence values while achieving increased processing speed compared to a sampling baseline. Additionally, reliability diagrams reveal that our method produces underconfidence rather than overconfident predictions, an advantage for safety-critical applications. Our sampling-free approach offers well-calibrated and time-efficient predictions for LiDAR scene semantic segmentation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHOICE: Benchmarking the Remote Sensing Capabilities of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2411.18145</link>
<guid>https://arxiv.org/abs/2411.18145</guid>
<content:encoded><![CDATA[
arXiv:2411.18145v3 Announce Type: replace 
Abstract: The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensive benchmark designed to objectively evaluate the hierarchical remote sensing capabilities of VLMs. Focusing on 2 primary capability dimensions essential to remote sensing: perception and reasoning, we further categorize 6 secondary dimensions and 23 leaf tasks to ensure a well-rounded assessment coverage. CHOICE guarantees the quality of all 10,507 problems through a rigorous process of data collection from 50 globally distributed cities, question construction and quality control. The newly curated data and the format of multiple-choice questions with definitive answers allow for an objective and straightforward performance assessment. Our evaluation of 3 proprietary and 21 open-source VLMs highlights their critical limitations within this specialized context. We hope that CHOICE will serve as a valuable resource and offer deeper insights into the challenges and potential of VLMs in the field of remote sensing. We will release CHOICE at https://github.com/ShawnAn-WHU/CHOICE.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Strategies for Isolated Sign Language Recognition</title>
<link>https://arxiv.org/abs/2412.11553</link>
<guid>https://arxiv.org/abs/2412.11553</guid>
<content:encoded><![CDATA[
arXiv:2412.11553v2 Announce Type: replace 
Abstract: Accurate recognition and interpretation of sign language are crucial for enhancing communication accessibility for deaf and hard of hearing individuals. However, current approaches of Isolated Sign Language Recognition (ISLR) often face challenges such as low data quality and variability in gesturing speed. This paper introduces a comprehensive model training pipeline for ISLR designed to accommodate the distinctive characteristics and constraints of the Sign Language (SL) domain. The constructed pipeline incorporates carefully selected image and video augmentations to tackle the challenges of low data quality and varying sign speeds. Including an additional regression head combined with IoU-balanced classification loss enhances the model's awareness of the gesture and simplifies capturing temporal information. Extensive experiments demonstrate that the developed training pipeline easily adapts to different datasets and architectures. Additionally, the ablation study shows that each proposed component expands the potential to consider ISLR task specifics. The presented strategies enhance recognition performance across various ISLR benchmarks and achieve state-of-the-art results on the WLASL and Slovo datasets.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining</title>
<link>https://arxiv.org/abs/2501.00958</link>
<guid>https://arxiv.org/abs/2501.00958</guid>
<content:encoded><![CDATA[
arXiv:2501.00958v4 Announce Type: replace 
Abstract: Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Our code are available at https://github.com/DAMO-NLP-SG/multimodal_textbook.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding</title>
<link>https://arxiv.org/abs/2501.01645</link>
<guid>https://arxiv.org/abs/2501.01645</guid>
<content:encoded><![CDATA[
arXiv:2501.01645v3 Announce Type: replace 
Abstract: Multimodal large language models have become a popular topic in deep visual understanding due to many promising real-world applications. However, hour-long video understanding, spanning over one hour and containing tens of thousands of visual frames, remains under-explored because of 1) challenging long-term video analyses, 2) inefficient large-model approaches, and 3) lack of large-scale benchmark datasets. Among them, in this paper, we focus on building a large-scale hour-long long video benchmark, HLV-1K, designed to evaluate long video understanding models. HLV-1K comprises 1009 hour-long videos with 14,847 high-quality question answering (QA) and multi-choice question asnwering (MCQA) pairs with time-aware query and diverse annotations, covering frame-level, within-event-level, cross-event-level, and long-term reasoning tasks. We evaluate our benchmark using existing state-of-the-art methods and demonstrate its value for testing deep long video understanding capabilities at different levels and for various tasks. This includes promoting future long video understanding tasks at a granular level, such as deep understanding of long live videos, meeting recordings, and movies.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning</title>
<link>https://arxiv.org/abs/2501.04698</link>
<guid>https://arxiv.org/abs/2501.04698</guid>
<content:encoded><![CDATA[
arXiv:2501.04698v2 Announce Type: replace 
Abstract: Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges for this task: 1) the identity decoupling issue, where directly adopting existing customization methods inevitably mix identity attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training a model that can well represent and decouple various customized concepts in video generation. To address these challenges, we introduce ConceptMaster, a novel framework that effectively addresses the identity decoupling issues while maintaining concept fidelity in video customization. Specifically, we propose to learn decoupled multi-concept embeddings and inject them into diffusion models in a standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To overcome the scarcity of high-quality MCVC data, we establish a data construction pipeline, which enables collection of high-quality multi-concept video-entity data pairs across diverse scenarios. A multi-concept video evaluation set is further devised to comprehensively validate our method from three dimensions, including concept fidelity, identity decoupling ability, and video generation quality, across six different concept composition scenarios. Extensive experiments demonstrate that ConceptMaster significantly outperforms previous methods for video customization tasks, showing great potential to generate personalized and semantically accurate content for video diffusion models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Do Not Understand Negation</title>
<link>https://arxiv.org/abs/2501.09425</link>
<guid>https://arxiv.org/abs/2501.09425</guid>
<content:encoded><![CDATA[
arXiv:2501.09425v2 Announce Type: replace 
Abstract: Many practical vision-language applications require models that understand negation, e.g., when using natural language to retrieve images which contain certain objects but not others. Despite advancements in vision-language models (VLMs) through large-scale training, their ability to comprehend negation remains underexplored. This study addresses the question: how well do current VLMs understand negation? We introduce NegBench, a new benchmark designed to evaluate negation understanding across 18 task variations and $79$k examples spanning image, video, and medical datasets. The benchmark consists of two core tasks designed to evaluate negation understanding in diverse multimodal settings: Retrieval with Negation and Multiple Choice Questions with Negated Captions. Our evaluation reveals that modern VLMs struggle significantly with negation, often performing at chance level. To address these shortcomings, we explore a data-centric approach wherein we finetune CLIP models on large-scale synthetic datasets containing millions of negated captions. We show that this approach can result in a 10% increase in recall on negated queries and a 28% boost in accuracy on multiple-choice questions with negated captions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LP-DETR: Layer-wise Progressive Relations for Object Detection</title>
<link>https://arxiv.org/abs/2502.05147</link>
<guid>https://arxiv.org/abs/2502.05147</guid>
<content:encoded><![CDATA[
arXiv:2502.05147v3 Announce Type: replace 
Abstract: This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach that enhances DETR-based object detection through multi-scale relation modeling. Our method introduces learnable spatial relationships between object queries through a relation-aware self-attention mechanism, which adaptively learns to balance different scales of relations (local, medium and global) across decoder layers. This progressive design enables the model to effectively capture evolving spatial dependencies throughout the detection pipeline. Extensive experiments on COCO 2017 dataset demonstrate that our method improves both convergence speed and detection accuracy compared to standard self-attention module. The proposed method achieves competitive results, reaching 52.3\% AP with 12 epochs and 52.5\% AP with 24 epochs using ResNet-50 backbone, and further improving to 58.0\% AP with Swin-L backbone. Furthermore, our analysis reveals an interesting pattern: the model naturally learns to prioritize local spatial relations in early decoder layers while gradually shifting attention to broader contexts in deeper layers, providing valuable insights for future research in object detection.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification</title>
<link>https://arxiv.org/abs/2502.07409</link>
<guid>https://arxiv.org/abs/2502.07409</guid>
<content:encoded><![CDATA[
arXiv:2502.07409v2 Announce Type: replace 
Abstract: Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model's ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this MGPATH.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image</title>
<link>https://arxiv.org/abs/2502.12894</link>
<guid>https://arxiv.org/abs/2502.12894</guid>
<content:encoded><![CDATA[
arXiv:2502.12894v2 Announce Type: replace 
Abstract: Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge</title>
<link>https://arxiv.org/abs/2502.13818</link>
<guid>https://arxiv.org/abs/2502.13818</guid>
<content:encoded><![CDATA[
arXiv:2502.13818v2 Announce Type: replace 
Abstract: Estimating the construction year of buildings is of great importance for sustainability. Sustainable buildings minimize energy consumption and are a key part of responsible and sustainable urban planning and development to effectively combat climate change. By using Artificial Intelligence (AI) and recently proposed powerful Transformer models, we are able to estimate the construction epoch of buildings from a multi-modal dataset. In this paper, we introduce a new benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD), containing top-view Very High Resolution (VHR) images, Earth Observation (EO) multi-spectral data from the Copernicus Sentinel-2 satellite constellation, and street-view images in many different cities in Europe that are co-localized with respect to the building under study and labelled with the construction epoch. We assess EO generalization performance on new/ previously unseen cities that have been held-out from training and appear only during inference. In this work, we present the community-based data challenge we organized based on MyCD. The AI4EO Challenge ESA MapYourCity was opened in 2024 for 4 months. In this paper, we present the Top-4 performing models of the challenge, and the evaluation results. During inference, the performance of the models using: i) both all three input modalities, and ii) only the two top-view modalities, i.e. without the street-view ground images, is examined. The evaluation results in this work show that the models to estimate the construction year of buildings are effective and can achieve good performance on this difficult important real-world task, even when inference is on previously unseen cities, as well as even when using only the two top-view modalities (i.e. VHR and Sentinel-2) during inference.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2503.01739</link>
<guid>https://arxiv.org/abs/2503.01739</guid>
<content:encoded><![CDATA[
arXiv:2503.01739v2 Announce Type: replace 
Abstract: Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset and code are publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO and https://github.com/WangWenhao0716/BenchUFO under the CC BY 4.0 License.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction</title>
<link>https://arxiv.org/abs/2503.09040</link>
<guid>https://arxiv.org/abs/2503.09040</guid>
<content:encoded><![CDATA[
arXiv:2503.09040v2 Announce Type: replace 
Abstract: Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph's links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at http://mlzxy.github.io/motion-blender-gs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.19703</link>
<guid>https://arxiv.org/abs/2503.19703</guid>
<content:encoded><![CDATA[
arXiv:2503.19703v2 Announce Type: replace 
Abstract: Highly accurate geometric precision and dense image features characterize True Digital Orthophoto Maps (TDOMs), which are in great demand for applications such as urban planning, infrastructure management, and environmental monitoring.Traditional TDOM generation methods need sophisticated processes, such as Digital Surface Models (DSM) and occlusion detection, which are computationally expensive and prone to errors.This work presents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free of explicit DSM and occlusion detection. With depth map generation, spatial information for every pixel within the TDOM is retrieved and can reconstruct the scene with high precision. Divide-and-conquer strategy achieves excellent GS training and rendering with high-resolution TDOMs at a lower resource cost, which preserves higher quality of rendering on complex terrain and thin structure without a decrease in efficiency. Experimental results demonstrate the efficiency of large-scale scene reconstruction and high-precision terrain modeling. This approach provides accurate spatial data, which assists users in better planning and decision-making based on maps.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Adaptation For Remote Sensing Visual Grounding</title>
<link>https://arxiv.org/abs/2503.23083</link>
<guid>https://arxiv.org/abs/2503.23083</guid>
<content:encoded><![CDATA[
arXiv:2503.23083v2 Announce Type: replace 
Abstract: Adapting pre-trained models has become an effective strategy in artificial intelligence, offering a scalable and efficient alternative to training models from scratch. In the context of remote sensing (RS), where visual grounding(VG) remains underexplored, this approach enables the deployment of powerful vision-language models to achieve robust cross-modal understanding while significantly reducing computational overhead. To address this, we applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for RS-specific VG tasks. Specifically, we evaluated LoRA placement across different modules in Grounding DINO and used BitFit and adapters to fine-tune the OFA foundation model pre-trained on general-purpose VG datasets. This approach achieved performance comparable to or surpassing current State Of The Art (SOTA) models while significantly reducing computational costs. This study highlights the potential of PEFT techniques to advance efficient and precise multi-modal analysis in RS, offering a practical and cost-effective alternative to full model training.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Schr\"odinger Diffusion Driven Signal Recovery in 3T BOLD fMRI Using Unmatched 7T Observations</title>
<link>https://arxiv.org/abs/2504.01004</link>
<guid>https://arxiv.org/abs/2504.01004</guid>
<content:encoded><![CDATA[
arXiv:2504.01004v2 Announce Type: replace 
Abstract: Ultra-high-field (7 Tesla) BOLD fMRI offers exceptional detail in both spatial and temporal domains, along with robust signal-to-noise characteristics, making it a powerful modality for studying visual information processing in the brain. However, due to the limited accessibility of 7T scanners, the majority of neuroimaging studies are still conducted using 3T systems, which inherently suffer from reduced fidelity in both resolution and SNR. To mitigate this limitation, we introduce a new computational approach designed to enhance the quality of 3T BOLD fMRI acquisitions. Specifically, we project both 3T and 7T datasets, sourced from different individuals and experimental setups, into a shared low-dimensional representation space. Within this space, we employ a lightweight, unsupervised Schr\"odinger Bridge framework to infer a high-SNR, high-resolution counterpart of the 3T data, without relying on paired supervision. This methodology is evaluated across multiple fMRI retinotopy datasets, including synthetically generated samples, and demonstrates a marked improvement in the reliability and fit of population receptive field (pRF) models applied to the enhanced 3T outputs. Our findings suggest that it is feasible to computationally approximate 7T-level quality from standard 3T acquisitions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation</title>
<link>https://arxiv.org/abs/2504.02697</link>
<guid>https://arxiv.org/abs/2504.02697</guid>
<content:encoded><![CDATA[
arXiv:2504.02697v2 Announce Type: replace 
Abstract: Atmospheric turbulence is a major source of image degradation in long-range imaging systems. Although numerous deep learning-based turbulence mitigation (TM) methods have been proposed, many are slow, memory-hungry, and do not generalize well. In the spatial domain, methods based on convolutional operators have a limited receptive field, so they cannot handle a large spatial dependency required by turbulence. In the temporal domain, methods relying on self-attention can, in theory, leverage the lucky effects of turbulence, but their quadratic complexity makes it difficult to scale to many frames. Traditional recurrent aggregation methods face parallelization challenges.
  In this paper, we present a new TM method based on two concepts: (1) A turbulence mitigation network based on the Selective State Space Model (MambaTM). MambaTM provides a global receptive field in each layer across spatial and temporal dimensions while maintaining linear computational complexity. (2) Learned Latent Phase Distortion (LPD). LPD guides the state space model. Unlike classical Zernike-based representations of phase distortion, the new LPD map uniquely captures the actual effects of turbulence, significantly improving the model's capability to estimate degradation by reducing the ill-posedness. Our proposed method exceeds current state-of-the-art networks on various synthetic and real-world TM benchmarks with significantly faster inference speed.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inter-event Interval Microscopy for Event Cameras</title>
<link>https://arxiv.org/abs/2504.04924</link>
<guid>https://arxiv.org/abs/2504.04924</guid>
<content:encoded><![CDATA[
arXiv:2504.04924v3 Announce Type: replace 
Abstract: Event cameras, an innovative bio-inspired sensor, differ from traditional cameras by sensing changes in intensity rather than directly perceiving intensity and recording these variations as a continuous stream of "events". The intensity reconstruction from these sparse events has long been a challenging problem. Previous approaches mainly focused on transforming motion-induced events into videos or achieving intensity imaging for static scenes by integrating modulation devices at the event camera acquisition end. In this paper, for the first time, we achieve event-to-intensity conversion using a static event camera for both static and dynamic scenes in fluorescence microscopy. Unlike conventional methods that primarily rely on event integration, the proposed Inter-event Interval Microscopy (IEIM) quantifies the time interval between consecutive events at each pixel. With a fixed threshold in the event camera, the time interval can precisely represent the intensity. At the hardware level, the proposed IEIM integrates a pulse light modulation device within a microscope equipped with an event camera, termed Pulse Modulation-based Event-driven Fluorescence Microscopy. Additionally, we have collected IEIMat dataset under various scenes including high dynamic range and high-speed scenarios. Experimental results on the IEIMat dataset demonstrate that the proposed IEIM achieves superior spatial and temporal resolution, as well as a higher dynamic range, with lower bandwidth compared to other methods. The code and the IEIMat dataset will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMNV: A Dataset of Media-Published News Videos for Fake News Detection</title>
<link>https://arxiv.org/abs/2504.07687</link>
<guid>https://arxiv.org/abs/2504.07687</guid>
<content:encoded><![CDATA[
arXiv:2504.07687v3 Announce Type: replace 
Abstract: News media, particularly video-based platforms, have become deeply embed-ded in daily life, concurrently amplifying the risks of misinformation dissem-ination. Consequently, multimodal fake news detection has garnered signifi-cant research attention. However, existing datasets predominantly comprise user-generated videos characterized by crude editing and limited public en-gagement, whereas professionally crafted fake news videos disseminated by media outlets-often politically or virally motivated-pose substantially greater societal harm. To address this gap, we construct FMNV, a novel da-taset exclusively composed of news videos published by media organizations. Through empirical analysis of existing datasets and our curated collection, we categorize fake news videos into four distinct types. Building upon this taxonomy, we employ Large Language Models (LLMs) to automatically generate deceptive content by manipulating authentic media-published news videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream architecture that integrates spatio-temporal motion features from a 3D ResNeXt-101 backbone and static visual semantics from CLIP. The two streams are fused via an attention-based mechanism, while co-attention modules refine the visual, textual, and audio features for effective multi-modal aggregation. Comparative experiments demonstrate both the generali-zation capability of FMNV across multiple baselines and the superior detec-tion efficacy of FMNVD. This work establishes critical benchmarks for de-tecting high-impact fake news in media ecosystems while advancing meth-odologies for cross-modal inconsistency analysis. Our dataset is available in https://github.com/DennisIW/FMNV.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Hyperspectral Images Into Chemical Maps: An End-to-End Deep Learning Approach</title>
<link>https://arxiv.org/abs/2504.14131</link>
<guid>https://arxiv.org/abs/2504.14131</guid>
<content:encoded><![CDATA[
arXiv:2504.14131v3 Announce Type: replace 
Abstract: Current approaches to chemical map generation from hyperspectral images are based on models such as partial least squares (PLS) regression, generating pixel-wise predictions that do not consider spatial context and suffer from a high degree of noise. This study proposes an end-to-end deep learning approach using a modified version of U-Net and a custom loss function to directly obtain chemical maps from hyperspectral images, skipping all intermediate steps required for traditional pixel-wise analysis. We compare the U-Net with the traditional PLS regression on a real dataset of pork belly samples with associated mean fat reference values. The U-Net obtains a test set root mean squared error of between 9% and 13% lower than that of PLS regression on the task of mean fat prediction. At the same time, U-Net generates fine detail chemical maps where 99.91% of the variance is spatially correlated. Conversely, only 2.53% of the variance in the PLS-generated chemical maps is spatially correlated, indicating that each pixel-wise prediction is largely independent of neighboring pixels. Additionally, while the PLS-generated chemical maps contain predictions far beyond the physically possible range of 0-100%, U-Net learns to stay inside this range. Thus, the findings of this study indicate that U-Net is superior to PLS for chemical map generation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models</title>
<link>https://arxiv.org/abs/2504.15026</link>
<guid>https://arxiv.org/abs/2504.15026</guid>
<content:encoded><![CDATA[
arXiv:2504.15026v2 Announce Type: replace 
Abstract: Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. Existing methods primarily focus on ensuring that watermark embedding does not degrade the model performance. However, they often overlook critical challenges in real-world deployment scenarios, such as the complexity of watermark key management, user-defined generation parameters, and the difficulty of verification by arbitrary third parties. To address this issue, we propose Gaussian Shading++, a diffusion model watermarking method tailored for real-world deployment. We propose a double-channel design that leverages pseudorandom error-correcting codes to encode the random seed required for watermark pseudorandomization, achieving performance-lossless watermarking under a fixed watermark key and overcoming key management challenges. Additionally, we model the distortions introduced during generation and inversion as an additive white Gaussian noise channel and employ a novel soft decision decoding strategy during extraction, ensuring strong robustness even when generation parameters vary. To enable third-party verification, we incorporate public key signatures, which provide a certain level of resistance against forgery attacks even when model inversion capabilities are fully disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only maintains performance losslessness but also outperforms existing methods in terms of robustness, making it a more practical solution for real-world deployment.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiTPainter: Efficient Video Inpainting with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.15661</link>
<guid>https://arxiv.org/abs/2504.15661</guid>
<content:encoded><![CDATA[
arXiv:2504.15661v2 Announce Type: replace 
Abstract: Many existing video inpainting algorithms utilize optical flows to construct the corresponding maps and then propagate pixels from adjacent frames to missing areas by mapping. Despite the effectiveness of the propagation mechanism, they might encounter blurry and inconsistencies when dealing with inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT) has emerged as a revolutionary technique for video generation tasks. However, pretrained DiT models for video generation all contain a large amount of parameters, which makes it very time consuming to apply to video inpainting tasks. In this paper, we present DiTPainter, an end-to-end video inpainting model based on Diffusion Transformer (DiT). DiTPainter uses an efficient transformer network designed for video inpainting, which is trained from scratch instead of initializing from any large pretrained models. DiTPainter can address videos with arbitrary lengths and can be applied to video decaptioning and video completion tasks with an acceptable time cost. Experiments show that DiTPainter outperforms existing video inpainting algorithms with higher quality and better spatial-temporal consistency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection</title>
<link>https://arxiv.org/abs/2504.16404</link>
<guid>https://arxiv.org/abs/2504.16404</guid>
<content:encoded><![CDATA[
arXiv:2504.16404v2 Announce Type: replace 
Abstract: Cattle lameness is often caused by hoof injuries or interdigital dermatitis, leads to pain and significantly impacts essential physiological activities such as walking, feeding, and drinking. This study presents a deep learning-based model for detecting cattle lameness, sickness, or gait abnormalities using publicly available video data. The dataset consists of 50 unique videos from 40 individual cattle, recorded from various angles in both indoor and outdoor environments. Half of the dataset represents naturally walking (normal/non-lame) cattle, while the other half consists of cattle exhibiting gait abnormalities (lame). To enhance model robustness and generalizability, data augmentation was applied to the training data. The pre-processed videos were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A comparative analysis of the results demonstrates strong classification performance. Specifically, the 3D CNN model achieved a video-level classification accuracy of 90%, with precision, recall, and f1-score of 90.9%, 90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower accuracy of 85%. This study highlights the effectiveness of directly applying classification models to learn spatiotemporal features from video data, offering an alternative to traditional multi-stage approaches that typically involve object detection, pose estimation, and feature extraction. Besides, the findings demonstrate that the proposed deep learning models, particularly the 3D CNN, effectively classify and detect lameness in cattle while simplifying the processing pipeline.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior</title>
<link>https://arxiv.org/abs/2504.17551</link>
<guid>https://arxiv.org/abs/2504.17551</guid>
<content:encoded><![CDATA[
arXiv:2504.17551v2 Announce Type: replace 
Abstract: Urban land use classification and mapping are critical for urban planning, resource management, and environmental monitoring. Existing remote sensing techniques often lack precision in complex urban environments due to the absence of ground-level details. Unlike aerial perspectives, street view images provide a ground-level view that captures more human and social activities relevant to land use in complex urban scenes. Existing street view-based methods primarily rely on supervised classification, which is challenged by the scarcity of high-quality labeled data and the difficulty of generalizing across diverse urban landscapes. This study introduces an unsupervised contrastive clustering model for street view images with a built-in geographical prior, to enhance clustering performance. When combined with a simple visual assignment of the clusters, our approach offers a flexible and customizable solution to land use mapping, tailored to the specific needs of urban planners. We experimentally show that our method can generate land use maps from geotagged street view image datasets of two cities. As our methodology relies on the universal spatial coherence of geospatial data ("Tobler's law"), it can be adapted to various settings where street view images are available, to enable scalable, unsupervised land use mapping and updating. The code will be available at https://github.com/lin102/CCGP.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical and Multimodal Data for Daily Activity Understanding</title>
<link>https://arxiv.org/abs/2504.17696</link>
<guid>https://arxiv.org/abs/2504.17696</guid>
<content:encoded><![CDATA[
arXiv:2504.17696v3 Announce Type: replace 
Abstract: Daily Activity Recordings for Artificial Intelligence (DARai, pronounced "Dahr-ree") is a multimodal, hierarchically annotated dataset constructed to understand human activities in real-world settings. DARai consists of continuous scripted and unscripted recordings of 50 participants in 10 different environments, totaling over 200 hours of data from 20 sensors including multiple camera views, depth and radar sensors, wearable inertial measurement units (IMUs), electromyography (EMG), insole pressure sensors, biomonitor sensors, and gaze tracker.
  To capture the complexity in human activities, DARai is annotated at three levels of hierarchy: (i) high-level activities (L1) that are independent tasks, (ii) lower-level actions (L2) that are patterns shared between activities, and (iii) fine-grained procedures (L3) that detail the exact execution steps for actions. The dataset annotations and recordings are designed so that 22.7% of L2 actions are shared between L1 activities and 14.2% of L3 procedures are shared between L2 actions. The overlap and unscripted nature of DARai allows counterfactual activities in the dataset.
  Experiments with various machine learning models showcase the value of DARai in uncovering important challenges in human-centered applications. Specifically, we conduct unimodal and multimodal sensor fusion experiments for recognition, temporal localization, and future action anticipation across all hierarchical annotation levels. To highlight the limitations of individual sensors, we also conduct domain-variant experiments that are enabled by DARai's multi-sensor and counterfactual activity design setup.
  The code, documentation, and dataset are available at the dedicated DARai website: https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency</title>
<link>https://arxiv.org/abs/2504.18589</link>
<guid>https://arxiv.org/abs/2504.18589</guid>
<content:encoded><![CDATA[
arXiv:2504.18589v4 Announce Type: replace 
Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements. The project can be found at https://alibaba-damo-academy.github.io/VCBench/.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding</title>
<link>https://arxiv.org/abs/2504.21435</link>
<guid>https://arxiv.org/abs/2504.21435</guid>
<content:encoded><![CDATA[
arXiv:2504.21435v3 Announce Type: replace 
Abstract: With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on standalone videos and mainly assess "visual elements" like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a series. To address this challenge, we propose SeriesBench, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while PC-DCoT enables these MLLMs to achieve performance improvements. Overall, our SeriesBench and PC-DCoT highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation</title>
<link>https://arxiv.org/abs/2504.21650</link>
<guid>https://arxiv.org/abs/2504.21650</guid>
<content:encoded><![CDATA[
arXiv:2504.21650v2 Announce Type: replace 
Abstract: The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Language Models as Text-to-Image Model Evaluators</title>
<link>https://arxiv.org/abs/2505.00759</link>
<guid>https://arxiv.org/abs/2505.00759</guid>
<content:encoded><![CDATA[
arXiv:2505.00759v2 Announce Type: replace 
Abstract: The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinearity Enhanced Adaptive Activation Functions</title>
<link>https://arxiv.org/abs/2403.19896</link>
<guid>https://arxiv.org/abs/2403.19896</guid>
<content:encoded><![CDATA[
arXiv:2403.19896v2 Announce Type: replace-cross 
Abstract: A general procedure for introducing parametric, learned, nonlinearity into activation functions is found to enhance the accuracy of representative neural networks without requiring significant additional computational resources. Examples are given based on the standard rectified linear unit (ReLU) as well as several other frequently employed activation functions. The associated accuracy improvement is quantified both in the context of the MNIST digit data set and a convolutional neural network (CNN) benchmark example.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures</title>
<link>https://arxiv.org/abs/2404.06080</link>
<guid>https://arxiv.org/abs/2404.06080</guid>
<content:encoded><![CDATA[
arXiv:2404.06080v3 Announce Type: replace-cross 
Abstract: This study presents a computer-aided diagnosis (CAD) system to assist early detection of lung metastases during endobronchial ultrasound (EBUS) procedures, significantly reducing follow-up time and enabling timely treatment. Due to limited cytology images and morphological similarities among cells, classifying lung metastases is challenging, and existing research rarely targets this issue directly.To overcome data scarcity and improve classification, the authors propose a few-shot learning model using a hybrid pretrained backbone with fine-grained classification and contrastive learning. Parameter-efficient fine-tuning on augmented support sets enhances generalization and transferability. The model achieved 49.59% accuracy, outperforming existing methods. With 20 image samples, accuracy improved to 55.48%, showing strong potential for identifying rare or novel cancer types in low-data clinical environments.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinically inspired enhance Explainability and Interpretability of an AI-Tool for BCC diagnosis based on expert annotation</title>
<link>https://arxiv.org/abs/2407.00104</link>
<guid>https://arxiv.org/abs/2407.00104</guid>
<content:encoded><![CDATA[
arXiv:2407.00104v2 Announce Type: replace-cross 
Abstract: An AI tool has been developed to provide interpretable support for the diagnosis of BCC via teledermatology, thus speeding up referrals and optimizing resource utilization. The interpretability is provided in two ways: on the one hand, the main BCC dermoscopic patterns are found in the image to justify the BCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM, a clinically inspired visual explanation is developed where the relevant features for diagnosis are located. Since there is no established ground truth for BCC dermoscopic features, a standard reference is inferred from the diagnosis of four dermatologists using an Expectation Maximization (EM) based algorithm. The results demonstrate significant improvements in classification accuracy and interpretability, positioning this approach as a valuable tool for early BCC detection and referral to dermatologists. The BCC/non-BCC classification achieved an accuracy rate of 90%. For Clinically-inspired XAI results, the detection of BCC patterns useful to clinicians reaches 99% accuracy. As for the Clinically-inspired Visual XAI results, the mean of the Grad-CAM normalized value within the manually segmented clinical features is 0.57, while outside this region it is 0.16. This indicates that the model struggles to accurately identify the regions of the BCC patterns. These results prove the ability of the AI tool to provide a useful explanation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2409.12514</link>
<guid>https://arxiv.org/abs/2409.12514</guid>
<content:encoded><![CDATA[
arXiv:2409.12514v5 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that \methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMPERROR: A Flexible Generative Perception Error Model for Probing Self-Driving Planners</title>
<link>https://arxiv.org/abs/2411.07719</link>
<guid>https://arxiv.org/abs/2411.07719</guid>
<content:encoded><![CDATA[
arXiv:2411.07719v2 Announce Type: replace-cross 
Abstract: To handle the complexities of real-world traffic, learning planners for self-driving from data is a promising direction. While recent approaches have shown great progress, they typically assume a setting in which the ground-truth world state is available as input. However, when deployed, planning needs to be robust to the long-tail of errors incurred by a noisy perception system, which is often neglected in evaluation. To address this, previous work has proposed drawing adversarial samples from a perception error model (PEM) mimicking the noise characteristics of a target object detector. However, these methods use simple PEMs that fail to accurately capture all failure modes of detection. In this paper, we present EMPERROR, a novel transformer-based generative PEM, apply it to stress-test an imitation learning (IL)-based planner and show that it imitates modern detectors more faithfully than previous work. Furthermore, it is able to produce realistic noisy inputs that increase the planner's collision rate by up to 85%, demonstrating its utility as a valuable tool for a more complete evaluation of self-driving planners.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression</title>
<link>https://arxiv.org/abs/2412.03293</link>
<guid>https://arxiv.org/abs/2412.03293</guid>
<content:encoded><![CDATA[
arXiv:2412.03293v2 Announce Type: replace-cross 
Abstract: In this paper, we present DiffusionVLA, a novel framework that seamlessly combines the autoregression model with the diffusion model for learning visuomotor policy. Central to our approach is a next-token prediction objective, enabling the model to reason effectively over the user's query in the context of current observations. Subsequently, a diffusion model is attached to generate robust action outputs. To enhance policy learning through self-reasoning, we introduce a novel reasoning injection module that integrates reasoning phrases directly into the policy learning process. The whole framework is simple and flexible, making it easy to deploy and upgrade. We conduct extensive experiments using multiple real robots to validate the effectiveness of DiffusionVLA. Our tests include a challenging factory sorting task, where DiffusionVLA successfully categorizes objects, including those not seen during training. We observe that the reasoning module makes the model interpretable. It allows observers to understand the model thought process and identify potential causes of policy failures. Additionally, we test DiffusionVLA on a zero-shot bin-picking task, achieving 63.7\% accuracy on 102 previously unseen objects. Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments. Furthermore, DiffusionVLA can follow novel instructions and retain conversational ability. Notably, DiffusionVLA is data-efficient and fast at inference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can train from scratch on less than 50 demonstrations for a complex task. Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Scene Coordinate Regression with Efficient Keypoint Detection and Sequential Information</title>
<link>https://arxiv.org/abs/2412.06488</link>
<guid>https://arxiv.org/abs/2412.06488</guid>
<content:encoded><![CDATA[
arXiv:2412.06488v2 Announce Type: replace-cross 
Abstract: Scene Coordinate Regression (SCR) is a visual localization technique that utilizes deep neural networks (DNN) to directly regress 2D-3D correspondences for camera pose estimation. However, current SCR methods often face challenges in handling repetitive textures and meaningless areas due to their reliance on implicit triangulation. In this paper, we propose an efficient and accurate SCR system. Compared to existing SCR methods, we propose a unified architecture for both scene encoding and salient keypoint detection, allowing our system to prioritize the encoding of informative regions. This design significantly improves computational efficiency. Additionally, we introduce a mechanism that utilizes sequential information during both mapping and relocalization. The proposed method enhances the implicit triangulation, especially in environments with repetitive textures. Comprehensive experiments conducted across indoor and outdoor datasets demonstrate that the proposed system outperforms state-of-the-art (SOTA) SCR methods. Our single-frame relocalization mode improves the recall rate of our baseline by 6.4% and increases the running speed from 56Hz to 90Hz. Furthermore, our sequence-based mode increases the recall rate by 11% while maintaining the original efficiency.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation</title>
<link>https://arxiv.org/abs/2501.05014</link>
<guid>https://arxiv.org/abs/2501.05014</guid>
<content:encoded><![CDATA[
arXiv:2501.05014v2 Announce Type: replace-cross 
Abstract: The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control</title>
<link>https://arxiv.org/abs/2502.05855</link>
<guid>https://arxiv.org/abs/2502.05855</guid>
<content:encoded><![CDATA[
arXiv:2502.05855v2 Announce Type: replace-cross 
Abstract: Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert that is separable from the VLA on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks. We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like Octo, OpenVLA, and Diffusion Policy.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analysis of Data Transformation Effects on Segment Anything 2</title>
<link>https://arxiv.org/abs/2503.00042</link>
<guid>https://arxiv.org/abs/2503.00042</guid>
<content:encoded><![CDATA[
arXiv:2503.00042v2 Announce Type: replace-cross 
Abstract: Video object segmentation (VOS) is a critical task in the development of video perception and understanding. The Segment-Anything Model 2 (SAM 2), released by Meta AI, is the current state-of-the-art architecture for end-to-end VOS. SAM 2 performs very well on both clean video data and augmented data, and completely intelligent video perception requires an understanding of how this architecture is capable of achieving such quality results. To better understand how each step within the SAM 2 architecture permits high-quality video segmentation, a variety of complex video transformations are passed through the architecture, and the impact at each stage of the process is measured. It is observed that each progressive stage enables the filtering of complex transformation noise and the emphasis of the object of interest. Contributions include the creation of complex transformation video datasets, an analysis of how each stage of the SAM 2 architecture interprets these transformations, and visualizations of segmented objects through each stage. By better understanding how each model structure impacts overall video understanding, VOS development can work to improve real-world applicability and performance tracking, localizing, and segmenting objects despite complex cluttered scenes and obscurations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GBT-SAM: Adapting a Foundational Deep Learning Model for Generalizable Brain Tumor Segmentation via Efficient Integration of Multi-Parametric MRI Data</title>
<link>https://arxiv.org/abs/2503.04325</link>
<guid>https://arxiv.org/abs/2503.04325</guid>
<content:encoded><![CDATA[
arXiv:2503.04325v3 Announce Type: replace-cross 
Abstract: Gliomas are aggressive brain tumors that require accurate imaging-based diagnosis, with segmentation playing a critical role in evaluating morphology and treatment decisions. Manual delineation of gliomas is time-consuming and prone to variability, motivating the use of deep learning to improve consistency and alleviate clinical workload. However, existing methods often fail to fully exploit the information available in multi-parametric MRI (mp-MRI), particularly inter-slice contextual features, and typically require considerable computational resources while lacking robustness across tumor type variations. We present GBT-SAM, a parameter-efficient deep learning framework that adapts the Segment Anything Model (SAM), a large-scale vision model, to volumetric mp-MRI data. GBT-SAM reduces input complexity by selecting fewer than 2.6\% of slices per scan while incorporating all four MRI modalities, preserving essential tumor-related information with minimal cost. Furthermore, our model is trained by a two-step fine-tuning strategy that incorporates a depth-aware module to capture inter-slice correlations and lightweight adaptation layers, resulting in just 6.5M trainable parameters, which is the lowest among SAM-based approaches. GBT-SAM achieves a Dice Score of 93.54 on the BraTS Adult Glioma dataset and demonstrates robust performance on Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. These results highlight GBT-SAM's potential as a computationally efficient and domain-robust framework for brain tumor segmentation using mp-MRI. Our code and models are available at https://github.com/vpulab/med-sam-brain .
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decadal analysis of sea surface temperature patterns, climatology, and anomalies in temperate coastal waters with Landsat-8 TIRS observations</title>
<link>https://arxiv.org/abs/2503.05843</link>
<guid>https://arxiv.org/abs/2503.05843</guid>
<content:encoded><![CDATA[
arXiv:2503.05843v2 Announce Type: replace-cross 
Abstract: Sea surface temperature (SST) is a fundamental physical parameter characterising the thermal state of sea surface. Due to the intricate thermal interactions between land, sea, and atmosphere, the spatial gradients of SST in coastal waters often appear at finer spatial scales than those in open ocean waters. The Thermal Infrared Sensor (TIRS) onboard Landsat-8, with its 100-meter spatial resolution, offers a unique opportunity to uncover fine-scale coastal SST patterns that would otherwise be overlooked by coarser-resolution thermal sensors. In this study, we first analysed the spatiotemporal patterns of SST in South Australia's temperate coastal waters from 2014 to 2023 by developing an operational approach for SST retrieval from the Landsat-8 TIRS sensor. A buoy was deployed off the coast of Port Lincoln, South Australia, to validate the quality of SST retrievals. Then the daily baseline climatology of SST with 100 m resolution was constructed, which allowed for the detection and analysis of anomalous SST events. Our results suggest the following: (1) the satellite-derived SST data aligned well with the in-situ measured SST values; (2) the semi-enclosed, shallow regions of Upper Spencer Gulf and Upper St Vincent Gulf showed higher temperatures during summer and cooler temperatures during winter than waters closer to the open ocean, resulting in a higher seasonal variation in SST; (3) the near-shore shallow areas in Spencer Gulf and St Vincent Gulf, and regions surrounding Kangaroo Island, were identified to have a higher probability of SST anomalies compared to the rest of the study area; and (4) anomalous SST events were more likely to happen during the warm months than the cool months. We hope these findings would be helpful in supporting the fishing and aquaculture industries in the coastal waters of South Australia.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic quality control in multi-centric fetal brain MRI super-resolution reconstruction</title>
<link>https://arxiv.org/abs/2503.10156</link>
<guid>https://arxiv.org/abs/2503.10156</guid>
<content:encoded><![CDATA[
arXiv:2503.10156v3 Announce Type: replace-cross 
Abstract: Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where acquisitions and image processing techniques are less standardized than in adult imaging. In this work, we focus on automated quality control of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an important processing step where multiple stacks of thick 2D slices are registered together and combined to build a single, isotropic and artifact-free T2 weighted volume. We propose FetMRQC$_{SR}$, a machine-learning method that extracts more than 100 image quality metrics to predict image quality scores using a random forest model. This approach is well suited to a problem that is high dimensional, with highly heterogeneous data and small datasets. We validate FetMRQC$_{SR}$ in an out-of-domain (OOD) setting and report high performance (ROC AUC = 0.89), even when faced with data from an unknown site or SRR method. We also investigate failure cases and show that they occur in $45\%$ of the images due to ambiguous configurations for which the rating from the expert is arguable. These results are encouraging and illustrate how a non deep learning-based method like FetMRQC$_{SR}$ is well suited to this multifaceted problem. Our tool, along with all the code used to generate, train and evaluate the model are available at https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/ .
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Integrated Layered Attention (AILA)</title>
<link>https://arxiv.org/abs/2503.22742</link>
<guid>https://arxiv.org/abs/2503.22742</guid>
<content:encoded><![CDATA[
arXiv:2503.22742v2 Announce Type: replace-cross 
Abstract: We propose Adaptive Integrated Layered Attention (AILA), a neural network architecture that combines dense skip connections with different mechanisms for adaptive feature reuse across network layers. We evaluate AILA on three challenging tasks: price forecasting for various commodities and indices (S&amp;P 500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In all cases, AILA matches strong deep learning baselines (LSTMs, Transformers, and ResNets), achieving it at a fraction of the training and inference time. Notably, we implement and test two versions of the model - AILA-Architecture 1, which uses simple linear layers as the connection mechanism between layers, and AILA-Architecture 2, which implements an attention mechanism to selectively focus on outputs from previous layers. Both architectures are applied in a single-task learning setting, with each model trained separately for individual tasks. Results confirm that AILA's adaptive inter-layer connections yield robust gains by flexibly reusing pertinent features at multiple network depths. The AILA approach thus presents an extension to existing architectures, improving long-range sequence modeling, image recognition with optimised computational speed, and SOTA classification performance in practice.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging</title>
<link>https://arxiv.org/abs/2504.01953</link>
<guid>https://arxiv.org/abs/2504.01953</guid>
<content:encoded><![CDATA[
arXiv:2504.01953v2 Announce Type: replace-cross 
Abstract: Understanding the complex myocardial architecture is critical for diagnosing and treating heart disease. However, existing methods often struggle to accurately capture this intricate structure from Diffusion Tensor Imaging (DTI) data, particularly due to the lack of ground truth labels and the ambiguous, intertwined nature of fiber trajectories. We present a novel deep learning framework for unsupervised clustering of myocardial fibers, providing a data-driven approach to identifying distinct fiber bundles. We uniquely combine a Bidirectional Long Short-Term Memory network to capture local sequential information along fibers, with a Transformer autoencoder to learn global shape features, with pointwise incorporation of essential anatomical context. Clustering these representations using a density-based algorithm identifies 33 to 62 robust clusters, successfully capturing the subtle distinctions in fiber trajectories with varying levels of granularity. Our framework offers a new, flexible, and quantitative way to analyze myocardial structure, achieving a level of delineation that, to our knowledge, has not been previously achieved, with potential applications in improving surgical planning, characterizing disease-related remodeling, and ultimately, advancing personalized cardiac care.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining</title>
<link>https://arxiv.org/abs/2504.21414</link>
<guid>https://arxiv.org/abs/2504.21414</guid>
<content:encoded><![CDATA[
<div> Adaptation, informative model structures, few-shot segmentation, cross-domain, domain characteristics <br />
<br />
Summary: The proposed method, Informative Structure Adaptation (ISA), addresses the challenges of cross-domain few-shot segmentation by adapting well-trained FSS models for target domains without the need for retraining. By identifying domain-specific model structures and measuring parameter importance using a novel structure Fisher score, ISA effectively learns domain characteristics from few-shot labeled support samples during inference. The method progressively trains selected model structures with hierarchically constructed training samples, accommodating shifts in domain characteristics. Extensive experiments validate the superior performance of ISA across multiple cross-domain few-shot segmentation benchmarks. This approach eliminates the costly retraining process and equips existing FSS models with flexible adaptation capabilities for new domains, demonstrating the effectiveness of leveraging informative model structures for domain adaptation in segmentation tasks. <div>
arXiv:2504.21414v2 Announce Type: replace 
Abstract: Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes in new domains, which is often challenging due to the diverse characteristics of target domains and the limited availability of support data. Most CD-FSS methods redesign and retrain in-domain FSS models using various domain-generalization techniques, which are effective but costly to train. To address these issues, we propose adapting informative model structures of the well-trained FSS model for target domains by learning domain characteristics from few-shot labeled support samples during inference, thereby eliminating the need for retraining. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing well-trained in-domain FSS models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers</title>
<link>https://arxiv.org/abs/2504.21476</link>
<guid>https://arxiv.org/abs/2504.21476</guid>
<content:encoded><![CDATA[
<div> Keywords: GarmentDiffusion, generative modeling, sewing patterns, multimodal inputs, 3D sewing patterns

Summary: 
GarmentDiffusion is introduced as a new generative model for creating precise 3D sewing patterns from various input modalities such as text, image, and incomplete sewing patterns. The model efficiently encodes sewing pattern parameters into compact edge token representations, significantly reducing the sequence length compared to existing methods. By utilizing a diffusion transformer, the model can denoise all edge tokens along the temporal axis while maintaining a constant number of denoising steps across different datasets. GarmentDiffusion accelerates sewing pattern generation speed by 100 times compared to previous models like SewingGPT. The model achieves state-of-the-art results on DressCodeData and GarmentCodeData datasets, demonstrating its effectiveness in generating diverse and accurate sewing patterns. More information about the project can be found on the project website at https://shenfu-research.github.io/Garment-Diffusion/.<br /><br />Summary: <div>
arXiv:2504.21476v2 Announce Type: replace 
Abstract: Garment sewing patterns are fundamental design elements that bridge the gap between design concepts and practical manufacturing. The generative modeling of sewing patterns is crucial for creating diversified garments. However, existing approaches are limited either by reliance on a single input modality or by suboptimal generation efficiency. In this work, we present GarmentDiffusion, a new generative model capable of producing centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text, image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing pattern parameters into compact edge token representations, achieving a sequence length that is 10 times shorter than that of the autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we simultaneously denoise all edge tokens along the temporal axis, while maintaining a constant number of denoising steps regardless of dataset-specific edge and panel statistics. With all combination of designs of our model, the sewing pattern generation speed is accelerated by 100 times compared to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well as on the largest sewing pattern dataset, namely GarmentCodeData. The project website is available at https://shenfu-research.github.io/Garment-Diffusion/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance</title>
<link>https://arxiv.org/abs/2504.21497</link>
<guid>https://arxiv.org/abs/2504.21497</guid>
<content:encoded><![CDATA[
<div> 3D face parametric model, video face reenactment, latent diffusion framework, FLAME model, motion control <br />
<br />
Summary: 
This study presents a novel method for video face reenactment by integrating a 3D face parametric model into a latent diffusion framework. The use of the FLAME model allows for improved shape consistency and motion control in generating face animations. Depth maps, normal maps, and rendering maps derived from FLAME sequences are incorporated into the denoising UNet using a Geometric Guidance Encoder. A multi-layer feature fusion module with self-attention mechanisms combines facial appearance and motion latent features. By utilizing the 3D face parametric model as motion guidance, precise expression and head pose modeling are achieved. Experimental results demonstrate the method's ability to generate high-quality face animations with accurate expression and head pose variation modeling, as well as strong generalization performance on out-of-domain images. <div>
arXiv:2504.21497v2 Announce Type: replace 
Abstract: In this study, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This not only enables precise extraction of motion features from driving videos, but also contributes to the faithful preservation of face shape and geometry. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. These maps serve as motion guidance and are encoded into the denoising UNet through a specifically designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module with integrated self-attention mechanisms is used to combine facial appearance and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at https://github.com/weimengting/MagicPortrait.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality</title>
<link>https://arxiv.org/abs/2505.00308</link>
<guid>https://arxiv.org/abs/2505.00308</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Quality Assessment, Auto-contours, Radiotherapy, Bayesian Ordinal Classification<br />
<br />
Summary: 
This study presents a Deep Learning-based quality assessment approach for evaluating auto-generated contours in radiotherapy, specifically focusing on Online Adaptive Radiotherapy (OART). By leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. The BOC model classifies auto-contour quality and quantifies prediction uncertainty, delivering robust performance across scenarios with varying amounts of manual labels. Fine-tuning with minimal manual labels and calibration resulted in high accuracy on test data. The use of calibrated thresholds accurately predicted over 93% of auto-contours' qualities, reducing the need for manual reviews and identifying cases requiring correction. This QA model enhances contouring efficiency in OART by reducing manual workload and facilitating informed clinical decisions, ensuring safer and more reliable radiotherapy workflows through uncertainty quantification. <br /><br /> <div>
arXiv:2505.00308v2 Announce Type: replace 
Abstract: Purpose: This study presents a Deep Learning (DL)-based quality assessment (QA) approach for evaluating auto-generated contours (auto-contours) in radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. Methods: We developed a BOC model to classify auto-contour quality and quantify prediction uncertainty. A calibration step was used to optimize uncertainty thresholds that meet clinical accuracy needs. The method was validated under three data scenarios: no manual labels, limited labels, and extensive labels. For rectum contours in prostate cancer, we applied geometric surrogate labels when manual labels were absent, transfer learning when limited, and direct supervision when ample labels were available. Results: The BOC model delivered robust performance across all scenarios. Fine-tuning with just 30 manual labels and calibrating with 34 subjects yielded over 90% accuracy on test data. Using the calibrated threshold, over 93% of the auto-contours' qualities were accurately predicted in over 98% of cases, reducing unnecessary manual reviews and highlighting cases needing correction. Conclusion: The proposed QA model enhances contouring efficiency in OART by reducing manual workload and enabling fast, informed clinical decisions. Through uncertainty quantification, it ensures safer, more reliable radiotherapy workflows.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports</title>
<link>https://arxiv.org/abs/2505.00228</link>
<guid>https://arxiv.org/abs/2505.00228</guid>
<content:encoded><![CDATA[
<div> Dataset, chest X-ray, AI systems, medical imaging, radiological reports
Summary:
- ReXGradient-160K is introduced as the largest chest X-ray dataset publicly available, containing 160,000 studies from 109,487 patients across 3 U.S. health systems.
- The dataset includes multiple images per study and detailed radiology reports, making it valuable for AI system development and report generation models in medical imaging.
- It is partitioned into training, validation, public test sets, and a private test set for model evaluation.
- The dataset aims to prompt research in medical imaging AI and enhance automated radiological analysis.
- The dataset will be open-sourced for public access at https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.<br /><br />Summary: <div>
arXiv:2505.00228v2 Announce Type: replace-cross 
Abstract: We present ReXGradient-160K, representing the largest publicly available chest X-ray dataset to date in terms of the number of patients. This dataset contains 160,000 chest X-ray studies with paired radiological reports from 109,487 unique patients across 3 U.S. health systems (79 medical sites). This comprehensive dataset includes multiple images per study and detailed radiology reports, making it particularly valuable for the development and evaluation of AI systems for medical imaging and automated report generation models. The dataset is divided into training (140,000 studies), validation (10,000 studies), and public test (10,000 studies) sets, with an additional private test set (10,000 studies) reserved for model evaluation on the ReXrank benchmark. By providing this extensive dataset, we aim to accelerate research in medical imaging AI and advance the state-of-the-art in automated radiological analysis. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA</title>
<link>https://arxiv.org/abs/2505.06356</link>
<guid>https://arxiv.org/abs/2505.06356</guid>
<content:encoded><![CDATA[
<div> Keywords: pretraining datasets, multimodal models, toxicity, mitigation strategies, open source

<br /><br />Summary: This paper investigates the presence of toxic content in the LLaVA image-text pretraining dataset, which is crucial for developing multimodal models. The authors analyze various categories of toxicity and identify how harmful content appears across different modalities. They reveal that the dataset contains a significant amount of toxic image-text pairs. To address this issue, the paper proposes targeted mitigation strategies, resulting in the removal of 7,531 toxic pairs from the original dataset. Additionally, the authors provide guidelines for establishing effective toxicity detection pipelines to prevent the incorporation of harmful content in future models. They emphasize the importance of actively identifying and filtering out various forms of toxic content, such as hate speech, explicit imagery, and targeted harassment, to foster more responsible and equitable multimodal systems. To facilitate further research, the refined toxicity-mitigated dataset is made available as open source, promoting transparency and encouraging other researchers to build upon these findings. This study highlights critical issues surrounding the ethical implications of AI and the importance of creating safer data environments for multimodal learning. <div>
arXiv:2505.06356v1 Announce Type: new 
Abstract: Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present a comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of a refined toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our findings underscore the need to actively identify and filter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering</title>
<link>https://arxiv.org/abs/2505.06370</link>
<guid>https://arxiv.org/abs/2505.06370</guid>
<content:encoded><![CDATA[
<div> Keywords: Lung Cancer, CT Images, Deep Learning, Nodule Classification, Semi-Supervised Learning

Summary:
Lung cancer is a major cause of mortality worldwide, highlighting the importance of early detection of malignant pulmonary nodules in CT images. The LMLCC-Net is a novel deep learning framework proposed in this study, utilizing a 3D CNN with Hounsfield Unit-based intensity filtering to classify nodules. This approach considers intensity patterns and textures, leveraging the significant differences between benign and malignant nodules. Multiple branches extract features using learnable filters, with different combinations and ranges explored for optimal performance. Additionally, a semi-supervised learning scheme aids in labeling ambiguous cases, while a lightweight model enhances classification efficiency. Experimental results on the LUNA16 dataset demonstrate superior accuracy, sensitivity, and AUC compared to existing methods. The proposed method shows promise in assisting radiologists in nodule classification, ultimately improving patient care. 

<br /><br />Summary: <div>
arXiv:2505.06370v1 Announce Type: new 
Abstract: Lung cancer is the leading cause of patient mortality in the world. Early diagnosis of malignant pulmonary nodules in CT images can have a significant impact on reducing disease mortality and morbidity. In this work, we propose LMLCC-Net, a novel deep learning framework for classifying nodules from CT scan images using a 3D CNN, considering Hounsfield Unit (HU)-based intensity filtering. Benign and malignant nodules have significant differences in their intensity profile of HU, which was not exploited in the literature. Our method considers the intensity pattern as well as the texture for the prediction of malignancies. LMLCC-Net extracts features from multiple branches that each use a separate learnable HU-based intensity filtering stage. Various combinations of branches and learnable ranges of filters were explored to finally produce the best-performing model. In addition, we propose a semi-supervised learning scheme for labeling ambiguous cases and also developed a lightweight model to classify the nodules. The experimental evaluations are carried out on the LUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of 91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of 91.87%, showing improved performance compared to existing methods. The proposed method can have a significant impact in helping radiologists in the classification of pulmonary nodules and improving patient care.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust &amp; Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal</title>
<link>https://arxiv.org/abs/2505.06381</link>
<guid>https://arxiv.org/abs/2505.06381</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical disease prediction, deep learning, Knowledge Distillation, Ant Colony Optimization, context-aware predictor<br />
Summary:<br />
- Medical disease prediction using imaging data is challenging due to variability and complexity.
- Deep learning models, including Knowledge Distillation, show promise but struggle with uncertainty and generalization.
- A novel framework combining Ant Colony Optimization and context-aware predictor addresses these limitations.
- The framework adjusts temperature based on image quality, disease complexity, and model confidence.
- Ant Colony Optimization efficiently selects teacher-student model pairs, outperforming current optimization methods.
- Evaluation on three benchmark datasets demonstrates significant improvement in accuracy, surpassing existing benchmarks.
Summary: <div>
arXiv:2505.06381v1 Announce Type: new 
Abstract: Medical disease prediction, particularly through imaging, remains a challenging task due to the complexity and variability of medical data, including noise, ambiguity, and differing image quality. Recent deep learning models, including Knowledge Distillation (KD) methods, have shown promising results in brain tumor image identification but still face limitations in handling uncertainty and generalizing across diverse medical conditions. Traditional KD methods often rely on a context-unaware temperature parameter to soften teacher model predictions, which does not adapt effectively to varying uncertainty levels present in medical images. To address this issue, we propose a novel framework that integrates Ant Colony Optimization (ACO) for optimal teacher-student model selection and a novel context-aware predictor approach for temperature scaling. The proposed context-aware framework adjusts the temperature based on factors such as image quality, disease complexity, and teacher model confidence, allowing for more robust knowledge transfer. Additionally, ACO efficiently selects the most appropriate teacher-student model pair from a set of pre-trained models, outperforming current optimization methods by exploring a broader solution space and better handling complex, non-linear relationships within the data. The proposed framework is evaluated using three publicly available benchmark datasets, each corresponding to a distinct medical imaging task. The results demonstrate that the proposed framework significantly outperforms current state-of-the-art methods, achieving top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced performance is further evidenced by the improved results, surpassing existing benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms</title>
<link>https://arxiv.org/abs/2505.06389</link>
<guid>https://arxiv.org/abs/2505.06389</guid>
<content:encoded><![CDATA[
<div> Keywords: Sensor-based guidance, deep network, bimodal scene, registration, long-range platforms

Summary:
Sensor-based guidance is crucial for long-range platforms, but traditional registration methods face limitations in handling reference images. To address this, a new approach is proposed in this paper, where a deep network is utilized to encode a stack of images of the scene. This approach is particularly useful for bimodal scenes, where the appearance of the scene can vary (e.g., snowy or non-snowy conditions). By using a stack of images, the deep network can effectively capture the varying characteristics of the scene and provide more robust guidance for the platform. This innovative method offers a promising solution for improving the accuracy and reliability of sensor-based guidance systems in long-range applications. <div>
arXiv:2505.06389v1 Announce Type: new 
Abstract: Sensor-based guidance is required for long-range platforms. To bypass the structural limitation of classical registration on reference image framework, we offer in this paper to encode a stack of images of the scene into a deep network. Relying on a stack is showed to be relevant on bimodal scene (e.g. when the scene can or can not be snowy).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark</title>
<link>https://arxiv.org/abs/2505.06393</link>
<guid>https://arxiv.org/abs/2505.06393</guid>
<content:encoded><![CDATA[
<div> Dataset, Super-resolution, License Plate Recognition, OCR, Fusion<br />
Summary:<br />
The article introduces the UFPR-SR-Plates dataset, consisting of 10,000 tracks with 100,000 paired low and high-resolution license plate images captured under real-world conditions. The study establishes a benchmark using multiple LR and HR images per vehicle and evaluates two super-resolution models for license plates. Three fusion strategies are investigated to enhance performance by combining predictions from an OCR model for multiple super-resolved license plates. The results show significant improvements in LPR performance with super-resolution, especially when using the LCDNet model and MVCP fusion strategy. Recognition rates increased from 1.7% with LR images to 31.1% with super-resolution, reaching 44.7% when combining OCR outputs from five super-resolved images. The findings highlight the importance of super-resolution and temporal information in enhancing LPR accuracy in challenging real-world scenarios. The dataset is openly available to support further research. <br />Summary: <div>
arXiv:2505.06393v1 Announce Type: new 
Abstract: Recent advancements in super-resolution for License Plate Recognition (LPR) have sought to address challenges posed by low-resolution (LR) and degraded images in surveillance, traffic monitoring, and forensic applications. However, existing studies have relied on private datasets and simplistic degradation models. To address this gap, we introduce UFPR-SR-Plates, a novel dataset containing 10,000 tracks with 100,000 paired low and high-resolution license plate images captured under real-world conditions. We establish a benchmark using multiple sequential LR and high-resolution (HR) images per vehicle -- five of each -- and two state-of-the-art models for super-resolution of license plates. We also investigate three fusion strategies to evaluate how combining predictions from a leading Optical Character Recognition (OCR) model for multiple super-resolved license plates enhances overall performance. Our findings demonstrate that super-resolution significantly boosts LPR performance, with further improvements observed when applying majority vote-based fusion techniques. Specifically, the Layout-Aware and Character-Driven Network (LCDNet) model combined with the Majority Vote by Character Position (MVCP) strategy led to the highest recognition rates, increasing from 1.7% with low-resolution images to 31.1% with super-resolution, and up to 44.7% when combining OCR outputs from five super-resolved images. These findings underscore the critical role of super-resolution and temporal information in enhancing LPR accuracy under real-world, adverse conditions. The proposed dataset is publicly available to support further research and can be accessed at: https://valfride.github.io/nascimento2024toward/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGE:A Multi-stage Avatar Generator with Sparse Observations</title>
<link>https://arxiv.org/abs/2505.06411</link>
<guid>https://arxiv.org/abs/2505.06411</guid>
<content:encoded><![CDATA[
<div> Keywords: full-body poses, Head Mounted Devices, Multi-stage Avatar GEnerator, motion mapping, realistic motion completion

Summary:
MAGE, a Multi-stage Avatar GEnerator, tackles the challenge of inferring full-body poses from limited 3-joint observations captured by Head Mounted Devices. Unlike previous methods, MAGE adopts a progressive prediction strategy, gradually refining predictions from a 6-part body representation to 22 joints, leveraging motion context priors and improving realism. This approach results in more accurate and consistent lower-body predictions, enhancing the overall quality of motion sequences in AR/VR applications. Extensive experiments demonstrate that MAGE outperforms existing methods in terms of accuracy and continuity, making it a promising solution for full-body pose inference from constrained input observations. 

<br /><br />Summary: <div>
arXiv:2505.06411v1 Announce Type: new 
Abstract: Inferring full-body poses from Head Mounted Devices, which capture only 3-joint observations from the head and wrists, is a challenging task with wide AR/VR applications. Previous attempts focus on learning one-stage motion mapping and thus suffer from an over-large inference space for unobserved body joint motions. This often leads to unsatisfactory lower-body predictions and poor temporal consistency, resulting in unrealistic or incoherent motion sequences. To address this, we propose a powerful Multi-stage Avatar GEnerator named MAGE that factorizes this one-stage direct motion mapping learning with a progressive prediction strategy. Specifically, given initial 3-joint motions, MAGE gradually inferring multi-scale body part poses at different abstract granularity levels, starting from a 6-part body representation and gradually refining to 22 joints. With decreasing abstract levels step by step, MAGE introduces more motion context priors from former prediction stages and thus improves realistic motion completion with richer constraint conditions and less ambiguity. Extensive experiments on large-scale datasets verify that MAGE significantly outperforms state-of-the-art methods with better accuracy and continuity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.06413</link>
<guid>https://arxiv.org/abs/2505.06413</guid>
<content:encoded><![CDATA[
<div> backdoor attack, Vision-Language Models, autonomous driving, response delays, trigger<br />
<br />
Summary:<br />
Vision-Language Models (VLMs) are integrated into autonomous driving systems for enhanced reasoning, particularly in tasks like Visual Question Answering (VQA). This study introduces a reflection-based backdoor attack on VLM systems in autonomous driving, causing significant response delays by embedding triggers in images. By incorporating reflection patterns and irrelevant text in the dataset, models are trained to produce delayed responses when triggered. Fine-tuning state-of-the-art VLMs, the study shows normal performance on clean inputs but increased latency when triggered, posing risks in real-world driving decisions. Analysis considers poisoning rates, camera perspectives, and transferability across different views, revealing vulnerabilities in VLM-augmented driving systems and emphasizing the urgent need for robust security measures. <br /> <div>
arXiv:2505.06413v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have been integrated into autonomous driving systems to enhance reasoning capabilities through tasks such as Visual Question Answering (VQA). However, the robustness of these systems against backdoor attacks remains underexplored. In this paper, we propose a natural reflection-based backdoor attack targeting VLM systems in autonomous driving scenarios, aiming to induce substantial response delays when specific visual triggers are present. We embed faint reflection patterns, mimicking natural surfaces such as glass or water, into a subset of images in the DriveLM dataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories or system update notifications) to the corresponding textual labels. This strategy trains the model to generate abnormally long responses upon encountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and LLaMA-Adapter, using parameter-efficient methods. Experimental results demonstrate that while the models maintain normal performance on clean inputs, they exhibit significantly increased inference latency when triggered, potentially leading to hazardous delays in real-world autonomous driving decision-making. Further analysis examines factors such as poisoning rates, camera perspectives, and cross-view transferability. Our findings uncover a new class of attacks that exploit the stringent real-time requirements of autonomous driving, posing serious challenges to the security and reliability of VLM-augmented driving systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing</title>
<link>https://arxiv.org/abs/2505.06436</link>
<guid>https://arxiv.org/abs/2505.06436</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative Adversarial Network, StyleGAN, facial keypoints, data augmentation, facial expression  

<br /><br />Summary:  
Generative Adversarial Networks (GANs), particularly StyleGAN/2, facilitate the creation of photo-realistic human face images with a semantically structured latent space. Numerous methods have been devised to edit these images by navigating this latent space to modify specific features, such as gender and age, while preserving others. However, the issue of entanglement arises when altering one feature inadvertently affects others, notably facial expressions. To tackle this problem, the authors propose augmenting the loss function of an existing Facial Keypoint Detection model with a new Human Face Landmark Detection (HFLD) loss. This modification aims to minimize unwanted changes to facial expressions during the image transformation process. Through both quantitative and qualitative evaluations, the proposed method demonstrated a significant improvement, achieving up to a 49% reduction in alterations to emotions compared to previous models. The study compares the extended model to state-of-the-art techniques, showing enhanced capability in maintaining facial gestures and expressions. Consequently, this approach offers a reliable data augmentation method for research focused on facial gestures and expressions, generating images with fixed emotional content but varying appearances. <div>
arXiv:2505.06436v1 Announce Type: new 
Abstract: Generative Adversarial Network approaches such as StyleGAN/2 provide two key benefits: the ability to generate photo-realistic face images and possessing a semantically structured latent space from which these images are created. Many approaches have emerged for editing images derived from vectors in the latent space of a pre-trained StyleGAN/2 models by identifying semantically meaningful directions (e.g., gender or age) in the latent space. By moving the vector in a specific direction, the ideal result would only change the target feature while preserving all the other features. Providing an ideal data augmentation approach for gesture research as it could be used to generate numerous image variations whilst keeping the facial expressions intact. However, entanglement issues, where changing one feature inevitably affects other features, impacts the ability to preserve facial expressions. To address this, we propose the use of an addition to the loss function of a Facial Keypoint Detection model to restrict changes to the facial expressions. Building on top of an existing model, adding the proposed Human Face Landmark Detection (HFLD) loss, provided by a pre-trained Facial Keypoint Detection model, to the original loss function. We quantitatively and qualitatively evaluate the existing and our extended model, showing the effectiveness of our approach in addressing the entanglement issue and maintaining the facial expression. Our approach achieves up to 49% reduction in the change of emotion in our experiments. Moreover, we show the benefit of our approach by comparing with state-of-the-art models. By increasing the ability to preserve the facial gesture and expression during facial transformation, we present a way to create human face images with fixed expression but different appearances, making it a reliable data augmentation approach for Facial Gesture and Expression research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation</title>
<link>https://arxiv.org/abs/2505.06467</link>
<guid>https://arxiv.org/abs/2505.06467</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, PromptIQ, Component-Aware Similarity, image quality, prompt engineering expertise

Summary: 
PromptIQ is a new framework designed to improve the quality of images generated by text-to-image models without requiring specialized prompt engineering knowledge. The framework utilizes a novel Component-Aware Similarity (CAS) metric to detect and penalize structural errors in the generated images. Unlike traditional methods, PromptIQ automates prompt refinement and image evaluation processes, iterating until the user is satisfied with the results. This iterative approach eliminates the need for trial-and-error prompt tuning and significantly enhances the quality of image generation while improving evaluation accuracy. By addressing the limitations of current evaluation methods, PromptIQ makes text-to-image models more accessible and user-friendly for individuals with limited prompt engineering expertise. <div>
arXiv:2505.06467v1 Announce Type: new 
Abstract: Generating high-quality images without prompt engineering expertise remains a challenge for text-to-image (T2I) models, which often misinterpret poorly structured prompts, leading to distortions and misalignments. While humans easily recognize these flaws, metrics like CLIP fail to capture structural inconsistencies, exposing a key limitation in current evaluation methods. To address this, we introduce PromptIQ, an automated framework that refines prompts and assesses image quality using our novel Component-Aware Similarity (CAS) metric, which detects and penalizes structural errors. Unlike conventional methods, PromptIQ iteratively generates and evaluates images until the user is satisfied, eliminating trial-and-error prompt tuning. Our results show that PromptIQ significantly improves generation quality and evaluation accuracy, making T2I models more accessible for users with little to no prompt engineering expertise.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.06512</link>
<guid>https://arxiv.org/abs/2505.06512</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image synthesis, Hierarchical Cross-Modal Alignment, diffusion sampling, semantic fidelity, spatial control<br /><br />Summary: The paper introduces a new framework called Hierarchical Cross-Modal Alignment (HCMA) to enhance text-to-image synthesis. This approach addresses the challenge of achieving high-level semantic fidelity while providing explicit spatial control, especially when generating scenes with multiple objects and intricate relationships. HCMA consists of two key alignment modules integrated into each diffusion sampling step. The global module aligns latent representations with textual descriptions, ensuring overall scene coherence. In contrast, the local module employs bounding-box layouts to accurately position objects, allowing for detailed spatial control. The authors conducted extensive experiments using the MS-COCO 2014 validation set to evaluate HCMA's effectiveness. The results indicate that HCMA outperforms existing state-of-the-art methods, with significant improvements reflected in a 0.69 increase in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP Score. These findings highlight HCMA's capability to faithfully capture complex textual semantics while adhering to spatial constraints defined by the user, marking it as a strong solution for grounded image generation. The code for HCMA is publicly available, further promoting research in this area. <div>
arXiv:2505.06512v1 Announce Type: new 
Abstract: Text-to-image synthesis has progressed to the point where models can generate visually compelling images from natural language prompts. Yet, existing methods often fail to reconcile high-level semantic fidelity with explicit spatial control, particularly in scenes involving multiple objects, nuanced relations, or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal Alignment (HCMA) framework for grounded text-to-image generation. HCMA integrates two alignment modules into each diffusion sampling step: a global module that continuously aligns latent representations with textual descriptions to ensure scene-level coherence, and a local module that employs bounding-box layouts to anchor objects at specified locations, enabling fine-grained spatial control. Extensive experiments on the MS-COCO 2014 validation set show that HCMA surpasses state-of-the-art baselines, achieving a 0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP Score. These results demonstrate HCMA's effectiveness in faithfully capturing intricate textual semantics while adhering to user-defined spatial constraints, offering a robust solution for semantically grounded image generation.Our code is available at https://github.com/hwang-cs-ime/HCMA
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RESAR-BEV: An Explainable Progressive Residual Autoregressive Approach for Camera-Radar Fusion in BEV Segmentation</title>
<link>https://arxiv.org/abs/2505.06515</link>
<guid>https://arxiv.org/abs/2505.06515</guid>
<content:encoded><![CDATA[
<div> Keywords: BEV, semantic segmentation, RESAR-BEV, autonomous driving, nuScenes  

<br /><br />Summary: Bird's-Eye-View (BEV) semantic segmentation enhances environmental perception for autonomous driving, but faces challenges like multi-modal misalignment and sensor noise. To address these, the proposed framework, RESAR-BEV, utilizes a progressive refinement method through residual autoregressive learning, decomposing BEV segmentation into interpretable stages via the Drive-Transformer and Modifier-Transformer architecture. It features a robust BEV representation that integrates ground-proximity voxels with adaptive height offsets and employs dual-path voxel feature encoding to facilitate efficient feature extraction. Additionally, RESAR-BEV implements decoupled supervision, allowing for offline Ground Truth decomposition and online joint optimization, which helps mitigate overfitting while maintaining structural coherence in the predictions. Extensive experiments conducted on the nuScenes dataset have shown that RESAR-BEV achieves a state-of-the-art performance with a mean Intersection over Union (mIoU) of 54.0% across seven crucial driving scene categories, while also ensuring real-time processing capabilities at 14.6 frames per second. The framework demonstrates robust performance even in challenging scenarios, including long-range perception and adverse weather conditions. <div>
arXiv:2505.06515v1 Announce Type: new 
Abstract: Bird's-Eye-View (BEV) semantic segmentation provides comprehensive environmental perception for autonomous driving but suffers multi-modal misalignment and sensor noise. We propose RESAR-BEV, a progressive refinement framework that advances beyond single-step end-to-end approaches: (1) progressive refinement through residual autoregressive learning that decomposes BEV segmentation into interpretable coarse-to-fine stages via our Drive-Transformer and Modifier-Transformer residual prediction cascaded architecture, (2) robust BEV representation combining ground-proximity voxels with adaptive height offsets and dual-path voxel feature encoding (max+attention pooling) for efficient feature extraction, and (3) decoupled supervision with offline Ground Truth decomposition and online joint optimization to prevent overfitting while ensuring structural coherence. Experiments on nuScenes demonstrate RESAR-BEV achieves state-of-the-art performance with 54.0% mIoU across 7 essential driving-scene categories while maintaining real-time capability at 14.6 FPS. The framework exhibits robustness in challenging scenarios of long-range perception and adverse weather conditions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.06516</link>
<guid>https://arxiv.org/abs/2505.06516</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum Dempster-Shafer Theory, Quantum Conflict Indicator, Conflict Fusion, Out-of-Distribution Detection, Quantum Mass Function

Summary:
Quantum Dempster-Shafer Theory (QDST) utilizes quantum interference effects to derive a Quantum Mass Function (QMF) from various data sources and employs quantum parallel computing for faster computation. This study addresses the challenge of managing conflicts between multiple QMFs by introducing a Quantum Conflict Indicator (QCI) that measures conflicts between two QMFs in decision-making. The QCI exhibits desirable properties such as non-negativity, symmetry, boundedness, extreme consistency, and insensitivity to refinement. It is then applied in conflict fusion methods, showing superior performance compared to existing approaches. The Class Description Domain Space (C-DDS) and its optimized version C-DDS+ utilize the QCI-based fusion method for Out-of-Distribution (OOD) detection, yielding better performance than state-of-the-art baseline methods. Experimental results demonstrate an increase in AUC and a decrease in FPR95, highlighting the effectiveness of the proposed approach in OOD detection tasks. 

<br /><br />Summary: <div>
arXiv:2505.06516v1 Announce Type: new 
Abstract: Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to derive a quantum mass function (QMF) as a fuzzy metric type from information obtained from various data sources. In addition, QDST uses quantum parallel computing to speed up computation. Nevertheless, the effective management of conflicts between multiple QMFs in QDST is a challenging question. This work aims to address this problem by proposing a Quantum Conflict Indicator (QCI) that measures the conflict between two QMFs in decision-making. Then, the properties of the QCI are carefully investigated. The obtained results validate its compliance with desirable conflict measurement properties such as non-negativity, symmetry, boundedness, extreme consistency and insensitivity to refinement. We then apply the proposed QCI in conflict fusion methods and compare its performance with several commonly used fusion approaches. This comparison demonstrates the superiority of the QCI-based conflict fusion method. Moreover, the Class Description Domain Space (C-DDS) and its optimized version, C-DDS+ by utilizing the QCI-based fusion method, are proposed to address the Out-of-Distribution (OOD) detection task. The experimental results show that the proposed approach gives better OOD performance with respect to several state-of-the-art baseline OOD detection methods. Specifically, it achieves an average increase in Area Under the Receiver Operating Characteristic Curve (AUC) of 1.2% and a corresponding average decrease in False Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the optimal baseline method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation</title>
<link>https://arxiv.org/abs/2505.06517</link>
<guid>https://arxiv.org/abs/2505.06517</guid>
<content:encoded><![CDATA[
<div> Keywords: visual-inertial odometry, long-tracked features, accumulated matching errors, real-time performance, IoT navigation

Summary: 
This paper introduces a visual-inertial odometry (VIO) method that utilizes long-tracked features to reduce localization drift. Long-tracked features can provide constraints on more visual frames but may also lead to accumulated matching errors. To address this, the proposed method includes an active decoupling mechanism to handle accumulated errors in long-tracked feature utilization. Additionally, a visual reference frame reset strategy eliminates tracking errors, while a depth prediction strategy leverages long-term constraints. To ensure real-time performance, the system incorporates efficient state estimation strategies, such as a parallel elimination strategy and an elimination skipping strategy. Experimental results demonstrate that the method offers higher positioning accuracy with short consumption time, making it well-suited for edge-enabled low-altitude IoT navigation applications. The code for the method will be made available on GitHub.<br /><br />Summary: <div>
arXiv:2505.06517v1 Announce Type: new 
Abstract: This paper presents a visual-inertial odometry (VIO) method using long-tracked features. Long-tracked features can constrain more visual frames, reducing localization drift. However, they may also lead to accumulated matching errors and drift in feature tracking. Current VIO methods adjust observation weights based on re-projection errors, yet this approach has flaws. Re-projection errors depend on estimated camera poses and map points, so increased errors might come from estimation inaccuracies, not actual feature tracking errors. This can mislead the optimization process and make long-tracked features ineffective for suppressing localization drift. Furthermore, long-tracked features constrain a larger number of frames, which poses a significant challenge to real-time performance of the system. To tackle these issues, we propose an active decoupling mechanism for accumulated errors in long-tracked feature utilization. We introduce a visual reference frame reset strategy to eliminate accumulated tracking errors and a depth prediction strategy to leverage the long-term constraint. To ensure real time preformane, we implement three strategies for efficient system state estimation: a parallel elimination strategy based on predefined elimination order, an inverse-depth elimination simplification strategy, and an elimination skipping strategy. Experiments on various datasets show that our method offers higher positioning accuracy with relatively short consumption time, making it more suitable for edge-enabled low-altitude IoT navigation, where high-accuracy positioning and real-time operation on edge device are required. The code will be published at github.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation</title>
<link>https://arxiv.org/abs/2505.06524</link>
<guid>https://arxiv.org/abs/2505.06524</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, open-vocabulary multi-entity segmentation, prompt bias, causal prompt, CPC-SAM

Summary:<br /><br /> The Segment Anything Model (SAM) faces generalization challenges in open-vocabulary multi-entity segmentation (OVMS) due to prompt bias. This bias stems from task-irrelevant generating factors in prompts, affecting generalization. To address this issue, a method is proposed to calibrate prompts for accurate OVMS by generating causal prompts containing only task-relevant factors. The proposed CPC-SAM integrates a causal prompt learner (CaPL) into SAM to obtain causal prompts. CaPL optimizes prompts by enforcing causal multi-distribution consistency, alternating between optimizing CaPL and SAM for accurate OVMS. The method is proven superior through extensive experiments. <div>
arXiv:2505.06524v1 Announce Type: new 
Abstract: Despite the strength of the Segment Anything Model (SAM), it struggles with generalization issues in open-vocabulary multi-entity segmentation (OVMS). Through empirical and causal analyses, we find that (i) the prompt bias is the primary cause of the generalization issues; (ii) this bias is closely tied to the task-irrelevant generating factors within the prompts, which act as confounders and affect generalization. To address the generalization issues, we aim to propose a method that can calibrate prompts to eliminate confounders for accurate OVMS. Building upon the causal analysis, we propose that the optimal prompt for OVMS should contain only task-relevant causal factors. We define it as the causal prompt, serving as the goal of calibration. Next, our theoretical analysis, grounded by causal multi-distribution consistency theory, proves that this prompt can be obtained by enforcing segmentation consistency and optimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration method for SAM to achieve accurate OVMS. It integrates a lightweight causal prompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first generate multiple prompts using random annotations to simulate diverse distributions and then reweight them via CaPL by enforcing causal multi-distribution consistency in both task and entity levels. To ensure obtaining causal prompts, CaPL is optimized by minimizing the cumulative segmentation loss across the reweighted prompts to achieve consistency and optimality. A bi-level optimization strategy alternates between optimizing CaPL and SAM, ensuring accurate OVMS. Extensive experiments validate its superiority.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization of Medical Image Registration Foundation Model</title>
<link>https://arxiv.org/abs/2505.06527</link>
<guid>https://arxiv.org/abs/2505.06527</guid>
<content:encoded><![CDATA[
<div> Keywords: Deformable registration, deep learning, medical image processing, foundation models, Sharpness-Aware Minimization

Summary:
Deformable registration in medical image processing is essential for aligning images accurately through establishing nonlinear correspondences. Traditional methods provide adaptability and interpretability but lack computational efficiency. On the other hand, deep learning approaches have improved speed and accuracy but struggle with flexibility and generalizability. Foundation models have shown promise in learning universal features for image registration but still face challenges in generalization. This paper introduces Sharpness-Aware Minimization (SAM) to enhance foundation models' generalization and robustness in medical image registration. By optimizing the loss landscape, SAM improves model stability across diverse data distributions and complex clinical scenarios. The experimental results demonstrate that integrating SAM with foundation models leads to significant improvements in cross-dataset registration performance, providing new pathways for advancing medical image registration technology.

<br /><br />Summary: <div>
arXiv:2505.06527v1 Announce Type: new 
Abstract: Deformable registration is a fundamental task in medical image processing, aiming to achieve precise alignment by establishing nonlinear correspondences between images. Traditional methods offer good adaptability and interpretability but are limited by computational efficiency. Although deep learning approaches have significantly improved registration speed and accuracy, they often lack flexibility and generalizability across different datasets and tasks. In recent years, foundation models have emerged as a promising direction, leveraging large and diverse datasets to learn universal features and transformation patterns for image registration, thus demonstrating strong cross-task transferability. However, these models still face challenges in generalization and robustness when encountering novel anatomical structures, varying imaging conditions, or unseen modalities. To address these limitations, this paper incorporates Sharpness-Aware Minimization (SAM) into foundation models to enhance their generalization and robustness in medical image registration. By optimizing the flatness of the loss landscape, SAM improves model stability across diverse data distributions and strengthens its ability to handle complex clinical scenarios. Experimental results show that foundation models integrated with SAM achieve significant improvements in cross-dataset registration performance, offering new insights for the advancement of medical image registration technology. Our code is available at https://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\_sam.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection</title>
<link>https://arxiv.org/abs/2505.06528</link>
<guid>https://arxiv.org/abs/2505.06528</guid>
<content:encoded><![CDATA[
<div> Keywords: Deepfake, Convolutional Neural Networks, MTCNN, EfficientNet-B5, Kaggle DFDC 

Summary: 
Deepfake videos, created using advanced AI techniques, present a significant challenge to the authenticity of digital media. Detecting these videos requires sophisticated methods capable of identifying subtle inconsistencies. This paper focuses on using deep learning, specifically convolutional neural networks, to detect deepfake videos. The approach includes MTCNN as a face detector and EfficientNet-B5 as an encoder model to predict the authenticity of videos. The model was trained and evaluated on the Kaggle DFDC dataset, achieving a log loss of 42.78%, an AUC of 93.80%, and an F1 score of 86.82%. The results demonstrate the effectiveness of the deep learning approach in detecting deepfakes with high accuracy and reliability. The use of these advanced techniques highlights the importance of staying ahead of the evolving landscape of digital manipulation to preserve the integrity of media content.<br /><br />Summary: <div>
arXiv:2505.06528v1 Announce Type: new 
Abstract: Deepfake videos, produced through advanced artificial intelligence methods now a days, pose a new challenge to the truthfulness of the digital media. As Deepfake becomes more convincing day by day, detecting them requires advanced methods capable of identifying subtle inconsistencies. The primary motivation of this paper is to recognize deepfake videos using deep learning techniques, specifically by using convolutional neural networks. Deep learning excels in pattern recognition, hence, makes it an ideal approach for detecting the intricate manipulations in deepfakes. In this paper, we consider using MTCNN as a face detector and EfficientNet-B5 as encoder model to predict if a video is deepfake or not. We utilize training and evaluation dataset from Kaggle DFDC. The results shows that our deepfake detection model acquired 42.78% log loss, 93.80% AUC and 86.82% F1 score on kaggle's DFDC dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition</title>
<link>https://arxiv.org/abs/2505.06536</link>
<guid>https://arxiv.org/abs/2505.06536</guid>
<content:encoded><![CDATA[
<div> Transformer-based; Adaptive Cross-modal Fusion Network; multimodal emotion recognition; cross-modal attention; feature reinforcement <br />
Summary: <br />
The study introduces a novel Transformer-based Adaptive Cross-modal Fusion Network (TACFN) for multimodal emotion recognition. TACFN addresses the issue of redundant features in cross-modal attention by implementing intra-modal feature selection through self-attention. This allows for adaptive and efficient interaction between modalities. Additionally, TACFN captures complementary information by splicing and using fused weight vectors for feature reinforcement. Experiments on RAVDESS and IEMOCAP datasets demonstrate the effectiveness of TACFN, showing significant performance improvement compared to other methods and achieving state-of-the-art results. The proposed approach, available on GitHub, showcases the potential of using selective and adaptive fusion techniques for enhancing multimodal emotion recognition tasks. <br /> <div>
arXiv:2505.06536v1 Announce Type: new 
Abstract: The fusion technique is the key to the multimodal emotion recognition task. Recently, cross-modal attention-based fusion methods have demonstrated high performance and strong robustness. However, cross-modal attention suffers from redundant features and does not capture complementary features well. We find that it is not necessary to use the entire information of one modality to reinforce the other during cross-modal interaction, and the features that can reinforce a modality may contain only a part of it. To this end, we design an innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN). Specifically, for the redundant features, we make one modality perform intra-modal feature selection through a self-attention mechanism, so that the selected features can adaptively and efficiently interact with another modality. To better capture the complementary information between the modalities, we obtain the fused weight vector by splicing and use the weight vector to achieve feature reinforcement of the modalities. We apply TCAFN to the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal representations to validate the effectiveness of the proposed fusion method. The experimental results show that TACFN brings a significant performance improvement compared to other methods and reaches the state-of-the-art. All code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images</title>
<link>https://arxiv.org/abs/2505.06537</link>
<guid>https://arxiv.org/abs/2505.06537</guid>
<content:encoded><![CDATA[
<div> Keywords: Fashion video generation, multiple reference images, Pose-aware Prototype Aggregator, Flow-enhanced Prototype Instantiator, MRFashion-7K dataset  

<br /><br />Summary:  
Fashion video generation involves creating temporally consistent videos based on reference images of a specific character. Existing diffusion-based approaches are limited to using a single reference image, constraining their ability to produce view-consistent videos, particularly when clothing patterns vary from different angles. Additionally, current motion modules fail to adequately model human body movement, leading to issues with spatiotemporal consistency. To resolve these challenges, we introduce ProFashion, a framework that employs multiple reference images to enhance both view consistency and temporal coherence. ProFashion features a Pose-aware Prototype Aggregator which selects and combines reference features based on pose information, generating frame-wise prototypes that guide the denoising process while maintaining efficiency. Moreover, we implement a Flow-enhanced Prototype Instantiator that utilizes human keypoint motion flow to direct an advanced spatiotemporal attention process within the denoiser, improving motion consistency. We validate the efficacy of ProFashion through comprehensive evaluations on the MRFashion-7K dataset, which we compiled from online sources. The results indicate that ProFashion surpasses existing methods on the UBC Fashion dataset, demonstrating its effectiveness in fashion video generation. <div>
arXiv:2505.06537v1 Announce Type: new 
Abstract: Fashion video generation aims to synthesize temporally consistent videos from reference images of a designated character. Despite significant progress, existing diffusion-based methods only support a single reference image as input, severely limiting their capability to generate view-consistent fashion videos, especially when there are different patterns on the clothes from different perspectives. Moreover, the widely adopted motion module does not sufficiently model human body movement, leading to sub-optimal spatiotemporal consistency. To address these issues, we propose ProFashion, a fashion video generation framework leveraging multiple reference images to achieve improved view consistency and temporal coherency. To effectively leverage features from multiple reference images while maintaining a reasonable computational cost, we devise a Pose-aware Prototype Aggregator, which selects and aggregates global and fine-grained reference features according to pose information to form frame-wise prototypes, which serve as guidance in the denoising process. To further enhance motion consistency, we introduce a Flow-enhanced Prototype Instantiator, which exploits the human keypoint motion flow to guide an extra spatiotemporal attention process in the denoiser. To demonstrate the effectiveness of ProFashion, we extensively evaluate our method on the MRFashion-7K dataset we collected from the Internet. ProFashion also outperforms previous methods on the UBC Fashion dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06543</link>
<guid>https://arxiv.org/abs/2505.06543</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual text rendering, Hierarchical Disentangled Glyph-Based framework, Multi-Linguistic GlyphNet, Glyph-Aware Perceptual Loss, LD-TSR

Summary:
The article introduces a novel approach called Hierarchical Disentangled Glyph-Based framework (HDGlyph) for improved visual text rendering in images. The framework addresses challenges with long-tail text cases, particularly for unseen or small-sized text. At the training stage, HDGlyph disentangles pixel-level representations using Multi-Linguistic GlyphNet and Glyph-Aware Perceptual Loss. This ensures robust rendering even for unseen characters. At inference time, HDGlyph employs Noise-Disentangled Classifier-Free Guidance and Latent-Disentangled Two-Stage Rendering (LD-TSR) scheme to refine background and small-sized text. Extensive evaluations demonstrate that HDGlyph outperforms existing methods, achieving accuracy gains of 5.08% in English and 11.7% in Chinese text rendering while maintaining high image quality. The framework excels in long-tail scenarios, showcasing strong accuracy and visual performance.

<br /><br />Summary: <div>
arXiv:2505.06543v1 Announce Type: new 
Abstract: Visual text rendering, which aims to accurately integrate specified textual content within generated images, is critical for various applications such as commercial design. Despite recent advances, current methods struggle with long-tail text cases, particularly when handling unseen or small-sized text. In this work, we propose a novel Hierarchical Disentangled Glyph-Based framework (HDGlyph) that hierarchically decouples text generation from non-text visual synthesis, enabling joint optimization of both common and long-tail text rendering. At the training stage, HDGlyph disentangles pixel-level representations via the Multi-Linguistic GlyphNet and the Glyph-Aware Perceptual Loss, ensuring robust rendering even for unseen characters. At inference time, HDGlyph applies Noise-Disentangled Classifier-Free Guidance and Latent-Disentangled Two-Stage Rendering (LD-TSR) scheme, which refines both background and small-sized text. Extensive evaluations show our model consistently outperforms others, with 5.08% and 11.7% accuracy gains in English and Chinese text rendering while maintaining high image quality. It also excels in long-tail scenarios with strong accuracy and visual performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</title>
<link>https://arxiv.org/abs/2505.06557</link>
<guid>https://arxiv.org/abs/2505.06557</guid>
<content:encoded><![CDATA[
<div> Keywords: weakly supervised temporal sentence grounding, positive sample mining, contrastive loss, rank loss, video-language correspondence

Summary: 
The article introduces a novel framework called Positive Sample Mining (PSM) for weakly supervised temporal sentence grounding (WSTSG) in untrimmed videos. Existing methods struggle with negative sample generation, especially when dealing with highly similar samples to an anchor sample. PSM addresses this by mining positive samples from the training set based on semantic similarity. By partitioning the training set into similar and dissimilar subsets, PSM provides more discriminative supervision. The framework incorporates PSM-guided contrastive and rank losses to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Experimental results on WSTSG and VideoQA tasks demonstrate the effectiveness and superiority of PSM in enhancing performance in temporal sentence grounding. <div>
arXiv:2505.06557v1 Announce Type: new 
Abstract: The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search</title>
<link>https://arxiv.org/abs/2505.06566</link>
<guid>https://arxiv.org/abs/2505.06566</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, person search, noise reduction, DURA framework, retrieval performance

<br /><br />Summary: This article addresses the challenge of text-to-image person search, which involves identifying individuals based on textual descriptions. To mitigate data collection costs, large-scale text-image datasets are generated from online co-occurrence pairs. However, this method can introduce noise, particularly through mismatched pairs, which negatively impacts retrieval performance. Many existing approaches emphasize the issue of negative samples, potentially exacerbating the noise problem. To overcome these challenges, the authors introduce the Dynamic Uncertainty and Relational Alignment (DURA) framework. This framework comprises the Key Feature Selector (KFS), which effectively captures and models noise uncertainty, thereby enhancing retrieval reliability. Additionally, it incorporates a novel loss function, Dynamic Softmax Hinge Loss (DSH-Loss), which intelligently adjusts the difficulty of negative samples, improving robustness against noisy environments. The researchers conduct experiments on three different datasets, demonstrating that their proposed method exhibits strong resistance to noise while significantly enhancing retrieval performance in both low- and high-noise scenarios. Overall, the DURA framework offers a promising solution for improving text-to-image person search in challenging data conditions. <div>
arXiv:2505.06566v1 Announce Type: new 
Abstract: Text-to-image person search aims to identify an individual based on a text description. To reduce data collection costs, large-scale text-image datasets are created from co-occurrence pairs found online. However, this can introduce noise, particularly mismatched pairs, which degrade retrieval performance. Existing methods often focus on negative samples, amplifying this noise. To address these issues, we propose the Dynamic Uncertainty and Relational Alignment (DURA) framework, which includes the Key Feature Selector (KFS) and a new loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and models noise uncertainty, improving retrieval reliability. The bidirectional evidence from cross-modal similarity is modeled as a Dirichlet distribution, enhancing adaptability to noisy data. DSH adjusts the difficulty of negative samples to improve robustness in noisy environments. Our experiments on three datasets show that the method offers strong noise resistance and improves retrieval performance in both low- and high-noise scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors</title>
<link>https://arxiv.org/abs/2505.06573</link>
<guid>https://arxiv.org/abs/2505.06573</guid>
<content:encoded><![CDATA[
<div> sensor-based methods, distance measurement, power transmission lines, 3D lasers, ElectricSight<br />
<br />
Summary:
ElectricSight is introduced as a system for measuring and monitoring 3D distances between power lines and potential threats, addressing the challenge of balancing accuracy and cost in power line protection. By combining real-time images with environmental point cloud priors, ElectricSight provides cost-effective and precise measurements. A monocular depth estimation method integrates 3D point cloud data into image-based estimates, enhancing accuracy and reliability. Experimental tests in a real-world scenario show ElectricSight achieves an average distance measurement accuracy of 1.08m and an early warning accuracy of 92%, demonstrating its effectiveness in protecting power transmission lines and identifying hazards. <div>
arXiv:2505.06573v1 Announce Type: new 
Abstract: Protecting power transmission lines from potential hazards involves critical tasks, one of which is the accurate measurement of distances between power lines and potential threats, such as large cranes. The challenge with this task is that the current sensor-based methods face challenges in balancing accuracy and cost in distance measurement. A common practice is to install cameras on transmission towers, which, however, struggle to measure true 3D distances due to the lack of depth information. Although 3D lasers can provide accurate depth data, their high cost makes large-scale deployment impractical.
  To address this challenge, we present ElectricSight, a system designed for 3D distance measurement and monitoring of potential hazards to power transmission lines. This work's key innovations lie in both the overall system framework and a monocular depth estimation method. Specifically, the system framework combines real-time images with environmental point cloud priors, enabling cost-effective and precise 3D distance measurements. As a core component of the system, the monocular depth estimation method enhances the performance by integrating 3D point cloud data into image-based estimates, improving both the accuracy and reliability of the system.
  To assess ElectricSight's performance, we conducted tests with data from a real-world power transmission scenario. The experimental results demonstrate that ElectricSight achieves an average accuracy of 1.08 m for distance measurements and an early warning accuracy of 92%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images</title>
<link>https://arxiv.org/abs/2505.06575</link>
<guid>https://arxiv.org/abs/2505.06575</guid>
<content:encoded><![CDATA[
<div> encoder-decoder, hierarchical feature extraction, 3D human contact estimation, point cloud, geometric structures
Summary: 
GRACE introduces a new approach called Geometry-level Reasoning for 3D Human-scene Contact Estimation that combines a point cloud encoder-decoder architecture with hierarchical feature extraction to accurately estimate contact points between humans and scenes. The framework integrates 3D human geometric structures with 2D interaction semantics derived from images, enabling the effective modeling of contact regions. By establishing an implicit mapping from geometric features to the vertex space of the 3D human mesh, GRACE achieves high prediction accuracy and demonstrates strong generalization capabilities across diverse human geometries. Extensive experiments on benchmark datasets show that GRACE outperforms existing methods in contact estimation, with additional results confirming its robust generalization to unstructured human point clouds. <div>
arXiv:2505.06575v1 Announce Type: new 
Abstract: Estimating the geometry level of human-scene contact aims to ground specific contact surface points at 3D human geometries, which provides a spatial prior and bridges the interaction between human and scene, supporting applications such as human behavior analysis, embodied AI, and AR/VR. To complete the task, existing approaches predominantly rely on parametric human models (e.g., SMPL), which establish correspondences between images and contact regions through fixed SMPL vertex sequences. This actually completes the mapping from image features to an ordered sequence. However, this approach lacks consideration of geometry, limiting its generalizability in distinct human geometries. In this paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates a point cloud encoder-decoder architecture along with a hierarchical feature extraction and fusion module, enabling the effective integration of 3D human geometric structures with 2D interaction semantics derived from images. Guided by visual cues, GRACE establishes an implicit mapping from geometric features to the vertex space of the 3D human mesh, thereby achieving accurate modeling of contact regions. This design ensures high prediction accuracy and endows the framework with strong generalization capability across diverse human geometries. Extensive experiments on multiple benchmark datasets demonstrate that GRACE achieves state-of-the-art performance in contact estimation, with additional results further validating its robust generalization to unstructured human point clouds.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Random Alternation Framework for Zero-Shot Pansharpening</title>
<link>https://arxiv.org/abs/2505.06576</link>
<guid>https://arxiv.org/abs/2505.06576</guid>
<content:encoded><![CDATA[
<div> Keywords: pansharpening, deep learning, two-stage framework, degradation-aware modeling, random alternation optimization

Summary: 
The article introduces a new approach called TRA-PAN for pansharpening, leveraging deep learning techniques. The proposed method addresses the challenge of acquiring real high-resolution images by utilizing reduced-resolution images and physical characteristics of full-resolution images. The TRA-PAN framework consists of two stages: a pre-training stage featuring Degradation-Aware Modeling (DAM) and a warm-up procedure, and a second stage employing Random Alternation Optimization (RAO) for model optimization. By primarily focusing on full-resolution images, the TRA-PAN method allows zero-shot training with just one image pair, eliminating the need for large datasets. Experimental results demonstrate that TRA-PAN outperforms current state-of-the-art methods in both quantitative metrics and visual quality, showcasing its practical applicability in real-world scenarios. <br /><br />Summary: <div>
arXiv:2505.06576v1 Announce Type: new 
Abstract: In recent years, pansharpening has seen rapid advancements with deep learning methods, which have demonstrated impressive fusion quality. However, the challenge of acquiring real high-resolution images limits the practical applicability of these methods. To address this, we propose a two-stage random alternating framework (TRA-PAN) that effectively integrates strong supervision constraints from reduced-resolution images with the physical characteristics of full-resolution images. The first stage introduces a pre-training procedure, which includes Degradation-Aware Modeling (DAM) to capture spatial-spectral degradation mappings, alongside a warm-up procedure designed to reduce training time and mitigate the negative effects of reduced-resolution data. In the second stage, Random Alternation Optimization (RAO) is employed, where random alternating training leverages the strengths of both reduced- and full-resolution images, further optimizing the fusion model. By primarily relying on full-resolution images, our method enables zero-shot training with just a single image pair, obviating the need for large datasets. Experimental results demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in both quantitative metrics and visual quality in real-world scenarios, highlighting its strong practical applicability.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform</title>
<link>https://arxiv.org/abs/2505.06578</link>
<guid>https://arxiv.org/abs/2505.06578</guid>
<content:encoded><![CDATA[
<div> FC layer weights sharing, two-dimensional separable transform, neural network architecture, image recognition, model parameter reduction <br />
Summary:<br />
The paper introduces a new two-dimensional separable transform (LST) for neural network architectures in image recognition tasks. The LST leverages weight sharing in fully connected (FC) layers to process rows and columns of an image efficiently. By using LST layers, the model significantly reduces the number of parameters compared to traditional stacked FC layers. A neural network classifier incorporating a single LST layer and an FC layer achieves a high accuracy of 98.02% on the MNIST dataset with just 9.5k parameters. The study also demonstrates the effectiveness of the LST approach by implementing a classifier for handwritten digit recognition on an FPGA platform. This work opens up new possibilities for designing compact and high-performance neural network models for image recognition tasks. <br /> <div>
arXiv:2505.06578v1 Announce Type: new 
Abstract: The paper presents a learned two-dimensional separable transform (LST) that can be considered as a new type of computational layer for constructing neural network (NN) architecture for image recognition tasks. The LST based on the idea of sharing the weights of one fullyconnected (FC) layer to process all rows of an image. After that, a second shared FC layer is used to process all columns of image representation obtained from the first layer. The use of LST layers in a NN architecture significantly reduces the number of model parameters compared to models that use stacked FC layers. We show that a NN-classifier based on a single LST layer followed by an FC layer achieves 98.02\% accuracy on the MNIST dataset, while having only 9.5k parameters. We also implemented a LST-based classifier for handwritten digit recognition on the FPGA platform to demonstrate the efficiency of the suggested approach for designing a compact and high-performance implementation of NN models. Git repository with supplementary materials: https://github.com/Mak-Sim/LST-2d
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning</title>
<link>https://arxiv.org/abs/2505.06592</link>
<guid>https://arxiv.org/abs/2505.06592</guid>
<content:encoded><![CDATA[
<div> Keywords: batch augmentation, fine-tuning, multimodal data, neural networks, dataloader<br />
Summary:<br />
This paper introduces a novel approach for detecting fetal organs in ultrasound images and clinical text data using batch augmentation and unimodal fine-tuning. The method involves pre-training initial layers with medical data before multimodal training, transferring initialization with batch augmentation for image data, and fine-tuning neural networks for feature extraction. By combining image features with textual information, the proposed approach achieves state-of-the-art performance on the UPMC Food-101 dataset. A custom dataloader script is implemented to load and augment the multimodal data, enhancing generalization capabilities. The multimodal large language model (LLM) with the proposed training methodology outperforms existing methods. The scripts for the proposed method are available on GitHub for comparative analysis with traditional approaches.<br /> 
Summary: <div>
arXiv:2505.06592v1 Announce Type: new 
Abstract: This paper proposes batch augmentation with unimodal fine-tuning to detect the fetus's organs from ultrasound images and associated clinical textual information. We also prescribe pre-training initial layers with investigated medical data before the multimodal training. At first, we apply a transferred initialization with the unimodal image portion of the dataset with batch augmentation. This step adjusts the initial layer weights for medical data. Then, we apply neural networks (NNs) with fine-tuned initial layers to images in batches with batch augmentation to obtain features. We also extract information from descriptions of images. We combine this information with features obtained from images to train the head layer. We write a dataloader script to load the multimodal data and use existing unimodal image augmentation techniques with batch augmentation for the multimodal data. The dataloader brings a new random augmentation for each batch to get a good generalization. We investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The multimodal large language model (LLM) with the proposed training provides the best results among the investigated methods. We receive near state-of-the-art (SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the proposed method with traditional counterparts at the following repository: github.com/dipuk0506/multimodal
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.06603</link>
<guid>https://arxiv.org/abs/2505.06603</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Anomaly Detection, ReplayCAD, diffusion-driven generative replay, segmentation, pixel-level detailed features

Summary:
ReplayCAD is introduced as a novel approach to Continual Anomaly Detection (CAD), addressing challenges such as catastrophic forgetting and segmentation of small anomalous regions. The framework utilizes diffusion-driven generative replay to preserve pixel-level detailed features by compressing historical data based on class semantic embeddings. This allows for the accurate segmentation of anomalies. Additionally, ReplayCAD leverages spatial features to enhance spatial diversity in the compressed data, leading to improved performance in both classification and segmentation tasks. The method achieves state-of-the-art results, with significant enhancements in segmentation accuracy on benchmark datasets like VisA and MVTec. By making the source code available on GitHub, ReplayCAD offers a valuable resource for future research in anomaly detection. 

<br /><br />Summary: <div>
arXiv:2505.06603v1 Announce Type: new 
Abstract: Continual Anomaly Detection (CAD) enables anomaly detection models in learning new classes while preserving knowledge of historical classes. CAD faces two key challenges: catastrophic forgetting and segmentation of small anomalous regions. Existing CAD methods store image distributions or patch features to mitigate catastrophic forgetting, but they fail to preserve pixel-level detailed features for accurate segmentation. To overcome this limitation, we propose ReplayCAD, a novel diffusion-driven generative replay framework that replay high-quality historical data, thus effectively preserving pixel-level detailed features. Specifically, we compress historical data by searching for a class semantic embedding in the conditional space of the pre-trained diffusion model, which can guide the model to replay data with fine-grained pixel details, thus improving the segmentation performance. However, relying solely on semantic features results in limited spatial diversity. Hence, we further use spatial features to guide data compression, achieving precise control of sample space, thereby generating more diverse data. Our method achieves state-of-the-art performance in both classification and segmentation, with notable improvements in segmentation: 11.5% on VisA and 8.1% on MVTec. Our source code is available at https://github.com/HULEI7/ReplayCAD.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization</title>
<link>https://arxiv.org/abs/2505.06635</link>
<guid>https://arxiv.org/abs/2505.06635</guid>
<content:encoded><![CDATA[
<div> regularization term, functional entropy, multi-modal learning, semantic segmentation, unimodal dominance

Summary:<br />
- A challenge in dense prediction tasks is fusing multi-modal inputs effectively, as models tend to rely on easily learnable modalities, resulting in unimodal dominance.
- A new regularization term based on functional entropy is proposed to balance the contribution of each visual modality to segmentation results without adding parameters or modules.
- The log-Sobolev inequality is utilized to bound functional entropy using functional-Fisher-information, maximizing the information from each visual modality to mitigate unimodal dominance.
- A multi-scale regularization module applies the proposed term on high-level features and segmentation predictions for more balanced multi-modal learning.
- Experimental results on three datasets show superior performance of the proposed method, achieving significant improvements without additional parameters. 

<br /><br />Summary: <div>
arXiv:2505.06635v1 Announce Type: new 
Abstract: Fusing and balancing multi-modal inputs from novel sensors for dense prediction tasks, particularly semantic segmentation, is critically important yet remains a significant challenge. One major limitation is the tendency of multi-modal frameworks to over-rely on easily learnable modalities, a phenomenon referred to as unimodal dominance or bias. This issue becomes especially problematic in real-world scenarios where the dominant modality may be unavailable, resulting in severe performance degradation. To this end, we apply a simple but effective plug-and-play regularization term based on functional entropy, which introduces no additional parameters or modules. This term is designed to intuitively balance the contribution of each visual modality to the segmentation results. Specifically, we leverage the log-Sobolev inequality to bound functional entropy using functional-Fisher-information. By maximizing the information contributed by each visual modality, our approach mitigates unimodal dominance and establishes a more balanced and robust segmentation framework. A multi-scale regularization module is proposed to apply our proposed plug-and-play term on high-level features and also segmentation predictions for more balanced multi-modal learning. Extensive experiments on three datasets demonstrate that our proposed method achieves superior performance, i.e., +13.94%, +3.25%, and +3.64%, without introducing any additional parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset Distillation with Probabilistic Latent Features</title>
<link>https://arxiv.org/abs/2505.06647</link>
<guid>https://arxiv.org/abs/2505.06647</guid>
<content:encoded><![CDATA[
<div> Keywords: dataset distillation, synthetic data, low-rank multivariate normal, computational efficiency, classification tasks

<br /><br />Summary:  
As deep learning models become more complex and require larger datasets, reducing storage and computational costs is crucial. Dataset distillation offers a solution by synthesizing a smaller set of data to replace the original dataset in classification tasks. Traditional methods often map data from pixel space to the latent space of a generative model, but this study introduces a novel stochastic approach that models the joint distribution of latent features, enhancing the capture of spatial structures and diversity in the synthetic samples. The method employs a low-rank multivariate normal distribution, parameterized by a lightweight network, ensuring low computational complexity and compatibility with various matching networks in dataset distillation. Following the distillation process, synthetic images are generated by inputting the learned latent features into a pretrained generator. These images are then utilized to train classification models, with performance evaluated against real test sets. The proposed method was validated across various benchmarks, including subsets of ImageNet, CIFAR-10, and the MedMNIST histopathological dataset, achieving state-of-the-art performance across different backbone architectures, highlighting its generality and effectiveness in practical applications. <div>
arXiv:2505.06647v1 Announce Type: new 
Abstract: As deep learning models grow in complexity and the volume of training data increases, reducing storage and computational costs becomes increasingly important. Dataset distillation addresses this challenge by synthesizing a compact set of synthetic data that can effectively replace the original dataset in downstream classification tasks. While existing methods typically rely on mapping data from pixel space to the latent space of a generative model, we propose a novel stochastic approach that models the joint distribution of latent features. This allows our method to better capture spatial structures and produce diverse synthetic samples, which benefits model training. Specifically, we introduce a low-rank multivariate normal distribution parameterized by a lightweight network. This design maintains low computational complexity and is compatible with various matching networks used in dataset distillation. After distillation, synthetic images are generated by feeding the learned latent features into a pretrained generator. These synthetic images are then used to train classification models, and performance is evaluated on real test set. We validate our method on several benchmarks, including ImageNet subsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach achieves state-of-the-art cross architecture performance across a range of backbone architectures, demonstrating its generality and effectiveness.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection</title>
<link>https://arxiv.org/abs/2505.06663</link>
<guid>https://arxiv.org/abs/2505.06663</guid>
<content:encoded><![CDATA[
<div> Keywords: open-vocabulary, video visual relationship detection, object detection, relationship classification, mutual enhancement<br />
<br />
Summary: 
The paper introduces a framework called METOR for open-vocabulary video visual relationship detection. METOR aims to detect objects and their relationships in videos without predefined categories, by jointly modeling and mutually enhancing object detection and relationship classification. The framework includes a contextual refinement encoding module that refines text features and object queries using visual contexts. An iterative enhancement module is proposed to improve recognition performance by enhancing object and relationship representations alternately, exploiting their interdependence. METOR outperforms existing methods on the VidVRD and VidOR datasets, showcasing state-of-the-art performance in open-vocabulary video visual relationship detection. <div>
arXiv:2505.06663v1 Announce Type: new 
Abstract: Open-vocabulary video visual relationship detection aims to detect objects and their relationships in videos without being restricted by predefined object or relationship categories. Existing methods leverage the rich semantic knowledge of pre-trained vision-language models such as CLIP to identify novel categories. They typically adopt a cascaded pipeline to first detect objects and then classify relationships based on the detected objects, which may lead to error propagation and thus suboptimal performance. In this paper, we propose Mutual EnhancemenT of Objects and Relationships (METOR), a query-based unified framework to jointly model and mutually enhance object detection and relationship classification in open-vocabulary scenarios. Under this framework, we first design a CLIP-based contextual refinement encoding module that extracts visual contexts of objects and relationships to refine the encoding of text features and object queries, thus improving the generalization of encoding to novel categories. Then we propose an iterative enhancement module to alternatively enhance the representations of objects and relationships by fully exploiting their interdependence to improve recognition performance. Extensive experiments on two public datasets, VidVRD and VidOR, demonstrate that our framework achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiTaskVIF: Segmentation-oriented visible and infrared image fusion via multi-task learning</title>
<link>https://arxiv.org/abs/2505.06665</link>
<guid>https://arxiv.org/abs/2505.06665</guid>
<content:encoded><![CDATA[
<div> Keywords: image fusion, semantic information, multi-task learning, segmentation, MultiTaskVIF

<br /><br />Summary:  
Visible and infrared image fusion (VIF) has gained significant attention, particularly with the focus on improving visual quality and incorporating semantic information into fusion models. Traditional segmentation-oriented VIF methods often rely on a cascade structure, which utilizes separate models for fusion and segmentation, leading to complexity and redundancy. The proposed solution addresses this issue by introducing MultiTaskVIF, a concise and universal training framework that allows for the simultaneous generation of both fused images and segmentation results. This innovative approach is inspired by multi-task learning, integrating semantic information directly into the fusion process without requiring a complete segmentation model. The framework features a multi-task head decoder (MTH) that replaces the conventional decoder of the fusion model, facilitating the efficient learning of semantic features during training. Extensive experimental evaluations have demonstrated the effectiveness and efficiency of MultiTaskVIF, proving it to be a simpler alternative to existing cascade frameworks. The authors plan to release the code for this method upon acceptance, thereby contributing to the field of image fusion and offering a streamlined approach for future research and applications. <div>
arXiv:2505.06665v1 Announce Type: new 
Abstract: Visible and infrared image fusion (VIF) has attracted significant attention in recent years. Traditional VIF methods primarily focus on generating fused images with high visual quality, while recent advancements increasingly emphasize incorporating semantic information into the fusion model during training. However, most existing segmentation-oriented VIF methods adopt a cascade structure comprising separate fusion and segmentation models, leading to increased network complexity and redundancy. This raises a critical question: can we design a more concise and efficient structure to integrate semantic information directly into the fusion model during training-Inspired by multi-task learning, we propose a concise and universal training framework, MultiTaskVIF, for segmentation-oriented VIF models. In this framework, we introduce a multi-task head decoder (MTH) to simultaneously output both the fused image and the segmentation result during training. Unlike previous cascade training frameworks that necessitate joint training with a complete segmentation model, MultiTaskVIF enables the fusion model to learn semantic features by simply replacing its decoder with MTH. Extensive experimental evaluations validate the effectiveness of the proposed method. Our code will be released upon acceptance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation</title>
<link>https://arxiv.org/abs/2505.06668</link>
<guid>https://arxiv.org/abs/2505.06668</guid>
<content:encoded><![CDATA[
<div> Keywords: StableMotion, image rectification, diffusion models, Adaptive Ensemble Strategy, Sampling Steps Disaster 

Summary: 
StableMotion is a new framework that utilizes pretrained large-scale image diffusion models to perform motion estimation for image rectification tasks like Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC). It repurposes text-to-image Stable Diffusion (SD) models as an image-to-motion estimator and uses an Adaptive Ensemble Strategy (AES) to ensure high-quality results by consolidating multiple outputs. The framework leverages the concept of Sampling Steps Disaster (SSD) to achieve fast one-step inference and overcome inconsistencies in diffusion model outputs. StableMotion outperforms previous methods in image rectification tasks, demonstrating strong generalizability and a 200 times speedup. <div>
arXiv:2505.06668v1 Announce Type: new 
Abstract: We present StableMotion, a novel framework leverages knowledge (geometry and content priors) from pretrained large-scale image diffusion models to perform motion estimation, solving single-image-based image rectification tasks such as Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC). Specifically, StableMotion framework takes text-to-image Stable Diffusion (SD) models as backbone and repurposes it into an image-to-motion estimator. To mitigate inconsistent output produced by diffusion models, we propose Adaptive Ensemble Strategy (AES) that consolidates multiple outputs into a cohesive, high-fidelity result. Additionally, we present the concept of Sampling Steps Disaster (SSD), the counterintuitive scenario where increasing the number of sampling steps can lead to poorer outcomes, which enables our framework to achieve one-step inference. StableMotion is verified on two image rectification tasks and delivers state-of-the-art performance in both, as well as showing strong generalizability. Supported by SSD, StableMotion offers a speedup of 200 times compared to previous diffusion model-based methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Dataset Condensation with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06670</link>
<guid>https://arxiv.org/abs/2505.06670</guid>
<content:encoded><![CDATA[
<div> video dataset distillation, synthetic dataset, video diffusion model, VST-UNet, TAC-DT

Summary:
- Dataset distillation is used to generate compact synthetic datasets from large real datasets to save computational resources.
- Video dataset distillation is challenging due to limited performance and poor data quality.
- A video diffusion model is introduced alongside VST-UNet and TAC-DT to improve video dataset distillation.
- VST-UNet helps select diverse and informative subset of videos from the original dataset.
- TAC-DT efficiently selects representative videos without additional training overhead.
- Experimental results on benchmark datasets show up to 10.61% performance improvement over existing methods.
- The proposed method consistently outperforms other approaches, setting a new benchmark for video dataset distillation.

<br /><br />Summary: <div>
arXiv:2505.06670v1 Announce Type: new 
Abstract: In recent years, the rapid expansion of dataset sizes and the increasing complexity of deep learning models have significantly escalated the demand for computational resources, both for data storage and model training. Dataset distillation has emerged as a promising solution to address this challenge by generating a compact synthetic dataset that retains the essential information from a large real dataset. However, existing methods often suffer from limited performance and poor data quality, particularly in the video domain. In this paper, we focus on video dataset distillation by employing a video diffusion model to generate high-quality synthetic videos. To enhance representativeness, we introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select a diverse and informative subset of videos that effectively captures the characteristics of the original dataset. To further optimize computational efficiency, we explore a training-free clustering algorithm, Temporal-Aware Cluster-based Distillation (TAC-DT), to select representative videos without requiring additional training overhead. We validate the effectiveness of our approach through extensive experiments on four benchmark datasets, demonstrating performance improvements of up to \(10.61\%\) over the state-of-the-art. Our method consistently outperforms existing approaches across all datasets, establishing a new benchmark for video dataset distillation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking the Text-to-Video Generative Models</title>
<link>https://arxiv.org/abs/2505.06679</link>
<guid>https://arxiv.org/abs/2505.06679</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, generative models, jailbreak attack, safety concerns, optimization-based

<br /><br />Summary: This paper addresses the vulnerabilities of text-to-video generative models, which have advanced significantly through diffusion models like Pika, Luma, Kling, and Sora. While these models excel at generating content, they are susceptible to jailbreak attacks that can produce unsafe material such as pornography and violence. Previous efforts, like T2VSafetyBench, have evaluated these models against unsafe prompts but lacked systematic analysis of their vulnerabilities. The authors propose the first optimization-based jailbreak attack tailored for text-to-video models, framing prompt generation as an optimization problem with three objectives: maximizing semantic similarity between input and generated prompts, evading safety filters, and ensuring generated videos are semantically aligned with the original input. To improve the robustness of prompts, they introduce a mutation strategy that creates multiple prompt variants in each iteration, selecting the most effective one based on averaged scores. Extensive experiments conducted on various models, including Open-Sora, Pika, Luma, and Kling, show that their approach not only achieves a higher success rate in bypassing safety measures but also generates videos that maintain greater semantic relevance to the input prompts. <div>
arXiv:2505.06679v1 Announce Type: new 
Abstract: Text-to-video generative models have achieved significant progress, driven by the rapid advancements in diffusion models, with notable examples including Pika, Luma, Kling, and Sora. Despite their remarkable generation ability, their vulnerability to jailbreak attack, i.e. to generate unsafe content, including pornography, violence, and discrimination, raises serious safety concerns. Existing efforts, such as T2VSafetyBench, have provided valuable benchmarks for evaluating the safety of text-to-video models against unsafe prompts but lack systematic studies for exploiting their vulnerabilities effectively. In this paper, we propose the \textit{first} optimization-based jailbreak attack against text-to-video models, which is specifically designed. Our approach formulates the prompt generation task as an optimization problem with three key objectives: (1) maximizing the semantic similarity between the input and generated prompts, (2) ensuring that the generated prompts can evade the safety filter of the text-to-video model, and (3) maximizing the semantic similarity between the generated videos and the original input prompts. To further enhance the robustness of the generated prompts, we introduce a prompt mutation strategy that creates multiple prompt variants in each iteration, selecting the most effective one based on the averaged score. This strategy not only improves the attack success rate but also boosts the semantic relevance of the generated video. We conduct extensive experiments across multiple text-to-video models, including Open-Sora, Pika, Luma, and Kling. The results demonstrate that our method not only achieves a higher attack success rate compared to baseline methods but also generates videos with greater semantic similarity to the original input prompts.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnfoldIR: Rethinking Deep Unfolding Network in Illumination Degradation Image Restoration</title>
<link>https://arxiv.org/abs/2505.06683</link>
<guid>https://arxiv.org/abs/2505.06683</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep unfolding networks, illumination degradation image restoration, UnfoldIR, reflectance-assisted illumination correction, illumination-guided reflectance enhancement

Summary:
UnfoldIR is a novel Deep Unfolding Network (DUN) method for illumination degradation image restoration (IDIR) tasks. It addresses limitations by introducing a new task-specific restoration model with dedicated regularization terms for illumination smoothing and texture enhancement. The iterative optimized solution is unfolded into a multistage network consisting of Reflectance-Assisted Illumination Correction (RAIC) and Illumination-Guided Reflectance Enhancement (IGRE) modules. RAIC enforces illumination smoothness using a visual state space (VSS), while IGRE globally aligns similar textures to enhance details in degraded regions. An inter-stage information consistent loss is proposed to ensure network stability in later stages and maintain structural preservation. Experiments validate the effectiveness of UnfoldIR across various IDIR tasks and downstream problems. <div>
arXiv:2505.06683v1 Announce Type: new 
Abstract: Deep unfolding networks (DUNs) are widely employed in illumination degradation image restoration (IDIR) to merge the interpretability of model-based approaches with the generalization of learning-based methods. However, the performance of DUN-based methods remains considerably inferior to that of state-of-the-art IDIR solvers. Our investigation indicates that this limitation does not stem from structural shortcomings of DUNs but rather from the limited exploration of the unfolding structure, particularly for (1) constructing task-specific restoration models, (2) integrating advanced network architectures, and (3) designing DUN-specific loss functions. To address these issues, we propose a novel DUN-based method, UnfoldIR, for IDIR tasks. UnfoldIR first introduces a new IDIR model with dedicated regularization terms for smoothing illumination and enhancing texture. We unfold the iterative optimized solution of this model into a multistage network, with each stage comprising a reflectance-assisted illumination correction (RAIC) module and an illumination-guided reflectance enhancement (IGRE) module. RAIC employs a visual state space (VSS) to extract non-local features, enforcing illumination smoothness, while IGRE introduces a frequency-aware VSS to globally align similar textures, enabling mildly degraded regions to guide the enhancement of details in more severely degraded areas. This suppresses noise while enhancing details. Furthermore, given the multistage structure, we propose an inter-stage information consistent loss to maintain network stability in the final stages. This loss contributes to structural preservation and sustains the model's performance even in unsupervised settings. Experiments verify our effectiveness across 5 IDIR tasks and 3 downstream problems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FNBench: Benchmarking Robust Federated Learning against Noisy Labels</title>
<link>https://arxiv.org/abs/2505.06684</link>
<guid>https://arxiv.org/abs/2505.06684</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, label noise, benchmark study, robustness, regularization

Summary:
Federated learning faces the challenge of label noise within distributed datasets, affecting performance. The FNBench benchmark study evaluates state-of-the-art methods under unified settings, considering synthetic label noise, human-annotation errors, and systematic errors. This study sheds light on why noisy labels degrade federated learning performance. A representation-aware regularization method is proposed to enhance robustness against label noise. The open-sourced source code and observations provided in this study contribute to understanding and addressing the impact of label noise in federated learning. Future directions for research are also outlined to further advance the field. <div>
arXiv:2505.06684v1 Announce Type: new 
Abstract: Robustness to label noise within data is a significant challenge in federated learning (FL). From the data-centric perspective, the data quality of distributed datasets can not be guaranteed since annotations of different clients contain complicated label noise of varying degrees, which causes the performance degradation. There have been some early attempts to tackle noisy labels in FL. However, there exists a lack of benchmark studies on comprehensively evaluating their practical performance under unified settings. To this end, we propose the first benchmark study FNBench to provide an experimental investigation which considers three diverse label noise patterns covering synthetic label noise, imperfect human-annotation errors and systematic errors. Our evaluation incorporates eighteen state-of-the-art methods over five image recognition datasets and one text classification dataset. Meanwhile, we provide observations to understand why noisy labels impair FL, and additionally exploit a representation-aware regularization method to enhance the robustness of existing methods against noisy labels based on our observations. Finally, we discuss the limitations of this work and propose three-fold future directions. To facilitate related communities, our source code is open-sourced at https://github.com/Sprinter1999/FNBench.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search</title>
<link>https://arxiv.org/abs/2505.06694</link>
<guid>https://arxiv.org/abs/2505.06694</guid>
<content:encoded><![CDATA[
<div> Keywords: underwater object detection, sonar imagery, Detection Transformer, Neural Architecture Search, NAS-DETR

Summary:
- Proposal of a Detection Transformer architecture optimized with a Neural Architecture Search approach for object detection in sonar images.
- Introduction of an improved Zero-shot Neural Architecture Search method based on the maximum entropy principle to identify a high-performance CNN-Transformer backbone for sonar image detection.
- Combination of the selected backbone with a Feature Pyramid Network and a deformable attention-based Transformer decoder to construct a complete network architecture for enhanced performance.
- Extensive experiments showcasing state-of-the-art performance on two representative datasets while maintaining real-time efficiency and minimal computational complexity.
- Correlation analysis between key parameters and differential entropy-based fitness function to enhance the interpretability of the proposed framework.

<br /><br />Summary: In this study, a novel approach to underwater object detection using sonar imagery is introduced. By combining a Detection Transformer architecture with a Neural Architecture Search method, the authors achieve state-of-the-art performance on representative datasets. The proposed method incorporates an improved Zero-shot Neural Architecture Search technique and advanced network components to enhance detection accuracy. Through extensive experiments, the effectiveness of the approach is demonstrated, highlighting its real-time efficiency and minimal computational complexity. Additionally, correlation analysis is performed to improve the interpretability of the framework, making this work a significant contribution to the field of sonar object detection. <div>
arXiv:2505.06694v1 Announce Type: new 
Abstract: Underwater object detection using sonar imagery has become a critical and rapidly evolving research domain within marine technology. However, sonar images are characterized by lower resolution and sparser features compared to optical images, which seriously degrades the performance of object detection.To address these challenges, we specifically propose a Detection Transformer (DETR) architecture optimized with a Neural Architecture Search (NAS) approach called NAS-DETR for object detection in sonar images. First, an improved Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy principle is proposed to identify a real-time, high-representational-capacity CNN-Transformer backbone for sonar image detection. This method enables the efficient discovery of high-performance network architectures with low computational and time overhead. Subsequently, the backbone is combined with a Feature Pyramid Network (FPN) and a deformable attention-based Transformer decoder to construct a complete network architecture. This architecture integrates various advanced components and training schemes to enhance overall performance. Extensive experiments demonstrate that this architecture achieves state-of-the-art performance on two Representative datasets, while maintaining minimal overhead in real-time efficiency and computational complexity. Furthermore, correlation analysis between the key parameters and differential entropy-based fitness function is performed to enhance the interpretability of the proposed framework. To the best of our knowledge, this is the first work in the field of sonar object detection to integrate the DETR architecture with a NAS search mechanism.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images</title>
<link>https://arxiv.org/abs/2505.06710</link>
<guid>https://arxiv.org/abs/2505.06710</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-instance learning, feature extractor, weakly-supervised learning, data augmentation, pathological images<br />
Summary: <br />
This paper introduces a novel approach to pre-train feature extractors for multi-instance learning (MIL) using weakly-supervised learning on whole-slide pathological images (WSI). The method focuses on learning instance-level representations by propagating weak bag-level labels to instances for supervised learning. It incorporates strong data augmentation, a non-linear prediction head, and a robust loss function to enhance feature learning for MIL. Experimental results demonstrate superior performance compared to other pre-training methods like ImageNet and self-supervised learning in various downstream tasks on large-scale WSI datasets. The proposed scheme is also scalable and compatible with fine-tuning pathological-specific models and pre-training on multiple merged datasets. This work, the first to emphasize representation learning for MIL, showcases the importance of optimizing feature extraction for improved performance in WSI analysis. <br /> <div>
arXiv:2505.06710v1 Announce Type: new 
Abstract: Various multi-instance learning (MIL) based approaches have been developed and successfully applied to whole-slide pathological images (WSI). Existing MIL methods emphasize the importance of feature aggregators, but largely neglect the instance-level representation learning. They assume that the availability of a pre-trained feature extractor can be directly utilized or fine-tuned, which is not always the case. This paper proposes to pre-train feature extractor for MIL via a weakly-supervised scheme, i.e., propagating the weak bag-level labels to the corresponding instances for supervised learning. To learn effective features for MIL, we further delve into several key components, including strong data augmentation, a non-linear prediction head and the robust loss function. We conduct experiments on common large-scale WSI datasets and find it achieves better performance than other pre-training schemes (e.g., ImageNet pre-training and self-supervised learning) in different downstream tasks. We further show the compatibility and scalability of the proposed scheme by deploying it in fine-tuning the pathological-specific models and pre-training on merged multiple datasets. To our knowledge, this is the first work focusing on the representation learning for MIL.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers</title>
<link>https://arxiv.org/abs/2505.06745</link>
<guid>https://arxiv.org/abs/2505.06745</guid>
<content:encoded><![CDATA[
<div> framework, symbolic rule extraction, Vision Transformers, sparse concept layer, logic programs <br />
 <br />Summary: 
This paper introduces a framework for extracting symbolic rules from Vision Transformers (ViTs). It addresses the challenge posed by ViTs' lack of modular concept detectors and reliance on global self-attention mechanisms by proposing a sparse concept layer inspired by Sparse Autoencoders. This layer learns a binarized representation of high-level visual concepts through a combination of sparsity, entropy minimization, and supervised contrastive loss. The resulting binarized concept activations are used as input to the FOLD-SE-M algorithm to generate logic programs, enabling symbolic reasoning. The method achieves higher classification accuracy than standard ViTs while providing interpretable and semantically meaningful rule-sets that serve as a logic-based decision layer operating directly on sparse concept representations. This work represents a significant advancement in neuro-symbolic AI by bridging the gap between transformer-based vision models and symbolic logic programming. <div>
arXiv:2505.06745v1 Announce Type: new 
Abstract: Recent neuro-symbolic approaches have successfully extracted symbolic rule-sets from CNN-based models to enhance interpretability. However, applying similar techniques to Vision Transformers (ViTs) remains challenging due to their lack of modular concept detectors and reliance on global self-attention mechanisms. We propose a framework for symbolic rule extraction from ViTs by introducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This linear layer operates on attention-weighted patch representations and learns a disentangled, binarized representation in which individual neurons activate for high-level visual concepts. To encourage interpretability, we apply a combination of L1 sparsity, entropy minimization, and supervised contrastive loss. These binarized concept activations are used as input to the FOLD-SE-M algorithm, which generates a rule-set in the form of logic programs. Our method achieves a 5.14% better classification accuracy than the standard ViT while enabling symbolic reasoning. Crucially, the extracted rule-set is not merely post-hoc but acts as a logic-based decision layer that operates directly on the sparse concept representations. The resulting programs are concise and semantically meaningful. This work is the first to extract executable logic programs from ViTs using sparse symbolic representations. It bridges the gap between transformer-based vision models and symbolic logic programming, providing a step forward in interpretable and verifiable neuro-symbolic AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning</title>
<link>https://arxiv.org/abs/2505.06796</link>
<guid>https://arxiv.org/abs/2505.06796</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Fake News Detection, Dataset, Shallow-Deep Multitask Learning, Image-text Generation, Deepfake Modeling

Summary:
The article introduces a new dataset called Multimodal Fake News Detection (MFND) to identify and localize authentic fake news that is easily manipulated by deepfake modeling attacks. A Shallow-Deep Multitask Learning (SDML) model is proposed to effectively detect fake news by leveraging unimodal and mutual modal features to capture the underlying semantics of news. The model employs momentum distillation-based contrastive learning for fine-grained image and text alignment and an adaptive cross-modal fusion module to enhance mutual modal features. It also incorporates a two-branch framework for image and text features to make four predictions through detection and localization projections. Experimental results show the model's superiority in detecting fake news on both mainstream and newly proposed datasets. The code and dataset are available for further research. 

<br /><br />Summary: 
- Introduction of Multimodal Fake News Detection dataset (MFND) to identify and localize authentic fake news affected by deepfake modeling attacks.
- Proposal of a Shallow-Deep Multitask Learning (SDML) model leveraging unimodal and mutual modal features for effective fake news detection.
- Utilization of momentum distillation-based contrastive learning and adaptive cross-modal fusion module for image-text alignment and feature enhancement.
- Implementation of a two-branch framework for image and text features to make four predictions through detection and localization projections.
- Superiority of the model demonstrated through experiments on both mainstream and newly proposed datasets. Availability of code and dataset for further research. <div>
arXiv:2505.06796v1 Announce Type: new 
Abstract: Multimodal news contains a wealth of information and is easily affected by deepfake modeling attacks. To combat the latest image and text generation methods, we present a new Multimodal Fake News Detection dataset (MFND) containing 11 manipulated types, designed to detect and localize highly authentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning (SDML) model for fake news, which fully uses unimodal and mutual modal features to mine the intrinsic semantics of news. Under shallow inference, we propose the momentum distillation-based light punishment contrastive learning for fine-grained uniform spatial image and text semantic alignment, and an adaptive cross-modal fusion module to enhance mutual modal features. Under deep inference, we design a two-branch framework to augment the image and text unimodal features, respectively merging with mutual modalities features, for four predictions via dedicated detection and localization projections. Experiments on both mainstream and our proposed datasets demonstrate the superiority of the model. Codes and dataset are released at https://github.com/yunan-wang33/sdml.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge</title>
<link>https://arxiv.org/abs/2505.06814</link>
<guid>https://arxiv.org/abs/2505.06814</guid>
<content:encoded><![CDATA[
<div> Keywords: M4IVQA, multi-modal, multilingual, medical instructional videos, reasoning  

<br /><br />Summary:  
The M4IVQA challenge has been introduced following the successful CMIVQA and MMIVQA challenges. It aims to advance research in multi-modal, multilingual, and multi-hop medical instructional question answering systems, specifically focusing on medical instructional videos. The challenge features three tracks: M4TAGSV, which centers on Temporal Answer Grounding in a Single Video; M4VCR, focused on Video Corpus Retrieval; and M4TAGVC, which involves Temporal Answer Grounding in a Video Corpus. Participants are tasked with creating algorithms that can effectively process both video and text data, understand queries in multiple languages, and accurately respond to complex multi-hop medical questions. The challenge is expected to foster innovations in multimodal reasoning systems applicable to healthcare, enhancing both emergency response capabilities and medical education within multilingual communities. The official website for more information is https://cmivqa.github.io/. <div>
arXiv:2505.06814v1 Announce Type: new 
Abstract: Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is https://cmivqa.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning for Multi-class Image Classification</title>
<link>https://arxiv.org/abs/2505.06825</link>
<guid>https://arxiv.org/abs/2505.06825</guid>
<content:encoded><![CDATA[
<div> active learning, image classification, CNN classifier, uncertainty metrics, training set size

Summary:
Active learning is proposed as a method to reduce the number of training examples needed for image classification tasks, particularly for CNN classifiers. By assigning values to image examples using different uncertainty metrics, high-value examples can be strategically selected in a smaller training set size. Results on digit recognition and fruit classification datasets demonstrate the effectiveness of active learning, with formal comparisons across four different uncertainty metrics. Active learning shows marked improvement over random sampling, especially for more difficult classification tasks. The study also indicates the viability of active learning for simpler binary classification tasks, showcasing its potential for a wide range of image classification problems. <div>
arXiv:2505.06825v1 Announce Type: new 
Abstract: A principle bottleneck in image classification is the large number of training examples needed to train a classifier. Using active learning, we can reduce the number of training examples to teach a CNN classifier by strategically selecting examples. Assigning values to image examples using different uncertainty metrics allows the model to identify and select high-value examples in a smaller training set size. We demonstrate results for digit recognition and fruit classification on the MNIST and Fruits360 data sets. We formally compare results for four different uncertainty metrics. Finally, we observe active learning is also effective on simpler (binary) classification tasks, but marked improvement from random sampling is more evident on more difficult tasks. We show active learning is a viable algorithm for image classification problems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification</title>
<link>https://arxiv.org/abs/2505.06831</link>
<guid>https://arxiv.org/abs/2505.06831</guid>
<content:encoded><![CDATA[
<div> Keywords: group-robust generalization, spurious correlations, Class-Conditional Distribution Balancing (CCDB), Bias Exploration via Overfitting (BEO), fine-grained variant

Summary: 
- Achieving group-robust generalization without bias annotations is challenging
- Spurious correlations often arise from mismatches in distributions of bias attributes
- CCDB addresses this issue through simple distribution matching but uses a single Gaussian for approximation
- BEO proposes modeling distributions as mixtures of latent groups for more detailed representation
- FG-CCDB, a fine-grained variant, performs precise distribution matching and balancing within each group
- FG-CCDB achieves stronger mitigation of spurious correlations through group-level reweighting
- BEO acts as a proxy for bias annotations and can be integrated with bias-supervised methods
- The combination of BEO and FG-CCDB performs comparably to bias-supervised approaches in binary classification and outperforms them in highly biased multi-class scenarios

<br /><br />Summary: <div>
arXiv:2505.06831v1 Announce Type: new 
Abstract: Achieving group-robust generalization in the presence of spurious correlations remains a significant challenge, particularly when bias annotations are unavailable. Recent studies on Class-Conditional Distribution Balancing (CCDB) reveal that spurious correlations often stem from mismatches between the class-conditional and marginal distributions of bias attributes. They achieve promising results by addressing this issue through simple distribution matching in a bias-agnostic manner. However, CCDB approximates each distribution using a single Gaussian, which is overly simplistic and rarely holds in real-world applications. To address this limitation, we propose a novel method called Bias Exploration via Overfitting (BEO), which captures each distribution in greater detail by modeling it as a mixture of latent groups. Building on these group-level descriptions, we introduce a fine-grained variant of CCDB, termed FG-CCDB, which performs more precise distribution matching and balancing within each group. Through group-level reweighting, FG-CCDB learns sample weights from a global perspective, achieving stronger mitigation of spurious correlations without incurring substantial storage or computational costs. Extensive experiments demonstrate that BEO serves as a strong proxy for ground-truth bias annotations and can be seamlessly integrated with bias-supervised methods. Moreover, when combined with FG-CCDB, our method performs on par with bias-supervised approaches on binary classification tasks and significantly outperforms them in highly biased multi-class scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Instruction Tuning with Chain of Region-of-Interest</title>
<link>https://arxiv.org/abs/2505.06840</link>
<guid>https://arxiv.org/abs/2505.06840</guid>
<content:encoded><![CDATA[
<div> Keywords: high-resolution images, multimodal large language models, Chain of Region-of-Interest, visual instruction tuning, computational efficiency 

Summary: 
The study introduces a novel method called Chain of Region-of-Interest (CoRoI) for Visual Instruction Tuning to address the computational challenges of high-resolution images in multimodal large language models. Inspired by the selective nature of the human visual system, CoRoI prioritizes informative regions in images to improve visual comprehension and recognition without processing lengthy high-resolution tokens. Extensive experiments across 11 benchmarks validate the effectiveness of CoRoI across various model sizes, consistently outperforming existing methods like LLaVA-NeXT and proprietary models like Gemini Pro 1.0 and GPT-4V on multiple benchmarks. The proposed method demonstrates superior performance, especially with a finetuned 34B model surpassing proprietary methods on six benchmarks and outperforming GPT-4V on specific tasks. Overall, CoRoI enhances multimodal visual comprehension while providing computational efficiency for large language models. 

<br /><br />Summary: <div>
arXiv:2505.06840v1 Announce Type: new 
Abstract: High-resolution (HR) images are pivotal for enhancing the recognition and understanding capabilities of multimodal large language models (MLLMs). However, directly increasing image resolution can significantly escalate computational demands. In this study, we propose a method called Chain of Region-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating the computational burden associated with high-resolution images for MLLMs. Drawing inspiration from the selective nature of the human visual system, we recognize that not all regions within high-resolution images carry equal importance. CoRoI seeks to identify and prioritize the most informative regions, thereby enhancing multimodal visual comprehension and recognition while circumventing the need for processing lengthy HR image tokens. Through extensive experiments on 11 benchmarks, we validate the efficacy of CoRoI across varying sizes, ranging from 7B to 34B in parameters. Our models consistently demonstrate superior performance across diverse multimodal benchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all benchmarks and our finetuned 34B model surpasses proprietary methods like Gemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB, SEED-I, and MME.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach</title>
<link>https://arxiv.org/abs/2505.06853</link>
<guid>https://arxiv.org/abs/2505.06853</guid>
<content:encoded><![CDATA[
<div> Pan American Health Organization, cancer cases, Latin America, osteosarcoma, surgical safety margins

Summary:
The Pan American Health Organization reports a rising trend in cancer cases in Latin America, with osteosarcoma being a common and lethal bone cancer affecting the youth. Detection of osteosarcoma poses challenges due to its unique characteristics. Surgical removal of osteosarcoma requires precise safety margins to ensure complete resection while sparing healthy tissue. A novel approach is proposed in this study for estimating the confidence interval of surgical safety margins in osteosarcoma surgery around the knee. The method utilizes MRI and X-ray data, digital processing techniques, and unsupervised learning algorithms like k-means clustering to delineate tumor boundaries. Results from experiments demonstrate the potential of automated, patient-specific determination of safety margins, which could enhance the efficiency and accuracy of osteosarcoma surgery.<br /><br />Summary: <div>
arXiv:2505.06853v1 Announce Type: new 
Abstract: According to the Pan American Health Organization, the number of cancer cases in Latin America was estimated at 4.2 million in 2022 and is projected to rise to 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone cancers affecting young people, is difficult to detect due to its unique texture and intensity. Surgical removal of osteosarcoma requires precise safety margins to ensure complete resection while preserving healthy tissue. Therefore, this study proposes a method for estimating the confidence interval of surgical safety margins in osteosarcoma surgery around the knee. The proposed approach uses MRI and X-ray data from open-source repositories, digital processing techniques, and unsupervised learning algorithms (such as k-means clustering) to define tumor boundaries. Experimental results highlight the potential for automated, patient-specific determination of safety margins.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies</title>
<link>https://arxiv.org/abs/2505.06855</link>
<guid>https://arxiv.org/abs/2505.06855</guid>
<content:encoded><![CDATA[
<div> Keywords: text recognition, self-supervised learning, masked image modeling, multi-masking strategy, real-world datasets

Summary:
The study focuses on improving text recognition methods by addressing the limitations of training on synthetic datasets. Existing techniques suffer from performance disparities when handling complex real-world images due to their inability to replicate real-world scenarios accurately. The researchers propose a Multi-Masking Strategy (MMS) that integrates different masking techniques, including random patch, blockwise, and span masking, into the Masked Image Modeling (MIM) framework. By combining low and high-level textual representations, MMS outperforms existing self-supervised methods after fine-tuning with real data. The approach enhances performance in various text-related tasks such as recognition, segmentation, and text-image super-resolution. This novel strategy aims to bridge the gap between synthetic and real-world data, leading to more accurate and robust text recognition systems.<br /><br />Summary: <div>
arXiv:2505.06855v1 Announce Type: new 
Abstract: Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level textual representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuRN: Neuro-inspired Domain Generalization for Image Classification</title>
<link>https://arxiv.org/abs/2505.06881</link>
<guid>https://arxiv.org/abs/2505.06881</guid>
<content:encoded><![CDATA[
<div> Neural Response Normalization, domain generalization, image classification, deep learning architectures, neuro-inspired

Summary: 
Neural Response Normalization (NeuRN) layer is introduced to improve domain generalization in image classification tasks. Inspired by neurons in the visual cortex, NeuRN aims to enhance deep learning models' performance on unseen datasets. Experimenting with various deep learning architectures, including those from Neural Architecture Search and Vision Transformer, models integrated with NeuRN show improved performance compared to baseline models. A novel method that uses the Needleman-Wunsch algorithm is proposed to compute similarity between deep learning architectures, aiding in selecting models for experimentation. Results demonstrate the effectiveness of NeuRN in cross-domain image classification, paving the way for future neuro-inspired deep learning models. 

<br /><br />Summary: <div>
arXiv:2505.06881v1 Announce Type: new 
Abstract: Domain generalization in image classification is a crucial challenge, with models often failing to generalize well across unseen datasets. We address this issue by introducing a neuro-inspired Neural Response Normalization (NeuRN) layer which draws inspiration from neurons in the mammalian visual cortex, which aims to enhance the performance of deep learning architectures on unseen target domains by training deep learning models on a source domain. The performance of these models is considered as a baseline and then compared against models integrated with NeuRN on image classification tasks. We perform experiments across a range of deep learning architectures, including ones derived from Neural Architecture Search and Vision Transformer. Additionally, in order to shortlist models for our experiment from amongst the vast range of deep neural networks available which have shown promising results, we also propose a novel method that uses the Needleman-Wunsch algorithm to compute similarity between deep learning architectures. Our results demonstrate the effectiveness of NeuRN by showing improvement against baseline in cross-domain image classification tasks. Our framework attempts to establish a foundation for future neuro-inspired deep learning models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization</title>
<link>https://arxiv.org/abs/2505.06886</link>
<guid>https://arxiv.org/abs/2505.06886</guid>
<content:encoded><![CDATA[
<div> Keywords: mouse, visual cortex, deep learning, NeuRN, neural representations  

<br /><br />Summary: The mouse serves as a vital model in systems neuroscience, particularly in understanding how its visual cortex responds to various natural scene stimuli. This study examines the functional alignment of the mouse visual cortex with deep learning models in object classification tasks. A novel representational learning strategy reveals a significant resemblance between the mouse's visual processing and top-performing deep learning models, observed at both population and single-cell levels. The research introduces a Neural Response Normalization (NeuRN) layer, inspired by the activation profiles of excitatory and inhibitory neurons, which enhances the representational similarity. Testing NeuRN within deep learning frameworks demonstrates marked improvements in robustness, especially against data shifts in domain generalization tasks. This work outlines a transformative approach for comparing the mouse visual cortex's architecture with advanced AI models, enabling a deeper understanding of neural representations. The findings suggest that deep learning models can benefit from insights gained from mouse vision, potentially leading to enhanced performance in real-world applications. Overall, this research holds significant implications for advancing AI models by integrating insights from biological systems. <div>
arXiv:2505.06886v1 Announce Type: new 
Abstract: The mouse is one of the most studied animal models in the field of systems neuroscience. Understanding the generalized patterns and decoding the neural representations that are evoked by the diverse range of natural scene stimuli in the mouse visual cortex is one of the key quests in computational vision. In recent years, significant parallels have been drawn between the primate visual cortex and hierarchical deep neural networks. However, their generalized efficacy in understanding mouse vision has been limited. In this study, we investigate the functional alignment between the mouse visual cortex and deep learning models for object classification tasks. We first introduce a generalized representational learning strategy that uncovers a striking resemblance between the functional mapping of the mouse visual cortex and high-performing deep learning models on both top-down (population-level) and bottom-up (single cell-level) scenarios. Next, this representational similarity across the two systems is further enhanced by the addition of Neural Response Normalization (NeuRN) layer, inspired by the activation profile of excitatory and inhibitory neurons in the visual cortex. To test the performance effect of NeuRN on real-world tasks, we integrate it into deep learning models and observe significant improvements in their robustness against data shifts in domain generalization tasks. Our work proposes a novel framework for comparing the functional architecture of the mouse visual cortex with deep learning models. Our findings carry broad implications for the development of advanced AI models that draw inspiration from the mouse visual cortex, suggesting that these models serve as valuable tools for studying the neural representations of the mouse visual cortex and, as a result, enhancing their performance on real-world tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization</title>
<link>https://arxiv.org/abs/2505.06894</link>
<guid>https://arxiv.org/abs/2505.06894</guid>
<content:encoded><![CDATA[
<div> NeuGen, brain-inspired normalization, NeRF architectures, generalization, image rendering

Summary:
Neural Radiance Fields (NeRF) have revolutionized novel view synthesis but face challenges in generalizing across diverse scenes. This study proposes integrating a brain-inspired normalization technique, NeuGen, into leading NeRF architectures like MVSNeRF and GeoNeRF. NeuGen extracts domain-invariant features to enhance models' generalization capabilities, improving accuracy and robustness in image rendering. Integration of NeuGen shows enhanced performance in benchmarks across diverse datasets, enabling better generalization across varied scenes. Comprehensive evaluations, both quantitative and qualitative, demonstrate that this approach outperforms existing models in generalizability and rendering quality. By merging neuroscientific principles with deep learning frameworks, this work sets a new standard for improved generalizability and efficiency in novel view synthesis. A demo showcasing the study is available at https://neugennerf.github.io.

<br /><br />Summary: <div>
arXiv:2505.06894v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts the domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures and cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows improved performance on benchmarks on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2505.06898</link>
<guid>https://arxiv.org/abs/2505.06898</guid>
<content:encoded><![CDATA[
<div> Keywords: Generalist Medical AI, multi-modal explainability, prognostic capabilities, uncertainty quantification, clinician-centric  

<br /><br />Summary: This article introduces XMedGPT, a multi-modal AI assistant designed for clinical use, addressing limitations in existing Generalist Medical AI (GMAI) systems. XMedGPT enhances medical decision-making by integrating textual and visual interpretability, providing accurate diagnostic outputs while grounding anatomical references in medical images. The system introduces a reliability indexing mechanism to quantify uncertainty through interactive question-answering and produces validated results across four key areas: multi-modal interpretability, uncertainty quantification, prognostic modeling, and rigorous benchmarking. The model achieves a notable IoU of 0.703 across anatomical regions and a Kendall's tau-b of 0.479, reflecting strong alignment between visual rationales and clinical outcomes. It excels in uncertainty estimation with AUC scores of 0.862 for visual question answering and 0.764 for radiology report generation. In cancer prognosis, it outperforms prior models by 26.9% and GPT-4o by 25.0%. Availability of extensive benchmarking across 347 datasets and validation across four anatomical systems indicates the model's exceptional generalizability, evidenced by performance improvements of 20.7% in-domain and 16.7% on a large in-house dataset. XMedGPT signifies a notable advancement in clinician-centered AI integration for diverse healthcare applications. <div>
arXiv:2505.06898v1 Announce Type: new 
Abstract: Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but also grounds referenced anatomical sites within medical images, bridging critical gaps in interpretability and enhancing clinician usability. To support real-world deployment, we introduce a reliability indexing mechanism that quantifies uncertainty through consistency-based assessment via interactive question-answering. We validate XMedGPT across four pillars: multi-modal interpretability, uncertainty quantification, and prognostic modeling, and rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between visual rationales and clinical outcomes. For uncertainty estimation, it attains an AUC of 0.862 on visual question answering and 0.764 on radiology report generation. In survival and recurrence prediction for lung and glioma cancers, it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%. Rigorous benchmarking across 347 datasets covers 40 imaging modalities and external validation spans 4 anatomical systems confirming exceptional generalizability, with performance gains surpassing existing GMAI by 20.7% for in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together, XMedGPT represents a significant leap forward in clinician-centric AI integration, offering trustworthy and scalable support for diverse healthcare applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection</title>
<link>https://arxiv.org/abs/2505.06903</link>
<guid>https://arxiv.org/abs/2505.06903</guid>
<content:encoded><![CDATA[
arXiv:2505.06903v1 Announce Type: new 
Abstract: Temporal medical image analysis is essential for clinical decision-making, yet existing methods either align images and text at a coarse level - causing potential semantic mismatches - or depend solely on visual information, lacking medical semantic integration. We present CheXLearner, the first end-to-end framework that unifies anatomical region detection, Riemannian manifold-based structure alignment, and fine-grained regional semantic guidance. Our proposed Med-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to robustly align anatomical structures and capture pathologically meaningful discrepancies across temporal chest X-rays. By introducing regional progression descriptions as supervision, CheXLearner achieves enhanced cross-modal representation learning and supports dynamic low-level feature optimization. Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and 80.32% (+11.05%) F1-score on anatomical region progression detection - substantially outperforming state-of-the-art baselines, especially in structurally complex regions. Additionally, our model attains a 91.52% average AUC score in downstream disease classification, validating its superior feature representation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction</title>
<link>https://arxiv.org/abs/2505.06905</link>
<guid>https://arxiv.org/abs/2505.06905</guid>
<content:encoded><![CDATA[
arXiv:2505.06905v1 Announce Type: new 
Abstract: Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. Recently, models trained on synthetic data and refined through domain adaptation have shown remarkable performance in MHE, yet it remains unclear how these models make predictions or how reliable they truly are. In this paper, we investigate a state-of-the-art MHE model trained purely on synthetic data to explore where the model looks when making height predictions. Through systematic analyses, we find that the model relies heavily on shadow cues, a factor that can lead to overestimation or underestimation of heights when shadows deviate from expected norms. Furthermore, the inherent difficulty of evaluating regression tasks with the human eye underscores additional limitations of purely synthetic training. To address these issues, we propose a novel correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep-learning outputs to improve local accuracy and achieve spatially consistent corrections. Our method comprises two stages: pre-processing raw ICESat-2 data, followed by a random forest-based approach to densely refine height estimates. Experiments in three representative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal substantial error reductions, with mean absolute error (MAE) decreased by 22.8\%, 6.9\%, and 4.9\%, respectively. These findings highlight the critical role of shadow awareness in synthetic data-driven models and demonstrate how fusing imperfect real-world LiDAR data can bolster the robustness of MHE, paving the way for more reliable and scalable 3D mapping solutions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI</title>
<link>https://arxiv.org/abs/2505.06912</link>
<guid>https://arxiv.org/abs/2505.06912</guid>
<content:encoded><![CDATA[
arXiv:2505.06912v1 Announce Type: new 
Abstract: Despite strong performance in medical question-answering, the clinical adoption of Large Language Models (LLMs) is critically hampered by their opaque 'black-box' reasoning, limiting clinician trust. This challenge is compounded by the predominant reliance of current medical LLMs on corpora from scientific literature or synthetic data, which often lack the granular expert validation and high clinical relevance essential for advancing their specialized medical capabilities. To address these critical gaps, we introduce a highly clinically relevant dataset with 31,247 medical question-answer pairs, each accompanied by expert-validated chain-of-thought (CoT) explanations. This resource, spanning multiple clinical domains, was curated via a scalable human-LLM hybrid pipeline: LLM-generated rationales were iteratively reviewed, scored, and refined by medical experts against a structured rubric, with substandard outputs revised through human effort or guided LLM regeneration until expert consensus. This publicly available dataset provides a vital source for the development of medical LLMs that capable of transparent and verifiable reasoning, thereby advancing safer and more interpretable AI in medicine.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion</title>
<link>https://arxiv.org/abs/2505.06920</link>
<guid>https://arxiv.org/abs/2505.06920</guid>
<content:encoded><![CDATA[
arXiv:2505.06920v1 Announce Type: new 
Abstract: Acquiring accurately aligned multi-modal image pairs is fundamental for achieving high-quality multi-modal image fusion. To address the lack of ground truth in current multi-modal image registration and fusion methods, we propose a novel self-supervised \textbf{B}i-directional \textbf{S}elf-\textbf{R}egistration framework (\textbf{B-SR}). Specifically, B-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator (IPDG) to achieve self-supervised global-local registration. Visible-infrared image pairs with spatially misaligned differences are aligned to obtain global differences through the registration module. The same image pairs are processed by PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain local differences. IPDG converts the obtained local differences into pseudo-global differences, which are used to perform global-local difference consistency with the global differences. Furthermore, aiming at eliminating the effect of modal gaps on the registration module, we design a neighborhood dynamic alignment loss to achieve cross-modal image edge alignment. Extensive experiments on misaligned multi-modal images demonstrate the effectiveness of the proposed method in multi-modal image alignment and fusion against the competing methods. Our code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network</title>
<link>https://arxiv.org/abs/2505.06937</link>
<guid>https://arxiv.org/abs/2505.06937</guid>
<content:encoded><![CDATA[
arXiv:2505.06937v1 Announce Type: new 
Abstract: In this paper, the dual-optical attention fusion crowd head point counting model (TAPNet) is proposed to address the problem of the difficulty of accurate counting in complex scenes such as crowd dense occlusion and low light in crowd counting tasks under UAV view. The model designs a dual-optical attention fusion module (DAFP) by introducing complementary information from infrared images to improve the accuracy and robustness of all-day crowd counting. In order to fully utilize different modal information and solve the problem of inaccurate localization caused by systematic misalignment between image pairs, this paper also proposes an adaptive two-optical feature decomposition fusion module (AFDF). In addition, we optimize the training strategy to improve the model robustness through spatial random offset data augmentation. Experiments on two challenging public datasets, DroneRGBT and GAIIC2, show that the proposed method outperforms existing techniques in terms of performance, especially in challenging dense low-light scenes. Code is available at https://github.com/zz-zik/TAPNet
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Learning for Class Distribution Mismatch</title>
<link>https://arxiv.org/abs/2505.06948</link>
<guid>https://arxiv.org/abs/2505.06948</guid>
<content:encoded><![CDATA[
arXiv:2505.06948v1 Announce Type: new 
Abstract: Class distribution mismatch (CDM) refers to the discrepancy between class distributions in training data and target tasks. Previous methods address this by designing classifiers to categorize classes known during training, while grouping unknown or new classes into an "other" category. However, they focus on semi-supervised scenarios and heavily rely on labeled data, limiting their applicability and performance. To address this, we propose Unsupervised Learning for Class Distribution Mismatch (UCDM), which constructs positive-negative pairs from unlabeled data for classifier training. Our approach randomly samples images and uses a diffusion model to add or erase semantic classes, synthesizing diverse training pairs. Additionally, we introduce a confidence-based labeling mechanism that iteratively assigns pseudo-labels to valuable real-world data and incorporates them into the training process. Extensive experiments on three datasets demonstrate UCDM's superiority over previous semi-supervised methods. Specifically, with a 60% mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%, and 72.5% in classifying known, unknown, and new classes.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.06951</link>
<guid>https://arxiv.org/abs/2505.06951</guid>
<content:encoded><![CDATA[
arXiv:2505.06951v1 Announce Type: new 
Abstract: In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.06975</link>
<guid>https://arxiv.org/abs/2505.06975</guid>
<content:encoded><![CDATA[
arXiv:2505.06975v1 Announce Type: new 
Abstract: The primary challenge in accelerating image super-resolution lies in reducing computation while maintaining performance and adaptability. Motivated by the observation that high-frequency regions (e.g., edges and textures) are most critical for reconstruction, we propose a training-free adaptive masking module for acceleration that dynamically focuses computation on these challenging areas. Specifically, our method first extracts high-frequency components via Gaussian blur subtraction and adaptively generates binary masks using K-means clustering to identify regions requiring intensive processing. Our method can be easily integrated with both CNNs and Transformers. For CNN-based architectures, we replace standard $3 \times 3$ convolutions with an unfold operation followed by $1 \times 1$ convolutions, enabling pixel-wise sparse computation guided by the mask. For Transformer-based models, we partition the mask into non-overlapping windows and selectively process tokens based on their average values. During inference, unnecessary pixels or windows are pruned, significantly reducing computation. Moreover, our method supports dilation-based mask adjustment to control the processing scope without retraining, and is robust to unseen degradations (e.g., noise, compression). Extensive experiments on benchmarks demonstrate that our method reduces FLOPs by 24--43% for state-of-the-art models (e.g., CARN, SwinIR) while achieving comparable or better quantitative metrics. The source code is available at https://github.com/shangwei5/AMSR
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition</title>
<link>https://arxiv.org/abs/2505.06982</link>
<guid>https://arxiv.org/abs/2505.06982</guid>
<content:encoded><![CDATA[
arXiv:2505.06982v1 Announce Type: new 
Abstract: Recent progress in image-based medical disease detection encounters challenges such as limited annotated data sets, inadequate spatial feature analysis, data security issues, and inefficient training frameworks. This study introduces a data-efficient image transformer (DeIT)-based approach that overcomes these challenges by utilizing multiscale patch embedding for better feature extraction and stratified weighted random sampling to address class imbalance. The model also incorporates a LoRA-enhanced transformer encoder, a distillation framework, and federated learning for decentralized training, improving both efficiency and data security. Consequently, it achieves state-of-the-art performance, with the highest AUC, F1 score, precision, minimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations improve interpretability by highlighting critical pathological regions, enhancing the model's clinical relevance. These results highlight the potential of this approach to advance AI-powered medical imaging and disease detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation</title>
<link>https://arxiv.org/abs/2505.06985</link>
<guid>https://arxiv.org/abs/2505.06985</guid>
<content:encoded><![CDATA[
arXiv:2505.06985v1 Announce Type: new 
Abstract: Both zero-shot and tuning-based customized text-to-image (CT2I) generation have made significant progress for storytelling content creation. In contrast, research on customized text-to-video (CT2V) generation remains relatively limited. Existing zero-shot CT2V methods suffer from poor generalization, while another line of work directly combining tuning-based T2I models with temporal motion modules often leads to the loss of structural and texture information. To bridge this gap, we propose an autoregressive structure and texture propagation module (STPM), which extracts key structural and texture features from the reference subject and injects them autoregressively into each video frame to enhance consistency. Additionally, we introduce a test-time reward optimization (TTRO) method to further refine fine-grained details. Quantitative and qualitative experiments validate the effectiveness of STPM and TTRO, demonstrating improvements of 7.8 and 13.1 in CLIP-I and DINO consistency metrics over the baseline, respectively.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding</title>
<link>https://arxiv.org/abs/2505.06991</link>
<guid>https://arxiv.org/abs/2505.06991</guid>
<content:encoded><![CDATA[
arXiv:2505.06991v1 Announce Type: new 
Abstract: This report presents our semantic segmentation framework developed by team ACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which focuses on parsing outdoor scenes into nine semantic categories under real-world conditions. Our method integrates a Swin Transformer backbone enhanced with Rotary Position Embedding (RoPE) for improved spatial generalization, alongside a Color Shift Estimation-and-Correction module designed to compensate for illumination inconsistencies in natural environments. To further improve training stability, we adopt a quantile-based denoising strategy that downweights the top 2.5\% of highest-error pixels, treating them as noise and suppressing their influence during optimization. Evaluated on the official GOOSE test set, our approach achieved a mean Intersection over Union (mIoU) of 0.848, demonstrating the effectiveness of combining color correction, positional encoding, and error-aware denoising in robust semantic segmentation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replay-Based Continual Learning with Dual-Layered Distillation and a Streamlined U-Net for Efficient Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.06995</link>
<guid>https://arxiv.org/abs/2505.06995</guid>
<content:encoded><![CDATA[
arXiv:2505.06995v1 Announce Type: new 
Abstract: Recent advancements in text-to-image diffusion models are hindered by high computational demands, limiting accessibility and scalability. This paper introduces KDC-Diff, a novel stable diffusion framework that enhances efficiency while maintaining image quality. KDC-Diff features a streamlined U-Net architecture with nearly half the parameters of the original U-Net (482M), significantly reducing model complexity. We propose a dual-layered distillation strategy to ensure high-fidelity generation, transferring semantic and structural insights from a teacher to a compact student model while minimizing quality degradation. Additionally, replay-based continual learning is integrated to mitigate catastrophic forgetting, allowing the model to retain prior knowledge while adapting to new data. Despite operating under extremely low computational resources, KDC-Diff achieves state-of-the-art performance on the Oxford Flowers and Butterflies & Moths 100 Species datasets, demonstrating competitive metrics such as FID, CLIP, and LPIPS. Moreover, it significantly reduces inference time compared to existing models. These results establish KDC-Diff as a highly efficient and adaptable solution for text-to-image generation, particularly in computationally constrained environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07001</link>
<guid>https://arxiv.org/abs/2505.07001</guid>
<content:encoded><![CDATA[
arXiv:2505.07001v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) are becoming increasingly popular in the medical domain, bridging the gap between medical images and clinical language. Existing VLMs demonstrate an impressive ability to comprehend medical images and text queries to generate detailed, descriptive diagnostic medical reports. However, hallucination--the tendency to generate descriptions that are inconsistent with the visual content--remains a significant issue in VLMs, with particularly severe implications in the medical field. To facilitate VLM research on gastrointestinal (GI) image analysis and study hallucination, we curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2 images are generated using ChatGPT, which introduces some hallucinated or incorrect texts. In the second stage, medical experts systematically review these reports, and identify and correct potential inaccuracies to ensure high-quality, clinically reliable annotations. Unlike traditional datasets that contain only descriptive texts, our dataset also features tags identifying hallucinated sentences and their corresponding corrections. A common approach to reducing hallucination in VLM is to finetune the model on a small-scale, problem-specific dataset. However, we take a different strategy using our dataset. Instead of finetuning the VLM solely for generating textual reports, we finetune it to detect and correct hallucinations, an approach we call hallucination-aware finetuning. Our results show that this approach is better than simply finetuning for descriptive report generation. Additionally, we conduct an extensive evaluation of state-of-the-art VLMs across several metrics, establishing a benchmark. GitHub Repo: https://github.com/bhattarailab/Hallucination-Aware-VLM.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation</title>
<link>https://arxiv.org/abs/2505.07003</link>
<guid>https://arxiv.org/abs/2505.07003</guid>
<content:encoded><![CDATA[
arXiv:2505.07003v1 Announce Type: new 
Abstract: Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception</title>
<link>https://arxiv.org/abs/2505.07007</link>
<guid>https://arxiv.org/abs/2505.07007</guid>
<content:encoded><![CDATA[
arXiv:2505.07007v1 Announce Type: new 
Abstract: Micro-expressions (MEs) are crucial psychological responses with significant potential for affective computing. However, current automatic micro-expression recognition (MER) research primarily focuses on discrete emotion classification, neglecting a convincing analysis of the subtle dynamic movements and inherent emotional cues. The rapid progress in multimodal large language models (MLLMs), known for their strong multimodal comprehension and language generation abilities, offers new possibilities. MLLMs have shown success in various vision-language tasks, indicating their potential to understand MEs comprehensively, including both fine-grained motion patterns and underlying emotional semantics. Nevertheless, challenges remain due to the subtle intensity and short duration of MEs, as existing MLLMs are not designed to capture such delicate frame-level facial dynamics. In this paper, we propose a novel Micro-Expression Large Language Model (MELLM), which incorporates a subtle facial motion perception strategy with the strong inference capabilities of MLLMs, representing the first exploration of MLLMs in the domain of ME analysis. Specifically, to explicitly guide the MLLM toward motion-sensitive regions, we construct an interpretable motion-enhanced color map by fusing onset-apex optical flow dynamics with the corresponding grayscale onset frame as the model input. Additionally, specialized fine-tuning strategies are incorporated to further enhance the model's visual perception of MEs. Furthermore, we construct an instruction-description dataset based on Facial Action Coding System (FACS) annotations and emotion labels to train our MELLM. Comprehensive evaluations across multiple benchmark datasets demonstrate that our model exhibits superior robustness and generalization capabilities in ME understanding (MEU). Code is available at https://github.com/zyzhangUstc/MELLM.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization</title>
<link>https://arxiv.org/abs/2505.07013</link>
<guid>https://arxiv.org/abs/2505.07013</guid>
<content:encoded><![CDATA[
arXiv:2505.07013v1 Announce Type: new 
Abstract: Remote physiological sensing using camera-based technologies offers transformative potential for non-invasive vital sign monitoring across healthcare and human-computer interaction domains. Although deep learning approaches have advanced the extraction of physiological signals from video data, existing methods have not been sufficiently assessed for their robustness to domain shifts. These shifts in remote physiological sensing include variations in ambient conditions, camera specifications, head movements, facial poses, and physiological states which often impact real-world performance significantly. Cross-dataset evaluation provides an objective measure to assess generalization capabilities across these domain shifts. We introduce Target Signal Constrained Factorization module (TSFM), a novel multidimensional attention mechanism that explicitly incorporates physiological signal characteristics as factorization constraints, allowing more precise feature extraction. Building on this innovation, we present MMRPhys, an efficient dual-branch 3D-CNN architecture designed for simultaneous multitask estimation of photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal RGB and thermal video inputs. Through comprehensive cross-dataset evaluation on five benchmark datasets, we demonstrate that MMRPhys with TSFM significantly outperforms state-of-the-art methods in generalization across domain shifts for rPPG and rRSP estimation, while maintaining a minimal inference latency suitable for real-time applications. Our approach establishes new benchmarks for robust multitask and multimodal physiological sensing and offers a computationally efficient framework for practical deployment in unconstrained environments. The web browser-based application featuring on-device real-time inference of MMRPhys model is available at https://physiologicailab.github.io/mmrphys-live
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision-Language Foundation Model for Leaf Disease Identification</title>
<link>https://arxiv.org/abs/2505.07019</link>
<guid>https://arxiv.org/abs/2505.07019</guid>
<content:encoded><![CDATA[
arXiv:2505.07019v1 Announce Type: new 
Abstract: Leaf disease identification plays a pivotal role in smart agriculture. However, many existing studies still struggle to integrate image and textual modalities to compensate for each other's limitations. Furthermore, many of these approaches rely on pretraining with constrained datasets such as ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target COntrastive learning for Leaf Disease identification), a context-aware vision-language foundation model tailored to address these challenges for agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf images and corresponding symptom descriptions, comprising over 186,000 image-caption pairs aligned with 97 unique concepts. Through task-agnostic pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence in contrastive learning by smoothing labels, thereby improving model generalization and robustness on fine-grained classification tasks. Experimental results demonstrate that SCOLD outperforms existing vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across several benchmarks, including zero-shot and few-shot classification, image-text retrieval, and image classification, while maintaining a competitive parameter footprint. Ablation studies further highlight SCOLD's effectiveness in contrast to its counterparts. The proposed approach significantly advances the agricultural vision-language foundation model, offering strong performance with minimal or no supervised fine-tuning. This work lays a solid groundwork for future research on models trained with long-form and simplified contexts, tasks involving class ambiguity, and multi-modal systems for intelligent plant disease diagnostics. The code for this study is available at https://huggingface.co/enalis/scold
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MarkMatch: Same-Hand Stuffing Detection</title>
<link>https://arxiv.org/abs/2505.07032</link>
<guid>https://arxiv.org/abs/2505.07032</guid>
<content:encoded><![CDATA[
arXiv:2505.07032v1 Announce Type: new 
Abstract: We present MarkMatch, a retrieval system for detecting whether two paper ballot marks were filled by the same hand. Unlike the previous SOTA method BubbleSig, which used binary classification on isolated mark pairs, MarkMatch ranks stylistic similarity between a query mark and a mark in the database using contrastive learning. Our model is trained with a dense batch similarity matrix and a dual loss objective. Each sample is contrasted against many negatives within each batch, enabling the model to learn subtle handwriting difference and improve generalization under handwriting variation and visual noise, while diagonal supervision reinforces high confidence on true matches. The model achieves an F1 score of 0.943, surpassing BubbleSig's best performance. MarkMatch also integrates Segment Anything Model for flexible mark extraction via box- or point-based prompts. The system offers election auditors a practical tool for visual, non-biometric investigation of suspicious ballots.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection</title>
<link>https://arxiv.org/abs/2505.07040</link>
<guid>https://arxiv.org/abs/2505.07040</guid>
<content:encoded><![CDATA[
arXiv:2505.07040v1 Announce Type: new 
Abstract: Fabric defect detection confronts two fundamental challenges. First, conventional non-maximum suppression disrupts gradient flow, which hinders genuine end-to-end learning. Second, acquiring pixel-level annotations at industrial scale is prohibitively costly. Addressing these limitations, we propose a differentiable NMS framework for fabric defect detection that achieves superior localization precision through end-to-end optimization. We reformulate NMS as a differentiable bipartite matching problem solved through the Sinkhorn-Knopp algorithm, maintaining uninterrupted gradient flow throughout the network. This approach specifically targets the irregular morphologies and ambiguous boundaries of fabric defects by integrating proposal quality, feature similarity, and spatial relationships. Our entropy-constrained mask refinement mechanism further enhances localization precision through principled uncertainty modeling. Extensive experiments on the Tianchi fabric defect dataset demonstrate significant performance improvements over existing methods while maintaining real-time speeds suitable for industrial deployment. The framework exhibits remarkable adaptability across different architectures and generalizes effectively to general object detection tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.07050</link>
<guid>https://arxiv.org/abs/2505.07050</guid>
<content:encoded><![CDATA[
arXiv:2505.07050v1 Announce Type: new 
Abstract: Unsupervised Domain Adaptation (UDA) aims to align source and target domain distributions to close the domain gap, but still struggles with obtaining the target data. Fortunately, Domain Generalization (DG) excels without the need for any target data. Recent works expose that depth maps contribute to improved generalized performance in the UDA tasks, but they ignore the noise and holes in depth maps due to device and environmental factors, failing to sufficiently and effectively learn domain-invariant representation. Although high-sensitivity region suppression has shown promising results in learning domain-invariant features, existing methods cannot be directly applicable to depth maps due to their unique characteristics. Hence, we propose a novel framework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal stylization flow (DSSS), focusing on learning domain-invariant features from depth maps for the DG semantic segmentation. Specifically, we propose the RGB-D inter-modal stylization flow to generate stylized depth maps for sensitivity detection, cleverly utilizing RGB information as the stylization source. Then, a class-wise soft spatial sensitivity suppression is designed to identify and emphasize non-sensitive depth features that contain more domain-invariant information. Furthermore, an RGB-D soft alignment loss is proposed to ensure that the stylized depth maps only align part of the RGB features while still retaining the unique depth information. To our best knowledge, our DSSS framework is the first work to integrate RGB and Depth information in the multi-class DG semantic segmentation task. Extensive experiments over multiple backbone networks show that our framework achieves remarkable performance improvement.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.07057</link>
<guid>https://arxiv.org/abs/2505.07057</guid>
<content:encoded><![CDATA[
arXiv:2505.07057v1 Announce Type: new 
Abstract: Video generation based on diffusion models presents a challenging multimodal task, with video editing emerging as a pivotal direction in this field. Recent video editing approaches primarily fall into two categories: training-required and training-free methods. While training-based methods incur high computational costs, training-free alternatives often yield suboptimal performance. To address these limitations, we propose DAPE, a high-quality yet cost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for video editing. In the first stage, we design an efficient norm-tuning method to enhance temporal consistency in generated videos. The second stage introduces a vision-friendly adapter to improve visual quality. Additionally, we identify critical shortcomings in existing benchmarks, including limited category diversity, imbalanced object distribution, and inconsistent frame counts. To mitigate these issues, we curate a large dataset benchmark comprising 232 videos with rich annotations and 6 editing prompts, enabling objective and comprehensive evaluation of advanced methods. Extensive experiments on existing datasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate that DAPE significantly improves temporal coherence and text-video alignment while outperforming previous state-of-the-art approaches.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed1.5-VL Technical Report</title>
<link>https://arxiv.org/abs/2505.07062</link>
<guid>https://arxiv.org/abs/2505.07062</guid>
<content:encoded><![CDATA[
arXiv:2505.07062v1 Announce Type: new 
Abstract: We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.07071</link>
<guid>https://arxiv.org/abs/2505.07071</guid>
<content:encoded><![CDATA[
arXiv:2505.07071v1 Announce Type: new 
Abstract: Diffusion-based image super-resolution (SR) methods have demonstrated remarkable performance. Recent advancements have introduced deterministic sampling processes that reduce inference from 15 iterative steps to a single step, thereby significantly improving the inference speed of existing diffusion models. However, their efficiency remains limited when handling complex semantic regions due to the single-step inference. To address this limitation, we propose SAMSR, a semantic-guided diffusion framework that incorporates semantic segmentation masks into the sampling process. Specifically, we introduce the SAM-Noise Module, which refines Gaussian noise using segmentation masks to preserve spatial and semantic features. Furthermore, we develop a pixel-wise sampling strategy that dynamically adjusts the residual transfer rate and noise strength based on pixel-level semantic weights, prioritizing semantically rich regions during the diffusion process. To enhance model training, we also propose a semantic consistency loss, which aligns pixel-wise semantic weights between predictions and ground truth. Extensive experiments on both real-world and synthetic datasets demonstrate that SAMSR significantly improves perceptual quality and detail recovery, particularly in semantically complex images. Our code is released at https://github.com/Liu-Zihang/SAMSR.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering</title>
<link>https://arxiv.org/abs/2505.07073</link>
<guid>https://arxiv.org/abs/2505.07073</guid>
<content:encoded><![CDATA[
arXiv:2505.07073v1 Announce Type: new 
Abstract: Concept-based explanations have emerged as an effective approach within Explainable Artificial Intelligence, enabling interpretable insights by aligning model decisions with human-understandable concepts. However, existing methods rely on computationally intensive procedures and struggle to efficiently capture complex, semantic concepts. Recently, the Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT) framework, introduced by Varshney et al. (2025), attempts to identify concepts via dimension-wise traversal of the latent space of a Variational Autoencoder trained on counterfactual trajectories. Extending the CDCT framework, this work introduces Concept Directions via Latent Clustering (CDLC), which extracts global, class-specific concept directions by clustering latent difference vectors derived from factual and diffusion-generated counterfactual image pairs. CDLC substantially reduces computational complexity by eliminating the exhaustive latent dimension traversal required in CDCT and enables the extraction of multidimensional semantic concepts encoded across the latent dimensions. This approach is validated on a real-world skin lesion dataset, demonstrating that the extracted concept directions align with clinically recognized dermoscopic features and, in some cases, reveal dataset-specific biases or unknown biomarkers. These results highlight that CDLC is interpretable, scalable, and applicable across high-stakes domains and diverse data modalities.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression</title>
<link>https://arxiv.org/abs/2505.07119</link>
<guid>https://arxiv.org/abs/2505.07119</guid>
<content:encoded><![CDATA[
arXiv:2505.07119v1 Announce Type: new 
Abstract: Visual Anomaly Detection (VAD) is a key task in industrial settings, where minimizing waste and operational costs is essential. Deploying deep learning models within Internet of Things (IoT) environments introduces specific challenges due to the limited computational power and bandwidth of edge devices. This study investigates how to perform VAD effectively under such constraints by leveraging compact and efficient processing strategies. We evaluate several data compression techniques, examining the trade-off between system latency and detection accuracy. Experiments on the MVTec AD benchmark demonstrate that significant compression can be achieved with minimal loss in anomaly detection performance compared to uncompressed data.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework</title>
<link>https://arxiv.org/abs/2505.07165</link>
<guid>https://arxiv.org/abs/2505.07165</guid>
<content:encoded><![CDATA[
arXiv:2505.07165v1 Announce Type: new 
Abstract: Recently, numerous pancreas segmentation methods have achieved promising performance on local single-source datasets. However, these methods don't adequately account for generalizability issues, and hence typically show limited performance and low stability on test data from other sources. Considering the limited availability of distinct data sources, we seek to improve the generalization performance of a pancreas segmentation model trained with a single-source dataset, i.e., the single source generalization task. In particular, we propose a dual self-supervised learning model that incorporates both global and local anatomical contexts. Our model aims to fully exploit the anatomical features of the intra-pancreatic and extra-pancreatic regions, and hence enhance the characterization of the high-uncertainty regions for more robust generalization. Specifically, we first construct a global-feature contrastive self-supervised learning module that is guided by the pancreatic spatial structure. This module obtains complete and consistent pancreatic features through promoting intra-class cohesion, and also extracts more discriminative features for differentiating between pancreatic and non-pancreatic tissues through maximizing inter-class separation. It mitigates the influence of surrounding tissue on the segmentation outcomes in high-uncertainty regions. Subsequently, a local-image restoration self-supervised learning module is introduced to further enhance the characterization of the high uncertainty regions. In this module, informative anatomical contexts are actually learned to recover randomly corrupted appearance patterns in those regions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.07172</link>
<guid>https://arxiv.org/abs/2505.07172</guid>
<content:encoded><![CDATA[
arXiv:2505.07172v1 Announce Type: new 
Abstract: Despite significant advancements in multimodal reasoning tasks, existing Large Vision-Language Models (LVLMs) are prone to producing visually ungrounded responses when interpreting associated images. In contrast, when humans embark on learning new knowledge, they often rely on a set of fundamental pre-study principles: reviewing outlines to grasp core concepts, summarizing key points to guide their focus and enhance understanding. However, such preparatory actions are notably absent in the current instruction tuning processes. This paper presents Re-Critic, an easily scalable rationale-augmented framework designed to incorporate fundamental rules and chain-of-thought (CoT) as a bridge to enhance reasoning abilities. Specifically, Re-Critic develops a visual rationale synthesizer that scalably augments raw instructions with rationale explanation. To probe more contextually grounded responses, Re-Critic employs an in-context self-critic mechanism to select response pairs for preference tuning. Experiments demonstrate that models fine-tuned with our rationale-augmented dataset yield gains that extend beyond hallucination-specific tasks to broader multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ranking-aware Continual Learning for LiDAR Place Recognition</title>
<link>https://arxiv.org/abs/2505.07198</link>
<guid>https://arxiv.org/abs/2505.07198</guid>
<content:encoded><![CDATA[
arXiv:2505.07198v1 Announce Type: new 
Abstract: Place recognition plays a significant role in SLAM, robot navigation, and autonomous driving applications. Benefiting from deep learning, the performance of LiDAR place recognition (LPR) has been greatly improved. However, many existing learning-based LPR methods suffer from catastrophic forgetting, which severely harms the performance of LPR on previously trained places after training on a new environment. In this paper, we introduce a continual learning framework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate forgetting. Inspired by the ranking process of place recognition retrieval, we present a ranking-aware knowledge distillation loss that encourages the network to preserve the high-level place recognition knowledge. We also introduce a knowledge fusion module to integrate the knowledge of old and new models for LiDAR place recognition. Our extensive experiments demonstrate that KDF can be applied to different networks to overcome catastrophic forgetting, surpassing the state-of-the-art methods in terms of mean Recall@1 and forgetting score.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2505.07209</link>
<guid>https://arxiv.org/abs/2505.07209</guid>
<content:encoded><![CDATA[
arXiv:2505.07209v1 Announce Type: new 
Abstract: Concept Bottleneck Models (CBMs) try to make the decision-making process transparent by exploring an intermediate concept space between the input image and the output prediction. Existing CBMs just learn coarse-grained relations between the whole image and the concepts, less considering local image information, leading to two main drawbacks: i) they often produce spurious visual-concept relations, hence decreasing model reliability; and ii) though CBMs could explain the importance of every concept to the final prediction, it is still challenging to tell which visual region produces the prediction. To solve these problems, this paper proposes a Disentangled Optimal Transport CBM (DOT-CBM) framework to explore fine-grained visual-concept relations between local image patches and concepts. Specifically, we model the concept prediction process as a transportation problem between the patches and concepts, thereby achieving explicit fine-grained feature alignment. We also incorporate orthogonal projection losses within the modality to enhance local feature disentanglement. To further address the shortcut issues caused by statistical biases in the data, we utilize the visual saliency map and concept label statistics as transportation priors. Thus, DOT-CBM can visualize inversion heatmaps, provide more reliable concept predictions, and produce more accurate class predictions. Comprehensive experiments demonstrate that our proposed DOT-CBM achieves SOTA performance on several tasks, including image classification, local part detection and out-of-distribution generalization.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection</title>
<link>https://arxiv.org/abs/2505.07219</link>
<guid>https://arxiv.org/abs/2505.07219</guid>
<content:encoded><![CDATA[
arXiv:2505.07219v1 Announce Type: new 
Abstract: Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detector's backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at https://github.com/qinhongda8/LDDS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Dance Video Archives Challenge Computer Vision</title>
<link>https://arxiv.org/abs/2505.07249</link>
<guid>https://arxiv.org/abs/2505.07249</guid>
<content:encoded><![CDATA[
arXiv:2505.07249v1 Announce Type: new 
Abstract: The accuracy and efficiency of human body pose estimation depend on the quality of the data to be processed and of the particularities of these data. To demonstrate how dance videos can challenge pose estimation techniques, we proposed a new 3D human body pose estimation pipeline which combined up-to-date techniques and methods that had not been yet used in dance analysis. Second, we performed tests and extensive experimentations from dance video archives, and used visual analytic tools to evaluate the impact of several data parameters on human body pose. Our results are publicly available for research at https://www.couleur.org/articles/arXiv-1-2025/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incomplete In-context Learning</title>
<link>https://arxiv.org/abs/2505.07251</link>
<guid>https://arxiv.org/abs/2505.07251</guid>
<content:encoded><![CDATA[
arXiv:2505.07251v1 Announce Type: new 
Abstract: Large vision language models (LVLMs) achieve remarkable performance through Vision In-context Learning (VICL), a process that depends significantly on demonstrations retrieved from an extensive collection of annotated examples (retrieval database). Existing studies often assume that the retrieval database contains annotated examples for all labels. However, in real-world scenarios, delays in database updates or incomplete data annotation may result in the retrieval database containing labeled samples for only a subset of classes. We refer to this phenomenon as an \textbf{incomplete retrieval database} and define the in-context learning under this condition as \textbf{Incomplete In-context Learning (IICL)}. To address this challenge, we propose \textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage framework designed to mitigate the limitations of IICL. The Iterative Judgments Stage reformulates an \(\boldsymbol{m}\)-class classification problem into a series of \(\boldsymbol{m}\) binary classification tasks, effectively converting the IICL setting into a standard VICL scenario. The Integrated Prediction Stage further refines the classification process by leveraging both the input image and the predictions from the Iterative Judgments Stage to enhance overall classification accuracy. IJIP demonstrates considerable performance across two LVLMs and two datasets under three distinct conditions of label incompleteness, achieving the highest accuracy of 93.9\%. Notably, even in scenarios where labels are fully available, IJIP still achieves the best performance of all six baselines. Furthermore, IJIP can be directly applied to \textbf{Prompt Learning} and is adaptable to the \textbf{text domain}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2505.07254</link>
<guid>https://arxiv.org/abs/2505.07254</guid>
<content:encoded><![CDATA[
arXiv:2505.07254v1 Announce Type: new 
Abstract: This work addresses the critical lack of precision in state estimation in the Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of selecting the appropriate motion model. Existing literature commonly relies on constant motion models for estimating the states of objects, neglecting the complex motion dynamics unique to each object. Consequently, trajectory division and imprecise object localization arise, especially under occlusion conditions. The core of these challenges lies in the limitations of the current Kalman filter formulation, which fails to account for the variability of motion dynamics as objects navigate their environments. This work introduces a novel formulation of the Kalman filter that incorporates motion dynamics, allowing the motion model to adaptively adjust according to changes in the object's movement. The proposed Kalman filter substantially improves state estimation, localization, and trajectory prediction compared to the traditional Kalman filter. This is reflected in tracking performance that surpasses recent benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\% and 0.81\% in higher order tracking accuracy (HOTA) and multi-object tracking accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter consistently outperforms the baseline across various detectors. Additionally, it shows an enhanced capability in managing long occlusions compared to the baseline Kalman filter, achieving margins of 1.22\% in higher order tracking accuracy (HOTA) and 1.55\% in multi-object tracking accuracy (MOTA) on the KITTI dataset. The formulation's efficiency is evident, with an additional processing time of only approximately 0.078 ms per frame, ensuring its applicability in real-time applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Similarity Search in Automotive Production</title>
<link>https://arxiv.org/abs/2505.07256</link>
<guid>https://arxiv.org/abs/2505.07256</guid>
<content:encoded><![CDATA[
arXiv:2505.07256v1 Announce Type: new 
Abstract: Visual quality inspection in automotive production is essential for ensuring the safety and reliability of vehicles. Computer vision (CV) has become a popular solution for these inspections due to its cost-effectiveness and reliability. However, CV models require large, annotated datasets, which are costly and time-consuming to collect. To reduce the need for extensive training data, we propose a novel image classification pipeline that combines similarity search using a vision-based foundation model with synthetic data. Our approach leverages a DINOv2 model to transform input images into feature vectors, which are then compared to pre-classified reference images using cosine distance measurements. By utilizing synthetic data instead of real images as references, our pipeline achieves high classification accuracy without relying on real data. We evaluate this approach in eight real-world inspection scenarios and demonstrate that it meets the high performance requirements of production environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2505.07263</link>
<guid>https://arxiv.org/abs/2505.07263</guid>
<content:encoded><![CDATA[
arXiv:2505.07263v1 Announce Type: new 
Abstract: We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers</title>
<link>https://arxiv.org/abs/2505.07300</link>
<guid>https://arxiv.org/abs/2505.07300</guid>
<content:encoded><![CDATA[
arXiv:2505.07300v1 Announce Type: new 
Abstract: Training-free Neural Architecture Search (NAS) efficiently identifies high-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot and one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the need for model training, and (ii) interpretable, with proxy designs often theoretically grounded. Despite rapid developments in the field, current SOTA ZC proxies are typically constrained to well-established convolutional search spaces. With the rise of Large Language Models shaping the future of deep learning, this work extends ZC proxy applicability to Vision Transformers (ViTs). We present a new benchmark using the Autoformer search space evaluated on 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients information (L-SWAG), a novel, generalizable metric that characterizes both convolutional and transformer architectures across 14 tasks. Additionally, previous works highlighted how different proxies contain complementary information, motivating the need for a ML model to identify useful combinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low Information gain and Bias Re-Alignment), a method that strategically combines proxies to best represent a specific benchmark. Integrated into the NAS search, LIBRA-NAS outperforms evolution and gradient-based NAS techniques by identifying an architecture with a 17.0% test error on ImageNet1k in just 0.1 GPU days.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</title>
<link>https://arxiv.org/abs/2505.07301</link>
<guid>https://arxiv.org/abs/2505.07301</guid>
<content:encoded><![CDATA[
arXiv:2505.07301v1 Announce Type: new 
Abstract: In 3D Human Motion Prediction (HMP), conventional methods train HMP models with expensive motion capture data. However, the data collection cost of such motion capture data limits the data diversity, which leads to poor generalizability to unseen motions or subjects. To address this issue, this paper proposes to enhance HMP with additional learning using estimated poses from easily available videos. The 2D poses estimated from the monocular videos are carefully transformed into motion capture-style 3D motions through our pipeline. By additional learning with the obtained motions, the HMP model is adapted to the test domain. The experimental results demonstrate the quantitative and qualitative impact of our method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Privacy-Aware AI-Based Ergonomic Analysis</title>
<link>https://arxiv.org/abs/2505.07306</link>
<guid>https://arxiv.org/abs/2505.07306</guid>
<content:encoded><![CDATA[
arXiv:2505.07306v1 Announce Type: new 
Abstract: Musculoskeletal disorders (MSDs) are a leading cause of injury and productivity loss in the manufacturing industry, incurring substantial economic costs. Ergonomic assessments can mitigate these risks by identifying workplace adjustments that improve posture and reduce strain. Camera-based systems offer a non-intrusive, cost-effective method for continuous ergonomic tracking, but they also raise significant privacy concerns. To address this, we propose a privacy-aware ergonomic assessment framework utilizing machine learning techniques. Our approach employs adversarial training to develop a lightweight neural network that obfuscates video data, preserving only the essential information needed for human pose estimation. This obfuscation ensures compatibility with standard pose estimation algorithms, maintaining high accuracy while protecting privacy. The obfuscated video data is transmitted to a central server, where state-of-the-art keypoint detection algorithms extract body landmarks. Using multi-view integration, 3D keypoints are reconstructed and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system provides a secure, effective solution for ergonomic monitoring in industrial environments, addressing both privacy and workplace safety concerns.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning</title>
<link>https://arxiv.org/abs/2505.07322</link>
<guid>https://arxiv.org/abs/2505.07322</guid>
<content:encoded><![CDATA[
arXiv:2505.07322v1 Announce Type: new 
Abstract: High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming increasingly prevalent, intensifying the demand for converting Standard Dynamic Range (SDR) content to HDR. Existing methods primarily rely on fixed tone mapping operators, which are inadequate for handling SDR inputs with diverse styles commonly found in real-world scenarios. To address this challenge, we propose a generalized SDR-to-HDR method that handles diverse styles in real-world SDR content, termed Realistic Style Disentangled Representation Learning (RealRep). By disentangling luminance and chrominance, we analyze the intrinsic differences between contents with varying styles and propose a disentangled multi-view style representation learning method. This approach captures the guidance prior of true luminance and chrominance distributions across different styles, even when the SDR style distributions exhibit significant variations, thereby establishing a robust embedding space for inverse tone mapping. Motivated by the difficulty of directly utilizing degradation representation priors, we further introduce the Degradation-Domain Aware Controlled Mapping Network (DDACMNet), a two-stage framework that performs adaptive hierarchical mapping guided by a control-aware normalization mechanism. DDACMNet dynamically modulates the mapping process via degradation-conditioned hierarchical features, enabling robust adaptation across diverse degradation domains. Extensive experiments show that RealRep consistently outperforms state-of-the-art methods with superior generalization and perceptually faithful HDR color gamut reconstruction.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video</title>
<link>https://arxiv.org/abs/2505.07333</link>
<guid>https://arxiv.org/abs/2505.07333</guid>
<content:encoded><![CDATA[
arXiv:2505.07333v1 Announce Type: new 
Abstract: Fast 3D clothed human reconstruction from monocular video remains a significant challenge in computer vision, particularly in balancing computational efficiency with reconstruction quality. Current approaches are either focused on static image reconstruction but too computationally intensive, or achieve high quality through per-video optimization that requires minutes to hours of processing, making them unsuitable for real-time applications. To this end, we present TemPoFast3D, a novel method that leverages temporal coherency of human appearance to reduce redundant computation while maintaining reconstruction quality. Our approach is a "plug-and play" solution that uniquely transforms pixel-aligned reconstruction networks to handle continuous video streams by maintaining and refining a canonical appearance representation through efficient coordinate mapping. Extensive experiments demonstrate that TemPoFast3D matches or exceeds state-of-the-art methods across standard metrics while providing high-quality textured reconstruction across diverse pose and appearance, with a maximum speed of 12 FPS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction</title>
<link>https://arxiv.org/abs/2505.07336</link>
<guid>https://arxiv.org/abs/2505.07336</guid>
<content:encoded><![CDATA[
arXiv:2505.07336v1 Announce Type: new 
Abstract: Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Pre-trained Autoregressive Diffusion Transformer</title>
<link>https://arxiv.org/abs/2505.07344</link>
<guid>https://arxiv.org/abs/2505.07344</guid>
<content:encoded><![CDATA[
arXiv:2505.07344v1 Announce Type: new 
Abstract: In this work, we present GPDiT, a Generative Pre-trained Autoregressive Diffusion Transformer that unifies the strengths of diffusion and autoregressive modeling for long-range video synthesis, within a continuous latent space. Instead of predicting discrete tokens, GPDiT autoregressively predicts future latent frames using a diffusion loss, enabling natural modeling of motion dynamics and semantic consistency across frames. This continuous autoregressive framework not only enhances generation quality but also endows the model with representation capabilities. Additionally, we introduce a lightweight causal attention variant and a parameter-free rotation-based time-conditioning mechanism, improving both the training and inference efficiency. Extensive experiments demonstrate that GPDiT achieves strong performance in video generation quality, video representation ability, and few-shot learning tasks, highlighting its potential as an effective framework for video modeling in continuous space.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography</title>
<link>https://arxiv.org/abs/2505.07347</link>
<guid>https://arxiv.org/abs/2505.07347</guid>
<content:encoded><![CDATA[
arXiv:2505.07347v1 Announce Type: new 
Abstract: Echocardiographers can detect pulmonary hypertension using Doppler echocardiography; however, accurately assessing its progression often proves challenging. Right heart catheterization (RHC), the gold standard for precise evaluation, is invasive and unsuitable for routine use, limiting its practicality for timely diagnosis and monitoring of pulmonary hypertension progression. Here, we propose MePH, a multi-view, multi-modal vision-language model to accurately assess pulmonary hypertension progression using non-invasive echocardiography. We constructed a large dataset comprising paired standardized echocardiogram videos, spectral images and RHC data, covering 1,237 patient cases from 12 medical centers. For the first time, MePH precisely models the correlation between non-invasive multi-view, multi-modal echocardiography and the pressure and resistance obtained via RHC. We show that MePH significantly outperforms echocardiographers' assessments using echocardiography, reducing the mean absolute error in estimating mean pulmonary arterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and 43.81%, respectively. In eight independent external hospitals, MePH achieved a mean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an area under the curve of 0.921, surpassing echocardiographers (area under the curve of 0.842) in accurately predicting the severity of pulmonary hypertension, whether mild or severe. A prospective study demonstrated that MePH can predict treatment efficacy for patients. Our work provides pulmonary hypertension patients with a non-invasive and timely method for monitoring disease progression, improving the accuracy and efficiency of pulmonary hypertension management while enabling earlier interventions and more personalized treatment decisions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild</title>
<link>https://arxiv.org/abs/2505.07373</link>
<guid>https://arxiv.org/abs/2505.07373</guid>
<content:encoded><![CDATA[
arXiv:2505.07373v1 Announce Type: new 
Abstract: Neural implicit surface reconstruction using volume rendering techniques has recently achieved significant advancements in creating high-fidelity surfaces from multiple 2D images. However, current methods primarily target scenes with consistent illumination and struggle to accurately reconstruct 3D geometry in uncontrolled environments with transient occlusions or varying appearances. While some neural radiance field (NeRF)-based variants can better manage photometric variations and transient objects in complex scenes, they are designed for novel view synthesis rather than precise surface reconstruction due to limited surface constraints. To overcome this limitation, we introduce a novel approach that applies multiple geometric constraints to the implicit surface optimization process, enabling more accurate reconstructions from unconstrained image collections. First, we utilize sparse 3D points from structure-from-motion (SfM) to refine the signed distance function estimation for the reconstructed surface, with a displacement compensation to accommodate noise in the sparse points. Additionally, we employ robust normal priors derived from a normal predictor, enhanced by edge prior filtering and multi-view consistency constraints, to improve alignment with the actual surface geometry. Extensive testing on the Heritage-Recon benchmark and other datasets has shown that the proposed method can accurately reconstruct surfaces from in-the-wild images, yielding geometries with superior accuracy and granularity compared to existing techniques. Our approach enables high-quality 3D reconstruction of various landmarks, making it applicable to diverse scenarios such as digital preservation of cultural heritage sites.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.07375</link>
<guid>https://arxiv.org/abs/2505.07375</guid>
<content:encoded><![CDATA[
arXiv:2505.07375v1 Announce Type: new 
Abstract: Point cloud anomaly detection is essential for various industrial applications. The huge computation and storage costs caused by the increasing product classes limit the application of single-class unsupervised methods, necessitating the development of multi-class unsupervised methods. However, the feature similarity between normal and anomalous points from different class data leads to the feature confusion problem, which greatly hinders the performance of multi-class methods. Therefore, we introduce a multi-class point cloud anomaly detection method, named GLFM, leveraging global-local feature matching to progressively separate data that are prone to confusion across multiple classes. Specifically, GLFM is structured into three stages: Stage-I proposes an anomaly synthesis pipeline that stretches point clouds to create abundant anomaly data that are utilized to adapt the point cloud feature extractor for better feature representation. Stage-II establishes the global and local memory banks according to the global and local feature distributions of all the training data, weakening the impact of feature confusion on the establishment of the memory bank. Stage-III implements anomaly detection of test data leveraging its feature distance from global and local memory banks. Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts dataset showcase our proposed GLFM's superior point cloud anomaly detection performance. The code is available at https://github.com/hustCYQ/GLFM-Multi-class-3DAD.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apple's Synthetic Defocus Noise Pattern: Characterization and Forensic Applications</title>
<link>https://arxiv.org/abs/2505.07380</link>
<guid>https://arxiv.org/abs/2505.07380</guid>
<content:encoded><![CDATA[
arXiv:2505.07380v1 Announce Type: new 
Abstract: iPhone portrait-mode images contain a distinctive pattern in out-of-focus regions simulating the bokeh effect, which we term Apple's Synthetic Defocus Noise Pattern (SDNP). If overlooked, this pattern can interfere with blind forensic analyses, especially PRNU-based camera source verification, as noted in earlier works. Since Apple's SDNP remains underexplored, we provide a detailed characterization, proposing a method for its precise estimation, modeling its dependence on scene brightness, ISO settings, and other factors. Leveraging this characterization, we explore forensic applications of the SDNP, including traceability of portrait-mode images across iPhone models and iOS versions in open-set scenarios, assessing its robustness under post-processing. Furthermore, we show that masking SDNP-affected regions in PRNU-based camera source verification significantly reduces false positives, overcoming a critical limitation in camera attribution, and improving state-of-the-art techniques.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Semantic Encoding and Decoding for Video Surveillance</title>
<link>https://arxiv.org/abs/2505.07381</link>
<guid>https://arxiv.org/abs/2505.07381</guid>
<content:encoded><![CDATA[
arXiv:2505.07381v1 Announce Type: new 
Abstract: With the continuous increase in the number and resolution of video surveillance cameras, the burden of transmitting and storing surveillance video is growing. Traditional communication methods based on Shannon's theory are facing optimization bottlenecks. Semantic communication, as an emerging communication method, is expected to break through this bottleneck and reduce the storage and transmission consumption of video. Existing semantic decoding methods often require many samples to train the neural network for each scene, which is time-consuming and labor-intensive. In this study, a semantic encoding and decoding method for surveillance video is proposed. First, the sketch was extracted as semantic information, and a sketch compression method was proposed to reduce the bit rate of semantic information. Then, an image translation network was proposed to translate the sketch into a video frame with a reference frame. Finally, a few-shot sketch decoding network was proposed to reconstruct video from sketch. Experimental results showed that the proposed method achieved significantly better video reconstruction performance than baseline methods. The sketch compression method could effectively reduce the storage and transmission consumption of semantic information with little compromise on video quality. The proposed method provides a novel semantic encoding and decoding method that only needs a few training samples for each surveillance scene, thus improving the practicality of the semantic communication system.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Visualization in 3D Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2505.07387</link>
<guid>https://arxiv.org/abs/2505.07387</guid>
<content:encoded><![CDATA[
arXiv:2505.07387v1 Announce Type: new 
Abstract: Understanding the computations of convolutional neural networks requires effective visualization of their kernels. While maximal activation methods have proven successful in highlighting the preferred features of 2D convolutional kernels, directly applying these techniques to 3D convolutions often leads to uninterpretable results due to the higher dimensionality and complexity of 3D features. To address this challenge, we propose a novel visualization approach for 3D convolutional kernels that disentangles their texture and motion preferences. Our method begins with a data-driven decomposition of the optimal input that maximally activates a given kernel. We then introduce a two-stage optimization strategy to extract distinct texture and motion components from this input. Applying our approach to visualize kernels at various depths of several pre-trained models, we find that the resulting visualizations--particularly those capturing motion--clearly reveal the preferred dynamic patterns encoded by 3D kernels. These results demonstrate the effectiveness of our method in providing interpretable insights into 3D convolutional operations. Code is available at https://github.com/YatangLiLab/3DKernelVisualizer.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.07396</link>
<guid>https://arxiv.org/abs/2505.07396</guid>
<content:encoded><![CDATA[
arXiv:2505.07396v1 Announce Type: new 
Abstract: Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models' updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection</title>
<link>https://arxiv.org/abs/2505.07398</link>
<guid>https://arxiv.org/abs/2505.07398</guid>
<content:encoded><![CDATA[
arXiv:2505.07398v1 Announce Type: new 
Abstract: State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we are the first to observe that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-GFusion module adaptively adjusts the weights of image Bird's-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-LFusion module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion method surpasses previous state-of-the-art methods. Moreover, our DepthFusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture</title>
<link>https://arxiv.org/abs/2505.07444</link>
<guid>https://arxiv.org/abs/2505.07444</guid>
<content:encoded><![CDATA[
arXiv:2505.07444v1 Announce Type: new 
Abstract: Efficient crop-weed segmentation is critical for site-specific weed control in precision agriculture. Conventional CNN-based methods struggle to generalize and rely on RGB imagery, limiting performance under complex field conditions. To address these challenges, we propose a lightweight transformer-CNN hybrid. It processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using specialized encoders and dynamic modality integration. Evaluated on the WeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of 78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7 million parameters, the model offers high accuracy, computational efficiency, and potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and edge devices, advancing precision weed management.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing degeneracies in latent interpolation for diffusion models</title>
<link>https://arxiv.org/abs/2505.07481</link>
<guid>https://arxiv.org/abs/2505.07481</guid>
<content:encoded><![CDATA[
arXiv:2505.07481v1 Announce Type: new 
Abstract: There is an increasing interest in using image-generating diffusion models for deep data augmentation and image morphing. In this context, it is useful to interpolate between latents produced by inverting a set of input images, in order to generate new images representing some mixture of the inputs. We observe that such interpolation can easily lead to degenerate results when the number of inputs is large. We analyze the cause of this effect theoretically and experimentally, and suggest a suitable remedy. The suggested approach is a relatively simple normalization scheme that is easy to use whenever interpolation between latents is needed. We measure image quality using FID and CLIP embedding distance and show experimentally that baseline interpolation methods lead to a drop in quality metrics long before the degeneration issue is clearly visible. In contrast, our method significantly reduces the degeneration effect and leads to improved quality metrics also in non-degenerate situations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocVXQA: Context-Aware Visual Explanations for Document Question Answering</title>
<link>https://arxiv.org/abs/2505.07496</link>
<guid>https://arxiv.org/abs/2505.07496</guid>
<content:encoded><![CDATA[
arXiv:2505.07496v1 Announce Type: new 
Abstract: We propose DocVXQA, a novel framework for visually self-explainable document question answering. The framework is designed not only to produce accurate answers to questions but also to learn visual heatmaps that highlight contextually critical regions, thereby offering interpretable justifications for the model's decisions. To integrate explanations into the learning process, we quantitatively formulate explainability principles as explicit learning objectives. Unlike conventional methods that emphasize only the regions pertinent to the answer, our framework delivers explanations that are \textit{contextually sufficient} while remaining \textit{representation-efficient}. This fosters user trust while achieving a balance between predictive performance and interpretability in DocVQA applications. Extensive experiments, including human evaluation, provide strong evidence supporting the effectiveness of our method. The code is available at https://github.com/dali92002/DocVXQA.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models</title>
<link>https://arxiv.org/abs/2505.07500</link>
<guid>https://arxiv.org/abs/2505.07500</guid>
<content:encoded><![CDATA[
arXiv:2505.07500v1 Announce Type: new 
Abstract: The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as "bring me a spoon", without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAIS: Memory-Attention for Interactive Segmentation</title>
<link>https://arxiv.org/abs/2505.07511</link>
<guid>https://arxiv.org/abs/2505.07511</guid>
<content:encoded><![CDATA[
arXiv:2505.07511v1 Announce Type: new 
Abstract: Interactive medical segmentation reduces annotation effort by refining predictions through user feedback. Vision Transformer (ViT)-based models, such as the Segment Anything Model (SAM), achieve state-of-the-art performance using user clicks and prior masks as prompts. However, existing methods treat interactions as independent events, leading to redundant corrections and limited refinement gains. We address this by introducing MAIS, a Memory-Attention mechanism for Interactive Segmentation that stores past user inputs and segmentation states, enabling temporal context integration. Our approach enhances ViT-based segmentation across diverse imaging modalities, achieving more efficient and accurate refinements.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images</title>
<link>https://arxiv.org/abs/2505.07530</link>
<guid>https://arxiv.org/abs/2505.07530</guid>
<content:encoded><![CDATA[
arXiv:2505.07530v1 Announce Type: new 
Abstract: Synthetic face datasets are increasingly used to overcome the limitations of real-world biometric data, including privacy concerns, demographic imbalance, and high collection costs. However, many existing methods lack fine-grained control over identity attributes and fail to produce paired, identity-consistent images under structured capture conditions. We introduce FLUXSynID, a framework for generating high-resolution synthetic face datasets with user-defined identity attribute distributions and paired document-style and trusted live capture images. The dataset generated using the FLUXSynID framework shows improved alignment with real-world identity distributions and greater inter-set diversity compared to prior work. The FLUXSynID framework for generating custom datasets, along with a dataset of 14,889 synthetic identities, is publicly released to support biometric research, including face recognition and morphing attack detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability</title>
<link>https://arxiv.org/abs/2505.07533</link>
<guid>https://arxiv.org/abs/2505.07533</guid>
<content:encoded><![CDATA[
arXiv:2505.07533v1 Announce Type: new 
Abstract: Monitoring and analyzing electrocardiogram (ECG) signals, even under varying physiological conditions, including those influenced by physical activity, drugs and stress, is crucial to accurately assess cardiac health. However, current AI-based methods often fail to account for how these factors interact and alter ECG patterns, ultimately limiting their applicability in real-world settings. This study introduces IKrNet, a novel neural network model, which identifies drug-specific patterns in ECGs amidst certain physiological conditions. IKrNet's architecture incorporates spatial and temporal dynamics by using a convolutional backbone with varying receptive field size to capture spatial features. A bi-directional Long Short-Term Memory module is also employed to model temporal dependencies. By treating heart rate variability as a surrogate for physiological fluctuations, we evaluated IKrNet's performance across diverse scenarios, including conditions with physical stress, drug intake alone, and a baseline without drug presence. Our assessment follows a clinical protocol in which 990 healthy volunteers were administered 80mg of Sotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a life-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art models' accuracy and stability in varying physiological conditions, underscoring its clinical viability.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning</title>
<link>https://arxiv.org/abs/2505.07538</link>
<guid>https://arxiv.org/abs/2505.07538</guid>
<content:encoded><![CDATA[
arXiv:2505.07538v1 Announce Type: new 
Abstract: We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior -- mirroring the causal structure of language -- into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture -- like that in LLMs -- without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: https://selftok-team.github.io/report/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIFStream: 4D Gaussian-based Immersive Video with Feature Stream</title>
<link>https://arxiv.org/abs/2505.07539</link>
<guid>https://arxiv.org/abs/2505.07539</guid>
<content:encoded><![CDATA[
arXiv:2505.07539v1 Announce Type: new 
Abstract: Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynID: Passport Synthetic Dataset for Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2505.07540</link>
<guid>https://arxiv.org/abs/2505.07540</guid>
<content:encoded><![CDATA[
arXiv:2505.07540v1 Announce Type: new 
Abstract: The demand for Presentation Attack Detection (PAD) to identify fraudulent ID documents in remote verification systems has significantly risen in recent years. This increase is driven by several factors, including the rise of remote work, online purchasing, migration, and advancements in synthetic images. Additionally, we have noticed a surge in the number of attacks aimed at the enrolment process. Training a PAD to detect fake ID documents is very challenging because of the limited number of ID documents available due to privacy concerns. This work proposes a new passport dataset generated from a hybrid method that combines synthetic data and open-access information using the ICAO requirement to obtain realistic training and testing images.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies</title>
<link>https://arxiv.org/abs/2505.07552</link>
<guid>https://arxiv.org/abs/2505.07552</guid>
<content:encoded><![CDATA[
arXiv:2505.07552v1 Announce Type: new 
Abstract: Teachers' visual attention and its distribution across the students in classrooms can constitute important implications for student engagement, achievement, and professional teacher training. Despite that, inferring the information about where and which student teachers focus on is not trivial. Mobile eye tracking can provide vital help to solve this issue; however, the use of mobile eye tracking alone requires a significant amount of manual annotations. To address this limitation, we present an automated processing pipeline concept that requires minimal manually annotated data to recognize which student the teachers focus on. To this end, we utilize state-of-the-art face detection models and face recognition feature embeddings to train face recognition models with transfer learning in the classroom context and combine these models with the teachers' gaze from mobile eye trackers. We evaluated our approach with data collected from four different classrooms, and our results show that while it is possible to estimate the visually focused students with reasonable performance in all of our classroom setups, U-shaped and small classrooms led to the best results with accuracies of approximately 0.7 and 0.9, respectively. While we did not evaluate our method for teacher-student interactions and focused on the validity of the technical approach, as our methodology does not require a vast amount of manually annotated data and offers a non-intrusive way of handling teachers' visual attention, it could help improve instructional strategies, enhance classroom management, and provide feedback for professional teacher development.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs</title>
<link>https://arxiv.org/abs/2505.07556</link>
<guid>https://arxiv.org/abs/2505.07556</guid>
<content:encoded><![CDATA[
arXiv:2505.07556v1 Announce Type: new 
Abstract: Event cameras offer significant advantages over traditional frame-based sensors. These include microsecond temporal resolution, robustness under varying lighting conditions and low power consumption. Nevertheless, the effective processing of their sparse, asynchronous event streams remains challenging. Existing approaches to this problem can be categorised into two distinct groups. The first group involves the direct processing of event data with neural models, such as Spiking Neural Networks or Graph Convolutional Neural Networks. However, this approach is often accompanied by a compromise in terms of qualitative performance. The second group involves the conversion of events into dense representations with handcrafted aggregation functions, which can boost accuracy at the cost of temporal fidelity. This paper introduces a novel Self-Supervised Event Representation (SSER) method leveraging Gated Recurrent Unit (GRU) networks to achieve precise per-pixel encoding of event timestamps and polarities without temporal discretisation. The recurrent layers are trained in a self-supervised manner to maximise the fidelity of event-time encoding. The inference is performed with event representations generated asynchronously, thus ensuring compatibility with high-throughput sensors. The experimental validation demonstrates that SSER outperforms aggregation-based baselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx object detection datasets. Furthermore, the paper presents the first hardware implementation of recurrent representation for event data on a System-on-Chip FPGA, achieving sub-microsecond latency and power consumption between 1-2 W, suitable for real-time, power-efficient applications. Code is available at https://github.com/vision-agh/RecRepEvent.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework</title>
<link>https://arxiv.org/abs/2505.07573</link>
<guid>https://arxiv.org/abs/2505.07573</guid>
<content:encoded><![CDATA[
arXiv:2505.07573v1 Announce Type: new 
Abstract: Kidney abnormality segmentation has important potential to enhance the clinical workflow, especially in settings requiring quantitative assessments. Kidney volume could serve as an important biomarker for renal diseases, with changes in volume correlating directly with kidney function. Currently, clinical practice often relies on subjective visual assessment for evaluating kidney size and abnormalities, including tumors and cysts, which are typically staged based on diameter, volume, and anatomical location. To support a more objective and reproducible approach, this research aims to develop a robust, thoroughly validated kidney abnormality segmentation algorithm, made publicly available for clinical and research use. We employ publicly available training datasets and leverage the state-of-the-art medical image segmentation framework nnU-Net. Validation is conducted using both proprietary and public test datasets, with segmentation performance quantified by Dice coefficient and the 95th percentile Hausdorff distance. Furthermore, we analyze robustness across subgroups based on patient sex, age, CT contrast phases, and tumor histologic subtypes. Our findings demonstrate that our segmentation algorithm, trained exclusively on publicly available data, generalizes effectively to external test sets and outperforms existing state-of-the-art models across all tested datasets. Subgroup analyses reveal consistent high performance, indicating strong robustness and reliability. The developed algorithm and associated code are publicly accessible at https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study</title>
<link>https://arxiv.org/abs/2505.07576</link>
<guid>https://arxiv.org/abs/2505.07576</guid>
<content:encoded><![CDATA[
arXiv:2505.07576v1 Announce Type: new 
Abstract: Semiconductor manufacturing is a complex, multistage process. Automated visual inspection of Scanning Electron Microscope (SEM) images is indispensable for minimizing equipment downtime and containing costs. Most previous research considers supervised approaches, assuming a sufficient number of anomalously labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging research domain, focuses on unsupervised learning, avoiding the costly defect collection phase while providing explanations of the predictions. We introduce a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset. Our results demonstrate the efficacy of modern VAD approaches in this field.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions</title>
<link>https://arxiv.org/abs/2505.07611</link>
<guid>https://arxiv.org/abs/2505.07611</guid>
<content:encoded><![CDATA[
arXiv:2505.07611v1 Announce Type: new 
Abstract: Traffic accident prediction and detection are critical for enhancing road safety,and vision-based traffic accident anticipation (Vision-TAA) has emerged as a promising approach in the era of deep learning.This paper reviews 147 recent studies,focusing on the application of supervised,unsupervised,and hybrid deep learning models for accident prediction,alongside the use of real-world and synthetic datasets.Current methodologies are categorized into four key approaches: image and video feature-based prediction, spatiotemporal feature-based prediction, scene understanding,and multimodal data fusion.While these methods demonstrate significant potential,challenges such as data scarcity,limited generalization to complex scenarios,and real-time performance constraints remain prevalent. This review highlights opportunities for future research,including the integration of multimodal data fusion, self-supervised learning,and Transformer-based architectures to enhance prediction accuracy and scalability.By synthesizing existing advancements and identifying critical gaps, this paper provides a foundational reference for developing robust and adaptive Vision-TAA systems,contributing to road safety and traffic management.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Convolution Improves Neural Predictivity in the Retina</title>
<link>https://arxiv.org/abs/2505.07620</link>
<guid>https://arxiv.org/abs/2505.07620</guid>
<content:encoded><![CDATA[
arXiv:2505.07620v1 Announce Type: new 
Abstract: We present a novel approach to neural response prediction that incorporates higher-order operations directly within convolutional neural networks (CNNs). Our model extends traditional 3D CNNs by embedding higher-order operations within the convolutional operator itself, enabling direct modeling of multiplicative interactions between neighboring pixels across space and time. Our model increases the representational power of CNNs without increasing their depth, therefore addressing the architectural disparity between deep artificial networks and the relatively shallow processing hierarchy of biological visual systems. We evaluate our approach on two distinct datasets: salamander retinal ganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC responses to controlled geometric transformations. Our higher-order CNN (HoCNN) achieves superior performance while requiring only half the training data compared to standard architectures, demonstrating correlation coefficients up to 0.75 with neural responses (against 0.80$\pm$0.02 retinal reliability). When integrated into state-of-the-art architectures, our approach consistently improves performance across different species and stimulus conditions. Analysis of the learned representations reveals that our network naturally encodes fundamental geometric transformations, particularly scaling parameters that characterize object expansion and contraction. This capability is especially relevant for specific cell types, such as transient OFF-alpha and transient ON cells, which are known to detect looming objects and object motion respectively, and where our model shows marked improvement in response prediction. The correlation coefficients for scaling parameters are more than twice as high in HoCNN (0.72) compared to baseline models (0.32).
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios</title>
<link>https://arxiv.org/abs/2505.07622</link>
<guid>https://arxiv.org/abs/2505.07622</guid>
<content:encoded><![CDATA[
arXiv:2505.07622v1 Announce Type: new 
Abstract: Cross-view geo-localization is a promising solution for large-scale localization problems, requiring the sequential execution of retrieval and metric localization tasks to achieve fine-grained predictions. However, existing methods typically focus on designing standalone models for these two tasks, resulting in inefficient collaboration and increased training overhead. In this paper, we propose UnifyGeo, a novel unified hierarchical geo-localization framework that integrates retrieval and metric localization tasks into a single network. Specifically, we first employ a unified learning strategy with shared parameters to jointly learn multi-granularity representation, facilitating mutual reinforcement between these two tasks. Subsequently, we design a re-ranking mechanism guided by a dedicated loss function, which enhances geo-localization performance by improving both retrieval accuracy and metric localization references. Extensive experiments demonstrate that UnifyGeo significantly outperforms the state-of-the-arts in both task-isolated and task-associated settings. Remarkably, on the challenging VIGOR benchmark, which supports fine-grained localization evaluation, the 1-meter-level localization recall rate improves from 1.53\% to 39.64\% and from 0.43\% to 25.58\% under same-area and cross-area evaluations, respectively. Code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.07652</link>
<guid>https://arxiv.org/abs/2505.07652</guid>
<content:encoded><![CDATA[
arXiv:2505.07652v1 Announce Type: new 
Abstract: Current diffusion-based text-to-video methods are limited to producing short video clips of a single shot and lack the capability to generate multi-shot videos with discrete transitions where the same character performs distinct activities across the same or different backgrounds. To address this limitation we propose a framework that includes a dataset collection pipeline and architectural extensions to video diffusion models to enable text-to-multi-shot video generation. Our approach enables generation of multi-shot videos as a single video with full attention across all frames of all shots, ensuring character and background consistency, and allows users to control the number, duration, and content of shots through shot-specific conditioning. This is achieved by incorporating a transition token into the text-to-video model to control at which frames a new shot begins and a local attention masking strategy which controls the transition token's effect and allows shot-specific prompting. To obtain training data we propose a novel data collection pipeline to construct a multi-shot video dataset from existing single-shot video datasets. Extensive experiments demonstrate that fine-tuning a pre-trained text-to-video model for a few thousand iterations is enough for the model to subsequently be able to generate multi-shot videos with shot-specific control, outperforming the baselines. You can find more details in https://shotadapter.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anatomical Attention Alignment representation for Radiology Report Generation</title>
<link>https://arxiv.org/abs/2505.07689</link>
<guid>https://arxiv.org/abs/2505.07689</guid>
<content:encoded><![CDATA[
arXiv:2505.07689v1 Announce Type: new 
Abstract: Automated Radiology report generation (RRG) aims at producing detailed descriptions of medical images, reducing radiologists' workload and improving access to high-quality diagnostic services. Existing encoder-decoder models only rely on visual features extracted from raw input images, which can limit the understanding of spatial structures and semantic relationships, often resulting in suboptimal text generation. To address this, we propose Anatomical Attention Alignment Network (A3Net), a framework that enhance visual-textual understanding by constructing hyper-visual representations. Our approach integrates a knowledge dictionary of anatomical structures with patch-level visual features, enabling the model to effectively associate image regions with their corresponding anatomical entities. This structured representation improves semantic reasoning, interpretability, and cross-modal alignment, ultimately enhancing the accuracy and clinical relevance of generated reports. Experimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net significantly improves both visual perception and text generation quality. Our code is available at \href{https://github.com/Vinh-AI/A3Net}{GitHub}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond CLIP Generalization: Against Forward&amp;Backward Forgetting Adapter for Continual Learning of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07690</link>
<guid>https://arxiv.org/abs/2505.07690</guid>
<content:encoded><![CDATA[
arXiv:2505.07690v1 Announce Type: new 
Abstract: This study aims to address the problem of multi-domain task incremental learning~(MTIL), which requires that vision-language models~(VLMs) continuously acquire new knowledge while maintaining their inherent zero-shot recognition capability. Existing paradigms delegate the testing of unseen-domain samples to the original CLIP, which only prevents the degradation of the model's zero-shot capability but fails to enhance the generalization of the VLM further. To this end, we propose a novel MTIL framework, named AFA, which comprises two core modules: (1) an against forward-forgetting adapter that learns task-invariant information for each dataset in the incremental tasks to enhance the zero-shot recognition ability of VLMs; (2) an against backward-forgetting adapter that strengthens the few-shot learning capability of VLMs while supporting incremental learning. Extensive experiments demonstrate that the AFA method significantly outperforms existing state-of-the-art approaches, especially in few-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP in terms of transferability. The code is provided in the Supplementary Material.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Driven Pseudo-Label Reliability Assessment: Redefining Thresholding for Semi-Supervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.07691</link>
<guid>https://arxiv.org/abs/2505.07691</guid>
<content:encoded><![CDATA[
arXiv:2505.07691v1 Announce Type: new 
Abstract: Semi-supervised learning leverages unlabeled data to enhance model performance, addressing the limitations of fully supervised approaches. Among its strategies, pseudo-supervision has proven highly effective, typically relying on one or multiple teacher networks to refine pseudo-labels before training a student network. A common practice in pseudo-supervision is filtering pseudo-labels based on pre-defined confidence thresholds or entropy. However, selecting optimal thresholds requires large labeled datasets, which are often scarce in real-world semi-supervised scenarios. To overcome this challenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic feedback-driven thresholding strategy for pseudo-label selection. Instead of relying on static confidence thresholds, ENCORE estimates class-wise true-positive confidence within the unlabeled dataset and continuously adjusts thresholds based on the model's response to different levels of pseudo-label filtering. This feedback-driven mechanism ensures the retention of informative pseudo-labels while filtering unreliable ones, enhancing model training without manual threshold tuning. Our method seamlessly integrates into existing pseudo-supervision frameworks and significantly improves segmentation performance, particularly in data-scarce conditions. Extensive experiments demonstrate that integrating ENCORE with existing pseudo-supervision frameworks enhances performance across multiple datasets and network architectures, validating its effectiveness in semi-supervised learning.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images</title>
<link>https://arxiv.org/abs/2505.07704</link>
<guid>https://arxiv.org/abs/2505.07704</guid>
<content:encoded><![CDATA[
arXiv:2505.07704v1 Announce Type: new 
Abstract: Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Spiking Vision Transformer for Object Detection with Event Cameras</title>
<link>https://arxiv.org/abs/2505.07715</link>
<guid>https://arxiv.org/abs/2505.07715</guid>
<content:encoded><![CDATA[
arXiv:2505.07715v1 Announce Type: new 
Abstract: Event-based object detection has gained increasing attention due to its advantages such as high temporal resolution, wide dynamic range, and asynchronous address-event representation. Leveraging these advantages, Spiking Neural Networks (SNNs) have emerged as a promising approach, offering low energy consumption and rich spatiotemporal dynamics. To further enhance the performance of event-based object detection, this study proposes a novel hybrid spike vision Transformer (HsVT) model. The HsVT model integrates a spatial feature extraction module to capture local and global features, and a temporal feature extraction module to model time dependencies and long-term patterns in event sequences. This combination enables HsVT to capture spatiotemporal features, improving its capability to handle complex event-based object detection tasks. To support research in this area, we developed and publicly released The Fall Detection Dataset as a benchmark for event-based object detection tasks. This dataset, captured using an event-based camera, ensures facial privacy protection and reduces memory usage due to the event representation format. We evaluated the HsVT model on GEN1 and Fall Detection datasets across various model sizes. Experimental results demonstrate that HsVT achieves significant performance improvements in event detection with fewer parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gameplay Highlights Generation</title>
<link>https://arxiv.org/abs/2505.07721</link>
<guid>https://arxiv.org/abs/2505.07721</guid>
<content:encoded><![CDATA[
arXiv:2505.07721v1 Announce Type: new 
Abstract: In this work, we enable gamers to share their gaming experience on social media by automatically generating eye-catching highlight reels from their gameplay session Our automation will save time for gamers while increasing audience engagement. We approach the highlight generation problem by first identifying intervals in the video where interesting events occur and then concatenate them. We developed an in-house gameplay event detection dataset containing interesting events annotated by humans using VIA video annotator. Traditional techniques for highlight detection such as game engine integration requires expensive collaboration with game developers. OCR techniques which detect patches of specific images or texts require expensive per game engineering and may not generalize across game UI and different language. We finetuned a multimodal general purpose video understanding model such as X-CLIP using our dataset which generalizes across multiple games in a genre without per game engineering. Prompt engineering was performed to improve the classification performance of this multimodal model. Our evaluation showed that such a finetuned model can detect interesting events in first person shooting games from unseen gameplay footage with more than 90% accuracy. Moreover, our model performed significantly better on low resource games (small dataset) when trained along with high resource games, showing signs of transfer learning. To make the model production ready, we used ONNX libraries to enable cross platform inference. These libraries also provide post training quantization tools to reduce model size and inference time for deployment. ONNX runtime libraries with DirectML backend were used to perform efficient inference on Windows OS. We show that natural language supervision in the X-CLIP model leads to data efficient and highly performant video recognition models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention</title>
<link>https://arxiv.org/abs/2505.07734</link>
<guid>https://arxiv.org/abs/2505.07734</guid>
<content:encoded><![CDATA[
arXiv:2505.07734v1 Announce Type: new 
Abstract: Detecting AI-synthetic faces presents a critical challenge: it is hard to capture consistent structural relationships between facial regions across diverse generation techniques. Current methods, which focus on specific artifacts rather than fundamental inconsistencies, often fail when confronted with novel generative models. To address this limitation, we introduce Layer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer designed for robust facial forgery detection. This model integrates distinct Region-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation (LAMM) components within each layer. RG-MHA utilizes facial landmarks to create regional attention masks, guiding the model to scrutinize architectural inconsistencies across different facial areas. Crucially, the separate LAMM module dynamically generates layer-specific parameters, including mask weights and gating values, based on network context. These parameters then modulate the behavior of RG-MHA, enabling adaptive adjustment of regional focus across network depths. This architecture facilitates the capture of subtle, hierarchical forgery cues ubiquitous among diverse generation techniques, such as GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT demonstrates superior performance, achieving 94.09% mean ACC (a +5.45% improvement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results demonstrate LAMM-ViT's exceptional ability to generalize and its potential for reliable deployment against evolving synthetic media threats.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BodyGPS: Anatomical Positioning System</title>
<link>https://arxiv.org/abs/2505.07744</link>
<guid>https://arxiv.org/abs/2505.07744</guid>
<content:encoded><![CDATA[
arXiv:2505.07744v1 Announce Type: new 
Abstract: We introduce a new type of foundational model for parsing human anatomy in medical images that works for different modalities. It supports supervised or unsupervised training and can perform matching, registration, classification, or segmentation with or without user interaction. We achieve this by training a neural network estimator that maps query locations to atlas coordinates via regression. Efficiency is improved by sparsely sampling the input, enabling response times of less than 1 ms without additional accelerator hardware. We demonstrate the utility of the algorithm in both CT and MRI modalities.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets</title>
<link>https://arxiv.org/abs/2505.07747</link>
<guid>https://arxiv.org/abs/2505.07747</guid>
<content:encoded><![CDATA[
arXiv:2505.07747v1 Announce Type: new 
Abstract: While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Visual Autoregressive Generation via Score Maximization</title>
<link>https://arxiv.org/abs/2505.07812</link>
<guid>https://arxiv.org/abs/2505.07812</guid>
<content:encoded><![CDATA[
arXiv:2505.07812v1 Announce Type: new 
Abstract: Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DanceGRPO: Unleashing GRPO on Visual Generation</title>
<link>https://arxiv.org/abs/2505.07818</link>
<guid>https://arxiv.org/abs/2505.07818</guid>
<content:encoded><![CDATA[
arXiv:2505.07818v1 Announce Type: new 
Abstract: Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeltaDPD: Exploiting Dynamic Temporal Sparsity in Recurrent Neural Networks for Energy-Efficient Wideband Digital Predistortion</title>
<link>https://arxiv.org/abs/2505.06250</link>
<guid>https://arxiv.org/abs/2505.06250</guid>
<content:encoded><![CDATA[
arXiv:2505.06250v1 Announce Type: cross 
Abstract: Digital Predistortion (DPD) is a popular technique to enhance signal quality in wideband RF power amplifiers (PAs). With increasing bandwidth and data rates, DPD faces significant energy consumption challenges during deployment, contrasting with its efficiency goals. State-of-the-art DPD models rely on recurrent neural networks (RNN), whose computational complexity hinders system efficiency. This paper introduces DeltaDPD, exploring the dynamic temporal sparsity of input signals and neuronal hidden states in RNNs for energy-efficient DPD, reducing arithmetic operations and memory accesses while preserving satisfactory linearization performance. Applying a TM3.1a 200MHz-BW 256-QAM OFDM signal to a 3.5 GHz GaN Doherty RF PA, DeltaDPD achieves -50.03 dBc in Adjacent Channel Power Ratio (ACPR), -37.22 dB in Normalized Mean Square Error (NMSE) and -38.52 dBc in Error Vector Magnitude (EVM) with 52% temporal sparsity, leading to a 1.8X reduction in estimated inference power. The DeltaDPD code will be released after formal publication at https://www.opendpd.com.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attonsecond Streaking Phase Retrieval Via Deep Learning Methods</title>
<link>https://arxiv.org/abs/2505.06275</link>
<guid>https://arxiv.org/abs/2505.06275</guid>
<content:encoded><![CDATA[
arXiv:2505.06275v1 Announce Type: cross 
Abstract: Attosecond streaking phase retrieval is essential for resolving electron dynamics on sub-femtosecond time scales yet traditional algorithms rely on iterative minimization and central momentum approximations that degrade accuracy for broadband pulses. In this work phase retrieval is reformulated as a supervised computer-vision problem and four neural architectures are systematically compared. A convolutional network demonstrates strong sensitivity to local streak edges but lacks global context; a vision transformer captures long-range delay-energy correlations at the expense of local inductive bias; a hybrid CNN-ViT model unites local feature extraction and full-graph attention; and a capsule network further enforces spatial pose agreement through dynamic routing. A theoretical analysis introduces local, global and positional sensitivity measures and derives surrogate error bounds that predict the strict ordering $CNN<Capsule$. Controlled experiments on synthetic streaking spectrograms confirm this hierarchy, with the capsule network achieving the highest retrieval fidelity. Looking forward, embedding the strong-field integral into physics-informed neural networks and exploring photonic hardware implementations promise pathways toward real-time attosecond pulse characterization under demanding experimental conditions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Terahertz Spatial Wireless Channel Modeling with Radio Radiance Field</title>
<link>https://arxiv.org/abs/2505.06277</link>
<guid>https://arxiv.org/abs/2505.06277</guid>
<content:encoded><![CDATA[
arXiv:2505.06277v1 Announce Type: cross 
Abstract: Terahertz (THz) communication is a key enabler for 6G systems, offering ultra-wide bandwidth and unprecedented data rates. However, THz signal propagation differs significantly from lower-frequency bands due to severe free space path loss, minimal diffraction and specular reflection, and prominent scattering, making conventional channel modeling and pilot-based estimation approaches inefficient. In this work, we investigate the feasibility of applying radio radiance field (RRF) framework to the THz band. This method reconstructs a continuous RRF using visual-based geometry and sparse THz RF measurements, enabling efficient spatial channel state information (Spatial-CSI) modeling without dense sampling. We first build a fine simulated THz scenario, then we reconstruct the RRF and evaluate the performance in terms of both reconstruction quality and effectiveness in THz communication, showing that the reconstructed RRF captures key propagation paths with sparse training samples. Our findings demonstrate that RRF modeling remains effective in the THz regime and provides a promising direction for scalable, low-cost spatial channel reconstruction in future 6G networks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FEMSN: Frequency-Enhanced Multiscale Network for fault diagnosis of rotating machinery under strong noise environments</title>
<link>https://arxiv.org/abs/2505.06285</link>
<guid>https://arxiv.org/abs/2505.06285</guid>
<content:encoded><![CDATA[
arXiv:2505.06285v1 Announce Type: cross 
Abstract: Rolling bearings are critical components of rotating machinery, and their proper functioning is essential for industrial production. Most existing condition monitoring methods focus on extracting discriminative features from time-domain signals to assess bearing health status. However, under complex operating conditions, periodic impulsive characteristics related to fault information are often obscured by noise interference. Consequently, existing approaches struggle to learn distinctive fault-related features in such scenarios. To address this issue, this paper proposes a novel CNN-based model named FEMSN. Specifically, a Fourier Adaptive Denoising Encoder Layer (FADEL) is introduced as an input denoising layer to enhance key features while filtering out irrelevant information. Subsequently, a Multiscale Time-Frequency Fusion (MSTFF) module is employed to extract fused time-frequency features, further improving the model robustness and nonlinear representation capability. Additionally, a distillation layer is incorporated to expand the receptive field. Based on these advancements, a novel deep lightweight CNN model, termed the Frequency-Enhanced Multiscale Network (FEMSN), is developed. The effectiveness of FEMSN and FADEL in machine health monitoring and stability assessment is validated through two case studies.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompSLAM: Complementary Hierarchical Multi-Modal Localization and Mapping for Robot Autonomy in Underground Environments</title>
<link>https://arxiv.org/abs/2505.06483</link>
<guid>https://arxiv.org/abs/2505.06483</guid>
<content:encoded><![CDATA[
arXiv:2505.06483v1 Announce Type: cross 
Abstract: Robot autonomy in unknown, GPS-denied, and complex underground environments requires real-time, robust, and accurate onboard pose estimation and mapping for reliable operations. This becomes particularly challenging in perception-degraded subterranean conditions under harsh environmental factors, including darkness, dust, and geometrically self-similar structures. This paper details CompSLAM, a highly resilient and hierarchical multi-modal localization and mapping framework designed to address these challenges. Its flexible architecture achieves resilience through redundancy by leveraging the complementary nature of pose estimates derived from diverse sensor modalities. Developed during the DARPA Subterranean Challenge, CompSLAM was successfully deployed on all aerial, legged, and wheeled robots of Team Cerberus during their competition-winning final run. Furthermore, it has proven to be a reliable odometry and mapping solution in various subsequent projects, with extensions enabling multi-robot map sharing for marsupial robotic deployments and collaborative mapping. This paper also introduces a comprehensive dataset acquired by a manually teleoperated quadrupedal robot, covering a significant portion of the DARPA Subterranean Challenge finals course. This dataset evaluates CompSLAM's robustness to sensor degradations as the robot traverses 740 meters in an environment characterized by highly variable geometries and demanding lighting conditions. The CompSLAM code and the DARPA SubT Finals dataset are made publicly available for the benefit of the robotics community
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations</title>
<link>https://arxiv.org/abs/2505.06502</link>
<guid>https://arxiv.org/abs/2505.06502</guid>
<content:encoded><![CDATA[
arXiv:2505.06502v1 Announce Type: cross 
Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional methods, even with limited training data (e.g., only 13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning, offering improved accuracy and efficiency for image processing, enhanced process understanding, and broader applications to scientific research. The source codes and data will be made publicly available at https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities</title>
<link>https://arxiv.org/abs/2505.06507</link>
<guid>https://arxiv.org/abs/2505.06507</guid>
<content:encoded><![CDATA[
arXiv:2505.06507v1 Announce Type: cross 
Abstract: Computer-aided design (CAD) is fundamental to modern engineering and manufacturing, but creating CAD models still requires expert knowledge and specialized software. Recent advances in large language models (LLMs) open up the possibility of generative CAD, where natural language is directly translated into parametric 3D models. However, most existing methods generate task-specific command sequences that pretrained models cannot directly handle. These sequences must be converted into CAD representations such as CAD vectors before a 3D model can be produced, which requires training models from scratch and adds unnecessary complexity. To tackle this issue, we propose generating CadQuery code directly from text, leveraging the strengths of pretrained LLMs to produce 3D models without intermediate representations, using this Python-based scripting language. Since LLMs already excel at Python generation and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly effective. Given that these capabilities typically improve with scale, we hypothesize that larger models will perform better after fine-tuning. To enable this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We fine-tune six open-source LLMs of varying sizes and observe consistent improvements. Our best model achieves a top-1 exact match of 69.3%, up from 58.8%, and reduces Chamfer Distance by 48.6%. Project page: https://github.com/Text-to-CadQuery/Text-to-CadQuery.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation</title>
<link>https://arxiv.org/abs/2505.06594</link>
<guid>https://arxiv.org/abs/2505.06594</guid>
<content:encoded><![CDATA[
arXiv:2505.06594v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) often struggle to balance visual and textual information when summarizing complex multimodal inputs, such as entire TV show episodes. In this paper, we propose a zero-shot video-to-text summarization approach that builds its own screenplay representation of an episode, effectively integrating key video moments, dialogue, and character information into a unified document. Unlike previous approaches, we simultaneously generate screenplays and name the characters in zero-shot, using only the audio, video, and transcripts as input. Additionally, we highlight that existing summarization metrics can fail to assess the multimodal content in summaries. To address this, we introduce MFactSum, a multimodal metric that evaluates summaries with respect to both vision and text modalities. Using MFactSum, we evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating superiority against state-of-the-art VLMs such as Gemini 1.5 by generating summaries containing 20% more relevant visual information while requiring 75% less of the video as input.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Representation Transferring to Lightweight Models via Perception Coherence</title>
<link>https://arxiv.org/abs/2505.06595</link>
<guid>https://arxiv.org/abs/2505.06595</guid>
<content:encoded><![CDATA[
arXiv:2505.06595v1 Announce Type: cross 
Abstract: In this paper, we propose a method for transferring feature representation to lightweight student models from larger teacher models. We mathematically define a new notion called \textit{perception coherence}. Based on this notion, we propose a loss function, which takes into account the dissimilarities between data points in feature space through their ranking. At a high level, by minimizing this loss function, the student model learns to mimic how the teacher model \textit{perceives} inputs. More precisely, our method is motivated by the fact that the representational capacity of the student model is weaker than the teacher model. Hence, we aim to develop a new method allowing for a better relaxation. This means that, the student model does not need to preserve the absolute geometry of the teacher one, while preserving global coherence through dissimilarity ranking. Our theoretical insights provide a probabilistic perspective on the process of feature representation transfer. Our experiments results show that our method outperforms or achieves on-par performance compared to strong baseline methods for representation transferring.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models</title>
<link>https://arxiv.org/abs/2505.06621</link>
<guid>https://arxiv.org/abs/2505.06621</guid>
<content:encoded><![CDATA[
arXiv:2505.06621v1 Announce Type: cross 
Abstract: The distribution of child sexual abuse imagery (CSAI) is an ever-growing concern of our modern world; children who suffered from this heinous crime are revictimized, and the growing amount of illegal imagery distributed overwhelms law enforcement agents (LEAs) with the manual labor of categorization. To ease this burden researchers have explored methods for automating data triage and detection of CSAI, but the sensitive nature of the data imposes restricted access and minimal interaction between real data and learning algorithms, avoiding leaks at all costs. In observing how these restrictions have shaped the literature we formalize a definition of "Proxy Tasks", i.e., the substitute tasks used for training models for CSAI without making use of CSA data. Under this new terminology we review current literature and present a protocol for making conscious use of Proxy Tasks together with consistent input from LEAs to design better automation in this field. Finally, we apply this protocol to study -- for the first time -- the task of Few-shot Indoor Scene Classification on CSAI, showing a final model that achieves promising results on a real-world CSAI dataset whilst having no weights actually trained on sensitive data.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification</title>
<link>https://arxiv.org/abs/2505.06646</link>
<guid>https://arxiv.org/abs/2505.06646</guid>
<content:encoded><![CDATA[
arXiv:2505.06646v1 Announce Type: cross 
Abstract: Deep learning for radiologic image analysis is a rapidly growing field in biomedical research and is likely to become a standard practice in modern medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray images that are classified by the presence or absence of 14 different diseases, we reproduced an algorithm known as CheXNet, as well as explored other algorithms that outperform CheXNet's baseline metrics. Model performance was primarily evaluated using the F1 score and AUC-ROC, both of which are critical metrics for imbalanced, multi-label classification tasks in medical imaging. The best model achieved an average AUC-ROC score of 0.85 and an average F1 score of 0.39 across all 14 disease classifications present in the dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding</title>
<link>https://arxiv.org/abs/2505.06685</link>
<guid>https://arxiv.org/abs/2505.06685</guid>
<content:encoded><![CDATA[
arXiv:2505.06685v1 Announce Type: cross 
Abstract: Emotion understanding in videos aims to accurately recognize and interpret individuals' emotional states by integrating contextual, visual, textual, and auditory cues. While Large Multimodal Models (LMMs) have demonstrated significant progress in general vision-language (VL) tasks, their performance in emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on emotion-related tasks often leads to catastrophic forgetting, hindering their ability to generalize across diverse tasks. To address these challenges, we present Emotion-Qwen, a tailored multimodal framework designed to enhance both emotion understanding and general VL reasoning. Emotion-Qwen incorporates a sophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm, which dynamically routes inputs to balance emotion-specific and general-purpose processing. The model is pre-trained in a three-stage pipeline on large-scale general and emotional image datasets to support robust multimodal representations. Furthermore, we construct the Video Emotion Reasoning (VER) dataset, comprising more than 40K bilingual video clips with fine-grained descriptive annotations, to further enrich Emotion-Qwen's emotional reasoning capability. Experimental results demonstrate that Emotion-Qwen achieves state-of-the-art performance on multiple emotion recognition benchmarks, while maintaining competitive results on general VL tasks. Code and models are available at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark</title>
<link>https://arxiv.org/abs/2505.06746</link>
<guid>https://arxiv.org/abs/2505.06746</guid>
<content:encoded><![CDATA[
arXiv:2505.06746v1 Announce Type: cross 
Abstract: We introduce M$^3$CAD, a novel benchmark designed to advance research in generic cooperative autonomous driving. M$^3$CAD comprises 204 sequences with 30k frames, spanning a diverse range of cooperative driving scenarios. Each sequence includes multiple vehicles and sensing modalities, e.g., LiDAR point clouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving tasks, including object detection and tracking, mapping, motion forecasting, occupancy prediction, and path planning. This rich multimodal setup enables M$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving research, significantly broadening the scope of research in the field. To our knowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored for cooperative multi-task autonomous driving research. We evaluate the state-of-the-art end-to-end solution on M$^3$CAD to establish baseline performance. To foster cooperative autonomous driving research, we also propose E2EC, a simple yet effective framework for cooperative driving solution that leverages inter-vehicle shared information for improved path planning. We release M$^3$CAD, along with our baseline models and evaluation results, to support the development of robust cooperative autonomous driving systems. All resources will be made publicly available on https://github.com/zhumorui/M3CAD
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HistDiST: Histopathological Diffusion-based Stain Transfer</title>
<link>https://arxiv.org/abs/2505.06793</link>
<guid>https://arxiv.org/abs/2505.06793</guid>
<content:encoded><![CDATA[
arXiv:2505.06793v1 Announce Type: cross 
Abstract: Hematoxylin and Eosin (H&amp;E) staining is the cornerstone of histopathology but lacks molecular specificity. While Immunohistochemistry (IHC) provides molecular insights, it is costly and complex, motivating H&amp;E-to-IHC translation as a cost-effective alternative. Existing translation methods are mainly GAN-based, often struggling with training instability and limited structural fidelity, while diffusion-based approaches remain underexplored. We propose HistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity H&amp;E-to-IHC translation. HistDiST introduces a dual-conditioning strategy, utilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&amp;E representations to ensure pathology-relevant context and structural consistency. To overcome brightness biases, we incorporate a rescaled noise schedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition at the final timestep. During inference, DDIM inversion preserves the morphological structure, while an eta-cosine noise schedule introduces controlled stochasticity, balancing structural consistency and molecular fidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel pathology-aware metric leveraging GigaPath embeddings to assess molecular relevance. Extensive evaluations on MIST and BCI datasets demonstrate that HistDiST significantly outperforms existing methods, achieving a 28% improvement in MRA on the H&amp;E-to-Ki67 translation task, highlighting its effectiveness in capturing true IHC semantics.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation</title>
<link>https://arxiv.org/abs/2505.06803</link>
<guid>https://arxiv.org/abs/2505.06803</guid>
<content:encoded><![CDATA[
arXiv:2505.06803v1 Announce Type: cross 
Abstract: Audio large language models (LLMs) are considered experts at recognizing sound objects, yet their performance relative to LLMs in other sensory modalities, such as visual or audio-visual LLMs, and to humans using their ears, eyes, or both remains unexplored. To investigate this, we systematically evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio, Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of different classes from audio-only, silent video, or sounded video inputs. We uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the sensory discrepancy between human ears and eyes. To reduce this gap, we introduce a cross-modal distillation framework, where an LLM in one modality serves as the teacher and another as the student, with knowledge transfer in sound classes predicted as more challenging to the student by a heuristic model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice versa, leads to notable improvements, particularly in challenging classes. This work highlights the sensory gap in LLMs from a human-aligned perspective and proposes a principled approach to enhancing modality-specific perception in multimodal LLMs.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing Data Estimation for MR Spectroscopic Imaging via Mask-Free Deep Learning Methods</title>
<link>https://arxiv.org/abs/2505.06811</link>
<guid>https://arxiv.org/abs/2505.06811</guid>
<content:encoded><![CDATA[
arXiv:2505.06811v1 Announce Type: cross 
Abstract: Magnetic Resonance Spectroscopic Imaging (MRSI) is a powerful tool for non-invasive mapping of brain metabolites, providing critical insights into neurological conditions. However, its utility is often limited by missing or corrupted data due to motion artifacts, magnetic field inhomogeneities, or failed spectral fitting-especially in high resolution 3D acquisitions. To address this, we propose the first deep learning-based, mask-free framework for estimating missing data in MRSI metabolic maps. Unlike conventional restoration methods that rely on explicit masks to identify missing regions, our approach implicitly detects and estimates these areas using contextual spatial features through 2D and 3D U-Net architectures. We also introduce a progressive training strategy to enhance robustness under varying levels of data degradation. Our method is evaluated on both simulated and real patient datasets and consistently outperforms traditional interpolation techniques such as cubic and linear interpolation. The 2D model achieves an MSE of 0.002 and an SSIM of 0.97 with 20% missing voxels, while the 3D model reaches an MSE of 0.001 and an SSIM of 0.98 with 15% missing voxels. Qualitative results show improved fidelity in estimating missing data, particularly in metabolically heterogeneous regions and ventricular regions. Importantly, our model generalizes well to real-world datasets without requiring retraining or mask input. These findings demonstrate the effectiveness and broad applicability of mask-free deep learning for MRSI restoration, with strong potential for clinical and research integration.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Robotic Policy Learning via Latent Space Backward Planning</title>
<link>https://arxiv.org/abs/2505.06861</link>
<guid>https://arxiv.org/abs/2505.06861</guid>
<content:encoded><![CDATA[
arXiv:2505.06861v1 Announce Type: cross 
Abstract: Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can still result in off-task predictions due to accumulation errors, leading to misalignment with long-term goals. This raises a critical question: Can robotic planning be both efficient and accurate enough for real-time control in long-horizon, multi-stage tasks? To address this, we propose a Latent Space Backward Planning scheme (LBP), which begins by grounding the task into final latent goals, followed by recursively predicting intermediate subgoals closer to the current state. The grounded final goal enables backward subgoal planning to always remain aware of task completion, facilitating on-task prediction along the entire planning horizon. The subgoal-conditioned policy incorporates a learnable token to summarize the subgoal sequences and determines how each subgoal guides action extraction. Through extensive simulation and real-robot long-horizon experiments, we show that LBP outperforms existing fine-grained and forward planning methods, achieving SOTA performance. Project Page: https://lbp-authors.github.io
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Classification Using a Diffusion Model as a Pre-Training Model</title>
<link>https://arxiv.org/abs/2505.06890</link>
<guid>https://arxiv.org/abs/2505.06890</guid>
<content:encoded><![CDATA[
arXiv:2505.06890v1 Announce Type: cross 
Abstract: In this paper, we propose a diffusion model that integrates a representation-conditioning mechanism, where the representations derived from a Vision Transformer (ViT) are used to condition the internal process of a Transformer-based diffusion model. This approach enables representation-conditioned data generation, addressing the challenge of requiring large-scale labeled datasets by leveraging self-supervised learning on unlabeled data. We evaluate our method through a zero-shot classification task for hematoma detection in brain imaging. Compared to the strong contrastive learning baseline, DINOv2, our method achieves a notable improvement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its effectiveness in image classification.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence</title>
<link>https://arxiv.org/abs/2505.06907</link>
<guid>https://arxiv.org/abs/2505.06907</guid>
<content:encoded><![CDATA[
arXiv:2505.06907v1 Announce Type: cross 
Abstract: The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, has reshaped the artificial intelligence landscape. As prominent examples of foundational models (FMs) built on LLMs, these models exhibit remarkable capabilities in generating human-like content, bringing us closer to achieving artificial general intelligence (AGI). However, their large-scale nature, sensitivity to privacy concerns, and substantial computational demands present significant challenges to personalized customization for end users. To bridge this gap, this paper presents the vision of artificial personalized intelligence (API), focusing on adapting these powerful models to meet the specific needs and preferences of users while maintaining privacy and efficiency. Specifically, this paper proposes personalized federated intelligence (PFI), which integrates the privacy-preserving advantages of federated learning (FL) with the zero-shot generalization capabilities of FMs, enabling personalized, efficient, and privacy-protective deployment at the edge. We first review recent advances in both FL and FMs, and discuss the potential of leveraging FMs to enhance federated systems. We then present the key motivations behind realizing PFI and explore promising opportunities in this space, including efficient PFI, trustworthy PFI, and PFI empowered by retrieval-augmented generation (RAG). Finally, we outline key challenges and future research directions for deploying FM-powered FL systems at the edge with improved personalization, computational efficiency, and privacy guarantees. Overall, this survey aims to lay the groundwork for the development of API as a complement to AGI, with a particular focus on PFI as a key enabling technique.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-AIMS: AI-Powered Microscopy Image Analysis</title>
<link>https://arxiv.org/abs/2505.06918</link>
<guid>https://arxiv.org/abs/2505.06918</guid>
<content:encoded><![CDATA[
arXiv:2505.06918v1 Announce Type: cross 
Abstract: This paper presents a systematic solution for the intelligent recognition and automatic analysis of microscopy images. We developed a data engine that generates high-quality annotated datasets through a combination of the collection of diverse microscopy images from experiments, synthetic data generation and a human-in-the-loop annotation process. To address the unique challenges of microscopy images, we propose a segmentation model capable of robustly detecting both small and large objects. The model effectively identifies and separates thousands of closely situated targets, even in cluttered visual environments. Furthermore, our solution supports the precise automatic recognition of image scale bars, an essential feature in quantitative microscopic analysis. Building upon these components, we have constructed a comprehensive intelligent analysis platform and validated its effectiveness and practicality in real-world applications. This study not only advances automatic recognition in microscopy imaging but also ensures scalability and generalizability across multiple application domains, offering a powerful tool for automated microscopic analysis in interdisciplinary research.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whitened CLIP as a Likelihood Surrogate of Images and Captions</title>
<link>https://arxiv.org/abs/2505.06934</link>
<guid>https://arxiv.org/abs/2505.06934</guid>
<content:encoded><![CDATA[
arXiv:2505.06934v1 Announce Type: cross 
Abstract: Likelihood approximations for images are not trivial to compute and can be useful in many applications. We examine the use of Contrastive Language-Image Pre-training (CLIP) to assess the likelihood of images and captions. We introduce \textit{Whitened CLIP}, a novel transformation of the CLIP latent space via an invertible linear operation. This transformation ensures that each feature in the embedding space has zero mean, unit standard deviation, and no correlation with all other features, resulting in an identity covariance matrix. We show that the whitened embeddings statistics can be well approximated as a standard normal distribution, thus, the log-likelihood is estimated simply by the square Euclidean norm in the whitened embedding space. The whitening procedure is completely training-free and performed using a pre-computed whitening matrix, hence, is very fast. We present several preliminary experiments demonstrating the properties and applicability of these likelihood scores to images and captions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing</title>
<link>https://arxiv.org/abs/2505.06963</link>
<guid>https://arxiv.org/abs/2505.06963</guid>
<content:encoded><![CDATA[
arXiv:2505.06963v1 Announce Type: cross 
Abstract: This paper introduces an innovative approach for the autonomous landing of Unmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera, therefore obviating the requirement for depth estimation cameras. Drawing on the inherent human estimating process, the proposed method reframes the landing task as an optimization problem. The UAV employs variations in the visual characteristics of a specially designed lenticular circle on the landing pad, where the perceived color and form provide critical information for estimating both altitude and depth. Reinforcement learning algorithms are utilized to approximate the functions governing these estimations, enabling the UAV to ascertain ideal landing settings via training. This method's efficacy is assessed by simulations and experiments, showcasing its potential for robust and accurate autonomous landing without dependence on complex sensor setups. This research contributes to the advancement of cost-effective and efficient UAV landing solutions, paving the way for wider applicability across various fields.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VALISENS: A Validated Innovative Multi-Sensor System for Cooperative Automated Driving</title>
<link>https://arxiv.org/abs/2505.06980</link>
<guid>https://arxiv.org/abs/2505.06980</guid>
<content:encoded><![CDATA[
arXiv:2505.06980v1 Announce Type: cross 
Abstract: Perception is a core capability of automated vehicles and has been significantly advanced through modern sensor technologies and artificial intelligence. However, perception systems still face challenges in complex real-world scenarios. To improve robustness against various external factors, multi-sensor fusion techniques are essential, combining the strengths of different sensor modalities. With recent developments in Vehicle-to-Everything (V2X communication, sensor fusion can now extend beyond a single vehicle to a cooperative multi-agent system involving Connected Automated Vehicle (CAV) and intelligent infrastructure. This paper presents VALISENS, an innovative multi-sensor system distributed across multiple agents. It integrates onboard and roadside LiDARs, radars, thermal cameras, and RGB cameras to enhance situational awareness and support cooperative automated driving. The thermal camera adds critical redundancy for perceiving Vulnerable Road User (VRU), while fusion with roadside sensors mitigates visual occlusions and extends the perception range beyond the limits of individual vehicles. We introduce the corresponding perception module built on this sensor system, which includes object detection, tracking, motion forecasting, and high-level data fusion. The proposed system demonstrates the potential of cooperative perception in real-world test environments and lays the groundwork for future Cooperative Intelligent Transport Systems (C-ITS) applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Three-Phase Dynamics of Generalization Power of a DNN</title>
<link>https://arxiv.org/abs/2505.06993</link>
<guid>https://arxiv.org/abs/2505.06993</guid>
<content:encoded><![CDATA[
arXiv:2505.06993v1 Announce Type: cross 
Abstract: This paper proposes a new perspective for analyzing the generalization power of deep neural networks (DNNs), i.e., directly disentangling and analyzing the dynamics of generalizable and non-generalizable interaction encoded by a DNN through the training process. Specifically, this work builds upon the recent theoretical achievement in explainble AI, which proves that the detailed inference logic of DNNs can be can be strictly rewritten as a small number of AND-OR interaction patterns. Based on this, we propose an efficient method to quantify the generalization power of each interaction, and we discover a distinct three-phase dynamics of the generalization power of interactions during training. In particular, the early phase of training typically removes noisy and non-generalizable interactions and learns simple and generalizable ones. The second and the third phases tend to capture increasingly complex interactions that are harder to generalize. Experimental results verify that the learning of non-generalizable interactions is the the direct cause for the gap between the training and testing losses.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy of Groups in Dense Street Imagery</title>
<link>https://arxiv.org/abs/2505.07085</link>
<guid>https://arxiv.org/abs/2505.07085</guid>
<content:encoded><![CDATA[
arXiv:2505.07085v1 Announce Type: cross 
Abstract: Spatially and temporally dense street imagery (DSI) datasets have grown unbounded. In 2024, individual companies possessed around 3 trillion unique images of public streets. DSI data streams are only set to grow as companies like Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze collisions. Academic researchers leverage DSI to explore novel approaches to urban analysis. Despite good-faith efforts by DSI providers to protect individual privacy through blurring faces and license plates, these measures fail to address broader privacy concerns. In this work, we find that increased data density and advancements in artificial intelligence enable harmful group membership inferences from supposedly anonymized data. We perform a penetration test to demonstrate how easily sensitive group affiliations can be inferred from obfuscated pedestrians in 25,232,608 dashcam images taken in New York City. We develop a typology of identifiable groups within DSI and analyze privacy implications through the lens of contextual integrity. Finally, we discuss actionable recommendations for researchers working with data from DSI providers.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems</title>
<link>https://arxiv.org/abs/2505.07110</link>
<guid>https://arxiv.org/abs/2505.07110</guid>
<content:encoded><![CDATA[
arXiv:2505.07110v1 Announce Type: cross 
Abstract: Based on the DeepSORT algorithm, this study explores the application of visual tracking technology in intelligent human-computer interaction, especially in the field of gesture recognition and tracking. With the rapid development of artificial intelligence and deep learning technology, visual-based interaction has gradually replaced traditional input devices and become an important way for intelligent systems to interact with users. The DeepSORT algorithm can achieve accurate target tracking in dynamic environments by combining Kalman filters and deep learning feature extraction methods. It is especially suitable for complex scenes with multi-target tracking and fast movements. This study experimentally verifies the superior performance of DeepSORT in gesture recognition and tracking. It can accurately capture and track the user's gesture trajectory and is superior to traditional tracking methods in terms of real-time and accuracy. In addition, this study also combines gesture recognition experiments to evaluate the recognition ability and feedback response of the DeepSORT algorithm under different gestures (such as sliding, clicking, and zooming). The experimental results show that DeepSORT can not only effectively deal with target occlusion and motion blur but also can stably track in a multi-target environment, achieving a smooth user interaction experience. Finally, this paper looks forward to the future development direction of intelligent human-computer interaction systems based on visual tracking and proposes future research focuses such as algorithm optimization, data fusion, and multimodal interaction in order to promote a more intelligent and personalized interactive experience. Keywords-DeepSORT, visual tracking, gesture recognition, human-computer interaction
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skull stripping with purely synthetic data</title>
<link>https://arxiv.org/abs/2505.07159</link>
<guid>https://arxiv.org/abs/2505.07159</guid>
<content:encoded><![CDATA[
arXiv:2505.07159v1 Announce Type: cross 
Abstract: While many skull stripping algorithms have been developed for multi-modal and multi-species cases, there is still a lack of a fundamentally generalizable approach. We present PUMBA(PUrely synthetic Multimodal/species invariant Brain extrAction), a strategy to train a model for brain extraction with no real brain images or labels. Our results show that even without any real images or anatomical priors, the model achieves comparable accuracy in multi-modal, multi-species and pathological cases. This work presents a new direction of research for any generalizable medical image segmentation task.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Metrics that matter: Evaluating image quality metrics for medical image generation</title>
<link>https://arxiv.org/abs/2505.07175</link>
<guid>https://arxiv.org/abs/2505.07175</guid>
<content:encoded><![CDATA[
arXiv:2505.07175v1 Announce Type: cross 
Abstract: Evaluating generative models for synthetic medical imaging is crucial yet challenging, especially given the high standards of fidelity, anatomical accuracy, and safety required for clinical applications. Standard evaluation of generated images often relies on no-reference image quality metrics when ground truth images are unavailable, but their reliability in this complex domain is not well established. This study comprehensively assesses commonly used no-reference image quality metrics using brain MRI data, including tumour and vascular images, providing a representative exemplar for the field. We systematically evaluate metric sensitivity to a range of challenges, including noise, distribution shifts, and, critically, localised morphological alterations designed to mimic clinically relevant inaccuracies. We then compare these metric scores against model performance on a relevant downstream segmentation task, analysing results across both controlled image perturbations and outputs from different generative model architectures. Our findings reveal significant limitations: many widely-used no-reference image quality metrics correlate poorly with downstream task suitability and exhibit a profound insensitivity to localised anatomical details crucial for clinical validity. Furthermore, these metrics can yield misleading scores regarding distribution shifts, e.g. data memorisation. This reveals the risk of misjudging model readiness, potentially leading to the deployment of flawed tools that could compromise patient safety. We conclude that ensuring generative models are truly fit for clinical purpose requires a multifaceted validation framework, integrating performance on relevant downstream tasks with the cautious interpretation of carefully selected no-reference image quality metrics.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards user-centered interactive medical image segmentation in VR with an assistive AI agent</title>
<link>https://arxiv.org/abs/2505.07214</link>
<guid>https://arxiv.org/abs/2505.07214</guid>
<content:encoded><![CDATA[
arXiv:2505.07214v1 Announce Type: cross 
Abstract: Crucial in disease analysis and surgical planning, manual segmentation of volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and challenging to master, while fully automatic algorithms can benefit from user-feedback. Therefore, with the complementary power of the latest radiological AI foundation models and virtual reality (VR)'s intuitive data interaction, we propose SAMIRA, a novel conversational AI agent that assists users with localizing, segmenting, and visualizing 3D medical concepts in VR. Through speech-based interaction, the agent helps users understand radiological features, locate clinical targets, and generate segmentation masks that can be refined with just a few point prompts. The system also supports true-to-scale 3D visualization of segmented pathology to enhance patient-specific anatomical understanding. Furthermore, to determine the optimal interaction paradigm under near-far attention-switching for refining segmentation masks in an immersive, human-in-the-loop workflow, we compare VR controller pointing, head pointing, and eye tracking as input modes. With a user study, evaluations demonstrated a high usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well as strong support for the proposed VR system's guidance, training potential, and integration of AI in radiological segmentation tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Plane Vision Transformer for Hemorrhage Classification Using Axial and Sagittal MRI Data</title>
<link>https://arxiv.org/abs/2505.07349</link>
<guid>https://arxiv.org/abs/2505.07349</guid>
<content:encoded><![CDATA[
arXiv:2505.07349v1 Announce Type: cross 
Abstract: Identifying brain hemorrhages from magnetic resonance imaging (MRI) is a critical task for healthcare professionals. The diverse nature of MRI acquisitions with varying contrasts and orientation introduce complexity in identifying hemorrhage using neural networks. For acquisitions with varying orientations, traditional methods often involve resampling images to a fixed plane, which can lead to information loss. To address this, we propose a 3D multi-plane vision transformer (MP-ViT) for hemorrhage classification with varying orientation data. It employs two separate transformer encoders for axial and sagittal contrasts, using cross-attention to integrate information across orientations. MP-ViT also includes a modality indication vector to provide missing contrast information to the model. The effectiveness of the proposed model is demonstrated with extensive experiments on real world clinical dataset consists of 10,084 training, 1,289 validation and 1,496 test subjects. MP-ViT achieved substantial improvement in area under the curve (AUC), outperforming the vision transformer (ViT) by 5.5% and CNN-based architectures by 1.8%. These results highlight the potential of MP-ViT in improving performance for hemorrhage detection when different orientation contrasts are needed.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2505.07411</link>
<guid>https://arxiv.org/abs/2505.07411</guid>
<content:encoded><![CDATA[
arXiv:2505.07411v1 Announce Type: cross 
Abstract: Pruning is a widely used method for compressing Deep Neural Networks (DNNs), where less relevant parameters are removed from a DNN model to reduce its size. However, removing parameters reduces model accuracy, so pruning is typically combined with fine-tuning, and sometimes other operations such as rewinding weights, to recover accuracy. A common approach is to repeatedly prune and then fine-tune, with increasing amounts of model parameters being removed in each step. While straightforward to implement, pruning pipelines that follow this approach are computationally expensive due to the need for repeated fine-tuning.
  In this paper we propose ICE-Pruning, an iterative pruning pipeline for DNNs that significantly decreases the time required for pruning by reducing the overall cost of fine-tuning, while maintaining a similar accuracy to existing pruning pipelines. ICE-Pruning is based on three main components: i) an automatic mechanism to determine after which pruning steps fine-tuning should be performed; ii) a freezing strategy for faster fine-tuning in each pruning step; and iii) a custom pruning-aware learning rate scheduler to further improve the accuracy of each pruning step and reduce the overall time consumption. We also propose an efficient auto-tuning stage for the hyperparameters (e.g., freezing percentage) introduced by the three components. We evaluate ICE-Pruning on several DNN models and datasets, showing that it can accelerate pruning by up to 9.61x. Code is available at https://github.com/gicLAB/ICE-Pruning
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Continuous Generative Models</title>
<link>https://arxiv.org/abs/2505.07447</link>
<guid>https://arxiv.org/abs/2505.07447</guid>
<content:encoded><![CDATA[
arXiv:2505.07447v1 Announce Type: cross 
Abstract: Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model</title>
<link>https://arxiv.org/abs/2505.07449</link>
<guid>https://arxiv.org/abs/2505.07449</guid>
<content:encoded><![CDATA[
arXiv:2505.07449v1 Announce Type: cross 
Abstract: In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at https://github.com/mar-cry/Ophora.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts</title>
<link>https://arxiv.org/abs/2505.07477</link>
<guid>https://arxiv.org/abs/2505.07477</guid>
<content:encoded><![CDATA[
arXiv:2505.07477v1 Announce Type: cross 
Abstract: Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by $\sim 90\%$ while maintaining superior performance. Code is available at https://github.com/deng-ai-lab/SDO.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Optimized Conditional Diffusion for Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.07548</link>
<guid>https://arxiv.org/abs/2505.07548</guid>
<content:encoded><![CDATA[
arXiv:2505.07548v1 Announce Type: cross 
Abstract: Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet the scarcity of High-Confidence Pseudo-Labeled Target Domain Samples (\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical alignment, causing DA failures. To address this challenge, we propose \textbf{N}oise \textbf{O}ptimized \textbf{C}onditional \textbf{D}iffusion for \textbf{D}omain \textbf{A}daptation (\textbf{NOCDDA}), which seamlessly integrates the generative capabilities of conditional diffusion models with the decision-making requirements of DA to achieve task-coupled optimization for efficient adaptation. For robust cross-domain consistency, we modify the DA classifier to align with the conditional diffusion classifier within a unified optimization framework, enabling forward training on noise-varying cross-domain samples. Furthermore, we argue that the conventional \( \mathcal{N}(\mathbf{0}, \mathbf{I}) \) initialization in diffusion models often generates class-confused hcpl-tds, compromising discriminative DA. To resolve this, we introduce a class-aware noise optimization strategy that refines sampling regions for reverse class-specific hcpl-tds generation, effectively enhancing cross-domain alignment. Extensive experiments across 5 benchmark datasets and 29 DA tasks demonstrate significant performance gains of \textbf{NOCDDA} over 31 state-of-the-art methods, validating its robustness and effectiveness.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding</title>
<link>https://arxiv.org/abs/2505.07600</link>
<guid>https://arxiv.org/abs/2505.07600</guid>
<content:encoded><![CDATA[
arXiv:2505.07600v1 Announce Type: cross 
Abstract: Manipulating clothes is challenging due to their complex dynamics, high deformability, and frequent self-occlusions. Garments exhibit a nearly infinite number of configurations, making explicit state representations difficult to define. In this paper, we analyze BiFold, a model that predicts language-conditioned pick-and-place actions from visual observations, while implicitly encoding garment state through end-to-end learning. To address scenarios such as crumpled garments or recovery from failed manipulations, BiFold leverages temporal context to improve state estimation. We examine the internal representations of the model and present evidence that its fine-tuning and temporal context enable effective alignment between text and image regions, as well as temporal consistency.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Brain: A Neuroscience-inspired Framework for Embodied Agents</title>
<link>https://arxiv.org/abs/2505.07634</link>
<guid>https://arxiv.org/abs/2505.07634</guid>
<content:encoded><![CDATA[
arXiv:2505.07634v1 Announce Type: cross 
Abstract: The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breast Cancer Classification in Deep Ultraviolet Fluorescence Images Using a Patch-Level Vision Transformer Framework</title>
<link>https://arxiv.org/abs/2505.07654</link>
<guid>https://arxiv.org/abs/2505.07654</guid>
<content:encoded><![CDATA[
arXiv:2505.07654v1 Announce Type: cross 
Abstract: Breast-conserving surgery (BCS) aims to completely remove malignant lesions while maximizing healthy tissue preservation. Intraoperative margin assessment is essential to achieve a balance between thorough cancer resection and tissue conservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM) enables rapid acquisition of whole surface images (WSIs) for excised tissue, providing contrast between malignant and normal tissues. However, breast cancer classification with DUV WSIs is challenged by high resolutions and complex histopathological features. This study introduces a DUV WSI classification framework using a patch-level vision transformer (ViT) model, capturing local and global features. Grad-CAM++ saliency weighting highlights relevant spatial regions, enhances result interpretability, and improves diagnostic accuracy for benign and malignant tissue classification. A comprehensive 5-fold cross-validation demonstrates the proposed approach significantly outperforms conventional deep learning methods, achieving a classification accuracy of 98.33%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells</title>
<link>https://arxiv.org/abs/2505.07661</link>
<guid>https://arxiv.org/abs/2505.07661</guid>
<content:encoded><![CDATA[
arXiv:2505.07661v1 Announce Type: cross 
Abstract: We present SparseAttnNet, a new hierarchical attention-driven framework for efficient image classification that adaptively selects and processes only the most informative pixels from images. Traditional convolutional neural networks typically process the entire images regardless of information density, leading to computational inefficiency and potential focus on irrelevant features. Our approach leverages a dynamic selection mechanism that uses coarse attention distilled by fine multi-head attention from the downstream layers of the model, allowing the model to identify and extract the most salient k pixels, where k is adaptively learned during training based on loss convergence trends. Once the top-k pixels are selected, the model processes only these pixels, embedding them as words in a language model to capture their semantics, followed by multi-head attention to incorporate global context. For biological cell images, we demonstrate that SparseAttnNet can process approximately 15% of the pixels instead of the full image. Applied to cell classification tasks using white blood cells images from the following modalities: optical path difference (OPD) images from digital holography for stain-free cells, images from motion-sensitive (event) camera from stain-free cells, and brightfield microscopy images of stained cells, For all three imaging modalities, SparseAttnNet achieves competitive accuracy while drastically reducing computational requirements in terms of both parameters and floating-point operations per second, compared to traditional CNNs and Vision Transformers. Since the model focuses on biologically relevant regions, it also offers improved explainability. The adaptive and lightweight nature of SparseAttnNet makes it ideal for deployment in resource-constrained and high-throughput settings, including imaging flow cytometry.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization</title>
<link>https://arxiv.org/abs/2505.07675</link>
<guid>https://arxiv.org/abs/2505.07675</guid>
<content:encoded><![CDATA[
arXiv:2505.07675v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ptimization ($\mathbf{\texttt{DHO}}$) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that $\texttt{DHO}$ mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that $\texttt{DHO}$ consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation</title>
<link>https://arxiv.org/abs/2505.07687</link>
<guid>https://arxiv.org/abs/2505.07687</guid>
<content:encoded><![CDATA[
arXiv:2505.07687v1 Announce Type: cross 
Abstract: Accurate multi-modal medical image translation requires ha-rmonizing global anatomical semantics and local structural fidelity, a challenge complicated by intermodality information loss and structural distortion. We propose ABS-Mamba, a novel architecture integrating the Segment Anything Model 2 (SAM2) for organ-aware semantic representation, specialized convolutional neural networks (CNNs) for preserving modality-specific edge and texture details, and Mamba's selective state-space modeling for efficient long- and short-range feature dependencies. Structurally, our dual-resolution framework leverages SAM2's image encoder to capture organ-scale semantics from high-resolution inputs, while a parallel CNNs branch extracts fine-grained local features. The Robust Feature Fusion Network (RFFN) integrates these epresentations, and the Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using spiral scanning and bidirectional state-space dynamics. A three-stage skip fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank Adaptation (LoRA+) fine-tuning to enable precise domain specialization while maintaining the foundational capabilities of the pre-trained components. Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering high-fidelity cross-modal synthesis that preserves anatomical semantics and structural details to enhance diagnostic accuracy in clinical applications. The code is available at https://github.com/gatina-yone/ABS-Mamba
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skeletonization of neuronal processes using Discrete Morse techniques from computational topology</title>
<link>https://arxiv.org/abs/2505.07754</link>
<guid>https://arxiv.org/abs/2505.07754</guid>
<content:encoded><![CDATA[
arXiv:2505.07754v1 Announce Type: cross 
Abstract: To understand biological intelligence we need to map neuronal networks in vertebrate brains. Mapping mesoscale neural circuitry is done using injections of tracers that label groups of neurons whose axons project to different brain regions. Since many neurons are labeled, it is difficult to follow individual axons. Previous approaches have instead quantified the regional projections using the total label intensity within a region. However, such a quantification is not biologically meaningful. We propose a new approach better connected to the underlying neurons by skeletonizing labeled axon fragments and then estimating a volumetric length density. Our approach uses a combination of deep nets and the Discrete Morse (DM) technique from computational topology. This technique takes into account nonlocal connectivity information and therefore provides noise-robustness. We demonstrate the utility and scalability of the approach on whole-brain tracer injected data. We also define and illustrate an information theoretic measure that quantifies the additional information obtained, compared to the skeletonized tracer injection fragments, when individual axon morphologies are available. Our approach is the first application of the DM technique to computational neuroanatomy. It can help bridge between single-axon skeletons and tracer injections, two important data types in mapping neural networks in vertebrates.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution</title>
<link>https://arxiv.org/abs/2505.07766</link>
<guid>https://arxiv.org/abs/2505.07766</guid>
<content:encoded><![CDATA[
arXiv:2505.07766v1 Announce Type: cross 
Abstract: User privacy is a crucial concern in robotic applications, especially when mobile service robots are deployed in personal or sensitive environments. However, many robotic downstream tasks require the use of cameras, which may raise privacy risks. To better understand user perceptions of privacy in relation to visual data, we conducted a user study investigating how different image modalities and image resolutions affect users' privacy concerns. The results show that depth images are broadly viewed as privacy-safe, and a similarly high proportion of respondents feel the same about semantic segmentation images. Additionally, the majority of participants consider 32*32 resolution RGB images to be almost sufficiently privacy-preserving, while most believe that 16*16 resolution can fully guarantee privacy protection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies</title>
<link>https://arxiv.org/abs/2505.07813</link>
<guid>https://arxiv.org/abs/2505.07813</guid>
<content:encoded><![CDATA[
arXiv:2505.07813v1 Announce Type: cross 
Abstract: Large-scale, diverse robot datasets have emerged as a promising path toward enabling dexterous manipulation policies to generalize to novel environments, but acquiring such datasets presents many challenges. While teleoperation provides high-fidelity datasets, its high cost limits its scalability. Instead, what if people could use their own hands, just as they do in everyday life, to collect data? In DexWild, a diverse team of data collectors uses their hands to collect hours of interactions across a multitude of environments and objects. To record this data, we create DexWild-System, a low-cost, mobile, and easy-to-use device. The DexWild learning framework co-trains on both human and robot demonstrations, leading to improved performance compared to training on each dataset individually. This combination results in robust robot policies capable of generalizing to novel environments, tasks, and embodiments with minimal additional robot-specific data. Experimental results demonstrate that DexWild significantly improves performance, achieving a 68.5% success rate in unseen environments-nearly four times higher than policies trained with robot data only-and offering 5.8x better cross-embodiment generalization. Video results, codebases, and instructions at https://dexwild.github.io
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.07815</link>
<guid>https://arxiv.org/abs/2505.07815</guid>
<content:encoded><![CDATA[
arXiv:2505.07815v1 Announce Type: cross 
Abstract: Exploration is essential for general-purpose robotic learning, especially in open-ended environments where dense rewards, explicit goals, or task-specific supervision are scarce. Vision-language models (VLMs), with their semantic reasoning over objects, spatial relations, and potential outcomes, present a compelling foundation for generating high-level exploratory behaviors. However, their outputs are often ungrounded, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity. Human exploration is often driven by the desire to discover novel scene configurations and to deepen understanding of the environment. Similarly, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE enables more diverse and meaningful exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the entropy of visited states. Moreover, the collected experience supports downstream learning, producing policies that closely match or exceed the performance of those trained on human-collected demonstrations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pixel Motion as Universal Representation for Robot Control</title>
<link>https://arxiv.org/abs/2505.07817</link>
<guid>https://arxiv.org/abs/2505.07817</guid>
<content:encoded><![CDATA[
arXiv:2505.07817v1 Announce Type: cross 
Abstract: We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a self-supervised manner, enabling diffusion model training on web-scale video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo for visualizations.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H$^{\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning</title>
<link>https://arxiv.org/abs/2505.07819</link>
<guid>https://arxiv.org/abs/2505.07819</guid>
<content:encoded><![CDATA[
arXiv:2505.07819v1 Announce Type: cross 
Abstract: Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce $\textbf{Triply-Hierarchical Diffusion Policy}~(\textbf{H$^{\mathbf{3}}$DP})$, a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H$^{3}$DP contains $\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H$^{3}$DP yields a $\mathbf{+27.5\%}$ average relative improvement over baselines across $\mathbf{44}$ simulation tasks and achieves superior performance in $\mathbf{4}$ challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review helps learn better: Temporal Supervised Knowledge Distillation</title>
<link>https://arxiv.org/abs/2307.00811</link>
<guid>https://arxiv.org/abs/2307.00811</guid>
<content:encoded><![CDATA[
arXiv:2307.00811v3 Announce Type: replace 
Abstract: Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and advantages of our method over existing knowledge distillation methods, including various network architectures and different tasks (image classification and object detection) .
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Condition numbers in multiview geometry, instability in relative pose estimation, and RANSAC</title>
<link>https://arxiv.org/abs/2310.02719</link>
<guid>https://arxiv.org/abs/2310.02719</guid>
<content:encoded><![CDATA[
arXiv:2310.02719v2 Announce Type: replace 
Abstract: In this paper, we introduce a general framework for analyzing the numerical conditioning of minimal problems in multiple view geometry, using tools from computational algebra and Riemannian geometry. Special motivation comes from the fact that relative pose estimation, based on standard 5-point or 7-point Random Sample Consensus (RANSAC) algorithms, can fail even when no outliers are present and there is enough data to support a hypothesis. We argue that these cases arise due to the intrinsic instability of the 5- and 7-point minimal problems. We apply our framework to characterize the instabilities, both in terms of the world scenes that lead to infinite condition number, and directly in terms of ill-conditioned image data. The approach produces computational tests for assessing the condition number before solving the minimal problem. Lastly, synthetic and real data experiments suggest that RANSAC serves not only to remove outliers, but in practice it also selects for well-conditioned image data, which is consistent with our theory.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic and Expressive Variation in Image Captions Across Languages</title>
<link>https://arxiv.org/abs/2310.14356</link>
<guid>https://arxiv.org/abs/2310.14356</guid>
<content:encoded><![CDATA[
arXiv:2310.14356v5 Announce Type: replace 
Abstract: Computer vision often treats human perception as homogeneous: an implicit assumption that visual stimuli are perceived similarly by everyone. This assumption is reflected in the way researchers collect datasets and train vision models. By contrast, literature in cross-cultural psychology and linguistics has provided evidence that people from different cultural backgrounds observe vastly different concepts even when viewing the same visual stimuli. In this paper, we study how these differences manifest themselves in vision-language datasets and models, using language as a proxy for culture. By comparing textual descriptions generated across 7 languages for the same images, we find significant differences in the semantic content and linguistic expression. When datasets are multilingual as opposed to monolingual, descriptions have higher semantic coverage on average, where coverage is measured using scene graphs, model embeddings, and linguistic taxonomies. For example, multilingual descriptions have on average 29.9% more objects, 24.5% more relations, and 46.0% more attributes than a set of monolingual captions. When prompted to describe images in different languages, popular models (e.g. LLaVA) inherit this bias and describe different parts of the image. Moreover, finetuning models on captions from one language performs best on corresponding test data from that language, while finetuning on multilingual data performs consistently well across all test data compositions. Our work points towards the need to account for and embrace the diversity of human perception in the computer vision community.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Feature-Guided Diffusion Models for Shadow Removal</title>
<link>https://arxiv.org/abs/2312.02156</link>
<guid>https://arxiv.org/abs/2312.02156</guid>
<content:encoded><![CDATA[
arXiv:2312.02156v2 Announce Type: replace 
Abstract: Recovering textures under shadows has remained a challenging problem due to the difficulty of inferring shadow-free scenes from shadow images. In this paper, we propose the use of diffusion models as they offer a promising approach to gradually refine the details of shadow regions during the diffusion process. Our method improves this process by conditioning on a learned latent feature space that inherits the characteristics of shadow-free images, thus avoiding the limitation of conventional methods that condition on degraded images only. Additionally, we propose to alleviate potential local optima during training by fusing noise features with the diffusion network. We demonstrate the effectiveness of our approach which outperforms the previous best method by 13% in terms of RMSE on the AISTD dataset. Further, we explore instance-level shadow removal, where our model outperforms the previous best method by 82% in terms of RMSE on the DESOBA dataset.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Habitat Information for Fine-grained Bird Identification</title>
<link>https://arxiv.org/abs/2312.14999</link>
<guid>https://arxiv.org/abs/2312.14999</guid>
<content:encoded><![CDATA[
arXiv:2312.14999v2 Announce Type: replace 
Abstract: Traditional bird classifiers mostly rely on the visual characteristics of birds. Some prior works even train classifiers to be invariant to the background, completely discarding the living environment of birds. Instead, we are the first to explore integrating habitat information, one of the four major cues for identifying birds by ornithologists, into modern bird classifiers. We focus on two leading model types: (1) CNNs and ViTs trained on the downstream bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with habitat-augmented data results in an improvement of up to +0.83 and +0.23 points on NABirds and CUB-200, respectively. Similarly, adding habitat descriptors to the prompts for CLIP yields a substantial accuracy boost of up to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find consistent accuracy improvement after integrating habitat features into the image augmentation process and into the textual descriptors of vision-language CLIP classifiers. Code is available at: https://anonymous.4open.science/r/reasoning-8B7E/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Complementary Knowledge Distillation for Efficient Dense Image Prediction</title>
<link>https://arxiv.org/abs/2401.13174</link>
<guid>https://arxiv.org/abs/2401.13174</guid>
<content:encoded><![CDATA[
arXiv:2401.13174v4 Announce Type: replace 
Abstract: It has been revealed that small efficient dense image prediction (EDIP) models, trained using the knowledge distillation (KD) framework, encounter two key challenges, including maintaining boundary region completeness and preserving target region connectivity, despite their favorable capacity to recognize main object regions. In this work, we propose a complementary boundary and context distillation (BCD) method within the KD framework for EDIPs, which facilitates the targeted knowledge transfer from large accurate teacher models to compact efficient student models. Specifically, the boundary distillation component focuses on extracting explicit object-level semantic boundaries from the hierarchical feature maps of the backbone network to enhance the student model's mask quality in boundary regions. Concurrently, the context distillation component leverages self-relations as a bridge to transfer implicit pixel-level contexts from the teacher model to the student model, ensuring strong connectivity in target regions. Our proposed BCD method is specifically designed for EDIP tasks and is characterized by its simplicity and efficiency. Extensive experimental results across semantic segmentation, object detection, and instance segmentation on various representative datasets demonstrate that our method can outperform existing methods without requiring extra supervisions or incurring increased inference costs, resulting in well-defined object boundaries and smooth connecting regions.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Msmsfnet: a multi-stream and multi-scale fusion net for edge detection</title>
<link>https://arxiv.org/abs/2404.04856</link>
<guid>https://arxiv.org/abs/2404.04856</guid>
<content:encoded><![CDATA[
arXiv:2404.04856v3 Announce Type: replace 
Abstract: Edge detection is a long-standing problem in computer vision. Despite the efficiency of existing algorithms, their performance, however, rely heavily on the pre-trained weights of the backbone network on the ImageNet dataset. The use of pre-trained weights in previous methods significantly increases the difficulty to design new models for edge detection without relying on existing well-trained ImageNet models, as pre-training the model on the ImageNet dataset is expensive and becomes compulsory to ensure the fairness of comparison. Besides, the pre-training and fine-tuning strategy is not always useful and sometimes even inaccessible. For instance, the pre-trained weights on the ImageNet dataset are unlikely to be helpful for edge detection in Synthetic Aperture Radar (SAR) images due to strong differences in the statistics between optical images and SAR images. Moreover, no dataset has comparable size to the ImageNet dataset for SAR image processing. In this work, we study the performance achievable by state-of-the-art deep learning based edge detectors in publicly available datasets when they are trained from scratch, and devise a new network architecture, the multi-stream and multi-scale fusion net (msmsfnet), for edge detection. We show in our experiments that by training all models from scratch, our model outperforms state-of-the-art edge detectors in three publicly available datasets. We also demonstrate the efficiency of our model for edge detection in SAR images, where no useful pre-trained weight is available. Finally, We show that our model is able to achieve competitive performance on the BSDS500 dataset when the pre-trained weights are used.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Reconstruction of Optical Doppler Tomography with Alternative State Space Model and Attention</title>
<link>https://arxiv.org/abs/2404.17484</link>
<guid>https://arxiv.org/abs/2404.17484</guid>
<content:encoded><![CDATA[
arXiv:2404.17484v2 Announce Type: replace 
Abstract: Optical coherence Doppler tomography (ODT) is an emerging blood flow imaging technique. The fundamental unit of ODT is the 1D depth-resolved trace named raw A-scans (or A-line). A 2D ODT image (B-scan) is formed by reconstructing a cross-sectional flow image via Doppler phase-subtraction of raw A-scans along B-line. To obtain a high-fidelity B-scan, densely sampled A-scans are required currently, leading to prolonged scanning time and increased storage demands. Addressing this issue, we propose a novel sparse ODT reconstruction framework with an Alternative State Space Attention Network (ASSAN) that effectively reduces raw A-scans needed. Inspired by the distinct distributions of information along A-line and B-line, ASSAN applies 1D State Space Model (SSM) to each A-line to learn the intra-A-scan representation, while using 1D gated self-attention along B-line to capture the inter-A-scan features. In addition, an effective feedforward network based on sequential 1D convolutions along different axes is employed to enhance the local feature. In validation experiments on real animal data, ASSAN shows clear effectiveness in the reconstruction in comparison with state-of-the-art reconstruction methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPL: Memory Augmentation and Pseudo-Labeling for Semi-Supervised Anomaly Detection</title>
<link>https://arxiv.org/abs/2405.06198</link>
<guid>https://arxiv.org/abs/2405.06198</guid>
<content:encoded><![CDATA[
arXiv:2405.06198v3 Announce Type: replace 
Abstract: Large unlabeled data and difficult-to-identify anomalies are the urgent issues need to overcome in most industrial scene. In order to address this issue, a new meth-odology for detecting surface defects in in-dustrial settings is introduced, referred to as Memory Augmentation and Pseudo-Labeling(MAPL). The methodology first in-troduces an anomaly simulation strategy, which significantly improves the model's ability to recognize rare or unknown anom-aly types by generating simulated anomaly samples. To cope with the problem of the lack of labeling of anomalous simulated samples, a pseudo-labeler method based on a one-classifier ensemble was employed in this study, which enhances the robustness of the model in the case of limited labeling data by automatically selecting key pseudo-labeling hyperparameters. Meanwhile, a memory-enhanced learning mechanism is introduced to effectively predict abnormal regions by analyzing the difference be-tween the input samples and the normal samples in the memory pool. An end-to-end learning framework is employed by MAPL to identify the abnormal regions directly from the input data, which optimizes the ef-ficiency and real-time performance of de-tection. By conducting extensive trials on the recently developed BHAD dataset (in-cluding MVTec AD [1], Visa [2], and MDPP [3]), MAPL achieves an average im-age-level AUROC score of 86.2%, demon-strating a 5.1% enhancement compared to the original MemSeg [4] model. The source code is available at https://github.com/jzc777/MAPL.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Compressed Sensing for Image Reconstruction with Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2405.17456</link>
<guid>https://arxiv.org/abs/2405.17456</guid>
<content:encoded><![CDATA[
arXiv:2405.17456v3 Announce Type: replace 
Abstract: We examine the problem of selecting a small set of linear measurements for reconstructing high-dimensional signals. Well-established methods for optimizing such measurements include principal component analysis (PCA), independent component analysis (ICA) and compressed sensing (CS) based on random projections, all of which rely on axis- or subspace-aligned statistical characterization of the signal source. However, many naturally occurring signals, including photographic images, contain richer statistical structure. To exploit such structure, we introduce a general method for obtaining an optimized set of linear measurements for efficient image reconstruction, where the signal statistics are expressed by the prior implicit in a neural network trained to perform denoising (known as a ``diffusion model''). We demonstrate that the optimal measurements derived for two natural image datasets differ from those of PCA, ICA, or CS, and result in substantially lower mean squared reconstruction error. Interestingly, the marginal distributions of the measurement values are asymmetrical (skewed), substantially more so than those of previous methods. We also find that optimizing with respect to perceptual loss, as quantified by structural similarity (SSIM), leads to measurements different from those obtained when optimizing for MSE. Our results highlight the importance of incorporating the specific statistical regularities of natural signals when designing effective linear measurements.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight UDF Learning Framework for 3D Reconstruction Based on Local Shape Functions</title>
<link>https://arxiv.org/abs/2407.01330</link>
<guid>https://arxiv.org/abs/2407.01330</guid>
<content:encoded><![CDATA[
arXiv:2407.01330v2 Announce Type: replace 
Abstract: Unsigned distance fields (UDFs) provide a versatile framework for representing a diverse array of 3D shapes, encompassing both watertight and non-watertight geometries. Traditional UDF learning methods typically require extensive training on large 3D shape datasets, which is costly and necessitates re-training for new datasets. This paper presents a novel neural framework, LoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging local shape functions to learn UDFs. We observe that 3D shapes manifest simple patterns in localized regions, prompting us to develop a training dataset of point cloud patches characterized by mathematical functions that represent a continuum from smooth surfaces to sharp edges and corners. Our approach learns features within a specific radius around each query point and utilizes an attention mechanism to focus on the crucial features for UDF estimation. Despite being highly lightweight, with only 653 KB of trainable parameters and a modest-sized training dataset with 0.5 GB storage, our method enables efficient and robust surface reconstruction from point clouds without requiring for shape-specific training. Furthermore, our method exhibits enhanced resilience to noise and outliers in point clouds compared to existing methods. We conduct comprehensive experiments and comparisons across various datasets, including synthetic and real-scanned point clouds, to validate our method's efficacy. Notably, our lightweight framework offers rapid and reliable initialization for other unsupervised iterative approaches, improving both the efficiency and accuracy of their reconstructions. Our project and code are available at https://jbhu67.github.io/LoSF-UDF.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPPP: Halpern-type Preconditioned Proximal Point Algorithms and Applications to Image Restoration</title>
<link>https://arxiv.org/abs/2407.13120</link>
<guid>https://arxiv.org/abs/2407.13120</guid>
<content:encoded><![CDATA[
arXiv:2407.13120v3 Announce Type: replace 
Abstract: Recently, the degenerate preconditioned proximal point (PPP) method provides a unified and flexible framework for designing and analyzing operator-splitting algorithms such as Douglas-Rachford (DR). However, the degenerate PPP method exhibits weak convergence in the infinite-dimensional Hilbert space and lacks accelerated variants. To address these issues, we propose a Halpern-type PPP (HPPP) algorithm, which leverages the strong convergence and acceleration properties of Halpern's iteration method. Moreover, we propose a novel algorithm for image restoration by combining HPPP with denoiser priors such as Plug-and-Play (PnP) prior, which can be viewed as an accelerated PnP method. Finally, numerical experiments including several toy examples and image restoration validate the effectiveness of our proposed algorithms.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation</title>
<link>https://arxiv.org/abs/2407.18715</link>
<guid>https://arxiv.org/abs/2407.18715</guid>
<content:encoded><![CDATA[
arXiv:2407.18715v3 Announce Type: replace 
Abstract: Scene Graph Generation (SGG) remains a challenging task due to its compositional property. Previous approaches improve prediction efficiency through end-to-end learning. However, these methods exhibit limited performance as they assume unidirectional conditioning between entities and predicates, which restricts effective information interaction. To address this limitation, we propose a novel bidirectional conditioning factorization in a semantic-aligned space for SGG, enabling efficient and generalizable interaction between entities and predicates. Specifically, we introduce an end-to-end scene graph generation model, the Bidirectional Conditioning Transformer (BCTR), to implement this factorization. BCTR consists of two key modules. First, the Bidirectional Conditioning Generator (BCG) performs multi-stage interactive feature augmentation between entities and predicates, enabling mutual enhancement between these predictions. Second, Random Feature Alignment (RFA) is present to regularize feature space by distilling multi-modal knowledge from pre-trained models. Within this regularized feature space, BCG is feasible to capture interaction patterns across diverse relationships during training, and the learned interaction patterns can generalize to unseen but semantically related relationships during inference. Extensive experiments on Visual Genome and Open Image V6 show that BCTR achieves state-of-the-art performance on both benchmarks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking</title>
<link>https://arxiv.org/abs/2408.12232</link>
<guid>https://arxiv.org/abs/2408.12232</guid>
<content:encoded><![CDATA[
arXiv:2408.12232v2 Announce Type: replace 
Abstract: Hyperspectral object tracking (HOT) has exhibited potential in various applications, particularly in scenes where objects are camouflaged. Existing trackers can effectively retrieve objects via band regrouping because of the bias in existing HOT datasets, where most objects tend to have distinguishing visual appearances rather than spectral characteristics. This bias allows the tracker to directly use the visual features obtained from the false-color images generated by hyperspectral images without the need to extract spectral features. To tackle this bias, we find that the tracker should focus on the spectral information when object appearance is unreliable. Thus, we provide a new task called hyperspectral camouflaged object tracking (HCOT) and meticulously construct a large-scale HCOT dataset, termed BihoT, which consists of 41,912 hyperspectral images covering 49 video sequences. The dataset covers various artificial camouflage scenes where objects have similar appearances, diverse spectrums, and frequent occlusion, making it a very challenging dataset for HCOT. Besides, a simple but effective baseline model, named spectral prompt-based distractor-aware network (SPDAN), is proposed, comprising a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN fine-tunes powerful RGB trackers with spectral prompts and alleviates the insufficiency of training samples. Moreover, the DAM utilizes a novel statistic to capture the distractor caused by occlusion from objects and background. Extensive experiments demonstrate that our proposed SPDAN achieves state-of-the-art performance on the proposed BihoT and other HOT datasets.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camouflaged Object Tracking: A Benchmark</title>
<link>https://arxiv.org/abs/2408.13877</link>
<guid>https://arxiv.org/abs/2408.13877</guid>
<content:encoded><![CDATA[
arXiv:2408.13877v3 Announce Type: replace 
Abstract: Visual tracking has seen remarkable advancements, largely driven by the availability of large-scale training datasets that have enabled the development of highly accurate and robust algorithms. While significant progress has been made in tracking general objects, research on more challenging scenarios, such as tracking camouflaged objects, remains limited. Camouflaged objects, which blend seamlessly with their surroundings or other objects, present unique challenges for detection and tracking in complex environments. This challenge is particularly critical in applications such as military, security, agriculture, and marine monitoring, where precise tracking of camouflaged objects is essential. To address this gap, we introduce the Camouflaged Object Tracking Dataset (COTD), a specialized benchmark designed specifically for evaluating camouflaged object tracking methods. The COTD dataset comprises 200 sequences and approximately 80,000 frames, each annotated with detailed bounding boxes. Our evaluation of 20 existing tracking algorithms reveals significant deficiencies in their performance with camouflaged objects. To address these issues, we propose a novel tracking framework, HiPTrack-MLS, which demonstrates promising results in improving tracking performance for camouflaged objects. COTD and code are avialable at https://github.com/openat25/HIPTrack-MLS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel fusion of Sentinel-1 and Sentinel-2 with climate data for crop phenology estimation using Machine Learning</title>
<link>https://arxiv.org/abs/2409.00020</link>
<guid>https://arxiv.org/abs/2409.00020</guid>
<content:encoded><![CDATA[
arXiv:2409.00020v2 Announce Type: replace 
Abstract: Crop phenology describes the physiological development stages of crops from planting to harvest which is valuable information for decision makers to plan and adapt agricultural management strategies. In the era of big Earth observation data ubiquity, attempts have been made to accurately detect crop phenology using Remote Sensing (RS) and high resolution weather data. However, most studies have focused on large scale predictions of phenology or developed methods which are not adequate to help crop modeler communities on leveraging Sentinel-1 and Sentinal-2 data and fusing them with high resolution climate data, using a novel framework. For this, we trained a Machine Learning (ML) LightGBM model to predict 13 phenological stages for eight major crops across Germany at 20 m scale. Observed phonologies were taken from German national phenology network (German Meteorological Service; DWD) between 2017 and 2021. We proposed a thorough feature selection analysis to find the best combination of RS and climate data to detect phenological stages. At national scale, predicted phenology resulted in a reasonable precision of R2 > 0.43 and a low Mean Absolute Error of 6 days, averaged over all phenological stages and crops. The spatio-temporal analysis of the model predictions demonstrates its transferability across different spatial and temporal context of Germany. The results indicated that combining radar sensors with climate data yields a very promising performance for a multitude of practical applications. Moreover, these improvements are expected to be useful to generate highly valuable input for crop model calibrations and evaluations, facilitate informed agricultural decisions, and contribute to sustainable food production to address the increasing global food demand.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGEV++: Iterative Multi-range Geometry Encoding Volumes for Stereo Matching</title>
<link>https://arxiv.org/abs/2409.00638</link>
<guid>https://arxiv.org/abs/2409.00638</guid>
<content:encoded><![CDATA[
arXiv:2409.00638v3 Announce Type: replace 
Abstract: Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23\% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9\% and 54.8\% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks. The code is publicly available at https://github.com/gangweix/IGEV and https://github.com/gangweix/IGEV-plusplus.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance</title>
<link>https://arxiv.org/abs/2409.06002</link>
<guid>https://arxiv.org/abs/2409.06002</guid>
<content:encoded><![CDATA[
arXiv:2409.06002v4 Announce Type: replace 
Abstract: Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using \textit{Class-Prompt Appending} and \textit{Visual Prior Blending} to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a \textit{class balancing algorithm} to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at \href{https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this https URL}.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation</title>
<link>https://arxiv.org/abs/2409.18653</link>
<guid>https://arxiv.org/abs/2409.18653</guid>
<content:encoded><![CDATA[
arXiv:2409.18653v2 Announce Type: replace 
Abstract: This study investigates the application and performance of the Segment Anything Model 2 (SAM2) in the challenging task of video camouflaged object segmentation (VCOS). VCOS involves detecting objects that blend seamlessly in the surroundings for videos, due to similar colors and textures, poor light conditions, etc. Compared to the objects in normal scenes, camouflaged objects are much more difficult to detect. SAM2, a video foundation model, has shown potential in various tasks. But its effectiveness in dynamic camouflaged scenarios remains under-explored. This study presents a comprehensive study on SAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged video datasets using different models and prompts (click, box, and mask). Second, we explore the integration of SAM2 with existing multimodal large language models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by fine-tuning it on the video camouflaged dataset. Our comprehensive experiments demonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged objects in videos. We also show that this ability could be further improved by specifically adjusting SAM2's parameters for VCOS. The code is available at https://github.com/zhoustan/SAM2-VCOS
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTransPDM: A Graph-embedded Transformer with Positional Decoupling for Pedestrian Crossing Intention Prediction</title>
<link>https://arxiv.org/abs/2409.20223</link>
<guid>https://arxiv.org/abs/2409.20223</guid>
<content:encoded><![CDATA[
arXiv:2409.20223v2 Announce Type: replace 
Abstract: Understanding and predicting pedestrian crossing behavioral intention is crucial for the driving safety of autonomous vehicles. Nonetheless, challenges emerge when using promising images or environmental context masks to extract various factors for time-series network modeling, causing pre-processing errors or a loss of efficiency. Typically, pedestrian positions captured by onboard cameras are often distorted and do not accurately reflect their actual movements. To address these issues, GTransPDM -- a Graph-embedded Transformer with a Position Decoupling Module -- was developed for pedestrian crossing intention prediction by leveraging multi-modal features. First, a positional decoupling module was proposed to decompose pedestrian lateral motion and encode depth cues in the image view. Then, a graph-embedded Transformer was designed to capture the spatio-temporal dynamics of human pose skeletons, integrating essential factors such as position, skeleton, and ego-vehicle motion. Experimental results indicate that the proposed method achieves 92% accuracy on the PIE dataset and 87% accuracy on the JAAD dataset, with a processing speed of 0.05ms. It outperforms the state-of-the-art in comparison.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Large Motion Models with Million-Level Human Motions</title>
<link>https://arxiv.org/abs/2410.03311</link>
<guid>https://arxiv.org/abs/2410.03311</guid>
<content:encoded><![CDATA[
arXiv:2410.03311v2 Announce Type: replace 
Abstract: Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted toward developing large motion models. Despite some progress, current efforts remain far from achieving truly generalist models, primarily due to the lack of massive high-quality data. To address this gap, we present MotionLib, the first million-level dataset for motion generation, which is at least 15$\times$ larger than existing counterparts and enriched with hierarchical text descriptions. Using MotionLib, we train a large motion model named Being-M0, demonstrating robust performance across a wide range of human activities, including unseen ones. Through systematic investigation, for the first time, we highlight the importance of scaling both data and model size for advancing motion generation, along with key insights to achieve this goal. To better integrate the motion modality, we propose Motionbook, an innovative motion encoding approach including (1) a compact yet lossless feature to represent motions; (2) a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity, significantly enhancing the representational power of motion tokens. We believe this work lays the groundwork for developing more versatile and powerful motion generation models in the future. For further details, visit https://github.com/BeingBeyond/Being-M0.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rotating-star Pattern for Camera Calibration</title>
<link>https://arxiv.org/abs/2410.13371</link>
<guid>https://arxiv.org/abs/2410.13371</guid>
<content:encoded><![CDATA[
arXiv:2410.13371v3 Announce Type: replace 
Abstract: Camera calibration is fundamental to 3D vision, and the choice of calibration pattern greatly affects the accuracy. To address aberration issue, star-shaped pattern has been proposed as alternatives to traditional checkerboard. However, such pattern suffers from aliasing artifacts. In this paper, we present a novel solution by employing a series of checkerboard patterns rotated around a central point instead of a single star-shaped pattern. We further propose a complete feature extraction algorithm tailored for this design. Experimental results demonstrate that our approach offers improved accuracy over the conventional star-shaped pattern and achieves high stability across varying exposure levels.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural Networks</title>
<link>https://arxiv.org/abs/2410.19794</link>
<guid>https://arxiv.org/abs/2410.19794</guid>
<content:encoded><![CDATA[
arXiv:2410.19794v2 Announce Type: replace 
Abstract: Deep Neural Networks (DNNs) are increasingly deployed across applications. However, ensuring their reliability remains a challenge, and in many situations, alternative models with similar functionality and accuracy are available. Traditional accuracy-based evaluations often fail to capture behavioral differences between models, especially with limited test datasets, making it difficult to select or combine models effectively. Differential testing addresses this by generating test inputs that expose discrepancies in DNN model behavior. However, existing approaches face significant limitations: many rely on model internals or are constrained by available seed inputs. To address these challenges, we propose DiffGAN, a black-box test image generation approach for differential testing of DNN models. DiffGAN leverages a Generative Adversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II to generate diverse and valid triggering inputs that reveal behavioral discrepancies between models. DiffGAN employs two custom fitness functions, focusing on diversity and divergence, to guide the exploration of the GAN input space and identify discrepancies between models' outputs. By strategically searching this space, DiffGAN generates inputs with specific features that trigger differences in model behavior. DiffGAN is black-box, making it applicable in more situations. We evaluate DiffGAN on eight DNN model pairs trained on widely used image datasets. Our results show DiffGAN significantly outperforms a SOTA baseline, generating four times more triggering inputs, with greater diversity and validity, within the same budget. Additionally, the generated inputs improve the accuracy of a machine learning-based model selection mechanism, which selects the best-performing model based on input characteristics and can serve as a smart output voting mechanism when using alternative models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Veri-Car: Towards Open-world Vehicle Information Retrieval</title>
<link>https://arxiv.org/abs/2411.06864</link>
<guid>https://arxiv.org/abs/2411.06864</guid>
<content:encoded><![CDATA[
arXiv:2411.06864v4 Announce Type: replace 
Abstract: Many industrial and service sectors require tools to extract vehicle characteristics from images. This is a complex task not only by the variety of noise, and large number of classes, but also by the constant introduction of new vehicle models to the market. In this paper, we present Veri-Car, an information retrieval integrated approach designed to help on this task. It leverages supervised learning techniques to accurately identify the make, type, model, year, color, and license plate of cars. The approach also addresses the challenge of handling open-world problems, where new car models and variations frequently emerge, by employing a sophisticated combination of pre-trained models, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust performance, achieving high precision and accuracy in classifying both seen and unseen data. Additionally, it integrates an ensemble license plate detection, and an OCR model to extract license plate numbers with impressive accuracy.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning</title>
<link>https://arxiv.org/abs/2411.07742</link>
<guid>https://arxiv.org/abs/2411.07742</guid>
<content:encoded><![CDATA[
arXiv:2411.07742v4 Announce Type: replace 
Abstract: This paper studies point cloud perception within outdoor environments. Existing methods face limitations in recognizing objects located at a distance or occluded, due to the sparse nature of outdoor point clouds. In this work, we observe a significant mitigation of this problem by accumulating multiple temporally consecutive point cloud sweeps, resulting in a remarkable improvement in perception accuracy. However, the computation cost also increases, hindering previous approaches from utilizing a large number of point cloud sweeps. To tackle this challenge, we find that a considerable portion of points in the accumulated point cloud is redundant, and discarding these points has minimal impact on perception accuracy. We introduce a simple yet effective Gumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a learned end-to-end sampling. The GSP layer is decoupled from other network components and thus can be seamlessly integrated into existing point cloud network architectures. Without incurring additional computational overhead, we increase the number of point cloud sweeps from 10, a common practice, to as many as 40. Consequently, there is a significant enhancement in perception performance. For instance, in nuScenes 3D object detection and BEV map segmentation tasks, our pruning strategy improves several 3D perception baseline methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCi-YOLOv8n: Enhanced Fire Detection with CARAFE and Context-Guided Modules</title>
<link>https://arxiv.org/abs/2411.11011</link>
<guid>https://arxiv.org/abs/2411.11011</guid>
<content:encoded><![CDATA[
arXiv:2411.11011v2 Announce Type: replace 
Abstract: Fire incidents in urban and forested areas pose serious threats,underscoring the need for more effective detection technologies. To address these challenges, we present CCi-YOLOv8n, an enhanced YOLOv8 model with targeted improvements for detecting small fires and smoke. The model integrates the CARAFE up-sampling operator and a context-guided module to reduce information loss during up-sampling and down-sampling, thereby retaining richer feature representations. Additionally, an inverted residual mobile block enhanced C2f module captures small targets and fine smoke patterns, a critical improvement over the original model's detection capacity.For validation, we introduce Web-Fire, a dataset curated for fire and smoke detection across diverse real-world scenarios. Experimental results indicate that CCi-YOLOv8n outperforms YOLOv8n in detection precision, confirming its effectiveness for robust fire detection tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transmission Line Defect Detection Based on UAV Patrol Images and Vision-language Pretraining</title>
<link>https://arxiv.org/abs/2411.11370</link>
<guid>https://arxiv.org/abs/2411.11370</guid>
<content:encoded><![CDATA[
arXiv:2411.11370v2 Announce Type: replace 
Abstract: Unmanned aerial vehicle (UAV) patrol inspection has emerged as a predominant approach in transmission line monitoring owing to its cost-effectiveness. Detecting defects in transmission lines is a critical task during UAV patrol inspection. However, due to imaging distance and shooting angles, UAV patrol images often suffer from insufficient defect-related visual information, which has an adverse effect on detection accuracy. In this article, we propose a novel method for detecting defects in UAV patrol images, which is based on vision-language pretraining for transmission line (VLP-TL) and a progressive transfer strategy (PTS). Specifically, VLP-TL contains two novel pretraining tasks tailored for the transmission line scenario, aimimg at pretraining an image encoder with abundant knowledge acquired from both visual and linguistic information. Transferring the pretrained image encoder to the defect detector as its backbone can effectively alleviate the insufficient visual information problem. In addition, the PTS further improves transfer performance by progressively bridging the gap between pretraining and downstream defection detection. Experimental results demonstrate that the proposed method significantly improves defect detection accuracy by jointly utilizing multimodal information, overcoming the limitations of insufficient defect-related visual information provided by UAV patrol images.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding</title>
<link>https://arxiv.org/abs/2411.11904</link>
<guid>https://arxiv.org/abs/2411.11904</guid>
<content:encoded><![CDATA[
arXiv:2411.11904v3 Announce Type: replace 
Abstract: Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an object's position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching the performance of specialized methods on multiple benchmarks. Code available at https://github.com/zytx121/GeoGround
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph</title>
<link>https://arxiv.org/abs/2411.14494</link>
<guid>https://arxiv.org/abs/2411.14494</guid>
<content:encoded><![CDATA[
arXiv:2411.14494v3 Announce Type: replace 
Abstract: A facial morph is an image created by combining two face images pertaining to two distinct identities. Face demorphing inverts the process and tries to recover the original images constituting a facial morph. While morph attack detection (MAD) techniques can be used to flag morph images, they do not divulge any visual information about the faces used to create them. Demorphing helps address this problem. Existing demorphing techniques are either very restrictive (assume identities during testing) or produce feeble outputs (both outputs look very similar). In this paper, we overcome these issues by proposing dc-GAN, a novel GAN-based demorphing method conditioned on the morph images. Our method overcomes morph-replication and produces high quality reconstructions of the bonafide images used to create the morphs. Moreover, our method is highly generalizable across demorphing paradigms (differential/reference-free). We conduct experiments on AMSL, FRLL-Morphs and MorDiff datasets to showcase the efficacy of our method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opt-In Art: Learning Art Styles Only from Few Examples</title>
<link>https://arxiv.org/abs/2412.00176</link>
<guid>https://arxiv.org/abs/2412.00176</guid>
<content:encoded><![CDATA[
arXiv:2412.00176v2 Announce Type: replace 
Abstract: We explore whether pre-training on datasets with paintings is necessary for a model to learn an artistic style with only a few examples. To investigate this, we train a text-to-image model exclusively on photographs, without access to any painting-related content. We show that it is possible to adapt a model that is trained without paintings to an artistic style, given only few examples. User studies and automatic evaluations confirm that our model (post-adaptation) performs on par with state-of-the-art models trained on massive datasets that contain artistic content like paintings, drawings or illustrations. Finally, using data attribution techniques, we analyze how both artistic and non-artistic datasets contribute to generating artistic-style images. Surprisingly, our findings suggest that high-quality artistic outputs can be achieved without prior exposure to artistic data, indicating that artistic style generation can occur in a controlled, opt-in manner using only a limited, carefully selected set of training examples.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SerialGen: Personalized Image Generation by First Standardization Then Personalization</title>
<link>https://arxiv.org/abs/2412.01485</link>
<guid>https://arxiv.org/abs/2412.01485</guid>
<content:encoded><![CDATA[
arXiv:2412.01485v2 Announce Type: replace 
Abstract: In this work, we are interested in achieving both high text controllability and whole-body appearance consistency in the generation of personalized human characters. We propose a novel framework, named SerialGen, which is a serial generation method consisting of two stages: first, a standardization stage that standardizes reference images, and then a personalized generation stage based on the standardized reference. Furthermore, we introduce two modules aimed at enhancing the standardization process. Our experimental results validate the proposed framework's ability to produce personalized images that faithfully recover the reference image's whole-body appearance while accurately responding to a wide range of text prompts. Through thorough analysis, we highlight the critical contribution of the proposed serial generation method and standardization model, evidencing enhancements in appearance consistency between reference and output images and across serial outputs generated from diverse text prompts. The term "Serial" in this work carries a double meaning: it refers to the two-stage method and also underlines our ability to generate serial images with consistent appearance throughout.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs</title>
<link>https://arxiv.org/abs/2412.01818</link>
<guid>https://arxiv.org/abs/2412.01818</guid>
<content:encoded><![CDATA[
arXiv:2412.01818v2 Announce Type: replace 
Abstract: Large vision-language models (LVLMs) generally contain significantly more visual tokens than their textual counterparts, resulting in a considerable computational burden. Recent efforts have been made to tackle this issue by pruning visual tokens early within the language model. Most existing works use attention scores between text and visual tokens to assess the importance of visual tokens. However, in this study, we first analyze the text-visual attention in the language model and find that this score is not an ideal indicator for token pruning. Based on the analysis, We propose VisPruner, a plug-and-play method that utilizes visual cues for more effective token pruning in LVLMs. Specifically, we first use visual attention to select a limited number of significant tokens. Then, we remove duplicate tokens from the remaining ones based on their similarity. By retaining diverse tokens alongside the initially selected important tokens, we maximally preserve the visual information of the input image. Experimental results demonstrate that our VisPruner sustains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing methods based on text-visual attention. Notably, without any training, VisPruner can reduce the FLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining comparable performance. Our code is available at https://github.com/Theia-4869/VisPruner.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis</title>
<link>https://arxiv.org/abs/2412.09521</link>
<guid>https://arxiv.org/abs/2412.09521</guid>
<content:encoded><![CDATA[
arXiv:2412.09521v2 Announce Type: replace 
Abstract: Pathological diagnosis is vital for determining disease characteristics, guiding treatment, and assessing prognosis, relying heavily on detailed, multi-scale analysis of high-resolution whole slide images (WSI). However, traditional pure vision models face challenges of redundant feature extraction, whereas existing large vision-language models (LVLMs) are limited by input resolution constraints, hindering their efficiency and accuracy. To overcome these issues, we propose two innovative strategies: the mixed task-guided feature enhancement, which directs feature extraction toward lesion-related details across scales, and the prompt-guided detail feature completion, which integrates coarse- and fine-grained features from WSI based on specific prompts without compromising inference speed. Leveraging a comprehensive dataset of 490,000 samples from diverse pathology tasks-including cancer detection, grading, vascular and neural invasion identification, and so on-we trained the pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that this model significantly outperforms existing methods in diagnostic accuracy and efficiency, offering an interactive, clinically aligned approach for auxiliary diagnosis in a wide range of pathology applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-QuAD: Multi-Level Quality-Adaptive Dynamic Network for Reliable Multimodal Classification</title>
<link>https://arxiv.org/abs/2412.14489</link>
<guid>https://arxiv.org/abs/2412.14489</guid>
<content:encoded><![CDATA[
arXiv:2412.14489v3 Announce Type: replace 
Abstract: Multimodal machine learning has achieved remarkable progress in many scenarios, but its reliability is undermined by varying sample quality. This paper finds that existing reliable multimodal classification methods not only fail to provide robust estimation of data quality, but also lack dynamic networks for sample-specific depth and parameters to achieve reliable inference. To this end, a novel framework for multimodal reliable classification termed \textit{Multi-level Quality-Adaptive Dynamic multimodal network} (Multi-QuAD) is proposed. Multi-QuAD first adopts a novel approach based on noise-free prototypes and a classifier-free design to reliably estimate the quality of each sample at both modality and feature levels. It then achieves sample-specific network depth via the \textbf{\textit{Global Confidence Normalized Depth (GCND)}} mechanism. By normalizing depth across modalities and samples, \textit{\textbf{GCND}} effectively mitigates the impact of challenging modality inputs on dynamic depth reliability. Furthermore, Multi-QuAD provides sample-adaptive network parameters via the \textbf{\textit{Layer-wise Greedy Parameter (LGP)}} mechanism driven by feature-level quality. The cross-modality layer-wise greedy strategy in \textbf{\textit{LGP}} designs a reliable parameter prediction paradigm for multimodal networks with variable architecture for the first time. Experiments conducted on four datasets demonstrate that Multi-QuAD significantly outperforms state-of-the-art methods in classification performance and reliability, exhibiting strong adaptability to data with diverse quality.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FusionSORT: Fusion Methods for Online Multi-object Visual Tracking</title>
<link>https://arxiv.org/abs/2501.00843</link>
<guid>https://arxiv.org/abs/2501.00843</guid>
<content:encoded><![CDATA[
arXiv:2501.00843v3 Announce Type: replace 
Abstract: In this work, we investigate four different fusion methods for associating detections to tracklets in multi-object visual tracking. In addition to considering strong cues such as motion and appearance information, we also consider weak cues such as height intersection-over-union (height-IoU) and tracklet confidence information in the data association using different fusion methods. These fusion methods include minimum, weighted sum based on IoU, Kalman filter (KF) gating, and hadamard product of costs due to the different cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and DanceTrack datasets, and find out that the choice of a fusion method is key for data association in multi-object visual tracking. We hope that this investigative work helps the computer vision research community to use the right fusion method for data association in multi-object visual tracking.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Tropical Cyclone Forecasting With Video Diffusion Models</title>
<link>https://arxiv.org/abs/2501.16003</link>
<guid>https://arxiv.org/abs/2501.16003</guid>
<content:encoded><![CDATA[
arXiv:2501.16003v5 Announce Type: replace 
Abstract: Tropical cyclone (TC) forecasting is crucial for disaster preparedness and mitigation. While recent deep learning approaches have shown promise, existing methods often treat TC evolution as a series of independent frame-to-frame predictions, limiting their ability to capture long-term dynamics. We present a novel application of video diffusion models for TC forecasting that explicitly models temporal dependencies through additional temporal layers. Our approach enables the model to generate multiple frames simultaneously, better capturing cyclone evolution patterns. We introduce a two-stage training strategy that significantly improves individual-frame quality and performance in low-data regimes. Experimental results show our method outperforms the previous approach of Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably, we extend the reliable forecasting horizon from 36 to 50 hours. Through comprehensive evaluation using both traditional metrics and Fr\'echet Video Distance (FVD), we demonstrate that our approach produces more temporally coherent forecasts while maintaining competitive single-frame quality. Code accessible at https://github.com/Ren-creater/forecast-video-diffmodels.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning</title>
<link>https://arxiv.org/abs/2502.00848</link>
<guid>https://arxiv.org/abs/2502.00848</guid>
<content:encoded><![CDATA[
arXiv:2502.00848v2 Announce Type: replace 
Abstract: Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, a.k.a., their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, e.g., the appearance of the Tesla Cybertruck. To this end, we present the first real-object-based retrieval-augmented generation framework (RealRAG), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by self-reflective contrastive learning, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to all types of state-of-the-art text-to-image generative models and also delivers remarkable performance boosts with all of them, such as a gain of 16.18% FID score with the auto-regressive model on the Stanford Car benchmark.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GP-GS: Gaussian Processes for Enhanced Gaussian Splatting</title>
<link>https://arxiv.org/abs/2502.02283</link>
<guid>https://arxiv.org/abs/2502.02283</guid>
<content:encoded><![CDATA[
arXiv:2502.02283v4 Announce Type: replace 
Abstract: 3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds often limits scene reconstruction quality. To address the limitation, this paper proposes a novel 3D reconstruction framework, Gaussian Processes enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian Process model is developed to enable adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. These densified point clouds provide high-quality initial 3D Gaussians, enhancing reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling</title>
<link>https://arxiv.org/abs/2502.02590</link>
<guid>https://arxiv.org/abs/2502.02590</guid>
<content:encoded><![CDATA[
arXiv:2502.02590v2 Announce Type: replace 
Abstract: 3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context. To address these limitations, we propose Articulate Anymesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints. Our experiments show that Articulate Anymesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system. Our Github website is https://articulate-anymesh.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</title>
<link>https://arxiv.org/abs/2502.04847</link>
<guid>https://arxiv.org/abs/2502.04847</guid>
<content:encoded><![CDATA[
arXiv:2502.04847v4 Announce Type: replace 
Abstract: Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2502.14908</link>
<guid>https://arxiv.org/abs/2502.14908</guid>
<content:encoded><![CDATA[
arXiv:2502.14908v2 Announce Type: replace 
Abstract: Vision language models (VLM) demonstrate sophisticated multimodal reasoning yet are prone to hallucination when confronted with knowledge conflicts, impeding their deployment in information-sensitive contexts. While existing research addresses robustness in unimodal models, the multimodal domain lacks systematic investigation of cross-modal knowledge conflicts. This research introduces \segsub, a framework for applying targeted image perturbations to investigate VLM resilience against knowledge conflicts. Our analysis reveals distinct vulnerability patterns: while VLMs are robust to parametric conflicts (20% adherence rates), they exhibit significant weaknesses in identifying counterfactual conditions (<30% accuracy) and resolving source conflicts (<1% accuracy). Correlations between contextual richness and hallucination rate (r = -0.368, p = 0.003) reveal the kinds of images that are likely to cause hallucinations. Through targeted fine-tuning on our benchmark dataset, we demonstrate improvements in VLM knowledge conflict detection, establishing a foundation for developing hallucination-resilient multimodal systems in information-sensitive environments.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical Inspired MRI Lesion Segmentation</title>
<link>https://arxiv.org/abs/2502.16032</link>
<guid>https://arxiv.org/abs/2502.16032</guid>
<content:encoded><![CDATA[
arXiv:2502.16032v2 Announce Type: replace 
Abstract: Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting pathological tissues in various diseases. Different MRI sequences have different contrast mechanisms and sensitivities for different types of lesions, which pose challenges to accurate and consistent lesion segmentation. In clinical practice, radiologists commonly use the sub-sequence feature, i.e. the difference between post contrast-enhanced T1-weighted (post) and pre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we propose a residual fusion method to learn subsequence representation for MRI lesion segmentation. Specifically, we iteratively and adaptively fuse features from pre- and post-contrast sequences at multiple resolutions, using dynamic weights to achieve optimal fusion and address diverse lesion enhancement patterns. Our method achieves state-of-the-art performances on BraTS2023 dataset for brain tumor segmentation and our in-house breast MRI dataset for breast lesion segmentation. Our method is clinically inspired and has the potential to facilitate lesion segmentation in various applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs</title>
<link>https://arxiv.org/abs/2502.19159</link>
<guid>https://arxiv.org/abs/2502.19159</guid>
<content:encoded><![CDATA[
arXiv:2502.19159v2 Announce Type: replace 
Abstract: Compared to width-wise pruning, depth-wise pruning can significantly accelerate inference in resource-constrained scenarios. However, treating the entire Transformer layer as the minimum pruning unit may degrade model performance by indiscriminately discarding the entire information of the layer. This paper reveals the ``Patch-like'' feature relationship between layers in large language models by analyzing the correlation of the outputs of different layers in the reproducing kernel Hilbert space. Building on this observation, we propose a sliding layer merging method that dynamically selects and fuses consecutive layers from top to bottom according to a pre-defined similarity threshold, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35% pruning on the Vicuna-7B model, our method achieved a 1.654% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at https://github.com/920927/SLM-a-sliding-layer-merging-method.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator</title>
<link>https://arxiv.org/abs/2503.01103</link>
<guid>https://arxiv.org/abs/2503.01103</guid>
<content:encoded><![CDATA[
arXiv:2503.01103v2 Announce Type: replace 
Abstract: While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of 1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512x512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Adaptive Gamma Context-Aware SSM-based Model for Metal Defect Detection</title>
<link>https://arxiv.org/abs/2503.01234</link>
<guid>https://arxiv.org/abs/2503.01234</guid>
<content:encoded><![CDATA[
arXiv:2503.01234v3 Announce Type: replace 
Abstract: Metal defect detection is critical in industrial quality assurance, yet existing methods struggle with grayscale variations and complex defect states, limiting its robustness. To address these challenges, this paper proposes a Self-Adaptive Gamma Context-Aware SSM-based model(GCM-DET). This advanced detection framework integrating a Dynamic Gamma Correction (GC) module to enhance grayscale representation and optimize feature extraction for precise defect reconstruction. A State-Space Search Management (SSM) architecture captures robust multi-scale features, effectively handling defects of varying shapes and scales. Focal Loss is employed to mitigate class imbalance and refine detection accuracy. Additionally, the CD5-DET dataset is introduced, specifically designed for port container maintenance, featuring significant grayscale variations and intricate defect patterns. Experimental results demonstrate that the proposed model achieves substantial improvements, with mAP@0.5 gains of 27.6\%, 6.6\%, and 2.6\% on the CD5-DET, NEU-DET, and GC10-DET datasets.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Layer Attention Efficiency through Pruning Redundant Retrievals</title>
<link>https://arxiv.org/abs/2503.06473</link>
<guid>https://arxiv.org/abs/2503.06473</guid>
<content:encoded><![CDATA[
arXiv:2503.06473v4 Announce Type: replace 
Abstract: Growing evidence suggests that layer attention mechanisms, which enhance interaction among layers in deep neural networks, have significantly advanced network architectures. However, existing layer attention methods suffer from redundancy, as attention weights learned by adjacent layers often become highly similar. This redundancy causes multiple layers to extract nearly identical features, reducing the model's representational capacity and increasing training time. To address this issue, we propose a novel approach to quantify redundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent layers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM) method that accurately identifies and skips redundant layers, thereby maintaining model stability. Our proposed Efficient Layer Attention (ELA) architecture, improves both training efficiency and overall performance, achieving a 30% reduction in training time while enhancing performance in tasks such as image classification and object detection.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction</title>
<link>https://arxiv.org/abs/2503.06587</link>
<guid>https://arxiv.org/abs/2503.06587</guid>
<content:encoded><![CDATA[
arXiv:2503.06587v2 Announce Type: replace 
Abstract: Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry reconstruction quality than the popular 3DGS by using 2D surfels to approximate thin surfaces. However, it falls short when dealing with glossy surfaces, resulting in visible holes in these areas. We found the reflection discontinuity causes the issue. To fit the jump from diffuse to specular reflection at different viewing angles, depth bias is introduced in the optimized Gaussian primitives. To address that, we first replace the depth distortion loss in 2DGS with a novel depth convergence loss, which imposes a strong constraint on depth continuity. Then, we rectified the depth criterion in determining the actual surface, which fully accounts for all the intersecting Gaussians along the ray. Qualitative and quantitative evaluations across various datasets reveal that our method significantly improves reconstruction quality, with more complete and accurate surfaces than 2DGS.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Referring to Any Person</title>
<link>https://arxiv.org/abs/2503.08507</link>
<guid>https://arxiv.org/abs/2503.08507</guid>
<content:encoded><![CDATA[
arXiv:2503.08507v2 Announce Type: replace 
Abstract: Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given a natural language description, a task we define as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, a novel dataset designed to tackle these challenges and better reflect real-world applications. From a model design perspective, we integrate a multimodal large language model with an object detection framework, constructing a robust referring model named RexSeek. Experimental results reveal that state-of-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEA-Research/RexSeek
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-camera orientation tracking method for anisotropic particles in particle-laden flows</title>
<link>https://arxiv.org/abs/2503.08694</link>
<guid>https://arxiv.org/abs/2503.08694</guid>
<content:encoded><![CDATA[
arXiv:2503.08694v2 Announce Type: replace 
Abstract: A method for particle orientation tracking is developed and demonstrated specifically for anisotropic particles. Using (high-speed) multi-camera recordings of anisotropic particles from different viewpoints, we reconstruct the 3D location and orientation of these particles using their known shape. This paper describes an algorithm which tracks the location and orientation of multiple anisotropic particles over time, enabling detailed investigations of location, orientation, and rotation statistics. The robustness and error of this method is quantified, and we explore the effects of noise, image size, the number of used cameras, and the camera arrangement by applying the algorithm to synthetic images. We showcase several use-cases of this method in several experiments (in both quiescent and turbulent fluids), demonstrating the effectiveness and broad applicability of the described tracking method. The proposed method is shown to work for widely different particle shapes, successfully tracks multiple particles simultaneously, and the method can distinguish between different types of particles.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network</title>
<link>https://arxiv.org/abs/2503.13179</link>
<guid>https://arxiv.org/abs/2503.13179</guid>
<content:encoded><![CDATA[
arXiv:2503.13179v2 Announce Type: replace 
Abstract: This study proposes a lightweight method for building image super-resolution using a Dilated Contextual Feature Modulation Network (DCFMN). The process includes obtaining high-resolution images, down-sampling them to low-resolution, enhancing the low-resolution images, constructing and training a lightweight network model, and generating super-resolution outputs. To address challenges such as regular textures and long-range dependencies in building images, the DCFMN integrates an expansion separable modulation unit and a local feature enhancement module. The former employs multiple expansion convolutions equivalent to a large kernel to efficiently aggregate multi-scale features while leveraging a simple attention mechanism for adaptivity. The latter encodes local features, mixes channel information, and ensures no additional computational burden during inference through reparameterization. This approach effectively resolves the limitations of existing lightweight super-resolution networks in modeling long-range dependencies, achieving accurate and efficient global feature modeling without increasing computational costs, and significantly improving both reconstruction quality and lightweight efficiency for building image super-resolution models.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Improving Motion Diffusion Models with Sparse Keyframes</title>
<link>https://arxiv.org/abs/2503.13859</link>
<guid>https://arxiv.org/abs/2503.13859</guid>
<content:encoded><![CDATA[
arXiv:2503.13859v2 Announce Type: replace 
Abstract: Recent advances in motion diffusion models have led to remarkable progress in diverse motion generation tasks, including text-to-motion synthesis. However, existing approaches represent motions as dense frame sequences, requiring the model to process redundant or less informative frames. The processing of dense animation frames imposes significant training complexity, especially when learning intricate distributions of large motion datasets even with modern neural architectures. This severely limits the performance of generative motion models for downstream tasks. Inspired by professional animators who mainly focus on sparse keyframes, we propose a novel diffusion framework explicitly designed around sparse and geometrically meaningful keyframes. Our method reduces computation by masking non-keyframes and efficiently interpolating missing frames. We dynamically refine the keyframe mask during inference to prioritize informative frames in later diffusion steps. Extensive experiments show that our approach consistently outperforms state-of-the-art methods in text alignment and motion realism, while also effectively maintaining high performance at significantly fewer diffusion steps. We further validate the robustness of our framework by using it as a generative prior and adapting it to different downstream tasks.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection</title>
<link>https://arxiv.org/abs/2503.13903</link>
<guid>https://arxiv.org/abs/2503.13903</guid>
<content:encoded><![CDATA[
arXiv:2503.13903v2 Announce Type: replace 
Abstract: Video object detection has made significant progress in recent years thanks to convolutional neural networks (CNNs) and vision transformers (ViTs). Typically, CNNs excel at capturing local features but struggle to model global representations. Conversely, ViTs are adept at capturing long-range global features but face challenges in representing local feature details. Off-the-shelf video object detection methods solely rely on CNNs or ViTs to conduct feature aggregation, which hampers their capability to simultaneously leverage global and local information, thereby resulting in limited detection performance. In this paper, we propose a Transformer-GraphFormer Blender Network (TGBFormer) for video object detection, with three key technical improvements to fully exploit the advantages of transformers and graph convolutional networks while compensating for their limitations. First, we develop a spatial-temporal transformer module to aggregate global contextual information, constituting global representations with long-range feature dependencies. Second, we introduce a spatial-temporal GraphFormer module that utilizes local spatial and temporal relationships to aggregate features, generating new local representations that are complementary to the transformer outputs. Third, we design a global-local feature blender module to adaptively couple transformer-based global representations and GraphFormer-based local representations. Extensive experiments demonstrate that our TGBFormer establishes new state-of-the-art results on the ImageNet VID dataset. Particularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS on a single Tesla A100 GPU.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision Centric Remote Sensing Benchmark</title>
<link>https://arxiv.org/abs/2503.15816</link>
<guid>https://arxiv.org/abs/2503.15816</guid>
<content:encoded><![CDATA[
arXiv:2503.15816v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision-language tasks but their remote sensing (RS) counterpart are relatively under explored. Unlike natural images, RS imagery presents unique challenges that current MLLMs struggle to handle, particularly in visual grounding and spatial reasoning. This study investigates the limitations of CLIP-based MLLMs in RS, highlighting their failure to differentiate visually distinct yet semantically similar RS images. To address this, we introduce a remote sensing multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models incorrectly assign high similarity scores to visually distinct RS images. Through a visual question answering (VQA) evaluation, we analyze the performance of state-of-the-art MLLMs, revealing significant limitations in RS specific representation learning. The results provide valuable insights into the weaknesses of CLIP-based visual encoding and offer a foundation for future research to develop more effective MLLMs tailored for remote sensing applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation</title>
<link>https://arxiv.org/abs/2503.15831</link>
<guid>https://arxiv.org/abs/2503.15831</guid>
<content:encoded><![CDATA[
arXiv:2503.15831v2 Announce Type: replace 
Abstract: Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2503.15970</link>
<guid>https://arxiv.org/abs/2503.15970</guid>
<content:encoded><![CDATA[
arXiv:2503.15970v2 Announce Type: replace 
Abstract: Facial Expression Recognition (FER) plays a crucial role in human affective analysis and has been widely applied in computer vision tasks such as human-computer interaction and psychological assessment. The 8th Affective Behavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions using the video-based Aff-Wild2 dataset. This challenge includes various tasks, including the video-based EXPR recognition track, which is our primary focus. In this paper, we demonstrate that addressing label ambiguity and class imbalance, which are known to cause performance degradation, can lead to meaningful performance improvements. Specifically, we propose Video-based Noise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to each frame in a clip to address label ambiguity and effectively capture temporal variations in facial expressions. Furthermore, we introduce a simple and effective augmentation strategy to reduce redundancy between consecutive frames, which is a primary cause of overfitting. Through extensive experiments, we validate the effectiveness of our approach, demonstrating significant improvements in video-based FER performance.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2503.16188</link>
<guid>https://arxiv.org/abs/2503.16188</guid>
<content:encoded><![CDATA[
arXiv:2503.16188v4 Announce Type: replace 
Abstract: This paper investigates the role of explicit thinking process in rule-based reinforcement fine-tuning (RFT) for MLLMs. We first propose CLS-RL for MLLM image classification, using verifiable rewards for fine-tuning. Experiments show CLS-RL significantly outperforms SFT and yields a cross-dataset generalization effect. We then rethink and question whether explicit thinking in RFT is always necessary. Challenging the convention that explicit thinking is crucial for the success of RFT, we introduce No-Thinking-RL, exploring RFT without thinking by introducing a simple equality accuracy reward. We evaluate No-Thinking-RL on 6 diverse tasks across different model sizes and types. Experimental results reveal three key findings: 1). Visual perception tasks do not require thinking during RFT, as No-Thinking-RL consistently outperforms or matches Thinking-based RFT across model sizes. 2).} Models with limited capabilities struggle to generate high-quality CoT for RFT, making Thinking-based RFT less effective than No-Thinking-RL. 3). There are inconsistencies between the answers in the thinking and answer tags for some responses of thinking-based RFT, which show lower accuracy than the overall accuracy. We hypothesize that explicit thinking before verifiable answers may hinder reward convergence and reduce performance. To test this hypothesis, we propose Think-After-Answer, which places thinking after the answer to mitigate this effect for experimental verification. Lastly, we conduct a pilot study to explore whether MLLMs can learn when to think during RFT, introducing an Adaptive-Thinking method. Experiments show that it converges to a specific prompt depending on model capability and task complexity, achieving comparable or better performance than both Thinking and No-Thinking-RL. This suggests MLLMs can adaptively decide to think or not based on their capabilities and task complexity.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2504.04893</link>
<guid>https://arxiv.org/abs/2504.04893</guid>
<content:encoded><![CDATA[
arXiv:2504.04893v3 Announce Type: replace 
Abstract: Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In this paper, we introduce SCAM, the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words. Through extensive benchmarking of Vision-Language Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly degrade performance, and identify that training data and model architecture influence the susceptibility to these attacks. Our findings reveal that typographic attacks persist in state-of-the-art Large Vision-Language Models (LVLMs) due to the choice of their vision encoder, though larger Large Language Models (LLMs) backbones help mitigate their vulnerability. Additionally, we demonstrate that synthetic attacks closely resemble real-world (handwritten) attacks, validating their use in research. Our work provides a comprehensive resource and empirical insights to facilitate future research toward robust and trustworthy multimodal AI systems. We publicly release the datasets introduced in this paper along with the code for evaluations at www.bliss.berlin/research/scam.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FANeRV: Frequency Separation and Augmentation based Neural Representation for Video</title>
<link>https://arxiv.org/abs/2504.06755</link>
<guid>https://arxiv.org/abs/2504.06755</guid>
<content:encoded><![CDATA[
arXiv:2504.06755v3 Announce Type: replace 
Abstract: Neural representations for video (NeRV) have gained considerable attention for their strong performance across various video tasks. However, existing NeRV methods often struggle to capture fine spatial details, resulting in vague reconstructions. In this paper, we present a Frequency Separation and Augmentation based Neural Representation for video (FANeRV), which addresses these limitations with its core Wavelet Frequency Upgrade Block. This block explicitly separates input frames into high and low-frequency components using discrete wavelet transform, followed by targeted enhancement using specialized modules. Finally, a specially designed gated network effectively fuses these frequency components for optimal reconstruction. Additionally, convolutional residual enhancement blocks are integrated into the later stages of the network to balance parameter distribution and improve the restoration of high-frequency details. Experimental results demonstrate that FANeRV significantly improves reconstruction performance and excels in multiple tasks, including video compression, inpainting, and interpolation, outperforming existing NeRV methods.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Wide-Angle Image Using Narrow-Angle View of the Same Scene</title>
<link>https://arxiv.org/abs/2504.09455</link>
<guid>https://arxiv.org/abs/2504.09455</guid>
<content:encoded><![CDATA[
arXiv:2504.09455v2 Announce Type: replace 
Abstract: A common dilemma while photographing a scene is whether to capture it at a wider angle, allowing more of the scene to be covered but in less detail or to click in a narrow angle that captures better details but leaves out portions of the scene. We propose a novel method in this paper that infuses wider shots with finer quality details that is usually associated with an image captured by the primary lens by capturing the same scene using both narrow and wide field of view (FoV) lenses. We do so by training a Generative Adversarial Network (GAN)-based model to learn to extract the visual quality parameters from a narrow-angle shot and to transfer these to the corresponding wide-angle image of the scene using residual connections and an attention-based fusion module. We have mentioned in details the proposed technique to isolate the visual essence of an image and to transfer it into another image. We have also elaborately discussed our implementation details and have presented the results of evaluation over several benchmark datasets and comparisons with contemporary advancements in the field.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FocusedAD: Character-centric Movie Audio Description</title>
<link>https://arxiv.org/abs/2504.12157</link>
<guid>https://arxiv.org/abs/2504.12157</guid>
<content:encoded><![CDATA[
arXiv:2504.12157v3 Announce Type: replace 
Abstract: Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForgetMe: Evaluating Selective Forgetting in Generative Models</title>
<link>https://arxiv.org/abs/2504.12574</link>
<guid>https://arxiv.org/abs/2504.12574</guid>
<content:encoded><![CDATA[
arXiv:2504.12574v2 Announce Type: replace 
Abstract: The widespread adoption of diffusion models in image generation has increased the demand for privacy-compliant unlearning. However, due to the high-dimensional nature and complex feature representations of diffusion models, achieving selective unlearning remains challenging, as existing methods struggle to remove sensitive information while preserving the consistency of non-sensitive regions. To address this, we propose an Automatic Dataset Creation Framework based on prompt-based layered editing and training-free local feature removal, constructing the ForgetMe dataset and introducing the Entangled evaluation metric. The Entangled metric quantifies unlearning effectiveness by assessing the similarity and consistency between the target and background regions and supports both paired (Entangled-D) and unpaired (Entangled-S) image data, enabling unsupervised evaluation. The ForgetMe dataset encompasses a diverse set of real and synthetic scenarios, including CUB-200-2011 (Birds), Stanford-Dogs, ImageNet, and a synthetic cat dataset. We apply LoRA fine-tuning on Stable Diffusion to achieve selective unlearning on this dataset and validate the effectiveness of both the ForgetMe dataset and the Entangled metric, establishing them as benchmarks for selective unlearning. Our work provides a scalable and adaptable solution for advancing privacy-preserving generative AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compile Scene Graphs with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13617</link>
<guid>https://arxiv.org/abs/2504.13617</guid>
<content:encoded><![CDATA[
arXiv:2504.13617v3 Announce Type: replace 
Abstract: Next-token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. We design a set of graph-centric rewards, including three recall-based variants -- Hard Recall, Hard Recall+Relax, and Soft Recall -- which evaluate semantic and spatial alignment between predictions and ground truth at the object and relation levels. A format consistency reward further ensures that outputs follow the expected structural schema. Extensive experiments on the VG150 and PSG benchmarks show that R1-SGG substantially reduces failure rates and achieves strong performance in Recall and mean Recall, surpassing traditional SGG models and existing multimodal language models. Our code is available at https://github.com/gpt4vision/R1-SGG
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Sound Source Localization with Joint Slot Attention on Image and Audio</title>
<link>https://arxiv.org/abs/2504.15118</link>
<guid>https://arxiv.org/abs/2504.15118</guid>
<content:encoded><![CDATA[
arXiv:2504.15118v2 Announce Type: replace 
Abstract: Sound source localization (SSL) is the task of locating the source of sound within an image. Due to the lack of localization labels, the de facto standard in SSL has been to represent an image and audio as a single embedding vector each, and use them to learn SSL via contrastive learning. To this end, previous work samples one of local image features as the image embedding and aggregates all local audio features to obtain the audio embedding, which is far from optimal due to the presence of noise and background irrelevant to the actual target in the input. We present a novel SSL method that addresses this chronic issue by joint slot attention on image and audio. To be specific, two slots competitively attend image and audio features to decompose them into target and off-target representations, and only target representations of image and audio are used for contrastive learning. Also, we introduce cross-modal attention matching to further align local features of image and audio. Our method achieved the best in almost all settings on three public benchmarks for SSL, and substantially outperformed all the prior work in cross-modal retrieval.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs</title>
<link>https://arxiv.org/abs/2504.17040</link>
<guid>https://arxiv.org/abs/2504.17040</guid>
<content:encoded><![CDATA[
arXiv:2504.17040v2 Announce Type: replace 
Abstract: We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses, we demonstrate that DToMe effectively adapts token reduction based on image complexity and, unlike existing systems, provides users more control over computational costs. Project page: https://mikewangwzhl.github.io/dymu/.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TableCenterNet: A one-stage network for table structure recognition</title>
<link>https://arxiv.org/abs/2504.17522</link>
<guid>https://arxiv.org/abs/2504.17522</guid>
<content:encoded><![CDATA[
arXiv:2504.17522v2 Announce Type: replace 
Abstract: Table structure recognition aims to parse tables in unstructured data into machine-understandable formats. Recent methods address this problem through a two-stage process or optimized one-stage approaches. However, these methods either require multiple networks to be serially trained and perform more time-consuming sequential decoding, or rely on complex post-processing algorithms to parse the logical structure of tables. They struggle to balance cross-scenario adaptability, robustness, and computational efficiency. In this paper, we propose a one-stage end-to-end table structure parsing network called TableCenterNet. This network unifies the prediction of table spatial and logical structure into a parallel regression task for the first time, and implicitly learns the spatial-logical location mapping laws of cells through a synergistic architecture of shared feature extraction layers and task-specific decoding. Compared with two-stage methods, our method is easier to train and faster to infer. Experiments on benchmark datasets show that TableCenterNet can effectively parse table structures in diverse scenarios and achieve state-of-the-art performance on the TableGraph-24k dataset. Code is available at https://github.com/dreamy-xay/TableCenterNet.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras</title>
<link>https://arxiv.org/abs/2504.18864</link>
<guid>https://arxiv.org/abs/2504.18864</guid>
<content:encoded><![CDATA[
arXiv:2504.18864v2 Announce Type: replace 
Abstract: The need for accurate and non-intrusive flow measurement methods has led to the widespread adoption of Particle Image Velocimetry (PIV), a powerful diagnostic tool in fluid motion estimation. This study investigates the tremendous potential of spike cameras (a type of ultra-high-speed, high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike Imaging Velocimetry (SIV), designed specifically for highly turbulent and intricate flow fields. To aggregate motion features from the spike stream while minimizing information loss, we incorporate a Detail-Preserving Hierarchical Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to extract contextual features from highly complex fluid flows. Furthermore, we present a spike-based PIV dataset, Particle Scenes with Spike and Displacement (PSSD), which provides labeled data for three challenging fluid dynamics scenarios. Our proposed method achieves superior performance compared to existing baseline methods on PSSD. The datasets and our implementation of SIV are open-sourced in the supplementary materials.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems</title>
<link>https://arxiv.org/abs/2311.11796</link>
<guid>https://arxiv.org/abs/2311.11796</guid>
<content:encoded><![CDATA[
arXiv:2311.11796v2 Announce Type: replace-cross 
Abstract: As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gemini: A Family of Highly Capable Multimodal Models</title>
<link>https://arxiv.org/abs/2312.11805</link>
<guid>https://arxiv.org/abs/2312.11805</guid>
<content:encoded><![CDATA[
arXiv:2312.11805v5 Announce Type: replace-cross 
Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond DAGs: A Latent Partial Causal Model for Multimodal Learning</title>
<link>https://arxiv.org/abs/2402.06223</link>
<guid>https://arxiv.org/abs/2402.06223</guid>
<content:encoded><![CDATA[
arXiv:2402.06223v2 Announce Type: replace-cross 
Abstract: Directed acyclic graphs (DAGs) are fundamental graph structures in causal modeling, but identifying the desired DAG from observational data often requires strong assumptions that may not hold in real-world scenarios, especially for latent causal models and complex multimodal data. This raises the question of whether we can relax or bypass the DAG assumption while maintaining practical utility. In this work, we propose a novel latent partial causal model for multimodal data, featuring two latent coupled variables, connected by an undirected edge, to represent the transfer of knowledge across modalities. Under specific statistical assumptions, we establish an identifiability result, demonstrating that representations learned by multimodal contrastive learning correspond to the latent coupled variables up to a trivial transformation. This result deepens our understanding of the why multimodal contrastive learning works, highlights its potential for disentanglement, and expands the utility of pre-trained models like CLIP. Synthetic experiments confirm the robustness of our findings, even when the assumptions are partially violated. Most importantly, experiments on a pre-trained CLIP model embodies disentangled representations, enabling few-shot learning and improving domain generalization across diverse real-world datasets. Together, these contributions push the boundaries of multimodal contrastive learning, both theoretically and, crucially, in practical applications.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples</title>
<link>https://arxiv.org/abs/2404.19460</link>
<guid>https://arxiv.org/abs/2404.19460</guid>
<content:encoded><![CDATA[
arXiv:2404.19460v3 Announce Type: replace-cross 
Abstract: Adversarial examples are typically optimized with gradient-based attacks. While novel attacks are continuously proposed, each is shown to outperform its predecessors using different experimental setups, hyperparameter settings, and number of forward and backward calls to the target models. This provides overly-optimistic and even biased evaluations that may unfairly favor one particular attack over the others. In this work, we aim to overcome these limitations by proposing AttackBench, i.e., the first evaluation framework that enables a fair comparison among different attacks. To this end, we first propose a categorization of gradient-based attacks, identifying their main components and differences. We then introduce our framework, which evaluates their effectiveness and efficiency. We measure these characteristics by (i) defining an optimality metric that quantifies how close an attack is to the optimal solution, and (ii) limiting the number of forward and backward queries to the model, such that all attacks are compared within a given maximum query budget. Our extensive experimental analysis compares more than $100$ attack implementations with a total of over $800$ different configurations against CIFAR-10 and ImageNet models, highlighting that only very few attacks outperform all the competing approaches. Within this analysis, we shed light on several implementation issues that prevent many attacks from finding better solutions or running at all. We release AttackBench as a publicly-available benchmark, aiming to continuously update it to include and evaluate novel gradient-based attacks for optimizing adversarial examples.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2408.14419</link>
<guid>https://arxiv.org/abs/2408.14419</guid>
<content:encoded><![CDATA[
arXiv:2408.14419v2 Announce Type: replace-cross 
Abstract: We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models. CHARTOM consists of specially designed data visualizing charts. Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question). Both questions have significant societal benefits. We detail the construction of the CHARTOM benchmark including its calibration on human performance. We benchmark leading LLMs as of late 2024 - including GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset and found that our benchmark was challenging to all of them, suggesting room for future large language models to improve.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering</title>
<link>https://arxiv.org/abs/2409.02426</link>
<guid>https://arxiv.org/abs/2409.02426</guid>
<content:encoded><![CDATA[
arXiv:2409.02426v3 Announce Type: replace-cross 
Abstract: Recent empirical studies have demonstrated that diffusion models can effectively learn the image distribution and generate new samples. Remarkably, these models can achieve this even with a small number of training samples despite a large image dimension, circumventing the curse of dimensionality. In this work, we provide theoretical insights into this phenomenon by leveraging key empirical observations: (i) the low intrinsic dimensionality of image data, (ii) a union of manifold structure of image data, and (iii) the low-rank property of the denoising autoencoder in trained diffusion models. These observations motivate us to assume the underlying data distribution of image data as a mixture of low-rank Gaussians and to parameterize the denoising autoencoder as a low-rank model according to the score function of the assumed distribution. With these setups, we rigorously show that optimizing the training loss of diffusion models is equivalent to solving the canonical subspace clustering problem over the training samples. Based on this equivalence, we further show that the minimal number of samples required to learn the underlying distribution scales linearly with the intrinsic dimensions under the above data and model assumptions. This insight sheds light on why diffusion models can break the curse of dimensionality and exhibit the phase transition in learning distributions. Moreover, we empirically establish a correspondence between the subspaces and the semantic representations of image data, facilitating image editing. We validate these results with corroborated experimental results on both simulated distributions and image datasets.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2410.13439</link>
<guid>https://arxiv.org/abs/2410.13439</guid>
<content:encoded><![CDATA[
arXiv:2410.13439v4 Announce Type: replace-cross 
Abstract: Supervised contrastive learning has achieved remarkable success by leveraging label information; however, determining positive samples in multi-label scenarios remains a critical challenge. In multi-label supervised contrastive learning (MSCL), multi-label relations are not yet fully defined, leading to ambiguity in identifying positive samples and formulating contrastive loss functions to construct the representation space. To address these challenges, we: (i) first define five distinct multi-label relations in MSCL to systematically identify positive samples, (ii) introduce a novel Similarity-Dissimilarity Loss that dynamically re-weights samples through computing the similarity and dissimilarity factors between positive samples and given anchors based on multi-label relations, and (iii) further provide theoretical grounded proofs for our method through rigorous mathematical analysis that supports the formulation and effectiveness of the proposed loss function. We conduct the experiments across both image and text modalities, and extend the evaluation to medical domain. The results demonstrate that our method consistently outperforms baselines in a comprehensive evaluation, confirming its effectiveness and robustness. Code is available at: https://github.com/guangminghuang/similarity-dissimilarity-loss.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering</title>
<link>https://arxiv.org/abs/2410.21000</link>
<guid>https://arxiv.org/abs/2410.21000</guid>
<content:encoded><![CDATA[
arXiv:2410.21000v3 Announce Type: replace-cross 
Abstract: Medical Visual Question Answering (MedVQA) has attracted growing interest at the intersection of medical image understanding and natural language processing for clinical applications. By interpreting medical images and providing precise answers to relevant clinical inquiries, MedVQA has the potential to support diagnostic decision-making and reduce workload across various fields like radiology. While recent approaches rely heavily on unified large pre-trained Visual-Language Models, research on more efficient fusion mechanisms remains relatively limited in this domain. In this paper, we introduce a fusion model, OMniBAN, that integrates Orthogonality loss, Multi-head attention, and a Bilinear Attention Network to achieve high computational efficiency as well as solid performance. We conduct comprehensive experiments and demonstrate how bilinear attention fusion can approximate the performance of larger fusion models like cross-modal Transformer. Our results show that OMniBAN requires fewer parameters (approximately 2/3 of Transformer-based Co-Attention) and substantially lower FLOPs (approximately 1/4), while achieving comparable overall performance and even slight improvements on closed-ended questions on two key MedVQA benchmarks. This balance between efficiency and accuracy suggests that OMniBAN could be a viable option for real-world medical image question answering, where computational resources are often constrained.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relationships between the degrees of freedom in the affine Gaussian derivative model for visual receptive fields and 2-D affine image transformations, with application to covariance properties of simple cells in the primary visual cortex</title>
<link>https://arxiv.org/abs/2411.05673</link>
<guid>https://arxiv.org/abs/2411.05673</guid>
<content:encoded><![CDATA[
arXiv:2411.05673v3 Announce Type: replace-cross 
Abstract: When observing the surface patterns of objects delimited by smooth surfaces, the projections of the surface patterns to the image domain will be subject to substantial variabilities, as induced by variabilities in the geometric viewing conditions, and as generated by either monocular or binocular imaging conditions, or by relative motions between the object and the observer over time. To first order of approximation, the image deformations of such projected surface patterns can be modelled as local linearizations in terms of local 2-D spatial affine transformations.
  This paper presents a theoretical analysis of relationships between the degrees of freedom in 2-D spatial affine image transformations and the degrees of freedom in the affine Gaussian derivative model for visual receptive fields. For this purpose, we first describe a canonical decomposition of 2-D affine transformations on a product form, closely related to a singular value decomposition, while in closed form, and which reveals the degrees of freedom in terms of (i) uniform scaling transformations, (ii) an overall amount of global rotation, (iii) a complementary non-uniform scaling transformation and (iv) a relative normalization to a preferred symmetry orientation in the image domain. Then, we show how these degrees of freedom relate to the degrees of freedom in the affine Gaussian derivative model.
  Finally, we use these theoretical results to consider whether we could regard the biological receptive fields in the primary visual cortex of higher mammals as being able to span the degrees of freedom of 2-D spatial affine transformations, based on interpretations of existing neurophysiological experimental results.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Shot Real-to-Sim via End-to-End Differentiable Simulation and Rendering</title>
<link>https://arxiv.org/abs/2412.00259</link>
<guid>https://arxiv.org/abs/2412.00259</guid>
<content:encoded><![CDATA[
arXiv:2412.00259v4 Announce Type: replace-cross 
Abstract: Identifying predictive world models for robots in novel environments from sparse online observations is essential for robot task planning and execution in novel environments. However, existing methods that leverage differentiable programming to identify world models are incapable of jointly optimizing the geometry, appearance, and physical properties of the scene. In this work, we introduce a novel rigid object representation that allows the joint identification of these properties. Our method employs a novel differentiable point-based geometry representation coupled with a grid-based appearance field, which allows differentiable object collision detection and rendering. Combined with a differentiable physical simulator, we achieve end-to-end optimization of world models, given the sparse visual and tactile observations of a physical motion sequence. Through a series of world model identification tasks in simulated and real environments, we show that our method can learn both simulation- and rendering-ready world models from only one robot action sequence. The code and additional videos are available at our project website: https://tianyi20.github.io/rigid-world-model.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous Navigation Application</title>
<link>https://arxiv.org/abs/2412.03887</link>
<guid>https://arxiv.org/abs/2412.03887</guid>
<content:encoded><![CDATA[
arXiv:2412.03887v4 Announce Type: replace-cross 
Abstract: Maritime environmental sensing requires overcoming challenges from complex conditions such as harsh weather, platform perturbations, large dynamic objects, and the requirement for long detection ranges. While cameras and LiDAR are commonly used in ground vehicle navigation, their applicability in maritime settings is limited by range constraints and hardware maintenance issues. Radar sensors, however, offer robust long-range detection capabilities and resilience to physical contamination from weather and saline conditions, making it a powerful sensor for maritime navigation. Among various radar types, X-band radar is widely employed for maritime vessel navigation, providing effective long-range detection essential for situational awareness and collision avoidance. Nevertheless, it exhibits limitations during berthing operations where near-field detection is critical. To address this shortcoming, we incorporate W-band radar, which excels in detecting nearby objects with a higher update rate. We present a comprehensive maritime sensor dataset featuring multi-range detection capabilities. This dataset integrates short-range LiDAR data, medium-range W-band radar data, and long-range X-band radar data into a unified framework. Additionally, it includes object labels for oceanic object detection usage, derived from radar and stereo camera images. The dataset comprises seven sequences collected from diverse regions with varying levels of \bl{navigation algorithm} estimation difficulty, ranging from easy to challenging, and includes common locations suitable for global localization tasks. This dataset serves as a valuable resource for advancing research in place recognition, odometry estimation, SLAM, object detection, and dynamic object elimination within maritime environments. Dataset can be found at https://sites.google.com/view/rpmmoana.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stereo Hand-Object Reconstruction for Human-to-Robot Handover</title>
<link>https://arxiv.org/abs/2412.07487</link>
<guid>https://arxiv.org/abs/2412.07487</guid>
<content:encoded><![CDATA[
arXiv:2412.07487v3 Announce Type: replace-cross 
Abstract: Jointly estimating hand and object shape facilitates the grasping task in human-to-robot handovers. However, relying on hand-crafted prior knowledge about the geometric structure of the object fails when generalising to unseen objects, and depth sensors fail to detect transparent objects such as drinking glasses. In this work, we propose a stereo-based method for hand-object reconstruction that combines single-view reconstructions probabilistically to form a coherent stereo reconstruction. We learn 3D shape priors from a large synthetic hand-object dataset to ensure that our method is generalisable, and use RGB inputs to better capture transparent objects. We show that our method reduces the object Chamfer distance compared to existing RGB based hand-object reconstruction methods on single view and stereo settings. We process the reconstructed hand-object shape with a projection-based outlier removal step and use the output to guide a human-to-robot handover pipeline with wide-baseline stereo RGB cameras. Our hand-object reconstruction enables a robot to successfully receive a diverse range of household objects from the human.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</title>
<link>https://arxiv.org/abs/2501.19047</link>
<guid>https://arxiv.org/abs/2501.19047</guid>
<content:encoded><![CDATA[
arXiv:2501.19047v4 Announce Type: replace-cross 
Abstract: To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation</title>
<link>https://arxiv.org/abs/2502.05485</link>
<guid>https://arxiv.org/abs/2502.05485</guid>
<content:encoded><![CDATA[
arXiv:2502.05485v4 Announce Type: replace-cross 
Abstract: Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results, code, and dataset are provided at: https://hamster-robot.github.io/
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaWorld: Learning Adaptable World Models with Latent Actions</title>
<link>https://arxiv.org/abs/2503.18938</link>
<guid>https://arxiv.org/abs/2503.18938</guid>
<content:encoded><![CDATA[
arXiv:2503.18938v2 Announce Type: replace-cross 
Abstract: World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging State Space Models in Long Range Genomics</title>
<link>https://arxiv.org/abs/2504.06304</link>
<guid>https://arxiv.org/abs/2504.06304</guid>
<content:encoded><![CDATA[
arXiv:2504.06304v2 Announce Type: replace-cross 
Abstract: Long-range dependencies are critical for understanding genomic structure and function, yet most conventional methods struggle with them. Widely adopted transformer-based models, while excelling at short-context tasks, are limited by the attention module's quadratic computational complexity and inability to extrapolate to sequences longer than those seen in training. In this work, we explore State Space Models (SSMs) as a promising alternative by benchmarking two SSM-inspired architectures, Caduceus and Hawk, on long-range genomics modeling tasks under conditions parallel to a 50M parameter transformer baseline. We discover that SSMs match transformer performance and exhibit impressive zero-shot extrapolation across multiple tasks, handling contexts 10 to 100 times longer than those seen during training, indicating more generalizable representations better suited for modeling the long and complex human genome. Moreover, we demonstrate that these models can efficiently process sequences of 1M tokens on a single GPU, allowing for modeling entire genomic regions at once, even in labs with limited compute. Our findings establish SSMs as efficient and scalable for long-context genomic analysis.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoLa: B-Rep Generation using a Holistic Latent Representation</title>
<link>https://arxiv.org/abs/2504.14257</link>
<guid>https://arxiv.org/abs/2504.14257</guid>
<content:encoded><![CDATA[
arXiv:2504.14257v3 Announce Type: replace-cross 
Abstract: We introduce a novel representation for learning and generating Computer-Aided Design (CAD) models in the form of $\textit{boundary representations}$ (B-Reps). Our representation unifies the continuous geometric properties of B-Rep primitives in different orders (e.g., surfaces and curves) and their discrete topological relations in a $\textit{holistic latent}$ (HoLa) space. This is based on the simple observation that the topological connection between two surfaces is intrinsically tied to the geometry of their intersecting curve. Such a prior allows us to reformulate topology learning in B-Reps as a geometric reconstruction problem in Euclidean space. Specifically, we eliminate the presence of curves, vertices, and all the topological connections in the latent space by learning to distinguish and derive curve geometries from a pair of surface primitives via a neural intersection network. To this end, our holistic latent space is only defined on surfaces but encodes a full B-Rep model, including the geometry of surfaces, curves, vertices, and their topological relations. Our compact and holistic latent space facilitates the design of a first diffusion-based generator to take on a large variety of inputs including point clouds, single/multi-view images, 2D sketches, and text prompts. Our method significantly reduces ambiguities, redundancies, and incoherences among the generated B-Rep primitives, as well as training complexities inherent in prior multi-step B-Rep learning pipelines, while achieving greatly improved validity rate over current state of the art: 82% vs. $\approx$50%.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniAudio: Generating Spatial Audio from 360-Degree Video</title>
<link>https://arxiv.org/abs/2504.14906</link>
<guid>https://arxiv.org/abs/2504.14906</guid>
<content:encoded><![CDATA[
arXiv:2504.14906v2 Announce Type: replace-cross 
Abstract: Traditional video-to-audio generation techniques primarily focus on field-of-view (FoV) video and non-spatial audio, often missing the spatial cues necessary for accurately representing sound sources in 3D environments. To address this limitation, we introduce a novel task, 360V2SA, to generate spatial audio from 360-degree videos, specifically producing First-order Ambisonics (FOA) audio - a standard format for representing 3D spatial audio that captures sound directionality and enables realistic 3D audio reproduction. We first create Sphere360, a novel dataset tailored for this task that is curated from real-world data. We also design an efficient semi-automated pipeline for collecting and cleaning paired video-audio data. To generate spatial audio from 360-degree video, we propose a novel framework OmniAudio, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a dual-branch framework that utilizes both panoramic and FoV video inputs to capture comprehensive local and global information from 360-degree videos. Experimental results demonstrate that OmniAudio achieves state-of-the-art performance across both objective and subjective metrics on Sphere360. Code and datasets will be released at https://github.com/liuhuadai/OmniAudio. The demo page is available at https://OmniAudio-360V2SA.github.io.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography</title>
<link>https://arxiv.org/abs/2504.19200</link>
<guid>https://arxiv.org/abs/2504.19200</guid>
<content:encoded><![CDATA[
arXiv:2504.19200v2 Announce Type: replace-cross 
Abstract: In situ synchrotron X-ray computed tomography enables dynamic material studies, but automated segmentation remains challenging due to complex imaging artefacts and limited training data. We present a methodology for deep learning-based segmentation by transforming high-quality ex situ laboratory data to train models for binary segmentation of in situ synchrotron data, demonstrated through copper oxide dissolution studies. Using a modified SegFormer architecture, our approach achieves high segmentation performance on unseen data while reducing processing time from hours to seconds per 3D dataset. The method maintains consistent performance over significant morphological changes during experiments, despite training only on static specimens. This methodology can be readily applied to diverse materials systems, accelerating the analysis of time-resolved tomographic data across scientific disciplines.
]]></content:encoded>
<pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data extraction and processing methods to aid the study of driving behaviors at intersections in naturalistic driving</title>
<link>https://arxiv.org/abs/2505.05487</link>
<guid>https://arxiv.org/abs/2505.05487</guid>
<content:encoded><![CDATA[
<div> intersection detection, head pose estimation, object detection, naturalistic driving studies, automated video processing

Summary:
Intersection detection and driver head pose estimation were successfully carried out using a custom-developed algorithm on data collected from in-car recording systems. The algorithm accurately detected intersection signage and driving maneuvers in the majority of instances. The detection of vehicle entry into intersections had a small error margin and the overlap between ground truth and estimated intersection bounds was high. Object detection using YOLO models successfully identified traffic lights, stop signs, pedestrians, and other vehicles on the road. Turning maneuvers were independently detected using vehicle self-motion patterns, and stop lines on the road surface were identified through changing intensity patterns over time. The algorithm correctly inferred intersection type, maneuver, and bounds using scene videos and speed data. Overall, the automated video processing algorithm demonstrated high accuracy and reliability in characterizing driver behavior at intersections in naturalistic driving studies.<br /><br />Summary: <div>
arXiv:2505.05487v1 Announce Type: new 
Abstract: Naturalistic driving studies use devices in participants' own vehicles to record daily driving over many months. Due to diverse and extensive amounts of data recorded, automated processing is necessary. This report describes methods to extract and characterize driver head scans at intersections from data collected from an in-car recording system that logged vehicle speed, GPS location, scene videos, and cabin videos. Custom tools were developed to mark the intersections, synchronize location and video data, and clip the cabin and scene videos for +/-100 meters from the intersection location. A custom-developed head pose detection AI model for wide angle head turns was run on the cabin videos to estimate the driver head pose, from which head scans >20 deg were computed in the horizontal direction. The scene videos were processed using a YOLO object detection model to detect traffic lights, stop signs, pedestrians, and other vehicles on the road. Turning maneuvers were independently detected using vehicle self-motion patterns. Stop lines on the road surface were detected using changing intensity patterns over time as the vehicle moved. The information obtained from processing the scene videos, along with the speed data was used in a rule-based algorithm to infer the intersection type, maneuver, and bounds. We processed 190 intersections from 3 vehicles driven in cities and suburban areas from Massachusetts and California. The automated video processing algorithm correctly detected intersection signage and maneuvers in 100% and 94% of instances, respectively. The median [IQR] error in detecting vehicle entry into the intersection was 1.1[0.4-4.9] meters and 0.2[0.1-0.54] seconds. The median overlap between ground truth and estimated intersection bounds was 0.88[0.82-0.93].
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Events to Enhancement: A Survey on Event-Based Imaging Technologies</title>
<link>https://arxiv.org/abs/2505.05488</link>
<guid>https://arxiv.org/abs/2505.05488</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, imaging tasks, image/video enhancement, light field estimation, challenges

Summary: 
Event cameras, with their high dynamic range and low latency capabilities, have become a disruptive technology in the field of imaging. However, a comprehensive study of recent advances and challenges in leveraging these benefits for various imaging tasks is still lacking. This survey aims to address this gap by first introducing a physical model and characteristics of different event sensors. The survey then delves into the interaction of image/video enhancement tasks with event cameras, highlighting advancements in this area. Furthermore, it explores advanced tasks such as light field estimation, multi-view generation, and photometric, which enable the capture of richer light information using event cameras. The survey concludes by discussing new challenges and open questions, providing a perspective on the rapidly evolving field of event imaging.  <br /><br />Summary: <div>
arXiv:2505.05488v1 Announce Type: new 
Abstract: Event cameras offering high dynamic range and low latency have emerged as disruptive technologies in imaging. Despite growing research on leveraging these benefits for different imaging tasks, a comprehensive study of recently advances and challenges are still lacking. This limits the broader understanding of how to utilize events in universal imaging applications. In this survey, we first introduce a physical model and the characteristics of different event sensors as the foundation. Following this, we highlight the advancement and interaction of image/video enhancement tasks with events. Additionally, we explore advanced tasks, which capture richer light information with events, \eg~light field estimation, multi-view generation, and photometric. Finally, we discuss new challenges and open questions offering a perspective for this rapidly evolving field. More continuously updated resources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection</title>
<link>https://arxiv.org/abs/2505.05491</link>
<guid>https://arxiv.org/abs/2505.05491</guid>
<content:encoded><![CDATA[
<div> dynamic dual fusion network, object detection, traffic sign, feature extraction, MDDFNet 

Summary:
The article introduces a novel object detection network called Mamba-based Dynamic Dual Fusion Network (MDDFNet) designed for traffic sign detection. The network addresses two main challenges in detecting small objects such as traffic signs: singular feature extraction and difficulties in handling objects of varying sizes. The MDDFNet integrates a dynamic dual fusion module that utilizes multiple branches to enhance feature diversity and a Mamba-based backbone that combines global feature fusion and local feature interactions. Extensive experiments on the TT100K dataset show that MDDFNet outperforms state-of-the-art detectors in terms of performance, while maintaining real-time processing capabilities typical of single-stage models. These results confirm the effectiveness of MDDFNet in detecting small traffic signs. 

<br /><br />Summary: <div>
arXiv:2505.05491v1 Announce Type: new 
Abstract: The Detection of small objects, especially traffic signs, is a critical sub-task in object detection and autonomous driving. Despite signficant progress in previous research, two main challenges remain. First, the issue of feature extraction being too singular. Second, the detection process struggles to efectively handle objects of varying sizes or scales. These problems are also prevalent in general object detection tasks. To address these challenges, we propose a novel object detection network, Mamba-based Dynamic Dual Fusion Network (MDDFNet), for traffic sign detection. The network integrates a dynamic dual fusion module and a Mamba-based backbone to simultaneously tackle the aforementioned issues. Specifically, the dynamic dual fusion module utilizes multiple branches to consolidate various spatial and semantic information, thus enhancing feature diversity. The Mamba-based backbone leverages global feature fusion and local feature interaction, combining features in an adaptive manner to generate unique classification characteristics. Extensive experiments conducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that MDDFNet outperforms other state-of-the-art detectors, maintaining real-time processing capabilities of single-stage models while achieving superior performance. This confirms the efectiveness of MDDFNet in detecting small traffic signs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision</title>
<link>https://arxiv.org/abs/2505.05492</link>
<guid>https://arxiv.org/abs/2505.05492</guid>
<content:encoded><![CDATA[
<div> Python, fairness, deep learning, vision classifiers, DebioxAI

Summary:
DetoxAI is a new Python library designed to address fairness in deep learning vision classifiers. Existing solutions for fairness in machine learning often focus on tabular data, leaving vision-based classification tasks overlooked. DetoxAI bridges this gap by implementing state-of-the-art debiasing algorithms, fairness metrics, and visualization tools specifically tailored for deep learning vision classifiers. The library supports interventions in internal representations for debiasing and includes attribution-based visualization tools and quantitative algorithmic fairness metrics for demonstrating bias mitigation. With a focus on improving fairness in vision classifiers, DetoxAI offers engineers and researchers valuable tools for assessing and improving the equity of their deep learning models. <div>
arXiv:2505.05492v1 Announce Type: new 
Abstract: While machine learning fairness has made significant progress in recent years, most existing solutions focus on tabular data and are poorly suited for vision-based classification tasks, which rely heavily on deep learning. To bridge this gap, we introduce DetoxAI, an open-source Python library for improving fairness in deep learning vision classifiers through post-hoc debiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness metrics, and visualization tools. It supports debiasing via interventions in internal representations and includes attribution-based visualization tools and quantitative algorithmic fairness metrics to show how bias is mitigated. This paper presents the motivation, design, and use cases of DetoxAI, demonstrating its tangible value to engineers and researchers.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning 3D Persistent Embodied World Models</title>
<link>https://arxiv.org/abs/2505.05495</link>
<guid>https://arxiv.org/abs/2505.05495</guid>
<content:encoded><![CDATA[
<div> video models, world model, simulation, memory, planning

Summary:
- The article introduces a new persistent embodied world model that incorporates memory of previously generated content, allowing for more consistent long-term simulation of future actions.
- A video diffusion model is used to predict RGB-D video of future observations, which is then aggregated into a 3D map of the environment.
- By conditioning the video model on this 3D spatial map, the model can simulate both seen and unseen parts of the world, enabling more accurate prediction of future outcomes.
- The proposed world model demonstrates efficacy in downstream applications such as planning and policy learning, showcasing its potential for enhancing decision-making processes in intelligent agents.
- The integration of memory into the world model addresses the limitations of existing myopic models, providing a more comprehensive and reliable framework for long-horizon planning in complex environments. 

<br /><br />Summary: <div>
arXiv:2505.05495v1 Announce Type: new 
Abstract: The ability to simulate the effects of future actions on the world is a crucial ability of intelligent embodied agents, enabling agents to anticipate the effects of their actions and make plans accordingly. While a large body of existing work has explored how to construct such world models using video models, they are often myopic in nature, without any memory of a scene not captured by currently observed images, preventing agents from making consistent long-horizon plans in complex environments where many parts of the scene are partially observed. We introduce a new persistent embodied world model with an explicit memory of previously generated content, enabling much more consistent long-horizon simulation. During generation time, our video diffusion model predicts RGB-D video of the future observations of the agent. This generation is then aggregated into a persistent 3D map of the environment. By conditioning the video model on this 3D spatial map, we illustrate how this enables video world models to faithfully simulate both seen and unseen parts of the world. Finally, we illustrate the efficacy of such a world model in downstream embodied applications, enabling effective planning and policy learning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preliminary Explorations with GPT-4o(mni) Native Image Generation</title>
<link>https://arxiv.org/abs/2505.05501</link>
<guid>https://arxiv.org/abs/2505.05501</guid>
<content:encoded><![CDATA[
<div> image generation, multimodal comprehension, task taxonomy, qualitative test, model capabilities

Summary:
GPT-4o(mni) by OpenAI showcases exceptional visual generation capabilities with strong multimodal understanding. The study evaluates the model across various task categories including traditional image generation, discriminative tasks, knowledge-based generation, commonsense-based generation, spatially-aware image generation, and temporally-aware image generation. Results indicate GPT-4o excels in general-purpose synthesis tasks like text-to-image generation and visual stylization but struggles with precise spatial reasoning, instruction-grounded generation, and consistent temporal prediction. The model also faces challenges with knowledge-intensive or domain-specific tasks, displaying hallucinations and factual errors. While GPT-4o represents a significant advancement in unified multimodal generation, there are limitations that need to be addressed before its application in professional or safety-critical domains.<br /><br />Summary: <div>
arXiv:2505.05501v1 Announce Type: new 
Abstract: Recently, the visual generation ability by GPT-4o(mni) has been unlocked by OpenAI. It demonstrates a very remarkable generation capability with excellent multimodal condition understanding and varied task instructions. In this paper, we aim to explore the capabilities of GPT-4o across various tasks. Inspired by previous study, we constructed a task taxonomy along with a carefully curated set of test samples to conduct a comprehensive qualitative test. Benefiting from GPT-4o's powerful multimodal comprehension, its image-generation process demonstrates abilities surpassing those of traditional image-generation tasks. Thus, regarding the dimensions of model capabilities, we evaluate its performance across six task categories: traditional image generation tasks, discriminative tasks, knowledge-based generation, commonsense-based generation, spatially-aware image generation, and temporally-aware image generation. These tasks not only assess the quality and conditional alignment of the model's outputs but also probe deeper into GPT-4o's understanding of real-world concepts. Our results reveal that GPT-4o performs impressively well in general-purpose synthesis tasks, showing strong capabilities in text-to-image generation, visual stylization, and low-level image processing. However, significant limitations remain in its ability to perform precise spatial reasoning, instruction-grounded generation, and consistent temporal prediction. Furthermore, when faced with knowledge-intensive or domain-specific scenarios, such as scientific illustrations or mathematical plots, the model often exhibits hallucinations, factual errors, or structural inconsistencies. These findings suggest that while GPT-4o marks a substantial advancement in unified multimodal generation, there is still a long way to go before it can be reliably applied to professional or safety-critical domains.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation</title>
<link>https://arxiv.org/abs/2505.05505</link>
<guid>https://arxiv.org/abs/2505.05505</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-3D models, attribute binding, hierarchical chain of generation, occluded object parts, semantic labels<br />
Summary:<br />
Recent advances in text-to-3D models have improved the rendering of high-quality assets but struggle with objects containing complex attributes. Existing approaches face challenges due to limited comprehension of long descriptions by text encoders, leading to incorrect attribute binding in generated results. Addressing these issues, a new automated method called Hierarchical Chain of Generation (HCoG) decomposes descriptions into parts and orders them based on occlusions, ensuring a disciplined generation process. HCoG generates components within blocks and binds attributes using target-region localization and 3D Gaussian kernel optimization. It introduces novel techniques like Gaussian Extension and Label Elimination for seamless part generation, resulting in structurally coherent 3D objects with complex attributes. Experimental results validate the effectiveness of HCoG in producing attribute-faithful objects. The code for HCoG is available at https://github.com/Wakals/GASCOL. <br /><br />Summary: <div>
arXiv:2505.05505v1 Announce Type: new 
Abstract: Recent text-to-3D models can render high-quality assets, yet they still stumble on objects with complex attributes. The key obstacles are: (1) existing text-to-3D approaches typically lift text-to-image models to extract semantics via text encoders, while the text encoder exhibits limited comprehension ability for long descriptions, leading to deviated cross-attention focus, subsequently wrong attribute binding in generated results. (2) Occluded object parts demand a disciplined generation order and explicit part disentanglement. Though some works introduce manual efforts to alleviate the above issues, their quality is unstable and highly reliant on manual information. To tackle above problems, we propose a automated method Hierarchical-Chain-of-Generation (HCoG). It leverages a large language model to decompose the long description into blocks representing different object parts, and orders them from inside out according to occlusions, forming a hierarchical chain. Within each block we first coarsely create components, then precisely bind attributes via target-region localization and corresponding 3D Gaussian kernel optimization. Between blocks, we introduce Gaussian Extension and Label Elimination to seamlessly generate new parts by extending new Gaussian kernels, re-assigning semantic labels, and eliminating unnecessary kernels, ensuring that only relevant parts are added without disrupting previously optimized parts. Experiments confirm that HCoG yields structurally coherent, attribute-faithful 3D objects with complex attributes. The code is available at https://github.com/Wakals/GASCOL .
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Occupancy World Model for Robots</title>
<link>https://arxiv.org/abs/2505.05512</link>
<guid>https://arxiv.org/abs/2505.05512</guid>
<content:encoded><![CDATA[
<div> Keywords: scene evolutions, embodied agents, occupancy world model, indoor robotics, 3D occupancy scene evolution prediction

Summary:
This work introduces a new framework, RoboOccWorld, for learning and forecasting the scene evolutions of fine-grained occupancy in indoor robotic scenarios. The proposed model, based on a combination of spatio-temporal receptive field and guided autoregressive transformer, utilizes Conditional Causal State Attention (CCSA) to incorporate camera poses and Hybrid Spatio-Temporal Aggregation (HSTA) for multi-scale cue exploitation. A restructuring of the OccWorld-ScanNet benchmark facilitates the evaluation of indoor 3D occupancy scene evolution prediction. Experimental results show that RoboOccWorld outperforms existing methods in this task. The release of code for this framework is expected soon. 

<br /><br />Summary: <div>
arXiv:2505.05512v1 Announce Type: new 
Abstract: Understanding and forecasting the scene evolutions deeply affect the exploration and decision of embodied agents. While traditional methods simulate scene evolutions through trajectory prediction of potential instances, current works use the occupancy world model as a generative framework for describing fine-grained overall scene dynamics. However, existing methods cluster on the outdoor structured road scenes, while ignoring the exploration of forecasting 3D occupancy scene evolutions for robots in indoor scenes. In this work, we explore a new framework for learning the scene evolutions of observed fine-grained occupancy and propose an occupancy world model based on the combined spatio-temporal receptive field and guided autoregressive transformer to forecast the scene evolutions, called RoboOccWorld. We propose the Conditional Causal State Attention (CCSA), which utilizes camera poses of next state as conditions to guide the autoregressive transformer to adapt and understand the indoor robotics scenarios. In order to effectively exploit the spatio-temporal cues from historical observations, Hybrid Spatio-Temporal Aggregation (HSTA) is proposed to obtain the combined spatio-temporal receptive field based on multi-scale spatio-temporal windows. In addition, we restructure the OccWorld-ScanNet benchmark based on local annotations to facilitate the evaluation of the indoor 3D occupancy scene evolution prediction task. Experimental results demonstrate that our RoboOccWorld outperforms state-of-the-art methods in indoor 3D occupancy scene evolution prediction task. The code will be released soon.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach</title>
<link>https://arxiv.org/abs/2505.05513</link>
<guid>https://arxiv.org/abs/2505.05513</guid>
<content:encoded><![CDATA[
<div> Asia, rice, classification, convolutional neural network, quality check  
Summary:  
- Rice cultivation and utilization are essential for international trade and nutrition, with Asian countries like China, India, and Thailand leading in production.  
- Different rice varieties, including basmati and jasmine, cater to diverse culinary preferences and cultural traditions.
- Manual rice grain quality check is laborious and error-prone, necessitating an automatic classification solution.
- A convolutional neural network (CNN) framework was developed for accurate rice grain variety classification, achieving high performance metrics and minimal misclassifications.
- Explainability techniques like LIME and SHAP offered valuable insights into the model's decision-making process and feature importance in classification outcomes. 

<br /><br />Summary: <div>
arXiv:2505.05513v1 Announce Type: new 
Abstract: Rice is an essential staple food worldwide that is important in promoting international trade, economic growth, and nutrition. Asian countries such as China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their significant contribution to the cultivation and utilization of rice. These nations are also known for cultivating different rice grains, including short and long grains. These sizes are further classified as basmati, jasmine, kainat saila, ipsala, arborio, etc., catering to diverse culinary preferences and cultural traditions. For both local and international trade, inspecting and maintaining the quality of rice grains to satisfy customers and preserve a country's reputation is necessary. Manual quality check and classification is quite a laborious and time-consuming process. It is also highly prone to mistakes. Therefore, an automatic solution must be proposed for the effective and efficient classification of different varieties of rice grains. This research paper presents an automatic framework based on a convolutional neural network (CNN) for classifying different varieties of rice grains. We evaluated the proposed model based on performance metrics such as accuracy, recall, precision, and F1-Score. The CNN model underwent rigorous training and validation, achieving a remarkable accuracy rate and a perfect area under each class's Receiver Operating Characteristic (ROC) curve. The confusion matrix analysis confirmed the model's effectiveness in distinguishing between the different rice varieties, indicating minimal misclassifications. Additionally, the integration of explainability techniques such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided valuable insights into the model's decision-making process, revealing how specific features of the rice grains influenced classification outcomes.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions</title>
<link>https://arxiv.org/abs/2505.05517</link>
<guid>https://arxiv.org/abs/2505.05517</guid>
<content:encoded><![CDATA[
<div> Keywords: functional grasp, human hand-object interaction (HOI), multi-finger robot hands, web images, simulator-augmented data

Summary: 
The study focuses on training a functional grasping model for multi-finger robot hands using human grasp information extracted from web images. This approach eliminates the need for costly teleoperated demonstrations and allows for the training of the model on a diverse range of objects. By reconstructing human hand-object interaction 3D meshes from RGB images and aligning object meshes with their accurate 3D shapes, the model is trained on 10 object categories and evaluated on 9 unseen objects. The model achieves a 75.8% success rate on seen objects and 61.8% across all objects in simulation, with significant improvements in functionality ratings compared to baselines. Simulator-augmented data further enhances performance, boosting the success rate to 83.4%. The sim-to-real transfer to the LEAP Hand demonstrates an 85% success rate, showcasing the effectiveness of the proposed approach.<br /><br />Summary: <div>
arXiv:2505.05517v1 Announce Type: new 
Abstract: Functional grasp is essential for enabling dexterous multi-finger robot hands to manipulate objects effectively. However, most prior work either focuses on power grasping, which simply involves holding an object still, or relies on costly teleoperated robot demonstrations to teach robots how to grasp each object functionally. Instead, we propose extracting human grasp information from web images since they depict natural and functional object interactions, thereby bypassing the need for curated demonstrations. We reconstruct human hand-object interaction (HOI) 3D meshes from RGB images, retarget the human hand to multi-finger robot hands, and align the noisy object mesh with its accurate 3D shape. We show that these relatively low-quality HOI data from inexpensive web sources can effectively train a functional grasping model. To further expand the grasp dataset for seen and unseen objects, we use the initially-trained grasping policy with web data in the IsaacGym simulator to generate physically feasible grasps while preserving functionality. We train the grasping model on 10 object categories and evaluate it on 9 unseen objects, including challenging items such as syringes, pens, spray bottles, and tongs, which are underrepresented in existing datasets. The model trained on the web HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across all objects in simulation, with a 6.7% improvement in success rate and a 1.8x increase in functionality ratings over baselines. Simulator-augmented data further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the LEAP Hand achieves a 85% success rate. Project website is at: https://webgrasp.github.io/.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Privacy Preservation for Robot Visual Perception</title>
<link>https://arxiv.org/abs/2505.05519</link>
<guid>https://arxiv.org/abs/2505.05519</guid>
<content:encoded><![CDATA[
<div> method, privacy-constrained, video streaming, deep learning, detection model

Summary:
A new method for privacy-constrained video streaming (PCVS) has been developed to conceal privacy-sensitive objects in real-time video streams. The approach uses a logical specification to determine which objects to blur out, ensuring the concealment of sensitive information such as faces. A detection model evaluates the presence of these objects in each frame, with a conformal prediction approach establishing a theoretical lower bound on the probability of object existence. PCVS demonstrates a high specification satisfaction rate of over 95% across multiple datasets, outperforming other methods and consistently exceeding theoretical bounds. The method has been successfully deployed on robots in real-time operation, preserving privacy without compromising functionality. <div>
arXiv:2505.05519v1 Announce Type: new 
Abstract: Many robots (e.g., iRobot's Roomba) operate based on visual observations from live video streams, and such observations may inadvertently include privacy-sensitive objects, such as personal identifiers. Existing approaches for preserving privacy rely on deep learning models, differential privacy, or cryptography. They lack guarantees for the complete concealment of all sensitive objects. Guaranteeing concealment requires post-processing techniques and thus is inadequate for real-time video streams. We develop a method for privacy-constrained video streaming, PCVS, that conceals sensitive objects within real-time video streams. PCVS takes a logical specification constraining the existence of privacy-sensitive objects, e.g., never show faces when a person exists. It uses a detection model to evaluate the existence of these objects in each incoming frame. Then, it blurs out a subset of objects such that the existence of the remaining objects satisfies the specification. We then propose a conformal prediction approach to (i) establish a theoretical lower bound on the probability of the existence of these objects in a sequence of frames satisfying the specification and (ii) update the bound with the arrival of each subsequent frame. Quantitative evaluations show that PCVS achieves over 95 percent specification satisfaction rate in multiple datasets, significantly outperforming other methods. The satisfaction rate is consistently above the theoretical bounds across all datasets, indicating that the established bounds hold. Additionally, we deploy PCVS on robots in real-time operation and show that the robots operate normally without being compromised when PCVS conceals objects.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GaMNet: A Hybrid Network with Gabor Fusion and NMamba for Efficient 3D Glioma Segmentation</title>
<link>https://arxiv.org/abs/2505.05520</link>
<guid>https://arxiv.org/abs/2505.05520</guid>
<content:encoded><![CDATA[
<div> Keywords: Gliomas, deep learning, GaMNet, lesion segmentation, interpretability <br />
Summary: GaMNet is a novel approach for glioma lesion segmentation using deep learning. The proposed method combines the NMamba module for global modeling with a multi-scale CNN for efficient local feature extraction. By incorporating Gabor filters at multiple scales, GaMNet improves interpretability and mimics the human visual system. This approach achieves high segmentation accuracy while using fewer parameters and offering faster computation compared to existing methods. Extensive experiments demonstrate that GaMNet outperforms other models by reducing false positives and negatives, ultimately enhancing the reliability of clinical diagnosis. <div>
arXiv:2505.05520v1 Announce Type: new 
Abstract: Gliomas are aggressive brain tumors that pose serious health risks. Deep learning aids in lesion segmentation, but CNN and Transformer-based models often lack context modeling or demand heavy computation, limiting real-time use on mobile medical devices. We propose GaMNet, integrating the NMamba module for global modeling and a multi-scale CNN for efficient local feature extraction. To improve interpretability and mimic the human visual system, we apply Gabor filters at multiple scales. Our method achieves high segmentation accuracy with fewer parameters and faster computation. Extensive experiments show GaMNet outperforms existing methods, notably reducing false positives and negatives, which enhances the reliability of clinical diagnosis.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP</title>
<link>https://arxiv.org/abs/2505.05528</link>
<guid>https://arxiv.org/abs/2505.05528</guid>
<content:encoded><![CDATA[
<div> attack, CLIP, adversarial perturbation, transferability, X-Transfer

Summary:
X-Transfer introduces a novel attack method targeting CLIP models, showcasing universal adversarial vulnerability. This vulnerability, termed as super transferability, allows for deceptive perturbations to be successful across various CLIP encoders and downstream VLMs. The method utilizes surrogate scaling, dynamically selecting suitable surrogates for efficient scaling instead of relying on fixed models. Through extensive evaluation, X-Transfer demonstrates superior performance compared to existing UAP methods, setting a new benchmark for adversarial transferability in CLIP models. The code for X-Transfer is openly accessible on the GitHub repository provided by the authors. <div>
arXiv:2505.05528v1 Announce Type: new 
Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downstream VLMs across different samples, tasks, and domains. We refer to this property as \textbf{super transferability}--a single perturbation achieving cross-data, cross-domain, cross-model, and cross-task adversarial transferability simultaneously. This is achieved through \textbf{surrogate scaling}, a key innovation of our approach. Unlike existing methods that rely on fixed surrogate models, which are computationally intensive to scale, X-Transfer employs an efficient surrogate scaling strategy that dynamically selects a small subset of suitable surrogates from a large search space. Extensive evaluations demonstrate that X-Transfer significantly outperforms previous state-of-the-art UAP methods, establishing a new benchmark for adversarial transferability across CLIP models. The code is publicly available in our \href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OXSeg: Multidimensional attention UNet-based lip segmentation using semi-supervised lip contours</title>
<link>https://arxiv.org/abs/2505.05531</link>
<guid>https://arxiv.org/abs/2505.05531</guid>
<content:encoded><![CDATA[
<div> Keywords: lip segmentation, attention UNet, multidimensional input, facial anomalies, fetal alcohol syndrome

Summary:<br />
- The proposed method integrates attention UNet and multidimensional input to improve lip segmentation accuracy.
- Local binary patterns are used to unravel micro-patterns in facial images for building multidimensional inputs.
- A mask generation method utilizing anatomical landmarks helps estimate complete lip contour, enhancing segmentation accuracy.
- The method achieved a mean dice score of 84.75% and a mean pixel accuracy of 99.77% in upper lip segmentation.
- Using a generative adversarial network (GAN), the method attained 98.55% accuracy in identifying fetal alcohol syndrome (FAS) based on lip-related facial anomalies.

<br /><br />Summary: <div>
arXiv:2505.05531v1 Announce Type: new 
Abstract: Lip segmentation plays a crucial role in various domains, such as lip synchronization, lipreading, and diagnostics. However, the effectiveness of supervised lip segmentation is constrained by the availability of lip contour in the training phase. A further challenge with lip segmentation is its reliance on image quality , lighting, and skin tone, leading to inaccuracies in the detected boundaries. To address these challenges, we propose a sequential lip segmentation method that integrates attention UNet and multidimensional input. We unravel the micro-patterns in facial images using local binary patterns to build multidimensional inputs. Subsequently, the multidimensional inputs are fed into sequential attention UNets, where the lip contour is reconstructed. We introduce a mask generation method that uses a few anatomical landmarks and estimates the complete lip contour to improve segmentation accuracy. This mask has been utilized in the training phase for lip segmentation. To evaluate the proposed method, we use facial images to segment the upper lips and subsequently assess lip-related facial anomalies in subjects with fetal alcohol syndrome (FAS). Using the proposed lip segmentation method, we achieved a mean dice score of 84.75%, and a mean pixel accuracy of 99.77% in upper lip segmentation. To further evaluate the method, we implemented classifiers to identify those with FAS. Using a generative adversarial network (GAN), we reached an accuracy of 98.55% in identifying FAS in one of the study populations. This method could be used to improve lip segmentation accuracy, especially around Cupid's bow, and shed light on distinct lip-related characteristics of FAS.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Vision, Language, &amp; Action Models in Procedurally Generated, Open Ended Action Environments</title>
<link>https://arxiv.org/abs/2505.05540</link>
<guid>https://arxiv.org/abs/2505.05540</guid>
<content:encoded><![CDATA[
<div> benchmark, VLM, VLA, zero-shot generalization, Procgen

Summary:<br />
(1) State-of-the-art vision-language-action models, including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST, were evaluated on diverse procedural tasks from the Procgen benchmark. (2) The models showed limitations in zero-shot generalization to out-of-distribution tasks, with performance influenced by factors like action representation and task complexity. (3) Vision-language-action models generally outperformed others due to their robust architecture. (4) Variants of vision-language models improved significantly when appropriately constrained, highlighting the impact of precise prompt engineering on performance. (5) The study emphasizes the importance of systematic evaluation and analysis of VLM and VLA models in diverse environments to enhance their generalization capabilities for real-world applications. 

<br /><br />Summary: <div>
arXiv:2505.05540v1 Announce Type: new 
Abstract: Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in out-of-distribution (OOD) environments, remains limited. In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLM and VLA models-including GPT-4o, GPT-4.1, OpenVLA,Pi0 Base, and Pi0 FAST-on diverse procedural tasks from the Procgen benchmark. Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexit; (2) VLAs generally outperform other models due to their robust architectural design; and (3) VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt to Polyp: Clinically-Aware Medical Image Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05573</link>
<guid>https://arxiv.org/abs/2505.05573</guid>
<content:encoded><![CDATA[
<div> fine-tuning, medical images, text-to-image synthesis, MSDM, healthcare AI<br />
Summary:<br />
This paper explores text-to-image synthesis in the medical domain, comparing fine-tuning large pre-trained latent diffusion models (FLUX, Kandinsky) with training compact domain-specific models (MSDM). The newly proposed MSDM model integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms to align medical text prompts with generated images efficiently. Evaluation on colonoscopy (MedVQA-GI) and radiology (ROCOv2) datasets shows that while large models achieve higher fidelity, MSDM delivers comparable quality with lower computational costs. Quantitative metrics and feedback from medical experts highlight the strengths and limitations of each approach. This study highlights the potential of text-to-image synthesis in addressing data scarcity challenges in healthcare AI while maintaining patient privacy. <br />Summary: <div>
arXiv:2505.05573v1 Announce Type: new 
Abstract: The generation of realistic medical images from text descriptions has significant potential to address data scarcity challenges in healthcare AI while preserving patient privacy. This paper presents a comprehensive study of text-to-image synthesis in the medical domain, comparing two distinct approaches: (1) fine-tuning large pre-trained latent diffusion models and (2) training small, domain-specific models. We introduce a novel model named MSDM, an optimized architecture based on Stable Diffusion that integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms to better align medical text prompts with generated images. Our study compares two approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus training compact domain-specific models (MSDM). Evaluation across colonoscopy (MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models achieve higher fidelity, our optimized MSDM delivers comparable quality with lower computational costs. Quantitative metrics and qualitative evaluations by medical experts reveal strengths and limitations of each approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steepest Descent Density Control for Compact 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.05587</link>
<guid>https://arxiv.org/abs/2505.05587</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D, rendering, optimization, efficiency
Summary:
The paper introduces a theoretical framework for improving density control in 3D Gaussian Splatting (3DGS). It addresses the issue of redundant point clouds generated by the densification algorithm, which leads to excessive memory usage and slower performance. The analysis reveals the importance of splitting in escaping saddle points and establishes necessary conditions for densification. It determines the minimal number of offspring Gaussians, identifies the optimal parameter update direction, and provides an analytical solution for normalizing offspring opacity. The proposed SteepGS approach incorporates steepest density control, achieving a significant reduction in Gaussian points while maintaining rendering quality. This results in a 50% decrease in points, enhancing efficiency and scalability of 3DGS. <br /><br />Summary: <div>
arXiv:2505.05587v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for real-time, high-resolution novel view synthesis. By representing scenes as a mixture of Gaussian primitives, 3DGS leverages GPU rasterization pipelines for efficient rendering and reconstruction. To optimize scene coverage and capture fine details, 3DGS employs a densification algorithm to generate additional points. However, this process often leads to redundant point clouds, resulting in excessive memory usage, slower performance, and substantial storage demands - posing significant challenges for deployment on resource-constrained devices. To address this limitation, we propose a theoretical framework that demystifies and improves density control in 3DGS. Our analysis reveals that splitting is crucial for escaping saddle points. Through an optimization-theoretic approach, we establish the necessary conditions for densification, determine the minimal number of offspring Gaussians, identify the optimal parameter update direction, and provide an analytical solution for normalizing off-spring opacity. Building on these insights, we introduce SteepGS, incorporating steepest density control, a principled strategy that minimizes loss while maintaining a compact point cloud. SteepGS achieves a ~50% reduction in Gaussian points without compromising rendering quality, significantly enhancing both efficiency and scalability.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation</title>
<link>https://arxiv.org/abs/2505.05589</link>
<guid>https://arxiv.org/abs/2505.05589</guid>
<content:encoded><![CDATA[
<div> Keywords: Reactive dance generation, diffusion-based framework, multi-scale controllability, interaction fidelity, temporal consistency

Summary:
ReactDance introduces a novel diffusion-based framework for reactive dance generation that addresses limitations of existing methods by capturing fine-grained spatial interactions and localized temporal context. The framework utilizes Group Residual Finite Scalar Quantization (GRFSQ) to disentangle motion representation across multiple scales, allowing for accurate capture of interaction semantics. Additionally, a Blockwise Local Context (BLC) sampling strategy is employed to eliminate error accumulation in long sequence generation. The model, implemented with Layer-Decoupled Classifier-free Guidance (LDCFG), enables granular control over motion semantics at varying scales. Through extensive experiments on standard benchmarks, ReactDance showcases superior performance compared to existing methods, achieving state-of-the-art results in terms of interaction fidelity, synchronization, and temporal consistency.<br /><br />Summary: ReactDance provides a cutting-edge solution for reactive dance generation with enhanced spatial coordination and temporal coherence, surpassing current methods by incorporating multi-scale controllability, disentangled motion representation, local context sampling, and granular motion guidance. <div>
arXiv:2505.05589v1 Announce Type: new 
Abstract: Reactive dance generation (RDG) produces follower movements conditioned on guiding dancer and music while ensuring spatial coordination and temporal coherence. However, existing methods overemphasize global constraints and optimization, overlooking local information, such as fine-grained spatial interactions and localized temporal context. Therefore, we present ReactDance, a novel diffusion-based framework for high-fidelity RDG with long-term coherence and multi-scale controllability. Unlike existing methods that struggle with interaction fidelity, synchronization, and temporal consistency in duet synthesis, our approach introduces two key innovations: 1)Group Residual Finite Scalar Quantization (GRFSQ), a multi-scale disentangled motion representation that captures interaction semantics from coarse body rhythms to fine-grained joint dynamics, and 2)Blockwise Local Context (BLC), a sampling strategy eliminating error accumulation in long sequence generation via local block causal masking and periodic positional encoding. Built on the decoupled multi-scale GRFSQ representation, we implement a diffusion model withLayer-Decoupled Classifier-free Guidance (LDCFG), allowing granular control over motion semantics across scales. Extensive experiments on standard benchmarks demonstrate that ReactDance surpasses existing methods, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization</title>
<link>https://arxiv.org/abs/2505.05591</link>
<guid>https://arxiv.org/abs/2505.05591</guid>
<content:encoded><![CDATA[
<div> Keywords: surface reconstruction, computer vision, gaussian splatting, indoor scenes, data-driven optimization<br />
Summary: <br />
Surface reconstruction is a crucial aspect of computer vision and graphics, with applications in various fields such as 3D modeling and robotics. Existing approaches to surface reconstruction based on volumetric rendering have limitations in modeling under-observed or textureless regions. The QuickSplat method introduces data-driven priors to generate dense initializations for optimizing large-scale indoor scenes using 2D gaussian splatting. This accelerates the optimization process and enhances the geometry of flat wall structures. The method also includes a densifier network that predicts new Gaussians based on rendering gradients, eliminating the need for heuristics in densification. Extensive experiments show that this data-driven optimization significantly improves runtime efficiency by 8x and reduces depth errors by up to 48% compared to current state-of-the-art methods. <br /><br />Summary: <div>
arXiv:2505.05591v1 Announce Type: new 
Abstract: Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can struggle to model under-observed or textureless regions. We introduce QuickSplat, which learns data-driven priors to generate dense initializations for 2D gaussian splatting optimization of large-scale indoor scenes. This provides a strong starting point for the reconstruction, which accelerates the convergence of the optimization and improves the geometry of flat wall structures. We further learn to jointly estimate the densification and update of the scene parameters during each iteration; our proposed densifier network predicts new Gaussians based on the rendering gradients of existing ones, removing the needs of heuristics for densification. Extensive experiments on large-scale indoor scene reconstruction demonstrate the superiority of our data-driven optimization. Concretely, we accelerate runtime by 8x, while decreasing depth errors by up to 48% in comparison to state of the art methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Satellite Object Localization with Dilated Convolutions and Attention-aided Spatial Pooling</title>
<link>https://arxiv.org/abs/2505.05599</link>
<guid>https://arxiv.org/abs/2505.05599</guid>
<content:encoded><![CDATA[
<div> Keywords: satellite imagery, object localization, YOLO-DCAP, multi-scale features, attention-aided spatial pooling <br />
Summary: <br />
Object localization in satellite imagery is complex due to variability in objects, low resolution, and interference. This research focuses on GW, Bore, and OE datasets, each with unique challenges. YOLO-DCAP, an enhanced YOLOv5 variant, addresses these challenges with a MDRC block for multi-scale feature capture and an AaSP module for global spatial focus. YOLO-DCAP outperforms base model and state-of-the-art methods, with 20.95% and 32.23% mAP50 and IoU improvements over base model, and 7.35% and 9.84% over alternatives. The approach is robust and generalizable across all three datasets. Open-source code is available at the provided GitHub link. 
<br /> <div>
arXiv:2505.05599v1 Announce Type: new 
Abstract: Object localization in satellite imagery is particularly challenging due to the high variability of objects, low spatial resolution, and interference from noise and dominant features such as clouds and city lights. In this research, we focus on three satellite datasets: upper atmospheric Gravity Waves (GW), mesospheric Bores (Bore), and Ocean Eddies (OE), each presenting its own unique challenges. These challenges include the variability in the scale and appearance of the main object patterns, where the size, shape, and feature extent of objects of interest can differ significantly. To address these challenges, we introduce YOLO-DCAP, a novel enhanced version of YOLOv5 designed to improve object localization in these complex scenarios. YOLO-DCAP incorporates a Multi-scale Dilated Residual Convolution (MDRC) block to capture multi-scale features at scale with varying dilation rates, and an Attention-aided Spatial Pooling (AaSP) module to focus on the global relevant spatial regions, enhancing feature selection. These structural improvements help to better localize objects in satellite imagery. Experimental results demonstrate that YOLO-DCAP significantly outperforms both the YOLO base model and state-of-the-art approaches, achieving an average improvement of 20.95% in mAP50 and 32.23% in IoU over the base model, and 7.35% and 9.84% respectively over state-of-the-art alternatives, consistently across all three satellite datasets. These consistent gains across all three satellite datasets highlight the robustness and generalizability of the proposed approach. Our code is open sourced at https://github.com/AI-4-atmosphere-remote-sensing/satellite-object-localization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Preliminary Study for GPT-4o on Image Restoration</title>
<link>https://arxiv.org/abs/2505.05621</link>
<guid>https://arxiv.org/abs/2505.05621</guid>
<content:encoded><![CDATA[
<div> image restoration, GPT-4o, multi-modal inputs, autoregressive architecture, image generation<br />
Summary:<br />
OpenAI's GPT-4o model, incorporating multi-modal inputs and outputs in an autoregressive framework, has shown remarkable image generation capabilities. However, a systematic evaluation reveals issues with pixel-level fidelity in image restoration tasks, such as variations in proportions and object positions. Despite these challenges, GPT-4o's outputs can serve as potent visual priors in tasks like dehazing and derainning, bolstering existing networks' performance. This study provides guidance for integrating GPT-4o into image restoration pipelines, potentially driving innovation in the field. Released images and datasets will support further research in the broader realm of image generation.<br /> <div>
arXiv:2505.05621v1 Announce Type: new 
Abstract: OpenAI's GPT-4o model, integrating multi-modal inputs and outputs within an autoregressive architecture, has demonstrated unprecedented performance in image generation. In this work, we investigate its potential impact on the image restoration community. We present the first systematic evaluation of GPT-4o across diverse restoration tasks. Our experiments reveal that, although restoration outputs from GPT-4o are visually appealing, they often suffer from pixel-level structural fidelity when compared to ground-truth images. Common issues are variations in image proportions, shifts in object positions and quantities, and changes in viewpoint.To address it, taking image dehazing, derainning, and low-light enhancement as representative case studies, we show that GPT-4o's outputs can serve as powerful visual priors, substantially enhancing the performance of existing dehazing networks. It offers practical guidelines and a baseline framework to facilitate the integration of GPT-4o into future image restoration pipelines. We hope the study on GPT-4o image restoration will accelerate innovation in the broader field of image generation areas. To support further research, we will release GPT-4o-restored images from over 10 widely used image restoration datasets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Looking Beyond Language Priors: Enhancing Visual Comprehension and Attention in Multimodal Models</title>
<link>https://arxiv.org/abs/2505.05626</link>
<guid>https://arxiv.org/abs/2505.05626</guid>
<content:encoded><![CDATA[
<div> vision, language, Multimodal Large Language Models, visual understanding, language generation

Summary:
- Multimodal Large Language Models (MLLMs) struggle to effectively combine vision and language, often relying too heavily on language priors.
- This study delves into how MLLMs internally process visual information from image regions.
- The researchers introduce techniques to enhance the model's grasp of visual content and ensure that this understanding shapes language generation.
- The resulting model shows superior multimodal comprehension, as evidenced by its ability to predict visually-relevant tokens and outperform on visually challenging tasks.
- Through a rigorous analysis, the model demonstrates a 10-point improvement on difficult multimodal tasks. 

<br /><br />Summary: <div>
arXiv:2505.05626v1 Announce Type: new 
Abstract: Achieving deep alignment between vision and language remains a central challenge for Multimodal Large Language Models (MLLMs). These models often fail to fully leverage visual input, defaulting to strong language priors. Our approach first provides insights into how MLLMs internally build visual understanding of image regions and then introduces techniques to amplify this capability. Specifically, we explore techniques designed both to deepen the model's understanding of visual content and to ensure that these visual insights actively guide language generation. We demonstrate the superior multimodal understanding of our resultant model through a detailed upstream analysis quantifying its ability to predict visually-dependent tokens as well as 10 pt boost on visually challenging tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models</title>
<link>https://arxiv.org/abs/2505.05635</link>
<guid>https://arxiv.org/abs/2505.05635</guid>
<content:encoded><![CDATA[
<div> Keywords: Open-vocabulary recognition, bird species, multimodal vision language encoders, retrieval-augmented generation, biodiversity monitoring<br />
Summary:<br />
- The article addresses the challenge of open-vocabulary bird species recognition, focusing on classifying species without predefined taxonomic categories.<br />
- Traditional benchmarks are limited in real-world scenarios where novel species emerge, showing reduced performance under open-vocabulary settings.<br />
- The proposed framework integrates structured textual knowledge from Wikipedia articles of 11,202 bird species to improve recognition capabilities.<br />
- The Visual Re-ranking Retrieval-Augmented Generation (VR-RAG) framework uses visual similarities to rerank top candidates, enabling recognition of unseen species.<br />
- Extensive experiments across five classification benchmarks demonstrate the effectiveness of the approach, improving performance and surpassing conventional VLM-based methods.<br />
Summary: <div>
arXiv:2505.05635v1 Announce Type: new 
Abstract: Open-vocabulary recognition remains a challenging problem in computer vision, as it requires identifying objects from an unbounded set of categories. This is particularly relevant in nature, where new species are discovered every year. In this work, we focus on open-vocabulary bird species recognition, where the goal is to classify species based on their descriptions without being constrained to a predefined set of taxonomic categories. Traditional benchmarks like CUB-200-2011 and Birdsnap have been evaluated in a closed-vocabulary paradigm, limiting their applicability to real-world scenarios where novel species continually emerge. We show that the performance of current systems when evaluated under settings closely aligned with open-vocabulary drops by a huge margin. To address this gap, we propose a scalable framework integrating structured textual knowledge from Wikipedia articles of 11,202 bird species distilled via GPT-4o into concise, discriminative summaries. We propose Visual Re-ranking Retrieval-Augmented Generation(VR-RAG), a novel, retrieval-augmented generation framework that uses visual similarities to rerank the top m candidates retrieved by a set of multimodal vision language encoders. This allows for the recognition of unseen taxa. Extensive experiments across five established classification benchmarks show that our approach is highly effective. By integrating VR-RAG, we improve the average performance of state-of-the-art Large Multi-Modal Model QWEN2.5-VL by 15.4% across five benchmarks. Our approach outperforms conventional VLM-based approaches, which struggle with unseen species. By bridging the gap between encyclopedic knowledge and visual recognition, our work advances open-vocabulary recognition, offering a flexible, scalable solution for biodiversity monitoring and ecological research.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Style Transfer for Enhancing Animal Facial Landmark Detection</title>
<link>https://arxiv.org/abs/2505.05640</link>
<guid>https://arxiv.org/abs/2505.05640</guid>
<content:encoded><![CDATA[
<div> Style Transfer, Neural Networks, Facial Landmark Detection, Data Augmentation, Animal Detection<br />
<br />
Summary:<br />
Neural Style Transfer (NST) is explored for enhancing animal facial landmark detectors training. By using cropped facial images rather than full-body images, structural consistency is improved, enhancing image quality. Challenges arose with annotation misalignment when training on style-transferred images, but Supervised Style Transfer (SST) helped maintain up to 98% of baseline accuracy. Augmenting the dataset with style-transferred images proved more effective than traditional methods, boosting robustness. The study focused on cat facial landmarks but suggests the method can be applied to other species and landmark detection models. <div>
arXiv:2505.05640v1 Announce Type: new 
Abstract: Neural Style Transfer (NST) is a technique for applying the visual characteristics of one image onto another while preserving structural content. Traditionally used for artistic transformations, NST has recently been adapted, e.g., for domain adaptation and data augmentation. This study investigates the use of this technique for enhancing animal facial landmark detectors training. As a case study, we use a recently introduced Ensemble Landmark Detector for 48 anatomical cat facial landmarks and the CatFLW dataset it was trained on, making three main contributions. First, we demonstrate that applying style transfer to cropped facial images rather than full-body images enhances structural consistency, improving the quality of generated images. Secondly, replacing training images with style-transferred versions raised challenges of annotation misalignment, but Supervised Style Transfer (SST) - which selects style sources based on landmark accuracy - retained up to 98% of baseline accuracy. Finally, augmenting the dataset with style-transferred images further improved robustness, outperforming traditional augmentation methods. These findings establish semantic style transfer as an effective augmentation strategy for enhancing the performance of facial landmark detection models for animals and beyond. While this study focuses on cat facial landmarks, the proposed method can be generalized to other species and landmark detection models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Moon's Many Faces: A Single Unified Transformer for Multimodal Lunar Reconstruction</title>
<link>https://arxiv.org/abs/2505.05644</link>
<guid>https://arxiv.org/abs/2505.05644</guid>
<content:encoded><![CDATA[
<div> transformer architecture, multimodal learning, reflectance parameter estimation, image-based 3D reconstruction, lunar images <br />
<br />
Summary: 
Multimodal learning, relatively unexplored in planetary science, is applied to the tasks of reflectance parameter estimation and image-based 3D reconstruction of lunar images. A unified transformer architecture is proposed and trained to learn shared representations from grayscale images, digital elevation models, surface normals, and albedo maps. The model can translate between different input and target modalities, enabling the simultaneous prediction of DEMs and albedo maps from grayscale images. This approach solves the 3D reconstruction problem while separating photometric parameters and height information. Results show that the model can learn meaningful relationships across multiple modalities, with potential for future enhancements like photometric normalization and co-registration by incorporating additional input sources. <div>
arXiv:2505.05644v1 Announce Type: new 
Abstract: Multimodal learning is an emerging research topic across multiple disciplines but has rarely been applied to planetary science. In this contribution, we identify that reflectance parameter estimation and image-based 3D reconstruction of lunar images can be formulated as a multimodal learning problem. We propose a single, unified transformer architecture trained to learn shared representations between multiple sources like grayscale images, digital elevation models, surface normals, and albedo maps. The architecture supports flexible translation from any input modality to any target modality. Predicting DEMs and albedo maps from grayscale images simultaneously solves the task of 3D reconstruction of planetary surfaces and disentangles photometric parameters and height information. Our results demonstrate that our foundation model learns physically plausible relations across these four modalities. Adding more input modalities in the future will enable tasks such as photometric normalization and co-registration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in OCR Translation? Vision-Based Approaches to Robust Document Retrieval</title>
<link>https://arxiv.org/abs/2505.05666</link>
<guid>https://arxiv.org/abs/2505.05666</guid>
<content:encoded><![CDATA[
<div> Vision-based RAG, OCR-based RAG, document quality, semantic answer evaluation benchmark, question-answering performance<br />
<br />
Summary:<br />
The study compares a vision-based RAG system (ColPali) and traditional OCR-based pipelines with Llama 3.2 and Nougat OCR. Vision-based RAG performs well on fine-tuned documents but struggles with generalization to unseen documents of varying quality. In contrast, OCR-based RAG shows better generalization capabilities. The study introduces a semantic answer evaluation benchmark to assess question-answering performance. It highlights the trade-offs between computational efficiency and semantic accuracy, providing practical guidance for RAG practitioners on choosing between OCR-dependent and vision-based document retrieval systems in production environments.<br /> <div>
arXiv:2505.05666v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has become a popular technique for enhancing the reliability and utility of Large Language Models (LLMs) by grounding responses in external documents. Traditional RAG systems rely on Optical Character Recognition (OCR) to first process scanned documents into text. However, even state-of-the-art OCRs can introduce errors, especially in degraded or complex documents. Recent vision-language approaches, such as ColPali, propose direct visual embedding of documents, eliminating the need for OCR. This study presents a systematic comparison between a vision-based RAG system (ColPali) and more traditional OCR-based pipelines utilizing Llama 3.2 (90B) and Nougat OCR across varying document qualities. Beyond conventional retrieval accuracy metrics, we introduce a semantic answer evaluation benchmark to assess end-to-end question-answering performance. Our findings indicate that while vision-based RAG performs well on documents it has been fine-tuned on, OCR-based RAG is better able to generalize to unseen documents of varying quality. We highlight the key trade-offs between computational efficiency and semantic accuracy, offering practical guidance for RAG practitioners in selecting between OCR-dependent and vision-based document retrieval systems in production environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling</title>
<link>https://arxiv.org/abs/2505.05672</link>
<guid>https://arxiv.org/abs/2505.05672</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D head avatars, photorealism, 3D Gaussian splatting, facial motion estimation, high-detail model

Summary: 
This paper introduces a new high-detail 3D head avatar model that improves on existing models by increasing the number of 3D Gaussians used for rendering at 4K resolution. The model is reconstructed from multiview input video and utilizes a mesh-based 3D morphable model for coarse deformation. 3D Gaussians, embedded within the UVD tangent space of the mesh, provide photoreal appearance and allow for densification where needed. A novel deformable Gaussian encoding and fitting procedure preserves appearance detail while capturing facial motion and high-frequency features like skin wrinkling. This approach addresses challenges such as inaccurate motion estimation and memory limitations, enhancing the fidelity and quality of photoreal avatars for applications in telepresence, extended reality, and entertainment. <div>
arXiv:2505.05672v1 Announce Type: new 
Abstract: Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstanceGen: Image Generation with Instance-level Instructions</title>
<link>https://arxiv.org/abs/2505.05678</link>
<guid>https://arxiv.org/abs/2505.05678</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, text-to-image, structural constraints, instance-level attributes, spatial relations <br />
Summary: 
In this new arXiv announcement, the focus is on improving pretrained text-to-image models' ability to understand complex prompts involving multiple objects and attributes. The approach involves integrating structural constraints in the form of fine-grained structural initialization from contemporary image generation models. By combining this structural guidance with instance-level instructions, the proposed technique aims to generate images that accurately represent all aspects of the text prompt, including object counts, attributes, and spatial relations between instances. This advancement addresses the limitations faced by current generative models in capturing the semantics of complex prompts, offering a promising solution for more accurate and detailed image generation. <br /><br />Summary: <div>
arXiv:2505.05678v1 Announce Type: new 
Abstract: Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instance-level attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, %leveraging additional structural inputs typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance a step further by making the observation that contemporary image generation models can directly provide a plausible \emph{fine-grained} structural initialization. We propose a technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos</title>
<link>https://arxiv.org/abs/2505.05681</link>
<guid>https://arxiv.org/abs/2505.05681</guid>
<content:encoded><![CDATA[
<div> Keywords: Nonhuman primates, Capuchin monkeys, Video-text models, Fine-tuning, Behavioral analysis

Summary: 
The study focuses on developing computational models to assist researchers in extracting useful clips from videos of capuchin monkeys in their natural habitat. By fine-tuning pre-trained video-text models, the researchers aim to address the challenge of training models solely based on raw, unlabeled video footage and weak audio descriptions. They propose a two-folded approach involving an agentic data treatment pipeline and a fine-tuning process using Microsoft's X-CLIP model with Low-Rank Adaptation (LoRA). The results show a significant improvement in retrieval performance, with a 167% uplift in Hits@5 for the 16 frames model and a 114% uplift for the 8 frame model on domain data. Additionally, the model effectively ranks various behaviors based on NDCG@K results, outperforming raw pre-trained models in behavioral analysis tasks.<br /><br />Summary: <div>
arXiv:2505.05681v1 Announce Type: new 
Abstract: Video recordings of nonhuman primates in their natural habitat are a common source for studying their behavior in the wild. We fine-tune pre-trained video-text foundational models for the specific domain of capuchin monkeys, with the goal of developing useful computational models to help researchers to retrieve useful clips from videos. We focus on the challenging problem of training a model based solely on raw, unlabeled video footage, using weak audio descriptions sometimes provided by field collaborators. We leverage recent advances in Multimodal Large Language Models (MLLMs) and Vision-Language Models (VLMs) to address the extremely noisy nature of both video and audio content. Specifically, we propose a two-folded approach: an agentic data treatment pipeline and a fine-tuning process. The data processing pipeline automatically extracts clean and semantically aligned video-text pairs from the raw videos, which are subsequently used to fine-tune a pre-trained Microsoft's X-CLIP model through Low-Rank Adaptation (LoRA). We obtained an uplift in $Hits@5$ of $167\%$ for the 16 frames model and an uplift of $114\%$ for the 8 frame model on our domain data. Moreover, based on $NDCG@K$ results, our model is able to rank well most of the considered behaviors, while the tested raw pre-trained models are not able to rank them at all. The code will be made available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyperspectralMAE: The Hyperspectral Imagery Classification Model using Fourier-Encoded Dual-Branch Masked Autoencoder</title>
<link>https://arxiv.org/abs/2505.05710</link>
<guid>https://arxiv.org/abs/2505.05710</guid>
<content:encoded><![CDATA[
<div> Transformer-based model, hyperspectral imagery, dual masking, spectral-spatial representations, transfer learning <br />
<br />
Summary: HyperspectralMAE is a Transformer-based model for hyperspectral data that uses dual masking during pre-training to learn robust spatial and spectral representations. It incorporates learnable harmonic Fourier positional embeddings based on wavelength and balances pixel-level accuracy and spectral-shape fidelity in reconstruction using mean-squared error and spectral angle mapper. With a large parameter capacity and 768-dimensional embeddings, it excels in transfer learning when fine-tuned for land-cover classification. Pre-trained on NASA Hyperion and DLR EnMAP datasets, HyperspectralMAE achieves state-of-the-art accuracy on the Indian Pines benchmark, showcasing the efficacy of dual masking and wavelength-aware embeddings in advancing hyperspectral image analysis. <br /><br />Summary: <div>
arXiv:2505.05710v1 Announce Type: new 
Abstract: Hyperspectral imagery provides rich spectral detail but poses unique challenges because of its high dimensionality in both spatial and spectral domains. We propose \textit{HyperspectralMAE}, a Transformer-based foundation model for hyperspectral data that employs a \textit{dual masking} strategy: during pre-training we randomly occlude 50\% of spatial patches and 50\% of spectral bands. This forces the model to learn representations capable of reconstructing missing information across both dimensions. To encode spectral order, we introduce learnable harmonic Fourier positional embeddings based on wavelength. The reconstruction objective combines mean-squared error (MSE) with the spectral angle mapper (SAM) to balance pixel-level accuracy and spectral-shape fidelity.
  The resulting model contains about $1.8\times10^{8}$ parameters and produces 768-dimensional embeddings, giving it sufficient capacity for transfer learning. We pre-trained HyperspectralMAE on two large hyperspectral corpora -- NASA EO-1 Hyperion ($\sim$1\,600 scenes, $\sim$$3\times10^{11}$ pixel spectra) and DLR EnMAP Level-0 ($\sim$1\,300 scenes, $\sim$$3\times10^{11}$ pixel spectra) -- and fine-tuned it for land-cover classification on the Indian Pines benchmark. HyperspectralMAE achieves state-of-the-art transfer-learning accuracy on Indian Pines, confirming that masked dual-dimensional pre-training yields robust spectral-spatial representations. These results demonstrate that dual masking and wavelength-aware embeddings advance hyperspectral image reconstruction and downstream analysis.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer</title>
<link>https://arxiv.org/abs/2505.05711</link>
<guid>https://arxiv.org/abs/2505.05711</guid>
<content:encoded><![CDATA[
<div> Proposed Keywords: temporal action detection, transformer, multi-dilated gated encoder, central-adjacent region integrated decoder, state-of-the-art performance

Summary:
In this paper, the authors address limitations in query-based detectors for temporal action detection (TAD) by proposing a new model called DiGIT. The model tackles challenges specific to TAD by introducing a multi-dilated gated encoder to reduce redundant information and capture fine-grained temporal details. Additionally, a central-adjacent region integrated decoder is implemented to improve the sampling strategy for deformable cross-attention. Through extensive experiments on benchmark datasets like THUMOS14, ActivityNet v1.3, and HACS-Segment, DiGIT achieves state-of-the-art performance. This innovative approach outperforms existing models by effectively capturing long-range temporal context and essential information for accurate action detection. The code for DiGIT is publicly available on GitHub, providing a valuable resource for further research in the field. 

<br /><br />Summary: <div>
arXiv:2505.05711v1 Announce Type: new 
Abstract: In this paper, we examine a key limitation in query-based detectors for temporal action detection (TAD), which arises from their direct adaptation of originally designed architectures for object detection. Despite the effectiveness of the existing models, they struggle to fully address the unique challenges of TAD, such as the redundancy in multi-scale features and the limited ability to capture sufficient temporal context. To address these issues, we propose a multi-dilated gated encoder and central-adjacent region integrated decoder for temporal action detection transformer (DiGIT). Our approach replaces the existing encoder that consists of multi-scale deformable attention and feedforward network with our multi-dilated gated encoder. Our proposed encoder reduces the redundant information caused by multi-level features while maintaining the ability to capture fine-grained and long-range temporal information. Furthermore, we introduce a central-adjacent region integrated decoder that leverages a more comprehensive sampling strategy for deformable cross-attention to capture the essential information. Extensive experiments demonstrate that DiGIT achieves state-of-the-art performance on THUMOS14, ActivityNet v1.3, and HACS-Segment. Code is available at: https://github.com/Dotori-HJ/DiGIT
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic-Space-Intervened Diffusive Alignment for Visual Classification</title>
<link>https://arxiv.org/abs/2505.05721</link>
<guid>https://arxiv.org/abs/2505.05721</guid>
<content:encoded><![CDATA[
<div> alignment, cross-modal, semantic space, diffusive, classification

Summary:
The paper introduces a novel approach called Semantic-Space-Intervened Diffusive Alignment (SeDA) for improving visual classification through cross-modal alignment. SeDA utilizes a semantic space as a bridge in the visual-to-textual projection process, leveraging the shared class-level information between visual and textual features. A bi-stage diffusion framework is implemented in SeDA, with a Diffusion-Controlled Semantic Learner modeling the semantic feature space of visual features and a Diffusion-Controlled Semantic Translator focusing on learning the distribution of textual features from the semantic space. The Progressive Feature Interaction Network facilitates stepwise feature interactions to integrate textual information into mapped features. Experimental results demonstrate that SeDA outperforms existing methods in cross-modal feature alignment, leading to enhanced performance across various scenarios. <br /><br />Summary: <div>
arXiv:2505.05721v1 Announce Type: new 
Abstract: Cross-modal alignment is an effective approach to improving visual classification. Existing studies typically enforce a one-step mapping that uses deep neural networks to project the visual features to mimic the distribution of textual features. However, they typically face difficulties in finding such a projection due to the two modalities in both the distribution of class-wise samples and the range of their feature values. To address this issue, this paper proposes a novel Semantic-Space-Intervened Diffusive Alignment method, termed SeDA, models a semantic space as a bridge in the visual-to-textual projection, considering both types of features share the same class-level information in classification. More importantly, a bi-stage diffusion framework is developed to enable the progressive alignment between the two modalities. Specifically, SeDA first employs a Diffusion-Controlled Semantic Learner to model the semantic features space of visual features by constraining the interactive features of the diffusion model and the category centers of visual features. In the later stage of SeDA, the Diffusion-Controlled Semantic Translator focuses on learning the distribution of textual features from the semantic space. Meanwhile, the Progressive Feature Interaction Network introduces stepwise feature interactions at each alignment step, progressively integrating textual information into mapped features. Experimental results show that SeDA achieves stronger cross-modal feature alignment, leading to superior performance over existing methods across multiple scenarios.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Are Your Best Teacher: Semi-Supervised Surgical Point Tracking with Cycle-Consistent Self-Distillation</title>
<link>https://arxiv.org/abs/2505.05722</link>
<guid>https://arxiv.org/abs/2505.05722</guid>
<content:encoded><![CDATA[
<div> Synthetic datasets, point tracking, surgical videos, domain shift, semi-supervised learning,<br />
<br />
Summary: SurgTracker is a framework designed to adapt synthetic-trained point trackers to surgical video environments. It utilizes filtered self-distillation to generate pseudo-labels and enforce geometric consistency during training. The approach improves tracking performance in high-shift domains such as surgery by utilizing only 80 unlabeled videos. SurgTracker addresses the challenges of domain shift and lack of labeled data in surgical videos, where scenes feature complex tissue deformation, occlusion, and lighting variation. This adaptation method provides stable supervision throughout training without the need for multiple teachers, making it computationally efficient. The results on the STIR benchmark demonstrate the potential of SurgTracker for robust tracking performance in data-scarce domains. <br /><br /> <div>
arXiv:2505.05722v1 Announce Type: new 
Abstract: Synthetic datasets have enabled significant progress in point tracking by providing large-scale, densely annotated supervision. However, deploying these models in real-world domains remains challenging due to domain shift and lack of labeled data-issues that are especially severe in surgical videos, where scenes exhibit complex tissue deformation, occlusion, and lighting variation. While recent approaches adapt synthetic-trained trackers to natural videos using teacher ensembles or augmentation-heavy pseudo-labeling pipelines, their effectiveness in high-shift domains like surgery remains unexplored. This work presents SurgTracker, a semi-supervised framework for adapting synthetic-trained point trackers to surgical video using filtered self-distillation. Pseudo-labels are generated online by a fixed teacher-identical in architecture and initialization to the student-and are filtered using a cycle consistency constraint to discard temporally inconsistent trajectories. This simple yet effective design enforces geometric consistency and provides stable supervision throughout training, without the computational overhead of maintaining multiple teachers. Experiments on the STIR benchmark show that SurgTracker improves tracking performance using only 80 unlabeled videos, demonstrating its potential for robust adaptation in high-shift, data-scarce domains.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dome-DETR: DETR with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection</title>
<link>https://arxiv.org/abs/2505.05741</link>
<guid>https://arxiv.org/abs/2505.05741</guid>
<content:encoded><![CDATA[
<div> Keywords: Tiny object detection, Dome-DETR, DeFE, MWAS, PAQI

Summary:
Dome-DETR is a new framework designed for efficient tiny object detection in various applications such as drone surveillance and autonomous systems. It addresses challenges in feature redundancy and high computational costs by introducing innovative techniques. The Density-Focal Extractor (DeFE) generates compact foreground masks to reduce redundancies, while Masked Window Attention Sparsification (MWAS) prioritizes informative regions using sparse attention. Progressive Adaptive Query Initialization (PAQI) dynamically adjusts query density for improved allocation. Dome-DETR achieves state-of-the-art performance on AI-TOD-V2 and VisDrone datasets with a compact model size and low computational complexity. The proposed framework demonstrates superior results (+3.3 AP on AI-TOD-V2 and +2.5 AP on VisDrone) compared to existing methods. The code for Dome-DETR will be made available upon acceptance. 

<br /><br />Summary: Dome-DETR introduces novel techniques for efficient tiny object detection, including DeFE for generating compact foreground masks, MWAS for focusing on informative regions, and PAQI for adaptive query initialization. It achieves state-of-the-art performance on AI-TOD-V2 and VisDrone datasets while maintaining low computational complexity and a compact model size. The proposed framework addresses the limitations of existing methods and demonstrates significant improvements in accuracy and efficiency. <div>
arXiv:2505.05741v1 Announce Type: new 
Abstract: Tiny object detection plays a vital role in drone surveillance, remote sensing, and autonomous systems, enabling the identification of small targets across vast landscapes. However, existing methods suffer from inefficient feature leverage and high computational costs due to redundant feature processing and rigid query allocation. To address these challenges, we propose Dome-DETR, a novel framework with Density-Oriented Feature-Query Manipulation for Efficient Tiny Object Detection. To reduce feature redundancies, we introduce a lightweight Density-Focal Extractor (DeFE) to produce clustered compact foreground masks. Leveraging these masks, we incorporate Masked Window Attention Sparsification (MWAS) to focus computational resources on the most informative regions via sparse attention. Besides, we propose Progressive Adaptive Query Initialization (PAQI), which adaptively modulates query density across spatial areas for better query allocation. Extensive experiments demonstrate that Dome-DETR achieves state-of-the-art performance (+3.3 AP on AI-TOD-V2 and +2.5 AP on VisDrone) while maintaining low computational complexity and a compact model size. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>kFuse: A novel density based agglomerative clustering</title>
<link>https://arxiv.org/abs/2505.05748</link>
<guid>https://arxiv.org/abs/2505.05748</guid>
<content:encoded><![CDATA[
<div> agglomerative clustering, kFuse, sub-cluster partitioning, boundary connectivity, density similarity
Summary:<br />
The paper introduces a novel density-based agglomerative clustering method called kFuse. It addresses issues in existing clustering methods by utilizing natural neighbors for sub-cluster partitioning, determining boundary connectivity between sub-clusters based on adjacent samples and shortest distances, assessing density similarity through mean density and variance calculations, and establishing merging rules based on connectivity and density. kFuse only requires the number of clusters to be specified at the final merging stage, enhancing accuracy by considering adjacent samples, distances, and densities during merging. Experimental results on synthetic and real-world datasets demonstrate the effectiveness of kFuse in improving clustering accuracy and identification capability. <br /> <div>
arXiv:2505.05748v1 Announce Type: new 
Abstract: Agglomerative clustering has emerged as a vital tool in data analysis due to its intuitive and flexible characteristics. However, existing agglomerative clustering methods often involve additional parameters for sub-cluster partitioning and inter-cluster similarity assessment. This necessitates different parameter settings across various datasets, which is undoubtedly challenging in the absence of prior knowledge. Moreover, existing agglomerative clustering techniques are constrained by the calculation method of connection distance, leading to unstable clustering results. To address these issues, this paper introduces a novel density-based agglomerative clustering method, termed kFuse. kFuse comprises four key components: (1) sub-cluster partitioning based on natural neighbors; (2) determination of boundary connectivity between sub-clusters through the computation of adjacent samples and shortest distances; (3) assessment of density similarity between sub-clusters via the calculation of mean density and variance; and (4) establishment of merging rules between sub-clusters based on boundary connectivity and density similarity. kFuse requires the specification of the number of clusters only at the final merging stage. Additionally, by comprehensively considering adjacent samples, distances, and densities among different sub-clusters, kFuse significantly enhances accuracy during the merging phase, thereby greatly improving its identification capability. Experimental results on both synthetic and real-world datasets validate the effectiveness of kFuse.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Infrastructure Surveying: A Framework for Geometric Measurements and Compliance Assessment Using Point Cloud Data</title>
<link>https://arxiv.org/abs/2505.05752</link>
<guid>https://arxiv.org/abs/2505.05752</guid>
<content:encoded><![CDATA[
<div> Keywords: automation, point cloud data, ADA compliance, deep learning, infrastructure surveying  

<br /><br />Summary: This paper discusses a framework for automating geometric measurements and compliance assessments in infrastructure surveying using point cloud data. The approach combines deep learning-based detection and segmentation with geometric and signal processing techniques to streamline surveying tasks. As a proof of concept, the authors apply this framework to assess curb ramps' compliance with the Americans with Disabilities Act (ADA), showcasing the effectiveness of point cloud data in automating surveys. A significant aspect of this work is the creation of a large annotated dataset of curb ramps, which is publicly accessible, aiding in robust model training and evaluation. The authors present experimental results comparing the proposed method with manual field measurements, demonstrating its accuracy and reliability. This method not only aims to reduce manual labor but also to enhance consistency in infrastructure assessments. Additionally, the framework lays the foundation for broader applications in infrastructure surveying and automated construction evaluation, encouraging further use of point cloud data in these fields. The associated annotated database, manual ramp survey data, and algorithms developed during this project are available on GitHub. <div>
arXiv:2505.05752v1 Announce Type: new 
Abstract: Automation can play a prominent role in improving efficiency, accuracy, and scalability in infrastructure surveying and assessing construction and compliance standards. This paper presents a framework for automation of geometric measurements and compliance assessment using point cloud data. The proposed approach integrates deep learning-based detection and segmentation, in conjunction with geometric and signal processing techniques, to automate surveying tasks. As a proof of concept, we apply this framework to automatically evaluate the compliance of curb ramps with the Americans with Disabilities Act (ADA), demonstrating the utility of point cloud data in survey automation. The method leverages a newly collected, large annotated dataset of curb ramps, made publicly available as part of this work, to facilitate robust model training and evaluation. Experimental results, including comparison with manual field measurements of several ramps, validate the accuracy and reliability of the proposed method, highlighting its potential to significantly reduce manual effort and improve consistency in infrastructure assessment. Beyond ADA compliance, the proposed framework lays the groundwork for broader applications in infrastructure surveying and automated construction evaluation, promoting wider adoption of point cloud data in these domains. The annotated database, manual ramp survey data, and developed algorithms are publicly available on the project's GitHub page: https://github.com/Soltanilara/SurveyAutomation.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A review of advancements in low-light image enhancement using deep learning</title>
<link>https://arxiv.org/abs/2505.05759</link>
<guid>https://arxiv.org/abs/2505.05759</guid>
<content:encoded><![CDATA[
<div> Keywords: low-light, computer vision, deep learning, image enhancement, vision tasks 

Summary:
This review focuses on the challenges faced by computer vision algorithms in low-light environments and the advancements made in using deep learning for low-light image processing. It examines recent deep-learning-based methods for enhancing low-light images, discussing their operation, enhancement mechanisms, and impact on downstream vision tasks such as segmentation, detection, and classification. The review provides a comprehensive analysis of the strengths and limitations of different enhancement techniques, offering insights for optimizing vision task performance in low-light conditions. Through clear illustrations and detailed explanations, the review serves as a valuable reference for researchers and practitioners seeking to improve the performance of computer vision algorithms in low-light scenarios. Future research directions in the field of low-light image enhancement are also proposed. 

<br /><br />Summary: <div>
arXiv:2505.05759v1 Announce Type: new 
Abstract: In low-light environments, the performance of computer vision algorithms often deteriorates significantly, adversely affecting key vision tasks such as segmentation, detection, and classification. With the rapid advancement of deep learning, its application to low-light image processing has attracted widespread attention and seen significant progress in recent years. However, there remains a lack of comprehensive surveys that systematically examine how recent deep-learning-based low-light image enhancement methods function and evaluate their effectiveness in enhancing downstream vison tasks. To address this gap, this review provides a detailed elaboration on how various recent approaches (from 2020) operate and their enhancement mechanisms, supplemented with clear illustrations. It also investigates the impact of different enhancement techniques on subsequent vision tasks, critically analyzing their strengths and limitations. Additionally, it proposes future research directions. This review serves as a useful reference for determining low-light image enhancement techniques and optimizing vision task performance in low-light conditions.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Describe Anything in Medical Images</title>
<link>https://arxiv.org/abs/2505.05804</link>
<guid>https://arxiv.org/abs/2505.05804</guid>
<content:encoded><![CDATA[
<div> Keywords: localized image captioning, medical imaging, vision-language models, region-specific captioning, clinical factuality<br />
Summary:<br />
Localized image captioning has seen advancements with models like DAM, but has not been extensively applied to medical imaging. To address this gap, MedDAM is proposed as the first framework utilizing large vision-language models for region-specific captioning in medical images. It incorporates medical expert-designed prompts and establishes a benchmark for evaluation focusing on clinical factuality through attribute-level verification tasks. MedDAM outperforms various leading models in the task, emphasizing the importance of region-level semantic alignment in medical image understanding. The framework shows promise for clinical vision-language integration. <div>
arXiv:2505.05804v1 Announce Type: new 
Abstract: Localized image captioning has made significant progress with models like the Describe Anything Model (DAM), which can generate detailed region-specific descriptions without explicit region-text supervision. However, such capabilities have yet to be widely applied to specialized domains like medical imaging, where diagnostic interpretation relies on subtle regional findings rather than global understanding. To mitigate this gap, we propose MedDAM, the first comprehensive framework leveraging large vision-language models for region-specific captioning in medical images. MedDAM employs medical expert-designed prompts tailored to specific imaging modalities and establishes a robust evaluation benchmark comprising a customized assessment protocol, data pre-processing pipeline, and specialized QA template library. This benchmark evaluates both MedDAM and other adaptable large vision-language models, focusing on clinical factuality through attribute-level verification tasks, thereby circumventing the absence of ground-truth region-caption pairs in medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and SkinCon datasets demonstrate MedDAM's superiority over leading peers (including GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and OMG-LLaVA) in the task, revealing the importance of region-level semantic alignment in medical image understanding and establishing MedDAM as a promising foundation for clinical vision-language integration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Segmentation via Variational Model Based Tailored UNet: A Deep Variational Framework</title>
<link>https://arxiv.org/abs/2505.05806</link>
<guid>https://arxiv.org/abs/2505.05806</guid>
<content:encoded><![CDATA[
<div> Keywords: image segmentation, variational models, deep learning, UNet, boundary preservation<br />
<br />
Summary: <br />
Traditional image segmentation methods, such as variational models based on PDEs, can struggle with parameter sensitivity and high computational costs. On the other hand, deep learning models like UNet are excellent at automatic feature extraction but lack interpretability and require extensive labeled data. To combine the strengths of both approaches, VM_TUNet integrates the fourth-order modified Cahn-Hilliard equation with UNet, offering both interpretability and adaptive feature learning. By introducing a data-driven operator and incorporating TFPM for boundary preservation, VM_TUNet outperforms existing methods in segmentation performance, particularly for precise boundary delineation. The experimental results on benchmark datasets validate the effectiveness of the proposed hybrid framework. <br /> <div>
arXiv:2505.05806v1 Announce Type: new 
Abstract: Traditional image segmentation methods, such as variational models based on partial differential equations (PDEs), offer strong mathematical interpretability and precise boundary modeling, but often suffer from sensitivity to parameter settings and high computational costs. In contrast, deep learning models such as UNet, which are relatively lightweight in parameters, excel in automatic feature extraction but lack theoretical interpretability and require extensive labeled data. To harness the complementary strengths of both paradigms, we propose Variational Model Based Tailored UNet (VM_TUNet), a novel hybrid framework that integrates the fourth-order modified Cahn-Hilliard equation with the deep learning backbone of UNet, which combines the interpretability and edge-preserving properties of variational methods with the adaptive feature learning of neural networks. Specifically, a data-driven operator is introduced to replace manual parameter tuning, and we incorporate the tailored finite point method (TFPM) to enforce high-precision boundary preservation. Experimental results on benchmark datasets demonstrate that VM_TUNet achieves superior segmentation performance compared to existing approaches, especially for fine boundary delineation.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition</title>
<link>https://arxiv.org/abs/2505.05829</link>
<guid>https://arxiv.org/abs/2505.05829</guid>
<content:encoded><![CDATA[
<div> Diffusion transformer, image generation, caching, acceleration, calibration<br />
<br />
Summary: 
The paper introduces an increment-calibrated caching method for accelerating diffusion transformer (DiT) models in image generation. The method utilizes pre-trained models for calibration parameters and employs channel-aware Singular Value Decomposition (SVD) to enhance the calibration effect. Experimental results demonstrate superior performance compared to existing cache-based methods, reducing computation by over 45% while improving Inception Score (IS) by 12 with minimal increase in Fréchet Inception Distance (FID). The proposed method eliminates the need for training and achieves efficient acceleration of DiT models, making it a promising approach for improving the scalability and generative capabilities of diffusion models. The code for implementing the method is available on GitHub for further exploration. 

Summary:<br />
Diffusion transformer, image generation, caching, acceleration, calibration. The paper proposes increment-calibrated caching for diffusion transformer (DiT) models, using pre-trained models for calibration and channel-aware SVD for correction. Experimental results show superior performance in computation reduction and Inception Score improvement with minimal FID increase. The method offers training-free acceleration of DiT models, aiding scalability and generative capabilities. Code is available on GitHub for implementation. <div>
arXiv:2505.05829v1 Announce Type: new 
Abstract: Diffusion transformer (DiT) models have achieved remarkable success in image generation, thanks for their exceptional generative capabilities and scalability. Nonetheless, the iterative nature of diffusion models (DMs) results in high computation complexity, posing challenges for deployment. Although existing cache-based acceleration methods try to utilize the inherent temporal similarity to skip redundant computations of DiT, the lack of correction may induce potential quality degradation. In this paper, we propose increment-calibrated caching, a training-free method for DiT acceleration, where the calibration parameters are generated from the pre-trained model itself with low-rank approximation. To deal with the possible correction failure arising from outlier activations, we introduce channel-aware Singular Value Decomposition (SVD), which further strengthens the calibration effect. Experimental results show that our method always achieve better performance than existing naive caching methods with a similar computation resource budget. When compared with 35-step DDIM, our method eliminates more than 45% computation and improves IS by 12 at the cost of less than 0.06 FID increase. Code is available at https://github.com/ccccczzy/icc.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-level Fuzzy Learning with Patch Guidance for Image Ordinal Regression</title>
<link>https://arxiv.org/abs/2505.05834</link>
<guid>https://arxiv.org/abs/2505.05834</guid>
<content:encoded><![CDATA[
<div> Keywords: ordinal regression, patch-level features, fuzzy learning, image classification, deep learning

Summary:
The paper introduces a novel framework called Dual-level Fuzzy Learning with Patch Guidance (DFPG) for image ordinal regression. DFPG aims to learn precise grading boundaries using patch-level supervision, despite having only image-level ordinal labels. It utilizes patch-labeling and filtering strategies to concentrate on patch-level features, and incorporates a dual-level fuzzy learning module that handles label ambiguity effectively. Through extensive experiments on various datasets, DFPG outperforms existing methods and excels in categorizing challenging samples. The proposed framework demonstrates the ability to discern samples from difficult-to-classify categories with accuracy, showcasing its potential in advancing image classification tasks. The code for DFPG is publicly available for further exploration and implementation. 

<br /><br />
Summary: <div>
arXiv:2505.05834v1 Announce Type: new 
Abstract: Ordinal regression bridges regression and classification by assigning objects to ordered classes. While human experts rely on discriminative patch-level features for decisions, current approaches are limited by the availability of only image-level ordinal labels, overlooking fine-grained patch-level characteristics. In this paper, we propose a Dual-level Fuzzy Learning with Patch Guidance framework, named DFPG that learns precise feature-based grading boundaries from ambiguous ordinal labels, with patch-level supervision. Specifically, we propose patch-labeling and filtering strategies to enable the model to focus on patch-level features exclusively with only image-level ordinal labels available. We further design a dual-level fuzzy learning module, which leverages fuzzy logic to quantitatively capture and handle label ambiguity from both patch-wise and channel-wise perspectives. Extensive experiments on various image ordinal regression datasets demonstrate the superiority of our proposed method, further confirming its ability in distinguishing samples from difficult-to-classify categories. The code is available at https://github.com/ZJUMAI/DFPG-ord.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry</title>
<link>https://arxiv.org/abs/2505.05845</link>
<guid>https://arxiv.org/abs/2505.05845</guid>
<content:encoded><![CDATA[
<div> keyword: knots, wood, detection, machine learning, automation
Summary: 
The paper introduces a novel automated pipeline for knot detection and pairing in wood, addressing the labor-intensive and inefficient manual annotation process. Utilizing high-resolution surface images of wooden boards and machine learning techniques, the pipeline achieves a high detection accuracy using YOLOv8l with an mAP@0.5 of 0.887. In the pairing stage, knots are analyzed and paired based on multidimensional feature extraction and a triplet neural network to map features into a latent space, enabling clustering algorithms to identify corresponding knots with a pairing accuracy of 0.85. The experiments highlight the importance of distances from knot start and end points to the bottom of the wooden board, as well as longitudinal coordinates, in achieving high pairing accuracy. The study showcases the potential of AI in advancing wood science and industry.<br /><br />Summary: <div>
arXiv:2505.05845v1 Announce Type: new 
Abstract: Knots in wood are critical to both aesthetics and structural integrity, making their detection and pairing essential in timber processing. However, traditional manual annotation was labor-intensive and inefficient, necessitating automation. This paper proposes a lightweight and fully automated pipeline for knot detection and pairing based on machine learning techniques. In the detection stage, high-resolution surface images of wooden boards were collected using industrial-grade cameras, and a large-scale dataset was manually annotated and preprocessed. After the transfer learning, the YOLOv8l achieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were analyzed and paired based on multidimensional feature extraction. A triplet neural network was used to map the features into a latent space, enabling clustering algorithms to identify and pair corresponding knots. The triplet network with learnable weights achieved a pairing accuracy of 0.85. Further analysis revealed that he distances from the knot's start and end points to the bottom of the wooden board, and the longitudinal coordinates play crucial roles in achieving high pairing accuracy. Our experiments validate the effectiveness of the proposed solution, demonstrating the potential of AI in advancing wood science and industry.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefRef: A Synthetic Dataset and Benchmark for Reconstructing Refractive and Reflective Objects</title>
<link>https://arxiv.org/abs/2505.05848</link>
<guid>https://arxiv.org/abs/2505.05848</guid>
<content:encoded><![CDATA[
<div> dataset, refractive, reflective, 3D reconstruction, neural rendering

Summary:
The article introduces a new synthetic dataset called RefRef for reconstructing scenes with refractive and reflective objects from posed images. The dataset includes 50 objects of varying complexity placed in different background types, resulting in 150 scenes. An oracle method is proposed to calculate accurate light paths for neural rendering based on object geometry and refractive indices. The performance of several state-of-the-art methods is benchmarked against the oracle, showing significant lag in performance. The challenges of handling refractive and reflective materials in 3D reconstruction and novel view synthesis are highlighted by the dataset and benchmark results. <div>
arXiv:2505.05848v1 Announce Type: new 
Abstract: Modern 3D reconstruction and novel view synthesis approaches have demonstrated strong performance on scenes with opaque Lambertian objects. However, most assume straight light paths and therefore cannot properly handle refractive and reflective materials. Moreover, datasets specialized for these effects are limited, stymieing efforts to evaluate performance and develop suitable techniques. In this work, we introduce a synthetic RefRef dataset and benchmark for reconstructing scenes with refractive and reflective objects from posed images. Our dataset has 50 such objects of varying complexity, from single-material convex shapes to multi-material non-convex shapes, each placed in three different background types, resulting in 150 scenes. We also propose an oracle method that, given the object geometry and refractive indices, calculates accurate light paths for neural rendering, and an approach based on this that avoids these assumptions. We benchmark these against several state-of-the-art methods and show that all methods lag significantly behind the oracle, highlighting the challenges of the task and dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PICD: Versatile Perceptual Image Compression with Diffusion Rendering</title>
<link>https://arxiv.org/abs/2505.05853</link>
<guid>https://arxiv.org/abs/2505.05853</guid>
<content:encoded><![CDATA[
<div> compression, image, text, diffusion, rendering

Summary:
- The article introduces a new perceptual image compression method called PICD that works effectively for both screen and natural images.
- PICD utilizes a diffusion rendering approach where text and image are encoded separately and combined using a diffusion model.
- Three levels of conditional information integration are incorporated into the diffusion models: domain level, adaptor level, and instance level.
- By fine-tuning the base diffusion model with text content prompts and efficiently controlling the model using compressed image and text input, PICD achieves high text accuracy and perceptual quality.
- Additionally, the PICD codec can serve as a high-quality perceptual codec for natural images without the need for text conditions. 

<br /><br />Summary: <div>
arXiv:2505.05853v1 Announce Type: new 
Abstract: Recently, perceptual image compression has achieved significant advancements, delivering high visual quality at low bitrates for natural images. However, for screen content, existing methods often produce noticeable artifacts when compressing text. To tackle this challenge, we propose versatile perceptual screen image compression with diffusion rendering (PICD), a codec that works well for both screen and natural images. More specifically, we propose a compression framework that encodes the text and image separately, and renders them into one image using diffusion model. For this diffusion rendering, we integrate conditional information into diffusion models at three distinct levels: 1). Domain level: We fine-tune the base diffusion model using text content prompts with screen content. 2). Adaptor level: We develop an efficient adaptor to control the diffusion model using compressed image and text as input. 3). Instance level: We apply instance-wise guidance to further enhance the decoding process. Empirically, our PICD surpasses existing perceptual codecs in terms of both text accuracy and perceptual quality. Additionally, without text conditions, our approach serves effectively as a perceptual codec for natural images.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Multi-Contrast Super-Resolution: Pairing Unpaired Synthesis with Implicit Representations</title>
<link>https://arxiv.org/abs/2505.05855</link>
<guid>https://arxiv.org/abs/2505.05855</guid>
<content:encoded><![CDATA[
<div> Keywords: MRI, multi-contrast, super-resolution, unpaired, neural representations

Summary:
The article introduces a Modular Multi-Contrast Super-Resolution (MCSR) framework for enhancing MRI quality. It addresses the challenge of cross-modal enhancement by decoupling the MCSR task into two stages: Unpaired Cross-Modal Synthesis (U-CMS) and Unsupervised Super-Resolution (U-SR). This framework eliminates the need for paired training data and supports arbitrary upscaling. The U-CMS stage translates high-resolution reference modalities into synthesized versions of the target contrast, while the U-SR stage reconstructs the final output using implicit neural representations conditioned on spatial coordinates. The proposed method achieves superior performance at 4x and 8x upscaling, with improved fidelity and anatomical consistency compared to existing methods. The framework shows promise for scalable, subject-specific, and data-efficient MCSR in clinical settings. 

<br /><br />Summary: <div>
arXiv:2505.05855v1 Announce Type: new 
Abstract: Magnetic Resonance Imaging (MRI) is critical for clinical diagnostics but is often limited by long acquisition times and low signal-to-noise ratios, especially in modalities like diffusion and functional MRI. The multi-contrast nature of MRI presents a valuable opportunity for cross-modal enhancement, where high-resolution (HR) modalities can serve as references to boost the quality of their low-resolution (LR) counterparts-motivating the development of Multi-Contrast Super-Resolution (MCSR) techniques. Prior work has shown that leveraging complementary contrasts can improve SR performance; however, effective feature extraction and fusion across modalities with varying resolutions remains a major challenge. Moreover, existing MCSR methods often assume fixed resolution settings and all require large, perfectly paired training datasets-conditions rarely met in real-world clinical environments. To address these challenges, we propose a novel Modular Multi-Contrast Super-Resolution (MCSR) framework that eliminates the need for paired training data and supports arbitrary upscaling. Our method decouples the MCSR task into two stages: (1) Unpaired Cross-Modal Synthesis (U-CMS), which translates a high-resolution reference modality into a synthesized version of the target contrast, and (2) Unsupervised Super-Resolution (U-SR), which reconstructs the final output using implicit neural representations (INRs) conditioned on spatial coordinates. This design enables scale-agnostic and anatomically faithful reconstruction by bridging un-paired cross-modal synthesis with unsupervised resolution enhancement. Experiments show that our method achieves superior performance at 4x and 8x upscaling, with improved fidelity and anatomical consistency over existing baselines. Our framework demonstrates strong potential for scalable, subject-specific, and data-efficient MCSR in real-world clinical settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Facial Image Compression with Consistency Preserving Diffusion Prior</title>
<link>https://arxiv.org/abs/2505.05870</link>
<guid>https://arxiv.org/abs/2505.05870</guid>
<content:encoded><![CDATA[
<div> Facial Image Compression, Diffusion Prior, High-Frequency Information, Visual Quality, Machine Vision Accuracy
Summary:<br /><br />Facial Image Compression with a Stable Diffusion Prior (FaSDiff) addresses the issue of unsatisfactory reconstructed image quality in existing learned face image compression methods at low bit rates. By using a high-frequency-sensitive compressor and a hybrid low-frequency enhancement module, FaSDiff captures fine image details and disentangles low-frequency facial semantics to improve visual quality and machine vision accuracy. The method preserves consistency through frequency enhancement, leveraging diffusion priors for superior human visual perception while minimizing performance loss in machine vision due to semantic inconsistency. Extensive experiments demonstrate that FaSDiff outperforms state-of-the-art methods in balancing human visual quality and machine vision accuracy. The code for FaSDiff will be released after the paper is accepted. <div>
arXiv:2505.05870v1 Announce Type: new 
Abstract: With the widespread application of facial image data across various domains, the efficient storage and transmission of facial images has garnered significant attention. However, the existing learned face image compression methods often produce unsatisfactory reconstructed image quality at low bit rates. Simply adapting diffusion-based compression methods to facial compression tasks results in reconstructed images that perform poorly in downstream applications due to insufficient preservation of high-frequency information. To further explore the diffusion prior in facial image compression, we propose Facial Image Compression with a Stable Diffusion Prior (FaSDiff), a method that preserves consistency through frequency enhancement. FaSDiff employs a high-frequency-sensitive compressor in an end-to-end framework to capture fine image details and produce robust visual prompts. Additionally, we introduce a hybrid low-frequency enhancement module that disentangles low-frequency facial semantics and stably modulates the diffusion prior alongside visual prompts. The proposed modules allow FaSDiff to leverage diffusion priors for superior human visual perception while minimizing performance loss in machine vision due to semantic inconsistency. Extensive experiments show that FaSDiff outperforms state-of-the-art methods in balancing human visual quality and machine vision accuracy. The code will be released after the paper is accepted.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Register and CLS tokens yield a decoupling of local and global features in large ViTs</title>
<link>https://arxiv.org/abs/2505.05892</link>
<guid>https://arxiv.org/abs/2505.05892</guid>
<content:encoded><![CDATA[
<div> Keywords: DINOv2 model, attention maps, register tokens, global image information, interpretability<br />
<br />
Summary: <br />
Recent research has uncovered issues with attention maps in the DINOv2 model, where artifacts hinder interpretability and performance. The problem arises from the model using patch tokens for global image information, resulting in inaccuracies. To rectify this, register tokens are introduced to store global information separately. However, it was found that this leads to a dominance of global information from register tokens, causing a disconnect between local and global features. Surprisingly, the CLS token also exhibits similar behavior in models lacking explicit register tokens. This study highlights the importance of careful attention map interpretation in large vision models and suggests a pathway towards more interpretable models by addressing the issues related to register and CLS tokens. <div>
arXiv:2505.05892v1 Announce Type: new 
Abstract: Recent work has shown that the attention maps of the widely popular DINOv2 model exhibit artifacts, which hurt both model interpretability and performance on dense image tasks. These artifacts emerge due to the model repurposing patch tokens with redundant local information for the storage of global image information. To address this problem, additional register tokens have been incorporated in which the model can store such information instead. We carefully examine the influence of these register tokens on the relationship between global and local image features, showing that while register tokens yield cleaner attention maps, these maps do not accurately reflect the integration of local image information in large models. Instead, global information is dominated by information extracted from register tokens, leading to a disconnect between local and global features. Inspired by these findings, we show that the CLS token itself, which can be interpreted as a register, leads to a very similar phenomenon in models without explicit register tokens. Our work shows that care must be taken when interpreting attention maps of large ViTs. Further, by clearly attributing the faulty behaviour to register and CLS tokens, we show a path towards more interpretable vision models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI</title>
<link>https://arxiv.org/abs/2505.05895</link>
<guid>https://arxiv.org/abs/2505.05895</guid>
<content:encoded><![CDATA[
<div> Dataset, vision-language framework, automotive infotainment systems, adaptation, Molmo-7B model

Summary:
Automotive infotainment systems require intelligent solutions for handling UI updates and design variations. A vision-language framework is introduced to understand and interact with these systems, along with the release of an open-source dataset called AutomotiveUI-Bench-4K. A synthetic data pipeline is presented for generating training data, and a Molmo-7B model is fine-tuned using Low-Rank Adaptation (LoRa). The resulting Evaluative Large Action Model (ELAM) achieves strong performance on the dataset, with a +5.2% improvement on ScreenSpot compared to the baseline model. Despite being trained for infotainment, ELAM achieves high accuracy on ScreenSpot, matching specialized models for other UI domains. This research showcases how data collection and fine-tuning can drive AI progress in automotive UI understanding, with a cost-efficient approach that allows deployment on consumer-grade GPUs.

Summary: <div>
arXiv:2505.05895v1 Announce Type: new 
Abstract: Modern automotive infotainment systems require intelligent and adaptive solutions to handle frequent User Interface (UI) updates and diverse design variations. We introduce a vision-language framework for understanding and interacting with automotive infotainment systems, enabling seamless adaptation across different UI designs. To further support research in this field, we release AutomotiveUI-Bench-4K, an open-source dataset of 998 images with 4,208 annotations. Additionally, we present a synthetic data pipeline to generate training data. We fine-tune a Molmo-7B-based model using Low-Rank Adaptation (LoRa) and incorporating reasoning generated by our pipeline, along with visual grounding and evaluation capabilities. The fine-tuned Evaluative Large Action Model (ELAM) achieves strong performance on AutomotiveUI-Bench-4K (model and dataset are available on Hugging Face) and demonstrating strong cross-domain generalization, including a +5.2% improvement on ScreenSpot over the baseline model. Notably, our approach achieves 80.4% average accuracy on ScreenSpot, closely matching or even surpassing specialized models for desktop, mobile, and web, such as ShowUI, despite being trained for the infotainment domain. This research investigates how data collection and subsequent fine-tuning can lead to AI-driven progress within automotive UI understanding and interaction. The applied method is cost-efficient and fine-tuned models can be deployed on consumer-grade GPUs.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining the Source of Defects from a Mechanical Perspective for 3D Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.05901</link>
<guid>https://arxiv.org/abs/2505.05901</guid>
<content:encoded><![CDATA[
<div> Anomaly detection, Mechanics Complementary framework, Corrective forces, Diverse Anomaly-Generation module, Corrective Force Prediction Network, hierarchical quality control strategy<br />
<br />
Summary: 
This paper introduces a new approach to anomaly detection that considers the underlying causes of anomalies and generates corrective forces for each point. The Mechanics Complementary framework for 3D anomaly detection (MC4AD) is proposed, along with a Diverse Anomaly-Generation module to simulate various anomalies. A Corrective Force Prediction Network (CFP-Net) is developed to predict internal and external corrective forces for each point. A combined loss function is introduced to properly constrain the corrective forces. The study also presents a hierarchical quality control strategy based on a three-way decision and introduces a new dataset, Anomaly-IntraVariance, for model evaluation. Nine state-of-the-art performers were achieved on the proposed and existing datasets with minimal parameters and fast inference speed. <div>
arXiv:2505.05901v1 Announce Type: new 
Abstract: In this paper, we go beyond identifying anomalies only in structural terms and think about better anomaly detection motivated by anomaly causes. Most anomalies are regarded as the result of unpredictable defective forces from internal and external sources, and their opposite forces are sought to correct the anomalies. We introduced a Mechanics Complementary framework for 3D anomaly detection (MC4AD) to generate internal and external Corrective forces for each point. A Diverse Anomaly-Generation (DA-Gen) module is first proposed to simulate various anomalies. Then, we present a Corrective Force Prediction Network (CFP-Net) with complementary representations for point-level representation to simulate the different contributions of internal and external corrective forces. A combined loss was proposed, including a new symmetric loss and an overall loss, to constrain the corrective forces properly. As a highlight, we consider 3D anomaly detection in industry more comprehensively, creating a hierarchical quality control strategy based on a three-way decision and contributing a dataset named Anomaly-IntraVariance with intraclass variance to evaluate the model. On the proposed and existing five datasets, we obtained nine state-of-the-art performers with the minimum parameters and the fastest inference speed. The source is available at https://github.com/hzzzzzhappy/MC4AD
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFEN: Dual Feature Equalization Network for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.05913</link>
<guid>https://arxiv.org/abs/2505.05913</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, dual feature equalization network, Swin Transformer, convolutional neural network, state-of-the-art performance

Summary:
The paper introduces a novel method for medical image segmentation, the dual feature equalization network, which combines the advantages of Swin Transformer and Convolutional Neural Network. It addresses the issue of unequal contextual feature information at image boundaries and low-class pixel regions by proposing image-level and class-level feature equalization modules. These modules enhance pixel feature representations by equalizing contextual information within the image and aggregating regions of the same class. By utilizing Swin Transformer for encoding and decoding, the model effectively captures long-range dependencies and spatial correlations. Experimental results on various datasets show superior performance, achieving state-of-the-art results. The code for the proposed method is publicly available, showcasing the reproducibility and potential for further research and development in medical image segmentation. 
<br /><br />Summary: <div>
arXiv:2505.05913v1 Announce Type: new 
Abstract: Current methods for medical image segmentation primarily focus on extracting contextual feature information from the perspective of the whole image. While these methods have shown effective performance, none of them take into account the fact that pixels at the boundary and regions with a low number of class pixels capture more contextual feature information from other classes, leading to misclassification of pixels by unequal contextual feature information. In this paper, we propose a dual feature equalization network based on the hybrid architecture of Swin Transformer and Convolutional Neural Network, aiming to augment the pixel feature representations by image-level equalization feature information and class-level equalization feature information. Firstly, the image-level feature equalization module is designed to equalize the contextual information of pixels within the image. Secondly, we aggregate regions of the same class to equalize the pixel feature representations of the corresponding class by class-level feature equalization module. Finally, the pixel feature representations are enhanced by learning weights for image-level equalization feature information and class-level equalization feature information. In addition, Swin Transformer is utilized as both the encoder and decoder, thereby bolstering the ability of the model to capture long-range dependencies and spatial correlations. We conducted extensive experiments on Breast Ultrasound Images (BUSI), International Skin Imaging Collaboration (ISIC2017), Automated Cardiac Diagnosis Challenge (ACDC) and PH$^2$ datasets. The experimental results demonstrate that our method have achieved state-of-the-art performance. Our code is publicly available at https://github.com/JianJianYin/DFEN.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CGTrack: Cascade Gating Network with Hierarchical Feature Aggregation for UAV Tracking</title>
<link>https://arxiv.org/abs/2505.05936</link>
<guid>https://arxiv.org/abs/2505.05936</guid>
<content:encoded><![CDATA[
arXiv:2505.05936v1 Announce Type: new 
Abstract: Recent advancements in visual object tracking have markedly improved the capabilities of unmanned aerial vehicle (UAV) tracking, which is a critical component in real-world robotics applications. While the integration of hierarchical lightweight networks has become a prevalent strategy for enhancing efficiency in UAV tracking, it often results in a significant drop in network capacity, which further exacerbates challenges in UAV scenarios, such as frequent occlusions and extreme changes in viewing angles. To address these issues, we introduce a novel family of UAV trackers, termed CGTrack, which combines explicit and implicit techniques to expand network capacity within a coarse-to-fine framework. Specifically, we first introduce a Hierarchical Feature Cascade (HFC) module that leverages the spirit of feature reuse to increase network capacity by integrating the deep semantic cues with the rich spatial information, incurring minimal computational costs while enhancing feature representation. Based on this, we design a novel Lightweight Gated Center Head (LGCH) that utilizes gating mechanisms to decouple target-oriented coordinates from previously expanded features, which contain dense local discriminative information. Extensive experiments on three challenging UAV tracking benchmarks demonstrate that CGTrack achieves state-of-the-art performance while running fast. Code will be available at https://github.com/Nightwatch-Fox11/CGTrack.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving 3D Attention via Triplet Squeeze and Excitation Block</title>
<link>https://arxiv.org/abs/2505.05943</link>
<guid>https://arxiv.org/abs/2505.05943</guid>
<content:encoded><![CDATA[
arXiv:2505.05943v1 Announce Type: new 
Abstract: The emergence of ConvNeXt and its variants has reaffirmed the conceptual and structural suitability of CNN-based models for vision tasks, re-establishing them as key players in image classification in general, and in facial expression recognition (FER) in particular. In this paper, we propose a new set of models that build on these advancements by incorporating a new set of attention mechanisms that combines Triplet attention with Squeeze-and-Excitation (TripSE) in four different variants. We demonstrate the effectiveness of these variants by applying them to the ResNet18, DenseNet and ConvNext architectures to validate their versatility and impact. Our study shows that incorporating a TripSE block in these CNN models boosts their performances, particularly for the ConvNeXt architecture, indicating its utility. We evaluate the proposed mechanisms and associated models across four datasets, namely CIFAR100, ImageNet, FER2013 and AffectNet datasets, where ConvNext with TripSE achieves state-of-the-art results with an accuracy of \textbf{78.27\%} on the popular FER2013 dataset, a new feat for this dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for Few-shot Action Recognition</title>
<link>https://arxiv.org/abs/2505.06002</link>
<guid>https://arxiv.org/abs/2505.06002</guid>
<content:encoded><![CDATA[
arXiv:2505.06002v1 Announce Type: new 
Abstract: Large-scale pre-trained models have achieved remarkable success in language and image tasks, leading an increasing number of studies to explore the application of pre-trained image models, such as CLIP, in the domain of few-shot action recognition (FSAR). However, current methods generally suffer from several problems: 1) Direct fine-tuning often undermines the generalization capability of the pre-trained model; 2) The exploration of task-specific information is insufficient in the visual tasks; 3) The semantic order information is typically overlooked during text modeling; 4) Existing cross-modal alignment techniques ignore the temporal coupling of multimodal information. To address these, we propose Task-Adapter++, a parameter-efficient dual adaptation method for both image and text encoders. Specifically, to make full use of the variations across different few-shot learning tasks, we design a task-specific adaptation for the image encoder so that the most discriminative information can be well noticed during feature extraction. Furthermore, we leverage large language models (LLMs) to generate detailed sequential sub-action descriptions for each action class, and introduce semantic order adapters into the text encoder to effectively model the sequential relationships between these sub-actions. Finally, we develop an innovative fine-grained cross-modal alignment strategy that actively maps visual features to reside in the same temporal stage as semantic descriptions. Extensive experiments fully demonstrate the effectiveness and superiority of the proposed method, which achieves state-of-the-art performance on 5 benchmarks consistently. The code is open-sourced at https://github.com/Jaulin-Bage/Task-Adapter-pp.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Perception: Interpretable Predictions via Instance-wise Grouped Feature Selection</title>
<link>https://arxiv.org/abs/2505.06003</link>
<guid>https://arxiv.org/abs/2505.06003</guid>
<content:encoded><![CDATA[
arXiv:2505.06003v1 Announce Type: new 
Abstract: Understanding the decision-making process of machine learning models provides valuable insights into the task, the data, and the reasons behind a model's failures. In this work, we propose a method that performs inherently interpretable predictions through the instance-wise sparsification of input images. To align the sparsification with human perception, we learn the masking in the space of semantically meaningful pixel regions rather than on pixel-level. Additionally, we introduce an explicit way to dynamically determine the required level of sparsity for each instance. We show empirically on semi-synthetic and natural image datasets that our inherently interpretable classifier produces more meaningful, human-understandable predictions than state-of-the-art benchmarks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document Image Rectification Bases on Self-Adaptive Multitask Fusion</title>
<link>https://arxiv.org/abs/2505.06038</link>
<guid>https://arxiv.org/abs/2505.06038</guid>
<content:encoded><![CDATA[
arXiv:2505.06038v1 Announce Type: new 
Abstract: Deformed document image rectification is essential for real-world document understanding tasks, such as layout analysis and text recognition. However, current multi-task methods -- such as background removal, 3D coordinate prediction, and text line segmentation -- often overlook the complementary features between tasks and their interactions. To address this gap, we propose a self-adaptive learnable multi-task fusion rectification network named SalmRec. This network incorporates an inter-task feature aggregation module that adaptively improves the perception of geometric distortions, enhances feature complementarity, and reduces negative interference. We also introduce a gating mechanism to balance features both within global tasks and between local tasks effectively. Experimental results on two English benchmarks (DIR300 and DocUNet) and one Chinese benchmark (DocReal) demonstrate that our method significantly improves rectification performance. Ablation studies further highlight the positive impact of different tasks on dewarping and the effectiveness of our proposed module.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Better Cephalometric Landmark Detection with Diffusion Data Generation</title>
<link>https://arxiv.org/abs/2505.06055</link>
<guid>https://arxiv.org/abs/2505.06055</guid>
<content:encoded><![CDATA[
arXiv:2505.06055v1 Announce Type: new 
Abstract: Cephalometric landmark detection is essential for orthodontic diagnostics and treatment planning. Nevertheless, the scarcity of samples in data collection and the extensive effort required for manual annotation have significantly impeded the availability of diverse datasets. This limitation has restricted the effectiveness of deep learning-based detection methods, particularly those based on large-scale vision models. To address these challenges, we have developed an innovative data generation method capable of producing diverse cephalometric X-ray images along with corresponding annotations without human intervention. To achieve this, our approach initiates by constructing new cephalometric landmark annotations using anatomical priors. Then, we employ a diffusion-based generator to create realistic X-ray images that correspond closely with these annotations. To achieve precise control in producing samples with different attributes, we introduce a novel prompt cephalometric X-ray image dataset. This dataset includes real cephalometric X-ray images and detailed medical text prompts describing the images. By leveraging these detailed prompts, our method improves the generation process to control different styles and attributes. Facilitated by the large, diverse generated data, we introduce large-scale vision detection models into the cephalometric landmark detection task to improve accuracy. Experimental results demonstrate that training with the generated data substantially enhances the performance. Compared to methods without using the generated data, our approach improves the Success Detection Rate (SDR) by 6.5%, attaining a notable 82.2%. All code and data are available at: https://um-lab.github.io/cepha-generation
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation</title>
<link>https://arxiv.org/abs/2505.06068</link>
<guid>https://arxiv.org/abs/2505.06068</guid>
<content:encoded><![CDATA[
arXiv:2505.06068v1 Announce Type: new 
Abstract: Deep learning has revolutionized medical image segmentation, yet its full potential remains constrained by the paucity of annotated datasets. While diffusion models have emerged as a promising approach for generating synthetic image-mask pairs to augment these datasets, they paradoxically suffer from the same data scarcity challenges they aim to mitigate. Traditional mask-only models frequently yield low-fidelity images due to their inability to adequately capture morphological intricacies, which can critically compromise the robustness and reliability of segmentation models. To alleviate this limitation, we introduce Siamese-Diffusion, a novel dual-component model comprising Mask-Diffusion and Image-Diffusion. During training, a Noise Consistency Loss is introduced between these components to enhance the morphological fidelity of Mask-Diffusion in the parameter space. During sampling, only Mask-Diffusion is used, ensuring diversity and scalability. Comprehensive experiments demonstrate the superiority of our method. Siamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps, while UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at GitHub.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Camera-Only Bird's Eye View Perception: A Neural Approach to LiDAR-Free Environmental Mapping for Autonomous Vehicles</title>
<link>https://arxiv.org/abs/2505.06113</link>
<guid>https://arxiv.org/abs/2505.06113</guid>
<content:encoded><![CDATA[
arXiv:2505.06113v1 Announce Type: new 
Abstract: Autonomous vehicle perception systems have traditionally relied on costly LiDAR sensors to generate precise environmental representations. In this paper, we propose a camera-only perception framework that produces Bird's Eye View (BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combines YOLOv11-based object detection with DepthAnythingV2 monocular depth estimation across multi-camera inputs to achieve comprehensive 360-degree scene understanding. We evaluate our approach on the OpenLane-V2 and NuScenes datasets, achieving up to 85% road segmentation accuracy and 85-90% vehicle detection rates when compared against LiDAR ground truth, with average positional errors limited to 1.2 meters. These results highlight the potential of deep learning to extract rich spatial information using only camera inputs, enabling cost-efficient autonomous navigation without sacrificing accuracy.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Photovoltaic Defect Image Generator with Boundary Alignment Smoothing Constraint for Domain Shift Mitigation</title>
<link>https://arxiv.org/abs/2505.06117</link>
<guid>https://arxiv.org/abs/2505.06117</guid>
<content:encoded><![CDATA[
arXiv:2505.06117v1 Announce Type: new 
Abstract: Accurate defect detection of photovoltaic (PV) cells is critical for ensuring quality and efficiency in intelligent PV manufacturing systems. However, the scarcity of rich defect data poses substantial challenges for effective model training. While existing methods have explored generative models to augment datasets, they often suffer from instability, limited diversity, and domain shifts. To address these issues, we propose PDIG, a Photovoltaic Defect Image Generator based on Stable Diffusion (SD). PDIG leverages the strong priors learned from large-scale datasets to enhance generation quality under limited data. Specifically, we introduce a Semantic Concept Embedding (SCE) module that incorporates text-conditioned priors to capture the relational concepts between defect types and their appearances. To further enrich the domain distribution, we design a Lightweight Industrial Style Adaptor (LISA), which injects industrial defect characteristics into the SD model through cross-disentangled attention. At inference, we propose a Text-Image Dual-Space Constraints (TIDSC) module, enforcing the quality of generated images via positional consistency and spatial smoothing alignment. Extensive experiments demonstrate that PDIG achieves superior realism and diversity compared to state-of-the-art methods. Specifically, our approach improves Frechet Inception Distance (FID) by 19.16 points over the second-best method and significantly enhances the performance of downstream defect detection tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation</title>
<link>https://arxiv.org/abs/2505.06133</link>
<guid>https://arxiv.org/abs/2505.06133</guid>
<content:encoded><![CDATA[
arXiv:2505.06133v1 Announce Type: new 
Abstract: The segmentation of substantial brain lesions is a significant and challenging task in the field of medical image segmentation. Substantial brain lesions in brain imaging exhibit high heterogeneity, with indistinct boundaries between lesion regions and normal brain tissue. Small lesions in single slices are difficult to identify, making the accurate and reproducible segmentation of abnormal regions, as well as their feature description, highly complex. Existing methods have the following limitations: 1) They rely solely on single-modal information for learning, neglecting the multi-modal information commonly used in diagnosis. This hampers the ability to comprehensively acquire brain lesion information from multiple perspectives and prevents the effective integration and utilization of multi-modal data inputs, thereby limiting a holistic understanding of lesions. 2) They are constrained by the amount of data available, leading to low sensitivity to small lesions and difficulty in detecting subtle pathological changes. 3) Current SAM-based models rely on external prompts, which cannot achieve automatic segmentation and, to some extent, affect diagnostic efficiency.To address these issues, we have developed a large-scale fully automated segmentation model specifically designed for brain lesion segmentation, named BrainSegDMLF. This model has the following features: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and integrates multi-modal data during the encoding process, providing the SAM encoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling Decoder, enabling the model to extract rich low-level and high-level features even with limited data, thereby detecting the presence of small lesions. 3) Automatic segmentation masks, allowing the model to generate lesion masks automatically without requiring manual prompts.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks</title>
<link>https://arxiv.org/abs/2505.06152</link>
<guid>https://arxiv.org/abs/2505.06152</guid>
<content:encoded><![CDATA[
arXiv:2505.06152v1 Announce Type: new 
Abstract: Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped, primarily due to less specialized text descriptions in current dermatology multimodal datasets. To address this issue, we propose MM-Skin, the first large-scale multimodal dermatology dataset that encompasses 3 imaging modalities, including clinical, dermoscopic, and pathological and nearly 10k high-quality image-text pairs collected from professional textbooks. In addition, we generate over 27k diverse, instruction-following vision question answering (VQA) samples (9 times the size of current largest dermatology VQA dataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a dermatology-specific VLM designed for precise and nuanced skin disease interpretation. Comprehensive benchmark evaluations of SkinVL on VQA, supervised fine-tuning (SFT) and zero-shot classification tasks across 8 datasets, reveal its exceptional performance for skin diseases in comparison to both general and medical VLM models. The introduction of MM-Skin and SkinVL offers a meaningful contribution to advancing the development of clinical dermatology VLM assistants. MM-Skin is available at https://github.com/ZwQ803/MM-Skin
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models</title>
<link>https://arxiv.org/abs/2505.06166</link>
<guid>https://arxiv.org/abs/2505.06166</guid>
<content:encoded><![CDATA[
arXiv:2505.06166v1 Announce Type: new 
Abstract: We address the task of generating 3D hair geometry from a single image, which is challenging due to the diversity of hairstyles and the lack of paired image-to-3D hair data. Previous methods are primarily trained on synthetic data and cope with the limited amount of such data by using low-dimensional intermediate representations, such as guide strands and scalp-level embeddings, that require post-processing to decode, upsample, and add realism. These approaches fail to reconstruct detailed hair, struggle with curly hair, or are limited to handling only a few hairstyles. To overcome these limitations, we propose DiffLocks, a novel framework that enables detailed reconstruction of a wide variety of hairstyles directly from a single image. First, we address the lack of 3D hair data by automating the creation of the largest synthetic hair dataset to date, containing 40K hairstyles. Second, we leverage the synthetic hair dataset to learn an image-conditioned diffusion-transfomer model that generates accurate 3D strands from a single frontal image. By using a pretrained image backbone, our method generalizes to in-the-wild images despite being trained only on synthetic data. Our diffusion model predicts a scalp texture map in which any point in the map contains the latent code for an individual hair strand. These codes are directly decoded to 3D strands without post-processing techniques. Representing individual strands, instead of guide strands, enables the transformer to model the detailed spatial structure of complex hairstyles. With this, DiffLocks can recover highly curled hair, like afro hairstyles, from a single image for the first time. Data and code is available at https://radualexandru.github.io/difflocks/
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting a Segmentation Foundation Model for Medical Image Classification</title>
<link>https://arxiv.org/abs/2505.06217</link>
<guid>https://arxiv.org/abs/2505.06217</guid>
<content:encoded><![CDATA[
arXiv:2505.06217v1 Announce Type: new 
Abstract: Recent advancements in foundation models, such as the Segment Anything Model (SAM), have shown strong performance in various vision tasks, particularly image segmentation, due to their impressive zero-shot segmentation capabilities. However, effectively adapting such models for medical image classification is still a less explored topic. In this paper, we introduce a new framework to adapt SAM for medical image classification. First, we utilize the SAM image encoder as a feature extractor to capture segmentation-based features that convey important spatial and contextual details of the image, while freezing its weights to avoid unnecessary overhead during training. Next, we propose a novel Spatially Localized Channel Attention (SLCA) mechanism to compute spatially localized attention weights for the feature maps. The features extracted from SAM's image encoder are processed through SLCA to compute attention weights, which are then integrated into deep learning classification models to enhance their focus on spatially relevant or meaningful regions of the image, thus improving classification performance. Experimental results on three public medical image classification datasets demonstrate the effectiveness and data-efficiency of our approach.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction</title>
<link>https://arxiv.org/abs/2505.06219</link>
<guid>https://arxiv.org/abs/2505.06219</guid>
<content:encoded><![CDATA[
arXiv:2505.06219v1 Announce Type: new 
Abstract: Next Best View (NBV) algorithms aim to acquire an optimal set of images using minimal resources, time, or number of captures to enable efficient 3D reconstruction of a scene. Existing approaches often rely on prior scene knowledge or additional image captures and often develop policies that maximize coverage. Yet, for many real scenes with complex geometry and self-occlusions, coverage maximization does not lead to better reconstruction quality directly. In this paper, we propose the View Introspection Network (VIN), which is trained to predict the reconstruction quality improvement of views directly, and the VIN-NBV policy. A greedy sequential sampling-based policy, where at each acquisition step, we sample multiple query views and choose the one with the highest VIN predicted improvement score. We design the VIN to perform 3D-aware featurization of the reconstruction built from prior acquisitions, and for each query view create a feature that can be decoded into an improvement score. We then train the VIN using imitation learning to predict the reconstruction improvement score. We show that VIN-NBV improves reconstruction quality by ~30% over a coverage maximization baseline when operating with constraints on the number of acquisitions or the time in motion.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECGDeDRDNet: A deep learning-based method for Electrocardiogram noise removal using a double recurrent dense network</title>
<link>https://arxiv.org/abs/2505.05477</link>
<guid>https://arxiv.org/abs/2505.05477</guid>
<content:encoded><![CDATA[
arXiv:2505.05477v1 Announce Type: cross 
Abstract: Electrocardiogram (ECG) signals are frequently corrupted by noise, such as baseline wander (BW), muscle artifacts (MA), and electrode motion (EM), which significantly degrade their diagnostic utility. To address this issue, we propose ECGDeDRDNet, a deep learning-based ECG Denoising framework leveraging a Double Recurrent Dense Network architecture. In contrast to traditional approaches, we introduce a double recurrent scheme to enhance information reuse from both ECG waveforms and the estimated clean image. For ECG waveform processing, our basic model employs LSTM layers cascaded with DenseNet blocks. The estimated clean ECG image, obtained by subtracting predicted noise components from the noisy input, is iteratively fed back into the model. This dual recurrent architecture enables comprehensive utilization of both temporal waveform features and spatial image details, leading to more effective noise suppression. Experimental results on the MIT-BIH dataset demonstrate that our method achieves superior performance compared to conventional image denoising methods in terms of PSNR and SSIM while also surpassing classical ECG denoising techniques in both SNR and RMSE.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Restoration via Multi-domain Learning</title>
<link>https://arxiv.org/abs/2505.05504</link>
<guid>https://arxiv.org/abs/2505.05504</guid>
<content:encoded><![CDATA[
arXiv:2505.05504v1 Announce Type: cross 
Abstract: Due to adverse atmospheric and imaging conditions, natural images suffer from various degradation phenomena. Consequently, image restoration has emerged as a key solution and garnered substantial attention. Although recent Transformer architectures have demonstrated impressive success across various restoration tasks, their considerable model complexity poses significant challenges for both training and real-time deployment. Furthermore, instead of investigating the commonalities among different degradations, most existing restoration methods focus on modifying Transformer under limited restoration priors. In this work, we first review various degradation phenomena under multi-domain perspective, identifying common priors. Then, we introduce a novel restoration framework, which integrates multi-domain learning into Transformer. Specifically, in Token Mixer, we propose a Spatial-Wavelet-Fourier multi-domain structure that facilitates local-region-global multi-receptive field modeling to replace vanilla self-attention. Additionally, in Feed-Forward Network, we incorporate multi-scale learning to fuse multi-domain features at different resolutions. Comprehensive experimental results across ten restoration tasks, such as dehazing, desnowing, motion deblurring, defocus deblurring, rain streak/raindrop removal, cloud removal, shadow removal, underwater enhancement and low-light enhancement, demonstrate that our proposed model outperforms state-of-the-art methods and achieves a favorable trade-off among restoration performance, parameter size, computational cost and inference latency. The code is available at: https://github.com/deng-ai-lab/SWFormer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StereoINR: Cross-View Geometry Consistent Stereo Super Resolution with Implicit Neural Representation</title>
<link>https://arxiv.org/abs/2505.05509</link>
<guid>https://arxiv.org/abs/2505.05509</guid>
<content:encoded><![CDATA[
arXiv:2505.05509v1 Announce Type: cross 
Abstract: Stereo image super-resolution (SSR) aims to enhance high-resolution details by leveraging information from stereo image pairs. However, existing stereo super-resolution (SSR) upsampling methods (e.g., pixel shuffle) often overlook cross-view geometric consistency and are limited to fixed-scale upsampling. The key issue is that previous upsampling methods use convolution to independently process deep features of different views, lacking cross-view and non-local information perception, making it difficult to select beneficial information from multi-view scenes adaptively. In this work, we propose Stereo Implicit Neural Representation (StereoINR), which innovatively models stereo image pairs as continuous implicit representations. This continuous representation breaks through the scale limitations, providing a unified solution for arbitrary-scale stereo super-resolution reconstruction of left-right views. Furthermore, by incorporating spatial warping and cross-attention mechanisms, StereoINR enables effective cross-view information fusion and achieves significant improvements in pixel-level geometric consistency. Extensive experiments across multiple datasets show that StereoINR outperforms out-of-training-distribution scale upsampling and matches state-of-the-art SSR methods within training-distribution scales.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Train Your Metamorphic Deep Neural Network</title>
<link>https://arxiv.org/abs/2505.05510</link>
<guid>https://arxiv.org/abs/2505.05510</guid>
<content:encoded><![CDATA[
arXiv:2505.05510v1 Announce Type: cross 
Abstract: Neural Metamorphosis (NeuMeta) is a recent paradigm for generating neural networks of varying width and depth. Based on Implicit Neural Representation (INR), NeuMeta learns a continuous weight manifold, enabling the direct generation of compressed models, including those with configurations not seen during training. While promising, the original formulation of NeuMeta proves effective only for the final layers of the undelying model, limiting its broader applicability. In this work, we propose a training algorithm that extends the capabilities of NeuMeta to enable full-network metamorphosis with minimal accuracy degradation. Our approach follows a structured recipe comprising block-wise incremental training, INR initialization, and strategies for replacing batch normalization. The resulting metamorphic networks maintain competitive accuracy across a wide range of compression ratios, offering a scalable solution for adaptable and efficient deployment of deep models. The code is available at: https://github.com/TSommariva/HTTY_NeuMeta.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guidance for Intra-cardiac Echocardiography Manipulation to Maintain Continuous Therapy Device Tip Visibility</title>
<link>https://arxiv.org/abs/2505.05518</link>
<guid>https://arxiv.org/abs/2505.05518</guid>
<content:encoded><![CDATA[
arXiv:2505.05518v1 Announce Type: cross 
Abstract: Intra-cardiac Echocardiography (ICE) plays a critical role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing real-time visualization of intracardiac structures. However, maintaining continuous visibility of the therapy device tip remains a challenge due to frequent adjustments required during manual ICE catheter manipulation. To address this, we propose an AI-driven tracking model that estimates the device tip incident angle and passing point within the ICE imaging plane, ensuring continuous visibility and facilitating robotic ICE catheter control.
  A key innovation of our approach is the hybrid dataset generation strategy, which combines clinical ICE sequences with synthetic data augmentation to enhance model robustness. We collected ICE images in a water chamber setup, equipping both the ICE catheter and device tip with electromagnetic (EM) sensors to establish precise ground-truth locations. Synthetic sequences were created by overlaying catheter tips onto real ICE images, preserving motion continuity while simulating diverse anatomical scenarios. The final dataset consists of 5,698 ICE-tip image pairs, ensuring comprehensive training coverage.
  Our model architecture integrates a pretrained ultrasound (US) foundation model, trained on 37.4M echocardiography images, for feature extraction. A transformer-based network processes sequential ICE frames, leveraging historical passing points and incident angles to improve prediction accuracy.
  Experimental results demonstrate that our method achieves 3.32 degree entry angle error, 12.76 degree rotation angle error. This AI-driven framework lays the foundation for real-time robotic ICE catheter adjustments, minimizing operator workload while ensuring consistent therapy device visibility. Future work will focus on expanding clinical datasets to further enhance model generalization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Drive Anywhere with Model-Based Reannotation11</title>
<link>https://arxiv.org/abs/2505.05592</link>
<guid>https://arxiv.org/abs/2505.05592</guid>
<content:encoded><![CDATA[
arXiv:2505.05592v1 Announce Type: cross 
Abstract: Developing broadly generalizable visual navigation policies for robots is a significant challenge, primarily constrained by the availability of large-scale, diverse training data. While curated datasets collected by researchers offer high quality, their limited size restricts policy generalization. To overcome this, we explore leveraging abundant, passively collected data sources, including large volumes of crowd-sourced teleoperation data and unlabeled YouTube videos, despite their potential for lower quality or missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework that utilizes a learned short-horizon, model-based expert model to relabel or generate high-quality actions for these passive datasets. This relabeled data is then distilled into LogoNav, a long-horizon navigation policy conditioned on visual goals or GPS waypoints. We demonstrate that LogoNav, trained using MBRA-processed data, achieves state-of-the-art performance, enabling robust navigation over distances exceeding 300 meters in previously unseen indoor and outdoor environments. Our extensive real-world evaluations, conducted across a fleet of robots (including quadrupeds) in six cities on three continents, validate the policy's ability to generalize and navigate effectively even amidst pedestrians in crowded settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Score-based Self-supervised MRI Denoising</title>
<link>https://arxiv.org/abs/2505.05631</link>
<guid>https://arxiv.org/abs/2505.05631</guid>
<content:encoded><![CDATA[
arXiv:2505.05631v1 Announce Type: cross 
Abstract: Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging tool that provides unparalleled soft tissue contrast and anatomical detail. Noise contamination, especially in accelerated and/or low-field acquisitions, can significantly degrade image quality and diagnostic accuracy. Supervised learning based denoising approaches have achieved impressive performance but require high signal-to-noise ratio (SNR) labels, which are often unavailable. Self-supervised learning holds promise to address the label scarcity issue, but existing self-supervised denoising methods tend to oversmooth fine spatial features and often yield inferior performance than supervised methods. We introduce Corruption2Self (C2S), a novel score-based self-supervised framework for MRI denoising. At the core of C2S is a generalized denoising score matching (GDSM) loss, which extends denoising score matching to work directly with noisy observations by modeling the conditional expectation of higher-SNR images given further corrupted observations. This allows the model to effectively learn denoising across multiple noise levels directly from noisy data. Additionally, we incorporate a reparameterization of noise levels to stabilize training and enhance convergence, and introduce a detail refinement extension to balance noise reduction with the preservation of fine spatial features. Moreover, C2S can be extended to multi-contrast denoising by leveraging complementary information across different MRI contrasts. We demonstrate that our method achieves state-of-the-art performance among self-supervised methods and competitive results compared to supervised counterparts across varying noise conditions and MRI contrasts on the M4Raw and fastMRI dataset.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraGauss: Ultrafast Gaussian Reconstruction of 3D Ultrasound Volumes</title>
<link>https://arxiv.org/abs/2505.05643</link>
<guid>https://arxiv.org/abs/2505.05643</guid>
<content:encoded><![CDATA[
arXiv:2505.05643v1 Announce Type: cross 
Abstract: Ultrasound imaging is widely used due to its safety, affordability, and real-time capabilities, but its 2D interpretation is highly operator-dependent, leading to variability and increased cognitive demand. 2D-to-3D reconstruction mitigates these challenges by providing standardized volumetric views, yet existing methods are often computationally expensive, memory-intensive, or incompatible with ultrasound physics. We introduce UltraGauss: the first ultrasound-specific Gaussian Splatting framework, extending view synthesis techniques to ultrasound wave propagation. Unlike conventional perspective-based splatting, UltraGauss models probe-plane intersections in 3D, aligning with acoustic image formation. We derive an efficient rasterization boundary formulation for GPU parallelization and introduce a numerically stable covariance parametrization, improving computational efficiency and reconstruction accuracy. On real clinical ultrasound data, UltraGauss achieves state-of-the-art reconstructions in 5 minutes, and reaching 0.99 SSIM within 20 minutes on a single GPU. A survey of expert clinicians confirms UltraGauss' reconstructions are the most realistic among competing methods. Our CUDA implementation will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New k-Space Model for Non-Cartesian Fourier Imaging</title>
<link>https://arxiv.org/abs/2505.05647</link>
<guid>https://arxiv.org/abs/2505.05647</guid>
<content:encoded><![CDATA[
arXiv:2505.05647v1 Announce Type: cross 
Abstract: For the past several decades, it has been popular to reconstruct Fourier imaging data using model-based approaches that can easily incorporate physical constraints and advanced regularization/machine learning priors. The most common modeling approach is to represent the continuous image as a linear combination of shifted "voxel" basis functions. Although well-studied and widely-deployed, this voxel-based model is associated with longstanding limitations, including high computational costs, slow convergence, and a propensity for artifacts. In this work, we reexamine this model from a fresh perspective, identifying new issues that may have been previously overlooked (including undesirable approximation, periodicity, and nullspace characteristics). Our insights motivate us to propose a new model that is more resilient to the limitations (old and new) of the previous approach. Specifically, the new model is based on a Fourier-domain basis expansion rather than the standard image-domain voxel-based approach. Illustrative results, which are presented in the context of non-Cartesian MRI reconstruction, demonstrate that the new model enables improved image quality (reduced artifacts) and/or reduced computational complexity (faster computations and improved convergence).
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models</title>
<link>https://arxiv.org/abs/2505.05659</link>
<guid>https://arxiv.org/abs/2505.05659</guid>
<content:encoded><![CDATA[
arXiv:2505.05659v1 Announce Type: cross 
Abstract: EfficientNet models are convolutional neural networks optimized for parameter allocation by jointly balancing network width, depth, and resolution. Renowned for their exceptional accuracy, these models have become a standard for image classification tasks across diverse computer vision benchmarks. While traditional neural networks learn correlations between feature channels during training, vector-valued neural networks inherently treat multidimensional data as coherent entities, taking for granted the inter-channel relationships. This paper introduces vector-valued EfficientNets (V-EfficientNets), a novel extension of EfficientNet designed to process arbitrary vector-valued data. The proposed models are evaluated on a medical image classification task, achieving an average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute lymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency, significantly reducing parameters while outperforming state-of-the-art models, including the original EfficientNet. The source code is available at https://github.com/mevalle/v-nets.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology</title>
<link>https://arxiv.org/abs/2505.05689</link>
<guid>https://arxiv.org/abs/2505.05689</guid>
<content:encoded><![CDATA[
arXiv:2505.05689v1 Announce Type: cross 
Abstract: Histopathology evaluation of tissue specimens through microscopic examination is essential for accurate disease diagnosis and prognosis. However, traditional manual analysis by specially trained pathologists is time-consuming, labor-intensive, cost-inefficient, and prone to inter-rater variability, potentially affecting diagnostic consistency and accuracy. As digital pathology images continue to proliferate, there is a pressing need for automated analysis to address these challenges. Recent advancements in artificial intelligence-based tools such as machine learning (ML) models, have significantly enhanced the precision and efficiency of analyzing histopathological slides. However, despite their impressive performance, ML models are invariant only to translation, lacking invariance to rotation and reflection. This limitation restricts their ability to generalize effectively, particularly in histopathology, where images intrinsically lack meaningful orientation. In this study, we develop robust, equivariant histopathological biomarkers through a novel symmetric convolutional kernel via unsupervised segmentation. The approach is validated using prostate tissue micro-array (TMA) images from 50 patients in the Gleason 2019 Challenge public dataset. The biomarkers extracted through this approach demonstrate enhanced robustness and generalizability against rotation compared to models using standard convolution kernels, holding promise for enhancing the accuracy, consistency, and robustness of ML models in digital pathology. Ultimately, this work aims to improve diagnostic and prognostic capabilities of histopathology beyond prostate cancer through equivariant imaging.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for MRI Reconstruction without High-Quality Training Reference</title>
<link>https://arxiv.org/abs/2505.05703</link>
<guid>https://arxiv.org/abs/2505.05703</guid>
<content:encoded><![CDATA[
arXiv:2505.05703v1 Announce Type: cross 
Abstract: Purpose: Deep learning has demonstrated strong potential for MRI reconstruction, but conventional supervised learning methods require high-quality reference images, which are often unavailable in practice. Self-supervised learning offers an alternative, yet its performance degrades at high acceleration rates. To overcome these limitations, we propose hybrid learning, a novel two-stage training framework that combines self-supervised and supervised learning for robust image reconstruction.
  Methods: Hybrid learning is implemented in two sequential stages. In the first stage, self-supervised learning is employed to generate improved images from noisy or undersampled reference data. These enhanced images then serve as pseudo-ground truths for the second stage, which uses supervised learning to refine reconstruction performance and support higher acceleration rates. We evaluated hybrid learning in two representative applications: (1) accelerated 0.55T spiral-UTE lung MRI using noisy reference data, and (2) 3D T1 mapping of the brain without access to fully sampled ground truth.
  Results: For spiral-UTE lung MRI, hybrid learning consistently improved image quality over both self-supervised and conventional supervised methods across different acceleration rates, as measured by SSIM and NMSE. For 3D T1 mapping, hybrid learning achieved superior T1 quantification accuracy across a wide dynamic range, outperforming self-supervised learning in all tested conditions.
  Conclusions: Hybrid learning provides a practical and effective solution for training deep MRI reconstruction networks when only low-quality or incomplete reference data are available. It enables improved image quality and accurate quantitative mapping across different applications and field strengths, representing a promising technique toward broader clinical deployment of deep learning-based MRI.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Learning of Semantic Embedding Representations for Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05732</link>
<guid>https://arxiv.org/abs/2505.05732</guid>
<content:encoded><![CDATA[
arXiv:2505.05732v1 Announce Type: cross 
Abstract: Generative models capture the true distribution of data, yielding semantically rich representations. Denoising diffusion models (DDMs) exhibit superior generative capabilities, though efficient representation learning for them are lacking. In this work, we employ a multi-level denoising autoencoder framework to expand the representation capacity of DDMs, which introduces sequentially consistent Diffusion Transformers and an additional timestep-dependent encoder to acquire embedding representations on the denoising Markov chain through self-conditional diffusion learning. Intuitively, the encoder, conditioned on the entire diffusion process, compresses high-dimensional data into directional vectors in latent under different noise levels, facilitating the learning of image embeddings across all timesteps. To verify the semantic adequacy of embeddings generated through this approach, extensive experiments are conducted on various datasets, demonstrating that optimally learned embeddings by DDMs surpass state-of-the-art self-supervised representation learning methods in most cases, achieving remarkable discriminative semantic representation quality. Our work justifies that DDMs are not only suitable for generative tasks, but also potentially advantageous for general-purpose deep learning applications.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications</title>
<link>https://arxiv.org/abs/2505.05736</link>
<guid>https://arxiv.org/abs/2505.05736</guid>
<content:encoded><![CDATA[
arXiv:2505.05736v1 Announce Type: cross 
Abstract: The scarcity of high-quality multimodal biomedical data limits the ability to effectively fine-tune pretrained Large Language Models (LLMs) for specialized biomedical tasks. To address this challenge, we introduce MINT (Multimodal Integrated kNowledge Transfer), a framework that aligns unimodal large decoder models with domain-specific decision patterns from multimodal biomedical data through preference optimization. While MINT supports different optimization techniques, we primarily implement it with the Odds Ratio Preference Optimization (ORPO) framework as its backbone. This strategy enables the aligned LLMs to perform predictive tasks using text-only or image-only inputs while retaining knowledge learnt from multimodal data. MINT leverages an upstream multimodal machine learning (MML) model trained on high-quality multimodal data to transfer domain-specific insights to downstream text-only or image-only LLMs. We demonstrate its effectiveness through two key applications: (1) Rare genetic disease prediction from texts, where MINT uses a multimodal encoder model, trained on facial photos and clinical notes, to generate a preference dataset for aligning a lightweight Llama 3.2-3B-Instruct. Despite relying on text input only, the MINT-derived model outperforms models trained with SFT, RAG, or DPO, and even outperforms Llama 3.1-405B-Instruct. (2) Tissue type classification using cell nucleus images, where MINT uses a vision-language foundation model as the preference generator, containing knowledge learnt from both text and histopathological images to align downstream image-only models. The resulting MINT-derived model significantly improves the performance of Llama 3.2-Vision-11B-Instruct on tissue type classification. In summary, MINT provides an effective strategy to align unimodal LLMs with high-quality multimodal expertise through preference optimization.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Diabetic Macular Edema Treatment Responses Using OCT: Dataset and Methods of APTOS Competition</title>
<link>https://arxiv.org/abs/2505.05768</link>
<guid>https://arxiv.org/abs/2505.05768</guid>
<content:encoded><![CDATA[
arXiv:2505.05768v1 Announce Type: cross 
Abstract: Diabetic macular edema (DME) significantly contributes to visual impairment in diabetic patients. Treatment responses to intravitreal therapies vary, highlighting the need for patient stratification to predict therapeutic benefits and enable personalized strategies. To our knowledge, this study is the first to explore pre-treatment stratification for predicting DME treatment responses. To advance this research, we organized the 2nd Asia-Pacific Tele-Ophthalmology Society (APTOS) Big Data Competition in 2021. The competition focused on improving predictive accuracy for anti-VEGF therapy responses using ophthalmic OCT images. We provided a dataset containing tens of thousands of OCT images from 2,000 patients with labels across four sub-tasks. This paper details the competition's structure, dataset, leading methods, and evaluation metrics. The competition attracted strong scientific community participation, with 170 teams initially registering and 41 reaching the final round. The top-performing team achieved an AUC of 80.06%, highlighting the potential of AI in personalized DME treatment and clinical decision-making.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalizability of Kolmogorov-Arnold Networks via Error-Correcting Output Codes</title>
<link>https://arxiv.org/abs/2505.05798</link>
<guid>https://arxiv.org/abs/2505.05798</guid>
<content:encoded><![CDATA[
arXiv:2505.05798v1 Announce Type: cross 
Abstract: Kolmogorov-Arnold Networks (KAN) offer universal function approximation using univariate spline compositions without nonlinear activations. In this work, we integrate Error-Correcting Output Codes (ECOC) into the KAN framework to transform multi-class classification into multiple binary tasks, improving robustness via Hamming-distance decoding. Our proposed KAN with ECOC method outperforms vanilla KAN on a challenging blood cell classification dataset, achieving higher accuracy under diverse hyperparameter settings. Ablation studies further confirm that ECOC consistently enhances performance across FastKAN and FasterKAN variants. These results demonstrate that ECOC integration significantly boosts KAN generalizability in critical healthcare AI applications. To the best of our knowledge, this is the first integration of ECOC with KAN for enhancing multi-class medical image classification performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks</title>
<link>https://arxiv.org/abs/2505.05800</link>
<guid>https://arxiv.org/abs/2505.05800</guid>
<content:encoded><![CDATA[
arXiv:2505.05800v1 Announce Type: cross 
Abstract: Robotic manipulation in 3D requires learning an $N$ degree-of-freedom joint space trajectory of a robot manipulator. Robots must possess semantic and visual perception abilities to transform real-world mappings of their workspace into the low-level control necessary for object manipulation. Recent work has demonstrated the capabilities of fine-tuning large Vision-Language Models (VLMs) to learn the mapping between RGB images, language instructions, and joint space control. These models typically take as input RGB images of the workspace and language instructions, and are trained on large datasets of teleoperated robot demonstrations. In this work, we explore methods to improve the scene context awareness of a popular recent Vision-Language-Action model by integrating chain-of-thought reasoning, depth perception, and task-oriented region of interest detection. Our experiments in the LIBERO simulation environment show that our proposed model, 3D-CAVLA, improves the success rate across various LIBERO task suites, achieving an average success rate of 98.1$\%$. We also evaluate the zero-shot capabilities of our method, demonstrating that 3D scene awareness leads to robust learning and adaptation for completely unseen tasks. 3D-CAVLA achieves an absolute improvement of 8.8$\%$ on unseen tasks. We will open-source our code and the unseen tasks dataset to promote community-driven research here: https://3d-cavla.github.io
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards order of magnitude X-ray dose reduction in breast cancer imaging using phase contrast and deep denoising</title>
<link>https://arxiv.org/abs/2505.05812</link>
<guid>https://arxiv.org/abs/2505.05812</guid>
<content:encoded><![CDATA[
arXiv:2505.05812v1 Announce Type: cross 
Abstract: Breast cancer is the most frequently diagnosed human cancer in the United States at present. Early detection is crucial for its successful treatment. X-ray mammography and digital breast tomosynthesis are currently the main methods for breast cancer screening. However, both have known limitations in terms of their sensitivity and specificity to breast cancers, while also frequently causing patient discomfort due to the requirement for breast compression. Breast computed tomography is a promising alternative, however, to obtain high-quality images, the X-ray dose needs to be sufficiently high. As the breast is highly radiosensitive, dose reduction is particularly important. Phase-contrast computed tomography (PCT) has been shown to produce higher-quality images at lower doses and has no need for breast compression. It is demonstrated in the present study that, when imaging full fresh mastectomy samples with PCT, deep learning-based image denoising can further reduce the radiation dose by a factor of 16 or more, without any loss of image quality. The image quality has been assessed both in terms of objective metrics, such as spatial resolution and contrast-to-noise ratio, as well as in an observer study by experienced medical imaging specialists and radiologists. This work was carried out in preparation for live patient PCT breast cancer imaging, initially at specialized synchrotron facilities.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints</title>
<link>https://arxiv.org/abs/2505.05957</link>
<guid>https://arxiv.org/abs/2505.05957</guid>
<content:encoded><![CDATA[
arXiv:2505.05957v1 Announce Type: cross 
Abstract: While classical convolutional neural networks (CNNs) have revolutionized image classification, the emergence of quantum computing presents new opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs) leverage quantum mechanical properties and hold potential to outperform classical approaches. However, their implementation on current noisy intermediate-scale quantum (NISQ) devices remains challenging due to hardware limitations. In our research, we address this challenge by introducing an encoding scheme that significantly reduces the input dimensionality. We demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to directly process $28\times 28$ pixel MNIST images, eliminating the need for classical dimensionality reduction pre-processing. Additionally, we propose an automated framework based on expressibility, entanglement, and complexity characteristics to identify the building blocks of QCNNs, parameterized quantum circuits (PQCs). Our approach demonstrates advantages in accuracy and convergence speed with a similar parameter count compared to both hybrid QCNNs and classical CNNs. We validated our experiments on IBM's Heron r2 quantum processor, achieving $96.08\%$ classification accuracy, surpassing the $71.74\%$ benchmark of traditional approaches under identical training conditions. These results represent one of the first implementations of image classifications on real quantum hardware and validate the potential of quantum computing in this area.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding</title>
<link>https://arxiv.org/abs/2505.06020</link>
<guid>https://arxiv.org/abs/2505.06020</guid>
<content:encoded><![CDATA[
arXiv:2505.06020v1 Announce Type: cross 
Abstract: Understanding visual art requires reasoning across multiple perspectives -- cultural, historical, and stylistic -- beyond mere object recognition. While recent multimodal large language models (MLLMs) perform well on general image captioning, they often fail to capture the nuanced interpretations that fine art demands. We propose ArtRAG, a novel, training-free framework that combines structured knowledge with retrieval-augmented generation (RAG) for multi-perspective artwork explanation. ArtRAG automatically constructs an Art Context Knowledge Graph (ACKG) from domain-specific textual sources, organizing entities such as artists, movements, themes, and historical events into a rich, interpretable graph. At inference time, a multi-granular structured retriever selects semantically and topologically relevant subgraphs to guide generation. This enables MLLMs to produce contextually grounded, culturally informed art descriptions. Experiments on the SemArt and Artpedia datasets show that ArtRAG outperforms several heavily trained baselines. Human evaluations further confirm that ArtRAG generates coherent, insightful, and culturally enriched interpretations.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Are You Wrong? Counterfactual Explanations for Language Grounding with 3D Objects</title>
<link>https://arxiv.org/abs/2505.06030</link>
<guid>https://arxiv.org/abs/2505.06030</guid>
<content:encoded><![CDATA[
arXiv:2505.06030v1 Announce Type: cross 
Abstract: Combining natural language and geometric shapes is an emerging research area with multiple applications in robotics and language-assisted design. A crucial task in this domain is object referent identification, which involves selecting a 3D object given a textual description of the target. Variability in language descriptions and spatial relationships of 3D objects makes this a complex task, increasing the need to better understand the behavior of neural network models in this domain. However, limited research has been conducted in this area. Specifically, when a model makes an incorrect prediction despite being provided with a seemingly correct object description, practitioners are left wondering: "Why is the model wrong?". In this work, we present a method answering this question by generating counterfactual examples. Our method takes a misclassified sample, which includes two objects and a text description, and generates an alternative yet similar formulation that would have resulted in a correct prediction by the model. We have evaluated our approach with data from the ShapeTalk dataset along with three distinct models. Our counterfactual examples maintain the structure of the original description, are semantically similar and meaningful. They reveal weaknesses in the description, model bias and enhance the understanding of the models behavior. Theses insights help practitioners to better interact with systems as well as engineers to improve models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations</title>
<link>https://arxiv.org/abs/2505.06079</link>
<guid>https://arxiv.org/abs/2505.06079</guid>
<content:encoded><![CDATA[
arXiv:2505.06079v1 Announce Type: cross 
Abstract: Preference feedback collected by human or VLM annotators is often noisy, presenting a significant challenge for preference-based reinforcement learning that relies on accurate preference labels. To address this challenge, we propose TREND, a novel framework that integrates few-shot expert demonstrations with a tri-teaching strategy for effective noise mitigation. Our method trains three reward models simultaneously, where each model views its small-loss preference pairs as useful knowledge and teaches such useful pairs to its peer network for updating the parameters. Remarkably, our approach requires as few as one to three expert demonstrations to achieve high performance. We evaluate TREND on various robotic manipulation tasks, achieving up to 90% success rates even with noise levels as high as 40%, highlighting its effective robustness in handling noisy preference feedback. Project page: https://shuaiyihuang.github.io/publications/TREND.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2MNet: Speckle-To-Mesh Net for Three-Dimensional Cardiac Morphology Reconstruction via Echocardiogram</title>
<link>https://arxiv.org/abs/2505.06105</link>
<guid>https://arxiv.org/abs/2505.06105</guid>
<content:encoded><![CDATA[
arXiv:2505.06105v1 Announce Type: cross 
Abstract: Echocardiogram is the most commonly used imaging modality in cardiac assessment duo to its non-invasive nature, real-time capability, and cost-effectiveness. Despite its advantages, most clinical echocardiograms provide only two-dimensional views, limiting the ability to fully assess cardiac anatomy and function in three dimensions. While three-dimensional echocardiography exists, it often suffers from reduced resolution, limited availability, and higher acquisition costs. To overcome these challenges, we propose a deep learning framework S2MNet that reconstructs continuous and high-fidelity 3D heart models by integrating six slices of routinely acquired 2D echocardiogram views. Our method has three advantages. First, our method avoid the difficulties on training data acquasition by simulate six of 2D echocardiogram images from corresponding slices of a given 3D heart mesh. Second, we introduce a deformation field-based method, which avoid spatial discontinuities or structural artifacts in 3D echocardiogram reconstructions. We validate our method using clinically collected echocardiogram and demonstrate that our estimated left ventricular volume, a key clinical indicator of cardiac function, is strongly correlated with the doctor measured GLPS, a clinical measurement that should demonstrate a negative correlation with LVE in medical theory. This association confirms the reliability of our proposed 3D construction method.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Application of Deep Learning for Lymph Node Segmentation: A Systematic Review</title>
<link>https://arxiv.org/abs/2505.06118</link>
<guid>https://arxiv.org/abs/2505.06118</guid>
<content:encoded><![CDATA[
arXiv:2505.06118v1 Announce Type: cross 
Abstract: Automatic lymph node segmentation is the cornerstone for advances in computer vision tasks for early detection and staging of cancer. Traditional segmentation methods are constrained by manual delineation and variability in operator proficiency, limiting their ability to achieve high accuracy. The introduction of deep learning technologies offers new possibilities for improving the accuracy of lymph node image analysis. This study evaluates the application of deep learning in lymph node segmentation and discusses the methodologies of various deep learning architectures such as convolutional neural networks, encoder-decoder networks, and transformers in analyzing medical imaging data across different modalities. Despite the advancements, it still confronts challenges like the shape diversity of lymph nodes, the scarcity of accurately labeled datasets, and the inadequate development of methods that are robust and generalizable across different imaging modalities. To the best of our knowledge, this is the first study that provides a comprehensive overview of the application of deep learning techniques in lymph node segmentation task. Furthermore, this study also explores potential future research directions, including multimodal fusion techniques, transfer learning, and the use of large-scale pre-trained models to overcome current limitations while enhancing cancer diagnosis and treatment planning strategies.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wasserstein Distances Made Explainable: Insights into Dataset Shifts and Transport Phenomena</title>
<link>https://arxiv.org/abs/2505.06123</link>
<guid>https://arxiv.org/abs/2505.06123</guid>
<content:encoded><![CDATA[
arXiv:2505.06123v1 Announce Type: cross 
Abstract: Wasserstein distances provide a powerful framework for comparing data distributions. They can be used to analyze processes over time or to detect inhomogeneities within data. However, simply calculating the Wasserstein distance or analyzing the corresponding transport map (or coupling) may not be sufficient for understanding what factors contribute to a high or low Wasserstein distance. In this work, we propose a novel solution based on Explainable AI that allows us to efficiently and accurately attribute Wasserstein distances to various data components, including data subgroups, input features, or interpretable subspaces. Our method achieves high accuracy across diverse datasets and Wasserstein distance specifications, and its practical utility is demonstrated in two use cases.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills</title>
<link>https://arxiv.org/abs/2505.06176</link>
<guid>https://arxiv.org/abs/2505.06176</guid>
<content:encoded><![CDATA[
arXiv:2505.06176v1 Announce Type: cross 
Abstract: Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (e.g., Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that is challenging to plan for most novices. In this paper, we ask if a multimodal large language model (MLLM) can be taught to critique raw photographs, suggest suitable remedies, and finally realize them with a given set of pre-authored procedural image operations. We demonstrate that MLLMs can be first made aware of the underlying image processing operations, by training them to solve specially designed visual puzzles. Subsequently, such an operation-aware MLLM can both plan and propose edit sequences. To facilitate training, given a set of expert-edited photos, we synthesize a reasoning dataset by procedurally manipulating the expert edits and then grounding a pretrained LLM on the visual adjustments, to synthesize reasoning for finetuning. The proposed retouching operations are, by construction, understandable by the users, preserve object details and resolution, and can be optionally overridden. We evaluate our setup on a variety of test examples and show advantages, in terms of explainability and identity preservation, over existing generative and other procedural alternatives. Code, data, models, and supplementary results can be found via our project website at https://monetgpt.github.io.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet</title>
<link>https://arxiv.org/abs/2505.06185</link>
<guid>https://arxiv.org/abs/2505.06185</guid>
<content:encoded><![CDATA[
arXiv:2505.06185v1 Announce Type: cross 
Abstract: This paper proposes a method MTL-Swin-Unet which is multi-task learning using transformers for classification and semantic segmentation. For spurious-correlation problems, this method allows us to enhance the image representation with two other image representations: representation obtained by semantic segmentation and representation obtained by image reconstruction. In our experiments, the proposed method outperformed in F-value measure than other classifiers when the test data included slices from the same patient (no covariate shift). Similarly, when the test data did not include slices from the same patient (covariate shift setting), the proposed method outperformed in AUC measure.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Concepts</title>
<link>https://arxiv.org/abs/2505.06191</link>
<guid>https://arxiv.org/abs/2505.06191</guid>
<content:encoded><![CDATA[
arXiv:2505.06191v1 Announce Type: cross 
Abstract: This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented using a combination of symbolic programs and neural network representations. Leveraging such neuro-symbolic concepts, the agent can efficiently learn and recombine them to solve various tasks across different domains, ranging from 2D images, videos, 3D scenes, and robotic manipulation tasks. This concept-centric framework offers several advantages, including data efficiency, compositional generalization, continual learning, and zero-shot transfer.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp Segmentation</title>
<link>https://arxiv.org/abs/2505.06210</link>
<guid>https://arxiv.org/abs/2505.06210</guid>
<content:encoded><![CDATA[
arXiv:2505.06210v1 Announce Type: cross 
Abstract: Convolutional neural network (CNN) and Transformer-based architectures are two dominant deep learning models for polyp segmentation. However, CNNs have limited capability for modeling long-range dependencies, while Transformers incur quadratic computational complexity. Recently, State Space Models such as Mamba have been recognized as a promising approach for polyp segmentation because they not only model long-range interactions effectively but also maintain linear computational complexity. However, Mamba-based architectures still struggle to capture topological features (e.g., connected components, loops, voids), leading to inaccurate boundary delineation and polyp segmentation. To address these limitations, we propose a new approach called Topo-VM-UNetV2, which encodes topological features into the Mamba-based state-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of two stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for the training and test images, which are then used to compute topology attention maps. Specifically, we first compute persistence diagrams of the PMs, then we generate persistence score maps by assigning persistence values (i.e., the difference between death and birth times) of each topological feature to its birth location, finally we transform persistence scores into attention weights using the sigmoid function. Stage 2: These topology attention maps are integrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to form a topology-guided semantics and detail infusion (Topo-SDI) module for enhancing the segmentation results. Extensive experiments on five public polyp segmentation datasets demonstrate the effectiveness of our proposed method. The code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let Humanoids Hike! Integrative Skill Development on Complex Trails</title>
<link>https://arxiv.org/abs/2505.06218</link>
<guid>https://arxiv.org/abs/2505.06218</guid>
<content:encoded><![CDATA[
arXiv:2505.06218v1 Announce Type: cross 
Abstract: Hiking on complex trails demands balance, agility, and adaptive decision-making over unpredictable terrain. Current humanoid research remains fragmented and inadequate for hiking: locomotion focuses on motor skills without long-term goals or situational awareness, while semantic navigation overlooks real-world embodiment and local terrain variability. We propose training humanoids to hike on complex trails, driving integrative skill development across visual perception, decision making, and motor execution. We develop a learning framework, LEGO-H, that enables a vision-equipped humanoid robot to hike complex trails autonomously. We introduce two technical innovations: 1) A temporal vision transformer variant - tailored into Hierarchical Reinforcement Learning framework - anticipates future local goals to guide movement, seamlessly integrating locomotion with goal-directed navigation. 2) Latent representations of joint movement patterns, combined with hierarchical metric learning - enhance Privileged Learning scheme - enable smooth policy transfer from privileged training to onboard execution. These components allow LEGO-H to handle diverse physical and environmental challenges without relying on predefined motion patterns. Experiments across varied simulated trails and robot morphologies highlight LEGO-H's versatility and robustness, positioning hiking as a compelling testbed for embodied autonomy and LEGO-H as a baseline for future humanoid development.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anymate: A Dataset and Baselines for Learning 3D Object Rigging</title>
<link>https://arxiv.org/abs/2505.06227</link>
<guid>https://arxiv.org/abs/2505.06227</guid>
<content:encoded><![CDATA[
arXiv:2505.06227v1 Announce Type: cross 
Abstract: Rigging and skinning are essential steps to create realistic 3D animations, often requiring significant expertise and manual effort. Traditional attempts at automating these processes rely heavily on geometric heuristics and often struggle with objects of complex geometry. Recent data-driven approaches show potential for better generality, but are often constrained by limited training data. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets paired with expert-crafted rigging and skinning information -- 70 times larger than existing datasets. Using this dataset, we propose a learning-based auto-rigging framework with three sequential modules for joint, connectivity, and skinning weight prediction. We systematically design and experiment with various architectures as baselines for each module and conduct comprehensive evaluations on our dataset to compare their performance. Our models significantly outperform existing methods, providing a foundation for comparing future methods in automated rigging and skinning. Code and dataset can be found at https://anymate3d.github.io/.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation</title>
<link>https://arxiv.org/abs/2303.17051</link>
<guid>https://arxiv.org/abs/2303.17051</guid>
<content:encoded><![CDATA[
arXiv:2303.17051v4 Announce Type: replace 
Abstract: The recent popularity of foundation models and the pre-train-and-adapt paradigm, where a large-scale model is transferred to downstream tasks, is gaining attention for volumetric medical image segmentation. However, current transfer learning strategies devoted to full fine-tuning for transfer learning may require significant resources and yield sub-optimal results when the labeled data of the target task is scarce. This makes its applicability in real clinical settings challenging since these institutions are usually constrained on data and computational resources to develop proprietary solutions. To address this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a novel and realistic scenario for adapting medical image segmentation foundation models. This setting considers the key role of both data- and parameter-efficiency during adaptation. Building on a foundation model pre-trained on open-access CT organ segmentation sources, we propose leveraging Parameter-Efficient Fine-Tuning and black-box Adapters to address such challenges. Furthermore, novel efficient adaptation methodologies are introduced in this work, which include Spatial black-box Adapters that are more appropriate for dense prediction tasks and constrained transductive inference, leveraging task-specific prior knowledge. Our comprehensive transfer learning experiments confirm the suitability of foundation models in medical image segmentation and unveil the limitations of popular fine-tuning strategies in few-shot scenarios.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SuperBench: A Super-Resolution Benchmark Dataset for Scientific Machine Learning</title>
<link>https://arxiv.org/abs/2306.14070</link>
<guid>https://arxiv.org/abs/2306.14070</guid>
<content:encoded><![CDATA[
arXiv:2306.14070v2 Announce Type: replace 
Abstract: Super-resolution (SR) techniques aim to enhance data resolution, enabling the retrieval of finer details, and improving the overall quality and fidelity of the data representation. There is growing interest in applying SR methods to complex spatiotemporal systems within the Scientific Machine Learning (SciML) community, with the hope of accelerating numerical simulations and/or improving forecasts in weather, climate, and related areas. However, the lack of standardized benchmark datasets for comparing and validating SR methods hinders progress and adoption in SciML. To address this, we introduce SuperBench, the first benchmark dataset featuring high-resolution datasets, including data from fluid flows, cosmology, and weather. Here, we focus on validating spatial SR performance from data-centric and physics-preserved perspectives, as well as assessing robustness to data degradation tasks. While deep learning-based SR methods (developed in the computer vision community) excel on certain tasks, despite relatively limited prior physics information, we identify limitations of these methods in accurately capturing intricate fine-scale features and preserving fundamental physical properties and constraints in scientific data. These shortcomings highlight the importance and subtlety of incorporating domain knowledge into ML models. We anticipate that SuperBench will help to advance SR methods for science.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image space formalism of convolutional neural networks for k-space interpolation</title>
<link>https://arxiv.org/abs/2402.17410</link>
<guid>https://arxiv.org/abs/2402.17410</guid>
<content:encoded><![CDATA[
arXiv:2402.17410v2 Announce Type: replace 
Abstract: Purpose: Noise resilience in image reconstructions by scan-specific robust artificial neural networks for k-space interpolation (RAKI) is linked to nonlinear activations in k-space. To gain a deeper understanding of this relationship, an image space formalism of RAKI is introduced for analyzing noise propagation analytically, identifying and characterizing image reconstruction features and to describe the role of nonlinear activations in a human readable manner. Methods: The image space formalism for RAKI inference is employed by expressing nonlinear activations in k-space as element-wise multiplications with activation masks, which transform into convolutions in image space. Jacobians of the de-aliased, coil-combined image relative to the aliased coil images can be expressed algebraically, and thus, the noise amplification is quantified analytically (g-factor maps). We analyze the role of nonlinearity for noise resilience by controlling the degree of nonlinearity in the reconstruction model via the negative slope parameter in leaky ReLU. Results: The analytical g-factor maps correspond with those obtained from Monte Carlo simulations and from an auto differentiation approach for in vivo brain images. Apparent blurring and contrast loss artifacts are identified as implications of enhanced noise resilience. These residual artifacts can be traded against noise resilience by adjusting the degree of nonlinearity in the model (Tikhonov-like regularization) in case of limited training data. The inspection of image space activations reveals an autocorrelation pattern leading to a potential center artifact. Conclusion: The image space formalism of RAKI provides the means for analytical quantitative noisepropagation analysis and human-readable visualization of the effects of the nonlinear activation functions in k-space.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations</title>
<link>https://arxiv.org/abs/2403.07887</link>
<guid>https://arxiv.org/abs/2403.07887</guid>
<content:encoded><![CDATA[
arXiv:2403.07887v4 Announce Type: replace 
Abstract: Several accounts of human cognition posit that our intelligence is rooted in our ability to form abstract composable concepts, ground them in our environment, and reason over these grounded entities. This trifecta of human thought has remained elusive in modern intelligent machines. In this work, we investigate whether slot representations extracted from visual scenes serve as appropriate compositional abstractions for grounding and reasoning. We present the Neural Slot Interpreter (NSI), which learns to ground object semantics in slots. At the core of NSI is a nested schema that uses simple syntax rules to organize the object semantics of a scene into object-centric schema primitives. Then, the NSI metric learns to ground primitives into slots through a structured contrastive learning objective that reasons over the intermodal alignment. Experiments with a bi-modal object-property and scene retrieval task demonstrate the grounding efficacy and interpretability of correspondences learned by NSI. From a scene representation standpoint, we find that emergent NSI slots that move beyond the image grid by binding to spatial objects facilitate improved visual grounding compared to conventional bounding-box-based approaches. From a data efficiency standpoint, we empirically validate that NSI learns more generalizable representations from a fixed amount of annotation data than the traditional approach. We also show that the grounded slots surpass unsupervised slots in real-world object discovery and scale with scene complexity. Finally, we investigate the downstream efficacy of the grounded slots. Vision Transformers trained on grounding-aware NSI tokenizers using as few as ten tokens outperform patch-based tokens on challenging few-shot classification tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to build the best medical image segmentation algorithm using foundation models: a comprehensive empirical study with Segment Anything Model</title>
<link>https://arxiv.org/abs/2404.09957</link>
<guid>https://arxiv.org/abs/2404.09957</guid>
<content:encoded><![CDATA[
arXiv:2404.09957v3 Announce Type: replace 
Abstract: Automated segmentation is a fundamental medical image analysis task, which enjoys significant advances due to the advent of deep learning. While foundation models have been useful in natural language processing and some vision tasks for some time, the foundation model developed with image segmentation in mind - Segment Anything Model (SAM) - has been developed only recently and has shown similar promise. However, there are still no systematic analyses or "best-practice" guidelines for optimal fine-tuning of SAM for medical image segmentation. This work summarizes existing fine-tuning strategies with various backbone architectures, model components, and fine-tuning algorithms across 18 combinations, and evaluates them on 17 datasets covering all common radiology modalities. Our study reveals that (1) fine-tuning SAM leads to slightly better performance than previous segmentation methods, (2) fine-tuning strategies that use parameter-efficient learning in both the encoder and decoder are superior to other strategies, (3) network architecture has a small impact on final performance, (4) further training SAM with self-supervised learning can improve final model performance. We also demonstrate the ineffectiveness of some methods popular in the literature and further expand our experiments into few-shot and prompt-based settings. Lastly, we released our code and MRI-specific fine-tuned weights, which consistently obtained superior performance over the original SAM, at https://github.com/mazurowski-lab/finetune-SAM.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed Text Recognition with Efficient Parameter Fine-Tuning and Transformer</title>
<link>https://arxiv.org/abs/2404.12734</link>
<guid>https://arxiv.org/abs/2404.12734</guid>
<content:encoded><![CDATA[
arXiv:2404.12734v4 Announce Type: replace 
Abstract: With the rapid development of OCR technology, mixed-scene text recognition has become a key technical challenge. Although deep learning models have achieved significant results in specific scenarios, their generality and stability still need improvement, and the high demand for computing resources affects flexibility. To address these issues, this paper proposes DLoRA-TrOCR, a parameter-efficient hybrid text spotting method based on a pre-trained OCR Transformer. By embedding a weight-decomposed DoRA module in the image encoder and a LoRA module in the text decoder, this method can be efficiently fine-tuned on various downstream tasks. Our method requires no more than 0.7\% trainable parameters, not only accelerating the training efficiency but also significantly improving the recognition accuracy and cross-dataset generalization performance of the OCR system in mixed text scenes. Experiments show that our proposed DLoRA-TrOCR outperforms other parameter-efficient fine-tuning methods in recognizing complex scenes with mixed handwritten, printed, and street text, achieving a CER of 4.02 on the IAM dataset, a F1 score of 94.29 on the SROIE dataset, and a WAR of 86.70 on the STR Benchmark, reaching state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrowdMoGen: Zero-Shot Text-Driven Collective Motion Generation</title>
<link>https://arxiv.org/abs/2407.06188</link>
<guid>https://arxiv.org/abs/2407.06188</guid>
<content:encoded><![CDATA[
arXiv:2407.06188v2 Announce Type: replace 
Abstract: While recent advances in text-to-motion generation have shown promising results, they typically assume all individuals are grouped as a single unit. Scaling these methods to handle larger crowds and ensuring that individuals respond appropriately to specific events remains a significant challenge. This is primarily due to the complexities of scene planning, which involves organizing groups, planning their activities, and coordinating interactions, and controllable motion generation. In this paper, we present CrowdMoGen, the first zero-shot framework for collective motion generation, which effectively groups individuals and generates event-aligned motion sequences from text prompts. 1) Being limited by the available datasets for training an effective scene planning module in a supervised manner, we instead propose a crowd scene planner that leverages pre-trained large language models (LLMs) to organize individuals into distinct groups. While LLMs offer high-level guidance for group divisions, they lack the low-level understanding of human motion. To address this, we further propose integrating an SMPL-based joint prior to generate context-appropriate activities, which consists of both joint trajectories and textual descriptions. 2) Secondly, to incorporate the assigned activities into the generative network, we introduce a collective motion generator that integrates the activities into a transformer-based network in a joint-wise manner, maintaining the spatial constraints during the multi-step denoising process. Extensive experiments demonstrate that CrowdMoGen significantly outperforms previous approaches, delivering realistic, event-driven motion sequences that are spatially coherent. As the first framework of collective motion generation, CrowdMoGen has the potential to advance applications in urban simulation, crowd planning, and other large-scale interactive environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation Learning and Identity Adversarial Training for Facial Behavior Understanding</title>
<link>https://arxiv.org/abs/2407.11243</link>
<guid>https://arxiv.org/abs/2407.11243</guid>
<content:encoded><![CDATA[
arXiv:2407.11243v2 Announce Type: replace 
Abstract: Facial Action Unit (AU) detection has gained significant attention as it enables the breakdown of complex facial expressions into individual muscle movements. In this paper, we revisit two fundamental factors in AU detection: diverse and large-scale data and subject identity regularization. Motivated by recent advances in foundation models, we highlight the importance of data and introduce Face9M, a diverse dataset comprising 9 million facial images from multiple public sources. Pretraining a masked autoencoder on Face9M yields strong performance in AU detection and facial expression tasks. More importantly, we emphasize that the Identity Adversarial Training (IAT) has not been well explored in AU tasks. To fill this gap, we first show that subject identity in AU datasets creates shortcut learning for the model and leads to sub-optimal solutions to AU predictions. Secondly, we demonstrate that strong IAT regularization is necessary to learn identity-invariant features. Finally, we elucidate the design space of IAT and empirically show that IAT circumvents the identity-based shortcut learning and results in a better solution. Our proposed methods, Facial Masked Autoencoder (FMAE) and IAT, are simple, generic and effective. Remarkably, the proposed FMAE-IAT approach achieves new state-of-the-art F1 scores on BP4D (67.1\%), BP4D+ (66.8\%), and DISFA (70.1\%) databases, significantly outperforming previous work. We release the code and model at https://github.com/forever208/FMAE-IAT.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAPTRv2: Attention-based Position Update Improves Tracking Any Point</title>
<link>https://arxiv.org/abs/2407.16291</link>
<guid>https://arxiv.org/abs/2407.16291</guid>
<content:encoded><![CDATA[
arXiv:2407.16291v2 Announce Type: replace 
Abstract: In this paper, we present TAPTRv2, a Transformer-based approach built upon TAPTR for solving the Tracking Any Point (TAP) task. TAPTR borrows designs from DEtection TRansformer (DETR) and formulates each tracking point as a point query, making it possible to leverage well-studied operations in DETR-like algorithms. TAPTRv2 improves TAPTR by addressing a critical issue regarding its reliance on cost-volume,which contaminates the point query\'s content feature and negatively impacts both visibility prediction and cost-volume computation. In TAPTRv2, we propose a novel attention-based position update (APU) operation and use key-aware deformable attention to realize. For each query, this operation uses key-aware attention weights to combine their corresponding deformable sampling positions to predict a new query position. This design is based on the observation that local attention is essentially the same as cost-volume, both of which are computed by dot-production between a query and its surrounding features. By introducing this new operation, TAPTRv2 not only removes the extra burden of cost-volume computation, but also leads to a substantial performance improvement. TAPTRv2 surpasses TAPTR and achieves state-of-the-art performance on many challenging datasets, demonstrating the superiority
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed-CP: Directed Collaborative Perception for Connected and Autonomous Vehicles via Proactive Attention</title>
<link>https://arxiv.org/abs/2409.08840</link>
<guid>https://arxiv.org/abs/2409.08840</guid>
<content:encoded><![CDATA[
arXiv:2409.08840v3 Announce Type: replace 
Abstract: Collaborative perception (CP) leverages visual data from connected and autonomous vehicles (CAV) to enhance an ego vehicle's field of view (FoV). Despite recent progress, current CP methods expand the ego vehicle's 360-degree perceptual range almost equally, which faces two key challenges. Firstly, in areas with uneven traffic distribution, focusing on directions with little traffic offers limited benefits. Secondly, under limited communication budgets, allocating excessive bandwidth to less critical directions lowers the perception accuracy in more vital areas. To address these issues, we propose Direct-CP, a proactive and direction-aware CP system aiming at improving CP in specific directions. Our key idea is to enable an ego vehicle to proactively signal its interested directions and readjust its attention to enhance local directional CP performance. To achieve this, we first propose an RSU-aided direction masking mechanism that assists an ego vehicle in identifying vital directions. Additionally, we design a direction-aware selective attention module to wisely aggregate pertinent features based on ego vehicle's directional priorities, communication budget, and the positional data of CAVs. Moreover, we introduce a direction-weighted detection loss (DWLoss) to capture the divergence between directional CP outcomes and the ground truth, facilitating effective model training. Extensive experiments on the V2X-Sim 2.0 dataset demonstrate that our approach achieves 19.8\% higher local perception accuracy in interested directions and 2.5\% higher overall perception accuracy than the state-of-the-art methods in collaborative 3D object detection tasks.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Screen Time Identification in Children with a Multi-View Vision Language Model and Screen Time Tracker</title>
<link>https://arxiv.org/abs/2410.01966</link>
<guid>https://arxiv.org/abs/2410.01966</guid>
<content:encoded><![CDATA[
arXiv:2410.01966v3 Announce Type: replace 
Abstract: Being able to accurately monitor the screen exposure of young children is important for research on phenomena linked to screen use such as childhood obesity, physical activity, and social interaction. Most existing studies rely upon self-report or manual measures from bulky wearable sensors, thus lacking efficiency and accuracy in capturing quantitative screen exposure data. In this work, we developed a novel sensor informatics framework that utilizes egocentric images from a wearable sensor, termed the screen time tracker (STT), and a vision language model (VLM). In particular, we devised a multi-view VLM that takes multiple views from egocentric image sequences and interprets screen exposure dynamically. We validated our approach by using a dataset of children's free-living activities, demonstrating significant improvement over existing methods in plain vision language models and object detection models. Results supported the promise of this monitoring approach, which could optimize behavioral research on screen exposure in children's naturalistic settings.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Egocentric and Exocentric Methods: A Short Survey</title>
<link>https://arxiv.org/abs/2410.20621</link>
<guid>https://arxiv.org/abs/2410.20621</guid>
<content:encoded><![CDATA[
arXiv:2410.20621v2 Announce Type: replace 
Abstract: Egocentric vision captures the scene from the point of view of the camera wearer, while exocentric vision captures the overall scene context. Jointly modeling ego and exo views is crucial to developing next-generation AI agents. The community has regained interest in the field of egocentric vision. While the third-person view and first-person have been thoroughly investigated, very few works aim to study both synchronously. Exocentric videos contain many relevant signals that are transferrable to egocentric videos. This paper provides a timely overview of works combining egocentric and exocentric visions, a very new but promising research topic. We describe in detail the datasets and present a survey of the key applications of ego-exo joint learning, where we identify the most recent advances. With the presentation of the current status of the progress, we believe this short but timely survey will be valuable to the broad video-understanding community, particularly when multi-view modeling is critical.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VladVA: Discriminative Fine-tuning of LVLMs</title>
<link>https://arxiv.org/abs/2412.04378</link>
<guid>https://arxiv.org/abs/2412.04378</guid>
<content:encoded><![CDATA[
arXiv:2412.04378v3 Announce Type: replace 
Abstract: Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a "bag of words" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown to be capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks.
  In this work, we propose to combine "the best of both worlds": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding.
  Our contributions include (1) a carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components; (2) a parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters; (3) significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities</title>
<link>https://arxiv.org/abs/2412.14123</link>
<guid>https://arxiv.org/abs/2412.14123</guid>
<content:encoded><![CDATA[
arXiv:2412.14123v3 Announce Type: replace 
Abstract: Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and scale-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of 5 multimodal datasets with varying characteristics and $11$ distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned or probed, we reach state-of-the-art results on the test sets of GeoPlex and for 6 external datasets across various environment monitoring tasks: land cover mapping, tree species identification, crop type classification, change detection, climate type classification, and segmentation of flood, burn scar, and deforestation. The code and models are available at https://github.com/gastruc/AnySat.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning an Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking</title>
<link>https://arxiv.org/abs/2412.20002</link>
<guid>https://arxiv.org/abs/2412.20002</guid>
<content:encoded><![CDATA[
arXiv:2412.20002v2 Announce Type: replace 
Abstract: Visual tracking has made significant strides due to the adoption of transformer-based models. Most state-of-the-art trackers struggle to meet real-time processing demands on mobile platforms with constrained computing resources, particularly for real-time unmanned aerial vehicle (UAV) tracking. To achieve a better balance between performance and efficiency, we introduce AVTrack, an adaptive computation framework designed to selectively activate transformer blocks for real-time UAV tracking. The proposed Activation Module (AM) dynamically optimizes the ViT architecture by selectively engaging relevant components, thereby enhancing inference efficiency without significant compromise to tracking performance. Furthermore, to tackle the challenges posed by extreme changes in viewing angles often encountered in UAV tracking, the proposed method enhances ViTs' effectiveness by learning view-invariant representations through mutual information (MI) maximization. Two effective design principles are proposed in the AVTrack. Building on it, we propose an improved tracker, dubbed AVTrack-MD, which introduces the novel MI maximization-based multi-teacher knowledge distillation (MD) framework. It harnesses the benefits of multiple teachers, specifically the off-the-shelf tracking models from the AVTrack, by integrating and refining their outputs, thereby guiding the learning process of the compact student network. Specifically, we maximize the MI between the softened feature representations from the multi-teacher models and the student model, leading to improved generalization and performance of the student model, particularly in noisy conditions. Extensive experiments on multiple UAV tracking benchmarks demonstrate that AVTrack-MD not only achieves performance comparable to the AVTrack baseline but also reduces model complexity, resulting in a significant 17\% increase in average tracking speed.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Class Discovery in Instance Segmentation</title>
<link>https://arxiv.org/abs/2502.08149</link>
<guid>https://arxiv.org/abs/2502.08149</guid>
<content:encoded><![CDATA[
arXiv:2502.08149v2 Announce Type: replace 
Abstract: This work addresses the task of generalized class discovery (GCD) in instance segmentation. The goal is to discover novel classes and obtain a model capable of segmenting instances of both known and novel categories, given labeled and unlabeled data. Since the real world contains numerous objects with long-tailed distributions, the instance distribution for each class is inherently imbalanced. To address the imbalanced distributions, we propose an instance-wise temperature assignment (ITA) method for contrastive learning and class-wise reliability criteria for pseudo-labels. The ITA method relaxes instance discrimination for samples belonging to head classes to enhance GCD. The reliability criteria are to avoid excluding most pseudo-labels for tail classes when training an instance segmentation network using pseudo-labels from GCD. Additionally, we propose dynamically adjusting the criteria to leverage diverse samples in the early stages while relying only on reliable pseudo-labels in the later stages. We also introduce an efficient soft attention module to encode object-specific representations for GCD. Finally, we evaluate our proposed method by conducting experiments on two settings: COCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Pretraining for Fine-Grained Plankton Recognition</title>
<link>https://arxiv.org/abs/2503.11341</link>
<guid>https://arxiv.org/abs/2503.11341</guid>
<content:encoded><![CDATA[
arXiv:2503.11341v2 Announce Type: replace 
Abstract: Plankton recognition is an important computer vision problem due to plankton's essential role in ocean food webs and carbon capture, highlighting the need for species-level monitoring. However, this task is challenging due to its fine-grained nature and dataset shifts caused by different imaging instruments and varying species distributions. As new plankton image datasets are collected at an increasing pace, there is a need for general plankton recognition models that require minimal expert effort for data labeling. In this work, we study large-scale self-supervised pretraining for fine-grained plankton recognition. We first employ masked autoencoding and a large volume of diverse plankton image data to pretrain a general-purpose plankton image encoder. Then we utilize fine-tuning to obtain accurate plankton recognition models for new datasets with a very limited number of labeled training images. Our experiments show that self-supervised pretraining with diverse plankton data clearly increases plankton recognition accuracy compared to standard ImageNet pretraining when the amount of training data is limited. Moreover, the accuracy can be further improved when unlabeled target data is available and utilized during the pretraining.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion</title>
<link>https://arxiv.org/abs/2503.20698</link>
<guid>https://arxiv.org/abs/2503.20698</guid>
<content:encoded><![CDATA[
arXiv:2503.20698v4 Announce Type: replace 
Abstract: Videos inherently contain multiple modalities, including visual events, text overlays, sounds, and speech, all of which are important for retrieval. However, state-of-the-art multimodal language models like VAST and LanguageBind are built on vision-language models (VLMs), and thus overly prioritize visual signals. Retrieval benchmarks further reinforce this bias by focusing on visual queries and neglecting other modalities. We create a search system MMMORRF that extracts text and features from both visual and audio modalities and integrates them with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is both effective and efficient, demonstrating practicality in searching videos based on users' information needs instead of visual descriptive queries. We evaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed for more targeted information needs, and find that it improves nDCG@20 by 81% over leading multimodal encoders and 37% over single-modality retrieval, demonstrating the value of integrating diverse modalities.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D</title>
<link>https://arxiv.org/abs/2503.22976</link>
<guid>https://arxiv.org/abs/2503.22976</guid>
<content:encoded><![CDATA[
arXiv:2503.22976v3 Announce Type: replace 
Abstract: Recent advances in LVLMs have improved vision-language understanding, but they still struggle with spatial perception, limiting their ability to reason about complex 3D scenes. Unlike previous approaches that incorporate 3D representations into models to improve spatial understanding, we aim to unlock the potential of VLMs by leveraging spatially relevant image data. To this end, we introduce a novel 2D spatial data generation and annotation pipeline built upon scene data with 3D ground-truth. This pipeline enables the creation of a diverse set of spatial tasks, ranging from basic perception tasks to more complex reasoning tasks. Leveraging this pipeline, we construct SPAR-7M, a large-scale dataset generated from thousands of scenes across multiple public datasets. In addition, we introduce SPAR-Bench, a benchmark designed to offer a more comprehensive evaluation of spatial capabilities compared to existing spatial benchmarks, supporting both single-view and multi-view inputs. Training on both SPAR-7M and large-scale 2D datasets enables our models to achieve state-of-the-art performance on 2D spatial benchmarks. Further fine-tuning on 3D task-specific datasets yields competitive results, underscoring the effectiveness of our dataset in enhancing spatial reasoning.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models For Seismic Data Processing: An Extensive Review</title>
<link>https://arxiv.org/abs/2503.24166</link>
<guid>https://arxiv.org/abs/2503.24166</guid>
<content:encoded><![CDATA[
arXiv:2503.24166v2 Announce Type: replace 
Abstract: Seismic processing plays a crucial role in transforming raw data into high-quality subsurface images, pivotal for various geoscience applications. Despite its importance, traditional seismic processing techniques face challenges such as noisy and damaged data and the reliance on manual, time-consuming workflows. The emergence of deep learning approaches has introduced effective and user-friendly alternatives, yet many of these deep learning approaches rely on synthetic datasets and specialized neural networks. Recently, foundation models have gained traction in the seismic domain, due to their success in the natural image domain. Therefore, we investigate the application of natural image foundation models on the three seismic processing tasks: demultiple, interpolation, and denoising. We evaluate the impact of different model characteristics, such as pre-training technique and neural network architecture, on performance and efficiency. Rather than proposing a single seismic foundation model, we critically examine various natural image foundation models and suggest some promising candidates for future exploration.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visualization of a multidimensional point cloud as a 3D swarm of avatars</title>
<link>https://arxiv.org/abs/2504.06751</link>
<guid>https://arxiv.org/abs/2504.06751</guid>
<content:encoded><![CDATA[
arXiv:2504.06751v2 Announce Type: replace 
Abstract: The article presents an innovative approach to the visualization of multidimensional data, using icons inspired by Chernoff faces. The approach merges classical projection techniques with the assignment of particular data dimensions to mimic features, capitalizing on the natural ability of the human brain to interpret facial expressions. We introduce a semantic division of data dimensions into intuitive and technical categories, assigning the former to avatar features and projecting the latter into a hyperspace of four, or potentially more dimensions. The technique is implemented as a plugin to the dpVision open-source image handling platform. The plugin allows the data to be interactively explored in the form of a swarm of avatars whose position in hyperspace as well as facial features represent various aspects of the data. Sample visualizations, based on synthetic test data as well as the 12-dimensional database on Portuguese Vinho Verde wines, confirm the usefulness of our approach to the analysis of complex data structures.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Patch distribution modeling framework adaptive cosine estimator (PaDiM-ACE) for anomaly detection and localization in synthetic aperture radar imagery</title>
<link>https://arxiv.org/abs/2504.08049</link>
<guid>https://arxiv.org/abs/2504.08049</guid>
<content:encoded><![CDATA[
arXiv:2504.08049v2 Announce Type: replace 
Abstract: This work presents a new approach to anomaly detection and localization in synthetic aperture radar imagery (SAR), expanding upon the existing patch distribution modeling framework (PaDiM). We introduce the adaptive cosine estimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at inference, an unbounded metric. ACE instead uses the cosine similarity metric, providing bounded anomaly detection scores. The proposed method is evaluated across multiple SAR datasets, with performance metrics including the area under the receiver operating curve (AUROC) at the image and pixel level, aiming for increased performance in anomaly detection and localization of SAR imagery. The code is publicly available: https://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-ACE.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2504.13580</link>
<guid>https://arxiv.org/abs/2504.13580</guid>
<content:encoded><![CDATA[
arXiv:2504.13580v2 Announce Type: replace 
Abstract: High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in ScanNet scenes with their 9D poses and CAD models. This time, we apply it to the recent ScanNet++ v1 dataset, which previously lacked such annotations. Our findings demonstrate that it is not only possible to train deep learning models on these automatically-obtained annotations but that the resulting models outperform those trained on manually annotated data. We validate this on two distinct tasks: point cloud completion and single-view CAD model retrieval and alignment. Our results underscore the potential of automatic 3D annotations to enhance model performance while significantly reducing annotation costs. To support future research in 3D scene understanding, we will release our annotations, which we call SCANnotate++, along with our trained models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Perception-Inspired Grain Segmentation Refinement Using Conditional Random Fields</title>
<link>https://arxiv.org/abs/2312.09968</link>
<guid>https://arxiv.org/abs/2312.09968</guid>
<content:encoded><![CDATA[
arXiv:2312.09968v2 Announce Type: replace-cross 
Abstract: Automated detection of grain boundaries in electron microscope images of polycrystalline materials could help accelerate the nanoscale characterization of myriad engineering materials and novel materials under scientific research. Accurate segmentation of interconnected line networks, such as grain boundaries in polycrystalline material microstructures, poses a significant challenge due to the fragmented masks produced by conventional computer vision algorithms, including convolutional neural networks. These algorithms struggle with thin masks, often necessitating post-processing for effective contour closure and continuity. Previous approaches in this domain have typically relied on custom post-processing techniques that are problem-specific and heavily dependent on the quality of the mask obtained from a computer vision algorithm. Addressing this issue, this paper introduces a fast, high-fidelity post-processing technique that is universally applicable to segmentation masks of interconnected line networks. Leveraging domain knowledge about grain boundary connectivity, this method employs conditional random fields and perceptual grouping rules to refine segmentation masks of any image with a discernible grain structure. This approach significantly enhances segmentation mask accuracy, achieving a 79% segment identification accuracy in validation with a U-Net model on electron microscopy images of a polycrystalline oxide. Additionally, a novel grain alignment metric is introduced, showing a 51% improvement in grain alignment. This method not only enables rapid and accurate segmentation but also facilitates an unprecedented level of data analysis, significantly improving the statistical representation of grain boundary networks, making it suitable for a range of disciplines where precise segmentation of interconnected line networks is essential.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Drift Detection in Medical Imaging with Sketching and Fine-Tuned Transformer</title>
<link>https://arxiv.org/abs/2408.08456</link>
<guid>https://arxiv.org/abs/2408.08456</guid>
<content:encoded><![CDATA[
arXiv:2408.08456v2 Announce Type: replace-cross 
Abstract: Distributional drift detection is important in medical applications as it helps ensure the accuracy and reliability of models by identifying changes in the underlying data distribution that could affect the prediction results of machine learning models. However, current methods have limitations in detecting drift, for example, the inclusion of abnormal datasets can lead to unfair comparisons. This paper presents an accurate and sensitive approach to detect distributional drift in CT-scan medical images by leveraging data-sketching and fine-tuning techniques. We developed a robust baseline library model for real-time anomaly detection, allowing for efficient comparison of incoming images and identification of anomalies. Additionally, we fine-tuned a pre-trained Vision Transformer model to extract relevant features, using mammography as a case study, significantly enhancing model accuracy to 99.11%. Combining with data-sketches and fine-tuning, our feature extraction evaluation demonstrated that cosine similarity scores between similar datasets provide greater improvements, from around 50% increased to 99.1%. Finally, the sensitivity evaluation shows that our solutions are highly sensitive to even 1% salt-and-pepper and speckle noise, and it is not sensitive to lighting noise (e.g., lighting conditions have no impact on data drift). The proposed methods offer a scalable and reliable solution for maintaining the accuracy of diagnostic models in dynamic clinical environments.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Autoregressive Transformers for Model-Agnostic Federated MRI Reconstruction</title>
<link>https://arxiv.org/abs/2502.04521</link>
<guid>https://arxiv.org/abs/2502.04521</guid>
<content:encoded><![CDATA[
arXiv:2502.04521v2 Announce Type: replace-cross 
Abstract: Although learning-based models hold great promise for MRI reconstruction, single-site models built on limited local datasets often suffer from poor generalization. This challenge has spurred interest in collaborative model training on multi-site datasets via federated learning (FL) -- a privacy-preserving framework that aggregates model updates instead of sharing imaging data. Conventional FL aggregates locally trained model weights into a global model, inherently constraining all sites to use a homogeneous model architecture. This rigidity forces sites to compromise on architectures tailored to their compute resources and application-specific needs, making conventional FL unsuitable for model-heterogeneous settings where each site may prefer a distinct architecture. To overcome this limitation, we introduce FedGAT, a novel model-agnostic FL technique based on generative autoregressive transformers. FedGAT decentralizes the training of a global generative prior that learns the distribution of multi-site MR images. For high-fidelity synthesis, we propose a novel site-prompted GAT prior that controllably synthesizes realistic MR images from desired sites via autoregressive prediction across spatial scales. Each site then trains its own reconstruction model -- using an architecture of its choice -- on a hybrid dataset augmenting its local MRI dataset with GAT-generated synthetic MR images emulating datasets from other sites. This hybrid training strategy enables site-specific reconstruction models to generalize more effectively across diverse data distributions while preserving data privacy. Comprehensive experiments on multi-institutional datasets demonstrate that FedGAT enables flexible, model-heterogeneous collaborations and achieves superior within-site and cross-site reconstruction performance compared to state-of-the-art FL baselines.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure-preserving contrastive learning for spatial time series</title>
<link>https://arxiv.org/abs/2502.06380</link>
<guid>https://arxiv.org/abs/2502.06380</guid>
<content:encoded><![CDATA[
arXiv:2502.06380v3 Announce Type: replace-cross 
Abstract: The effectiveness of neural network models largely relies on learning meaningful latent patterns from data, where self-supervised learning of informative representations can enhance model performance and generalisability. However, self-supervised representation learning for spatially characterised time series, which are ubiquitous in transportation domain, poses unique challenges due to the necessity of maintaining fine-grained spatio-temporal similarities in the latent space. In this study, we introduce two structure-preserving regularisers for the contrastive learning of spatial time series: one regulariser preserves the topology of similarities between instances, and the other preserves the graph geometry of similarities across spatial and temporal dimensions. To balance the contrastive learning objective and the need for structure preservation, we propose a dynamic weighting mechanism that adaptively manages this trade-off and stabilises training. We validate the proposed method through extensive experiments, including multivariate time series classification to demonstrate its general applicability, as well as macroscopic and microscopic traffic prediction to highlight its particular usefulness in encoding traffic interactions. Across all tasks, our method preserves the similarity structures more effectively and improves state-of-the-art task performances. This method can be integrated with an arbitrary neural network model and is particularly beneficial for time series data with spatial or geographical features. Furthermore, our findings suggest that well-preserved similarity structures in the latent space indicate more informative and useful representations. This provides insights to design more effective neural networks for data-driven transportation research. Our code is made openly accessible with all resulting data at https://github.com/yiru-jiao/spclt
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RS2AD: End-to-End Autonomous Driving Data Generation from Roadside Sensor Observations</title>
<link>https://arxiv.org/abs/2503.07085</link>
<guid>https://arxiv.org/abs/2503.07085</guid>
<content:encoded><![CDATA[
arXiv:2503.07085v3 Announce Type: replace-cross 
Abstract: End-to-end autonomous driving solutions, which process multi-modal sensory data to directly generate refined control commands, have become a dominant paradigm in autonomous driving research. However, these approaches predominantly depend on single-vehicle data collection for model training and optimization, resulting in significant challenges such as high data acquisition and annotation costs, the scarcity of critical driving scenarios, and fragmented datasets that impede model generalization. To mitigate these limitations, we introduce RS2AD, a novel framework for reconstructing and synthesizing vehicle-mounted LiDAR data from roadside sensor observations. Specifically, our method transforms roadside LiDAR point clouds into the vehicle-mounted LiDAR coordinate system by leveraging the target vehicle's relative pose. Subsequently, high-fidelity vehicle-mounted LiDAR data is synthesized through virtual LiDAR modeling, point cloud classification, and resampling techniques. To the best of our knowledge, this is the first approach to reconstruct vehicle-mounted LiDAR data from roadside sensor inputs. Extensive experimental evaluations demonstrate that incorporating the data generated by the RS2AD method (the RS2V-L dataset) into model training as a supplement to the KITTI dataset can significantly enhance the accuracy of 3D object detection and greatly improve the efficiency of end-to-end autonomous driving data generation. These findings strongly validate the effectiveness of the proposed method and underscore its potential in reducing dependence on costly vehicle-mounted data collection while improving the robustness of autonomous driving models.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage</title>
<link>https://arxiv.org/abs/2504.20007</link>
<guid>https://arxiv.org/abs/2504.20007</guid>
<content:encoded><![CDATA[
arXiv:2504.20007v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating video, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. We present our methodology, computational techniques, and findings, outlining a practical approach for law enforcement while advancing the frontiers of knowledge discovery from police BWC data.
]]></content:encoded>
<pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Histo-Miner: Deep Learning based Tissue Features Extraction Pipeline from H&amp;E Whole Slide Images of Cutaneous Squamous Cell Carcinoma</title>
<link>https://arxiv.org/abs/2505.04672</link>
<guid>https://arxiv.org/abs/2505.04672</guid>
<content:encoded><![CDATA[
<div> Keywords: digital pathology, Whole-Slide Images, cutaneous squamous cell carcinoma, Histo-Miner, tumor segmentation  

<br /><br />Summary: Recent advancements in digital pathology facilitate the comprehensive analysis of Whole-Slide Images (WSI) from tissue samples, but there is a scarcity of labeled datasets and open-source pipelines specific to skin tissue analysis. To address this gap, we introduce Histo-Miner, a deep learning-based pipeline specifically designed for analyzing skin WSIs, particularly focusing on patients with cutaneous squamous cell carcinoma (cSCC), a prevalent form of non-melanoma skin cancer. We created two datasets featuring 47,392 annotated cell nuclei and 144 tumor-segmented WSIs from cSCC patients. Histo-Miner utilizes convolutional neural networks and vision transformers for nucleus segmentation, classification, and tumor region segmentation, achieving competitive performance metrics. The results include a multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation, a macro-averaged F1 score of 0.832 for nucleus classification, and a mean Intersection over Union (mIoU) of 0.884 for tumor segmentation. Furthermore, Histo-Miner forecasts patient responses to immunotherapy using WSI analysis, identifying key immune features that predict therapy outcomes, thus demonstrating its clinical relevance and interpretative power for underlying biological insights. <div>
arXiv:2505.04672v1 Announce Type: new 
Abstract: Recent advancements in digital pathology have enabled comprehensive analysis of Whole-Slide Images (WSI) from tissue samples, leveraging high-resolution microscopy and computational capabilities. Despite this progress, there is a lack of labeled datasets and open source pipelines specifically tailored for analysis of skin tissue. Here we propose Histo-Miner, a deep learning-based pipeline for analysis of skin WSIs and generate two datasets with labeled nuclei and tumor regions. We develop our pipeline for the analysis of patient samples of cutaneous squamous cell carcinoma (cSCC), a frequent non-melanoma skin cancer. Utilizing the two datasets, comprising 47,392 annotated cell nuclei and 144 tumor-segmented WSIs respectively, both from cSCC patients, Histo-Miner employs convolutional neural networks and vision transformers for nucleus segmentation and classification as well as tumor region segmentation. Performance of trained models positively compares to state of the art with multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation, macro-averaged F1 of 0.832 for nucleus classification and mean Intersection over Union (mIoU) of 0.884 for tumor region segmentation. From these predictions we generate a compact feature vector summarizing tissue morphology and cellular interactions, which can be used for various downstream tasks. Here, we use Histo-Miner to predict cSCC patient response to immunotherapy based on pre-treatment WSIs from 45 patients. Histo-Miner identifies percentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor vicinity and the distances between granulocytes and plasma cells in tumors as predictive features for therapy response. This highlights the applicability of Histo-Miner to clinically relevant scenarios, providing direct interpretation of the classification and insights into the underlying biology.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Visual Trackers for Biomechanical Analysis of Running</title>
<link>https://arxiv.org/abs/2505.04713</link>
<guid>https://arxiv.org/abs/2505.04713</guid>
<content:encoded><![CDATA[
<div> Keywords: human pose estimation, deep learning, biomechanical analysis, joint trackers, root mean squared errors 

<br /><br />Summary: This work explores the advancements in human pose estimation, particularly in the context of biomechanics for sprinting. It evaluates the performance of six different trackers, including two point trackers and four joint trackers, by comparing their outputs against manual annotations from biomechanical experts across 5870 frames. The study focuses on forty sprints from five professional runners and emphasizes three crucial angles in sprint biomechanics: trunk inclination, hip flex extension, and knee flex extension. To enhance accuracy, a post-processing module is introduced for outlier detection and fusion prediction of joint angles. The results indicate that the joint-based models achieved root mean squared errors ranging from 11.41° to 4.37°. When utilizing the post-processing modules, accuracy improves further, with errors reducing to 6.99° and 3.88°, respectively. The findings suggest that human pose tracking is a promising tool for biomechanical analysis in running. Still, the study identifies opportunities for improvement, particularly for applications demanding high precision. This research highlights the potential of integrating advanced pose estimation techniques into sports analysis and performance evaluation. <div>
arXiv:2505.04713v1 Announce Type: new 
Abstract: Human pose estimation has witnessed significant advancements in recent years, mainly due to the integration of deep learning models, the availability of a vast amount of data, and large computational resources. These developments have led to highly accurate body tracking systems, which have direct applications in sports analysis and performance evaluation.
  This work analyzes the performance of six trackers: two point trackers and four joint trackers for biomechanical analysis in sprints. The proposed framework compares the results obtained from these pose trackers with the manual annotations of biomechanical experts for more than 5870 frames. The experimental framework employs forty sprints from five professional runners, focusing on three key angles in sprint biomechanics: trunk inclination, hip flex extension, and knee flex extension. We propose a post-processing module for outlier detection and fusion prediction in the joint angles.
  The experimental results demonstrate that using joint-based models yields root mean squared errors ranging from 11.41{\deg} to 4.37{\deg}. When integrated with the post-processing modules, these errors can be reduced to 6.99{\deg} and 3.88{\deg}, respectively. The experimental findings suggest that human pose tracking approaches can be valuable resources for the biomechanical analysis of running. However, there is still room for improvement in applications where high accuracy is required.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lay-Your-Scene: Natural Scene Layout Generation with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.04718</link>
<guid>https://arxiv.org/abs/2505.04718</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-layout, open-vocabulary, diffusion Transformer, scene generation, image editing  

<br /><br />Summary: Lay-Your-Scene (LayouSyn) is a novel pipeline designed for generating layouts of natural scenes from text prompts. The primary limitation of previous scene layout generation methods is their reliance on closed-vocabulary approaches or proprietary large language models, which hampers their flexibility and applicability in generating controllable images. LayouSyn addresses this by utilizing lightweight, open-source language models to extract scene elements from the prompts and implementing a new aspect-aware diffusion Transformer architecture for layout generation in an open-vocabulary format. Extensive experiments have shown that LayouSyn outperforms existing methodologies, achieving state-of-the-art results on complex spatial and numerical reasoning benchmarks. The article also presents two key applications for LayouSyn: Firstly, the method can integrate coarse initializations from larger language models, leading to improved outcomes. Secondly, it introduces a pipeline for seamlessly adding objects to existing images, underscoring LayouSyn's potential in image editing tasks. This combination of open-vocabulary capabilities and innovative architecture positions LayouSyn as a versatile tool in text-to-layout generation and editing applications. <div>
arXiv:2505.04718v1 Announce Type: new 
Abstract: We present Lay-Your-Scene (shorthand LayouSyn), a novel text-to-layout generation pipeline for natural scenes. Prior scene layout generation methods are either closed-vocabulary or use proprietary large language models for open-vocabulary generation, limiting their modeling capabilities and broader applicability in controllable image generation. In this work, we propose to use lightweight open-source language models to obtain scene elements from text prompts and a novel aspect-aware diffusion Transformer architecture trained in an open-vocabulary manner for conditional layout generation. Extensive experiments demonstrate that LayouSyn outperforms existing methods and achieves state-of-the-art performance on challenging spatial and numerical reasoning benchmarks. Additionally, we present two applications of LayouSyn. First, we show that coarse initialization from large language models can be seamlessly combined with our method to achieve better results. Second, we present a pipeline for adding objects to images, demonstrating the potential of LayouSyn in image editing applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>False Promises in Medical Imaging AI? Assessing Validity of Outperformance Claims</title>
<link>https://arxiv.org/abs/2505.04720</link>
<guid>https://arxiv.org/abs/2505.04720</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, AI, performance comparisons, false claims, Bayesian approach

<br /><br />Summary: This paper investigates the reliability of performance claims in medical imaging AI research, emphasizing that many studies assert superiority based on empirical mean performance without adequate validation. Analyzing a representative cohort of medical imaging papers, the authors apply a Bayesian framework to assess the probability of false claims regarding outperformance. They find that over 80% of studies claim to outperform existing methods, with a particularly high likelihood of false claims—over 5%—in 86% of classification papers and 53% of segmentation papers. This suggests that the majority of claims about new methods exceeding the state of the art may be unwarranted and often result purely from chance rather than genuine improvements. This investigation sheds light on a significant flaw in current benchmarking practices within the field, indicating that many assertions of advancement could mislead researchers and misdirect future investigations. Overall, the results call for more rigorous validation and transparent reporting to enhance the credibility of performance comparisons in medical imaging AI. <div>
arXiv:2505.04720v1 Announce Type: new 
Abstract: Performance comparisons are fundamental in medical imaging Artificial Intelligence (AI) research, often driving claims of superiority based on relative improvements in common performance metrics. However, such claims frequently rely solely on empirical mean performance. In this paper, we investigate whether newly proposed methods genuinely outperform the state of the art by analyzing a representative cohort of medical imaging papers. We quantify the probability of false claims based on a Bayesian approach that leverages reported results alongside empirically estimated model congruence to estimate whether the relative ranking of methods is likely to have occurred by chance. According to our results, the majority (>80%) of papers claims outperformance when introducing a new method. Our analysis further revealed a high probability (>5%) of false outperformance claims in 86% of classification papers and 53% of segmentation papers. These findings highlight a critical flaw in current benchmarking practices: claims of outperformance in medical imaging AI are frequently unsubstantiated, posing a risk of misdirecting future research efforts.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyb-KAN ViT: Hybrid Kolmogorov-Arnold Networks Augmented Vision Transformer</title>
<link>https://arxiv.org/abs/2505.04740</link>
<guid>https://arxiv.org/abs/2505.04740</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-Layer Perceptrons, Vision Transformers, Wavelet Functions, Hybrid Kolmogorov-Arnold Network, ImageNet-1K

<br /><br />Summary: This article presents a new framework called Hybrid Kolmogorov-Arnold Network-Vision Transformer (Hyb-KAN ViT) aimed at addressing the limitations of Multi-Layer Perceptrons (MLPs) in Vision Transformers (ViTs). It highlights the importance of leveraging the prebuilt modularity of the ViT architecture and integrating edge detection capabilities through wavelet functions. The framework proposes two innovative modules: Efficient-KAN (Eff-KAN), which substitutes MLP layers with spline functions, and Wavelet-KAN (Wav-KAN), which utilizes orthogonal wavelet transforms for multi-resolution feature extraction. These modules are effectively incorporated into ViT encoder layers and classification heads to improve spatial-frequency modeling while reducing computational constraints. Experimental evaluations on datasets such as ImageNet-1K, COCO, and ADE20K indicate that Hyb-KAN ViT achieves state-of-the-art performance across various tasks, including image recognition, object detection, and semantic segmentation. Additionally, ablation studies confirm the effectiveness of wavelet-driven spectral priors in enhancing segmentation and the efficiency of spline functions for detection tasks. Overall, the proposed framework establishes a new standard for optimizing parameter efficiency while facilitating multi-scale representation in vision architectures. <div>
arXiv:2505.04740v1 Announce Type: new 
Abstract: This study addresses the inherent limitations of Multi-Layer Perceptrons (MLPs) in Vision Transformers (ViTs) by introducing Hybrid Kolmogorov-Arnold Network (KAN)-ViT (Hyb-KAN ViT), a novel framework that integrates wavelet-based spectral decomposition and spline-optimized activation functions, prior work has failed to focus on the prebuilt modularity of the ViT architecture and integration of edge detection capabilities of Wavelet functions. We propose two key modules: Efficient-KAN (Eff-KAN), which replaces MLP layers with spline functions and Wavelet-KAN (Wav-KAN), leveraging orthogonal wavelet transforms for multi-resolution feature extraction. These modules are systematically integrated in ViT encoder layers and classification heads to enhance spatial-frequency modeling while mitigating computational bottlenecks. Experiments on ImageNet-1K (Image Recognition), COCO (Object Detection and Instance Segmentation), and ADE20K (Semantic Segmentation) demonstrate state-of-the-art performance with Hyb-KAN ViT. Ablation studies validate the efficacy of wavelet-driven spectral priors in segmentation and spline-based efficiency in detection tasks. The framework establishes a new paradigm for balancing parameter efficiency and multi-scale representation in vision architectures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective</title>
<link>https://arxiv.org/abs/2505.04758</link>
<guid>https://arxiv.org/abs/2505.04758</guid>
<content:encoded><![CDATA[
<div> Keywords: RGB-D, SATNet, lightweight, depth quality, feature representation

<br /><br />Summary: The authors propose a novel Speed-Accuracy Tradeoff Network (SATNet) specifically designed for Lightweight RGB-D Salient Object Detection (SOD). Recognizing the typical trade-offs between accuracy and efficiency in existing models, SATNet addresses three core areas: depth quality, modality fusion, and feature representation. First, to improve depth quality, a Depth Anything Model is introduced to generate high-quality depth maps, which helps bridge the gaps present in current datasets. Regarding modality fusion, the Decoupled Attention Module (DAM) is developed to enhance the consistency of features within and between different modalities, effectively separating them into dual-view feature vectors to boost discriminative capabilities. For feature representation, a Dual Information Representation Module (DIRM) employs a bi-directional inverted framework to expand the feature space generated by lightweight backbones, capturing both texture and saliency features. Additionally, the model incorporates two-way prediction heads to optimize parameters through bi-directional backpropagation. Finally, a Dual Feature Aggregation Module (DFAM) is designed for effective integration of texture and saliency features in the decoder. Experimental results demonstrate that SATNet outperforms state-of-the-art heavyweight models while maintaining a lightweight structure with only 5.2 million parameters and 415 frames per second (FPS). <div>
arXiv:2505.04758v1 Announce Type: new 
Abstract: Current RGB-D methods usually leverage large-scale backbones to improve accuracy but sacrifice efficiency. Meanwhile, several existing lightweight methods are difficult to achieve high-precision performance. To balance the efficiency and performance, we propose a Speed-Accuracy Tradeoff Network (SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth quality, modality fusion, and feature representation. Concerning depth quality, we introduce the Depth Anything Model to generate high-quality depth maps,which effectively alleviates the multi-modal gaps in the current datasets. For modality fusion, we propose a Decoupled Attention Module (DAM) to explore the consistency within and between modalities. Here, the multi-modal features are decoupled into dual-view feature vectors to project discriminable information of feature maps. For feature representation, we develop a Dual Information Representation Module (DIRM) with a bi-directional inverted framework to enlarge the limited feature space generated by the lightweight backbones. DIRM models texture features and saliency features to enrich feature space, and employ two-way prediction heads to optimal its parameters through a bi-directional backpropagation. Finally, we design a Dual Feature Aggregation Module (DFAM) in the decoder to aggregate texture and saliency features. Extensive experiments on five public RGB-D SOD datasets indicate that the proposed SATNet excels state-of-the-art (SOTA) CNN-based heavyweight models and achieves a lightweight framework with 5.2 M parameters and 415 FPS.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language-Action Models: Concepts, Progress, Applications and Challenges</title>
<link>https://arxiv.org/abs/2505.04769</link>
<guid>https://arxiv.org/abs/2505.04769</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language-Action, Agentic AI, AI Agents, Vision-language Models, robotics  

<br /><br />Summary: This foundational review synthesizes recent advancements in Vision-Language-Action (VLA) models, emphasizing their role in unifying perception, natural language understanding, and embodied actions. It begins by establishing the conceptual foundations of VLA systems, tracing their development from cross-modal learning to integrated systems combining vision-language models, action planners, and hierarchical controllers. The methodology employs a robust literature review of over 80 VLA models published in the last three years. Key advancements are highlighted in architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. Various application domains are discussed, including humanoid robotics, autonomous vehicles, medical robotics, precision agriculture, and augmented reality navigation. The review also confronts major challenges such as real-time control, multimodal action representation, and generalization to new tasks, while assessing ethical deployment risks. Proposed solutions involve agentic AI adaptation and cross-embodiment generalization. Finally, the article outlines a forward-looking roadmap for the convergence of VLA models and agentic AI, aiming to create socially aligned, adaptive, and general-purpose embodied agents. This work serves as a crucial reference for advancing intelligent robotics and artificial general intelligence. <div>
arXiv:2505.04769v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replay to Remember (R2R): An Efficient Uncertainty-driven Unsupervised Continual Learning Framework Using Generative Replay</title>
<link>https://arxiv.org/abs/2505.04787</link>
<guid>https://arxiv.org/abs/2505.04787</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Learning, Catastrophic Forgetting, generative replay, uncertainty-driven, synthetic labeled data

<br /><br />Summary: The article introduces a new framework for Continual Learning, termed "Replay to Remember (R2R)," aimed at addressing the challenge of Catastrophic Forgetting in neural networks. R2R utilizes a novel uncertainty-driven approach combined with Generative Replay to effectively assimilate new knowledge while preserving previous learning. The framework distinguishes itself by operating without the need for pre-trained models and pseudo-labels, relying instead on visual features extracted from unlabeled data. It employs a cluster-level uncertainty feedback mechanism, enhanced by dynamic thresholding, to adapt and improve continuously. Additionally, the architecture incorporates a generative replay mechanism, powered by a DeepSeek-R1 and CLIP VLM combination, to synthesize labeled data that reflects past experiences. This method mimics biological visual cognition, facilitating memory replay for enhanced performance in novel tasks. Extensive experiments conducted across benchmarks such as CIFAR-10, CIFAR-100, CINIC-10, SVHN, and TinyImageNet demonstrate the effectiveness of R2R, achieving unprecedented knowledge retention rates of 98.13%, 73.06%, 93.41%, 95.18%, and 59.74%, respectively, surpassing prior state-of-the-art results by over 4.36%. <div>
arXiv:2505.04787v1 Announce Type: new 
Abstract: Continual Learning entails progressively acquiring knowledge from new data while retaining previously acquired knowledge, thereby mitigating ``Catastrophic Forgetting'' in neural networks. Our work presents a novel uncertainty-driven Unsupervised Continual Learning framework using Generative Replay, namely ``Replay to Remember (R2R)''. The proposed R2R architecture efficiently uses unlabelled and synthetic labelled data in a balanced proportion using a cluster-level uncertainty-driven feedback mechanism and a VLM-powered generative replay module. Unlike traditional memory-buffer methods that depend on pretrained models and pseudo-labels, our R2R framework operates without any prior training. It leverages visual features from unlabeled data and adapts continuously using clustering-based uncertainty estimation coupled with dynamic thresholding. Concurrently, a generative replay mechanism along with DeepSeek-R1 powered CLIP VLM produces labelled synthetic data representative of past experiences, resembling biological visual thinking that replays memory to remember and act in new, unseen tasks. Extensive experimental analyses are carried out in CIFAR-10, CIFAR-100, CINIC-10, SVHN and TinyImageNet datasets. Our proposed R2R approach improves knowledge retention, achieving a state-of-the-art performance of 98.13%, 73.06%, 93.41%, 95.18%, 59.74%, respectively, surpassing state-of-the-art performance by over 4.36%.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World</title>
<link>https://arxiv.org/abs/2505.04788</link>
<guid>https://arxiv.org/abs/2505.04788</guid>
<content:encoded><![CDATA[
<div> Keywords: vanishing points, Manhattan world, convex relaxation, GlobustVP, semidefinite programming  

<br /><br />Summary: The paper addresses the task of determining vanishing points (VPs) in a Manhattan world, which is crucial for various 3D vision applications. It highlights the limitations of existing methods, which are either sub-optimal or computationally intensive in pursuit of global optimality. To overcome these drawbacks, the authors introduce convex relaxation techniques to jointly infer line-VP associations and VP locations. They propose a "soft" association scheme using a truncated multi-selection error, leading to a primal problem reformulated into a quadratically constrained quadratic programming (QCQP) problem. This problem is further relaxed into a convex semidefinite programming (SDP) problem. The key innovation is the development of a globally optimal outlier-robust iterative solver called GlobustVP, which updates one VP at a time while treating others as outliers. This iterative process is complemented by a local refinement that enforces the mutual orthogonality of the three VPs inherent in a Manhattan world. Comprehensive experiments on synthetic and real-world data validate that GlobustVP strikes a commendable balance between efficiency, robustness, and global optimality in comparison to existing methods. The code for this approach is publicly available. <div>
arXiv:2505.04788v1 Announce Type: new 
Abstract: Determining the vanishing points (VPs) in a Manhattan world, as a fundamental task in many 3D vision applications, consists of jointly inferring the line-VP association and locating each VP. Existing methods are, however, either sub-optimal solvers or pursuing global optimality at a significant cost of computing time. In contrast to prior works, we introduce convex relaxation techniques to solve this task for the first time. Specifically, we employ a ``soft'' association scheme, realized via a truncated multi-selection error, that allows for joint estimation of VPs' locations and line-VP associations. This approach leads to a primal problem that can be reformulated into a quadratically constrained quadratic programming (QCQP) problem, which is then relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP problem efficiently, we present a globally optimal outlier-robust iterative solver (called \textbf{GlobustVP}), which independently searches for one VP and its associated lines in each iteration, treating other lines as outliers. After each independent update of all VPs, the mutual orthogonality between the three VPs in a Manhattan world is reinforced via local refinement. Extensive experiments on both synthetic and real-world data demonstrate that \textbf{GlobustVP} achieves a favorable balance between efficiency, robustness, and global optimality compared to previous works. The code is publicly available at https://github.com/WU-CVGL/GlobustVP.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DetReIDX: A Stress-Test Dataset for Real-World UAV-Based Person Recognition</title>
<link>https://arxiv.org/abs/2505.04793</link>
<guid>https://arxiv.org/abs/2505.04793</guid>
<content:encoded><![CDATA[
<div> Keywords: Person Reidentification, DetReIDX, dataset, aerial-ground, real-world conditions

<br /><br />Summary: Person reidentification (ReID) technology has shown promise in controlled, ground-level environments but struggles in real-world scenarios due to variability in data such as resolution, viewpoint changes, and occlusions. Existing public datasets do not adequately represent these conditions, hindering progress in the field. In response, this paper introduces DetReIDX, a large-scale aerial-ground person dataset designed to stress-test ReID under realistic circumstances. Comprising over 13 million bounding boxes from 509 individuals, the dataset was collected across seven university campuses spanning three continents, at drone altitudes between 5.8 and 120 meters. A key feature of DetReIDX is that subjects were recorded over multiple sessions on different days, capturing variations in clothing, daylight, and locations, making it suitable for evaluating long-term ReID. Additionally, the dataset includes annotations for 16 soft biometric attributes and multitask labels, encompassing human detection, tracking, ReID, and action recognition. Empirical evidence demonstrates that state-of-the-art methods experience significant performance degradation—up to 80% in detection accuracy and over 70% in Rank-1 ReID—when tested with DetReIDX, underscoring the dataset's relevance. The dataset and evaluation protocols are available at https://www.it.ubi.pt/DetReIDX/. <div>
arXiv:2505.04793v1 Announce Type: new 
Abstract: Person reidentification (ReID) technology has been considered to perform relatively well under controlled, ground-level conditions, but it breaks down when deployed in challenging real-world settings. Evidently, this is due to extreme data variability factors such as resolution, viewpoint changes, scale variations, occlusions, and appearance shifts from clothing or session drifts. Moreover, the publicly available data sets do not realistically incorporate such kinds and magnitudes of variability, which limits the progress of this technology. This paper introduces DetReIDX, a large-scale aerial-ground person dataset, that was explicitly designed as a stress test to ReID under real-world conditions. DetReIDX is a multi-session set that includes over 13 million bounding boxes from 509 identities, collected in seven university campuses from three continents, with drone altitudes between 5.8 and 120 meters. More important, as a key novelty, DetReIDX subjects were recorded in (at least) two sessions on different days, with changes in clothing, daylight and location, making it suitable to actually evaluate long-term person ReID. Plus, data were annotated from 16 soft biometric attributes and multitask labels for detection, tracking, ReID, and action recognition. In order to provide empirical evidence of DetReIDX usefulness, we considered the specific tasks of human detection and ReID, where SOTA methods catastrophically degrade performance (up to 80% in detection accuracy and over 70% in Rank-1 ReID) when exposed to DetReIDXs conditions. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Synthetic Corruptions A Reliable Proxy For Real-World Corruptions?</title>
<link>https://arxiv.org/abs/2505.04835</link>
<guid>https://arxiv.org/abs/2505.04835</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, robustness, synthetic corruptions, semantic segmentation, benchmarking

<br /><br />Summary: Deep learning (DL) models are widely utilized in practical applications but show susceptibility to distribution shifts, particularly due to variations in weather and lighting. Collecting diverse real-world data for testing DL models' robustness can be resource-intensive. Therefore, synthetic corruptions serve as an appealing alternative for evaluating robustness. This study aims to determine whether synthetic corruptions are reliable proxies for real-world corruptions. We conduct the largest benchmarking study focused on semantic segmentation models, comparing their performance on datasets with real-world and synthetic corruptions. Our findings demonstrate a strong correlation in mean performance across both types of corruptions, suggesting that synthetic corruptions can be effectively used for robustness evaluation. Additionally, we examine corruption-specific correlations, which provide important insights into the circumstances under which synthetic corruptions accurately represent real-world scenarios. Overall, our research supports the viability of synthetic corruptions for testing the resilience of DL models against various adverse conditions, paving the way for enhanced model evaluation techniques. The study includes open-source code to facilitate further exploration in this area, available at the provided GitHub link. <div>
arXiv:2505.04835v1 Announce Type: new 
Abstract: Deep learning (DL) models are widely used in real-world applications but remain vulnerable to distribution shifts, especially due to weather and lighting changes. Collecting diverse real-world data for testing the robustness of DL models is resource-intensive, making synthetic corruptions an attractive alternative for robustness testing. However, are synthetic corruptions a reliable proxy for real-world corruptions? To answer this, we conduct the largest benchmarking study on semantic segmentation models, comparing performance on real-world corruptions and synthetic corruptions datasets. Our results reveal a strong correlation in mean performance, supporting the use of synthetic corruptions for robustness evaluation. We further analyze corruption-specific correlations, providing key insights to understand when synthetic corruptions succeed in representing real-world corruptions. Open-source Code: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/segmentation_david/semantic_segmentation
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Cells Clearly: Evaluating Machine Vision Strategies for Microglia Centroid Detection in 3D Images</title>
<link>https://arxiv.org/abs/2505.04838</link>
<guid>https://arxiv.org/abs/2505.04838</guid>
<content:encoded><![CDATA[
<div> Keywords: microglia, 3D images, ilastik, 3D Morph, Omnipose<br /><br />Summary: Microglia are crucial cells in the brain, and their morphology can provide insights into brain health. In this study, the author evaluates three different tools designed to identify the center points of microglia in 3D microscope images. The tools assessed include ilastik, 3D Morph, and Omnipose, each with unique methodologies for cell detection. The project aims to determine the effectiveness of each tool in accurately locating microglial cells and to compare the outcomes produced by these different software. Through the analysis, it becomes evident that each tool perceives the cells in distinct ways, leading to variations in the information extracted from the images. This highlights the importance of choosing the appropriate tool for specific research needs in neuroimaging, as the results can significantly influence our understanding of microglial function and overall brain health. Ultimately, this project contributes to the ongoing discourse on the best practices for analyzing microglia morphology in neurobiology research. <div>
arXiv:2505.04838v1 Announce Type: new 
Abstract: Microglia are important cells in the brain, and their shape can tell us a lot about brain health. In this project, I test three different tools for finding the center points of microglia in 3D microscope images. The tools include ilastik, 3D Morph, and Omnipose. I look at how well each one finds the cells and how their results compare. My findings show that each tool sees the cells in its own way, and this can affect the kind of information we get from the images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORXE: Orchestrating Experts for Dynamically Configurable Efficiency</title>
<link>https://arxiv.org/abs/2505.04850</link>
<guid>https://arxiv.org/abs/2505.04850</guid>
<content:encoded><![CDATA[
<div> Keywords: ORXE, AI models, adaptability, inference pathways, image classification

<br /><br />Summary: This paper introduces ORXE, a modular and adaptable framework designed for real-time configurable efficiency in AI models. ORXE utilizes a collection of pre-trained experts, each with varying computational costs and performance, allowing it to dynamically adjust inference pathways based on the complexity of input samples. The system stands out from conventional approaches due to its avoidance of complex metamodel training, thus simplifying the development process while achieving high efficiency and flexibility. A confidence-based gating mechanism is employed to optimize the allocation of computational resources for each input, providing a balance between inference cost and prediction performance, which can be adjusted during runtime. The authors implemented a training-free ORXE system specifically for image classification tasks, rigorously evaluating its efficiency and accuracy across different devices. Results indicate that ORXE consistently outperforms individual experts and other dynamic models in most scenarios. The proposed framework not only excels in image classification but also has the potential to be expanded to various applications, making it a scalable solution suitable for diverse real-world deployment contexts. <div>
arXiv:2505.04850v1 Announce Type: new 
Abstract: This paper presents ORXE, a modular and adaptable framework for achieving real-time configurable efficiency in AI models. By leveraging a collection of pre-trained experts with diverse computational costs and performance levels, ORXE dynamically adjusts inference pathways based on the complexity of input samples. Unlike conventional approaches that require complex metamodel training, ORXE achieves high efficiency and flexibility without complicating the development process. The proposed system utilizes a confidence-based gating mechanism to allocate appropriate computational resources for each input. ORXE also supports adjustments to the preference between inference cost and prediction performance across a wide range during runtime. We implemented a training-free ORXE system for image classification tasks, evaluating its efficiency and accuracy across various devices. The results demonstrate that ORXE achieves superior performance compared to individual experts and other dynamic models in most cases. This approach can be extended to other applications, providing a scalable solution for diverse real-world deployment scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix-QSAM: Mixed-Precision Quantization of the Segment Anything Model</title>
<link>https://arxiv.org/abs/2505.04861</link>
<guid>https://arxiv.org/abs/2505.04861</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, Post-Training Quantization, mixed-precision, Integer Quadratic Programming, bit-width allocation  

<br /><br />Summary: The Segment Anything Model (SAM) faces challenges in deployment on resource-constrained devices due to its high computational and memory requirements. Post-Training Quantization (PTQ) is a common solution for alleviating these demands, but traditional methods typically use fixed bit-width quantization, which can compromise accuracy and efficiency. The proposed solution, Mix-QSAM, introduces a mixed-precision PTQ framework tailored for SAM. It first establishes a layer-wise importance score using Kullback-Leibler (KL) divergence to measure each layer's output contribution. A novel metric, cross-layer synergy, is introduced based on causal mutual information to track dependencies between adjacent layers, ensuring consistent bit-width allocations and enhancing numerical stability. An Integer Quadratic Programming (IQP) formulation is employed to optimize bit-width assignments while adhering to model size and bit-operation constraints, allocating higher precision to essential layers and reducing bit-width for less critical ones. Experimental findings reveal that Mix-QSAM outperforms existing PTQ methods across instance segmentation and object detection tasks, achieving up to 20% higher average precision in mixed-precision settings of 6-bit and 4-bit, all while maintaining computational efficiency. <div>
arXiv:2505.04861v1 Announce Type: new 
Abstract: The Segment Anything Model (SAM) is a popular vision foundation model; however, its high computational and memory demands make deployment on resource-constrained devices challenging. While Post-Training Quantization (PTQ) is a practical approach for reducing computational overhead, existing PTQ methods rely on fixed bit-width quantization, leading to suboptimal accuracy and efficiency. To address this limitation, we propose Mix-QSAM, a mixed-precision PTQ framework for SAM. First, we introduce a layer-wise importance score, derived using Kullback-Leibler (KL) divergence, to quantify each layer's contribution to the model's output. Second, we introduce cross-layer synergy, a novel metric based on causal mutual information, to capture dependencies between adjacent layers. This ensures that highly interdependent layers maintain similar bit-widths, preventing abrupt precision mismatches that degrade feature propagation and numerical stability. Using these metrics, we formulate an Integer Quadratic Programming (IQP) problem to determine optimal bit-width allocation under model size and bit-operation constraints, assigning higher precision to critical layers while minimizing bit-width in less influential layers. Experimental results demonstrate that Mix-QSAM consistently outperforms existing PTQ methods on instance segmentation and object detection tasks, achieving up to 20% higher average precision under 6-bit and 4-bit mixed-precision settings, while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto-regressive transformation for image alignment</title>
<link>https://arxiv.org/abs/2505.04864</link>
<guid>https://arxiv.org/abs/2505.04864</guid>
<content:encoded><![CDATA[
<div> Keywords: image alignment, transformation field, multi-scale features, cross-attention, Auto-Regressive Transformation (ART)  

<br /><br />Summary: Existing image alignment methods face challenges in feature-sparse regions, large deformation, and extreme scale differences, leading to suboptimal accuracy. The iterative refinement of transformation fields is crucial for enhancing robustness against these issues. To address this, the authors propose a new method called Auto-Regressive Transformation (ART), which estimates transformations from coarse to fine within an auto-regressive framework. ART utilizes hierarchical multi-scale features and refines transformations by randomly sampling points at different scales. Additionally, the incorporation of a cross-attention layer allows the model to emphasize critical regions, promoting accurate alignment even under difficult conditions with limited features. Extensive experiments on various datasets indicate that ART significantly outperforms current state-of-the-art image alignment methods. As a result, ART is established as a powerful and innovative approach for achieving precise image alignment, demonstrating its broad applicability in diverse scenarios. This approach not only enhances accuracy but also addresses the inherent challenges faced in the field, making it a significant advancement in image processing techniques. <div>
arXiv:2505.04864v1 Announce Type: new 
Abstract: Existing methods for image alignment struggle in cases involving feature-sparse regions, extreme scale and field-of-view differences, and large deformations, often resulting in suboptimal accuracy. Robustness to these challenges improves through iterative refinement of the transformation field while focusing on critical regions in multi-scale image representations. We thus propose Auto-Regressive Transformation (ART), a novel method that iteratively estimates the coarse-to-fine transformations within an auto-regressive framework. Leveraging hierarchical multi-scale features, our network refines the transformations using randomly sampled points at each scale. By incorporating guidance from the cross-attention layer, the model focuses on critical regions, ensuring accurate alignment even in challenging, feature-limited conditions. Extensive experiments across diverse datasets demonstrate that ART significantly outperforms state-of-the-art methods, establishing it as a powerful new method for precise image alignment with broad applicability.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning</title>
<link>https://arxiv.org/abs/2505.04877</link>
<guid>https://arxiv.org/abs/2505.04877</guid>
<content:encoded><![CDATA[
<div> Keywords: Mixed Precision Quantization, optimization, quantization policies, CIFAR10, ImageNet

<br /><br />Summary: Mixed Precision Quantization (MPQ) is a critical strategy for optimizing neural networks by finding the ideal bitwidth for each layer. Traditional MPQ methods are hindered by their reliance on costly searches for quantization policies using large-scale datasets. To address this challenge, the proposed method first explores quantization strategies on smaller datasets and subsequently generalizes these findings to larger datasets, thus streamlining the process. The technique reduces the need for exhaustive fine-tuning on large-scale data by allowing for adjustments merely to the model weights. Key components of this approach include: employing sharpness-aware minimization to improve generalization in quantization, using implicit gradient direction alignment to manage conflicts in gradient objectives, and implementing an adaptive perturbation radius to speed up optimization. Theoretical evaluations and practical experiments support the effectiveness of the approach. When utilizing the CIFAR10 dataset, which is significantly smaller than ImageNet's training data, the method achieved comparable accuracy to models trained directly on ImageNet while demonstrating a computational cost reduction and improving efficiency by up to 150% compared to existing baselines. <div>
arXiv:2505.04877v1 Announce Type: new 
Abstract: Mixed Precision Quantization (MPQ) has become an essential technique for optimizing neural network by determining the optimal bitwidth per layer. Existing MPQ methods, however, face a major hurdle: they require a computationally expensive search for quantization policies on large-scale datasets. To resolve this issue, we introduce a novel approach that first searches for quantization policies on small datasets and then generalizes them to large-scale datasets. This approach simplifies the process, eliminating the need for large-scale quantization fine-tuning and only necessitating model weight adjustment. Our method is characterized by three key techniques: sharpness-aware minimization for enhanced quantization generalization, implicit gradient direction alignment to handle gradient conflicts among different optimization objectives, and an adaptive perturbation radius to accelerate optimization. Both theoretical analysis and experimental results validate our approach. Using the CIFAR10 dataset (just 0.5\% the size of ImageNet training data) for MPQ policy search, we achieved equivalent accuracy on ImageNet with a significantly lower computational cost, while improving efficiency by up to 150% over the baselines.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Branch Orthogonality for Improved Generalization in Face Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.04888</link>
<guid>https://arxiv.org/abs/2505.04888</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake, generative AI, detection, feature disentanglement, multimedia content

<br /><br />Summary: Remarkable advancements in generative AI have led to new deepfake categories that exhibit unprecedented realism, posing challenges to law enforcement and public trust. These face deepfakes have created significant confusion and deception, undermining societal faith in multimedia content. Existing deepfake detection methods are struggling to keep pace with the rapid evolution of deepfake technologies, primarily due to their reliance on specific forgery artifacts that limit generalization capabilities. To address the issue of malicious face deepfakes, this paper introduces a novel strategy that leverages coarse-to-fine spatial and semantic information, ensuring feature distinctiveness and reducing redundancy. A key innovation is the implementation of a feature orthogonality-based disentanglement strategy that facilitates branch-level and cross-branch feature disentanglement. This approach enables the integration of various feature vectors while maintaining simplicity and enhancing generalization. Experimental results on three public benchmarks—FaceForensics++, Celeb-DF, and the Deepfake Detection Challenge (DFDC)—demonstrate that this method outperforms current state-of-the-art techniques, achieving improvements of 5% on Celeb-DF and 7% on DFDC in cross-dataset evaluations. <div>
arXiv:2505.04888v1 Announce Type: new 
Abstract: Remarkable advancements in generative AI technology have given rise to a spectrum of novel deepfake categories with unprecedented leaps in their realism, and deepfakes are increasingly becoming a nuisance to law enforcement authorities and the general public. In particular, we observe alarming levels of confusion, deception, and loss of faith regarding multimedia content within society caused by face deepfakes, and existing deepfake detectors are struggling to keep up with the pace of improvements in deepfake generation. This is primarily due to their reliance on specific forgery artifacts, which limits their ability to generalise and detect novel deepfake types. To combat the spread of malicious face deepfakes, this paper proposes a new strategy that leverages coarse-to-fine spatial information, semantic information, and their interactions while ensuring feature distinctiveness and reducing the redundancy of the modelled features. A novel feature orthogonality-based disentanglement strategy is introduced to ensure branch-level and cross-branch feature disentanglement, which allows us to integrate multiple feature vectors without adding complexity to the feature space or compromising generalisation. Comprehensive experiments on three public benchmarks: FaceForensics++, Celeb-DF, and the Deepfake Detection Challenge (DFDC) show that these design choices enable the proposed approach to outperform current state-of-the-art methods by 5% on the Celeb-DF dataset and 7% on the DFDC dataset in a cross-dataset evaluation setting.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWT: A Foundational Organ-Wise Tokenization Framework for Medical Imaging</title>
<link>https://arxiv.org/abs/2505.04899</link>
<guid>https://arxiv.org/abs/2505.04899</guid>
<content:encoded><![CDATA[
<div> Keywords: representation learning, organ-wise tokenization, interpretability, medical imaging, segmentation

<br /><br />Summary: Recent advancements in representation learning have highlighted the limitations of holistic embeddings, which entangle multiple semantic components, specifically in the realm of medical imaging. To overcome these challenges, the authors propose an Organ-Wise Tokenization (OWT) framework, complemented by a Token Group-based Reconstruction (TGR) training paradigm. OWT distinctively separates an image into token groups, each corresponding to specific organs or semantic entities, thereby ensuring that individual token groups capture organ-specific information. This structural separation enhances interpretability, generalization, and efficiency while allowing for precise control in various downstream tasks. Experimental results utilizing CT and MRI datasets indicate that OWT significantly improves image reconstruction and segmentation performance. Furthermore, OWT facilitates innovative semantic-level generation and retrieval applications that traditional holistic embedding methods cannot achieve. These insights emphasize the promise of OWT as a foundational framework for semantically disentangled representation learning. This approach not only showcases broad scalability but also substantial applicability to practical medical imaging scenarios and beyond, paving the way for enhanced methodologies in the field. <div>
arXiv:2505.04899v1 Announce Type: new 
Abstract: Recent advances in representation learning often rely on holistic, black-box embeddings that entangle multiple semantic components, limiting interpretability and generalization. These issues are especially critical in medical imaging. To address these limitations, we propose an Organ-Wise Tokenization (OWT) framework with a Token Group-based Reconstruction (TGR) training paradigm. Unlike conventional approaches that produce holistic features, OWT explicitly disentangles an image into separable token groups, each corresponding to a distinct organ or semantic entity. Our design ensures each token group encapsulates organ-specific information, boosting interpretability, generalization, and efficiency while allowing fine-grained control in downstream tasks. Experiments on CT and MRI datasets demonstrate the effectiveness of OWT in not only achieving strong image reconstruction and segmentation performance, but also enabling novel semantic-level generation and retrieval applications that are out of reach for standard holistic embedding methods. These findings underscore the potential of OWT as a foundational framework for semantically disentangled representation learning, offering broad scalability and applicability to real-world medical imaging scenarios and beyond.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pro2SAM: Mask Prompt to SAM with Grid Points for Weakly Supervised Object Localization</title>
<link>https://arxiv.org/abs/2505.04905</link>
<guid>https://arxiv.org/abs/2505.04905</guid>
<content:encoded><![CDATA[
<div> Keywords: Weakly Supervised Object Localization, Class Activation Map, Segment Anything Model, fine-grained segmentation, mask prompt  

<br /><br />Summary: Weakly Supervised Object Localization (WSOL) seeks to identify objects using only image-level labels, minimizing annotation costs. Traditional methods like Class Activation Maps (CAM) and self-attention maps struggle to capture fine-grained pixel-level information, limiting WSOL's effectiveness. To overcome this challenge, the authors leverage the zero-shot generalization and fine-grained segmentation capabilities of the Segment Anything Model (SAM) to enhance activation of complete object regions. They introduce a novel mask prompt for the SAM, called the Pro2SAM network, which utilizes grid points instead of relying solely on single point prompts to address semantic ambiguity. A Global Token Transformer (GTFormer) generates a coarse-grained foreground map that acts as a flexible mask prompt, incorporating patch tokens and global tokens to capture foreground semantics. The authors then input dense grid points into SAM to maximize the likelihood of accurate foreground masks. They also propose a pixel-level similarity metric to match mask prompts with SAM, selecting the mask with the highest score as the final localization output. Experimental results demonstrate that Pro2SAM achieves state-of-the-art performance on the CUB-200-2011 and ILSVRC datasets, achieving Top-1 Localization rates of 84.03% and 66.85%, respectively. <div>
arXiv:2505.04905v1 Announce Type: new 
Abstract: Weakly Supervised Object Localization (WSOL), which aims to localize objects by only using image-level labels, has attracted much attention because of its low annotation cost in real applications. Current studies focus on the Class Activation Map (CAM) of CNN and the self-attention map of transformer to identify the region of objects. However, both CAM and self-attention maps can not learn pixel-level fine-grained information on the foreground objects, which hinders the further advance of WSOL. To address this problem, we initiatively leverage the capability of zero-shot generalization and fine-grained segmentation in Segment Anything Model (SAM) to boost the activation of integral object regions. Further, to alleviate the semantic ambiguity issue accrued in single point prompt-based SAM, we propose an innovative mask prompt to SAM (Pro2SAM) network with grid points for WSOL task. First, we devise a Global Token Transformer (GTFormer) to generate a coarse-grained foreground map as a flexible mask prompt, where the GTFormer jointly embeds patch tokens and novel global tokens to learn foreground semantics. Secondly, we deliver grid points as dense prompts into SAM to maximize the probability of foreground mask, which avoids the lack of objects caused by a single point/box prompt. Finally, we propose a pixel-level similarity metric to come true the mask matching from mask prompt to SAM, where the mask with the highest score is viewed as the final localization map. Experiments show that the proposed Pro2SAM achieves state-of-the-art performance on both CUB-200-2011 and ILSVRC, with 84.03\% and 66.85\% Top-1 Loc, respectively.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.04911</link>
<guid>https://arxiv.org/abs/2505.04911</guid>
<content:encoded><![CDATA[
<div> Keywords: SpatialPrompting, zero-shot spatial reasoning, multimodal, keyframe-driven, benchmark datasets<br /><br />Summary: This study presents SpatialPrompting, a new framework that leverages the reasoning capabilities of multimodal large language models to facilitate zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike traditional approaches that require costly 3D-specific fine-tuning and specialized inputs such as point clouds or voxel representations, SpatialPrompting employs a keyframe-driven prompt generation strategy. The method selects a diverse and informative set of keyframes from image sequences using metrics including vision-language similarity, Mahalanobis distance, field of view, and image sharpness. These keyframes are integrated with camera pose data to effectively abstract spatial relationships and infer complex 3D structures. This innovative framework establishes a new flexible paradigm for spatial reasoning by utilizing intuitive visual and positional cues. Moreover, it achieves state-of-the-art zero-shot performance on benchmark datasets like ScanQA and SQA3D across multiple metrics. Ultimately, SpatialPrompting eliminates the need for specialized 3D inputs and fine-tuning, providing a simpler and more scalable alternative to conventional methods, thus paving the way for advancements in spatial reasoning tasks. <div>
arXiv:2505.04911v1 Announce Type: new 
Abstract: This study introduces SpatialPrompting, a novel framework that harnesses the emergent reasoning capabilities of off-the-shelf multimodal large language models to achieve zero-shot spatial reasoning in three-dimensional (3D) environments. Unlike existing methods that rely on expensive 3D-specific fine-tuning with specialized 3D inputs such as point clouds or voxel-based features, SpatialPrompting employs a keyframe-driven prompt generation strategy. This framework uses metrics such as vision-language similarity, Mahalanobis distance, field of view, and image sharpness to select a diverse and informative set of keyframes from image sequences and then integrates them with corresponding camera pose data to effectively abstract spatial relationships and infer complex 3D structures. The proposed framework not only establishes a new paradigm for flexible spatial reasoning that utilizes intuitive visual and positional cues but also achieves state-of-the-art zero-shot performance on benchmark datasets, such as ScanQA and SQA3D, across several metrics. The proposed method effectively eliminates the need for specialized 3D inputs and fine-tuning, offering a simpler and more scalable alternative to conventional approaches.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing</title>
<link>https://arxiv.org/abs/2505.04915</link>
<guid>https://arxiv.org/abs/2505.04915</guid>
<content:encoded><![CDATA[
<div> Keywords: scene text editing, diffusion-based methods, glyph encoder, stroke-level precision, multi-scale features

<br /><br />Summary: Scene text editing involves modifying text in images while maintaining style and visual coherence. Diffusion-based methods have potential in text generation; however, they often generate distorted characters, particularly complex ones like Chinese. Characters consist of intricate stroke patterns that require precise maintenance in these systems. To address this issue, the authors introduce GlyphMastero, a glyph encoder designed to enhance the capability of latent diffusion models for generating text with stroke-level accuracy. Existing methods often overlook the hierarchical nature of text structures, leading to inefficiencies. GlyphMastero addresses this gap by capturing cross-level interactions between individual characters and text lines using a novel glyph attention module. Additionally, the model employs a feature pyramid network to integrate multi-scale features from an OCR backbone. This combination allows for detailed glyph-aware guidance, resulting in improved control over scene text generation. The proposed method demonstrates a significant 18.02% increase in sentence accuracy compared to the leading multi-lingual scene text editing methods and also reduces the text-region Fréchet inception distance by 53.28%, highlighting its effectiveness in generating high-quality scene text. <div>
arXiv:2505.04915v1 Announce Type: new 
Abstract: Scene text editing, a subfield of image editing, requires modifying texts in images while preserving style consistency and visual coherence with the surrounding environment. While diffusion-based methods have shown promise in text generation, they still struggle to produce high-quality results. These methods often generate distorted or unrecognizable characters, particularly when dealing with complex characters like Chinese. In such systems, characters are composed of intricate stroke patterns and spatial relationships that must be precisely maintained. We present GlyphMastero, a specialized glyph encoder designed to guide the latent diffusion model for generating texts with stroke-level precision. Our key insight is that existing methods, despite using pretrained OCR models for feature extraction, fail to capture the hierarchical nature of text structures - from individual strokes to stroke-level interactions to overall character-level structure. To address this, our glyph encoder explicitly models and captures the cross-level interactions between local-level individual characters and global-level text lines through our novel glyph attention module. Meanwhile, our model implements a feature pyramid network to fuse the multi-scale OCR backbone features at the global-level. Through these cross-level and multi-scale fusions, we obtain more detailed glyph-aware guidance, enabling precise control over the scene text generation process. Our method achieves an 18.02\% improvement in sentence accuracy over the state-of-the-art multi-lingual scene text editing baseline, while simultaneously reducing the text-region Fr\'echet inception distance by 53.28\%.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Simple Detector with Frame Dynamics is a Strong Tracker</title>
<link>https://arxiv.org/abs/2505.04917</link>
<guid>https://arxiv.org/abs/2505.04917</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared tracking, Anti-UAV, motion-aware learning, frame dynamics, trajectory constraint filtering  

<br /><br />Summary:  
Infrared object tracking is essential for Anti-Unmanned Aerial Vehicle (Anti-UAV) applications, yet existing trackers often face challenges with tiny targets due to reliance on cropped templates and limited motion capabilities. This paper proposes a novel infrared tiny-object tracker that significantly improves tracking performance by integrating global detection with motion-aware learning and temporal priors. The method introduces two key innovations to enhance effectiveness. First, it utilizes frame dynamics, employing frame differences and optical flow to better encode prior target features and motion characteristics, thereby improving target-background distinction. Second, it implements a trajectory constraint filtering strategy in the post-processing phase, which capitalizes on spatio-temporal priors to reduce false positives and bolster tracking robustness. Comprehensive experiments demonstrate that this approach consistently outperforms existing methods across various metrics in challenging infrared UAV tracking scenarios. The method has achieved notable success, earning state-of-the-art performance during the 4th Anti-UAV Challenge, where it secured 1st place in Track 1 and 2nd place in Track 2. <div>
arXiv:2505.04917v1 Announce Type: new 
Abstract: Infrared object tracking plays a crucial role in Anti-Unmanned Aerial Vehicle (Anti-UAV) applications. Existing trackers often depend on cropped template regions and have limited motion modeling capabilities, which pose challenges when dealing with tiny targets. To address this, we propose a simple yet effective infrared tiny-object tracker that enhances tracking performance by integrating global detection and motion-aware learning with temporal priors. Our method is based on object detection and achieves significant improvements through two key innovations. First, we introduce frame dynamics, leveraging frame difference and optical flow to encode both prior target features and motion characteristics at the input level, enabling the model to better distinguish the target from background clutter. Second, we propose a trajectory constraint filtering strategy in the post-processing stage, utilizing spatio-temporal priors to suppress false positives and enhance tracking robustness. Extensive experiments show that our method consistently outperforms existing approaches across multiple metrics in challenging infrared UAV tracking scenarios. Notably, we achieve state-of-the-art performance in the 4th Anti-UAV Challenge, securing 1st place in Track 1 and 2nd place in Track 2.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models</title>
<link>https://arxiv.org/abs/2505.04921</link>
<guid>https://arxiv.org/abs/2505.04921</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Reasoning, Large Multimodal Reasoning Models, Chain-of-Thought, Omni-modal Generalization, Agentic Behavior

<br /><br />Summary: The article discusses the importance of reasoning in artificial intelligence, particularly within Large Multimodal Reasoning Models (LMRMs) that integrate various modalities such as text, images, audio, and video. It highlights the evolution of multimodal reasoning from early, modular approaches to unified frameworks that enhance cross-modal understanding. Specifically, the authors review initial efforts where reasoning was implicitly included in task-specific modules, transitioning to contemporary models that incorporate Multimodal Chain-of-Thought (MCoT) and reinforcement learning to facilitate more structured reasoning processes. The survey addresses ongoing challenges, including omni-modal generalization, the depth of reasoning, and the need for agentic behavior in these systems. Furthermore, it presents a developmental roadmap to guide future research toward the conceptualization of native large multimodal reasoning models (N-LMRMs), which are designed to support scalable, adaptive reasoning and planning capabilities in complex, real-world environments. The authors aim to spark discussions on how to effectively advance multimodal reasoning research to create more robust AI systems capable of effective decision-making and problem-solving in diverse settings. <div>
arXiv:2505.04921v1 Announce Type: new 
Abstract: Reasoning lies at the heart of intelligence, shaping the ability to make decisions, draw conclusions, and generalize across domains. In artificial intelligence, as systems increasingly operate in open, uncertain, and multimodal environments, reasoning becomes essential for enabling robust and adaptive behavior. Large Multimodal Reasoning Models (LMRMs) have emerged as a promising paradigm, integrating modalities such as text, images, audio, and video to support complex reasoning capabilities and aiming to achieve comprehensive perception, precise understanding, and deep reasoning. As research advances, multimodal reasoning has rapidly evolved from modular, perception-driven pipelines to unified, language-centric frameworks that offer more coherent cross-modal understanding. While instruction tuning and reinforcement learning have improved model reasoning, significant challenges remain in omni-modal generalization, reasoning depth, and agentic behavior. To address these issues, we present a comprehensive and structured survey of multimodal reasoning research, organized around a four-stage developmental roadmap that reflects the field's shifting design philosophies and emerging capabilities. First, we review early efforts based on task-specific modules, where reasoning was implicitly embedded across stages of representation, alignment, and fusion. Next, we examine recent approaches that unify reasoning into multimodal LLMs, with advances such as Multimodal Chain-of-Thought (MCoT) and multimodal reinforcement learning enabling richer and more structured reasoning chains. Finally, drawing on empirical insights from challenging benchmarks and experimental cases of OpenAI O3 and O4-mini, we discuss the conceptual direction of native large multimodal reasoning models (N-LMRMs), which aim to support scalable, agentic, and adaptive reasoning and planning in complex, real-world environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Canny2Palm: Realistic and Controllable Palmprint Generation for Large-scale Pre-training</title>
<link>https://arxiv.org/abs/2505.04922</link>
<guid>https://arxiv.org/abs/2505.04922</guid>
<content:encoded><![CDATA[
<div> Keywords: palmprint recognition, Canny2Palm, synthetic data, Pix2Pix, large-scale pre-training

<br /><br />Summary: Palmprint recognition offers a secure and privacy-friendly biometric identification method, but it faces challenges due to limited palmprint data. Recent research focuses on synthesizing virtual palmprints for extensive pre-training. This paper presents a novel synthesis method called Canny2Palm, which utilizes the Canny edge detector to extract palm textures, conditioning a Pix2Pix network for realistic palmprint generation. By reassembling textures from diverse identities, new identities can be created by introducing new assemblies to the generator. The Canny2Palm method not only generates realistic data that aligns with the distribution of actual palmprints but also facilitates controllable diversity, allowing for the creation of a large-scale variety of new identities. In open-set palmprint recognition benchmarks, models pre-trained with data from Canny2Palm demonstrate improved performance, achieving up to 7.2% higher identification accuracy compared to state-of-the-art approaches. Furthermore, models pre-trained with Canny2Palm continue to show improvement as synthetic IDs increase to 10,000, while performances of those using existing methods tend to plateau, highlighting the effectiveness and potential of Canny2Palm for large-scale pre-training. <div>
arXiv:2505.04922v1 Announce Type: new 
Abstract: Palmprint recognition is a secure and privacy-friendly method of biometric identification. One of the major challenges to improve palmprint recognition accuracy is the scarcity of palmprint data. Recently, a popular line of research revolves around the synthesis of virtual palmprints for large-scale pre-training purposes. In this paper, we propose a novel synthesis method named Canny2Palm that extracts palm textures with Canny edge detector and uses them to condition a Pix2Pix network for realistic palmprint generation. By re-assembling palmprint textures from different identities, we are able to create new identities by seeding the generator with new assemblies. Canny2Palm not only synthesizes realistic data following the distribution of real palmprints but also enables controllable diversity to generate large-scale new identities. On open-set palmprint recognition benchmarks, models pre-trained with Canny2Palm synthetic data outperform the state-of-the-art with up to 7.2% higher identification accuracy. Moreover, the performance of models pre-trained with Canny2Palm continues to improve given 10,000 synthetic IDs while those with existing methods already saturate, demonstrating the potential of our method for large-scale pre-training.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FF-PNet: A Pyramid Network Based on Feature and Field for Brain Image Registration</title>
<link>https://arxiv.org/abs/2505.04938</link>
<guid>https://arxiv.org/abs/2505.04938</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image registration, pyramid registration network, Residual Feature Fusion Module, Residual Deformation Field Fusion Module, registration accuracy  

<br /><br />Summary: In recent years, deformable medical image registration techniques have significantly advanced, yet existing models still face challenges in efficiently extracting both coarse and fine-grained features in parallel. To address these limitations, a new pyramid registration network has been constructed, named FF-PNet, which leverages two innovative modules. For coarse-grained feature extraction, a Residual Feature Fusion Module (RFFM) is designed, while a Residual Deformation Field Fusion Module (RDFFM) is introduced for capturing fine-grained image deformations. The parallel operation of these two modules allows the model to effectively manage complex image deformations. Notably, the encoding stage of FF-PNet relies solely on traditional convolutional neural networks, eschewing attention mechanisms or multilayer perceptrons, yet still achieves significant improvements in registration accuracy. This emphasizes the enhanced feature decoding capabilities provided by RFFM and RDFFM. Comprehensive experiments conducted on the LPBA and OASIS datasets demonstrate that the proposed network consistently outperforms established methods in key evaluation metrics, such as the Dice Similarity Coefficient, confirming its efficacy in the domain of medical image registration. <div>
arXiv:2505.04938v1 Announce Type: new 
Abstract: In recent years, deformable medical image registration techniques have made significant progress. However, existing models still lack efficiency in parallel extraction of coarse and fine-grained features. To address this, we construct a new pyramid registration network based on feature and deformation field (FF-PNet). For coarse-grained feature extraction, we design a Residual Feature Fusion Module (RFFM), for fine-grained image deformation, we propose a Residual Deformation Field Fusion Module (RDFFM). Through the parallel operation of these two modules, the model can effectively handle complex image deformations. It is worth emphasizing that the encoding stage of FF-PNet only employs traditional convolutional neural networks without any attention mechanisms or multilayer perceptrons, yet it still achieves remarkable improvements in registration accuracy, fully demonstrating the superior feature decoding capabilities of RFFM and RDFFM. We conducted extensive experiments on the LPBA and OASIS datasets. The results show our network consistently outperforms popular methods in metrics like the Dice Similarity Coefficient.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building-Guided Pseudo-Label Learning for Cross-Modal Building Damage Mapping</title>
<link>https://arxiv.org/abs/2505.04941</link>
<guid>https://arxiv.org/abs/2505.04941</guid>
<content:encoded><![CDATA[
<div> Keywords: building damage assessment, remote sensing images, pseudo-label learning, multi-model fusion, change detection  

<br /><br />Summary: This study focuses on accurately assessing building damage using bi-temporal multi-modal remote sensing images, which is vital for disaster response and recovery. It introduces a Building-Guided Pseudo-Label Learning Framework to tackle the challenges posed by the mapping of building damage from pre-disaster optical and post-disaster SAR images. Initially, the research trains multiple building extraction models using pre-disaster optical images coupled with building labels. To improve building segmentation, strategies like multi-model fusion and test-time augmentation are applied to create pseudo-probabilities, culminating in a low-uncertainty pseudo-label training method for refinement. Subsequently, a change detection model is developed, using bi-temporal cross-modal images and damaged building labels. To enhance damage classification accuracy, a building-guided low-uncertainty pseudo-label refinement approach is introduced, utilizing prior knowledge of buildings to inform pseudo-label generation for damaged structures. The efficacy of this framework is evidenced by experimental results from the 2025 IEEE GRSS Data Fusion Contest dataset, where it achieved the highest mean Intersection over Union (mIoU) score of 54.28% and secured the top position in the competition. <div>
arXiv:2505.04941v1 Announce Type: new 
Abstract: Accurate building damage assessment using bi-temporal multi-modal remote sensing images is essential for effective disaster response and recovery planning. This study proposes a novel Building-Guided Pseudo-Label Learning Framework to address the challenges of mapping building damage from pre-disaster optical and post-disaster SAR images. First, we train a series of building extraction models using pre-disaster optical images and building labels. To enhance building segmentation, we employ multi-model fusion and test-time augmentation strategies to generate pseudo-probabilities, followed by a low-uncertainty pseudo-label training method for further refinement. Next, a change detection model is trained on bi-temporal cross-modal images and damaged building labels. To improve damage classification accuracy, we introduce a building-guided low-uncertainty pseudo-label refinement strategy, which leverages building priors from the previous step to guide pseudo-label generation for damaged buildings, reducing uncertainty and enhancing reliability. Experimental results on the 2025 IEEE GRSS Data Fusion Contest dataset demonstrate the effectiveness of our approach, which achieved the highest mIoU score (54.28%) and secured first place in the competition.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models</title>
<link>https://arxiv.org/abs/2505.04946</link>
<guid>https://arxiv.org/abs/2505.04946</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, benchmark, fidelity, evaluation, synthesis  

<br /><br />Summary: Recent advancements in scalable deep architectures and large-scale pretraining have significantly improved text-to-video generation, enabling high-fidelity content across diverse styles, with applications in advertising, entertainment, and education. However, these models face challenges in rendering precise on-screen text, such as captions and mathematical formulas, which is crucial for applications requiring textual accuracy. To address this gap, the authors introduce T2VTextBench, the first human-evaluation benchmark specifically designed to assess on-screen text fidelity and temporal consistency in text-to-video models. The benchmark includes a suite of prompts with complex text strings and dynamic scene changes, testing each model's ability to maintain detailed instructions throughout the video frames. The researchers evaluate ten state-of-the-art text-to-video systems, spanning both open-source and commercial offerings. Their findings reveal that most of the models struggle to generate legible and consistent text, highlighting a significant shortcoming in current video generation technology. This research underscores the necessity for further advancements in textual manipulation within video synthesis to improve the overall quality and utility of text-to-video applications. <div>
arXiv:2505.04946v1 Announce Type: new 
Abstract: Thanks to recent advancements in scalable deep architectures and large-scale pretraining, text-to-video generation has achieved unprecedented capabilities in producing high-fidelity, instruction-following content across a wide range of styles, enabling applications in advertising, entertainment, and education. However, these models' ability to render precise on-screen text, such as captions or mathematical formulas, remains largely untested, posing significant challenges for applications requiring exact textual accuracy. In this work, we introduce T2VTextBench, the first human-evaluation benchmark dedicated to evaluating on-screen text fidelity and temporal consistency in text-to-video models. Our suite of prompts integrates complex text strings with dynamic scene changes, testing each model's ability to maintain detailed instructions across frames. We evaluate ten state-of-the-art systems, ranging from open-source solutions to commercial offerings, and find that most struggle to generate legible, consistent text. These results highlight a critical gap in current video generators and provide a clear direction for future research aimed at enhancing textual manipulation in video synthesis.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Efficient Method for Accurate Pose Estimation and Error Correction of Cuboidal Objects</title>
<link>https://arxiv.org/abs/2505.04962</link>
<guid>https://arxiv.org/abs/2505.04962</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous picking, pose estimation, cuboidal objects, local registration, linear time approach

<br /><br />Summary: This paper introduces a solution for the autonomous picking of cuboidal objects from both organized and unorganized piles, emphasizing high precision. The focus is on an efficient method for accurate pose estimation of cuboid-shaped objects to minimize errors in target pose while being time-efficient. Traditional pose estimation techniques, particularly global point cloud registrations, often encounter minor pose errors, necessitating the use of local registration algorithms to enhance accuracy. However, these local methods introduce execution time overhead and uncertainty related to the final pose accuracy. To address these concerns, the authors propose an alternative approach that operates in linear time for pose error estimation and correction. The paper provides an overview of the entire solution and subsequently details the individual modules that constitute the proposed algorithm, presenting a comprehensive framework designed to improve the reliability and efficiency of pose estimation in robotic applications for picking tasks. The proposed method aims to achieve both precision and speed, which are critical for effective automation in various industrial settings. <div>
arXiv:2505.04962v1 Announce Type: new 
Abstract: The proposed system outlined in this paper is a solution to a use case that requires the autonomous picking of cuboidal objects from an organized or unorganized pile with high precision. This paper presents an efficient method for precise pose estimation of cuboid-shaped objects, which aims to reduce errors in target pose in a time-efficient manner. Typical pose estimation methods like global point cloud registrations are prone to minor pose errors for which local registration algorithms are generally used to improve pose accuracy. However, due to the execution time overhead and uncertainty in the error of the final achieved pose, an alternate, linear time approach is proposed for pose error estimation and correction. This paper presents an overview of the solution followed by a detailed description of individual modules of the proposed algorithm.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis</title>
<link>https://arxiv.org/abs/2505.04963</link>
<guid>https://arxiv.org/abs/2505.04963</guid>
<content:encoded><![CDATA[
<div> Keywords: medical imaging, ViCTr, liver cirrhosis, pathologies, image synthesis  

<br /><br />Summary: Synthesizing medical images, particularly for conditions like liver cirrhosis, poses significant challenges due to limited annotated data and domain gaps. Existing methodologies often struggle with maintaining anatomical fidelity and addressing diffuse pathologies. To tackle these issues, this work introduces ViCTr (Vital Consistency Transfer), a two-stage framework that integrates a rectified flow trajectory with a Tweedie-corrected diffusion process. The first stage involves pretraining ViCTr on the ATLAS-8k dataset utilizing Elastic Weight Consolidation (EWC) to preserve anatomy. In the second stage, the model undergoes adversarial fine-tuning with Low-Rank Adaptation (LoRA) to enable precise control over the severity of pathologies. Notably, ViCTr reformulates Tweedie's formula, allowing for one-step sampling that trims inference time from 50 to just 4 steps while maintaining anatomical realism. Evaluation on BTCV, AMOS, and CirrMRI600+ datasets reveals state-of-the-art performance, with a Medical Frechet Inception Distance (MFID) score that is 28% lower than previous techniques. Additionally, the model improves nnUNet segmentation by 3.8% mDSC when used for data augmentation. Radiologist reviews affirm the clinical indistinguishability of ViCTr-generated MRIs from actual scans, marking a significant advancement in AI-driven medical imaging research. <div>
arXiv:2505.04963v1 Announce Type: new 
Abstract: Synthesizing medical images remains challenging due to limited annotated pathological data, modality domain gaps, and the complexity of representing diffuse pathologies such as liver cirrhosis. Existing methods often struggle to maintain anatomical fidelity while accurately modeling pathological features, frequently relying on priors derived from natural images or inefficient multi-step sampling. In this work, we introduce ViCTr (Vital Consistency Transfer), a novel two-stage framework that combines a rectified flow trajectory with a Tweedie-corrected diffusion process to achieve high-fidelity, pathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k dataset using Elastic Weight Consolidation (EWC) to preserve critical anatomical structures. We then fine-tune the model adversarially with Low-Rank Adaptation (LoRA) modules for precise control over pathology severity. By reformulating Tweedie's formula within a linear trajectory framework, ViCTr supports one-step sampling, reducing inference from 50 steps to just 4, without sacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and CirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art performance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for cirrhosis synthesis 28% lower than existing approaches and improving nnUNet segmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews indicate that ViCTr-generated liver cirrhosis MRIs are clinically indistinguishable from real scans. To our knowledge, ViCTr is the first method to provide fine-grained, pathology-aware MRI synthesis with graded severity control, closing a critical gap in AI-driven medical imaging research.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAG-VLM: Fine-Tuning of a Large-Scale Model to Recognize Angiographic Images for Next-Generation Diagnostic Systems</title>
<link>https://arxiv.org/abs/2505.04964</link>
<guid>https://arxiv.org/abs/2505.04964</guid>
<content:encoded><![CDATA[
<div> Keywords: coronary angiography, AI, VLMs, decision support, key-frame detection  

<br /><br />Summary: Coronary angiography (CAG) is recognized as the gold-standard imaging technique for assessing coronary artery disease, but its interpretation is primarily dependent on expert cardiologists. To enhance decision-making with AI, the authors present a two-stage, physician-curated workflow alongside a bilingual CAG image-report dataset (Japanese/English). In the first stage, they sampled 14,686 frames from 539 exams and annotated them for key-frame detection and laterality classification. A ConvNeXt-Base CNN trained on this dataset demonstrated an impressive 0.96 F1 score for laterality classification, even with low-contrast frames. In the second stage, this CNN was applied to 243 independent exams to extract 1,114 key frames, which were paired with their associated pre-procedure reports and validated diagnostic and treatment summaries, forming a parallel corpus. The authors subsequently fine-tuned three open-source Vision-Language Models (VLMs)—PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3—utilizing Low-Rank Adaptation (LoRA), evaluating them against VLScore and cardiologist assessments. Although PaliGemma2 with LoRA received the highest VLScore, Gemma3 with LoRA garnered the best clinician rating, thus designating it as the optimal model, CAG-VLM, indicating its potential to aid cardiologists in generating clinical reports and treatment recommendations from CAG images. <div>
arXiv:2505.04964v1 Announce Type: new 
Abstract: Coronary angiography (CAG) is the gold-standard imaging modality for evaluating coronary artery disease, but its interpretation and subsequent treatment planning rely heavily on expert cardiologists. To enable AI-based decision support, we introduce a two-stage, physician-curated pipeline and a bilingual (Japanese/English) CAG image-report dataset. First, we sample 14,686 frames from 539 exams and annotate them for key-frame detection and left/right laterality; a ConvNeXt-Base CNN trained on this data achieves 0.96 F1 on laterality classification, even on low-contrast frames. Second, we apply the CNN to 243 independent exams, extract 1,114 key frames, and pair each with its pre-procedure report and expert-validated diagnostic and treatment summary, yielding a parallel corpus. We then fine-tune three open-source VLMs (PaliGemma2, Gemma3, and ConceptCLIP-enhanced Gemma3) via LoRA and evaluate them using VLScore and cardiologist review. Although PaliGemma2 w/LoRA attains the highest VLScore, Gemma3 w/LoRA achieves the top clinician rating (mean 7.20/10); we designate this best-performing model as CAG-VLM. These results demonstrate that specialized, fine-tuned VLMs can effectively assist cardiologists in generating clinical reports and treatment recommendations from CAG images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2505.04965</link>
<guid>https://arxiv.org/abs/2505.04965</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D visual grounding, DenseGrounding, ego-centric, language models, semantic enhancement  

<br /><br />Summary: Enabling intelligent agents to effectively comprehend and interact with 3D environments using natural language is essential for advancements in robotics and human-computer interaction. A primary challenge in this domain is ego-centric 3D visual grounding, where agents must identify target objects based on verbal descriptions. This task faces two significant hurdles: the loss of fine-grained visual semantics during the fusion of point clouds and ego-centric multi-view images, and the constraints imposed by arbitrary language descriptions on textual context. To tackle these challenges, we introduce DenseGrounding, which enhances both visual and textual semantics. Our approach includes a Hierarchical Scene Semantic Enhancer that captures global scene features while maintaining fine-grained details, thereby facilitating better cross-modal alignment. Additionally, we employ a Language Semantic Enhancer that utilizes large language models to generate enriched and contextually diverse textual descriptions during training. Extensive experiments demonstrate that DenseGrounding outperforms existing methods by 5.81% and 7.56% in accuracy on comprehensive and smaller datasets, respectively, setting a new state-of-the-art in ego-centric 3D visual grounding. Furthermore, our method won 1st place and received the Innovation Award at the CVPR 2024 Autonomous Grand Challenge, underscoring its effectiveness and robustness. <div>
arXiv:2505.04965v1 Announce Type: new 
Abstract: Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions. However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. For text descriptions, we propose a Language Semantic Enhancer that leverages large language models to provide rich context and diverse language descriptions with additional context during model training. Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, with improvements of 5.81% and 7.56% when trained on the comprehensive full dataset and smaller mini subset, respectively, further advancing the SOTA in egocentric 3D visual grounding. Our method also achieves 1st place and receives the Innovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</title>
<link>https://arxiv.org/abs/2505.04974</link>
<guid>https://arxiv.org/abs/2505.04974</guid>
<content:encoded><![CDATA[
<div> Keywords: Bilingual, Motion Generation, Dataset, Diffusion Model, Alignment

<br /><br />Summary: Bilingual text-to-motion generation synthesizes 3D human motions from bilingual text inputs and has significant potential in fields like gaming, film, and robotics. However, it faces challenges including the lack of bilingual motion-language datasets and alignment issues in diffusion models, which can result in semantically inconsistent or low-quality motions. To tackle these problems, the authors introduce BiHumanML3D, a novel bilingual human motion dataset, establishing an important benchmark for bilingual text-to-motion generation models. Additionally, they present the Bilingual Motion Diffusion model (BiMD), which utilizes cross-lingual aligned representations to capture semantic meanings, facilitating a unified bilingual approach. The study further introduces the Reward-guided sampling Alignment (ReAlign) method, featuring a step-aware reward model that evaluates alignment quality during the sampling process. This method employs a reward-guided strategy to steer the diffusion process toward better alignment. The reward model incorporates step-aware tokens along with a text-aligned module for semantic consistency and a motion-aligned module for realism, refining motions at each timestep. Experimental results indicate that the proposed approach considerably enhances text-motion alignment and motion quality when compared to existing state-of-the-art methods. <div>
arXiv:2505.04974v1 Announce Type: new 
Abstract: Bilingual text-to-motion generation, which synthesizes 3D human motions from bilingual text inputs, holds immense potential for cross-linguistic applications in gaming, film, and robotics. However, this task faces critical challenges: the absence of bilingual motion-language datasets and the misalignment between text and motion distributions in diffusion models, leading to semantically inconsistent or low-quality motions. To address these challenges, we propose BiHumanML3D, a novel bilingual human motion dataset, which establishes a crucial benchmark for bilingual text-to-motion generation models. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD), which leverages cross-lingual aligned representations to capture semantics, thereby achieving a unified bilingual model. Building upon this, we propose Reward-guided sampling Alignment (ReAlign) method, comprising a step-aware reward model to assess alignment quality during sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Experiments demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods. Project page: https://wengwanjiang.github.io/ReAlign-page/.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization</title>
<link>https://arxiv.org/abs/2505.04979</link>
<guid>https://arxiv.org/abs/2505.04979</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, attribute bias, deconfounding, debiasing, causal graph

<br /><br />Summary:  
Attribute bias in federated learning (FL) can degrade model performance by promoting non-causal associations. Current solutions either use data augmentation to enhance sample diversity or knowledge distillation for invariant representations, but they fall short of comprehensively analyzing inference paths and suffer from confounding factors. To overcome these challenges, the authors introduce the FedDDL method, which constructs a structured causal graph to analyze the inference process and utilizes backdoor adjustment to remove confounding paths. It includes an intra-client deconfounding learning module tailored for computer vision tasks, effectively decoupling backgrounds from objects and generating counterfactual samples that prevent the model from incorrectly using background information to infer labels. Additionally, it features an inter-client debiasing learning module that forms causal prototypes tailored to minimize background presence in prototypes. This innovative approach enhances the alignment of heterogeneous representations through causal prototypical regularization. Extensive experiments conducted on two benchmark datasets reveal that FedDDL significantly improves the model’s ability to focus on primary objects in unseen data, yielding an average increase of 4.5% in Top-1 Accuracy compared to nine existing state-of-the-art methods. <div>
arXiv:2505.04979v1 Announce Type: new 
Abstract: Attribute bias in federated learning (FL) typically leads local models to optimize inconsistently due to the learning of non-causal associations, resulting degraded performance. Existing methods either use data augmentation for increasing sample diversity or knowledge distillation for learning invariant representations to address this problem. However, they lack a comprehensive analysis of the inference paths, and the interference from confounding factors limits their performance. To address these limitations, we propose the \underline{Fed}erated \underline{D}econfounding and \underline{D}ebiasing \underline{L}earning (FedDDL) method. It constructs a structured causal graph to analyze the model inference process, and performs backdoor adjustment to eliminate confounding paths. Specifically, we design an intra-client deconfounding learning module for computer vision tasks to decouple background and objects, generating counterfactual samples that establish a connection between the background and any label, which stops the model from using the background to infer the label. Moreover, we design an inter-client debiasing learning module to construct causal prototypes to reduce the proportion of the background in prototype components. Notably, it bridges the gap between heterogeneous representations via causal prototypical regularization. Extensive experiments on 2 benchmarking datasets demonstrate that \methodname{} significantly enhances the model capability to focus on main objects in unseen data, leading to 4.5\% higher Top-1 Accuracy on average over 9 state-of-the-art existing methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StabStitch++: Unsupervised Online Video Stitching with Spatiotemporal Bidirectional Warps</title>
<link>https://arxiv.org/abs/2505.05001</link>
<guid>https://arxiv.org/abs/2505.05001</guid>
<content:encoded><![CDATA[
<div> Keywords: video stitching, warping shake, temporal stabilization, unsupervised learning, StabStitch++

<br /><br />Summary: The article addresses a new issue in video stitching known as warping shake, which results in undesirable temporal shakes due to sequentially unsmooth warps, even when input videos are stable. To tackle this issue, the authors propose a novel framework called StabStitch++, which integrates spatial stitching and temporal stabilization using unsupervised learning. Instead of the traditional approach that warps one image to align with another, StabStitch++ introduces a virtual midplane for projection, utilizing a differentiable bidirectional decomposition module to disentangle homography transformations. This evenly distributes alignment burdens and distortions across image views. The framework derives stitching trajectories by combining spatial and temporal warps inspired by video stabilization techniques. A warp smoothing model is introduced to create stable stitched videos, supported by a hybrid loss function that promotes content alignment, trajectory smoothness, and collaborative online processing. StabStitch++ improves upon its predecessor, StabStitch, by optimizing both alignment and stabilization simultaneously, particularly in real-time applications. The authors also establish a benchmarking dataset with diverse camera motions and scenes to evaluate their framework, demonstrating superior performance and efficiency in video stitching. <div>
arXiv:2505.05001v1 Announce Type: new 
Abstract: We retarget video stitching to an emerging issue, named warping shake, which unveils the temporal content shakes induced by sequentially unsmooth warps when extending image stitching to video stitching. Even if the input videos are stable, the stitched video can inevitably cause undesired warping shakes and affect the visual experience. To address this issue, we propose StabStitch++, a novel video stitching framework to realize spatial stitching and temporal stabilization with unsupervised learning simultaneously. First, different from existing learning-based image stitching solutions that typically warp one image to align with another, we suppose a virtual midplane between original image planes and project them onto it. Concretely, we design a differentiable bidirectional decomposition module to disentangle the homography transformation and incorporate it into our spatial warp, evenly spreading alignment burdens and projective distortions across two views. Then, inspired by camera paths in video stabilization, we derive the mathematical expression of stitching trajectories in video stitching by elaborately integrating spatial and temporal warps. Finally, a warp smoothing model is presented to produce stable stitched videos with a hybrid loss to simultaneously encourage content alignment, trajectory smoothness, and online collaboration. Compared with StabStitch that sacrifices alignment for stabilization, StabStitch++ makes no compromise and optimizes both of them simultaneously, especially in the online mode. To establish an evaluation benchmark and train the learning framework, we build a video stitching dataset with a rich diversity in camera motions and scenes. Experiments exhibit that StabStitch++ surpasses current solutions in stitching performance, robustness, and efficiency, offering compelling advancements in this field by building a real-time online video stitching system.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Thoracolumbar Stump Rib Detection and Analysis in a Large CT Cohort</title>
<link>https://arxiv.org/abs/2505.05004</link>
<guid>https://arxiv.org/abs/2505.05004</guid>
<content:encoded><![CDATA[
<div> Keywords: thoracolumbar, stump ribs, deep-learning, segmentation, morphology

<br /><br />Summary: This study focuses on thoracolumbar stump ribs, which are critical indicators of thoracolumbar transitional vertebrae or enumeration anomalies. Unlike previous research that qualitatively describes these anomalies through manual assessment, this work automates the detection of stump ribs and quantitatively analyzes their morphology. A high-resolution deep-learning model for rib segmentation was successfully trained, achieving a significant improvement in accuracy (Dice score of 0.997 compared to 0.779, p-value < 0.01). Additionally, an iterative algorithm and piece-wise linear interpolation were employed to assess rib length, resulting in a success rate of 98.2%. Morphological analysis revealed that stump ribs articulate more posteriorly at the vertebrae (-19.2 ± 3.8 vs. -13.8 ± 2.5, p-value < 0.01), are thinner (260.6 ± 103.4 vs. 563.6 ± 127.1, p-value < 0.01), and demonstrate a more downward and sideways orientation in the initial centimeters compared to full-length ribs. The study also achieved an F1-score of 0.84 in distinguishing stump ribs from regular ribs, even with partial visibility. The model weights and masks developed are made publicly available for further research. <div>
arXiv:2505.05004v1 Announce Type: new 
Abstract: Thoracolumbar stump ribs are one of the essential indicators of thoracolumbar transitional vertebrae or enumeration anomalies. While some studies manually assess these anomalies and describe the ribs qualitatively, this study aims to automate thoracolumbar stump rib detection and analyze their morphology quantitatively. To this end, we train a high-resolution deep-learning model for rib segmentation and show significant improvements compared to existing models (Dice score 0.997 vs. 0.779, p-value < 0.01). In addition, we use an iterative algorithm and piece-wise linear interpolation to assess the length of the ribs, showing a success rate of 98.2%. When analyzing morphological features, we show that stump ribs articulate more posteriorly at the vertebrae (-19.2 +- 3.8 vs -13.8 +- 2.5, p-value < 0.01), are thinner (260.6 +- 103.4 vs. 563.6 +- 127.1, p-value < 0.01), and are oriented more downwards and sideways within the first centimeters in contrast to full-length ribs. We show that with partially visible ribs, these features can achieve an F1-score of 0.84 in differentiating stump ribs from regular ones. We publish the model weights and masks for public use.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition</title>
<link>https://arxiv.org/abs/2505.05007</link>
<guid>https://arxiv.org/abs/2505.05007</guid>
<content:encoded><![CDATA[
<div> Keywords: map matching, Hidden Markov Model, lane markings, scenario recognition, multilevel road

<br /><br />Summary: Accurate online map matching is vital for vehicle navigation and intelligent driving features. Existing methods struggle in complex road networks, particularly in multilevel areas. This study introduces an online Standard Definition (SD) map matching technique that employs a Hidden Markov Model (HMM) enhanced by multiple probability factors. The method accurately matches maps by utilizing lane markings and scenario recognition. Initially, lane markings are generated using a multi-lane tracking approach and linked to the SD map through HMM, creating an enriched SD map. Vehicles can then re-localize by applying Iterative Closest Point (ICP) registration based on these lane markings. The probability factor for lane marking detection is determined through association probabilities between adjacent lanes and roads. Additionally, a driving scenario recognition model yields an emission probability factor, significantly enhancing map matching on elevated roads and the urban roads below them. Extensive road tests in Europe and China validate the method, revealing superior accuracy compared to existing techniques, notably in multilevel road regions. The proposed method achieves F1 scores of 98.04% on the Zenseact Open Dataset and 94.60% on Shanghai multilevel area data, outperforming benchmark methods. Implementation details are available at https://github.com/TRV-Lab/LMSR-OMM. <div>
arXiv:2505.05007v1 Announce Type: new 
Abstract: Accurate online map matching is fundamental to vehicle navigation and the activation of intelligent driving functions. Current online map matching methods are prone to errors in complex road networks, especially in multilevel road area. To address this challenge, we propose an online Standard Definition (SD) map matching method by constructing a Hidden Markov Model (HMM) with multiple probability factors. Our proposed method can achieve accurate map matching even in complex road networks by carefully leveraging lane markings and scenario recognition in the designing of the probability factors. First, the lane markings are generated by a multi-lane tracking method and associated with the SD map using HMM to build an enriched SD map. In areas covered by the enriched SD map, the vehicle can re-localize itself by performing Iterative Closest Point (ICP) registration for the lane markings. Then, the probability factor accounting for the lane marking detection can be obtained using the association probability between adjacent lanes and roads. Second, the driving scenario recognition model is applied to generate the emission probability factor of scenario recognition, which improves the performance of map matching on elevated roads and ordinary urban roads underneath them. We validate our method through extensive road tests in Europe and China, and the experimental results show that our proposed method effectively improves the online map matching accuracy as compared to other existing methods, especially in multilevel road area. Specifically, the experiments show that our proposed method achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset and test data of multilevel road areas in Shanghai respectively, significantly outperforming benchmark methods. The implementation is available at https://github.com/TRV-Lab/LMSR-OMM.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Contextual Embedding for Robust Far-View Borehole Detection</title>
<link>https://arxiv.org/abs/2505.05008</link>
<guid>https://arxiv.org/abs/2505.05008</guid>
<content:encoded><![CDATA[
<div> Keywords: controlled blasting, detection method, adaptive augmentation, embedding stabilization, contextual refinement  

<br /><br />Summary:  
In controlled blasting operations, the ability to detect densely packed small boreholes from far-view imagery is vital for ensuring safety and operational efficiency. Existing methods struggle due to the minute scale, dense arrangements, and the lack of distinctive visual features of these boreholes. To overcome these limitations, the authors propose a novel adaptive detection approach that enhances existing architectures like YOLO by employing consistent embedding representations through exponential moving average (EMA)-based statistical updates. The method introduces three key components: first, adaptive augmentation that utilizes dynamically updated image statistics to effectively manage variations in illumination and texture; second, embedding stabilization to guarantee consistent and reliable feature extraction; and third, contextual refinement that uses spatial context to enhance detection accuracy. The use of EMA is particularly beneficial given the challenges of limited visual complexity and the small scale of boreholes, leading to stable and robust representation learning. Experimental results on a challenging proprietary quarry-site dataset demonstrate significant improvements over baseline YOLO-based architectures, underscoring the proposed method's effectiveness under realistic and complex industrial conditions. <div>
arXiv:2505.05008v1 Announce Type: new 
Abstract: In controlled blasting operations, accurately detecting densely distributed tiny boreholes from far-view imagery is critical for operational safety and efficiency. However, existing detection methods often struggle due to small object scales, highly dense arrangements, and limited distinctive visual features of boreholes. To address these challenges, we propose an adaptive detection approach that builds upon existing architectures (e.g., YOLO) by explicitly leveraging consistent embedding representations derived through exponential moving average (EMA)-based statistical updates.
  Our method introduces three synergistic components: (1) adaptive augmentation utilizing dynamically updated image statistics to robustly handle illumination and texture variations; (2) embedding stabilization to ensure consistent and reliable feature extraction; and (3) contextual refinement leveraging spatial context for improved detection accuracy. The pervasive use of EMA in our method is particularly advantageous given the limited visual complexity and small scale of boreholes, allowing stable and robust representation learning even under challenging visual conditions. Experiments on a challenging proprietary quarry-site dataset demonstrate substantial improvements over baseline YOLO-based architectures, highlighting our method's effectiveness in realistic and complex industrial scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOAP: Style-Omniscient Animatable Portraits</title>
<link>https://arxiv.org/abs/2505.05022</link>
<guid>https://arxiv.org/abs/2505.05022</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D avatars, diffusion models, topology-consistent, animation controls, soap

<br /><br />Summary: Creating animatable 3D avatars from a single image is a complex task, particularly due to style limitations such as realistic, cartoon, or anime representations, as well as challenges in handling accessories and hairstyles. Traditional 3D diffusion models have made strides in reconstructing single-view images but often lack necessary animation controls and can produce artifacts due to the domain gap. To address these issues, we introduce SOAP, a style-omniscient framework designed to generate rigged, topology-consistent avatars from any portrait. Our approach utilizes a multiview diffusion model trained on a dataset of 24,000 3D heads across various styles and employs an adaptive optimization pipeline to deform the FLAME mesh while preserving its topology and rigging through differentiable rendering techniques. The outcome is textured avatars that support FACS-based animation and include realistic features such as eyeballs, teeth, and intricate hairstyles or accessories. Extensive experiments validate the effectiveness of our method, demonstrating its superiority over existing state-of-the-art techniques in both single-view head modeling and diffusion-based 3D generation from images. Our implementation and dataset are publicly accessible for research at https://github.com/TingtingLiao/soap. <div>
arXiv:2505.05022v1 Announce Type: new 
Abstract: Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles. While 3D diffusion models advance single-view reconstruction for general objects, outputs often lack animation controls or suffer from artifacts because of the domain gap. We propose SOAP, a style-omniscient framework to generate rigged, topology-consistent avatars from any portrait. Our method leverages a multiview diffusion model trained on 24K 3D heads with multiple styles and an adaptive optimization pipeline to deform the FLAME mesh while maintaining topology and rigging via differentiable rendering. The resulting textured avatars support FACS-based animation, integrate with eyeballs and teeth, and preserve details like braided hair or accessories. Extensive experiments demonstrate the superiority of our method over state-of-the-art techniques for both single-view head modeling and diffusion-based generation of Image-to-3D. Our code and data are publicly available for research purposes at https://github.com/TingtingLiao/soap.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Split Matching for Inductive Zero-shot Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.05023</link>
<guid>https://arxiv.org/abs/2505.05023</guid>
<content:encoded><![CDATA[
<div> Keywords: Zero-shot Semantic Segmentation, query-based segmentation, Split Matching, Hungarian matching, Multi-scale Feature Enhancement

<br /><br />Summary: 
This article focuses on Zero-shot Semantic Segmentation (ZSS), which involves segmenting categories not annotated during training. Traditional fine-tuning of vision-language models faces challenges, primarily overfitting to seen categories due to the absence of supervision for unseen classes. It highlights query-based segmentation as a promising alternative that excels in object localization without explicit labels. A major issue with conventional Hungarian matching in this context is its reliance on full supervision, often misclassifying unseen categories as background. To tackle this, the authors propose a novel Split Matching (SM) strategy that separates Hungarian matching into two components: one for annotated regions (seen classes) and another for unannotated regions (unseen candidates). They cluster CLIP dense features to create pseudo masks and derive region-level embeddings. Matching occurs independently for these two groups, focusing on class-level similarity and mask-level consistency. Additionally, they introduce a Multi-scale Feature Enhancement (MFE) module, which enhances decoder features through multi-scale aggregation for improved spatial detail capture. The work presents SM as the first decoupled Hungarian matching approach under the inductive ZSS setting, achieving state-of-the-art results on two standard benchmarks. <div>
arXiv:2505.05023v1 Announce Type: new 
Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment categories that are not annotated during training. While fine-tuning vision-language models has achieved promising results, these models often overfit to seen categories due to the lack of supervision for unseen classes. As an alternative to fully supervised approaches, query-based segmentation has shown great latent in ZSS, as it enables object localization without relying on explicit labels. However, conventional Hungarian matching, a core component in query-based frameworks, needs full supervision and often misclassifies unseen categories as background in the setting of ZSS. To address this issue, we propose Split Matching (SM), a novel assignment strategy that decouples Hungarian matching into two components: one for seen classes in annotated regions and another for latent classes in unannotated regions (referred to as unseen candidates). Specifically, we partition the queries into seen and candidate groups, enabling each to be optimized independently according to its available supervision. To discover unseen candidates, we cluster CLIP dense features to generate pseudo masks and extract region-level embeddings using CLS tokens. Matching is then conducted separately for the two groups based on both class-level similarity and mask-level consistency. Additionally, we introduce a Multi-scale Feature Enhancement (MFE) module that refines decoder features through residual multi-scale aggregation, improving the model's ability to capture spatial details across resolutions. SM is the first to introduce decoupled Hungarian matching under the inductive ZSS setting, and achieves state-of-the-art performance on two standard benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>xTrace: A Facial Expressive Behaviour Analysis Tool for Continuous Affect Recognition</title>
<link>https://arxiv.org/abs/2505.05043</link>
<guid>https://arxiv.org/abs/2505.05043</guid>
<content:encoded><![CDATA[
<div> Keywords: facial affect, emotion recognition, xTrace, video datasets, dimensional emotions

<br /><br />Summary: The paper addresses the challenges of recognizing expressive behaviors in face videos, focusing on two primary issues: the lack of large-scale labeled datasets and the difficulty of extracting meaningful facial features. To tackle these, the authors introduce xTrace, a tool designed for analyzing facial expressive behavior and predicting continuous values of emotions, specifically valence and arousal. xTrace is trained on an extensive dataset comprising approximately 450,000 videos that encapsulate a wide range of emotional expressions, enhancing its versatility in real-world applications. It employs facial affect descriptors that ensure explainability while maintaining high accuracy and low computational demands. The performance of xTrace is benchmarked against three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox. In validation using a set of 50,000 videos, xTrace achieved a mean Concordance Correlation Coefficient (CCC) of 0.86 and a mean absolute error of 0.13. A thorough error analysis reveals its high accuracy in recognizing emotions, robustness to non-frontal head poses, and a strong relationship between uncertainty estimates and accuracy, validating its effectiveness in the domain of Affective Computing. <div>
arXiv:2505.05043v1 Announce Type: new 
Abstract: Recognising expressive behaviours in face videos is a long-standing challenge in Affective Computing. Despite significant advancements in recent years, it still remains a challenge to build a robust and reliable system for naturalistic and in-the-wild facial expressive behaviour analysis in real time. This paper addresses two key challenges in building such a system: (1). The paucity of large-scale labelled facial affect video datasets with extensive coverage of the 2D emotion space, and (2). The difficulty of extracting facial video features that are discriminative, interpretable, robust, and computationally efficient. Toward addressing these challenges, we introduce xTrace, a robust tool for facial expressive behaviour analysis and predicting continuous values of dimensional emotions, namely valence and arousal, from in-the-wild face videos.
  To address challenge (1), our affect recognition model is trained on the largest facial affect video data set, containing ~450k videos that cover most emotion zones in the dimensional emotion space, making xTrace highly versatile in analysing a wide spectrum of naturalistic expressive behaviours. To address challenge (2), xTrace uses facial affect descriptors that are not only explainable, but can also achieve a high degree of accuracy and robustness with low computational complexity. The key components of xTrace are benchmarked against three existing tools: MediaPipe, OpenFace, and Augsburg Affect Toolbox. On an in-the-wild validation set composed of 50k videos, xTrace achieves 0.86 mean CCC and 0.13 mean absolute error values. We present a detailed error analysis of affect predictions from xTrace, illustrating (a). its ability to recognise emotions with high accuracy across most bins in the 2D emotion space, (b). its robustness to non-frontal head pose angles, and (c). a strong correlation between its uncertainty estimates and its accuracy.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UncertainSAM: Fast and Efficient Uncertainty Quantification of the Segment Anything Model</title>
<link>https://arxiv.org/abs/2505.05049</link>
<guid>https://arxiv.org/abs/2505.05049</guid>
<content:encoded><![CDATA[
<div> Keywords: Segment Anything Model, uncertainty quantification, Bayesian entropy, semi-supervised, predictive capabilities  

<br /><br />Summary:  
The introduction of the Segment Anything Model (SAM) has significantly impacted semantic segmentation applications, raising the importance of quantifying uncertainty in its outputs. The class-agnostic nature of SAM presents challenges for existing uncertainty quantification (UQ) methods. In response, this paper introduces a novel UQ model grounded in a Bayesian entropy framework that incorporates aleatoric, epistemic, and a newly defined task uncertainty. This theoretical foundation leads to the creation of USAM, a lightweight post-hoc method for uncertainty quantification. The model identifies the sources of uncertainty as stemming from under-parameterised models, poor prompts, or ambiguities within images. Demonstrating remarkable predictive capabilities, USAM is applied across several datasets, including SA-V, MOSE, ADE20k, DAVIS, and COCO, outperforming other methods. Furthermore, it offers a computationally efficient and user-friendly UQ solution capable of supporting user-prompting and enhancing semi-supervised learning pipelines. USAM allows users to effectively manage the tradeoff between accuracy and cost efficiency in their applications, making it a valuable tool in the realm of semantic segmentation. <div>
arXiv:2505.05049v1 Announce Type: new 
Abstract: The introduction of the Segment Anything Model (SAM) has paved the way for numerous semantic segmentation applications. For several tasks, quantifying the uncertainty of SAM is of particular interest. However, the ambiguous nature of the class-agnostic foundation model SAM challenges current uncertainty quantification (UQ) approaches. This paper presents a theoretically motivated uncertainty quantification model based on a Bayesian entropy formulation jointly respecting aleatoric, epistemic, and the newly introduced task uncertainty. We use this formulation to train USAM, a lightweight post-hoc UQ method. Our model traces the root of uncertainty back to under-parameterised models, insufficient prompts or image ambiguities. Our proposed deterministic USAM demonstrates superior predictive capabilities on the SA-V, MOSE, ADE20k, DAVIS, and COCO datasets, offering a computationally cheap and easy-to-use UQ alternative that can support user-prompting, enhance semi-supervised pipelines, or balance the tradeoff between accuracy and cost efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ULFine: Unbiased Lightweight Fine-tuning for Foundation-Model-Assisted Long-Tailed Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.05062</link>
<guid>https://arxiv.org/abs/2505.05062</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-Tailed Semi-Supervised Learning, large-scale visual foundation models, Unbiased Lightweight Fine-tuning, pseudo-labels, model performance  

<br /><br />Summary: This paper investigates the effects of large-scale visual foundation models, such as CLIP, on Long-Tailed Semi-Supervised Learning (LTSSL) using three strategies: Linear Probing (LP), Lightweight Fine-Tuning (LFT), and Full Fine-Tuning (FFT). The research reveals key insights: i) FFT leads to decreased model performance compared to LTSSL algorithms trained from scratch. Although LP and LFT enhance overall performance, they provide minimal benefits for tail classes. ii) LP generates numerous false pseudo-labels due to underlearned data. In contrast, while LFT reduces these false labels, it becomes overconfident, leading to biased fitting and exacerbating inherent pseudo-labeled and classifier biases in LTSSL, ultimately hindering performance in tail classes. To address these issues, the authors propose ULFine, an Unbiased Lightweight Fine-tuning strategy. ULFine mitigates overconfidence through confidence-aware adaptive fitting of textual prototypes and counters biases with complementary fusion of dual logits. The extensive experiments demonstrate that ULFine significantly decreases training costs by over ten times while substantially improving prediction accuracies compared to state-of-the-art methods. <div>
arXiv:2505.05062v1 Announce Type: new 
Abstract: Based on the success of large-scale visual foundation models like CLIP in various downstream tasks, this paper initially attempts to explore their impact on Long-Tailed Semi-Supervised Learning (LTSSL) by employing the foundation model with three strategies: Linear Probing (LP), Lightweight Fine-Tuning (LFT), and Full Fine-Tuning (FFT). Our analysis presents the following insights: i) Compared to LTSSL algorithms trained from scratch, FFT results in a decline in model performance, whereas LP and LFT, although boosting overall model performance, exhibit negligible benefits to tail classes. ii) LP produces numerous false pseudo-labels due to \textit{underlearned} training data, while LFT can reduce the number of these false labels but becomes overconfident about them owing to \textit{biased fitting} training data. This exacerbates the pseudo-labeled and classifier biases inherent in LTSSL, limiting performance improvement in the tail classes. With these insights, we propose a Unbiased Lightweight Fine-tuning strategy, \textbf{ULFine}, which mitigates the overconfidence via confidence-aware adaptive fitting of textual prototypes and counteracts the pseudo-labeled and classifier biases via complementary fusion of dual logits. Extensive experiments demonstrate that ULFine markedly decreases training costs by over ten times and substantially increases prediction accuracies compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FG-CLIP: Fine-Grained Visual and Textual Alignment</title>
<link>https://arxiv.org/abs/2505.05071</link>
<guid>https://arxiv.org/abs/2505.05071</guid>
<content:encoded><![CDATA[
<div> Keywords: FG-CLIP, fine-grained understanding, multimodal tasks, long captions, negative samples  

<br /><br />Summary:  
Contrastive Language-Image Pre-training (CLIP) has shown impressive performance in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its reliance on coarse-grained short captions. To improve this, the authors introduce Fine-Grained CLIP (FG-CLIP) featuring three key innovations. First, FG-CLIP utilizes large multimodal models to generate 1.6 billion long caption-image pairs, enhancing the capture of global semantic details. Second, a high-quality dataset is constructed containing 12 million images with 40 million region-specific bounding boxes aligned with detailed captions, promoting precise and context-rich representations. Third, the inclusion of 10 million hard fine-grained negative samples helps the model to better distinguish subtle semantic differences. Corresponding training methodologies are carefully designed to leverage this data. Extensive experiments confirm that FG-CLIP exceeds the performance of the original CLIP and other leading methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These findings underscore FG-CLIP's efficacy in capturing intricate image details and enhancing overall model performance. The related data, code, and models are publicly available at https://github.com/360CVGroup/FG-CLIP. <div>
arXiv:2505.05071v1 Announce Type: new 
Abstract: Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Affordances: Enabling Robots to Understand Object Functionality</title>
<link>https://arxiv.org/abs/2505.05074</link>
<guid>https://arxiv.org/abs/2505.05074</guid>
<content:encoded><![CDATA[
<div> Keywords: human-robot interaction, affordance prediction, reproducibility, Affordance Sheet, visual perception  

<br /><br />Summary: This work addresses the critical issue of reproducibility in human-robot interaction for assistive technologies, specifically focusing on the prediction of affordances, or the actions a robot can perform on objects. It critiques the varying formulations used for tasks like grasping detection and affordance classification, which can lead to unfair and unreliable benchmarks. To tackle this, a unified approach to visual affordance prediction is proposed. The authors conduct a comprehensive review of previous research, outlining strengths and weaknesses of existing methods and datasets, and identifying factors that hinder reproducibility. In pursuit of enhancing transparency, they introduce the Affordance Sheet, a documentation tool detailing solutions, datasets, and validation processes. Furthermore, they present a framework that links visual affordance prediction to the physical properties of objects, emphasizing that these properties influence robot interactions. By exploring the example of estimating object mass, the paper illustrates how this factor can significantly affect affordance predictions. Ultimately, this approach aims to connect affordance perception and robot actuation, synthesizing all relevant information about objects and their interactions to improve task accomplishment. <div>
arXiv:2505.05074v1 Announce Type: new 
Abstract: Human-robot interaction for assistive technologies relies on the prediction of affordances, which are the potential actions a robot can perform on objects. Predicting object affordances from visual perception is formulated differently for tasks such as grasping detection, affordance classification, affordance segmentation, and hand-object interaction synthesis. In this work, we highlight the reproducibility issue in these redefinitions, making comparative benchmarks unfair and unreliable. To address this problem, we propose a unified formulation for visual affordance prediction, provide a comprehensive and systematic review of previous works highlighting strengths and limitations of methods and datasets, and analyse what challenges reproducibility. To favour transparency, we introduce the Affordance Sheet, a document to detail the proposed solution, the datasets, and the validation. As the physical properties of an object influence the interaction with the robot, we present a generic framework that links visual affordance prediction to the physical world. Using the weight of an object as an example for this framework, we discuss how estimating object mass can affect the affordance prediction. Our approach bridges the gap between affordance perception and robot actuation, and accounts for the complete information about objects of interest and how the robot interacts with them to accomplish its task.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIDiff: Image Customization for Personalized Identities with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05081</link>
<guid>https://arxiv.org/abs/2505.05081</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image generation, PIDiff, identity information, diffusion model, StyleGAN

<br /><br />Summary: This paper introduces PIDiff, a novel fine-tuning-based diffusion model for generating personalized identities from text prompts and identity images. It addresses limitations in previous works that struggle with disentangling identity and background information, which often leads to diminished identity characteristics and reduced image diversity. Prior attempts combined the W+ space from StyleGAN with diffusion models to enhance identity representation but suffered from semantic interference during training due to entanglement. PIDiff overcomes these challenges by employing a fine-tuning strategy that leverages the W+ space for accurate feature extraction and localization, thereby avoiding semantic entanglement. Furthermore, PIDiff enables style editing by preserving the identity features across varying levels of detail. It also integrates a cross-attention block and an optimized parameter strategy to maintain both identity preservation and generative capabilities of pre-trained models during inference for in-the-wild images. Experimental results demonstrate the effectiveness of PIDiff in enhancing personalized identity image generation, showing improved accuracy and diversity while accurately reflecting identity characteristics in generated images. <div>
arXiv:2505.05081v1 Announce Type: new 
Abstract: Text-to-image generation for personalized identities aims at incorporating the specific identity into images using a text prompt and an identity image. Based on the powerful generative capabilities of DDPMs, many previous works adopt additional prompts, such as text embeddings and CLIP image embeddings, to represent the identity information, while they fail to disentangle the identity information and background information. As a result, the generated images not only lose key identity characteristics but also suffer from significantly reduced diversity. To address this issue, previous works have combined the W+ space from StyleGAN with diffusion models, leveraging this space to provide a more accurate and comprehensive representation of identity features through multi-level feature extraction. However, the entanglement of identity and background information in in-the-wild images during training prevents accurate identity localization, resulting in severe semantic interference between identity and background. In this paper, we propose a novel fine-tuning-based diffusion model for personalized identities text-to-image generation, named PIDiff, which leverages the W+ space and an identity-tailored fine-tuning strategy to avoid semantic entanglement and achieves accurate feature extraction and localization. Style editing can also be achieved by PIDiff through preserving the characteristics of identity features in the W+ space, which vary from coarse to fine. Through the combination of the proposed cross-attention block and parameter optimization strategy, PIDiff preserves the identity information and maintains the generation capability for in-the-wild images of the pre-trained model during inference. Our experimental results validate the effectiveness of our method in this task.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear Motion-Guided and Spatio-Temporal Aware Network for Unsupervised Event-Based Optical Flow</title>
<link>https://arxiv.org/abs/2505.05089</link>
<guid>https://arxiv.org/abs/2505.05089</guid>
<content:encoded><![CDATA[
<div> Keywords: event cameras, optical flow, spatio-temporal, nonlinear motion, unsupervised learning  

<br /><br />Summary: Event cameras capture continuous motion information, making them ideal for optical flow estimation. Existing learning-based methods often employ frame-based techniques, overlooking the spatio-temporal characteristics of events. These methods typically assume linear motion between consecutive events, leading to increased errors in long-time sequences. This study highlights the importance of spatio-temporal information and the need for accurate nonlinear motion modeling for effective optical flow estimation. To address these challenges, we introduce E-NMSTFlow, an unsupervised event-based optical flow network designed for long sequences. Key components of our approach include the Spatio-Temporal Motion Feature Aware (STMFA) module and the Adaptive Motion Feature Enhancement (AMFE) module, both of which leverage rich spatio-temporal data for better associations. Furthermore, we present a nonlinear motion compensation loss that enhances unsupervised learning by accounting for the nonlinear motion among events. Our extensive experiments underscore the effectiveness of E-NMSTFlow, demonstrating its superiority over existing methods. Notably, it achieves the top ranking among unsupervised learning approaches on the MVSEC and DSEC-Flow datasets. For further details, visit our project page at https://wynelio.github.io/E-NMSTFlow. <div>
arXiv:2505.05089v1 Announce Type: new 
Abstract: Event cameras have the potential to capture continuous motion information over time and space, making them well-suited for optical flow estimation. However, most existing learning-based methods for event-based optical flow adopt frame-based techniques, ignoring the spatio-temporal characteristics of events. Additionally, these methods assume linear motion between consecutive events within the loss time window, which increases optical flow errors in long-time sequences. In this work, we observe that rich spatio-temporal information and accurate nonlinear motion between events are crucial for event-based optical flow estimation. Therefore, we propose E-NMSTFlow, a novel unsupervised event-based optical flow network focusing on long-time sequences. We propose a Spatio-Temporal Motion Feature Aware (STMFA) module and an Adaptive Motion Feature Enhancement (AMFE) module, both of which utilize rich spatio-temporal information to learn spatio-temporal data associations. Meanwhile, we propose a nonlinear motion compensation loss that utilizes the accurate nonlinear motion between events to improve the unsupervised learning of our network. Extensive experiments demonstrate the effectiveness and superiority of our method. Remarkably, our method ranks first among unsupervised learning methods on the MVSEC and DSEC-Flow datasets. Our project page is available at https://wynelio.github.io/E-NMSTFlow.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DispBench: Benchmarking Disparity Estimation to Synthetic Corruptions</title>
<link>https://arxiv.org/abs/2505.05091</link>
<guid>https://arxiv.org/abs/2505.05091</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Learning, Disparity Estimation, Robustness, Benchmarking, Adversarial Attacks<br /><br />Summary: Deep learning (DL) has outperformed human capabilities in various benchmarks, particularly in disparity estimation—an essential task for applications like medical surgeries and autonomous navigation. However, reliance on DL methods for disparity estimation raises concerns due to their vulnerability to distribution shifts and adversarial attacks, which could undermine their reliability and generalization. Despite these challenges, a standardized benchmark to evaluate the robustness of disparity estimation techniques is currently lacking, impeding advancements in this area. To fill this void, we present DispBench, a comprehensive benchmarking tool designed to systematically assess the reliability of disparity estimation methods. DispBench tests robustness against synthetic image corruptions, including adversarial attacks and out-of-distribution shifts stemming from 2D Common Corruptions. This initiates the most extensive performance and robustness analysis of disparity estimation methods conducted to date, revealing significant correlations among accuracy, reliability, and generalization. The open-source code for DispBench is available at https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation. <div>
arXiv:2505.05091v1 Announce Type: new 
Abstract: Deep learning (DL) has surpassed human performance on standard benchmarks, driving its widespread adoption in computer vision tasks. One such task is disparity estimation, estimating the disparity between matching pixels in stereo image pairs, which is crucial for safety-critical applications like medical surgeries and autonomous navigation. However, DL-based disparity estimation methods are highly susceptible to distribution shifts and adversarial attacks, raising concerns about their reliability and generalization. Despite these concerns, a standardized benchmark for evaluating the robustness of disparity estimation methods remains absent, hindering progress in the field.
  To address this gap, we introduce DispBench, a comprehensive benchmarking tool for systematically assessing the reliability of disparity estimation methods. DispBench evaluates robustness against synthetic image corruptions such as adversarial attacks and out-of-distribution shifts caused by 2D Common Corruptions across multiple datasets and diverse corruption scenarios. We conduct the most extensive performance and robustness analysis of disparity estimation methods to date, uncovering key correlations between accuracy, reliability, and generalization. Open-source code for DispBench: https://github.com/shashankskagnihotri/benchmarking_robustness/tree/disparity_estimation/final/disparity_estimation
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05101</link>
<guid>https://arxiv.org/abs/2505.05101</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-object editing, localization, attribute-object mismatch, MDE-Edit, optimization

<br /><br />Summary: The paper addresses the challenges of multi-object editing in complex scenes, particularly when objects overlap or interact. Two main issues are presented: inaccurate localization of target objects leading to incomplete edits and attribute-object mismatches causing semantic conflicts, such as color bleeding. Current methods struggle with these due to attention alignment problems and feature entanglement. To improve upon existing techniques, the authors propose MDE-Edit, a training-free optimization method that operates at the inference stage. This approach focuses on precise localized image manipulation and introduces two key components: Object Alignment Loss (OAL), which aligns multi-layer cross-attention with segmentation masks for better object positioning, and Color Consistency Loss (CCL), which enhances attribute attention for target regions while minimizing leakage to adjacent areas. This dual-loss framework ensures coherent and accurate edits across multiple objects. Experiments showcase that MDE-Edit achieves superior editing accuracy and visual quality compared to state-of-the-art methods, establishing it as an effective solution for complex multi-object image manipulation tasks. <div>
arXiv:2505.05101v1 Announce Type: new 
Abstract: Multi-object editing aims to modify multiple objects or regions in complex scenes while preserving structural coherence. This task faces significant challenges in scenarios involving overlapping or interacting objects: (1) Inaccurate localization of target objects due to attention misalignment, leading to incomplete or misplaced edits; (2) Attribute-object mismatch, where color or texture changes fail to align with intended regions due to cross-attention leakage, creating semantic conflicts (\textit{e.g.}, color bleeding into non-target areas). Existing methods struggle with these challenges: approaches relying on global cross-attention mechanisms suffer from attention dilution and spatial interference between objects, while mask-based methods fail to bind attributes to geometrically accurate regions due to feature entanglement in multi-object scenarios. To address these limitations, we propose a training-free, inference-stage optimization approach that enables precise localized image manipulation in complex multi-object scenes, named MDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via two key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention with segmentation masks for precise object positioning, and Color Consistency Loss (CCL) amplifies target attribute attention within masks while suppressing leakage to adjacent regions. This dual-loss design ensures localized and coherent multi-object edits. Extensive experiments demonstrate that MDE-Edit outperforms state-of-the-art methods in editing accuracy and visual quality, offering a robust solution for complex multi-object image manipulation tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated vision-based assistance tools in bronchoscopy: stenosis severity estimation</title>
<link>https://arxiv.org/abs/2505.05136</link>
<guid>https://arxiv.org/abs/2505.05136</guid>
<content:encoded><![CDATA[
<div> Keywords: subglottic stenosis, bronchoscopy, automated evaluation, 3D model, dataset  

<br /><br />Summary: Subglottic stenosis involves the narrowing of the airway between the vocal cords and the trachea, typically assessed by estimating the percentage of obstruction through CT data or expert visual inspections, which can be subjective. This study proposes a novel automated pipeline for estimating subglottic stenosis severity during bronchoscopy without requiring the physician to navigate through the stenosed region. The method utilizes the physical effect of illumination decline in endoscopy to segment and track the airway lumen, generating a 3D model from a single frame to measure airway narrowing. Notably, this pipeline is the first of its kind for automated evaluation of subglottic stenosis severity using bronchoscopy images. Results demonstrate high consistency with ground-truth estimations from CT scans and expert assessments while showing reliable repeatability in multiple estimations for the same patient. Evaluation is performed using a newly created Subglottic Stenosis Dataset comprising real bronchoscopy procedures. This approach aims to facilitate quicker diagnoses and monitoring, minimizing radiation exposure for patients by eliminating the need for CT scans, and introduces the first public benchmark for assessing subglottic stenosis severity. <div>
arXiv:2505.05136v1 Announce Type: new 
Abstract: Purpose: Subglottic stenosis refers to the narrowing of the subglottis, the airway between the vocal cords and the trachea. Its severity is typically evaluated by estimating the percentage of obstructed airway. This estimation can be obtained from CT data or through visual inspection by experts exploring the region. However, visual inspections are inherently subjective, leading to less consistent and robust diagnoses. No public methods or datasets are currently available for automated evaluation of this condition from bronchoscopy video.
  Methods: We propose a pipeline for automated subglottic stenosis severity estimation during the bronchoscopy exploration, without requiring the physician to traverse the stenosed region. Our approach exploits the physical effect of illumination decline in endoscopy to segment and track the lumen and obtain a 3D model of the airway. This 3D model is obtained from a single frame and is used to measure the airway narrowing.
  Results: Our pipeline is the first to enable automated and robust subglottic stenosis severity measurement using bronchoscopy images. The results show consistency with ground-truth estimations from CT scans and expert estimations, and reliable repeatability across multiple estimations on the same patient. Our evaluation is performed on our new Subglottic Stenosis Dataset of real bronchoscopy procedures data.
  Conclusion: We demonstrate how to automate evaluation of subglottic stenosis severity using only bronchoscopy. Our approach can assist with and shorten diagnosis and monitoring procedures, with automated and repeatable estimations and less exploration time, and save radiation exposure to patients as no CT is required. Additionally, we release the first public benchmark for subglottic stenosis severity assessment.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models</title>
<link>https://arxiv.org/abs/2505.05163</link>
<guid>https://arxiv.org/abs/2505.05163</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, probabilistic embeddings, Gaussian Process, uncertainty calibration, cross-modal retrieval

<br /><br />Summary: Vision-Language Models (VLMs) aim to create joint representations by linking images and text within a shared latent space. However, traditional VLMs often face challenges in capturing the uncertainties related to ambiguities in visual and textual descriptions. These challenges arise from the multiple possible connections between images and texts. Current methods that address this issue typically require extensive datasets and do not fully utilize the robust representations learned by existing large-scale VLMs like CLIP. To overcome these limitations, this paper introduces GroVE, a novel post-hoc method for deriving probabilistic embeddings from fixed VLMs. GroVE employs a Gaussian Process Latent Variable Model (GPLVM) to construct a coherent low-dimensional latent space, effectively mapping image and text inputs into a unified representation. The optimization is based on objectives focused on single-modal embedding reconstruction and cross-modal alignment. After training, the resulting Gaussian Process model is capable of generating uncertainty-aware probabilistic embeddings. Evaluation results demonstrate that GroVE achieves state-of-the-art uncertainty calibration performance across various downstream tasks, such as cross-modal retrieval, visual question answering, and active learning. <div>
arXiv:2505.05163v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) learn joint representations by mapping images and text into a shared latent space. However, recent research highlights that deterministic embeddings from standard VLMs often struggle to capture the uncertainties arising from the ambiguities in visual and textual descriptions and the multiple possible correspondences between images and texts. Existing approaches tackle this by learning probabilistic embeddings during VLM training, which demands large datasets and does not leverage the powerful representations already learned by large-scale VLMs like CLIP. In this paper, we propose GroVE, a post-hoc approach to obtaining probabilistic embeddings from frozen VLMs. GroVE builds on Gaussian Process Latent Variable Model (GPLVM) to learn a shared low-dimensional latent space where image and text inputs are mapped to a unified representation, optimized through single-modal embedding reconstruction and cross-modal alignment objectives. Once trained, the Gaussian Process model generates uncertainty-aware probabilistic embeddings. Evaluation shows that GroVE achieves state-of-the-art uncertainty calibration across multiple downstream tasks, including cross-modal retrieval, visual question answering, and active learning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaniCar: Securing the Perception of Advanced Driving Assistance Systems Against Emergency Vehicle Lighting</title>
<link>https://arxiv.org/abs/2505.05183</link>
<guid>https://arxiv.org/abs/2505.05183</guid>
<content:encoded><![CDATA[
arXiv:2505.05183v1 Announce Type: new 
Abstract: The safety of autonomous cars has come under scrutiny in recent years, especially after 16 documented incidents involving Teslas (with autopilot engaged) crashing into parked emergency vehicles (police cars, ambulances, and firetrucks). While previous studies have revealed that strong light sources often introduce flare artifacts in the captured image, which degrade the image quality, the impact of flare on object detection performance remains unclear. In this research, we unveil PaniCar, a digital phenomenon that causes an object detector's confidence score to fluctuate below detection thresholds when exposed to activated emergency vehicle lighting. This vulnerability poses a significant safety risk, and can cause autonomous vehicles to fail to detect objects near emergency vehicles. In addition, this vulnerability could be exploited by adversaries to compromise the security of advanced driving assistance systems (ADASs). We assess seven commercial ADASs (Tesla Model 3, "manufacturer C", HP, Pelsee, AZDOME, Imagebon, Rexing), four object detectors (YOLO, SSD, RetinaNet, Faster R-CNN), and 14 patterns of emergency vehicle lighting to understand the influence of various technical and environmental factors. We also evaluate four SOTA flare removal methods and show that their performance and latency are insufficient for real-time driving constraints. To mitigate this risk, we propose Caracetamol, a robust framework designed to enhance the resilience of object detectors against the effects of activated emergency vehicle lighting. Our evaluation shows that on YOLOv3 and Faster RCNN, Caracetamol improves the models' average confidence of car detection by 0.20, the lower confidence bound by 0.33, and reduces the fluctuation range by 0.33. In addition, Caracetamol is capable of processing frames at a rate of between 30-50 FPS, enabling real-time ADAS car detection.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.05189</link>
<guid>https://arxiv.org/abs/2505.05189</guid>
<content:encoded><![CDATA[
arXiv:2505.05189v1 Announce Type: new 
Abstract: Prompt learning is one of the most effective paradigms for adapting pre-trained vision-language models (VLMs) to the biomedical image classification tasks in few shot scenarios. However, most of the current prompt learning methods only used the text prompts and ignored the particular structures (such as the complex anatomical structures and subtle pathological features) in the biomedical images. In this work, we propose Biomed-DPT, a knowledge-enhanced dual modality prompt tuning technique. In designing the text prompt, Biomed-DPT constructs a dual prompt including the template-driven clinical prompts and the large language model (LLM)-driven domain-adapted prompts, then extracts the clinical knowledge from the domain-adapted prompts through the knowledge distillation technique. In designing the vision prompt, Biomed-DPT introduces the zero vector as a soft prompt to leverage attention re-weighting so that the focus on non-diagnostic regions and the recognition of non-critical pathological features are avoided. Biomed-DPT achieves an average classification accuracy of 66.14\% across 11 biomedical image datasets covering 9 modalities and 10 organs, with performance reaching 78.06\% in base classes and 75.97\% in novel classes, surpassing the Context Optimization (CoOp) method by 6.20\%, 3.78\%, and 8.04\%, respectively. Our code are available at \underline{https://github.com/Kanyooo/Biomed-DPT}.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAM: Enhancing Anything with Diffusion Transformers for Blind Super-Resolution</title>
<link>https://arxiv.org/abs/2505.05209</link>
<guid>https://arxiv.org/abs/2505.05209</guid>
<content:encoded><![CDATA[
arXiv:2505.05209v1 Announce Type: new 
Abstract: Utilizing pre-trained Text-to-Image (T2I) diffusion models to guide Blind Super-Resolution (BSR) has become a predominant approach in the field. While T2I models have traditionally relied on U-Net architectures, recent advancements have demonstrated that Diffusion Transformers (DiT) achieve significantly higher performance in this domain. In this work, we introduce Enhancing Anything Model (EAM), a novel BSR method that leverages DiT and outperforms previous U-Net-based approaches. We introduce a novel block, $\Psi$-DiT, which effectively guides the DiT to enhance image restoration. This block employs a low-resolution latent as a separable flow injection control, forming a triple-flow architecture that effectively leverages the prior knowledge embedded in the pre-trained DiT. To fully exploit the prior guidance capabilities of T2I models and enhance their generalization in BSR, we introduce a progressive Masked Image Modeling strategy, which also reduces training costs. Additionally, we propose a subject-aware prompt generation strategy that employs a robust multi-modal model in an in-context learning framework. This strategy automatically identifies key image areas, provides detailed descriptions, and optimizes the utilization of T2I diffusion priors. Our experiments demonstrate that EAM achieves state-of-the-art results across multiple datasets, outperforming existing methods in both quantitative metrics and visual quality.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HQC-NBV: A Hybrid Quantum-Classical View Planning Approach</title>
<link>https://arxiv.org/abs/2505.05212</link>
<guid>https://arxiv.org/abs/2505.05212</guid>
<content:encoded><![CDATA[
arXiv:2505.05212v1 Announce Type: new 
Abstract: Efficient view planning is a fundamental challenge in computer vision and robotic perception, critical for tasks ranging from search and rescue operations to autonomous navigation. While classical approaches, including sampling-based and deterministic methods, have shown promise in planning camera viewpoints for scene exploration, they often struggle with computational scalability and solution optimality in complex settings. This study introduces HQC-NBV, a hybrid quantum-classical framework for view planning that leverages quantum properties to efficiently explore the parameter space while maintaining robustness and scalability. We propose a specific Hamiltonian formulation with multi-component cost terms and a parameter-centric variational ansatz with bidirectional alternating entanglement patterns that capture the hierarchical dependencies between viewpoint parameters. Comprehensive experiments demonstrate that quantum-specific components provide measurable performance advantages. Compared to the classical methods, our approach achieves up to 49.2% higher exploration efficiency across diverse environments. Our analysis of entanglement architecture and coherence-preserving terms provides insights into the mechanisms of quantum advantage in robotic exploration tasks. This work represents a significant advancement in integrating quantum computing into robotic perception systems, offering a paradigm-shifting solution for various robot vision tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Model Quantization: A Review</title>
<link>https://arxiv.org/abs/2505.05215</link>
<guid>https://arxiv.org/abs/2505.05215</guid>
<content:encoded><![CDATA[
arXiv:2505.05215v1 Announce Type: new 
Abstract: Recent success of large text-to-image models has empirically underscored the exceptional performance of diffusion models in generative tasks. To facilitate their efficient deployment on resource-constrained edge devices, model quantization has emerged as a pivotal technique for both compression and acceleration. This survey offers a thorough review of the latest advancements in diffusion model quantization, encapsulating and analyzing the current state of the art in this rapidly advancing domain. First, we provide an overview of the key challenges encountered in the quantization of diffusion models, including those based on U-Net architectures and Diffusion Transformers (DiT). We then present a comprehensive taxonomy of prevalent quantization techniques, engaging in an in-depth discussion of their underlying principles. Subsequently, we perform a meticulous analysis of representative diffusion model quantization schemes from both qualitative and quantitative perspectives. From a quantitative standpoint, we rigorously benchmark a variety of methods using widely recognized datasets, delivering an extensive evaluation of the most recent and impactful research in the field. From a qualitative standpoint, we categorize and synthesize the effects of quantization errors, elucidating these impacts through both visual analysis and trajectory examination. In conclusion, we outline prospective avenues for future research, proposing novel directions for the quantization of generative models in practical applications. The list of related papers, corresponding codes, pre-trained models and comparison results are publicly available at the survey project homepage https://github.com/TaylorJocelyn/Diffusion-Model-Quantization.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does CLIP perceive art the same way we do?</title>
<link>https://arxiv.org/abs/2505.05229</link>
<guid>https://arxiv.org/abs/2505.05229</guid>
<content:encoded><![CDATA[
arXiv:2505.05229v1 Announce Type: new 
Abstract: CLIP has emerged as a powerful multimodal model capable of connecting images and text through joint embeddings, but to what extent does it "see" the same way humans do - especially when interpreting artworks? In this paper, we investigate CLIP's ability to extract high-level semantic and stylistic information from paintings, including both human-created and AI-generated imagery. We evaluate its perception across multiple dimensions: content, scene understanding, artistic style, historical period, and the presence of visual deformations or artifacts. By designing targeted probing tasks and comparing CLIP's responses to human annotations and expert benchmarks, we explore its alignment with human perceptual and contextual understanding. Our findings reveal both strengths and limitations in CLIP's visual representations, particularly in relation to aesthetic cues and artistic intent. We further discuss the implications of these insights for using CLIP as a guidance mechanism during generative processes, such as style transfer or prompt-based image synthesis. Our work highlights the need for deeper interpretability in multimodal systems, especially when applied to creative domains where nuance and subjectivity play a central role.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PADriver: Towards Personalized Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.05240</link>
<guid>https://arxiv.org/abs/2505.05240</guid>
<content:encoded><![CDATA[
arXiv:2505.05240v1 Announce Type: new 
Abstract: In this paper, we propose PADriver, a novel closed-loop framework for personalized autonomous driving (PAD). Built upon Multi-modal Large Language Model (MLLM), PADriver takes streaming frames and personalized textual prompts as inputs. It autoaggressively performs scene understanding, danger level estimation and action decision. The predicted danger level reflects the risk of the potential action and provides an explicit reference for the final action, which corresponds to the preset personalized prompt. Moreover, we construct a closed-loop benchmark named PAD-Highway based on Highway-Env simulator to comprehensively evaluate the decision performance under traffic rules. The dataset contains 250 hours videos with high-quality annotation to facilitate the development of PAD behavior analysis. Experimental results on the constructed benchmark show that PADriver outperforms state-of-the-art approaches on different evaluation metrics, and enables various driving modes.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes</title>
<link>https://arxiv.org/abs/2505.05288</link>
<guid>https://arxiv.org/abs/2505.05288</guid>
<content:encoded><![CDATA[
arXiv:2505.05288v1 Announce Type: new 
Abstract: We introduce the novel task of Language-Guided Object Placement in Real 3D Scenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual prompt broadly describing where the 3D asset should be placed. The task here is to find a valid placement for the 3D asset that respects the prompt. Compared with other language-guided localization tasks in 3D scenes such as grounding, this task has specific challenges: it is ambiguous because it has multiple valid solutions, and it requires reasoning about 3D geometric relationships and free space. We inaugurate this task by proposing a new benchmark and evaluation protocol. We also introduce a new dataset for training 3D LLMs on this task, as well as the first method to serve as a non-trivial baseline. We believe that this challenging task and our new benchmark could become part of the suite of benchmarks used to evaluate and compare generalist 3D LLM models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRE-Mamba: A 4D State Space Model for Ultra-High-Frequent Event Camera Deraining</title>
<link>https://arxiv.org/abs/2505.05307</link>
<guid>https://arxiv.org/abs/2505.05307</guid>
<content:encoded><![CDATA[
arXiv:2505.05307v1 Announce Type: new 
Abstract: Event cameras excel in high temporal resolution and dynamic range but suffer from dense noise in rainy conditions. Existing event deraining methods face trade-offs between temporal precision, deraining effectiveness, and computational efficiency. In this paper, we propose PRE-Mamba, a novel point-based event camera deraining framework that fully exploits the spatiotemporal characteristics of raw event and rain. Our framework introduces a 4D event cloud representation that integrates dual temporal scales to preserve high temporal precision, a Spatio-Temporal Decoupling and Fusion module (STDF) that enhances deraining capability by enabling shallow decoupling and interaction of temporal and spatial information, and a Multi-Scale State Space Model (MS3M) that captures deeper rain dynamics across dual-temporal and multi-spatial scales with linear computational complexity. Enhanced by frequency-domain regularization, PRE-Mamba achieves superior performance (0.95 SR, 0.91 NR, and 0.4s/M events) with only 0.26M parameters on EventRain-27K, a comprehensive dataset with labeled synthetic and real-world sequences. Moreover, our method generalizes well across varying rain intensities, viewpoints, and even snowy conditions.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects</title>
<link>https://arxiv.org/abs/2505.05318</link>
<guid>https://arxiv.org/abs/2505.05318</guid>
<content:encoded><![CDATA[
arXiv:2505.05318v1 Announce Type: new 
Abstract: The rapid adoption of Vision Language Models (VLMs), pre-trained on large image-text and video-text datasets, calls for protecting and informing users about when to trust these systems. This survey reviews studies on trust dynamics in user-VLM interactions, through a multi-disciplinary taxonomy encompassing different cognitive science capabilities, collaboration modes, and agent behaviours. Literature insights and findings from a workshop with prospective VLM users inform preliminary requirements for future VLM trust studies.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-Augmented Deep Networks for Multiscale Building Segmentation in High-Resolution UAV and Satellite Imagery</title>
<link>https://arxiv.org/abs/2505.05321</link>
<guid>https://arxiv.org/abs/2505.05321</guid>
<content:encoded><![CDATA[
arXiv:2505.05321v1 Announce Type: new 
Abstract: Accurate building segmentation from high-resolution RGB imagery remains challenging due to spectral similarity with non-building features, shadows, and irregular building geometries. In this study, we present a comprehensive deep learning framework for multiscale building segmentation using RGB aerial and satellite imagery with spatial resolutions ranging from 0.4m to 2.7m. We curate a diverse, multi-sensor dataset and introduce feature-augmented inputs by deriving secondary representations including Principal Component Analysis (PCA), Visible Difference Vegetation Index (VDVI), Morphological Building Index (MBI), and Sobel edge filters from RGB channels. These features guide a Res-U-Net architecture in learning complex spatial patterns more effectively. We also propose training policies incorporating layer freezing, cyclical learning rates, and SuperConvergence to reduce training time and resource usage. Evaluated on a held-out WorldView-3 image, our model achieves an overall accuracy of 96.5%, an F1-score of 0.86, and an Intersection over Union (IoU) of 0.80, outperforming existing RGB-based benchmarks. This study demonstrates the effectiveness of combining multi-resolution imagery, feature augmentation, and optimized training strategies for robust building segmentation in remote sensing applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aesthetics Without Semantics</title>
<link>https://arxiv.org/abs/2505.05331</link>
<guid>https://arxiv.org/abs/2505.05331</guid>
<content:encoded><![CDATA[
arXiv:2505.05331v1 Announce Type: new 
Abstract: While it is easy for human observers to judge an image as beautiful or ugly, aesthetic decisions result from a combination of entangled perceptual and cognitive (semantic) factors, making the understanding of aesthetic judgements particularly challenging from a scientific point of view. Furthermore, our research shows a prevailing bias in current databases, which include mostly beautiful images, further complicating the study and prediction of aesthetic responses. We address these limitations by creating a database of images with minimal semantic content and devising, and next exploiting, a method to generate images on the ugly side of aesthetic valuations. The resulting Minimum Semantic Content (MSC) database consists of a large and balanced collection of 10,426 images, each evaluated by 100 observers. We next use established image metrics to demonstrate how augmenting an image set biased towards beautiful images with ugly images can modify, or even invert, an observed relationship between image features and aesthetics valuation. Taken together, our study reveals that works in empirical aesthetics attempting to link image content and aesthetic judgements may magnify, underestimate, or simply miss interesting effects due to a limitation of the range of aesthetic values they consider.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Inertial Poser: Progressive Real-Time Kinematic Chain Estimation for 3D Full-Body Pose from Three IMU Sensors</title>
<link>https://arxiv.org/abs/2505.05336</link>
<guid>https://arxiv.org/abs/2505.05336</guid>
<content:encoded><![CDATA[
arXiv:2505.05336v1 Announce Type: new 
Abstract: The motion capture system that supports full-body virtual representation is of key significance for virtual reality. Compared to vision-based systems, full-body pose estimation from sparse tracking signals is not limited by environmental conditions or recording range. However, previous works either face the challenge of wearing additional sensors on the pelvis and lower-body or rely on external visual sensors to obtain global positions of key joints. To improve the practicality of the technology for virtual reality applications, we estimate full-body poses using only inertial data obtained from three Inertial Measurement Unit (IMU) sensors worn on the head and wrists, thereby reducing the complexity of the hardware system. In this work, we propose a method called Progressive Inertial Poser (ProgIP) for human pose estimation, which combines neural network estimation with a human dynamics model, considers the hierarchical structure of the kinematic chain, and employs a multi-stage progressive network estimation with increased depth to reconstruct full-body motion in real time. The encoder combines Transformer Encoder and bidirectional LSTM (TE-biLSTM) to flexibly capture the temporal dependencies of the inertial sequence, while the decoder based on multi-layer perceptrons (MLPs) transforms high-dimensional features and accurately projects them onto Skinned Multi-Person Linear (SMPL) model parameters. Quantitative and qualitative experimental results on multiple public datasets show that our method outperforms state-of-the-art methods with the same inputs, and is comparable to recent works using six IMU sensors.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization</title>
<link>https://arxiv.org/abs/2505.05343</link>
<guid>https://arxiv.org/abs/2505.05343</guid>
<content:encoded><![CDATA[
arXiv:2505.05343v1 Announce Type: new 
Abstract: Large-scale vision-language models demonstrate strong multimodal alignment and generalization across diverse tasks. Among them, CLIP stands out as one of the most successful approaches. In this work, we extend the application of CLIP to sound source localization, proposing a self-supervised method operates without explicit text input. We introduce a framework that maps audios into tokens compatible with CLIP's text encoder, producing audio-driven embeddings. These embeddings are used to generate sounding region masks, from which visual features are extracted and aligned with the audio embeddings through a contrastive audio-visual correspondence objective. Our findings show that alignment knowledge of pre-trained multimodal foundation model enables our method to generate more complete and compact localization for sounding objects. We further propose an LLM-guided extension that distills object-aware audio-visual scene understanding into the model during training to enhance alignment. Extensive experiments across five diverse tasks demonstrate that our method, in all variants, outperforms state-of-the-art approaches and achieves strong generalization in zero-shot settings.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Super-Resolution and Segmentation for 1-m Impervious Surface Area Mapping in China's Yangtze River Economic Belt</title>
<link>https://arxiv.org/abs/2505.05367</link>
<guid>https://arxiv.org/abs/2505.05367</guid>
<content:encoded><![CDATA[
arXiv:2505.05367v1 Announce Type: new 
Abstract: We propose a novel joint framework by integrating super-resolution and segmentation, called JointSeg, which enables the generation of 1-meter ISA maps directly from freely available Sentinel-2 imagery. JointSeg was trained on multimodal cross-resolution inputs, offering a scalable and affordable alternative to traditional approaches. This synergistic design enables gradual resolution enhancement from 10m to 1m while preserving fine-grained spatial textures, and ensures high classification fidelity through effective cross-scale feature fusion. This method has been successfully applied to the Yangtze River Economic Belt (YREB), a region characterized by complex urban-rural patterns and diverse topography. As a result, a comprehensive ISA mapping product for 2021, referred to as ISA-1, was generated, covering an area of over 2.2 million square kilometers. Quantitative comparisons against the 10m ESA WorldCover and other benchmark products reveal that ISA-1 achieves an F1-score of 85.71%, outperforming bilinear-interpolation-based segmentation by 9.5%, and surpassing other ISA datasets by 21.43%-61.07%. In densely urbanized areas (e.g., Suzhou, Nanjing), ISA-1 reduces ISA overestimation through improved discrimination of green spaces and water bodies. Conversely, in mountainous regions (e.g., Ganzi, Zhaotong), it identifies significantly more ISA due to its enhanced ability to detect fragmented anthropogenic features such as rural roads and sparse settlements, demonstrating its robustness across diverse landscapes. Moreover, we present biennial ISA maps from 2017 to 2023, capturing spatiotemporal urbanization dynamics across representative cities. The results highlight distinct regional growth patterns: rapid expansion in upstream cities, moderate growth in midstream regions, and saturation in downstream metropolitan areas.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Threshold Modulation for Online Test-Time Adaptation of Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2505.05375</link>
<guid>https://arxiv.org/abs/2505.05375</guid>
<content:encoded><![CDATA[
arXiv:2505.05375v1 Announce Type: new 
Abstract: Recently, spiking neural networks (SNNs), deployed on neuromorphic chips, provide highly efficient solutions on edge devices in different scenarios. However, their ability to adapt to distribution shifts after deployment has become a crucial challenge. Online test-time adaptation (OTTA) offers a promising solution by enabling models to dynamically adjust to new data distributions without requiring source data or labeled target samples. Nevertheless, existing OTTA methods are largely designed for traditional artificial neural networks and are not well-suited for SNNs. To address this gap, we propose a low-power, neuromorphic chip-friendly online test-time adaptation framework, aiming to enhance model generalization under distribution shifts. The proposed approach is called Threshold Modulation (TM), which dynamically adjusts the firing threshold through neuronal dynamics-inspired normalization, being more compatible with neuromorphic hardware. Experimental results on benchmark datasets demonstrate the effectiveness of this method in improving the robustness of SNNs against distribution shifts while maintaining low computational cost. The proposed method offers a practical solution for online test-time adaptation of SNNs, providing inspiration for the design of future neuromorphic chips. The demo code is available at github.com/NneurotransmitterR/TM-OTTA-SNN.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans</title>
<link>https://arxiv.org/abs/2505.05376</link>
<guid>https://arxiv.org/abs/2505.05376</guid>
<content:encoded><![CDATA[
arXiv:2505.05376v1 Announce Type: new 
Abstract: We propose a novel method that reconstructs hair strands directly from colorless 3D scans by leveraging multi-modal hair orientation extraction. Hair strand reconstruction is a fundamental problem in computer vision and graphics that can be used for high-fidelity digital avatar synthesis, animation, and AR/VR applications. However, accurately recovering hair strands from raw scan data remains challenging due to human hair's complex and fine-grained structure. Existing methods typically rely on RGB captures, which can be sensitive to the environment and can be a challenging domain for extracting the orientation of guiding strands, especially in the case of challenging hairstyles. To reconstruct the hair purely from the observed geometry, our method finds sharp surface features directly on the scan and estimates strand orientation through a neural 2D line detector applied to the renderings of scan shading. Additionally, we incorporate a diffusion prior trained on a diverse set of synthetic hair scans, refined with an improved noise schedule, and adapted to the reconstructed contents via a scan-specific text prompt. We demonstrate that this combination of supervision signals enables accurate reconstruction of both simple and intricate hairstyles without relying on color information. To facilitate further research, we introduce Strands400, the largest publicly available dataset of hair strands with detailed surface geometry extracted from real-world data, which contains reconstructed hair strands from the scans of 400 subjects.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EDmamba: A Simple yet Effective Event Denoising Method with State Space Model</title>
<link>https://arxiv.org/abs/2505.05391</link>
<guid>https://arxiv.org/abs/2505.05391</guid>
<content:encoded><![CDATA[
arXiv:2505.05391v1 Announce Type: new 
Abstract: Event cameras excel in high-speed vision due to their high temporal resolution, high dynamic range, and low power consumption. However, as dynamic vision sensors, their output is inherently noisy, making efficient denoising essential to preserve their ultra-low latency and real-time processing capabilities. Existing event denoising methods struggle with a critical dilemma: computationally intensive approaches compromise the sensor's high-speed advantage, while lightweight methods often lack robustness across varying noise levels. To address this, we propose a novel event denoising framework based on State Space Models (SSMs). Our approach represents events as 4D event clouds and includes a Coarse Feature Extraction (CFE) module that extracts embedding features from both geometric and polarity-aware subspaces. The model is further composed of two essential components: A Spatial Mamba (S-SSM) that models local geometric structures and a Temporal Mamba (T-SSM) that captures global temporal dynamics, efficiently propagating spatiotemporal features across events. Experiments demonstrate that our method achieves state-of-the-art accuracy and efficiency, with 88.89K parameters, 0.0685s per 100K events inference time, and a 0.982 accuracy score, outperforming Transformer-based methods by 2.08% in denoising accuracy and 36X faster.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PillarMamba: Learning Local-Global Context for Roadside Point Cloud via Hybrid State Space Model</title>
<link>https://arxiv.org/abs/2505.05397</link>
<guid>https://arxiv.org/abs/2505.05397</guid>
<content:encoded><![CDATA[
arXiv:2505.05397v1 Announce Type: new 
Abstract: Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything (V2X) tasks, roadside perception has received increasing attention in recent years, as it can extend the perception range of connected vehicles and improve traffic safety. However, roadside point cloud oriented 3D object detection has not been effectively explored. To some extent, the key to the performance of a point cloud detector lies in the receptive field of the network and the ability to effectively utilize the scene context. The recent emergence of Mamba, based on State Space Model (SSM), has shaken up the traditional convolution and transformers that have long been the foundational building blocks, due to its efficient global receptive field. In this work, we introduce Mamba to pillar-based roadside point cloud perception and propose a framework based on Cross-stage State-space Group (CSG), called PillarMamba. It enhances the expressiveness of the network and achieves efficient computation through cross-stage feature fusion. However, due to the limitations of scan directions, state space model faces local connection disrupted and historical relationship forgotten. To address this, we propose the Hybrid State-space Block (HSB) to obtain the local-global context of roadside point cloud. Specifically, it enhances neighborhood connections through local convolution and preserves historical memory through residual attention. The proposed method outperforms the state-of-the-art methods on the popular large scale roadside benchmark: DAIR-V2X-I. The code will be released soon.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation</title>
<link>https://arxiv.org/abs/2505.05422</link>
<guid>https://arxiv.org/abs/2505.05422</guid>
<content:encoded><![CDATA[
arXiv:2505.05422v1 Announce Type: new 
Abstract: Pioneering token-based works such as Chameleon and Emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (VQ) tokens and incorporating CLIP-level semantics while enabling end-to-end multimodal autoregressive training with standard VQ tokens. TokLIP integrates a low-level discrete VQ tokenizer with a ViT-based token encoder to capture high-level continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize high-level features, TokLIP disentangles training objectives for comprehension and generation, allowing the direct application of advanced VQ tokenizers without the need for tailored quantization operations. Our empirical results demonstrate that TokLIP achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive Transformers in both comprehension and generation tasks. The code and models are available at https://github.com/TencentARC/TokLIP.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding</title>
<link>https://arxiv.org/abs/2505.05446</link>
<guid>https://arxiv.org/abs/2505.05446</guid>
<content:encoded><![CDATA[
arXiv:2505.05446v1 Announce Type: new 
Abstract: Visual Document Understanding has become essential with the increase of text-rich visual content. This field poses significant challenges due to the need for effective integration of visual perception and textual comprehension, particularly across diverse document types with complex layouts. Moreover, existing fine-tuning datasets for this domain often fall short in providing the detailed contextual information for robust understanding, leading to hallucinations and limited comprehension of spatial relationships among visual elements. To address these challenges, we propose an innovative pipeline that utilizes adaptive generation of markup languages, such as Markdown, JSON, HTML, and TiKZ, to build highly structured document representations and deliver contextually-grounded responses. We introduce two fine-grained structured datasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs for document parsing, and DocMark-Instruct, featuring 624k fine-tuning data annotations for grounded instruction following. Extensive experiments demonstrate that our proposed model significantly outperforms existing state-of-theart MLLMs across a range of visual document understanding benchmarks, facilitating advanced reasoning and comprehension capabilities in complex visual scenarios. Our code and models are released at https://github. com/Euphoria16/DocMark.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SITE: towards Spatial Intelligence Thorough Evaluation</title>
<link>https://arxiv.org/abs/2505.05456</link>
<guid>https://arxiv.org/abs/2505.05456</guid>
<content:encoded><![CDATA[
arXiv:2505.05456v1 Announce Type: new 
Abstract: Spatial intelligence (SI) represents a cognitive ability encompassing the visualization, manipulation, and reasoning about spatial relationships, underpinning disciplines from neuroscience to robotics. We introduce SITE, a benchmark dataset towards SI Thorough Evaluation in a standardized format of multi-choice visual question-answering, designed to assess large vision-language models' spatial intelligence across diverse visual modalities (single-image, multi-image, and video) and SI factors (figural to environmental scales, spatial visualization and orientation, intrinsic and extrinsic, static and dynamic). Our approach to curating the benchmark combines a bottom-up survey about 31 existing datasets and a top-down strategy drawing upon three classification systems in cognitive science, which prompt us to design two novel types of tasks about view-taking and dynamic scenes. Extensive experiments reveal that leading models fall behind human experts especially in spatial orientation, a fundamental SI factor. Moreover, we demonstrate a positive correlation between a model's spatial reasoning proficiency and its performance on an embodied AI task.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant</title>
<link>https://arxiv.org/abs/2505.05467</link>
<guid>https://arxiv.org/abs/2505.05467</guid>
<content:encoded><![CDATA[
arXiv:2505.05467v1 Announce Type: new 
Abstract: We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and (2) a decoupled, lightweight activation model that can be effortlessly integrated into existing Video-LLMs, enabling continuous proactive responses. To further support StreamBridge, we construct Stream-IT, a large-scale dataset tailored for streaming video understanding, featuring interleaved video-text sequences and diverse instruction formats. Extensive experiments show that StreamBridge significantly improves the streaming understanding capabilities of offline Video-LLMs across various tasks, outperforming even proprietary models such as GPT-4o and Gemini 1.5 Pro. Simultaneously, it achieves competitive or superior performance on standard video understanding benchmarks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Physically Stable and Buildable LEGO Designs from Text</title>
<link>https://arxiv.org/abs/2505.05469</link>
<guid>https://arxiv.org/abs/2505.05469</guid>
<content:encoded><![CDATA[
arXiv:2505.05469v1 Announce Type: new 
Abstract: We introduce LegoGPT, the first approach for generating physically stable LEGO brick models from text prompts. To achieve this, we construct a large-scale, physically stable dataset of LEGO designs, along with their associated captions, and train an autoregressive large language model to predict the next brick to add via next-token prediction. To improve the stability of the resulting designs, we employ an efficient validity check and physics-aware rollback during autoregressive inference, which prunes infeasible token predictions using physics laws and assembly constraints. Our experiments show that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO designs that align closely with the input text prompts. We also develop a text-based LEGO texturing method to generate colored and textured designs. We show that our designs can be assembled manually by humans and automatically by robotic arms. We also release our new dataset, StableText2Lego, containing over 47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed captions, along with our code and models at the project website: https://avalovelace1.github.io/LegoGPT/.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow-GRPO: Training Flow Matching Models via Online RL</title>
<link>https://arxiv.org/abs/2505.05470</link>
<guid>https://arxiv.org/abs/2505.05470</guid>
<content:encoded><![CDATA[
arXiv:2505.05470v1 Announce Type: new 
Abstract: We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from $63\%$ to $95\%$. In visual text rendering, its accuracy improves from $59\%$ to $92\%$, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, little to no reward hacking occurred, meaning rewards did not increase at the cost of image quality or diversity, and both remained stable in our experiments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation</title>
<link>https://arxiv.org/abs/2505.05472</link>
<guid>https://arxiv.org/abs/2505.05472</guid>
<content:encoded><![CDATA[
arXiv:2505.05472v1 Announce Type: new 
Abstract: Recent progress in unified models for image understanding and generation has been impressive, yet most approaches remain limited to single-modal generation conditioned on multiple modalities. In this paper, we present Mogao, a unified framework that advances this paradigm by enabling interleaved multi-modal generation through a causal approach. Mogao integrates a set of key technical improvements in architecture design, including a deep-fusion design, dual vision encoders, interleaved rotary position embeddings, and multi-modal classifier-free guidance, which allow it to harness the strengths of both autoregressive models for text generation and diffusion models for high-quality image synthesis. These practical improvements also make Mogao particularly effective to process interleaved sequences of text and images arbitrarily. To further unlock the potential of unified models, we introduce an efficient training strategy on a large-scale, in-house dataset specifically curated for joint text and image generation. Extensive experiments show that Mogao not only achieves state-of-the-art performance in multi-modal understanding and text-to-image generation, but also excels in producing high-quality, coherent interleaved outputs. Its emergent capabilities in zero-shot image editing and compositional generation highlight Mogao as a practical omni-modal foundation model, paving the way for future development and scaling the unified multi-modal systems.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion</title>
<link>https://arxiv.org/abs/2505.05473</link>
<guid>https://arxiv.org/abs/2505.05473</guid>
<content:encoded><![CDATA[
arXiv:2505.05473v1 Announce Type: new 
Abstract: Current Structure-from-Motion (SfM) methods typically follow a two-stage pipeline, combining learned or geometric pairwise reasoning with a subsequent global optimization step. In contrast, we propose a data-driven multi-view reasoning approach that directly infers 3D scene geometry and camera poses from multi-view images. Our framework, DiffusionSfM, parameterizes scene geometry and cameras as pixel-wise ray origins and endpoints in a global frame and employs a transformer-based denoising diffusion model to predict them from multi-view inputs. To address practical challenges in training diffusion models with missing data and unbounded scene coordinates, we introduce specialized mechanisms that ensure robust learning. We empirically validate DiffusionSfM on both synthetic and real datasets, demonstrating that it outperforms classical and learning-based approaches while naturally modeling uncertainty.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Scene Generation: A Survey</title>
<link>https://arxiv.org/abs/2505.05474</link>
<guid>https://arxiv.org/abs/2505.05474</guid>
<content:encoded><![CDATA[
arXiv:2505.05474v1 Announce Type: new 
Abstract: 3D scene generation seeks to synthesize spatially structured, semantically meaningful, and photorealistic environments for applications such as immersive media, robotics, autonomous driving, and embodied AI. Early methods based on procedural rules offered scalability but limited diversity. Recent advances in deep generative models (e.g., GANs, diffusion models) and 3D representations (e.g., NeRF, 3D Gaussians) have enabled the learning of real-world scene distributions, improving fidelity, diversity, and view consistency. Recent advances like diffusion models bridge 3D scene synthesis and photorealism by reframing generation as image or video synthesis problems. This survey provides a systematic overview of state-of-the-art approaches, organizing them into four paradigms: procedural generation, neural 3D-based generation, image-based generation, and video-based generation. We analyze their technical foundations, trade-offs, and representative results, and review commonly used datasets, evaluation protocols, and downstream applications. We conclude by discussing key challenges in generation capacity, 3D representation, data and annotations, and evaluation, and outline promising directions including higher fidelity, physics-aware and interactive generation, and unified perception-generation models. This review organizes recent advances in 3D scene generation and highlights promising directions at the intersection of generative AI, 3D vision, and embodied intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/hzxie/Awesome-3D-Scene-Generation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation</title>
<link>https://arxiv.org/abs/2505.05475</link>
<guid>https://arxiv.org/abs/2505.05475</guid>
<content:encoded><![CDATA[
arXiv:2505.05475v1 Announce Type: new 
Abstract: Creating high-quality animatable 3D human avatars from a single image remains a significant challenge in computer vision due to the inherent difficulty of reconstructing complete 3D information from a single viewpoint. Current approaches face a clear limitation: 3D Gaussian Splatting (3DGS) methods produce high-quality results but require multiple views or video sequences, while video diffusion models can generate animations from single images but struggle with consistency and identity preservation. We present SVAD, a novel approach that addresses these limitations by leveraging complementary strengths of existing techniques. Our method generates synthetic training data through video diffusion, enhances it with identity preservation and image restoration modules, and utilizes this refined data to train 3DGS avatars. Comprehensive evaluations demonstrate that SVAD outperforms state-of-the-art (SOTA) single-image methods in maintaining identity consistency and fine details across novel poses and viewpoints, while enabling real-time rendering capabilities. Through our data augmentation pipeline, we overcome the dependency on dense monocular or multi-view training data typically required by traditional 3DGS approaches. Extensive quantitative, qualitative comparisons show our method achieves superior performance across multiple metrics against baseline models. By effectively combining the generative power of diffusion models with both the high-quality results and rendering efficiency of 3DGS, our work establishes a new approach for high-fidelity avatar generation from a single image input.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChannelExplorer: Exploring Class Separability Through Activation Channel Visualization</title>
<link>https://arxiv.org/abs/2505.04647</link>
<guid>https://arxiv.org/abs/2505.04647</guid>
<content:encoded><![CDATA[
arXiv:2505.04647v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) achieve state-of-the-art performance in many vision tasks, yet understanding their internal behavior remains challenging, particularly how different layers and activation channels contribute to class separability. We introduce ChannelExplorer, an interactive visual analytics tool for analyzing image-based outputs across model layers, emphasizing data-driven insights over architecture analysis for exploring class separability. ChannelExplorer summarizes activations across layers and visualizes them using three primary coordinated views: a Scatterplot View to reveal inter- and intra-class confusion, a Jaccard Similarity View to quantify activation overlap, and a Heatmap View to inspect activation channel patterns. Our technique supports diverse model architectures, including CNNs, GANs, ResNet and Stable Diffusion models. We demonstrate the capabilities of ChannelExplorer through four use-case scenarios: (1) generating class hierarchy in ImageNet, (2) finding mislabeled images, (3) identifying activation channel contributions, and(4) locating latent states' position in Stable Diffusion model. Finally, we evaluate the tool with expert users.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Boundary Detection in Deep Learning-Based Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.04652</link>
<guid>https://arxiv.org/abs/2505.04652</guid>
<content:encoded><![CDATA[
arXiv:2505.04652v1 Announce Type: cross 
Abstract: Medical image segmentation is a pivotal task within the realms of medical image analysis and computer vision. While current methods have shown promise in accurately segmenting major regions of interest, the precise segmentation of boundary areas remains challenging. In this study, we propose a novel network architecture named CTO, which combines Convolutional Neural Networks (CNNs), Vision Transformer (ViT) models, and explicit edge detection operators to tackle this challenge. CTO surpasses existing methods in terms of segmentation accuracy and strikes a better balance between accuracy and efficiency, without the need for additional data inputs or label injections. Specifically, CTO adheres to the canonical encoder-decoder network paradigm, with a dual-stream encoder network comprising a mainstream CNN stream for capturing local features and an auxiliary StitchViT stream for integrating long-range dependencies. Furthermore, to enhance the model's ability to learn boundary areas, we introduce a boundary-guided decoder network that employs binary boundary masks generated by dedicated edge detection operators to provide explicit guidance during the decoding process. We validate the performance of CTO through extensive experiments conducted on seven challenging medical image segmentation datasets, namely ISIC 2016, PH2, ISIC 2018, CoNIC, LiTS17, and BTCV. Our experimental results unequivocally demonstrate that CTO achieves state-of-the-art accuracy on these datasets while maintaining competitive model complexity. The codes have been released at: https://github.com/xiaofang007/CTO.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Conversational Diagnostic AI with Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.04653</link>
<guid>https://arxiv.org/abs/2505.04653</guid>
<content:encoded><![CDATA[
arXiv:2505.04653v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated great potential for conducting diagnostic conversations but evaluation has been largely limited to language-only interactions, deviating from the real-world requirements of remote care delivery. Instant messaging platforms permit clinicians and patients to upload and discuss multimodal medical artifacts seamlessly in medical consultation, but the ability of LLMs to reason over such data while preserving other attributes of competent diagnostic conversation remains unknown. Here we advance the conversational diagnosis and management performance of the Articulate Medical Intelligence Explorer (AMIE) through a new capability to gather and interpret multimodal data, and reason about this precisely during consultations. Leveraging Gemini 2.0 Flash, our system implements a state-aware dialogue framework, where conversation flow is dynamically controlled by intermediate model outputs reflecting patient states and evolving diagnoses. Follow-up questions are strategically directed by uncertainty in such patient states, leading to a more structured multimodal history-taking process that emulates experienced clinicians. We compared AMIE to primary care physicians (PCPs) in a randomized, blinded, OSCE-style study of chat-based consultations with patient actors. We constructed 105 evaluation scenarios using artifacts like smartphone skin photos, ECGs, and PDFs of clinical documents across diverse conditions and demographics. Our rubric assessed multimodal capabilities and other clinically meaningful axes like history-taking, diagnostic accuracy, management reasoning, communication, and empathy. Specialist evaluation showed AMIE to be superior to PCPs on 7/9 multimodal and 29/32 non-multimodal axes (including diagnostic accuracy). The results show clear progress in multimodal conversational diagnostic AI, but real-world translation needs further research.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection</title>
<link>https://arxiv.org/abs/2505.04660</link>
<guid>https://arxiv.org/abs/2505.04660</guid>
<content:encoded><![CDATA[
arXiv:2505.04660v1 Announce Type: cross 
Abstract: Training fall detection systems is challenging due to the scarcity of real-world fall data, particularly from elderly individuals. To address this, we explore the potential of Large Language Models (LLMs) for generating synthetic fall data. This study evaluates text-to-motion (T2M, SATO, ParCo) and text-to-text models (GPT4o, GPT4, Gemini) in simulating realistic fall scenarios. We generate synthetic datasets and integrate them with four real-world baseline datasets to assess their impact on fall detection performance using a Long Short-Term Memory (LSTM) model. Additionally, we compare LLM-generated synthetic data with a diffusion-based method to evaluate their alignment with real accelerometer distributions. Results indicate that dataset characteristics significantly influence the effectiveness of synthetic data, with LLM-generated data performing best in low-frequency settings (e.g., 20Hz) while showing instability in high-frequency datasets (e.g., 200Hz). While text-to-motion models produce more realistic biomechanical data than text-to-text models, their impact on fall detection varies. Diffusion-based synthetic data demonstrates the closest alignment to real data but does not consistently enhance model performance. An ablation study further confirms that the effectiveness of synthetic data depends on sensor placement and fall representation. These findings provide insights into optimizing synthetic data generation for fall detection models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing 3D Medical Image Segmentation: Unleashing the Potential of Planarian Neural Networks in Artificial Intelligence</title>
<link>https://arxiv.org/abs/2505.04664</link>
<guid>https://arxiv.org/abs/2505.04664</guid>
<content:encoded><![CDATA[
arXiv:2505.04664v1 Announce Type: cross 
Abstract: Our study presents PNN-UNet as a method for constructing deep neural networks that replicate the planarian neural network (PNN) structure in the context of 3D medical image data. Planarians typically have a cerebral structure comprising two neural cords, where the cerebrum acts as a coordinator, and the neural cords serve slightly different purposes within the organism's neurological system. Accordingly, PNN-UNet comprises a Deep-UNet and a Wide-UNet as the nerve cords, with a densely connected autoencoder performing the role of the brain. This distinct architecture offers advantages over both monolithic (UNet) and modular networks (Ensemble-UNet). Our outcomes on a 3D MRI hippocampus dataset, with and without data augmentation, demonstrate that PNN-UNet outperforms the baseline UNet and several other UNet variants in image segmentation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WIR3D: Visually-Informed and Geometry-Aware 3D Shape Abstraction</title>
<link>https://arxiv.org/abs/2505.04813</link>
<guid>https://arxiv.org/abs/2505.04813</guid>
<content:encoded><![CDATA[
arXiv:2505.04813v1 Announce Type: cross 
Abstract: We present WIR3D, a technique for abstracting 3D shapes through a sparse set of visually meaningful curves in 3D. We optimize the parameters of Bezier curves such that they faithfully represent both the geometry and salient visual features (e.g. texture) of the shape from arbitrary viewpoints. We leverage the intermediate activations of a pre-trained foundation model (CLIP) to guide our optimization process. We divide our optimization into two phases: one for capturing the coarse geometry of the shape, and the other for representing fine-grained features. Our second phase supervision is spatially guided by a novel localized keypoint loss. This spatial guidance enables user control over abstracted features. We ensure fidelity to the original surface through a neural SDF loss, which allows the curves to be used as intuitive deformation handles. We successfully apply our method for shape abstraction over a broad dataset of shapes with varying complexity, geometric structure, and texture, and demonstrate downstream applications for feature control and shape deformation.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrated Image Reconstruction and Target Recognition based on Deep Learning Technique</title>
<link>https://arxiv.org/abs/2505.04836</link>
<guid>https://arxiv.org/abs/2505.04836</guid>
<content:encoded><![CDATA[
arXiv:2505.04836v1 Announce Type: cross 
Abstract: Computational microwave imaging (CMI) has gained attention as an alternative technique for conventional microwave imaging techniques, addressing their limitations such as hardware-intensive physical layer and slow data collection acquisition speed to name a few. Despite these advantages, CMI still encounters notable computational bottlenecks, especially during the image reconstruction stage. In this setting, both image recovery and object classification present significant processing demands. To address these challenges, our previous work introduced ClassiGAN, which is a generative deep learning model designed to simultaneously reconstruct images and classify targets using only back-scattered signals. In this study, we build upon that framework by incorporating attention gate modules into ClassiGAN. These modules are intended to refine feature extraction and improve the identification of relevant information. By dynamically focusing on important features and suppressing irrelevant ones, the attention mechanism enhances the overall model performance. The proposed architecture, named Att-ClassiGAN, significantly reduces the reconstruction time compared to traditional CMI approaches. Furthermore, it outperforms current advanced methods, delivering improved Normalized Mean Squared Error (NMSE), higher Structural Similarity Index (SSIM), and better classification outcomes for the reconstructed targets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.04851</link>
<guid>https://arxiv.org/abs/2505.04851</guid>
<content:encoded><![CDATA[
arXiv:2505.04851v1 Announce Type: cross 
Abstract: Despite the fact that popular text-to-image generation models cope well with international and general cultural queries, they have a significant knowledge gap regarding individual cultures. This is due to the content of existing large training datasets collected on the Internet, which are predominantly based on Western European or American popular culture. Meanwhile, the lack of cultural adaptation of the model can lead to incorrect results, a decrease in the generation quality, and the spread of stereotypes and offensive content. In an effort to address this issue, we examine the concept of cultural code and recognize the critical importance of its understanding by modern image generation models, an issue that has not been sufficiently addressed in the research community to date. We propose the methodology for collecting and processing the data necessary to form a dataset based on the cultural code, in particular the Russian one. We explore how the collected data affects the quality of generations in the national domain and analyze the effectiveness of our approach using the Kandinsky 3.1 text-to-image model. Human evaluation results demonstrate an increase in the level of awareness of Russian culture in the model.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation</title>
<link>https://arxiv.org/abs/2505.04860</link>
<guid>https://arxiv.org/abs/2505.04860</guid>
<content:encoded><![CDATA[
arXiv:2505.04860v1 Announce Type: cross 
Abstract: Learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. Eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. However, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. While prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. In this work, we propose Diffusion for COordinated Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. It employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across 2250 simulation trials and 300 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. Our project website is at: https://dcodaaug.github.io/D-CODA/.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced 3D Imaging Approach to TSV/TGV Metrology and Inspection Using Only Optical Microscopy</title>
<link>https://arxiv.org/abs/2505.04913</link>
<guid>https://arxiv.org/abs/2505.04913</guid>
<content:encoded><![CDATA[
arXiv:2505.04913v1 Announce Type: cross 
Abstract: This paper introduces an innovative approach to silicon and glass via inspection, which combines hybrid field microscopy with photometric stereo. Conventional optical microscopy techniques are generally limited to superficial inspections and struggle to effectively visualize the internal structures of silicon and glass vias. By utilizing various lighting conditions for 3D reconstruction, the proposed method surpasses these limitations. By integrating photometric stereo to the traditional optical microscopy, the proposed method not only enhances the capability to detect micro-scale defects but also provides a detailed visualization of depth and edge abnormality, which are typically not visible with conventional optical microscopy inspection. The experimental results demonstrated that the proposed method effectively captures intricate surface details and internal structures. Quantitative comparisons between the reconstructed models and actual measurements present the capability of the proposed method to significantly improve silicon and glass via inspection process. As a result, the proposed method achieves enhanced cost-effectiveness while maintaining high accuracy and repeatability, suggesting substantial advancements in silicon and glass via inspection techniques
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRe-3DGSMR: Motion-resolved reconstruction framework for free-breathing pulmonary MRI based on 3D Gaussian representation</title>
<link>https://arxiv.org/abs/2505.04959</link>
<guid>https://arxiv.org/abs/2505.04959</guid>
<content:encoded><![CDATA[
arXiv:2505.04959v1 Announce Type: cross 
Abstract: This study presents an unsupervised, motion-resolved reconstruction framework for high-resolution, free-breathing pulmonary magnetic resonance imaging (MRI), utilizing a three-dimensional Gaussian representation (3DGS). The proposed method leverages 3DGS to address the challenges of motion-resolved 3D isotropic pulmonary MRI reconstruction by enabling data smoothing between voxels for continuous spatial representation. Pulmonary MRI data acquisition is performed using a golden-angle radial sampling trajectory, with respiratory motion signals extracted from the center of k-space in each radial spoke. Based on the estimated motion signal, the k-space data is sorted into multiple respiratory phases. A 3DGS framework is then applied to reconstruct a reference image volume from the first motion state. Subsequently, a patient-specific convolutional neural network is trained to estimate the deformation vector fields (DVFs), which are used to generate the remaining motion states through spatial transformation of the reference volume. The proposed reconstruction pipeline is evaluated on six datasets from six subjects and bench-marked against three state-of-the-art reconstruction methods. The experimental findings demonstrate that the proposed reconstruction framework effectively reconstructs high-resolution, motion-resolved pulmonary MR images. Compared with existing approaches, it achieves superior image quality, reflected by higher signal-to-noise ratio and contrast-to-noise ratio. The proposed unsupervised 3DGS-based reconstruction method enables accurate motion-resolved pulmonary MRI with isotropic spatial resolution. Its superior performance in image quality metrics over state-of-the-art methods highlights its potential as a robust solution for clinical pulmonary MR imaging.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADD: Physics-Based Motion Imitation with Adversarial Differential Discriminators</title>
<link>https://arxiv.org/abs/2505.04961</link>
<guid>https://arxiv.org/abs/2505.04961</guid>
<content:encoded><![CDATA[
arXiv:2505.04961v1 Announce Type: cross 
Abstract: Multi-objective optimization problems, which require the simultaneous optimization of multiple terms, are prevalent across numerous applications. Existing multi-objective optimization methods often rely on manually tuned aggregation functions to formulate a joint optimization target. The performance of such hand-tuned methods is heavily dependent on careful weight selection, a time-consuming and laborious process. These limitations also arise in the setting of reinforcement-learning-based motion tracking for physically simulated characters, where intricately crafted reward functions are typically used to achieve high-fidelity results. Such solutions not only require domain expertise and significant manual adjustment, but also limit the applicability of the resulting reward function across diverse skills. To bridge this gap, we present a novel adversarial multi-objective optimization technique that is broadly applicable to a range of multi-objective optimization problems, including motion tracking. The proposed adversarial differential discriminator receives a single positive sample, yet is still effective at guiding the optimization process. We demonstrate that our technique can enable characters to closely replicate a variety of acrobatic and agile behaviors, achieving comparable quality to state-of-the-art motion-tracking methods, without relying on manually tuned reward functions. Results are best visualized through https://youtu.be/rz8BYCE9E2w.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General Transform: A Unified Framework for Adaptive Transform to Enhance Representations</title>
<link>https://arxiv.org/abs/2505.04969</link>
<guid>https://arxiv.org/abs/2505.04969</guid>
<content:encoded><![CDATA[
arXiv:2505.04969v1 Announce Type: cross 
Abstract: Discrete transforms, such as the discrete Fourier transform, are widely used in machine learning to improve model performance by extracting meaningful features. However, with numerous transforms available, selecting an appropriate one often depends on understanding the dataset's properties, making the approach less effective when such knowledge is unavailable. In this work, we propose General Transform (GT), an adaptive transform-based representation designed for machine learning applications. Unlike conventional transforms, GT learns data-driven mapping tailored to the dataset and task of interest. Here, we demonstrate that models incorporating GT outperform conventional transform-based approaches across computer vision and natural language processing tasks, highlighting its effectiveness in diverse learning scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI and Vision based Autonomous Navigation of Nano-Drones in Partially-Known Environments</title>
<link>https://arxiv.org/abs/2505.04972</link>
<guid>https://arxiv.org/abs/2505.04972</guid>
<content:encoded><![CDATA[
arXiv:2505.04972v1 Announce Type: cross 
Abstract: The miniaturisation of sensors and processors, the advancements in connected edge intelligence, and the exponential interest in Artificial Intelligence are boosting the affirmation of autonomous nano-size drones in the Internet of Robotic Things ecosystem. However, achieving safe autonomous navigation and high-level tasks such as exploration and surveillance with these tiny platforms is extremely challenging due to their limited resources. This work focuses on enabling the safe and autonomous flight of a pocket-size, 30-gram platform called Crazyflie 2.1 in a partially known environment. We propose a novel AI-aided, vision-based reactive planning method for obstacle avoidance under the ambit of Integrated Sensing, Computing and Communication paradigm. We deal with the constraints of the nano-drone by splitting the navigation task into two parts: a deep learning-based object detector runs on the edge (external hardware) while the planning algorithm is executed onboard. The results show the ability to command the drone at $\sim8$ frames-per-second and a model performance reaching a COCO mean-average-precision of $60.8$. Field experiments demonstrate the feasibility of the solution with the drone flying at a top speed of $1$ m/s while steering away from an obstacle placed in an unknown position and reaching the target destination. The outcome highlights the compatibility of the communication delay and the model performance with the requirements of the real-time navigation task. We provide a feasible alternative to a fully onboard implementation that can be extended to autonomous exploration with nano-drones.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication</title>
<link>https://arxiv.org/abs/2505.04996</link>
<guid>https://arxiv.org/abs/2505.04996</guid>
<content:encoded><![CDATA[
arXiv:2505.04996v1 Announce Type: cross 
Abstract: Full-body gestures play a pivotal role in natural interactions and are crucial for achieving effective communication. Nevertheless, most existing studies primarily focus on the gesture generation of speakers, overlooking the vital role of listeners in the interaction process and failing to fully explore the dynamic interaction between them. This paper innovatively proposes an Inter-Diffusion Generation Model of Speakers and Listeners for Effective Communication. For the first time, we integrate the full-body gestures of listeners into the generation framework. By devising a novel inter-diffusion mechanism, this model can accurately capture the complex interaction patterns between speakers and listeners during communication. In the model construction process, based on the advanced diffusion model architecture, we innovatively introduce interaction conditions and the GAN model to increase the denoising step size. As a result, when generating gesture sequences, the model can not only dynamically generate based on the speaker's speech information but also respond in realtime to the listener's feedback, enabling synergistic interaction between the two. Abundant experimental results demonstrate that compared with the current state-of-the-art gesture generation methods, the model we proposed has achieved remarkable improvements in the naturalness, coherence, and speech-gesture synchronization of the generated gestures. In the subjective evaluation experiments, users highly praised the generated interaction scenarios, believing that they are closer to real life human communication situations. Objective index evaluations also show that our model outperforms the baseline methods in multiple key indicators, providing more powerful support for effective communication.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image-Text Relation Prediction for Multilingual Tweets</title>
<link>https://arxiv.org/abs/2505.05040</link>
<guid>https://arxiv.org/abs/2505.05040</guid>
<content:encoded><![CDATA[
arXiv:2505.05040v1 Announce Type: cross 
Abstract: Various social networks have been allowing media uploads for over a decade now. Still, it has not always been clear what is their relation with the posted text or even if there is any at all. In this work, we explore how multilingual vision-language models tackle the task of image-text relation prediction in different languages, and construct a dedicated balanced benchmark data set from Twitter posts in Latvian along with their manual translations into English. We compare our results to previous work and show that the more recently released vision-language model checkpoints are becoming increasingly capable at this task, but there is still much room for further improvement.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADNP-15: An Open-Source Histopathological Dataset for Neuritic Plaque Segmentation in Human Brain Whole Slide Images with Frequency Domain Image Enhancement for Stain Normalization</title>
<link>https://arxiv.org/abs/2505.05041</link>
<guid>https://arxiv.org/abs/2505.05041</guid>
<content:encoded><![CDATA[
arXiv:2505.05041v1 Announce Type: cross 
Abstract: Alzheimer's Disease (AD) is a neurodegenerative disorder characterized by amyloid-beta plaques and tau neurofibrillary tangles, which serve as key histopathological features. The identification and segmentation of these lesions are crucial for understanding AD progression but remain challenging due to the lack of large-scale annotated datasets and the impact of staining variations on automated image analysis. Deep learning has emerged as a powerful tool for pathology image segmentation; however, model performance is significantly influenced by variations in staining characteristics, necessitating effective stain normalization and enhancement techniques. In this study, we address these challenges by introducing an open-source dataset (ADNP-15) of neuritic plaques (i.e., amyloid deposits combined with a crown of dystrophic tau-positive neurites) in human brain whole slide images. We establish a comprehensive benchmark by evaluating five widely adopted deep learning models across four stain normalization techniques, providing deeper insights into their influence on neuritic plaque segmentation. Additionally, we propose a novel image enhancement method that improves segmentation accuracy, particularly in complex tissue structures, by enhancing structural details and mitigating staining inconsistencies. Our experimental results demonstrate that this enhancement strategy significantly boosts model generalization and segmentation accuracy. All datasets and code are open-source, ensuring transparency and reproducibility while enabling further advancements in the field.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Image Classification from Fourier Ptychographic Microscopy Measurements without Reconstruction</title>
<link>https://arxiv.org/abs/2505.05054</link>
<guid>https://arxiv.org/abs/2505.05054</guid>
<content:encoded><![CDATA[
arXiv:2505.05054v1 Announce Type: cross 
Abstract: The computational imaging technique of Fourier Ptychographic Microscopy (FPM) enables high-resolution imaging with a wide field of view and can serve as an extremely valuable tool, e.g. in the classification of cells in medical applications. However, reconstructing a high-resolution image from tens or even hundreds of measurements is computationally expensive, particularly for a wide field of view. Therefore, in this paper, we investigate the idea of classifying the image content in the FPM measurements directly without performing a reconstruction step first. We show that Convolutional Neural Networks (CNN) can extract meaningful information from measurement sequences, significantly outperforming the classification on a single band-limited image (up to 12 %) while being significantly more efficient than a reconstruction of a high-resolution image. Furthermore, we demonstrate that a learned multiplexing of several raw measurements allows maintaining the classification accuracy while reducing the amount of data (and consequently also the acquisition time) significantly.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepSNet: A Nucleus Instance Segmentation model based on Boundary Regression and Structural Re-parameterization</title>
<link>https://arxiv.org/abs/2505.05073</link>
<guid>https://arxiv.org/abs/2505.05073</guid>
<content:encoded><![CDATA[
arXiv:2505.05073v1 Announce Type: cross 
Abstract: Pathological diagnosis is the gold standard for tumor diagnosis, and nucleus instance segmentation is a key step in digital pathology analysis and pathological diagnosis. However, the computational efficiency of the model and the treatment of overlapping targets are the major challenges in the studies of this problem. To this end, a neural network model RepSNet was designed based on a nucleus boundary regression and a structural re-parameterization scheme for segmenting and classifying the nuclei in H\&amp;E-stained histopathological images. First, RepSNet estimates the boundary position information (BPI) of the parent nucleus for each pixel. The BPI estimation incorporates the local information of the pixel and the contextual information of the parent nucleus. Then, the nucleus boundary is estimated by aggregating the BPIs from a series of pixels using a proposed boundary voting mechanism (BVM), and the instance segmentation results are computed from the estimated nucleus boundary using a connected component analysis procedure. The BVM intrinsically achieves a kind of synergistic belief enhancement among the BPIs from various pixels. Therefore, different from the methods available in literature that obtain nucleus boundaries based on a direct pixel recognition scheme, RepSNet computes its boundary decisions based on some guidances from macroscopic information using an integration mechanism. In addition, RepSNet employs a re-parametrizable encoder-decoder structure. This model can not only aggregate features from some receptive fields with various scales which helps segmentation accuracy improvement, but also reduce the parameter amount and computational burdens in the model inference phase through the structural re-parameterization technique. Extensive experiments demonstrated the superiorities of RepSNet compared to several typical benchmark models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The City that Never Settles: Simulation-based LiDAR Dataset for Long-Term Place Recognition Under Extreme Structural Changes</title>
<link>https://arxiv.org/abs/2505.05076</link>
<guid>https://arxiv.org/abs/2505.05076</guid>
<content:encoded><![CDATA[
arXiv:2505.05076v1 Announce Type: cross 
Abstract: Large-scale construction and demolition significantly challenge long-term place recognition (PR) by drastically reshaping urban and suburban environments. Existing datasets predominantly reflect limited or indoor-focused changes, failing to adequately represent extensive outdoor transformations. To bridge this gap, we introduce the City that Never Settles (CNS) dataset, a simulation-based dataset created using the CARLA simulator, capturing major structural changes-such as building construction and demolition-across diverse maps and sequences. Additionally, we propose TCR_sym, a symmetric version of the original TCR metric, enabling consistent measurement of structural changes irrespective of source-target ordering. Quantitative comparisons demonstrate that CNS encompasses more extensive transformations than current real-world benchmarks. Evaluations of state-of-the-art LiDAR-based PR methods on CNS reveal substantial performance degradation, underscoring the need for robust algorithms capable of handling significant environmental changes. Our dataset is available at https://github.com/Hyunho111/CNS_dataset.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSH-Net: A Self-Supervised and Hybrid Network for Noisy Image Watermark Removal</title>
<link>https://arxiv.org/abs/2505.05088</link>
<guid>https://arxiv.org/abs/2505.05088</guid>
<content:encoded><![CDATA[
arXiv:2505.05088v1 Announce Type: cross 
Abstract: Visible watermark removal is challenging due to its inherent complexities and the noise carried within images. Existing methods primarily rely on supervised learning approaches that require paired datasets of watermarked and watermark-free images, which are often impractical to obtain in real-world scenarios. To address this challenge, we propose SSH-Net, a Self-Supervised and Hybrid Network specifically designed for noisy image watermark removal. SSH-Net synthesizes reference watermark-free images using the watermark distribution in a self-supervised manner and adopts a dual-network design to address the task. The upper network, focused on the simpler task of noise removal, employs a lightweight CNN-based architecture, while the lower network, designed to handle the more complex task of simultaneously removing watermarks and noise, incorporates Transformer blocks to model long-range dependencies and capture intricate image features. To enhance the model's effectiveness, a shared CNN-based feature encoder is introduced before dual networks to extract common features that both networks can leverage. Our code will be available at https://github.com/wenyang001/SSH-Net.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Driver: Explainable Autonomous Driving with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.05098</link>
<guid>https://arxiv.org/abs/2505.05098</guid>
<content:encoded><![CDATA[
arXiv:2505.05098v1 Announce Type: cross 
Abstract: End-to-end autonomous driving has advanced significantly, offering benefits such as system simplicity and stronger driving performance in both open-loop and closed-loop settings than conventional pipelines. However, existing frameworks still suffer from low success rates in closed-loop evaluations, highlighting their limitations in real-world deployment. In this paper, we introduce X-Driver, a unified multi-modal large language models(MLLMs) framework designed for closed-loop autonomous driving, leveraging Chain-of-Thought(CoT) and autoregressive modeling to enhance perception and decision-making. We validate X-Driver across multiple autonomous driving tasks using public benchmarks in CARLA simulation environment, including Bench2Drive[6]. Our experimental results demonstrate superior closed-loop performance, surpassing the current state-of-the-art(SOTA) while improving the interpretability of driving decisions. These findings underscore the importance of structured reasoning in end-to-end driving and establish X-Driver as a strong baseline for future research in closed-loop autonomous driving.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MDAA-Diff: CT-Guided Multi-Dose Adaptive Attention Diffusion Model for PET Denoising</title>
<link>https://arxiv.org/abs/2505.05112</link>
<guid>https://arxiv.org/abs/2505.05112</guid>
<content:encoded><![CDATA[
arXiv:2505.05112v1 Announce Type: cross 
Abstract: Acquiring high-quality Positron Emission Tomography (PET) images requires administering high-dose radiotracers, which increases radiation exposure risks. Generating standard-dose PET (SPET) from low-dose PET (LPET) has become a potential solution. However, previous studies have primarily focused on single low-dose PET denoising, neglecting two critical factors: discrepancies in dose response caused by inter-patient variability, and complementary anatomical constraints derived from CT images. In this work, we propose a novel CT-Guided Multi-dose Adaptive Attention Denoising Diffusion Model (MDAA-Diff) for multi-dose PET denoising. Our approach integrates anatomical guidance and dose-level adaptation to achieve superior denoising performance under low-dose conditions. Specifically, this approach incorporates a CT-Guided High-frequency Wavelet Attention (HWA) module, which uses wavelet transforms to separate high-frequency anatomical boundary features from CT images. These extracted features are then incorporated into PET imaging through an adaptive weighted fusion mechanism to enhance edge details. Additionally, we propose the Dose-Adaptive Attention (DAA) module, a dose-conditioned enhancement mechanism that dynamically integrates dose levels into channel-spatial attention weight calculation. Extensive experiments on 18F-FDG and 68Ga-FAPI datasets demonstrate that MDAA-Diff outperforms state-of-the-art approaches in preserving diagnostic quality under reduced-dose conditions. Our code is publicly available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Active Contour Model for Silhouette Vectorization using B\'ezier Curves</title>
<link>https://arxiv.org/abs/2505.05132</link>
<guid>https://arxiv.org/abs/2505.05132</guid>
<content:encoded><![CDATA[
arXiv:2505.05132v1 Announce Type: cross 
Abstract: In this paper, we propose an active contour model for silhouette vectorization using cubic B\'ezier curves. Among the end points of the B\'ezier curves, we distinguish between corner and regular points where the orientation of the tangent vector is prescribed. By minimizing the distance of the B\'ezier curves to the silhouette boundary, the active contour model optimizes the location of the B\'ezier curves end points, the orientation of the tangent vectors in the regular points, and the estimation of the B\'ezier curve parameters. This active contour model can use the silhouette vectorization obtained by any method as an initial guess. The proposed method significantly reduces the average distance between the silhouette boundary and its vectorization obtained by the world-class graphic software Inkscape, Adobe Illustrator, and a curvature-based vectorization method, which we introduce for comparison. Our method also allows us to impose additional regularity on the B\'ezier curves by reducing their lengths.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on Anomaly Detection Methods Based on Diffusion Models</title>
<link>https://arxiv.org/abs/2505.05137</link>
<guid>https://arxiv.org/abs/2505.05137</guid>
<content:encoded><![CDATA[
arXiv:2505.05137v1 Announce Type: cross 
Abstract: Anomaly detection is a fundamental task in machine learning and data mining, with significant applications in cybersecurity, industrial fault diagnosis, and clinical disease monitoring. Traditional methods, such as statistical modeling and machine learning-based approaches, often face challenges in handling complex, high-dimensional data distributions. In this study, we explore the potential of diffusion models for anomaly detection, proposing a novel framework that leverages the strengths of diffusion probabilistic models (DPMs) to effectively identify anomalies in both image and audio data. The proposed method models the distribution of normal data through a diffusion process and reconstructs input data via reverse diffusion, using a combination of reconstruction errors and semantic discrepancies as anomaly indicators. To enhance the framework's performance, we introduce multi-scale feature extraction, attention mechanisms, and wavelet-domain representations, enabling the model to capture fine-grained structures and global dependencies in the data. Extensive experiments on benchmark datasets, including MVTec AD and UrbanSound8K, demonstrate that our method outperforms state-of-the-art anomaly detection techniques, achieving superior accuracy and robustness across diverse data modalities. This research highlights the effectiveness of diffusion models in anomaly detection and provides a robust and efficient solution for real-world applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concept-Based Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.05195</link>
<guid>https://arxiv.org/abs/2505.05195</guid>
<content:encoded><![CDATA[
arXiv:2505.05195v1 Announce Type: cross 
Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by explaining predictions through human-understandable concepts but typically assume that training and test data share the same distribution. This assumption often fails under domain shifts, leading to degraded performance and poor generalization. To address these limitations and improve the robustness of CBMs, we propose the Concept-based Unsupervised Domain Adaptation (CUDA) framework. CUDA is designed to: (1) align concept representations across domains using adversarial training, (2) introduce a relaxation threshold to allow minor domain-specific differences in concept distributions, thereby preventing performance drop due to over-constraints of these distributions, (3) infer concepts directly in the target domain without requiring labeled concept data, enabling CBMs to adapt to diverse domains, and (4) integrate concept learning into conventional domain adaptation (DA) with theoretical guarantees, improving interpretability and establishing new benchmarks for DA. Experiments demonstrate that our approach significantly outperforms the state-of-the-art CBM and DA methods on real-world datasets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Brain Tumor Detection in MRI: Fuzzy Sigmoid Convolution in Deep Learning</title>
<link>https://arxiv.org/abs/2505.05208</link>
<guid>https://arxiv.org/abs/2505.05208</guid>
<content:encoded><![CDATA[
arXiv:2505.05208v1 Announce Type: cross 
Abstract: Early detection and accurate diagnosis are essential to improving patient outcomes. The use of convolutional neural networks (CNNs) for tumor detection has shown promise, but existing models often suffer from overparameterization, which limits their performance gains. In this study, fuzzy sigmoid convolution (FSC) is introduced along with two additional modules: top-of-the-funnel and middle-of-the-funnel. The proposed methodology significantly reduces the number of trainable parameters without compromising classification accuracy. A novel convolutional operator is central to this approach, effectively dilating the receptive field while preserving input data integrity. This enables efficient feature map reduction and enhances the model's tumor detection capability. In the FSC-based model, fuzzy sigmoid activation functions are incorporated within convolutional layers to improve feature extraction and classification. The inclusion of fuzzy logic into the architecture improves its adaptability and robustness. Extensive experiments on three benchmark datasets demonstrate the superior performance and efficiency of the proposed model. The FSC-based architecture achieved classification accuracies of 99.17%, 99.75%, and 99.89% on three different datasets. The model employs 100 times fewer parameters than large-scale transfer learning architectures, highlighting its computational efficiency and suitability for detecting brain tumors early. This research offers lightweight, high-performance deep-learning models for medical imaging applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Objective Reinforcement Learning for Adaptive Personalized Autonomous Driving</title>
<link>https://arxiv.org/abs/2505.05223</link>
<guid>https://arxiv.org/abs/2505.05223</guid>
<content:encoded><![CDATA[
arXiv:2505.05223v1 Announce Type: cross 
Abstract: Human drivers exhibit individual preferences regarding driving style. Adapting autonomous vehicles to these preferences is essential for user trust and satisfaction. However, existing end-to-end driving approaches often rely on predefined driving styles or require continuous user feedback for adaptation, limiting their ability to support dynamic, context-dependent preferences. We propose a novel approach using multi-objective reinforcement learning (MORL) with preference-driven optimization for end-to-end autonomous driving that enables runtime adaptation to driving style preferences. Preferences are encoded as continuous weight vectors to modulate behavior along interpretable style objectives$\unicode{x2013}$including efficiency, comfort, speed, and aggressiveness$\unicode{x2013}$without requiring policy retraining. Our single-policy agent integrates vision-based perception in complex mixed-traffic scenarios and is evaluated in diverse urban environments using the CARLA simulator. Experimental results demonstrate that the agent dynamically adapts its driving behavior according to changing preferences while maintaining performance in terms of collision avoidance and route completion.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>White Light Specular Reflection Data Augmentation for Deep Learning Polyp Detection</title>
<link>https://arxiv.org/abs/2505.05248</link>
<guid>https://arxiv.org/abs/2505.05248</guid>
<content:encoded><![CDATA[
arXiv:2505.05248v1 Announce Type: cross 
Abstract: Colorectal cancer is one of the deadliest cancers today, but it can be prevented through early detection of malignant polyps in the colon, primarily via colonoscopies. While this method has saved many lives, human error remains a significant challenge, as missing a polyp could have fatal consequences for the patient. Deep learning (DL) polyp detectors offer a promising solution. However, existing DL polyp detectors often mistake white light reflections from the endoscope for polyps, which can lead to false positives.To address this challenge, in this paper, we propose a novel data augmentation approach that artificially adds more white light reflections to create harder training scenarios. Specifically, we first generate a bank of artificial lights using the training dataset. Then we find the regions of the training images that we should not add these artificial lights on. Finally, we propose a sliding window method to add the artificial light to the areas that fit of the training images, resulting in augmented images. By providing the model with more opportunities to make mistakes, we hypothesize that it will also have more chances to learn from those mistakes, ultimately improving its performance in polyp detection. Experimental results demonstrate the effectiveness of our new data augmentation method.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTL-UE: Learning to Learn Nothing for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2505.05279</link>
<guid>https://arxiv.org/abs/2505.05279</guid>
<content:encoded><![CDATA[
arXiv:2505.05279v1 Announce Type: cross 
Abstract: Most existing unlearnable strategies focus on preventing unauthorized users from training single-task learning (STL) models with personal data. Nevertheless, the paradigm has recently shifted towards multi-task data and multi-task learning (MTL), targeting generalist and foundation models that can handle multiple tasks simultaneously. Despite their growing importance, MTL data and models have been largely neglected while pursuing unlearnable strategies. This paper presents MTL-UE, the first unified framework for generating unlearnable examples for multi-task data and MTL models. Instead of optimizing perturbations for each sample, we design a generator-based structure that introduces label priors and class-wise feature embeddings which leads to much better attacking performance. In addition, MTL-UE incorporates intra-task and inter-task embedding regularization to increase inter-class separation and suppress intra-class variance which enhances the attack robustness greatly. Furthermore, MTL-UE is versatile with good supports for dense prediction tasks in MTL. It is also plug-and-play allowing integrating existing surrogate-dependent unlearnable methods with little adaptation. Extensive experiments show that MTL-UE achieves superior attacking performance consistently across 4 MTL datasets, 3 base UE methods, 5 model backbones, and 5 MTL task-weighting strategies.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Ophthalmology Foundation Models for Clinically Significant Age Macular Degeneration Detection</title>
<link>https://arxiv.org/abs/2505.05291</link>
<guid>https://arxiv.org/abs/2505.05291</guid>
<content:encoded><![CDATA[
arXiv:2505.05291v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) has enabled Vision Transformers (ViTs) to learn robust representations from large-scale natural image datasets, enhancing their generalization across domains. In retinal imaging, foundation models pretrained on either natural or ophthalmic data have shown promise, but the benefits of in-domain pretraining remain uncertain. To investigate this, we benchmark six SSL-pretrained ViTs on seven digital fundus image (DFI) datasets totaling 70,000 expert-annotated images for the task of moderate-to-late age-related macular degeneration (AMD) identification. Our results show that iBOT pretrained on natural images achieves the highest out-of-distribution generalization, with AUROCs of 0.80-0.97, outperforming domain-specific models, which achieved AUROCs of 0.78-0.96 and a baseline ViT-L with no pretraining, which achieved AUROCs of 0.68-0.91. These findings highlight the value of foundation models in improving AMD identification and challenge the assumption that in-domain pretraining is necessary. Furthermore, we release BRAMD, an open-access dataset (n=587) of DFIs with AMD labels from Brazil.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmented Deep Contexts for Spatially Embedded Video Coding</title>
<link>https://arxiv.org/abs/2505.05309</link>
<guid>https://arxiv.org/abs/2505.05309</guid>
<content:encoded><![CDATA[
arXiv:2505.05309v1 Announce Type: cross 
Abstract: Most Neural Video Codecs (NVCs) only employ temporal references to generate temporal-only contexts and latent prior. These temporal-only NVCs fail to handle large motions or emerging objects due to limited contexts and misaligned latent prior. To relieve the limitations, we propose a Spatially Embedded Video Codec (SEVC), in which the low-resolution video is compressed for spatial references. Firstly, our SEVC leverages both spatial and temporal references to generate augmented motion vectors and hybrid spatial-temporal contexts. Secondly, to address the misalignment issue in latent prior and enrich the prior information, we introduce a spatial-guided latent prior augmented by multiple temporal latent representations. At last, we design a joint spatial-temporal optimization to learn quality-adaptive bit allocation for spatial references, further boosting rate-distortion performance. Experimental results show that our SEVC effectively alleviates the limitations in handling large motions or emerging objects, and also reduces 11.9% more bitrate than the previous state-of-the-art NVC while providing an additional low-resolution bitstream. Our code and model are available at https://github.com/EsakaK/SEVC.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields</title>
<link>https://arxiv.org/abs/2505.05356</link>
<guid>https://arxiv.org/abs/2505.05356</guid>
<content:encoded><![CDATA[
arXiv:2505.05356v1 Announce Type: cross 
Abstract: We present a method to reconstruct dynamic scenes from monocular continuous-wave time-of-flight (C-ToF) cameras using raw sensor samples that achieves similar or better accuracy than neural volumetric approaches and is 100x faster. Quickly achieving high-fidelity dynamic 3D reconstruction from a single viewpoint is a significant challenge in computer vision. In C-ToF radiance field reconstruction, the property of interest-depth-is not directly measured, causing an additional challenge. This problem has a large and underappreciated impact upon the optimization when using a fast primitive-based scene representation like 3D Gaussian splatting, which is commonly used with multi-view data to produce satisfactory results and is brittle in its optimization otherwise. We incorporate two heuristics into the optimization to improve the accuracy of scene geometry represented by Gaussians. Experimental results show that our approach produces accurate reconstructions under constrained C-ToF sensing conditions, including for fast motions like swinging baseball bats. https://visual.cs.brown.edu/gftorf
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OcularAge: A Comparative Study of Iris and Periocular Images for Pediatric Age Estimation</title>
<link>https://arxiv.org/abs/2505.05374</link>
<guid>https://arxiv.org/abs/2505.05374</guid>
<content:encoded><![CDATA[
arXiv:2505.05374v1 Announce Type: cross 
Abstract: Estimating a child's age from ocular biometric images is challenging due to subtle physiological changes and the limited availability of longitudinal datasets. Although most biometric age estimation studies have focused on facial features and adult subjects, pediatric-specific analysis, particularly of the iris and periocular regions, remains relatively unexplored. This study presents a comparative evaluation of iris and periocular images for estimating the ages of children aged between 4 and 16 years. We utilized a longitudinal dataset comprising more than 21,000 near-infrared (NIR) images, collected from 288 pediatric subjects over eight years using two different imaging sensors. A multi-task deep learning framework was employed to jointly perform age prediction and age-group classification, enabling a systematic exploration of how different convolutional neural network (CNN) architectures, particularly those adapted for non-square ocular inputs, capture the complex variability inherent in pediatric eye images. The results show that periocular models consistently outperform iris-based models, achieving a mean absolute error (MAE) of 1.33 years and an age-group classification accuracy of 83.82%. These results mark the first demonstration that reliable age estimation is feasible from children's ocular images, enabling privacy-preserving age checks in child-centric applications. This work establishes the first longitudinal benchmark for pediatric ocular age estimation, providing a foundation for designing robust, child-focused biometric systems. The developed models proved resilient across different imaging sensors, confirming their potential for real-world deployment. They also achieved inference speeds of less than 10 milliseconds per image on resource-constrained VR headsets, demonstrating their suitability for real-time applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-based assignment decision network for multiple object tracking</title>
<link>https://arxiv.org/abs/2208.03571</link>
<guid>https://arxiv.org/abs/2208.03571</guid>
<content:encoded><![CDATA[
arXiv:2208.03571v3 Announce Type: replace 
Abstract: Data association is a crucial component for any multiple object tracking (MOT) method that follows the tracking-by-detection paradigm. To generate complete trajectories such methods employ a data association process to establish assignments between detections and existing targets during each timestep. Recent data association approaches try to solve either a multi-dimensional linear assignment task or a network flow minimization problem or tackle it via multiple hypotheses tracking. However, during inference an optimization step that computes optimal assignments is required for every sequence frame inducing additional complexity to any given solution. To this end, in the context of this work we introduce Transformer-based Assignment Decision Network (TADN) that tackles data association without the need of any explicit optimization during inference. In particular, TADN can directly infer assignment pairs between detections and active targets in a single forward pass of the network. We have integrated TADN in a rather simple MOT framework, designed a novel training strategy for efficient end-to-end training and demonstrated the high potential of our approach for online visual tracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and UA-DETRAC. Our proposed approach demonstrates strong performance in most evaluation metrics despite its simple nature as a tracker lacking significant auxiliary components such as occlusion handling or re-identification. The implementation of our method is publicly available at https://github.com/psaltaath/tadn-mot.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions</title>
<link>https://arxiv.org/abs/2303.12484</link>
<guid>https://arxiv.org/abs/2303.12484</guid>
<content:encoded><![CDATA[
arXiv:2303.12484v5 Announce Type: replace 
Abstract: Deep learning has significantly advanced medical imaging analysis (MIA), achieving state-of-the-art performance across diverse clinical tasks. However, its success largely depends on large-scale, high-quality labeled datasets, which are costly and time-consuming to obtain due to the need for expert annotation. To mitigate this limitation, label-efficient deep learning methods have emerged to improve model performance under limited supervision by leveraging labeled, unlabeled, and weakly labeled data. In this survey, we systematically review over 350 peer-reviewed studies and present a comprehensive taxonomy of label-efficient learning methods in MIA. These methods are categorized into four labeling paradigms: no label, insufficient label, inexact label, and label refinement. For each category, we analyze representative techniques across imaging modalities and clinical applications, highlighting shared methodological principles and task-specific adaptations. We also examine the growing role of health foundation models (HFMs) in enabling label-efficient learning through large-scale pre-training and transfer learning, enhancing the use of limited annotations in downstream tasks. Finally, we identify current challenges and future directions to facilitate the translation of label-efficient learning from research promise to everyday clinical care.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USTEP: Spatio-Temporal Predictive Learning under A Unified View</title>
<link>https://arxiv.org/abs/2310.05829</link>
<guid>https://arxiv.org/abs/2310.05829</guid>
<content:encoded><![CDATA[
arXiv:2310.05829v2 Announce Type: replace 
Abstract: Spatio-temporal predictive learning plays a crucial role in self-supervised learning, with wide-ranging applications across a diverse range of fields. Previous approaches for temporal modeling fall into two categories: recurrent-based and recurrent-free methods. The former, while meticulously processing frames one by one, neglect short-term spatio-temporal information redundancies, leading to inefficiencies. The latter naively stack frames sequentially, overlooking the inherent temporal dependencies. In this paper, we re-examine the two dominant temporal modeling approaches within the realm of spatio-temporal predictive learning, offering a unified perspective. Building upon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive learning), an innovative framework that reconciles the recurrent-based and recurrent-free methods by integrating both micro-temporal and macro-temporal scales. Extensive experiments on a wide range of spatio-temporal predictive learning demonstrate that USTEP achieves significant improvements over existing temporal modeling approaches, thereby establishing it as a robust solution for a wide range of spatio-temporal applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Views Are Better than One: Monocular 3D Pose Estimation with Multiview Consistency</title>
<link>https://arxiv.org/abs/2311.12421</link>
<guid>https://arxiv.org/abs/2311.12421</guid>
<content:encoded><![CDATA[
arXiv:2311.12421v3 Announce Type: replace 
Abstract: Deducing a 3D human pose from a single 2D image is inherently challenging because multiple 3D poses can correspond to the same 2D representation. 3D data can resolve this pose ambiguity, but it is expensive to record and requires an intricate setup that is often restricted to controlled lab environments. We propose a method that improves the performance of deep learning-based monocular 3D human pose estimation models by using multiview data only during training, but not during inference. We introduce a novel loss function, consistency loss, which operates on two synchronized views. This approach is simpler than previous models that require 3D ground truth or intrinsic and extrinsic camera parameters. Our consistency loss penalizes differences in two pose sequences after rigid alignment. We also demonstrate that our consistency loss substantially improves performance for fine-tuning without requiring 3D data. Furthermore, we show that using our consistency loss can yield state-of-the-art performance when training models from scratch in a semi-supervised manner. Our findings provide a simple way to capture new data, e.g in a new domain. This data can be added using off-the-shelf cameras with no calibration requirements. We make all our code and data publicly available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FieldNet: Efficient Real-Time Shadow Removal for Enhanced Vision in Field Robotics</title>
<link>https://arxiv.org/abs/2403.08142</link>
<guid>https://arxiv.org/abs/2403.08142</guid>
<content:encoded><![CDATA[
arXiv:2403.08142v2 Announce Type: replace 
Abstract: Shadows significantly hinder computer vision tasks in outdoor environments, particularly in field robotics, where varying lighting conditions complicate object detection and localisation. We present FieldNet, a novel deep learning framework for real-time shadow removal, optimised for resource-constrained hardware. FieldNet introduces a probabilistic enhancement module and a novel loss function to address challenges of inconsistent shadow boundary supervision and artefact generation, achieving enhanced accuracy and simplicity without requiring shadow masks during inference. Trained on a dataset of 10,000 natural images augmented with synthetic shadows, FieldNet outperforms state-of-the-art methods on benchmark datasets (ISTD, ISTD+, SRD), with up to $9$x speed improvements (66 FPS on Nvidia 2080Ti) and superior shadow removal quality (PSNR: 38.67, SSIM: 0.991). Real-world case studies in precision agriculture robotics demonstrate the practical impact of FieldNet in enhancing weed detection accuracy. These advancements establish FieldNet as a robust, efficient solution for real-time vision tasks in field robotics and beyond.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Human Gaussians from Single-View Image</title>
<link>https://arxiv.org/abs/2406.06050</link>
<guid>https://arxiv.org/abs/2406.06050</guid>
<content:encoded><![CDATA[
arXiv:2406.06050v5 Announce Type: replace 
Abstract: In this work, we tackle the task of learning 3D human Gaussians from a single image, focusing on recovering detailed appearance and geometry including unobserved regions. We introduce a single-view generalizable Human Gaussian Model (HGM), which employs a novel generate-then-refine pipeline with the guidance from human body prior and diffusion prior. Our approach uses a ControlNet to refine rendered back-view images from coarse predicted human Gaussians, then uses the refined image along with the input image to reconstruct refined human Gaussians. To mitigate the potential generation of unrealistic human poses and shapes, we incorporate human priors from the SMPL-X model as a dual branch, propagating image features from the SMPL-X volume to the image Gaussians using sparse convolution and attention mechanisms. Given that the initial SMPL-X estimation might be inaccurate, we gradually refine it with our HGM model. We validate our approach on several publicly available datasets. Our method surpasses previous methods in both novel view synthesis and surface reconstruction. Our approach also exhibits strong generalization for cross-dataset evaluation and in-the-wild images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Adverse Weather Crowd Counting via Multi-queue Contrastive Learning</title>
<link>https://arxiv.org/abs/2408.05956</link>
<guid>https://arxiv.org/abs/2408.05956</guid>
<content:encoded><![CDATA[
arXiv:2408.05956v3 Announce Type: replace 
Abstract: Currently, most crowd counting methods have outstanding performance under normal weather conditions. However, our experimental validation reveals two key obstacles limiting the accuracy improvement of crowd counting models: 1) the domain gap between the adverse weather and the normal weather images; 2) the weather class imbalance in the training set. To address the problems, we propose a two-stage crowd counting method named Multi-queue Contrastive Learning (MQCL). Specifically, in the first stage, our target is to equip the backbone network with weather-awareness capabilities. In this process, a contrastive learning method named multi-queue MoCo designed by us is employed to enable representation learning under weather class imbalance. After the first stage is completed, the backbone model is "mature" enough to extract weather-related representations. On this basis, we proceed to the second stage, in which we propose to refine the representations under the guidance of contrastive learning, enabling the conversion of the weather-aware representations to the normal weather domain. Through such representation and conversion, the model achieves robust counting performance under both normal and adverse weather conditions. Extensive experimental results show that, compared to the baseline, MQCL reduces the counting error under adverse weather conditions by 22%, while introducing only about 13% increase in computational burden, which achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep learning-based ecological analysis of camera trap images is impacted by training data quality and quantity</title>
<link>https://arxiv.org/abs/2408.14348</link>
<guid>https://arxiv.org/abs/2408.14348</guid>
<content:encoded><![CDATA[
arXiv:2408.14348v2 Announce Type: replace 
Abstract: Large image collections generated from camera traps offer valuable insights into species richness, occupancy, and activity patterns, significantly aiding biodiversity monitoring. However, the manual processing of these datasets is time-consuming, hindering analytical processes. To address this, deep neural networks have been adopted to automate image labelling, but the impact of classification error on ecological metrics remains unclear. Here, we analyse data from camera trap collections in an African savannah (82,300 images, 47 species) and an Asian sub-tropical dry forest (40,308 images, 29 species) to compare ecological metrics derived from expert-generated species identifications with those generated by deep learning classification models. We specifically assess the impact of deep learning model architecture, the proportion of label noise in the training data, and the size of the training dataset on three ecological metrics: species richness, occupancy, and activity patterns. Overall, ecological metrics derived from deep neural networks closely match those calculated from expert labels and remain robust to manipulations in the training pipeline. We found that the choice of deep learning model architecture does not impact ecological metrics, and ecological metrics related to the overall community (species richness, community occupancy) were resilient to up to 10% noise in the training dataset and a 50% reduction in the training dataset size. However, we caution that less common species are disproportionately affected by a reduction in deep neural network accuracy, and this has consequences for species-specific metrics (occupancy, diel activity patterns). To ensure the reliability of their findings, practitioners should prioritize creating large, clean training sets with balanced representation across species over exploring numerous deep learning model architectures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration</title>
<link>https://arxiv.org/abs/2408.15994</link>
<guid>https://arxiv.org/abs/2408.15994</guid>
<content:encoded><![CDATA[
arXiv:2408.15994v2 Announce Type: replace 
Abstract: Existing All-in-One image restoration methods often fail to perceive degradation types and severity levels simultaneously, overlooking the importance of fine-grained quality perception. Moreover, these methods often utilize highly customized backbones, which hinder their adaptability and integration into more advanced restoration networks. To address these limitations, we propose Perceive-IR, a novel backbone-agnostic All-in-One image restoration framework designed for fine-grained quality control across various degradation types and severity levels. Its modular structure allows core components to function independently of specific backbones, enabling seamless integration into advanced restoration models without significant modifications. Specifically, Perceive-IR operates in two key stages: 1) multi-level quality-driven prompt learning stage, where a fine-grained quality perceiver is meticulously trained to discern three tier quality levels by optimizing the alignment between prompts and images within the CLIP perception space. This stage ensures a nuanced understanding of image quality, laying the groundwork for subsequent restoration; 2) restoration stage, where the quality perceiver is seamlessly integrated with a difficulty-adaptive perceptual loss, forming a quality-aware learning strategy. This strategy not only dynamically differentiates sample learning difficulty but also achieves fine-grained quality control by driving the restored image toward the ground truth while pulling it away from both low- and medium-quality samples.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2409.03757</link>
<guid>https://arxiv.org/abs/2409.03757</guid>
<content:encoded><![CDATA[
arXiv:2409.03757v3 Announce Type: replace 
Abstract: Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks. Code: https://github.com/YunzeMan/Lexicon3D
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locality-aware Cross-modal Correspondence Learning for Dense Audio-Visual Events Localization</title>
<link>https://arxiv.org/abs/2409.07967</link>
<guid>https://arxiv.org/abs/2409.07967</guid>
<content:encoded><![CDATA[
arXiv:2409.07967v3 Announce Type: replace 
Abstract: Dense-localization Audio-Visual Events (DAVE) aims to identify time boundaries and corresponding categories for events that are both audible and visible in a long video, where events may co-occur and exhibit varying durations. However, complex audio-visual scenes often involve asynchronization between modalities, making accurate localization challenging. Existing DAVE solutions extract audio and visual features through unimodal encoders, and fuse them via dense cross-modal interaction. However, independent unimodal encoding struggles to emphasize shared semantics between modalities without cross-modal guidance, while dense cross-modal attention may over-attend to semantically unrelated audio-visual features. To address these problems, we present LoCo, a Locality-aware cross-modal Correspondence learning framework for DAVE. LoCo leverages the local temporal continuity of audio-visual events as important guidance to filter irrelevant cross-modal signals and enhance cross-modal alignment throughout both unimodal and cross-modal encoding stages. i) Specifically, LoCo applies Local Correspondence Feature (LCF) Modulation to enforce unimodal encoders to focus on modality-shared semantics by modulating agreement between audio and visual features based on local cross-modal coherence. ii) To better aggregate cross-modal relevant features, we further customize Local Adaptive Cross-modal (LAC) Interaction, which dynamically adjusts attention regions in a data-driven manner. This adaptive mechanism focuses attention on local event boundaries and accommodates varying event durations. By incorporating LCF and LAC, LoCo provides solid performance gains and outperforms existing DAVE methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Synthetic Texture Datasets: Challenges, Creation, and Curation</title>
<link>https://arxiv.org/abs/2409.10297</link>
<guid>https://arxiv.org/abs/2409.10297</guid>
<content:encoded><![CDATA[
arXiv:2409.10297v2 Announce Type: replace 
Abstract: The influence of textures on machine learning models has been an ongoing investigation, specifically in texture bias/learning, interpretability, and robustness. However, due to the lack of large and diverse texture data available, the findings in these works have been limited, as more comprehensive evaluations have not been feasible. Image generative models are able to provide data creation at scale, but utilizing these models for texture synthesis has been unexplored and poses additional challenges both in creating accurate texture images and validating those images. In this work, we introduce an extensible methodology and corresponding new dataset for generating high-quality, diverse texture images capable of supporting a broad set of texture-based tasks. Our pipeline consists of: (1) developing prompts from a range of descriptors to serve as input to text-to-image models, (2) adopting and adapting Stable Diffusion pipelines to generate and filter the corresponding images, and (3) further filtering down to the highest quality images. Through this, we create the Prompted Textures Dataset (PTD), a dataset of 362,880 texture images that span 56 textures. During the process of generating images, we find that NSFW safety filters in image generation pipelines are highly sensitive to texture (and flag up to 60\% of our texture images), uncovering a potential bias in these models and presenting unique challenges when working with texture data. Through both standard metrics and a human evaluation, we find that our dataset is high quality and diverse. Our dataset is available for download at https://zenodo.org/records/15359142.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated detection of underdiagnosed medical conditions via opportunistic imaging</title>
<link>https://arxiv.org/abs/2409.11686</link>
<guid>https://arxiv.org/abs/2409.11686</guid>
<content:encoded><![CDATA[
arXiv:2409.11686v3 Announce Type: replace 
Abstract: Abdominal computed tomography (CT) scans are frequently performed in clinical settings. Opportunistic CT involves repurposing routine CT images to extract diagnostic information and is an emerging tool for detecting underdiagnosed conditions such as sarcopenia, hepatic steatosis, and ascites. This study utilizes deep learning methods to promote accurate diagnosis and clinical documentation. We analyze 2,674 inpatient CT scans to identify discrepancies between imaging phenotypes (characteristics derived from opportunistic CT scans) and their corresponding documentation in radiology reports and ICD coding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans diagnosed with sarcopenia, hepatic steatosis, and ascites (respectively) through either opportunistic imaging or radiology reports were ICD-coded. Our findings demonstrate opportunistic CT's potential to enhance diagnostic precision and accuracy of risk adjustment models, offering advancements in precision medicine.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2410.03577</link>
<guid>https://arxiv.org/abs/2410.03577</guid>
<content:encoded><![CDATA[
arXiv:2410.03577v2 Announce Type: replace 
Abstract: Despite their impressive capabilities, multimodal large language models (MLLMs) are prone to hallucinations, i.e., the generated content that is nonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in MLLMs often stem from the sensitivity of text decoder to visual tokens, leading to a phenomenon akin to "amnesia" about visual information. To address this issue, we propose MemVR, a novel decoding paradigm inspired by common cognition: when the memory of an image seen the moment before is forgotten, people will look at it again for factual answers. Following this principle, we treat visual tokens as supplementary evidence, re-injecting them into the MLLM through Feed Forward Network (FFN) as "key-value memory" at the middle trigger layer. This "look-twice" mechanism occurs when the model exhibits high uncertainty during inference, effectively enhancing factual alignment. Comprehensive experimental evaluations demonstrate that MemVR significantly mitigates hallucination across various MLLMs and excels in general benchmarks without incurring additional time overhead. The implementation is available from https://github.com/1zhou-Wang/MemVR
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion</title>
<link>https://arxiv.org/abs/2410.03825</link>
<guid>https://arxiv.org/abs/2410.03825</guid>
<content:encoded><![CDATA[
arXiv:2410.03825v2 Announce Type: replace 
Abstract: Estimating geometry from dynamic scenes, where objects move and deform over time, remains a core challenge in computer vision. Current approaches often rely on multi-stage pipelines or global optimizations that decompose the problem into subtasks, like depth and flow, leading to complex systems prone to errors. In this paper, we present Motion DUSt3R (MonST3R), a novel geometry-first approach that directly estimates per-timestep geometry from dynamic scenes. Our key insight is that by simply estimating a pointmap for each timestep, we can effectively adapt DUST3R's representation, previously only used for static scenes, to dynamic scenes. However, this approach presents a significant challenge: the scarcity of suitable training data, namely dynamic, posed videos with depth labels. Despite this, we show that by posing the problem as a fine-tuning task, identifying several suitable datasets, and strategically training the model on this limited data, we can surprisingly enable the model to handle dynamics, even without an explicit motion representation. Based on this, we introduce new optimizations for several downstream video-specific tasks and demonstrate strong performance on video depth and camera pose estimation, outperforming prior work in terms of robustness and efficiency. Moreover, MonST3R shows promising results for primarily feed-forward 4D reconstruction.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneCraft: Layout-Guided 3D Scene Generation</title>
<link>https://arxiv.org/abs/2410.09049</link>
<guid>https://arxiv.org/abs/2410.09049</guid>
<content:encoded><![CDATA[
arXiv:2410.09049v3 Announce Type: replace 
Abstract: The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools. Although some pioneering methods have achieved automatic text-to-3D generation, they are generally limited to small-scale scenes with restricted control over the shape and texture. We introduce SceneCraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users. Central to our method is a rendering-based technique, which converts 3D semantic layouts into multi-view 2D proxy maps. Furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (NeRF) as the final scene representation. Without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts. Through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality. Code and more results are available at: https://orangesodahub.github.io/SceneCraft
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysFlow: Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation</title>
<link>https://arxiv.org/abs/2411.14423</link>
<guid>https://arxiv.org/abs/2411.14423</guid>
<content:encoded><![CDATA[
arXiv:2411.14423v4 Announce Type: replace 
Abstract: Realistic simulation of dynamic scenes requires accurately capturing diverse material properties and modeling complex object interactions grounded in physical principles. However, existing methods are constrained to basic material types with limited predictable parameters, making them insufficient to represent the complexity of real-world materials. We introduce PhysFlow, a novel approach that leverages multi-modal foundation models and video diffusion to achieve enhanced 4D dynamic scene simulation. Our method utilizes multi-modal models to identify material types and initialize material parameters through image queries, while simultaneously inferring 3D Gaussian splats for detailed scene representation. We further refine these material parameters using video diffusion with a differentiable Material Point Method (MPM) and optical flow guidance rather than render loss or Score Distillation Sampling (SDS) loss. This integrated framework enables accurate prediction and realistic simulation of dynamic interactions in real-world scenarios, advancing both accuracy and flexibility in physics-based simulations.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum Learning</title>
<link>https://arxiv.org/abs/2412.00626</link>
<guid>https://arxiv.org/abs/2412.00626</guid>
<content:encoded><![CDATA[
arXiv:2412.00626v2 Announce Type: replace 
Abstract: Harnessing low-light enhancement and domain adaptation, nighttime UAV tracking has made substantial strides. However, over-reliance on image enhancement, limited high-quality nighttime data, and a lack of integration between daytime and nighttime trackers hinder the development of an end-to-end trainable framework. Additionally, current ViT-based trackers demand heavy computational resources due to their reliance on the self-attention mechanism. In this paper, we propose a novel pure Mamba-based tracking framework (MambaNUT) that employs a state space model with linear complexity as its backbone, incorporating a single-stream architecture that integrates feature learning and template-search coupling within Vision Mamba. We introduce an adaptive curriculum learning (ACL) approach that dynamically adjusts sampling strategies and loss weights, thereby improving the model's ability of generalization. Our ACL is composed of two levels of curriculum schedulers: (1) sampling scheduler that transforms the data distribution from imbalanced to balanced, as well as from easier (daytime) to harder (nighttime) samples; (2) loss scheduler that dynamically assigns weights based on the size of the training data and IoU of individual instances. Exhaustive experiments on multiple nighttime UAV tracking benchmarks demonstrate that the proposed MambaNUT achieves state-of-the-art performance while requiring lower computational costs. The code will be available at https://github.com/wuyou3474/MambaNUT.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expanding Event Modality Applications through a Robust CLIP-Based Encoder</title>
<link>https://arxiv.org/abs/2412.03093</link>
<guid>https://arxiv.org/abs/2412.03093</guid>
<content:encoded><![CDATA[
arXiv:2412.03093v2 Announce Type: replace 
Abstract: This paper introduces a powerful encoder that transfers CLIP`s capabilities to event-based data, enhancing its utility and expanding its applicability across diverse domains. While large-scale datasets have significantly advanced image-based models, the scarcity of comprehensive event datasets has limited performance potential in event modality. To address this challenge, we adapt CLIP`s architecture to align event embeddings with image embeddings, supporting zero-shot learning and preserving text alignment while mitigating catastrophic forgetting. Our encoder achieves strong performance in object recognition, with competitive results in zero-shot and few-shot learning tasks. Notably, it generalizes effectively to events extracted from video data without requiring additional training, highlighting its versatility. Additionally, we integrate this encoder within a cross-modality framework that facilitates interaction across five modalities-Image, Event, Text, Sound, and Depth-expanding the possibilities for cross-modal applications. Overall, this work underscores the transformative potential of a robust event encoder, broadening the scope and utility of event-based data across various fields.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2412.07825</link>
<guid>https://arxiv.org/abs/2412.07825</guid>
<content:encoded><![CDATA[
arXiv:2412.07825v3 Announce Type: replace 
Abstract: 3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop a comprehensive understanding of the 3D scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning capabilities by balancing the data distribution and adopting a novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark a wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. Our 3DSRBench provide valuable findings and insights about the future development of LMMs with strong 3D reasoning capabilities. Our project page and dataset is available https://3dsrbench.github.io.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Transformers for Efficient Indoor Pathloss Radio Map Prediction</title>
<link>https://arxiv.org/abs/2412.09507</link>
<guid>https://arxiv.org/abs/2412.09507</guid>
<content:encoded><![CDATA[
arXiv:2412.09507v2 Announce Type: replace 
Abstract: Indoor pathloss prediction is a fundamental task in wireless network planning, yet it remains challenging due to environmental complexity and data scarcity. In this work, we propose a deep learning-based approach utilizing a vision transformer (ViT) architecture with DINO-v2 pretrained weights to model indoor radio propagation. Our method processes a floor map with additional features of the walls to generate indoor pathloss maps. We systematically evaluate the effects of architectural choices, data augmentation strategies, and feature engineering techniques. Our findings indicate that extensive augmentation significantly improves generalization, while feature engineering is crucial in low-data regimes. Through comprehensive experiments, we demonstrate the robustness of our model across different generalization scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interact with me: Joint Egocentric Forecasting of Intent to Interact, Attitude and Social Actions</title>
<link>https://arxiv.org/abs/2412.16698</link>
<guid>https://arxiv.org/abs/2412.16698</guid>
<content:encoded><![CDATA[
arXiv:2412.16698v3 Announce Type: replace 
Abstract: For efficient human-agent interaction, an agent should proactively recognize their target user and prepare for upcoming interactions. We formulate this challenging problem as the novel task of jointly forecasting a person's intent to interact with the agent, their attitude towards the agent and the action they will perform, from the agent's (egocentric) perspective. So we propose \emph{SocialEgoNet} - a graph-based spatiotemporal framework that exploits task dependencies through a hierarchical multitask learning approach. SocialEgoNet uses whole-body skeletons (keypoints from face, hands and body) extracted from only 1 second of video input for high inference speed. For evaluation, we augment an existing egocentric human-agent interaction dataset with new class labels and bounding box annotations. Extensive experiments on this augmented dataset, named JPL-Social, demonstrate \emph{real-time} inference and superior performance (average accuracy across all tasks: 83.15\%) of our model outperforming several competitive baselines. The additional annotations and code will be available upon acceptance.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balanced 3DGS: Gaussian-wise Parallelism Rendering with Fine-Grained Tiling</title>
<link>https://arxiv.org/abs/2412.17378</link>
<guid>https://arxiv.org/abs/2412.17378</guid>
<content:encoded><![CDATA[
arXiv:2412.17378v4 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) is increasingly attracting attention in both academia and industry owing to its superior visual quality and rendering speed. However, training a 3DGS model remains a time-intensive task, especially in load imbalance scenarios where workload diversity among pixels and Gaussian spheres causes poor renderCUDA kernel performance. We introduce Balanced 3DGS, a Gaussian-wise parallelism rendering with fine-grained tiling approach in 3DGS training process, perfectly solving load-imbalance issues. First, we innovatively introduce the inter-block dynamic workload distribution technique to map workloads to Streaming Multiprocessor(SM) resources within a single GPU dynamically, which constitutes the foundation of load balancing. Second, we are the first to propose the Gaussian-wise parallel rendering technique to significantly reduce workload divergence inside a warp, which serves as a critical component in addressing load imbalance. Based on the above two methods, we further creatively put forward the fine-grained combined load balancing technique to uniformly distribute workload across all SMs, which boosts the forward renderCUDA kernel performance by up to 7.52x. Besides, we present a self-adaptive render kernel selection strategy during the 3DGS training process based on different load-balance situations, which effectively improves training efficiency.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quaternionic Reweighted Amplitude Flow for Phase Retrieval in Image Reconstruction</title>
<link>https://arxiv.org/abs/2501.02180</link>
<guid>https://arxiv.org/abs/2501.02180</guid>
<content:encoded><![CDATA[
arXiv:2501.02180v2 Announce Type: replace 
Abstract: Quaternionic signal processing provides powerful tools for efficiently managing color signals by preserving the intrinsic correlations among signal dimensions through quaternion algebra. In this paper, we address the quaternionic phase retrieval problem by systematically developing novel algorithms based on an amplitude-based model. Specifically, we propose the Quaternionic Reweighted Amplitude Flow (QRAF) algorithm, which is further enhanced by three of its variants: incremental, accelerated, and adapted QRAF algorithms. In addition, we introduce the Quaternionic Perturbed Amplitude Flow (QPAF) algorithm, which has linear convergence. Extensive numerical experiments on both synthetic data and real images, demonstrate that our proposed methods significantly improve recovery performance and computational efficiency compared to state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Texture Image Synthesis Using Spatial GAN Based on Vision Transformers</title>
<link>https://arxiv.org/abs/2502.01842</link>
<guid>https://arxiv.org/abs/2502.01842</guid>
<content:encoded><![CDATA[
arXiv:2502.01842v2 Announce Type: replace 
Abstract: Texture synthesis is a fundamental task in computer vision, whose goal is to generate visually realistic and structurally coherent textures for a wide range of applications, from graphics to scientific simulations. While traditional methods like tiling and patch-based techniques often struggle with complex textures, recent advancements in deep learning have transformed this field. In this paper, we propose ViT-SGAN, a new hybrid model that fuses Vision Transformers (ViTs) with a Spatial Generative Adversarial Network (SGAN) to address the limitations of previous methods. By incorporating specialized texture descriptors such as mean-variance (mu, sigma) and textons into the self-attention mechanism of ViTs, our model achieves superior texture synthesis. This approach enhances the model's capacity to capture complex spatial dependencies, leading to improved texture quality that is superior to state-of-the-art models, especially for regular and irregular textures. Comparison experiments with metrics such as FID, IS, SSIM, and LPIPS demonstrate the substantial improvement of ViT-SGAN, which underlines its efficiency in generating diverse realistic textures.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with Saliency Maps</title>
<link>https://arxiv.org/abs/2502.08821</link>
<guid>https://arxiv.org/abs/2502.08821</guid>
<content:encoded><![CDATA[
arXiv:2502.08821v2 Announce Type: replace 
Abstract: The recent surge in advanced generative models, such as diffusion models and generative adversarial networks (GANs), has led to an alarming rise in AI-generated images across various domains on the web. While such technologies offer benefits such as democratizing artistic creation, they also pose challenges in misinformation, digital forgery, and authenticity verification. Additionally, the uncredited use of AI-generated images in media and marketing has sparked significant backlash from online communities. In response to this, we introduce DejAIvu, a Chrome Web extension that combines real-time AI-generated image detection with saliency-based explainability while users browse the web. Using an ONNX-optimized deep learning model, DejAIvu automatically analyzes images on websites such as Google Images, identifies AI-generated content using model inference, and overlays a saliency heatmap to highlight AI-related artifacts. Our approach integrates efficient in-browser inference, gradient-based saliency analysis, and a seamless user experience, ensuring that AI detection is both transparent and interpretable. We also evaluate DejAIvu across multiple pretrained architectures and benchmark datasets, demonstrating high accuracy and low latency, making it a practical and deployable tool for enhancing AI image accountability. The code for this system can be found at https://github.com/Noodulz/dejAIvu.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIVS: A Pluralistic Alignment Dataset for Inclusive Public Spaces</title>
<link>https://arxiv.org/abs/2503.01894</link>
<guid>https://arxiv.org/abs/2503.01894</guid>
<content:encoded><![CDATA[
arXiv:2503.01894v2 Announce Type: replace 
Abstract: We introduce the Local Intersectional Visual Spaces (LIVS) dataset, a benchmark for multi-criteria alignment, developed through a two-year participatory process with 30 community organizations to support the pluralistic alignment of text-to-image (T2I) models in inclusive urban planning. The dataset encodes 37,710 pairwise comparisons across 13,462 images, structured along six criteria - Accessibility, Safety, Comfort, Invitingness, Inclusivity, and Diversity - derived from 634 community-defined concepts. Using Direct Preference Optimization (DPO), we fine-tune Stable Diffusion XL to reflect multi-criteria spatial preferences and evaluate the LIVS dataset and the fine-tuned model through four case studies: (1) DPO increases alignment with annotated preferences, particularly when annotation volume is high; (2) preference patterns vary across participant identities, underscoring the need for intersectional data; (3) human-authored prompts generate more distinctive visual outputs than LLM-generated ones, influencing annotation decisiveness; and (4) intersectional groups assign systematically different ratings across criteria, revealing the limitations of single-objective alignment. While DPO improves alignment under specific conditions, the prevalence of neutral ratings indicates that community values are heterogeneous and often ambiguous. LIVS provides a benchmark for developing T2I models that incorporate local, stakeholder-driven preferences, offering a foundation for context-aware alignment in spatial design.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Video Super-Resolution: Towards Diffusion-Based Methods without Motion Alignment</title>
<link>https://arxiv.org/abs/2503.03355</link>
<guid>https://arxiv.org/abs/2503.03355</guid>
<content:encoded><![CDATA[
arXiv:2503.03355v4 Announce Type: replace 
Abstract: In this work, we rethink the approach to video super-resolution by introducing a method based on the Diffusion Posterior Sampling framework, combined with an unconditional video diffusion transformer operating in latent space. The video generation model, a diffusion transformer, functions as a space-time model. We argue that a powerful model, which learns the physics of the real world, can easily handle various kinds of motion patterns as prior knowledge, thus eliminating the need for explicit estimation of optical flows or motion parameters for pixel alignment. Furthermore, a single instance of the proposed video diffusion transformer model can adapt to different sampling conditions without re-training. Empirical results on synthetic and real-world datasets illustrate the feasibility of diffusion-based, alignment-free video super-resolution.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2503.05423</link>
<guid>https://arxiv.org/abs/2503.05423</guid>
<content:encoded><![CDATA[
arXiv:2503.05423v2 Announce Type: replace 
Abstract: Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn from distinct categories without retaining exemplars but easily suffers from catastrophic forgetting of learned knowledge. While existing EFCIL methods leverage knowledge distillation to alleviate forgetting, they still face two critical challenges: semantic shift and decision bias. Specifically, the embeddings of old tasks shift in the embedding space after learning new tasks, and the classifier becomes biased towards new tasks due to training solely with new data, thereby hindering the balance between old and new knowledge. To address these issues, we propose the Dual-Projection Shift Estimation and Classifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates semantic shift through a dual-projection, which combines a learnable transformation with a row-space projection to capture both task-wise and category-wise shifts. Furthermore, to mitigate decision bias, DPCR employs ridge regression to reformulate classifier training as a reconstruction process. This reconstruction exploits previous information encoded in covariance and prototype of each class after calibration with estimated shift, thereby reducing decision bias. Extensive experiments demonstrate that, across various datasets, DPCR effectively balances old and new tasks, outperforming state-of-the-art EFCIL methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Quantitative Evaluation of the Expressivity of BMI, Pose and Gender in Body Embeddings for Recognition and Identification</title>
<link>https://arxiv.org/abs/2503.06451</link>
<guid>https://arxiv.org/abs/2503.06451</guid>
<content:encoded><![CDATA[
arXiv:2503.06451v3 Announce Type: replace 
Abstract: Person Re-identification (ReID) systems that match individuals across images or video frames are essential in many real-world applications. However, existing methods are often influenced by attributes such as gender, pose, and body mass index (BMI), which vary in unconstrained settings and raise concerns related to fairness and generalization. To address this, we extend the notion of expressivity, defined as the mutual information between learned features and specific attributes, using a secondary neural network to quantify how strongly attributes are encoded. Applying this framework to three ReID models, we find that BMI consistently shows the highest expressivity in the final layers, indicating its dominant role in recognition. In the last attention layer, attributes are ranked as BMI > Pitch > Gender > Yaw, revealing their relative influences in representation learning. Expressivity values also evolve across layers and training epochs, reflecting a dynamic encoding of attributes. These findings demonstrate the central role of body attributes in ReID and establish a principled approach for uncovering attribute driven correlations.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Multimodal Large Language Models Handle Complex Multimodal Reasoning? Placing Them in An Extensible Escape Game</title>
<link>https://arxiv.org/abs/2503.10042</link>
<guid>https://arxiv.org/abs/2503.10042</guid>
<content:encoded><![CDATA[
arXiv:2503.10042v2 Announce Type: replace 
Abstract: The rapid advancing of Multimodal Large Language Models (MLLMs) has spurred interest in complex multimodal reasoning tasks in the real-world and virtual environment, which require coordinating multiple abilities, including visual perception, visual reasoning, spatial awareness, and target deduction. However, existing evaluations primarily assess the final task completion, often degrading assessments to isolated abilities such as visual grounding and visual question answering. Less attention is given to comprehensively and quantitatively analyzing reasoning process in multimodal environments, which is crucial for understanding model behaviors and underlying reasoning mechanisms beyond merely task success. To address this, we introduce MM-Escape, an extensible benchmark for investigating multimodal reasoning, inspired by real-world escape games. MM-Escape emphasizes intermediate model behaviors alongside final task completion. To achieve this, we develop EscapeCraft, a customizable and open environment that enables models to engage in free-form exploration for assessing multimodal reasoning. Extensive experiments show that MLLMs, regardless of scale, can successfully complete the simplest room escape tasks, with some exhibiting human-like exploration strategies. Yet, performance dramatically drops as task difficulty increases. Moreover, we observe that performance bottlenecks vary across models, revealing distinct failure modes and limitations in their multimodal reasoning abilities, such as repetitive trajectories without adaptive exploration, getting stuck in corners due to poor visual spatial awareness, and ineffective use of acquired props, such as the key. We hope our work sheds light on new challenges in multimodal reasoning, and uncovers potential improvements in MLLMs capabilities.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaskAttn-UNet: A Mask Attention-Driven Framework for Universal Low-Resolution Image Segmentation</title>
<link>https://arxiv.org/abs/2503.10686</link>
<guid>https://arxiv.org/abs/2503.10686</guid>
<content:encoded><![CDATA[
arXiv:2503.10686v2 Announce Type: replace 
Abstract: Low-resolution image segmentation is crucial in real-world applications such as robotics, augmented reality, and large-scale scene understanding, where high-resolution data is often unavailable due to computational constraints. To address this challenge, we propose MaskAttn-UNet, a novel segmentation framework that enhances the traditional U-Net architecture via a mask attention mechanism. Our model selectively emphasizes important regions while suppressing irrelevant backgrounds, thereby improving segmentation accuracy in cluttered and complex scenes. Unlike conventional U-Net variants, MaskAttn-UNet effectively balances local feature extraction with broader contextual awareness, making it particularly well-suited for low-resolution inputs. We evaluate our approach on three benchmark datasets with input images rescaled to 128x128 and demonstrate competitive performance across semantic, instance, and panoptic segmentation tasks. Our results show that MaskAttn-UNet achieves accuracy comparable to state-of-the-art methods at significantly lower computational cost than transformer-based models, making it an efficient and scalable solution for low-resolution segmentation in resource-constrained scenarios.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search is All You Need for Few-shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2504.11895</link>
<guid>https://arxiv.org/abs/2504.11895</guid>
<content:encoded><![CDATA[
arXiv:2504.11895v2 Announce Type: replace 
Abstract: Few-shot anomaly detection (FSAD) has emerged as a crucial yet challenging task in industrial inspection, where normal distribution modeling must be accomplished with only a few normal images. While existing approaches typically employ multi-modal foundation models combining language and vision modalities for prompt-guided anomaly detection, these methods often demand sophisticated prompt engineering and extensive manual tuning. In this paper, we demonstrate that a straightforward nearest-neighbor search framework can surpass state-of-the-art performance in both single-class and multi-class FSAD scenarios. Our proposed method, VisionAD, consists of four simple yet essential components: (1) scalable vision foundation models that extract universal and discriminative features; (2) dual augmentation strategies - support augmentation to enhance feature matching adaptability and query augmentation to address the oversights of single-view prediction; (3) multi-layer feature integration that captures both low-frequency global context and high-frequency local details with minimal computational overhead; and (4) a class-aware visual memory bank enabling efficient one-for-all multi-class detection. Extensive evaluations across MVTec-AD, VisA, and Real-IAD benchmarks demonstrate VisionAD's exceptional performance. Using only 1 normal images as support, our method achieves remarkable image-level AUROC scores of 97.4%, 94.8%, and 70.8% respectively, outperforming current state-of-the-art approaches by significant margins (+1.6%, +3.2%, and +1.4%). The training-free nature and superior few-shot capabilities of VisionAD make it particularly appealing for real-world applications where samples are scarce or expensive to obtain. Code is available at https://github.com/Qiqigeww/VisionAD.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections</title>
<link>https://arxiv.org/abs/2504.16612</link>
<guid>https://arxiv.org/abs/2504.16612</guid>
<content:encoded><![CDATA[
arXiv:2504.16612v2 Announce Type: replace 
Abstract: Purpose: In this study, we investigate the training of foundation models using federated learning to address data-sharing limitations and enable collaborative model training without data transfer for minimally invasive surgery. Methods: Inspired by the EndoViT study, we adapt the Masked Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is pretrained on the Endo700k dataset collection and later fine-tuned and evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition, and Surgical Phase Recognition. Results: Our findings demonstrate that integrating adaptive FedSAM into the federated MAE approach improves pretraining, leading to a reduction in reconstruction loss per patch. The application of FL-EndoViT in surgical downstream tasks results in performance comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over CEN-EndoViT in surgical scene segmentation when data is limited and in action triplet recognition when large datasets are used. Conclusion: These findings highlight the potential of federated learning for privacy-preserving training of surgical foundation models, offering a robust and generalizable solution for surgical data science. Effective collaboration requires adapting federated learning methods, such as the integration of FedSAM, which can accommodate the inherent data heterogeneity across institutions. In future, exploring FL in video-based models may enhance these capabilities by incorporating spatiotemporal dynamics crucial for real-world surgical environments.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointBA: Towards Backdoor Attacks in 3D Point Cloud</title>
<link>https://arxiv.org/abs/2103.16074</link>
<guid>https://arxiv.org/abs/2103.16074</guid>
<content:encoded><![CDATA[
arXiv:2103.16074v4 Announce Type: replace-cross 
Abstract: 3D deep learning has been increasingly more popular for a variety of tasks including many safety-critical applications. However, recently several works raise the security issues of 3D deep models. Although most of them consider adversarial attacks, we identify that backdoor attack is indeed a more serious threat to 3D deep learning systems but remains unexplored. We present the backdoor attacks in 3D point cloud with a unified framework that exploits the unique properties of 3D data and networks. In particular, we design two attack approaches on point cloud: the poison-label backdoor attack (PointPBA) and the clean-label backdoor attack (PointCBA). The first one is straightforward and effective in practice, while the latter is more sophisticated assuming there are certain data inspections. The attack algorithms are mainly motivated and developed by 1) the recent discovery of 3D adversarial samples suggesting the vulnerability of deep models under spatial transformation; 2) the proposed feature disentanglement technique that manipulates the feature of the data through optimization methods and its potential to embed a new task. Extensive experiments show the efficacy of the PointPBA with over 95% success rate across various 3D datasets and models, and the more stealthy PointCBA with around 50% success rate. Our proposed backdoor attack in 3D point cloud is expected to perform as a baseline for improving the robustness of 3D deep models.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An automated end-to-end deep learning-based framework for lung cancer diagnosis by detecting and classifying the lung nodules</title>
<link>https://arxiv.org/abs/2305.00046</link>
<guid>https://arxiv.org/abs/2305.00046</guid>
<content:encoded><![CDATA[
arXiv:2305.00046v2 Announce Type: replace-cross 
Abstract: Lung cancer is a leading cause of cancer-related deaths worldwide, and early detection is crucial for improving patient outcomes. Nevertheless, early diagnosis of cancer is a major challenge, particularly in low-resource settings where access to medical resources and trained radiologists is limited. The objective of this study is to propose an automated end-to-end deep learning-based framework for the early detection and classification of lung nodules, specifically for low-resource settings. The proposed framework consists of three stages: lung segmentation using a modified 3D U-Net named 3D Res-U-Net, nodule detection using YOLO-v5, and classification with a Vision Transformer-based architecture. We evaluated the proposed framework on a publicly available dataset, LUNA16. The proposed framework's performance was measured using the respective domain's evaluation matrices. The proposed framework achieved a 98.82% lung segmentation dice score while detecting the lung nodule with 0.76 mAP@50 from the segmented lung, at a low false-positive rate. The performance of both networks of the proposed framework was compared with other studies and found to outperform them regarding segmentation and detection accuracy. Additionally, our proposed Vision transformer network obtained an accuracy of 93.57%, which is 1.21% higher than the state-of-the-art networks. Our proposed end-to-end deep learning-based framework can effectively segment lungs, and detect and classify lung nodules, specifically in low-resource settings with limited access to radiologists. The proposed framework outperforms existing studies regarding all the respective evaluation metrics. The proposed framework can potentially improve the accuracy and efficiency of lung cancer screening in low-resource settings, ultimately leading to better patient outcomes.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Free Discontinuity Regression: With an Application to the Economic Effects of Internet Shutdowns</title>
<link>https://arxiv.org/abs/2309.14630</link>
<guid>https://arxiv.org/abs/2309.14630</guid>
<content:encoded><![CDATA[
arXiv:2309.14630v3 Announce Type: replace-cross 
Abstract: Sharp, multidimensional changepoints-abrupt shifts in a regression surface whose locations and magnitudes are unknown-arise in settings as varied as gene-expression profiling, financial covariance breaks, climate-regime detection, and urban socioeconomic mapping. Despite their prevalence, there are no current approaches that jointly estimate the location and size of the discontinuity set in a one-shot approach with statistical guarantees. We therefore introduce Free Discontinuity Regression (FDR), a fully nonparametric estimator that simultaneously (i) smooths a regression surface, (ii) segments it into contiguous regions, and (iii) provably recovers the precise locations and sizes of its jumps. By extending a convex relaxation of the Mumford-Shah functional to random spatial sampling and correlated noise, FDR overcomes the fixed-grid and i.i.d. noise assumptions of classical image-segmentation approaches, thus enabling its application to real-world data of any dimension. This yields the first identification and uniform consistency results for multivariate jump surfaces: under mild SBV regularity, the estimated function, its discontinuity set, and all jump sizes converge to their true population counterparts. Hyperparameters are selected automatically from the data using Stein's Unbiased Risk Estimate, and large-scale simulations up to three dimensions validate the theoretical results and demonstrate good finite-sample performance. Applying FDR to an internet shutdown in India reveals a 25-35% reduction in economic activity around the estimated shutdown boundaries-much larger than previous estimates. By unifying smoothing, segmentation, and effect-size recovery in a general statistical setting, FDR turns free-discontinuity ideas into a practical tool with formal guarantees for modern multivariate data.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Deep Learning Models for Breast Cancer Classification: A Comparative Study</title>
<link>https://arxiv.org/abs/2408.16859</link>
<guid>https://arxiv.org/abs/2408.16859</guid>
<content:encoded><![CDATA[
arXiv:2408.16859v2 Announce Type: replace-cross 
Abstract: This study evaluates the effectiveness of deep learning models in classifying histopathological images for early and accurate detection of breast cancer. Eight advanced models, including ResNet-50, DenseNet-121, ResNeXt-50, Vision Transformer (ViT), GoogLeNet (Inception v3), EfficientNet, MobileNet, and SqueezeNet, were compared using a dataset of 277,524 image patches. The Vision Transformer (ViT) model, with its attention-based mechanisms, achieved the highest validation accuracy of 94%, outperforming conventional CNNs. The study demonstrates the potential of advanced machine learning methods to enhance precision and efficiency in breast cancer diagnosis in clinical settings.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A nonlinear elasticity model in computer vision</title>
<link>https://arxiv.org/abs/2408.17237</link>
<guid>https://arxiv.org/abs/2408.17237</guid>
<content:encoded><![CDATA[
arXiv:2408.17237v3 Announce Type: replace-cross 
Abstract: The purpose of this paper is to analyze a nonlinear elasticity model introduced by the authors for comparing two images, regarded as bounded open subsets of $\R^n$ together with associated vector-valued intensity maps. Optimal transformations between the images are sought as minimisers of an integral functional among orientation-preserving homeomorphisms. The existence of minimisers is proved under natural coercivity and polyconvexity conditions, assuming only that the intensity functions are bounded measurable. Variants of the existence theorem are also proved, first under the constraint that finite sets of landmark points in the two images are mapped one to the other, and second when one image is to be compared to an unknown part of another.
  The question is studied as to whether for images related by an affine mapping the unique minimiser is given by that affine mapping. For a natural class of functional integrands an example is given guaranteeing that this property holds for pairs of images in which the second is a scaling of the first by a constant factor. However for the property to hold for arbitrary pairs of affinely related images it is shown that the integrand has to depend on the gradient of the transformation as a convex function of its determinant alone. This suggests a new model in which the integrand depends also on second derivatives of the transformation, and an example is given for which both existence of minimisers is assured and the above property holds for all pairs of affinely related images.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HESSO: Towards Automatic Efficient and User Friendly Any Neural Network Training and Pruning</title>
<link>https://arxiv.org/abs/2409.09085</link>
<guid>https://arxiv.org/abs/2409.09085</guid>
<content:encoded><![CDATA[
arXiv:2409.09085v2 Announce Type: replace-cross 
Abstract: Structured pruning is one of the most popular approaches to effectively compress the heavy deep neural networks (DNNs) into compact sub-networks while retaining performance. The existing methods suffer from multi-stage procedures along with significant engineering efforts and human expertise. The Only-Train-Once (OTO) series has been recently proposed to resolve the many pain points by streamlining the workflow by automatically conducting (i) search space generation, (ii) structured sparse optimization, and (iii) sub-network construction. However, the built-in sparse optimizers in the OTO series, i.e., the Half-Space Projected Gradient (HSPG) family, have limitations that require hyper-parameter tuning and the implicit controls of the sparsity exploration, consequently requires intervening by human expertise. To address such limitations, we propose a Hybrid Efficient Structured Sparse Optimizer (HESSO). HESSO could automatically and efficiently train a DNN to produce a high-performing subnetwork. Meanwhile, it is almost tuning-free and enjoys user-friendly integration for generic training applications. To address another common issue of irreversible performance collapse observed in pruning DNNs, we further propose a Corrective Redundant Identification Cycle (CRIC) for reliably identifying indispensable structures. We numerically demonstrate the efficacy of HESSO and its enhanced version HESSO-CRIC on a variety of applications ranging from computer vision to natural language processing, including large language model. The numerical results showcase that HESSO can achieve competitive even superior performance to varying state-of-the-arts and support most DNN architectures. Meanwhile, CRIC can effectively prevent the irreversible performance collapse and further enhance the performance of HESSO on certain applications.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CloudTrack: Scalable UAV Tracking with Cloud Semantics</title>
<link>https://arxiv.org/abs/2409.16111</link>
<guid>https://arxiv.org/abs/2409.16111</guid>
<content:encoded><![CDATA[
arXiv:2409.16111v3 Announce Type: replace-cross 
Abstract: Nowadays, unmanned aerial vehicles (UAVs) are commonly used in search and rescue scenarios to gather information in the search area. The automatic identification of the person searched for in aerial footage could increase the autonomy of such systems, reduce the search time, and thus increase the missed person's chances of survival. In this paper, we present a novel approach to perform semantically conditioned open vocabulary object tracking that is specifically designed to cope with the limitations of UAV hardware. Our approach has several advantages. It can run with verbal descriptions of the missing person, e.g., the color of the shirt, it does not require dedicated training to execute the mission and can efficiently track a potentially moving person. Our experimental results demonstrate the versatility and efficacy of our approach.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines</title>
<link>https://arxiv.org/abs/2410.12705</link>
<guid>https://arxiv.org/abs/2410.12705</guid>
<content:encoded><![CDATA[
arXiv:2410.12705v5 Announce Type: replace-cross 
Abstract: Vision Language Models (VLMs) often struggle with culture-specific knowledge, particularly in languages other than English and in underrepresented cultural contexts. To evaluate their understanding of such knowledge, we introduce WorldCuisines, a massive-scale benchmark for multilingual and multicultural, visually grounded language understanding. This benchmark includes a visual question answering (VQA) dataset with text-image pairs across 30 languages and dialects, spanning 9 language families and featuring over 1 million data points, making it the largest multicultural VQA benchmark to date. It includes tasks for identifying dish names and their origins. We provide evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances). Our findings show that while VLMs perform better with correct location context, they struggle with adversarial contexts and predicting specific regional cuisines and languages. To support future research, we release a knowledge base with annotated food entries and images along with the VQA data.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Scale MRI Collection and Segmentation of Cirrhotic Liver</title>
<link>https://arxiv.org/abs/2410.16296</link>
<guid>https://arxiv.org/abs/2410.16296</guid>
<content:encoded><![CDATA[
arXiv:2410.16296v2 Announce Type: replace-cross 
Abstract: Liver cirrhosis represents the end stage of chronic liver disease, characterized by extensive fibrosis and nodular regeneration that significantly increases mortality risk. While magnetic resonance imaging (MRI) offers a non-invasive assessment, accurately segmenting cirrhotic livers presents substantial challenges due to morphological alterations and heterogeneous signal characteristics. Deep learning approaches show promise for automating these tasks, but progress has been limited by the absence of large-scale, annotated datasets. Here, we present CirrMRI600+, the first comprehensive dataset comprising 628 high-resolution abdominal MRI scans (310 T1-weighted and 318 T2-weighted sequences, totaling nearly 40,000 annotated slices) with expert-validated segmentation labels for cirrhotic livers. The dataset includes demographic information, clinical parameters, and histopathological validation where available. Additionally, we provide benchmark results from 11 state-of-the-art deep learning experiments to establish performance standards. CirrMRI600+ enables the development and validation of advanced computational methods for cirrhotic liver analysis, potentially accelerating progress toward automated Cirrhosis visual staging and personalized treatment planning.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Convolution-based Unlearnable Datasets</title>
<link>https://arxiv.org/abs/2411.01742</link>
<guid>https://arxiv.org/abs/2411.01742</guid>
<content:encoded><![CDATA[
arXiv:2411.01742v2 Announce Type: replace-cross 
Abstract: The construction of large datasets for deep learning has raised concerns regarding unauthorized use of online data, leading to increased interest in protecting data from third-parties who want to use it for training. The Convolution-based Unlearnable DAtaset (CUDA) method aims to make data unlearnable by applying class-wise blurs to every image in the dataset so that neural networks learn relations between blur kernels and labels, as opposed to informative features for classifying clean data. In this work, we evaluate whether CUDA data remains unlearnable after image sharpening and frequency filtering, finding that this combination of simple transforms improves the utility of CUDA data for training. In particular, we observe a substantial increase in test accuracy over adversarial training for models trained with CUDA unlearnable data from CIFAR-10, CIFAR-100, and ImageNet-100. In training models to high accuracy using unlearnable data, we underscore the need for ongoing refinement in data poisoning techniques to ensure data privacy. Our method opens new avenues for enhancing the robustness of unlearnable datasets by highlighting that simple methods such as sharpening and frequency filtering are capable of breaking convolution-based unlearnable datasets.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Object-Centric Instruction Following: Integrating Foundation Models with Traditional Navigation</title>
<link>https://arxiv.org/abs/2411.07848</link>
<guid>https://arxiv.org/abs/2411.07848</guid>
<content:encoded><![CDATA[
arXiv:2411.07848v3 Announce Type: replace-cross 
Abstract: Large scale scenes such as multifloor homes can be robustly and efficiently mapped with a 3D graph of landmarks estimated jointly with robot poses in a factor graph, a technique commonly used in commercial robots such as drones and robot vacuums. In this work, we propose Language-Inferred Factor Graph for Instruction Following (LIFGIF), a zero-shot method to ground natural language instructions in such a map. LIFGIF also includes a policy for following natural language navigation instructions in a novel environment while the map is constructed, enabling robust navigation performance in the physical world. To evaluate LIFGIF, we present a new dataset, Object-Centric VLN (OC-VLN), in order to evaluate grounding of object-centric natural language navigation instructions. We compare to two state-of-the-art zero-shot baselines from related tasks, Object Goal Navigation and Vision Language Navigation, to demonstrate that LIFGIF outperforms them across all our evaluation metrics on OCVLN. Finally, we successfully demonstrate the effectiveness of LIFGIF for performing zero-shot object-centric instruction following in the real world on a Boston Dynamics Spot robot.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LUDO: Low-Latency Understanding of Deformable Objects using Point Cloud Occupancy Functions</title>
<link>https://arxiv.org/abs/2411.08777</link>
<guid>https://arxiv.org/abs/2411.08777</guid>
<content:encoded><![CDATA[
arXiv:2411.08777v4 Announce Type: replace-cross 
Abstract: Accurately determining the shape of objects and the location of their internal structures within deformable objects is crucial for medical tasks that require precise targeting, such as robotic biopsies. We introduce LUDO, a method for accurate low-latency understanding of deformable objects. LUDO reconstructs objects in their deformed state, including their internal structures, from a single-view point cloud observation in under 30 ms using occupancy networks. LUDO provides uncertainty estimates for its predictions. Additionally, it provides explainability by highlighting key features in its input observations. Both uncertainty and explainability are important for safety-critical applications such as surgical interventions. We demonstrate LUDO's abilities for autonomous targeting of internal regions of interest (ROIs) in deformable objects. We evaluate LUDO in real-world robotic experiments, achieving a success rate of 98.9% for puncturing various ROIs inside deformable objects. LUDO demonstrates the potential to interact with deformable objects without the need for deformable registration methods.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AirMorph: Topology-Preserving Deep Learning for Pulmonary Airway Analysis</title>
<link>https://arxiv.org/abs/2412.11039</link>
<guid>https://arxiv.org/abs/2412.11039</guid>
<content:encoded><![CDATA[
arXiv:2412.11039v2 Announce Type: replace-cross 
Abstract: Accurate anatomical labeling and analysis of the pulmonary structure and its surrounding anatomy from thoracic CT is getting increasingly important for understanding the etilogy of abnormalities or supporting targetted therapy and early interventions. Whilst lung and airway cell atlases have been attempted, there is a lack of fine-grained morphological atlases that are clinically deployable. In this work, we introduce AirMorph, a robust, end-to-end deep learning pipeline enabling fully automatic and comprehensive airway anatomical labeling at lobar, segmental, and subsegmental resolutions that can be used to create digital atlases of the lung. Evaluated across large-scale multi-center datasets comprising diverse pulmonary conditions, the AirMorph consistently outperformed existing segmentation and labeling methods in terms of accuracy, topological consistency, and completeness. To simplify clinical interpretation, we further introduce a compact anatomical signature quantifying critical morphological airway features, including stenosis, ectasia, tortuosity, divergence, length, and complexity. When applied to various pulmonary diseases such as pulmonary fibrosis, emphysema, atelectasis, consolidation, and reticular opacities, it demonstrates strong discriminative power, revealing disease-specific morphological patterns with high interpretability and explainability. Additionally, AirMorph supports efficient automated branching pattern analysis, potentially enhancing bronchoscopic navigation planning and procedural safety, offering a valuable clinical tool for improved diagnosis, targeted treatment, and personalized patient care.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Rate Control for Deep Video Compression with Rate-Distortion Prediction</title>
<link>https://arxiv.org/abs/2412.18834</link>
<guid>https://arxiv.org/abs/2412.18834</guid>
<content:encoded><![CDATA[
arXiv:2412.18834v2 Announce Type: replace-cross 
Abstract: Deep video compression has made significant progress in recent years, achieving rate-distortion performance that surpasses that of traditional video compression methods. However, rate control schemes tailored for deep video compression have not been well studied. In this paper, we propose a neural network-based $\lambda$-domain rate control scheme for deep video compression, which determines the coding parameter $\lambda$ for each to-be-coded frame based on the rate-distortion-$\lambda$ (R-D-$\lambda$) relationships directly learned from uncompressed frames, achieving high rate control accuracy efficiently without the need for pre-encoding. Moreover, this content-aware scheme is able to mitigate inter-frame quality fluctuations and adapt to abrupt changes in video content. Specifically, we introduce two neural network-based predictors to estimate the relationship between bitrate and $\lambda$, as well as the relationship between distortion and $\lambda$ for each frame. Then we determine the coding parameter $\lambda$ for each frame to achieve the target bitrate. Experimental results demonstrate that our approach achieves high rate control accuracy at the mini-GOP level with low time overhead and mitigates inter-frame quality fluctuations across video content of varying resolutions.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FrontierNet: Learning Visual Cues to Explore</title>
<link>https://arxiv.org/abs/2501.04597</link>
<guid>https://arxiv.org/abs/2501.04597</guid>
<content:encoded><![CDATA[
arXiv:2501.04597v2 Announce Type: replace-cross 
Abstract: Exploration of unknown environments is crucial for autonomous robots; it allows them to actively reason and decide on what new data to acquire for different tasks, such as mapping, object discovery, and environmental assessment. Existing solutions, such as frontier-based exploration approaches, rely heavily on 3D map operations, which are limited by map quality and, more critically, often overlook valuable context from visual cues. This work aims at leveraging 2D visual cues for efficient autonomous exploration, addressing the limitations of extracting goal poses from a 3D map. We propose a visual-only frontier-based exploration system, with FrontierNet as its core component. FrontierNet is a learning-based model that (i) proposes frontiers, and (ii) predicts their information gain, from posed RGB images enhanced by monocular depth priors. Our approach provides an alternative to existing 3D-dependent goal-extraction approaches, achieving a 15\% improvement in early-stage exploration efficiency, as validated through extensive simulations and real-world experiments. The project is available at https://github.com/cvg/FrontierNet.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating AI for Human-Centric Breast Cancer Diagnostics: A Multi-Scale and Multi-View Swin Transformer Framework</title>
<link>https://arxiv.org/abs/2503.13309</link>
<guid>https://arxiv.org/abs/2503.13309</guid>
<content:encoded><![CDATA[
arXiv:2503.13309v2 Announce Type: replace-cross 
Abstract: Despite advancements in Computer-Aided Diagnosis (CAD) systems, breast cancer remains one of the leading causes of cancer-related deaths among women worldwide. Recent breakthroughs in Artificial Intelligence (AI) have shown significant promise in development of advanced Deep Learning (DL) architectures for breast cancer diagnosis through mammography. In this context, the paper focuses on the integration of AI within a Human-Centric workflow to enhance breast cancer diagnostics. Key challenges are, however, largely overlooked such as reliance on detailed tumor annotations and susceptibility to missing views, particularly during test time. To address these issues, we propose a hybrid, multi-scale and multi-view Swin Transformer-based framework (MSMV-Swin) that enhances diagnostic robustness and accuracy. The proposed MSMV-Swin framework is designed to work as a decision-support tool, helping radiologists analyze multi-view mammograms more effectively. More specifically, the MSMV-Swin framework leverages the Segment Anything Model (SAM) to isolate the breast lobe, reducing background noise and enabling comprehensive feature extraction. The multi-scale nature of the proposed MSMV-Swin framework accounts for tumor-specific regions as well as the spatial characteristics of tissues surrounding the tumor, capturing both localized and contextual information. The integration of contextual and localized data ensures that MSMV-Swin's outputs align with the way radiologists interpret mammograms, fostering better human-AI interaction and trust. A hybrid fusion structure is then designed to ensure robustness against missing views, a common occurrence in clinical practice when only a single mammogram view is available.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment</title>
<link>https://arxiv.org/abs/2504.08603</link>
<guid>https://arxiv.org/abs/2504.08603</guid>
<content:encoded><![CDATA[
arXiv:2504.08603v2 Announce Type: replace-cross 
Abstract: Geometrically accurate and semantically expressive map representations have proven invaluable to facilitate robust and safe mobile robot navigation and task planning. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments is still an open problem. In this paper we present FindAnything, an open-world mapping and exploration framework that incorporates vision-language information into dense volumetric submaps. Thanks to the use of vision-language features, FindAnything bridges the gap between pure geometric and open-vocabulary semantic information for a higher level of understanding while allowing to explore any environment without the help of any external source of ground-truth pose information. We represent the environment as a series of volumetric occupancy submaps, resulting in a robust and accurate map representation that deforms upon pose updates when the underlying SLAM system corrects its drift, allowing for a locally consistent representation between submaps. Pixel-wise vision-language features are aggregated from efficient SAM (eSAM)-generated segments, which are in turn integrated into object-centric volumetric submaps, providing a mapping from open-vocabulary queries to 3D geometry that is scalable also in terms of memory usage. The open-vocabulary map representation of FindAnything achieves state-of-the-art semantic accuracy in closed-set evaluations on the Replica dataset. This level of scene understanding allows a robot to explore environments based on objects or areas of interest selected via natural language queries. Our system is the first of its kind to be deployed on resource-constrained devices, such as MAVs, leveraging vision-language information for real-world robotic tasks.
]]></content:encoded>
<pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models</title>
<link>https://arxiv.org/abs/2505.03821</link>
<guid>https://arxiv.org/abs/2505.03821</guid>
<content:encoded><![CDATA[
<div> Vision Language Models, visual perspective taking, spatial configurations, humanoid minifigure, spatial reasoning <br />
Summary:<br />
The study investigates Vision Language Models' (VLMs) ability in visual perspective taking through novel visual tasks inspired by human tests. Controlled scenes with humanoid minifigures and objects were used to create 144 tasks with varying spatial configurations. Diagnostic questions assessed scene understanding, spatial reasoning, and visual perspective taking. State-of-the-art models like GPT-4-Turbo showed strong scene understanding but struggled with spatial reasoning and perspective-taking. The gap between object recognition and deeper reasoning suggests a need for geometric representations and specialized training in VLM development. <div>
arXiv:2505.03821v1 Announce Type: new 
Abstract: We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN &amp; BNN) and Digital Image Colorimetry</title>
<link>https://arxiv.org/abs/2505.03826</link>
<guid>https://arxiv.org/abs/2505.03826</guid>
<content:encoded><![CDATA[
<div> machine learning, semiconductor manufacturing, etch depth prediction, in-situ monitoring, digital image colorimetry

Summary:
- The study focuses on the precise monitoring of etch depth and insulating material thickness in semiconductor manufacturing using a non-contact, in-situ approach based on machine learning (ML) techniques. 
- Two scenarios are explored: the use of an artificial neural network (ANN) to predict etch depth from process parameters and a Bayesian Neural Network (BNN) to incorporate variability from repeated measurements. 
- The BNN was found to provide reliable uncertainty estimates, enhancing the accuracy of etch depth prediction. 
- Additionally, the feasibility of using RGB data from digital image colorimetry (DIC) for etch depth prediction was demonstrated, showing strong performance even without explicit process parameters. 
- The integration of DIC and ML offers a cost-effective alternative for real-time, in-situ, and non-invasive monitoring in plasma etching processes, contributing to improved process stability and manufacturing efficiency.

<br /><br />Summary: <div>
arXiv:2505.03826v1 Announce Type: new 
Abstract: Precise monitoring of etch depth and the thickness of insulating materials, such as Silicon dioxide and silicon nitride, is critical to ensuring device performance and yield in semiconductor manufacturing. While conventional ex-situ analysis methods are accurate, they are constrained by time delays and contamination risks. To address these limitations, this study proposes a non-contact, in-situ etch depth prediction framework based on machine learning (ML) techniques. Two scenarios are explored. In the first scenario, an artificial neural network (ANN) is trained to predict average etch depth from process parameters, achieving a significantly lower mean squared error (MSE) compared to a linear baseline model. The approach is then extended to incorporate variability from repeated measurements using a Bayesian Neural Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage analysis confirms the BNN's capability to provide reliable uncertainty estimates. In the second scenario, we demonstrate the feasibility of using RGB data from digital image colorimetry (DIC) as input for etch depth prediction, achieving strong performance even in the absence of explicit process parameters. These results suggest that the integration of DIC and ML offers a viable, cost-effective alternative for real-time, in-situ, and non-invasive monitoring in plasma etching processes, contributing to enhanced process stability, and manufacturing efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoLLM Benchmarks and Evaluation: A Survey</title>
<link>https://arxiv.org/abs/2505.03829</link>
<guid>https://arxiv.org/abs/2505.03829</guid>
<content:encoded><![CDATA[
<div> benchmarks, evaluation methodologies, Video Large Language Models, performance trends, challenges

Summary:
This survey explores benchmarks and evaluation methodologies for Video Large Language Models (VideoLLMs), assessing current benchmarks, evaluation protocols, and limitations. It delves into various evaluation methods like closed-set, open-set, temporal, and spatiotemporal understanding tasks. The survey also showcases the trends in state-of-the-art VideoLLMs' performance across benchmarks and identifies challenges within current evaluation frameworks. Future research directions are proposed to enhance benchmark design, evaluation metrics, and protocols, emphasizing the necessity for more diverse, multimodal, and interpretability-focused benchmarks. This analysis equips researchers with structured knowledge on effectively evaluating VideoLLMs and suggests promising avenues for video understanding advancement with large language models. <br /><br />Summary: <div>
arXiv:2505.03829v1 Announce Type: new 
Abstract: The rapid development of Large Language Models (LLMs) has catalyzed significant advancements in video understanding technologies. This survey provides a comprehensive analysis of benchmarks and evaluation methodologies specifically designed or used for Video Large Language Models (VideoLLMs). We examine the current landscape of video understanding benchmarks, discussing their characteristics, evaluation protocols, and limitations. The paper analyzes various evaluation methodologies, including closed-set, open-set, and specialized evaluations for temporal and spatiotemporal understanding tasks. We highlight the performance trends of state-of-the-art VideoLLMs across these benchmarks and identify key challenges in current evaluation frameworks. Additionally, we propose future research directions to enhance benchmark design, evaluation metrics, and protocols, including the need for more diverse, multimodal, and interpretability-focused benchmarks. This survey aims to equip researchers with a structured understanding of how to effectively evaluate VideoLLMs and identify promising avenues for advancing the field of video understanding with large language models.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video Forgery Detection for Surveillance Cameras: A Review</title>
<link>https://arxiv.org/abs/2505.03832</link>
<guid>https://arxiv.org/abs/2505.03832</guid>
<content:encoded><![CDATA[
<div> Forensic Techniques, Video Forgery, Surveillance Footage, Authenticity, Video Analysis<br />
<br />Summary: The article discusses the increasing accessibility of video recording technology and the potential for manipulation through advanced editing tools. Surveillance footage is crucial for security and judicial processes, but concerns about authenticity have risen due to the ease of tampering. Various forensic techniques, including compression-based analysis, frame duplication detection, and machine learning approaches, are reviewed for their effectiveness in detecting video forgery. The need for stronger forensic capabilities to combat evolving forgery methods is emphasized to maintain the credibility and admissibility of surveillance recordings as legal evidence. <div>
arXiv:2505.03832v1 Announce Type: new 
Abstract: The widespread availability of video recording through smartphones and digital devices has made video-based evidence more accessible than ever. Surveillance footage plays a crucial role in security, law enforcement, and judicial processes. However, with the rise of advanced video editing tools, tampering with digital recordings has become increasingly easy, raising concerns about their authenticity. Ensuring the integrity of surveillance videos is essential, as manipulated footage can lead to misinformation and undermine judicial decisions. This paper provides a comprehensive review of existing forensic techniques used to detect video forgery, focusing on their effectiveness in verifying the authenticity of surveillance recordings. Various methods, including compression-based analysis, frame duplication detection, and machine learning-based approaches, are explored. The findings highlight the growing necessity for more robust forensic techniques to counteract evolving forgery methods. Strengthening video forensic capabilities will ensure that surveillance recordings remain credible and admissible as legal evidence.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PointExplainer: Towards Transparent Parkinson's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2505.03833</link>
<guid>https://arxiv.org/abs/2505.03833</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, Parkinson's disease, PointExplainer, interpretability, diagnosis

Summary:
PointExplainer is a new explainable diagnostic strategy using deep neural networks to analyze hand-drawn signals for early diagnosis of Parkinson's disease. By assigning attribution values to hand-drawn segments, PointExplainer identifies the regions driving the model's diagnosis, providing clear interpretability. The system encodes hand-drawn signals into 3D point clouds and trains an interpretable surrogate model to approximate the black-box diagnostic model's behavior. Consistency measures ensure faithfulness in explanations. Extensive experiments on benchmark datasets show that PointExplainer offers intuitive explanations without sacrificing diagnostic performance. The source code is available for further exploration. <div>
arXiv:2505.03833v1 Announce Type: new 
Abstract: Deep neural networks have shown potential in analyzing digitized hand-drawn signals for early diagnosis of Parkinson's disease. However, the lack of clear interpretability in existing diagnostic methods presents a challenge to clinical trust. In this paper, we propose PointExplainer, an explainable diagnostic strategy to identify hand-drawn regions that drive model diagnosis. Specifically, PointExplainer assigns discrete attribution values to hand-drawn segments, explicitly quantifying their relative contributions to the model's decision. Its key components include: (i) a diagnosis module, which encodes hand-drawn signals into 3D point clouds to represent hand-drawn trajectories, and (ii) an explanation module, which trains an interpretable surrogate model to approximate the local behavior of the black-box diagnostic model. We also introduce consistency measures to further address the issue of faithfulness in explanations. Extensive experiments on two benchmark datasets and a newly constructed dataset show that PointExplainer can provide intuitive explanations with no diagnostic performance degradation. The source code is available at https://github.com/chaoxuewang/PointExplainer.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Face Recognition via Improved Localization</title>
<link>https://arxiv.org/abs/2505.03837</link>
<guid>https://arxiv.org/abs/2505.03837</guid>
<content:encoded><![CDATA[
<div> Keywords: Biometric authentication, Deep learning, Face recognition, Explainable AI, Scaled Directed Divergence

Summary: 
This paper discusses the importance of explainable face recognition systems in the age of artificial intelligence. While deep learning-based face recognition systems are widely used, the lack of transparency and justification for their decisions can lead to a lack of trust from users. To address this issue, the paper introduces a method called Scaled Directed Divergence (SDD) for explainable face recognition. By using Class Activation Mapping (CAM), the SDD technique allows for the fine localization of relevant face features used by the deep learning model for its predictions. The experiments demonstrate that the SDD Class Activation Map provides specific and accurate visual explanations compared to traditional CAM. By providing transparent and visually clear explanations with narrow localization of relevant features, deep learning-based face recognition systems can enhance transparency and trust among users. <div>
arXiv:2505.03837v1 Announce Type: new 
Abstract: Biometric authentication has become one of the most widely used tools in the current technological era to authenticate users and to distinguish between genuine users and imposters. Face is the most common form of biometric modality that has proven effective. Deep learning-based face recognition systems are now commonly used across different domains. However, these systems usually operate like black-box models that do not provide necessary explanations or justifications for their decisions. This is a major disadvantage because users cannot trust such artificial intelligence-based biometric systems and may not feel comfortable using them when clear explanations or justifications are not provided. This paper addresses this problem by applying an efficient method for explainable face recognition systems. We use a Class Activation Mapping (CAM)-based discriminative localization (very narrow/specific localization) technique called Scaled Directed Divergence (SDD) to visually explain the results of deep learning-based face recognition systems. We perform fine localization of the face features relevant to the deep learning model for its prediction/decision. Our experiments show that the SDD Class Activation Map (CAM) highlights the relevant face features very specifically compared to the traditional CAM and very accurately. The provided visual explanations with narrow localization of relevant features can ensure much-needed transparency and trust for deep learning-based face recognition systems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation</title>
<link>https://arxiv.org/abs/2505.03846</link>
<guid>https://arxiv.org/abs/2505.03846</guid>
<content:encoded><![CDATA[
<div> Graph-Augmented Multimodal Encoder, facial graph, Geo Two-Stream Network, Graph Convolutional Networks, Convolutional Neural Networks, attention mechanisms, ResNet18, VGGFace, BiGRU, VGGish, XLM-Roberta

Summary:
GAME, a Graph-Augmented Multimodal Encoder, is proposed to analyze personality from short videos. It combines visual, auditory, and textual cues through a facial graph and a dual-branch network incorporating Graph Convolutional Networks, Convolutional Neural Networks, and attention mechanisms for facial cue analysis. Temporal dynamics are captured by a BiGRU with temporal attention modules. Audio features come from the VGGish network, while linguistic semantics are derived from the XLM-Roberta transformer. A Channel Attention-based Fusion module integrates the multimodal features, followed by a Multi-Layer Perceptron for personality prediction. Experiments demonstrate GAME's superiority over existing methods across various benchmarks, confirming its efficacy and versatility.<br /><br />Summary:GAME, a Graph-Augmented Multimodal Encoder, integrates visual, auditory, and textual cues to predict personality traits from short videos. It utilizes advanced techniques such as dual-branch networks, attention mechanisms, and temporal dynamics modeling to extract facial, audio, and linguistic features. A Channel Attention-based Fusion module effectively integrates these features, leading to improved performance compared to existing methods.GAME demonstrates superior performance across multiple benchmarks, affirming its effectiveness and generalizability in automatic personality prediction from short videos. <div>
arXiv:2505.03846v1 Announce Type: new 
Abstract: Apparent personality analysis from short videos poses significant chal-lenges due to the complex interplay of visual, auditory, and textual cues. In this paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to robustly model and fuse multi-source features for automatic personality prediction. For the visual stream, we construct a facial graph and introduce a dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks (GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to capture both structural and appearance-based facial cues. Complementing this, global context and iden-tity features are extracted using pretrained ResNet18 and VGGFace back-bones. To capture temporal dynamics, frame-level features are processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio representations are derived from the VGGish network, and linguistic se-mantics are captured via the XLM-Roberta transformer. To achieve effective multimodal integration, we propose a Channel Attention-based Fusion module, followed by a Multi-Layer Perceptron (MLP) regression head for predicting personality traits. Extensive experiments show that GAME con-sistently outperforms existing methods across multiple benchmarks, vali-dating its effectiveness and generalizability.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques</title>
<link>https://arxiv.org/abs/2505.03848</link>
<guid>https://arxiv.org/abs/2505.03848</guid>
<content:encoded><![CDATA[
<div> Keywords: semiconductor manufacturing, image data, clustering, Topological Data Analysis, self-supervised learning, transfer learning

Summary: 
This paper introduces an advanced clustering framework for semiconductor image data analysis. The framework combines deep Topological Data Analysis (TDA), self-supervised learning, and transfer learning techniques to enhance unsupervised image clustering. TDA captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data. Transfer learning enables adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. The framework was validated on synthetic and open-source semiconductor image datasets, successfully identifying clusters aligned with defect patterns and process variations. By integrating TDA, self-supervised learning, and transfer learning, this framework offers a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other industries with large-scale image datasets.

<br /><br />Summary: <div>
arXiv:2505.03848v1 Announce Type: new 
Abstract: Semiconductor manufacturing generates vast amounts of image data, crucial for defect identification and yield optimization, yet often exceeds manual inspection capabilities. Traditional clustering techniques struggle with high-dimensional, unlabeled data, limiting their effectiveness in capturing nuanced patterns. This paper introduces an advanced clustering framework that integrates deep Topological Data Analysis (TDA) with self-supervised and transfer learning techniques, offering a novel approach to unsupervised image clustering. TDA captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data, reducing reliance on labeled datasets. Transfer learning enhances the framework's adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. Validated on synthetic and open-source semiconductor image datasets, the framework successfully identifies clusters aligned with defect patterns and process variations. This study highlights the transformative potential of combining TDA, self-supervised learning, and transfer learning, providing a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other domains with large-scale image datasets.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Active Inference Model of Covert and Overt Visual Attention</title>
<link>https://arxiv.org/abs/2505.03856</link>
<guid>https://arxiv.org/abs/2505.03856</guid>
<content:encoded><![CDATA[
<div> Model, covert attention, overt attention, active inference, sensory precisions<br />
Summary:<br />
This paper presents a model of covert and overt visual attention within the framework of active inference, focusing on optimizing sensory precisions to minimize free-energy. The model considers both current environmental beliefs and sensory input to determine visual sensory precisions, guiding attentional allocation in covert and overt modalities. In experiments using the Posner cueing task and a target focus task with 2D visual data, reaction times are measured to study the interaction between exogenous and endogenous attention, as well as valid and invalid cueing. Results indicate that exogenous and valid cues lead to faster reaction times, with behavior similar to inhibition of return observed. Additionally, the study investigates different aspects of overt attention, finding that involuntary saccades are quicker than intentional ones but lack adaptability. Overall, the model demonstrates how dynamic optimization of sensory precisions can enhance attentional processes in complex sensory environments. <br /><br />Summary: <div>
arXiv:2505.03856v1 Announce Type: new 
Abstract: The ability to selectively attend to relevant stimuli while filtering out distractions is essential for agents that process complex, high-dimensional sensory input. This paper introduces a model of covert and overt visual attention through the framework of active inference, utilizing dynamic optimization of sensory precisions to minimize free-energy. The model determines visual sensory precisions based on both current environmental beliefs and sensory input, influencing attentional allocation in both covert and overt modalities. To test the effectiveness of the model, we analyze its behavior in the Posner cueing task and a simple target focus task using two-dimensional(2D) visual data. Reaction times are measured to investigate the interplay between exogenous and endogenous attention, as well as valid and invalid cueing. The results show that exogenous and valid cues generally lead to faster reaction times compared to endogenous and invalid cues. Furthermore, the model exhibits behavior similar to inhibition of return, where previously attended locations become suppressed after a specific cue-target onset asynchrony interval. Lastly, we investigate different aspects of overt attention and show that involuntary, reflexive saccades occur faster than intentional ones, but at the expense of adaptability.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation</title>
<link>https://arxiv.org/abs/2505.03896</link>
<guid>https://arxiv.org/abs/2505.03896</guid>
<content:encoded><![CDATA[
<div> Keywords: Retinal vessel segmentation, Neural Networks, Attention Gates, Kolmogorov-Arnold Network, Pixel-wise Contrastive Loss 

Summary: 
Retinal vessel segmentation is crucial for detecting ocular diseases early. The proposed AttUKAN model combines Attention Gates with Kolmogorov-Arnold Networks to enhance sensitivity and interpretability. A novel Label-guided Pixel-wise Contrastive Loss is introduced to extract more discriminative features. Experiment results on public datasets and a private dataset demonstrate that AttUKAN achieves the highest F1 and MIoU scores compared to other networks. The model outperforms existing methods in both quantitative and qualitative evaluations, showcasing its state-of-the-art performance. The code for AttUKAN will be available on GitHub for further research and implementation. <br /><br />Summary: <div>
arXiv:2505.03896v1 Announce Type: new 
Abstract: Retinal vessel segmentation is a vital early detection method for several severe ocular diseases. Despite significant progress in retinal vessel segmentation with the advancement of Neural Networks, there are still challenges to overcome. Specifically, retinal vessel segmentation aims to predict the class label for every pixel within a fundus image, with a primary focus on intra-image discrimination, making it vital for models to extract more discriminative features. Nevertheless, existing methods primarily focus on minimizing the difference between the output from the decoder and the label, but ignore fully using feature-level fine-grained representations from the encoder. To address these issues, we propose a novel Attention U-shaped Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we implement Attention Gates into Kolmogorov-Arnold Networks to enhance model sensitivity by suppressing irrelevant feature activations and model interpretability by non-linear modeling of KAN blocks. Additionally, we also design a novel Label-guided Pixel-wise Contrastive Loss to supervise our proposed AttUKAN to extract more discriminative features by distinguishing between foreground vessel-pixel pairs and background pairs. Experiments are conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%, 80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and 66.94% in the above datasets, which are the highest compared to 11 networks for retinal vessel segmentation. Quantitative and qualitative results show that our AttUKAN achieves state-of-the-art performance and outperforms existing retinal vessel segmentation methods. Our code will be available at https://github.com/stevezs315/AttUKAN.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces</title>
<link>https://arxiv.org/abs/2505.03974</link>
<guid>https://arxiv.org/abs/2505.03974</guid>
<content:encoded><![CDATA[
<div> Keywords: drones, infrastructure asset management, super-resolution, convolutional neural network, efficient sub-pixel convolutional neural network

Summary: 
This study addresses the challenge of low-resolution infrastructure images captured by drones for asset management. A framework combining convolutional neural network (CNN) and efficient sub-pixel convolutional neural network (ESPCNN) was developed. CNN accurately classifies distress classes in infrastructure images, while ESPCNN efficiently generates high-resolution images of positive distress. The ESPCNN outperforms bicubic interpolation in super-resolution evaluation metrics. The combination of CNN and ESPCNN effectively preprocesses images with negative distress, reducing computational costs and false alarms. Visual inspection shows that ESPCNN captures crack propagation and complex geometry. This framework is expected to aid highway agencies in accurate distress detection and improve asset management practices.<br /><br />Summary: <div>
arXiv:2505.03974v1 Announce Type: new 
Abstract: Recently, there has been an impetus for the application of cutting-edge data collection platforms such as drones mounted with camera sensors for infrastructure asset management. However, the sensor characteristics, proximity to the structure, hard-to-reach access, and environmental conditions often limit the resolution of the datasets. A few studies used super-resolution techniques to address the problem of low-resolution images. Nevertheless, these techniques were observed to increase computational cost and false alarms of distress detection due to the consideration of all the infrastructure images i.e., positive and negative distress classes. In order to address the pre-processing of false alarm and achieve efficient super-resolution, this study developed a framework consisting of convolutional neural network (CNN) and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately classified both the classes. ESPCNN, which is the lightweight super-resolution technique, generated high-resolution infrastructure image of positive distress obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the evaluation metrics for super-resolution. Based on the performance metrics, the combination of CNN and ESPCNN was observed to be effective in preprocessing the infrastructure images with negative distress, reducing the computational cost and false alarms in the next step of super-resolution. The visual inspection showed that EPSCNN is able to capture crack propagation, complex geometry of even minor cracks. The proposed framework is expected to help the highway agencies in accurately performing distress detection and assist in efficient asset management practices.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges</title>
<link>https://arxiv.org/abs/2505.03991</link>
<guid>https://arxiv.org/abs/2505.03991</guid>
<content:encoded><![CDATA[
<div> Keywords: Video event detection, deep learning, sports analytics, Convolutional Neural Networks, Transformers

Summary: 
This survey delves into the advancements in video event detection in sports analytics, focusing on Temporal Action Localization, Action Spotting, and Precise Event Spotting tasks. It explores the evolution of methodological approaches and evaluates existing datasets and evaluation metrics tailored for sports contexts. State-of-the-art techniques such as multi-modal approaches, self-supervised learning, and knowledge distillation are analyzed. The survey also discusses methods for generalizing across multiple sports and outlines key challenges and future research directions. The goal is to develop more efficient, generalizable, and robust event detection frameworks applicable to a variety of sports. Overall, this survey lays the groundwork for future research in multi-modal sports event detection. 

Summary:<br /><br />Keywords: Video event detection, deep learning, sports analytics, Convolutional Neural Networks, Transformers<br />This survey delves into the advancements in video event detection in sports analytics, focusing on Temporal Action Localization, Action Spotting, and Precise Event Spotting tasks. It explores the evolution of methodological approaches and evaluates existing datasets and evaluation metrics tailored for sports contexts. State-of-the-art techniques such as multi-modal approaches, self-supervised learning, and knowledge distillation are analyzed. The survey also discusses methods for generalizing across multiple sports and outlines key challenges and future research directions. The goal is to develop more efficient, generalizable, and robust event detection frameworks applicable to a variety of sports. Overall, this survey lays the groundwork for future research in multi-modal sports event detection. <div>
arXiv:2505.03991v1 Announce Type: new 
Abstract: Video event detection has become an essential component of sports analytics, enabling automated identification of key moments and enhancing performance analysis, viewer engagement, and broadcast efficiency. Recent advancements in deep learning, particularly Convolutional Neural Networks (CNNs) and Transformers, have significantly improved accuracy and efficiency in Temporal Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting (PES). This survey provides a comprehensive overview of these three key tasks, emphasizing their differences, applications, and the evolution of methodological approaches. We thoroughly review and categorize existing datasets and evaluation metrics specifically tailored for sports contexts, highlighting the strengths and limitations of each. Furthermore, we analyze state-of-the-art techniques, including multi-modal approaches that integrate audio and visual information, methods utilizing self-supervised learning and knowledge distillation, and approaches aimed at generalizing across multiple sports. Finally, we discuss critical open challenges and outline promising research directions toward developing more generalized, efficient, and robust event detection frameworks applicable to diverse sports. This survey serves as a foundation for future research on efficient, generalizable, and multi-modal sports event detection.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics</title>
<link>https://arxiv.org/abs/2505.04006</link>
<guid>https://arxiv.org/abs/2505.04006</guid>
<content:encoded><![CDATA[
<div> retinal imaging, Artificial Intelligence, oculomics, systemic health, early detection <br />
Summary: 
The article discusses the significance of the human eye's vascularized anatomy in providing insights into human health, with the retina serving as a crucial window for detecting and monitoring diseases. Advancements in imaging technology, particularly Artificial Intelligence, have revolutionized the field by enabling the analysis of retinal images for systemic health assessment. Oculomics, a novel approach in ophthalmology, combines retinal imaging with AI-driven analysis to identify non-invasive markers for timely intervention. The paper traces the evolution of retinal imaging techniques and underscores the importance of integrating AI in this domain. It also addresses potential challenges in the adoption of oculomics, highlighting research gaps and proposing future directions for this innovative field.<br /><br /> <div>
arXiv:2505.04006v1 Announce Type: new 
Abstract: The unique vascularized anatomy of the human eye, encased in the retina, provides an opportunity to act as a window for human health. The retinal structure assists in assessing the early detection, monitoring of disease progression and intervention for both ocular and non-ocular diseases. The advancement in imaging technology leveraging Artificial Intelligence has seized this opportunity to bridge the gap between the eye and human health. This track paves the way for unveiling systemic health insight from the ocular system and surrogating non-invasive markers for timely intervention and identification. The new frontiers of oculomics in ophthalmology cover both ocular and systemic diseases, and getting more attention to explore them. In this survey paper, we explore the evolution of retinal imaging techniques, the dire need for the integration of AI-driven analysis, and the shift of retinal imaging from classical techniques to oculomics. We also discuss some hurdles that may be faced in the progression of oculomics, highlighting the research gaps and future directions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FoodTrack: Estimating Handheld Food Portions with Egocentric Video</title>
<link>https://arxiv.org/abs/2505.04055</link>
<guid>https://arxiv.org/abs/2505.04055</guid>
<content:encoded><![CDATA[
<div> Keywords: FoodTrack, hand-held food items, egocentric video, food volume measurement, accuracy

Summary:
FoodTrack is a new framework proposed for accurately tracking and measuring the volume of hand-held food items using egocentric video. Unlike traditional approaches that rely on specific camera angles or gesture recognition, FoodTrack is robust to hand occlusions and flexible with varying camera and object poses. It estimates food volume directly, without making assumptions about bite size, offering a more accurate and adaptable solution for tracking food consumption. The framework achieved an absolute percentage loss of approximately 7.01% on a handheld food object, outperforming a previous approach that had a mean absolute percentage error of 16.40% in its best case scenario under less flexible conditions.<br /><br />Summary: FoodTrack is a novel framework for accurately tracking and measuring hand-held food items' volume using egocentric video. It provides a flexible and robust solution that directly estimates food volume without relying on intake gestures or making assumptions about bite size, resulting in a more accurate and adaptable method for tracking food consumption. <div>
arXiv:2505.04055v1 Announce Type: new 
Abstract: Accurately tracking food consumption is crucial for nutrition and health monitoring. Traditional approaches typically require specific camera angles, non-occluded images, or rely on gesture recognition to estimate intake, making assumptions about bite size rather than directly measuring food volume. We propose the FoodTrack framework for tracking and measuring the volume of hand-held food items using egocentric video which is robust to hand occlusions and flexible with varying camera and object poses. FoodTrack estimates food volume directly, without relying on intake gestures or fixed assumptions about bite size, offering a more accurate and adaptable solution for tracking food consumption. We achieve absolute percentage loss of approximately 7.01% on a handheld food object, improving upon a previous approach that achieved a 16.40% mean absolute percentage error in its best case, under less flexible conditions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding</title>
<link>https://arxiv.org/abs/2505.04058</link>
<guid>https://arxiv.org/abs/2505.04058</guid>
<content:encoded><![CDATA[
<div> 3D visual grounding, 2D-assisted framework, semantic-spatial scene graphs, referred object discrimination, relationship perception <br />
Summary:<br />
The article introduces a novel approach for 3D visual grounding by incorporating a 2D-assisted framework that focuses on discriminating referred objects for relationship perception in complex scenes. The proposed method utilizes 2D pre-trained attributes to guide multi-modal object encoding and employs a graph attention mechanism for relationship-oriented information fusion. This approach enhances object representation and facilitates iterative relational learning, leading to effective alignment between 3D vision and referential descriptions. Experimental results on benchmark datasets demonstrate the superior performance of the model, particularly in distinguishing multiple similar distractors. The dual-branch visual encoder and cross-modal interaction module play crucial roles in achieving accurate localization of unique targets in 3D scenes based on natural language descriptions. <br /> <div>
arXiv:2505.04058v1 Announce Type: new 
Abstract: 3D visual grounding aims to localize the unique target described by natural languages in 3D scenes. The significant gap between 3D and language modalities makes it a notable challenge to distinguish multiple similar objects through the described spatial relationships. Current methods attempt to achieve cross-modal understanding in complex scenes via a target-centered learning mechanism, ignoring the perception of referred objects. We propose a novel 2D-assisted 3D visual grounding framework that constructs semantic-spatial scene graphs with referred object discrimination for relationship perception. The framework incorporates a dual-branch visual encoder that utilizes 2D pre-trained attributes to guide the multi-modal object encoding. Furthermore, our cross-modal interaction module uses graph attention to facilitate relationship-oriented information fusion. The enhanced object representation and iterative relational learning enable the model to establish effective alignment between 3D vision and referential descriptions. Experimental results on the popular benchmarks demonstrate our superior performance compared to state-of-the-art methods, especially in addressing the challenges of multiple similar distractors.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2505.04087</link>
<guid>https://arxiv.org/abs/2505.04087</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-Time adaptation, robustness, data augmentations, entropy loss, selection mechanism

Summary:<br />
Test-Time adaptation (TTA) is a method used to improve model robustness against distribution shifts during inference. Existing TTA methods often rely on entropy-based unsupervised training but struggle to effectively utilize reliable samples due to a single round of training. This paper introduces Single-step Ensemble of Vicinal Augmentations (SEVA), a novel TTA approach that optimizes an upper bound of entropy loss to incorporate the effects of multiple augmentations in a single step. By using this efficient loss and a selection strategy, SEVA can enhance the potential of reliable samples and meet real-time requirements. The proposed method outperforms on challenging testing scenarios and various network architectures, demonstrating its effectiveness and adaptability. The code for SEVA will be made publicly available. <br />Summary: <div>
arXiv:2505.04087v1 Announce Type: new 
Abstract: Test-Time adaptation (TTA) aims to enhance model robustness against distribution shifts through rapid model adaptation during inference. While existing TTA methods often rely on entropy-based unsupervised training and achieve promising results, the common practice of a single round of entropy training is typically unable to adequately utilize reliable samples, hindering adaptation efficiency. In this paper, we discover augmentation strategies can effectively unleash the potential of reliable samples, but the rapidly growing computational cost impedes their real-time application. To address this limitation, we propose a novel TTA approach named Single-step Ensemble of Vicinal Augmentations (SEVA), which can take advantage of data augmentations without increasing the computational burden. Specifically, instead of explicitly utilizing the augmentation strategy to generate new data, SEVA develops a theoretical framework to explore the impacts of multiple augmentations on model adaptation and proposes to optimize an upper bound of the entropy loss to integrate the effects of multiple rounds of augmentation training into a single step. Furthermore, we discover and verify that using the upper bound as the loss is more conducive to the selection mechanism, as it can effectively filter out harmful samples that confuse the model. Combining these two key advantages, the proposed efficient loss and a complementary selection strategy can simultaneously boost the potential of reliable samples and meet the stringent time requirements of TTA. The comprehensive experiments on various network architectures across challenging testing scenarios demonstrate impressive performances and the broad adaptability of SEVA. The code will be publicly available.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking</title>
<link>https://arxiv.org/abs/2505.04088</link>
<guid>https://arxiv.org/abs/2505.04088</guid>
<content:encoded><![CDATA[
<div> Siamese Motion Mamba Tracker, thermal infrared object tracking, bidirectional state-space model, self-attention mechanism, motion features, edge details, computational efficiency, parameter-sharing strategy, motion edge-aware regression loss. 
<br />
Summary: 
The paper introduces the Siamese Motion Mamba Tracker (SMMT) for thermal infrared object tracking, addressing challenges like occlusion and background clutter. SMMT integrates bidirectional modeling and a self-attention mechanism to extract motion features and recover edge details. The tracker utilizes a Siamese parameter-sharing strategy to reduce computational redundancy while maintaining strong feature representation. Additionally, a motion edge-aware regression loss is proposed to enhance tracking accuracy for motion-blurred targets. Extensive experiments on four TIR tracking benchmarks demonstrate the superior performance of SMMT in thermal infrared target tracking. <div>
arXiv:2505.04088v1 Announce Type: new 
Abstract: Thermal infrared (TIR) object tracking often suffers from challenges such as target occlusion, motion blur, and background clutter, which significantly degrade the performance of trackers. To address these issues, this paper pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a bidirectional state-space model and a self-attention mechanism. Specifically, we introduce the Motion Mamba module into the Siamese architecture to ex-tract motion features and recover overlooked edge details using bidirectional modeling and self-attention. We propose a Siamese parameter-sharing strate-gy that allows certain convolutional layers to share weights. This approach reduces computational redundancy while preserving strong feature represen-tation. In addition, we design a motion edge-aware regression loss to improve tracking accuracy, especially for motion-blurred targets. Extensive experi-ments are conducted on four TIR tracking benchmarks, including LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT achieves superior performance in TIR target tracking.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction</title>
<link>https://arxiv.org/abs/2505.04105</link>
<guid>https://arxiv.org/abs/2505.04105</guid>
<content:encoded><![CDATA[
<div> Motion-Aware Image SYnthesis, GAN-based methods, SSIM loss, anatomical details, artifact correction  
Motion-Aware Image SYnthesis (MAISY) addresses the limitations of current image correction algorithms by focusing on localized features and introducing the Variance-Selective SSIM (VS-SSIM) loss function. By leveraging the Segment Anything Model (SAM) to identify motion patterns along anatomical boundaries and using VS-SSIM to preserve details in regions with high pixel variance, MAISY outperforms existing models in correcting motion artifacts in medical images. Experimental results on chest and head CT datasets show significant improvements in Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Dice scores, demonstrating the effectiveness of the proposed approach.<br /><br />Summary: <div>
arXiv:2505.04105v1 Announce Type: new 
Abstract: Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging.Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterize motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One2Any: One-Reference 6D Pose Estimation for Any Object</title>
<link>https://arxiv.org/abs/2505.04109</link>
<guid>https://arxiv.org/abs/2505.04109</guid>
<content:encoded><![CDATA[
<div> Keywords: 6D object pose estimation, single RGB-D image, pose encoding-decoding, reference object pose embedding, U-Net-based pose decoding<br />
Summary: <br />
- The paper introduces a novel method called One2Any for 6D object pose estimation using a single reference-single query RGB-D image, without the need for complete 3D models or multi-view images.
- The approach involves encoding the object shape, orientation, and texture into a Reference Object Pose Embedding (ROPE) from a single reference view, which is then used for pose decoding and generating Reference Object Coordinates (ROC) for new views.
- By treating object pose estimation as an encoding-decoding process and training the model on pair-wise pose data, the model exhibits excellent generalization to novel objects, outperforming methods that require multi-view or CAD inputs.
- Experimental results on benchmark datasets demonstrate that the proposed method achieves state-of-the-art accuracy and robustness, while being computationally efficient.
- The simplicity and scalability of the encoding-decoding framework allow for large-scale training and promising performance without the need for elaborate 3D models or category constraints. <br /> 
Summary: <div>
arXiv:2505.04109v1 Announce Type: new 
Abstract: 6D object pose estimation remains challenging for many applications due to dependencies on complete 3D models, multi-view images, or training limited to specific object categories. These requirements make generalization to novel objects difficult for which neither 3D models nor multi-view images may be available. To address this, we propose a novel method One2Any that estimates the relative 6-degrees of freedom (DOF) object pose using only a single reference-single query RGB-D image, without prior knowledge of its 3D model, multi-view data, or category constraints. We treat object pose estimation as an encoding-decoding process, first, we obtain a comprehensive Reference Object Pose Embedding (ROPE) that encodes an object shape, orientation, and texture from a single reference view. Using this embedding, a U-Net-based pose decoding module produces Reference Object Coordinate (ROC) for new views, enabling fast and accurate pose estimation. This simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability. Experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or CAD inputs, at a fraction of compute.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model</title>
<link>https://arxiv.org/abs/2505.04119</link>
<guid>https://arxiv.org/abs/2505.04119</guid>
<content:encoded><![CDATA[
<div> Keywords: pre-trained models, 3D vision, point cloud, parameter-efficient fine-tuning, geometric cues

Summary: 
Geometry-Aware Point Cloud Prompt (GAPrompt) is proposed to enhance the adaptability of 3D vision models by leveraging geometric cues. The approach includes a Point Prompt for fine-grained geometric details, a Point Shift Prompter for global shape information extraction, and Prompt Propagation for integrating shape information into the feature extraction process. GAPrompt outperforms existing parameter-efficient fine-tuning methods and achieves competitive results compared to full fine-tuning on various benchmarks. The approach utilizes only 2.19% of trainable parameters and is available on GitHub for further research.  <br /><br />Summary: <div>
arXiv:2505.04119v1 Announce Type: new 
Abstract: Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision Graph Prompting via Semantic Low-Rank Decomposition</title>
<link>https://arxiv.org/abs/2505.04121</link>
<guid>https://arxiv.org/abs/2505.04121</guid>
<content:encoded><![CDATA[
<div> Vision Graph Prompting, low-rank properties, semantic features, global structural patterns, fine-grained semantic dependencies <br />
Summary: <br />
The paper introduces Vision Graph Prompting (VGP), a framework designed for vision graph structures, leveraging the low-rank properties of semantically connected components. By decomposing low-rank semantic features and integrating them with prompts on vision graph topologies, VGP captures both global structural patterns and fine-grained semantic dependencies. The proposed method significantly improves transfer performance on various downstream tasks for Vision GNN (ViG) models, achieving results comparable to full fine-tuning while maintaining parameter efficiency. This innovative approach fills the gap in existing prompting methods tailored for Transformer-based models but limited for graph-based representations, showcasing the potential of semantic low-rank prompting in enhancing the adaptability and performance of vision graph models. <div>
arXiv:2505.04121v1 Announce Type: new 
Abstract: Vision GNN (ViG) demonstrates superior performance by representing images as graph structures, providing a more natural way to capture irregular semantic patterns beyond traditional grid or sequence-based representations. To efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning techniques like visual prompting become increasingly essential. However, existing prompting methods are primarily designed for Transformer-based models, neglecting the rich topological relationships among nodes and edges in graph-based representations, limiting their capacity to model complex semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel framework tailored for vision graph structures. Our core insight reveals that semantically connected components in the graph exhibit low-rank properties. Building on this observation, we introduce a semantic low-rank prompting method that decomposes low-rank semantic features and integrates them with prompts on vision graph topologies, capturing both global structural patterns and fine-grained semantic dependencies. Extensive experiments demonstrate our method significantly improves ViG's transfer performance on diverse downstream tasks, achieving results comparable to full fine-tuning while maintaining parameter efficiency. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R^3-VQA: "Read the Room" by Video Social Reasoning</title>
<link>https://arxiv.org/abs/2505.04147</link>
<guid>https://arxiv.org/abs/2505.04147</guid>
<content:encoded><![CDATA[
<div> Keywords: social reasoning, video dataset, mental states, social events, vision-language models

Summary: 
The paper introduces a new video dataset called R^3-VQA that focuses on social reasoning capabilities in complex social scenarios. This dataset includes annotations of social events, mental states (belief, intent, desire, emotion), and social causal chains. The dataset also includes human-annotated and model-generated QAs. The task R^3-VQA involves aspects such as Social Event Understanding, Mental State Estimation, and Social Causal Reasoning. The study evaluates the performance of large vision-language models (LVLMs) on social reasoning tasks and finds that they are still far from achieving human-level consistent social reasoning in complex scenarios. The results show that prompting LVLMs with Theory of Mind (ToM) can improve their performance on social reasoning tasks. Supplementary materials containing some dataset and codes are provided, with plans to release the full dataset and codes upon acceptance.

<br /><br />Summary: <div>
arXiv:2505.04147v1 Announce Type: new 
Abstract: "Read the room" is a significant social reasoning capability in human daily life. Humans can infer others' mental states from subtle social cues. Previous social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic interactions, incomplete mental state variables, single-step reasoning, etc.) and fall far short of the challenges present in real-life social interactions. In this paper, we contribute a valuable, high-quality, and comprehensive video dataset named R^3-VQA with precise and fine-grained annotations of social events and mental states (i.e., belief, intent, desire, and emotion) as well as corresponding social causal chains in complex social scenarios. Moreover, we include human-annotated and model-generated QAs. Our task R^3-VQA includes three aspects: Social Event Understanding, Mental State Estimation, and Social Causal Reasoning. As a benchmark, we comprehensively evaluate the social reasoning capabilities and consistencies of current state-of-the-art large vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs are still far from human-level consistent social reasoning in complex social scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on social reasoning tasks. We provide some of our dataset and codes in supplementary material and will release our full dataset and codes upon acceptance.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages</title>
<link>https://arxiv.org/abs/2505.04150</link>
<guid>https://arxiv.org/abs/2505.04150</guid>
<content:encoded><![CDATA[
<div> Keywords: muscle tissue regeneration, machine learning, Learning from Label Proportions, Ordinal Scale Learning, skeletal muscle recovery stages 

Summary: 
The article presents a new method, Ordinal Scale Learning from Similarity Proportion (OSLSP), to automate the evaluation of muscle tissue regeneration. The current approach relies on visual inspection, but OSLSP utilizes machine learning to provide a quantitative and objective analysis. Unlike existing methods, OSLSP can adapt the feature extractor for muscle tissues and considers the ordinal information of recovery stages and cell morphological changes. By using a similarity proportion loss derived from two bag combinations, OSLSP can update the feature extractor based on class proportion attention. In classification tasks of skeletal muscle recovery stages, the model with OSLSP outperforms pre-trained and fine-tuning models. The proposed method addresses limitations in current weakly supervised learning methods and offers a more effective approach to analyzing muscle tissue regeneration. 

<br /><br />Summary: <div>
arXiv:2505.04150v1 Announce Type: new 
Abstract: Evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. The conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. There is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. Given the limited availability of fully labeled data, a possible approach is Learning from Label Proportions (LLP), a weakly supervised learning method using class label proportions. However, current LLP methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. To address these issues, we propose Ordinal Scale Learning from Similarity Proportion (OSLSP), which uses a similarity proportion loss derived from two bag combinations. OSLSP can update the feature extractor by using class proportion attention to the ordinal scale of the class. Our model with OSLSP outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.04175</link>
<guid>https://arxiv.org/abs/2505.04175</guid>
<content:encoded><![CDATA[
<div> Keywords: Text recognition, OCR, ResNet, Vision Transformer, Deformable Convolutions

Summary: 
This paper introduces a novel end-to-end framework for text recognition in natural images. The framework combines ResNet and Vision Transformer backbones with advanced methodologies such as Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). Deformable Convolutions are used to enhance feature representation, adaptive dropout for regularization, and CRF for sequence modeling. The proposed method achieves high accuracies on benchmark datasets IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80, with an average accuracy of 77.77%. The results establish a new state-of-the-art for text recognition, showcasing the robustness of the approach across diverse and challenging datasets. This study demonstrates the effectiveness of combining different techniques to improve OCR performance in natural images. 

<br /><br />Summary: <div>
arXiv:2505.04175v1 Announce Type: new 
Abstract: Text recognition in natural images remains a challenging yet essential task, with broad applications spanning computer vision and natural language processing. This paper introduces a novel end-to-end framework that combines ResNet and Vision Transformer backbones with advanced methodologies, including Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). These innovations collectively enhance feature representation and improve Optical Character Recognition (OCR) performance. Specifically, the framework substitutes standard convolution layers in the third and fourth blocks with Deformable Convolutions, leverages adaptive dropout for regularization, and incorporates CRF for more refined sequence modeling. Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy of 77.77%. These results establish a new state-of-the-art for text recognition, demonstrating the robustness of the approach across diverse and challenging datasets.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S3D: Sketch-Driven 3D Model Generation</title>
<link>https://arxiv.org/abs/2505.04185</link>
<guid>https://arxiv.org/abs/2505.04185</guid>
<content:encoded><![CDATA[
<div> U-Net-based architecture, 2D sketches, 3D models, face segmentation masks, style alignment loss <br />
Summary: <br />
Generating high-quality 3D models from simple hand-drawn 2D sketches presents challenges due to data ambiguity and sparsity. The S3D framework addresses this by converting sketches into face segmentation masks using a U-Net-based encoder-decoder architecture. These masks are then utilized to create detailed 3D models that can be viewed from different angles. A novel style-alignment loss enhances reconstruction fidelity by aligning bottleneck features with initial 3D generation module outputs. The network's robustness is further improved with augmentation techniques applied to the sketch dataset. The S3D framework offers a streamlined approach that effectively generates high-quality 3D models from sketch inputs. Source code for the project is available on GitHub for public access at https://github.com/hailsong/S3D. <div>
arXiv:2505.04185v1 Announce Type: new 
Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due to the inherent ambiguity and sparsity of sketch data. In this paper, we present S3D, a novel framework that converts simple hand-drawn sketches into detailed 3D models. Our method utilizes a U-Net-based encoder-decoder architecture to convert sketches into face segmentation masks, which are then used to generate a 3D representation that can be rendered from novel views. To ensure robust consistency between the sketch domain and the 3D output, we introduce a novel style-alignment loss that aligns the U-Net bottleneck features with the initial encoder outputs of the 3D generation module, significantly enhancing reconstruction fidelity. To further enhance the network's robustness, we apply augmentation techniques to the sketch dataset. This streamlined framework demonstrates the effectiveness of S3D in generating high-quality 3D models from sketch inputs. The source code for this project is publicly available at https://github.com/hailsong/S3D.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning</title>
<link>https://arxiv.org/abs/2505.04192</link>
<guid>https://arxiv.org/abs/2505.04192</guid>
<content:encoded><![CDATA[
<div> Keywords: VideoPath-LLaVA, multimodal model, computational pathology, diagnostic reasoning, histopathology

Summary:
VideoPath-LLaVA is the first large multimodal model in computational pathology that integrates single patch images, keyframe-extracted clips, and manually segmented video pathology images to mimic the diagnostic process of pathologists. It is trained on the VideoPath-Instruct dataset, which consists of video and diagnosis-specific instructional pairs from educational histopathology videos. By transferring knowledge from existing single-image instruction datasets and fine-tuning on manually segmented videos, VideoPath-LLaVA sets a new benchmark in pathology video analysis. The model generates detailed histological descriptions and definitive sign-out diagnoses, bridging visual narratives with diagnostic reasoning. It offers a foundation for future AI systems supporting clinical decision-making. The code, data, and model are publicly available on GitHub at https://github.com/trinhvg/VideoPath-LLaVA. 

Summary: <br /><br /> <div>
arXiv:2505.04192v1 Announce Type: new 
Abstract: We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.
  Central to our approach is the VideoPath-Instruct dataset, comprising 4278 video and diagnosis-specific chain-of-thought instructional pairs sourced from educational histopathology videos on YouTube. Although high-quality data is critical for enhancing diagnostic reasoning, its creation is time-intensive and limited in volume. To overcome this challenge, we transfer knowledge from existing single-image instruction datasets to train on weakly annotated, keyframe-extracted clips, followed by fine-tuning on manually segmented videos. VideoPath-LLaVA establishes a new benchmark in pathology video analysis and offers a promising foundation for future AI systems that support clinical decision-making through integrated visual and diagnostic reasoning. Our code, data, and model are publicly available at https://github.com/trinhvg/VideoPath-LLaVA.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios</title>
<link>https://arxiv.org/abs/2505.04201</link>
<guid>https://arxiv.org/abs/2505.04201</guid>
<content:encoded><![CDATA[
<div> Keywords: tactile sensing, multimodal reasoning, commonsense reasoning, Mixture of Experts, physical properties<br />
Summary:<br />
This paper discusses the challenges of integrating tactile sensing into intelligent systems for multimodal reasoning, particularly for enabling commonsense reasoning about the physical world. Two key challenges are identified: modality discrepancy and open-ended tactile data scarcity. To address these challenges, the authors introduce SToLa, a Self-Adaptive Touch-Language framework that uses Mixture of Experts to manage tactile and language modalities. They also present a new tactile commonsense reasoning dataset and benchmark with diverse knowledge and questions. Experiments show that SToLa performs competitively on the PhysiCLeAR benchmark and self-created datasets, demonstrating the effectiveness of the Mixture of Experts architecture for multimodal management and the advantages it offers for tactile commonsense reasoning tasks.<br /><br />Summary: <div>
arXiv:2505.04201v1 Announce Type: new 
Abstract: This paper explores the challenges of integrating tactile sensing into intelligent systems for multimodal reasoning, particularly in enabling commonsense reasoning about the open-ended physical world. We identify two key challenges: modality discrepancy, where existing large touch-language models often treat touch as a mere sub-modality of language, and open-ended tactile data scarcity, where current datasets lack the diversity, open-endness and complexity needed for reasoning. To overcome these challenges, we introduce SToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of Experts (MoE) to dynamically process, unify, and manage tactile and language modalities, capturing their unique characteristics. Crucially, we also present a comprehensive tactile commonsense reasoning dataset and benchmark featuring free-form questions and responses, 8 physical properties, 4 interactive characteristics, and diverse commonsense knowledge. Experiments show SToLa exhibits competitive performance compared to existing models on the PhysiCLeAR benchmark and self-constructed datasets, proving the effectiveness of the Mixture of Experts architecture in multimodal management and the performance advantages for open-scenario tactile commonsense reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement</title>
<link>https://arxiv.org/abs/2505.04207</link>
<guid>https://arxiv.org/abs/2505.04207</guid>
<content:encoded><![CDATA[
<div> Dataset, RGB-D images, Pothole detection, Physical features analysis, YOLOv8 <br />
Summary:
A new dataset, PothRGBD, consisting of RGB-D images was created for accurate pothole detection. An improved YOLOv8-based model was developed for analyzing physical characteristics of potholes. The model, YOLOv8n-seg, integrated Dynamic Snake Convolution, Simple Attention Module, and Gaussian Error Linear Unit for enhanced segmentation of potholes with irregular edges. It achieved 91.9% precision, 85.2% recall, and 91.9% mAP@50, with the proposed model improving to 93.7% precision, 90.4% recall, and 93.8% mAP. The model accurately measured perimeter and depth on depth maps, showing a 1.96% increase in precision, 6.13% in recall, and 2.07% in mAP. This lightweight and effective model is suitable for real-time applications in intelligent transportation solutions. <br /><br />Summary: <div>
arXiv:2505.04207v1 Announce Type: new 
Abstract: Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes. In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis. The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU). The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity. In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CM1 - A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models</title>
<link>https://arxiv.org/abs/2505.04214</link>
<guid>https://arxiv.org/abs/2505.04214</guid>
<content:encoded><![CDATA[
<div> Keyphrases: Key-value information extraction, Handwritten documents, Large Vision Language Models, Few-shot learning, CM1 dataset

Summary:
The article discusses the challenge of automatically extracting key-value information from handwritten documents and the potential of Large Vision Language Models (LVLM) in addressing this issue. A new dataset, CM1, is introduced to evaluate the few-shot capabilities of LVLMs using historic forms from post-World War Two Europe. Three benchmarks are established for extracting name and birthdate information, considering different training set sizes. Baseline results for two LVLMs show that they outperform traditional full-page extraction models when only a few training samples are available. Despite competitive performance by the full-page model, the LVLMs benefit from their size and heavy pretraining in low-data scenarios. This suggests the potential of LVLMs in efficiently extracting key-value information from handwritten documents with limited training data.

<br /><br />Summary: <div>
arXiv:2505.04214v1 Announce Type: new 
Abstract: The automatic extraction of key-value information from handwritten documents is a key challenge in document analysis. A reliable extraction is a prerequisite for the mass digitization efforts of many archives. Large Vision Language Models (LVLM) are a promising technology to tackle this problem especially in scenarios where little annotated training data is available. In this work, we present a novel dataset specifically designed to evaluate the few-shot capabilities of LVLMs. The CM1 documents are a historic collection of forms with handwritten entries created in Europe to administer the Care and Maintenance program after World War Two. The dataset establishes three benchmarks on extracting name and birthdate information and, furthermore, considers different training set sizes. We provide baseline results for two different LVLMs and compare performances to an established full-page extraction model. While the traditional full-page model achieves highly competitive performances, our experiments show that when only a few training samples are available the considered LVLMs benefit from their size and heavy pretraining and outperform the classical approach.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation</title>
<link>https://arxiv.org/abs/2505.04229</link>
<guid>https://arxiv.org/abs/2505.04229</guid>
<content:encoded><![CDATA[
<div> Keywords: weak supervision, parking lot occupancy, satellite imagery, urban mobility analysis, vulnerable communities 

Summary: 
The study introduces a novel weak supervision framework for estimating parking lot occupancy using 3m resolution satellite imagery. By utilizing coarse temporal labels based on the assumption of parking lot usage patterns in Germany, the model achieves high accuracy in predicting parking lot occupancy levels. This approach reduces the dependency on costly high-resolution images, making it more accessible for applications in low-income regions. The framework shows potential for scalable urban mobility analysis and could be adapted to assess transit patterns and resource allocation in underserved communities. By providing a data-driven basis for decision-making, this method offers opportunities to improve the well-being of vulnerable populations through informed urban planning and resource distribution strategies. <br /><br />Summary: <div>
arXiv:2505.04229v1 Announce Type: new 
Abstract: The scarcity and high cost of labeled high-resolution imagery have long challenged remote sensing applications, particularly in low-income regions where high-resolution data are scarce. In this study, we propose a weak supervision framework that estimates parking lot occupancy using 3m resolution satellite imagery. By leveraging coarse temporal labels -- based on the assumption that parking lots of major supermarkets and hardware stores in Germany are typically full on Saturdays and empty on Sundays -- we train a pairwise comparison model that achieves an AUC of 0.92 on large parking lots. The proposed approach minimizes the reliance on expensive high-resolution images and holds promise for scalable urban mobility analysis. Moreover, the method can be adapted to assess transit patterns and resource allocation in vulnerable communities, providing a data-driven basis to improve the well-being of those most in need.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.04262</link>
<guid>https://arxiv.org/abs/2505.04262</guid>
<content:encoded><![CDATA[
<div> optimization, Gaussian Splatting, 3D generation, multi-view correlation, geometric consistency
<br />
Summary:<br />
The paper introduces Coupled Score Distillation (CSD), a framework that enhances text-to-3D generation by incorporating multi-view joint distribution priors to ensure geometrically consistent results. By optimizing as a multi-view joint optimization problem, CSD effectively couples multi-view priors to guide optimization across different viewpoints, leading to diverse 3D content generation. The framework directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization for geometrically consistent results. Additionally, a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, produces high-quality meshes. Experimental results demonstrate the efficiency and competitive quality of the proposed approach, addressing issues such as geometric inconsistencies and multi-face artifacts commonly encountered in 3D content generation. <div>
arXiv:2505.04262v1 Announce Type: new 
Abstract: Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to advance text-to-3D generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3D content. In this work, we propose Coupled Score Distillation (CSD), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3D generation while enabling the stable and direct optimization of 3D Gaussian Splatting. Specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3D assets. Additionally, we propose a framework that directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization to generate geometrically consistent 3D content. We further employ a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, to produce high-quality, refined meshes. Quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Object-Shot Enhanced Grounding Network for Egocentric Video</title>
<link>https://arxiv.org/abs/2505.04270</link>
<guid>https://arxiv.org/abs/2505.04270</guid>
<content:encoded><![CDATA[
<div> Object-Shot, Grounding Network, egocentric video, embodied intelligence, OSGNet <br />
<br />
Summary: OSGNet is introduced as a novel approach for egocentric video grounding, which differs from traditional exocentric video moment localization. The model leverages object information extracted from videos to enhance video representations, especially for objects mentioned in textual queries but not directly captured in video features. Additionally, OSGNet analyzes shot movements typical of egocentric videos to capture the wearer's attention information and improve modality alignment. Experimental results on three datasets demonstrate that OSGNet achieves state-of-the-art performance, showcasing its effectiveness in addressing the unique characteristics and challenges of egocentric video grounding tasks. The code for OSGNet is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2505.04270v1 Announce Type: new 
Abstract: Egocentric video grounding is a crucial task for embodied intelligence applications, distinct from exocentric video moment localization. Existing methods primarily focus on the distributional differences between egocentric and exocentric videos but often neglect key characteristics of egocentric videos and the fine-grained information emphasized by question-type queries. To address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding Network for egocentric video. Specifically, we extract object information from videos to enrich video representation, particularly for objects highlighted in the textual query but not directly captured in the video features. Additionally, we analyze the frequent shot movements inherent to egocentric videos, leveraging these features to extract the wearer's attention information, which enhances the model's ability to perform modality alignment. Experiments conducted on three datasets demonstrate that OSGNet achieves state-of-the-art performance, validating the effectiveness of our approach. Our code can be found at https://github.com/Yisen-Feng/OSGNet.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2505.04276</link>
<guid>https://arxiv.org/abs/2505.04276</guid>
<content:encoded><![CDATA[
<div> Transformer, Graph Convolutional Network, diffusion model, 3D Human Pose Estimation, state-of-the-art performance <br />
<br />
Summary: <br />
The article introduces HDiffTG, a novel 3D Human Pose Estimation method that combines Transformer, Graph Convolutional Network (GCN), and diffusion model in a unified framework. The Transformer captures global spatiotemporal dependencies, GCN models local skeletal structures, and the diffusion model provides step-by-step optimization for fine-tuning, striking a balance between global and local features. The integrated model demonstrates improved accuracy and robustness in pose estimation, particularly in occluded and complex scenarios. Lightweight optimizations and refined objective function design reduce computational overhead without compromising performance. Evaluation on Human3.6M and MPI-INF-3DHP datasets shows that HDiffTG achieves state-of-the-art performance on the latter while excelling in accuracy and computational efficiency, showcasing exceptional robustness in noisy and occluded environments. Source codes and models are available for further exploration. <div>
arXiv:2505.04276v1 Announce Type: new 
Abstract: We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that integrates Transformer, Graph Convolutional Network (GCN), and diffusion model into a unified framework. HDiffTG leverages the strengths of these techniques to significantly improve pose estimation accuracy and robustness while maintaining a lightweight design. The Transformer captures global spatiotemporal dependencies, the GCN models local skeletal structures, and the diffusion model provides step-by-step optimization for fine-tuning, achieving a complementary balance between global and local features. This integration enhances the model's ability to handle pose estimation under occlusions and in complex scenarios. Furthermore, we introduce lightweight optimizations to the integrated model and refine the objective function design to reduce computational overhead without compromising performance. Evaluation results on the Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves state-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling in both accuracy and computational efficiency. Additionally, the model exhibits exceptional robustness in noisy and occluded environments. Source codes and models are available at https://github.com/CirceJie/HDiffTG
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement</title>
<link>https://arxiv.org/abs/2505.04281</link>
<guid>https://arxiv.org/abs/2505.04281</guid>
<content:encoded><![CDATA[
<div> Keywords: Two-Stage Diffusion Model, Camera Feature Integration, Color Corrector, QID dataset, Low-light imaging

Summary: 
The paper introduces the Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. TS-Diff consists of a pre-training stage where noisy images are synthesized and Camera Feature Integration (CFI) modules learn generalizable features across different virtual cameras. In the aligning stage, a target-specific CFI$^T$ is fine-tuned using a small amount of real RAW data to adapt to specific camera noise characteristics. A color corrector is used to maintain color consistency during the diffusion process by adjusting global color distributions dynamically. The researchers also construct a new dataset, QID, for training and evaluation purposes, featuring quantifiable illumination levels and a wide dynamic range. Experimental results demonstrate the superior performance of TS-Diff on various datasets, excelling in denoising, generalization, and color consistency across different cameras and lighting conditions. The robustness and versatility of TS-Diff make it a practical solution for low-light imaging applications. <br /><br />Summary: <div>
arXiv:2505.04281v1 Announce Type: new 
Abstract: This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. Camera Feature Integration (CFI) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. During the aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is fine-tuned using a small amount of real RAW data to adapt to the noise characteristics of specific cameras. A structural reparameterization technique further simplifies CFI$^T$ for efficient deployment. To address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions. Additionally, a novel dataset, QID, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions. Experimental results demonstrate that TS-Diff achieves state-of-the-art performance on multiple datasets, including QID, SID, and ELD, excelling in denoising, generalization, and color consistency across various cameras and illumination levels. These findings highlight the robustness and versatility of TS-Diff, making it a practical solution for low-light imaging applications. Source codes and models are available at https://github.com/CircccleK/TS-Diff
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition</title>
<link>https://arxiv.org/abs/2505.04306</link>
<guid>https://arxiv.org/abs/2505.04306</guid>
<content:encoded><![CDATA[
<div> diffusion-based, generative expert, face recognition, occluded faces, identity-gated network

Summary:
The paper introduces a novel approach called identity-gated mixture of diffusion experts (MoDE) for occluded face recognition (OFR). Current OFR algorithms struggle with varying types and severity of occlusions, impacting daily life convenience. MoDE consists of diffusion-based generative experts that estimate complete images for occluded faces. An identity-gating network evaluates and integrates information from multiple reconstructed faces based on their contribution to identity. MoDE can be easily integrated into existing face recognition models. Experimental results on multiple datasets demonstrate the superior performance of MoDE in handling various occlusions compared to existing methods. <div>
arXiv:2505.04306v1 Announce Type: new 
Abstract: With the continuous impact of epidemics, people have become accustomed to wearing masks. However, most current occluded face recognition (OFR) algorithms lack prior knowledge of occlusions, resulting in poor performance when dealing with occluded faces of varying types and severity in reality. Recognizing occluded faces is still a significant challenge, which greatly affects the convenience of people's daily lives. In this paper, we propose an identity-gated mixture of diffusion experts (MoDE) for OFR. Each diffusion-based generative expert estimates one possible complete image for occluded faces. Considering the random sampling process of the diffusion model, which introduces inevitable differences and variations between the inpainted faces and the real ones. To ensemble effective information from multi-reconstructed faces, we introduce an identity-gating network to evaluate the contribution of each reconstructed face to the identity and adaptively integrate the predictions in the decision space. Moreover, our MoDE is a plug-and-play module for most existing face recognition models. Extensive experiments on three public face datasets and two datasets in the wild validate our advanced performance for various occlusions in comparison with the competing methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-turn Consistent Image Editing</title>
<link>https://arxiv.org/abs/2505.04320</link>
<guid>https://arxiv.org/abs/2505.04320</guid>
<content:encoded><![CDATA[
<div> Keywords: image editing, iterative refinement, flow matching, Linear Quadratic Regulators (LQR), adaptive attention highlighting <br />
Summary: <br />
The article introduces a multi-turn image editing framework for achieving flexible and iterative modifications in real-world applications. Existing editing methods often struggle with user intent ambiguity and producing consistent outcomes. The proposed framework utilizes flow matching for accurate image inversion and a dual-objective Linear Quadratic Regulators (LQR) for stable sampling, reducing error accumulation. An adaptive attention highlighting method is introduced to enhance editability and maintain multi-turn coherence by analyzing the role of transformers at different layers. Extensive experiments demonstrate that the framework significantly improves edit success rates and visual fidelity compared to current methods. <div>
arXiv:2505.04320v1 Announce Type: new 
Abstract: Many real-world applications, such as interactive photo retouching, artistic content creation, and product design, require flexible and iterative image editing. However, existing image editing methods primarily focus on achieving the desired modifications in a single step, which often struggles with ambiguous user intent, complex transformations, or the need for progressive refinements. As a result, these methods frequently produce inconsistent outcomes or fail to meet user expectations. To address these challenges, we propose a multi-turn image editing framework that enables users to iteratively refine their edits, progressively achieving more satisfactory results. Our approach leverages flow matching for accurate image inversion and a dual-objective Linear Quadratic Regulators (LQR) for stable sampling, effectively mitigating error accumulation. Additionally, by analyzing the layer-wise roles of transformers, we introduce a adaptive attention highlighting method that enhances editability while preserving multi-turn coherence. Extensive experiments demonstrate that our framework significantly improves edit success rates and visual fidelity compared to existing methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion</title>
<link>https://arxiv.org/abs/2505.04347</link>
<guid>https://arxiv.org/abs/2505.04347</guid>
<content:encoded><![CDATA[
<div> CountDiffusion, text-to-image synthesis, object quantity, diffusion model, counting model
<br />
Summary:
CountDiffusion is a framework designed to address the challenge of generating images with accurate object quantity from textual descriptions. It comprises two stages: the first stage involves generating an intermediate denoising result using a diffusion model and a counting model to determine object quantity, while the second stage employs a correction module to adjust the object quantity based on universal guidance. CountDiffusion can be seamlessly integrated into existing diffusion-based text-to-image generation models without the need for additional training. Experimental results demonstrate that CountDiffusion significantly enhances the ability of text-to-image models to generate images with precise object quantities. <div>
arXiv:2505.04347v1 Announce Type: new 
Abstract: Stable Diffusion has advanced text-to-image synthesis, but training models to generate images with accurate object quantity is still difficult due to the high computational cost and the challenge of teaching models the abstract concept of quantity. In this paper, we propose CountDiffusion, a training-free framework aiming at generating images with correct object quantity from textual descriptions. CountDiffusion consists of two stages. In the first stage, an intermediate denoising result is generated by the diffusion model to predict the final synthesized image with one-step denoising, and a counting model is used to count the number of objects in this image. In the second stage, a correction module is used to correct the object quantity by changing the attention map of the object with universal guidance. The proposed CountDiffusion can be plugged into any diffusion-based text-to-image (T2I) generation models without further training. Experiment results demonstrate the superiority of our proposed CountDiffusion, which improves the accurate object quantity generation ability of T2I models by a large margin.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing</title>
<link>https://arxiv.org/abs/2505.04369</link>
<guid>https://arxiv.org/abs/2505.04369</guid>
<content:encoded><![CDATA[
<div> wavelet transform analysis, low-frequency components, dehazing framework, Mamba blocks, self-guided contrastive regularization 

Summary: 
This paper introduces a novel dehazing framework called WDMamba, which leverages wavelet transform analysis to identify that haze-related information is mainly found in low-frequency components. The framework consists of two stages: low-frequency restoration using Mamba blocks to remove overall haze and detail enhancement to restore fine-grained information. The model incorporates self-guided contrastive regularization during training to improve detail retention and achieve more natural dehazing results. By using the coarse restored output as a hard negative example, the model learns discriminative representations leading to enhanced dehazing performance. Extensive evaluations on public dehazing benchmarks demonstrate the superiority of WDMamba over existing state-of-the-art approaches, both qualitatively and quantitatively. The code for the framework is also made publicly available for further research and implementation. <br /><br /> <div>
arXiv:2505.04369v1 Announce Type: new 
Abstract: In this paper, we reveal a novel haze-specific wavelet degradation prior observed through wavelet transform analysis, which shows that haze-related information predominantly resides in low-frequency components. Exploiting this insight, we propose a novel dehazing framework, WDMamba, which decomposes the image dehazing task into two sequential stages: low-frequency restoration followed by detail enhancement. This coarse-to-fine strategy enables WDMamba to effectively capture features specific to each stage of the dehazing process, resulting in high-quality restored images. Specifically, in the low-frequency restoration stage, we integrate Mamba blocks to reconstruct global structures with linear complexity, efficiently removing overall haze and producing a coarse restored image. Thereafter, the detail enhancement stage reinstates fine-grained information that may have been overlooked during the previous phase, culminating in the final dehazed output. Furthermore, to enhance detail retention and achieve more natural dehazing, we introduce a self-guided contrastive regularization during network training. By utilizing the coarse restored output as a hard negative example, our model learns more discriminative representations, substantially boosting the overall dehazing performance. Extensive evaluations on public dehazing benchmarks demonstrate that our method surpasses state-of-the-art approaches both qualitatively and quantitatively. Code is available at https://github.com/SunJ000/WDMamba.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise</title>
<link>https://arxiv.org/abs/2505.04375</link>
<guid>https://arxiv.org/abs/2505.04375</guid>
<content:encoded><![CDATA[
<div> Exploration, Vision Transformers, Model Size Impact, Label Noise, Active Learning<br />
Summary: <br />
This study investigates the impact of model size on the performance of vision transformers in active learning scenarios with label noise. Larger ViT models, particularly ViTl32, outperform smaller models in accuracy and calibration, even under moderate to high label noise levels. Swin Transformers show weaker robustness across all noise levels. Smaller patch sizes do not always lead to better performance, as ViTl16 performs worse than ViTl32 with higher computational cost. Information-based Active Learning strategies improve accuracy at moderate label noise rates but result in poorer calibration compared to models trained on randomly acquired labels, especially at high noise rates. These findings offer practical guidance for deploying vision transformers in resource-constrained environments, emphasizing the balance between model complexity, label noise, and compute efficiency in fine-tuning or distillation processes. <br /> <div>
arXiv:2505.04375v1 Announce Type: new 
Abstract: Fine-tuning pre-trained convolutional neural networks on ImageNet for downstream tasks is well-established. Still, the impact of model size on the performance of vision transformers in similar scenarios, particularly under label noise, remains largely unexplored. Given the utility and versatility of transformer architectures, this study investigates their practicality under low-budget constraints and noisy labels. We explore how classification accuracy and calibration are affected by symmetric label noise in active learning settings, evaluating four vision transformer configurations (Base and Large with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations (Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label noise rates. Our findings show that larger ViT models (ViTl32 in particular) consistently outperform their smaller counterparts in both accuracy and calibration, even under moderate to high label noise, while Swin Transformers exhibit weaker robustness across all noise levels. We find that smaller patch sizes do not always lead to better performance, as ViTl16 performs consistently worse than ViTl32 while incurring a higher computational cost. We also find that information-based Active Learning strategies only provide meaningful accuracy improvements at moderate label noise rates, but they result in poorer calibration compared to models trained on randomly acquired labels, especially at high label noise rates. We hope these insights provide actionable guidance for practitioners looking to deploy vision transformers in resource-constrained environments, where balancing model complexity, label noise, and compute efficiency is critical in model fine-tuning or distillation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label-efficient Single Photon Images Classification via Active Learning</title>
<link>https://arxiv.org/abs/2505.04376</link>
<guid>https://arxiv.org/abs/2505.04376</guid>
<content:encoded><![CDATA[
<div> LiDAR, 3D imaging, single-photon, active learning, image classification <br />
<br />
Summary: 
This paper introduces an active learning framework for single-photon image classification in LiDAR technology. It addresses the challenge of semantic interpretation in single-photon images by proposing a sampling strategy that combines synthetic augmentation to capture variability in imaging conditions. The method selectively annotates informative examples by prioritizing samples where the model is uncertain and sensitive to imaging conditions. Experimental results demonstrate superior performance compared to baseline methods, achieving high classification accuracy with minimal labeled samples. The proposed approach achieves 97% accuracy on synthetic data with only 1.5% labeled samples and maintains 90.63% accuracy on real-world data with just 8% labeled samples, outperforming existing methods. This study highlights the potential of active learning in enhancing classification performance in single-photon images, paving the way for broader integration of such data in real-world applications. <br /> <div>
arXiv:2505.04376v1 Announce Type: new 
Abstract: Single-photon LiDAR achieves high-precision 3D imaging in extreme environments through quantum-level photon detection technology. Current research primarily focuses on reconstructing 3D scenes from sparse photon events, whereas the semantic interpretation of single-photon images remains underexplored, due to high annotation costs and inefficient labeling strategies. This paper presents the first active learning framework for single-photon image classification. The core contribution is an imaging condition-aware sampling strategy that integrates synthetic augmentation to model variability across imaging conditions. By identifying samples where the model is both uncertain and sensitive to these conditions, the proposed method selectively annotates only the most informative examples. Experiments on both synthetic and real-world datasets show that our approach outperforms all baselines and achieves high classification accuracy with significantly fewer labeled samples. Specifically, our approach achieves 97% accuracy on synthetic single-photon data using only 1.5% labeled samples. On real-world data, we maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher than the best-performing baseline. This illustrates that active learning enables the same level of classification performance on single-photon images as on classical images, opening doors to large-scale integration of single-photon data in real-world applications.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tetrahedron-Net for Medical Image Registration</title>
<link>https://arxiv.org/abs/2505.04380</link>
<guid>https://arxiv.org/abs/2505.04380</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image registration, convolutional backbone, Tetrahedron-Net, encoder-decoder architecture, feature extraction.

Summary:
- Medical image registration is essential in image processing, and improving representation quality is key.
- Traditional U-Net networks use skip connections to enhance representation capacity.
- A new method called Tetrahedron-Net adds an extra decoder to interact with the encoder and original decoder.
- This simple yet effective approach improves registration results by reusing features and enhancing interactions.
- Tetrahedron-Net outperforms existing methods on medical image registration benchmarks.
- The Tetrahedron design can be incorporated into popular architectures like VoxelMorph, ViT-V-Net, and TransMorph for consistent performance gains.

<br /><br />Summary: 
Medical image registration's quality is crucial, with U-Net networks using skip connections to improve representation. The Tetrahedron-Net method enhances this by adding an extra decoder for improved interactions and reuse of features, leading to better registration results. It surpasses existing methods on benchmarks and can be integrated into various architectures for consistent performance boosts. <div>
arXiv:2505.04380v1 Announce Type: new 
Abstract: Medical image registration plays a vital role in medical image processing. Extracting expressive representations for medical images is crucial for improving the registration quality. One common practice for this end is constructing a convolutional backbone to enable interactions with skip connections among feature extraction layers. The de facto structure, U-Net-like networks, has attempted to design skip connections such as nested or full-scale ones to connect one single encoder and one single decoder to improve its representation capacity. Despite being effective, it still does not fully explore interactions with a single encoder and decoder architectures. In this paper, we embrace this observation and introduce a simple yet effective alternative strategy to enhance the representations for registrations by appending one additional decoder. The new decoder is designed to interact with both the original encoder and decoder. In this way, it not only reuses feature presentation from corresponding layers in the encoder but also interacts with the original decoder to corporately give more accurate registration results. The new architecture is concise yet generalized, with only one encoder and two decoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net. Three instantiations of Tetrahedron-Net are further constructed regarding the different structures of the appended decoder. Our extensive experiments prove that superior performance can be obtained on several representative benchmarks of medical image registration. Finally, such a ``Tetrahedron'' design can also be easily integrated into popular U-Net-like architectures including VoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution</title>
<link>https://arxiv.org/abs/2505.04384</link>
<guid>https://arxiv.org/abs/2505.04384</guid>
<content:encoded><![CDATA[
<div> deepfake attribution, multi-DisentAnglement, contrastive learning, novel classes, OSS-DFA task

Summary:
The paper introduces an innovative framework called DATA for deepfake attribution, focusing on open-world semi-supervised scenarios. It introduces the concept of 'Orthonormal Deepfake Basis' to disentangle method-specific features and reduce overfitting. An augmented-memory mechanism aids in novel class discovery and contrastive learning, improving class boundaries. Bases contrastive loss and center contrastive loss enhance feature standardization and discrimination. Experimental results show that DATA outperforms existing methods, achieving notable accuracy improvements in the OSS-DFA benchmark. <div>
arXiv:2505.04384v1 Announce Type: new 
Abstract: Deepfake attribution (DFA) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. However, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. Additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. To address these issues, in this paper we propose an innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all generation techniques can be abstracted into a similar architecture, DATA defines the concept of 'Orthonormal Deepfake Basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. Furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. Additionally, to enhance the standardization and discrimination of features, DATA uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. Extensive experimental evaluations show that DATA achieves state-of-the-art performance on the OSS-DFA benchmark, e.g., there are notable accuracy improvements in 2.55% / 5.7% under different settings, compared with the existing methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle</title>
<link>https://arxiv.org/abs/2505.04392</link>
<guid>https://arxiv.org/abs/2505.04392</guid>
<content:encoded><![CDATA[
<div> novel approach, road surface anomalies, visual tracking, preceding vehicle, predictive detection<br />
<br />
Summary: 
A novel approach for detecting road surface anomalies using visual tracking of a preceding vehicle is proposed. The method is versatile, able to predict different types of anomalies without the need for specific visual detectors. It can operate in low visibility or dense traffic conditions, where anomalies are often occluded. By detecting anomalies predictively, the method allows for pre-configuration of vehicle systems or planning avoidance maneuvers in autonomous driving scenarios. The challenge of weak and disturbed camera signals is addressed through an efficient method to compensate for camera pitch rotation. Experimental results demonstrate reliable detection of road anomalies in various conditions, including imperfect road surfaces, with real-time performance on standard consumer hardware. <div>
arXiv:2505.04392v1 Announce Type: new 
Abstract: A novel approach to detect road surface anomalies by visual tracking of a preceding vehicle is proposed. The method is versatile, predicting any kind of road anomalies, such as potholes, bumps, debris, etc., unlike direct observation methods that rely on training visual detectors of those cases. The method operates in low visibility conditions or in dense traffic where the anomaly is occluded by a preceding vehicle. Anomalies are detected predictively, i.e., before a vehicle encounters them, which allows to pre-configure low-level vehicle systems (such as chassis) or to plan an avoidance maneuver in case of autonomous driving. A challenge is that the signal coming from camera-based tracking of a preceding vehicle may be weak and disturbed by camera ego motion due to vibrations affecting the ego vehicle. Therefore, we propose an efficient method to compensate camera pitch rotation by an iterative robust estimator. Our experiments on both controlled setup and normal traffic conditions show that road anomalies can be detected reliably at a distance even in challenging cases where the ego vehicle traverses imperfect road surfaces. The method is effective and performs in real time on standard consumer hardware.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer</title>
<link>https://arxiv.org/abs/2505.04394</link>
<guid>https://arxiv.org/abs/2505.04394</guid>
<content:encoded><![CDATA[
<div> Transformer, lip reading, SwinLip, visual speech encoder, computational load<br />
Summary:<br />
- The paper introduces SwinLip, a lightweight scale of the Swin Transformer for efficient visual speech encoding in lip reading.
- Traditional ResNet-based models have high computational complexity, while SwinLip integrates Conformer temporal embeddings with spatial embeddings to reduce computational load.
- The hierarchical structure and window self-attention of Swin Transformer enhance the performance and inference speed of the lip reading network.
- SwinLip outperforms existing models in English LRW and Mandarin LRW-1000 datasets for word and sentence recognition.
- The SwinLip model achieves state-of-the-art performance on the Mandarin LRW-1000 dataset with reduced computation, making it a promising solution for efficient lip reading tasks. <br /> <div>
arXiv:2505.04394v1 Announce Type: new 
Abstract: This paper presents an efficient visual speech encoder for lip reading. While most recent lip reading studies have been based on the ResNet architecture and have achieved significant success, they are not sufficiently suitable for efficiently capturing lip reading features due to high computational complexity in modeling spatio-temporal information. Additionally, using a complex visual model not only increases the complexity of lip reading models but also induces delays in the overall network for multi-modal studies (e.g., audio-visual speech recognition, speech enhancement, and speech separation). To overcome the limitations of Convolutional Neural Network (CNN)-based models, we apply the hierarchical structure and window self-attention of the Swin Transformer to lip reading. We configure a new lightweight scale of the Swin Transformer suitable for processing lip reading data and present the SwinLip visual speech encoder, which efficiently reduces computational load by integrating modified Convolution-augmented Transformer (Conformer) temporal embeddings with conventional spatial embeddings in the hierarchical structure. Through extensive experiments, we have validated that our SwinLip successfully improves the performance and inference speed of the lip reading network when applied to various backbones for word and sentence recognition, reducing computational load. In particular, our SwinLip demonstrated robust performance in both English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art performance on the Mandarin LRW-1000 dataset with less computation compared to the existing state-of-the-art model.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep residual learning with product units</title>
<link>https://arxiv.org/abs/2505.04397</link>
<guid>https://arxiv.org/abs/2505.04397</guid>
<content:encoded><![CDATA[
<div> product-unit, residual neural network, convolutional networks, parameter efficiency, multiplicative feature interactions

Summary:
The proposed deep product-unit residual neural network (PURe) integrates product units into residual blocks to enhance the expressiveness and parameter efficiency of deep convolutional networks. By incorporating product units, which allow for multiplicative feature interactions, PURe offers a more powerful representation of complex patterns compared to traditional networks. The use of 2D product units in the second layer of each residual block eliminates the need for nonlinear activation functions, preserving structural information. Experimental results on Galaxy10 DECaLS, ImageNet, and CIFAR-10 datasets demonstrate that PURe achieves high test accuracy, surpassing deeper ResNet models while using fewer parameters and computational resources. PURe exhibits faster convergence, robustness to noise, and competitive classification performance, highlighting its potential for scalable and reliable deep learning in computer vision. 

<br /><br />Summary: <div>
arXiv:2505.04397v1 Announce Type: new 
Abstract: We propose a deep product-unit residual neural network (PURe) that integrates product units into residual blocks to improve the expressiveness and parameter efficiency of deep convolutional networks. Unlike standard summation neurons, product units enable multiplicative feature interactions, potentially offering a more powerful representation of complex patterns. PURe replaces conventional convolutional layers with 2D product units in the second layer of each residual block, eliminating nonlinear activation functions to preserve structural information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS, PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper ResNet152, while converging nearly five times faster and demonstrating strong robustness to Poisson noise. On ImageNet, PURe architectures outperform standard ResNet models at similar depths, with PURe34 achieving a top-1 accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet variants (ResNet50, ResNet101) while utilizing significantly fewer parameters and computational resources. On CIFAR-10, PURe consistently outperforms ResNet variants across varying depths, with PURe272 reaching 95.01% test accuracy, comparable to ResNet1001 but at less than half the model size. These results demonstrate that PURe achieves a favorable balance between accuracy, efficiency, and robustness. Compared to traditional residual networks, PURe not only achieves competitive classification performance with faster convergence and fewer parameters, but also demonstrates greater robustness to noise. Its effectiveness across diverse datasets highlights the potential of product-unit-based architectures for scalable and reliable deep learning in computer vision.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFSeg: Efficient Multi-frame 3D Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.04408</link>
<guid>https://arxiv.org/abs/2505.04408</guid>
<content:encoded><![CDATA[
<div> Efficient multi-frame 3D semantic segmentation framework, Point cloud sequences, Feature aggregation, Regularized feature extraction, Lightweight MLP-based point decoder, nuScenes dataset, Waymo dataset <br />
Summary: <br />
MFSeg is introduced as an efficient multi-frame 3D semantic segmentation framework that aggregates point cloud sequences at the feature level and regulates the feature extraction and aggregation process. This reduces computational overhead while maintaining high accuracy. The method also utilizes a lightweight MLP-based point decoder, eliminating the need to upsample redundant points from past frames. Experiments conducted on the nuScenes and Waymo datasets demonstrate that MFSeg surpasses existing methods, showcasing its effectiveness and efficiency in the field of semantic segmentation for 3D point clouds. <div>
arXiv:2505.04408v1 Announce Type: new 
Abstract: We propose MFSeg, an efficient multi-frame 3D semantic segmentation framework. By aggregating point cloud sequences at the feature level and regularizing the feature extraction and aggregation process, MFSeg reduces computational overhead while maintaining high accuracy. Moreover, by employing a lightweight MLP-based point decoder, our method eliminates the need to upsample redundant points from past frames. Experiments on the nuScenes and Waymo datasets show that MFSeg outperforms existing methods, demonstrating its effectiveness and efficiency.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception</title>
<link>https://arxiv.org/abs/2505.04410</link>
<guid>https://arxiv.org/abs/2505.04410</guid>
<content:encoded><![CDATA[
<div> Keywords: Dense visual prediction, Vision-Language Models, CLIP, DeCLIP, Object detection, Semantic segmentation

Summary:
DeCLIP addresses limitations in dense visual prediction tasks by enhancing CLIP through a new framework. The observation that CLIP struggles to aggregate information effectively from related regions is addressed by decoupling the self-attention module into "content" and "context" features. The "content" features align with image crop representations to improve local discriminability, while the "context" features learn to retain spatial correlations guided by vision foundation models like DINO. Extensive experiments show that DeCLIP outperforms existing methods in open-vocabulary dense prediction tasks, including object detection and semantic segmentation. This approach improves local feature representation, leading to enhanced performance in tasks where visual concepts are unbounded. The code for DeCLIP is available on GitHub for further exploration and implementation.  

<br /><br />Summary: <div>
arXiv:2505.04410v1 Announce Type: new 
Abstract: Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at \textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation</title>
<link>https://arxiv.org/abs/2505.04424</link>
<guid>https://arxiv.org/abs/2505.04424</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, arbitrary style transfer, lightweight model, uncertainty-aware multi-task learning, high-quality results

Summary: 
RLMiniStyler is a novel reinforcement learning-based framework for arbitrary style transfer, aiming to generate diverse stylized results in a lightweight manner. It uses a unified reinforcement learning policy to guide the style transfer process, resulting in smooth sequences of stylized images. Additionally, an uncertainty-aware multi-task learning strategy adjusts loss weights dynamically to achieve a balance between content and style requirements, improving model convergence. Experimental results demonstrate the superiority of RLMiniStyler over existing methods in generating high-quality artistic image sequences at a lower computational cost. The code for RLMiniStyler is available on GitHub for further exploration. <div>
arXiv:2505.04424v1 Announce Type: new 
Abstract: Arbitrary style transfer aims to apply the style of any given artistic image to another content image. Still, existing deep learning-based methods often require significant computational costs to generate diverse stylized results. Motivated by this, we propose a novel reinforcement learning-based framework for arbitrary style transfer RLMiniStyler. This framework leverages a unified reinforcement learning policy to iteratively guide the style transfer process by exploring and exploiting stylization feedback, generating smooth sequences of stylized results while achieving model lightweight. Furthermore, we introduce an uncertainty-aware multi-task learning strategy that automatically adjusts loss weights to adapt to the content and style balance requirements at different training stages, thereby accelerating model convergence. Through a series of experiments across image various resolutions, we have validated the advantages of RLMiniStyler over other state-of-the-art methods in generating high-quality, diverse artistic image sequences at a lower cost. Codes are available at https://github.com/fengxiaoming520/RLMiniStyler.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Real Facial Concepts for Independent Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.04460</link>
<guid>https://arxiv.org/abs/2505.04460</guid>
<content:encoded><![CDATA[
<div> concept capture module, real faces, deepfake detection, forgery artifacts, generalization

Summary:
- The article introduces RealID, a novel approach to improve deepfake detection models' generalization ability.
- RealID consists of two key modules: the Real Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier (IDC).
- The RealC2 module uses a MultiReal Memory to maintain various prototypes for real faces, helping the model develop a comprehensive understanding of real faces.
- The IDC module redefines the classification strategy by making independent decisions based on real face concepts and forgery artifacts.
- Extensive experiments on five popular datasets show that RealID outperforms existing methods by achieving a 1.74% improvement in average accuracy.<br /><br />Summary: <div>
arXiv:2505.04460v1 Announce Type: new 
Abstract: Deepfake detection models often struggle with generalization to unseen datasets, manifesting as misclassifying real instances as fake in target domains. This is primarily due to an overreliance on forgery artifacts and a limited understanding of real faces. To address this challenge, we propose a novel approach RealID to enhance generalization by learning a comprehensive concept of real faces while assessing the probabilities of belonging to the real and fake classes independently. RealID comprises two key modules: the Real Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier (IDC). With the assistance of a MultiReal Memory, RealC2 maintains various prototypes for real faces, allowing the model to capture a comprehensive concept of real class. Meanwhile, IDC redefines the classification strategy by making independent decisions based on the concept of the real class and the presence of forgery artifacts. Through the combined effect of the above modules, the influence of forgery-irrelevant patterns is alleviated, and extensive experiments on five widely used datasets demonstrate that RealID significantly outperforms existing state-of-the-art methods, achieving a 1.74% improvement in average accuracy.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation</title>
<link>https://arxiv.org/abs/2505.04481</link>
<guid>https://arxiv.org/abs/2505.04481</guid>
<content:encoded><![CDATA[
<div> parametric sequences, computer-aided design, Large Language Models, CAD-Llama, 3D shapes <br />
Summary: <br />
This study explores the use of Large Language Models (LLMs) for generating parametric sequences for computer-aided design (CAD) models. The research presents CAD-Llama, a framework that enhances LLMs for creating parametric 3D CAD models. A hierarchical annotation pipeline and code-like format are utilized to translate parametric CAD command sequences into Structured Parametric CAD Code (SPCC). An adaptive pretraining approach using SPCC and instruction tuning aligned with CAD guidelines is proposed to imbue LLMs with spatial knowledge. Experimental results show that CAD-Llama outperforms previous autoregressive methods and LLM baselines, showcasing its effectiveness in generating parametric 3D CAD models. <br /> <div>
arXiv:2505.04481v1 Announce Type: new 
Abstract: Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging</title>
<link>https://arxiv.org/abs/2505.04485</link>
<guid>https://arxiv.org/abs/2505.04485</guid>
<content:encoded><![CDATA[
arXiv:2505.04485v1 Announce Type: new 
Abstract: We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural network architecture built on top of the well-known KPConv, a widely adopted backbone for 3D point cloud analysis. Even though invariance and/or equivariance to Euclidean transformations are required for many common tasks, KPConv-based networks can only approximately achieve such properties when training on large datasets or with significant data augmentations. Using Frame Averaging, we allow to flexibly customize point cloud neural networks built with KPConv layers, by making them exactly invariant and/or equivariant to translations, rotations and/or reflections of the input point clouds. By simply wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical prior knowledge into it while preserving the number of learnable parameters and not compromising any input information. We showcase the benefit of such an introduced bias for point cloud classification and point cloud registration, especially in challenging cases such as scarce training data or randomly rotated test data.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Flow Matching using Latent Variables</title>
<link>https://arxiv.org/abs/2505.04486</link>
<guid>https://arxiv.org/abs/2505.04486</guid>
<content:encoded><![CDATA[
arXiv:2505.04486v1 Announce Type: new 
Abstract: Flow matching models have shown great potential in image generation tasks among probabilistic generative models. Building upon the ideas of continuous normalizing flows, flow matching models generalize the transport path of the diffusion models from a simple prior distribution to the data. Most flow matching models in the literature do not explicitly model the underlying structure/manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance. To this end, we present \texttt{Latent-CFM}, which provides simplified training/inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models. Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that \texttt{Latent-CFM} exhibits improved generation quality with significantly less training ($\sim 50\%$ less in some cases) and computation than state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments</title>
<link>https://arxiv.org/abs/2505.04488</link>
<guid>https://arxiv.org/abs/2505.04488</guid>
<content:encoded><![CDATA[
arXiv:2505.04488v1 Announce Type: new 
Abstract: The visually impaired population, especially the severely visually impaired, is currently large in scale, and daily activities pose significant challenges for them. Although many studies use large language and vision-language models to assist the blind, most focus on static content and fail to meet real-time perception needs in dynamic and complex environments, such as daily activities. To provide them with more effective intelligent assistance, it is imperative to incorporate advanced visual understanding technologies. Although real-time vision and speech interaction VideoLLMs demonstrate strong real-time visual understanding, no prior work has systematically evaluated their effectiveness in assisting visually impaired individuals. In this work, we conduct the first such evaluation. First, we construct a benchmark dataset (VisAssistDaily), covering three categories of assistive tasks for visually impaired individuals: Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that GPT-4o achieves the highest task success rate. Next, we conduct a user study to evaluate the models in both closed-world and open-world scenarios, further exploring the practical challenges of applying VideoLLMs in assistive contexts. One key issue we identify is the difficulty current models face in perceiving potential hazards in dynamic environments. To address this, we build an environment-awareness dataset named SafeVid and introduce a polling mechanism that enables the model to proactively detect environmental risks. We hope this work provides valuable insights and inspiration for future research in this field.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defining and Quantifying Creative Behavior in Popular Image Generators</title>
<link>https://arxiv.org/abs/2505.04497</link>
<guid>https://arxiv.org/abs/2505.04497</guid>
<content:encoded><![CDATA[
arXiv:2505.04497v1 Announce Type: new 
Abstract: Creativity of generative AI models has been a subject of scientific debate in the last years, without a conclusive answer. In this paper, we study creativity from a practical perspective and introduce quantitative measures that help the user to choose a suitable AI model for a given task. We evaluated our measures on a number of popular image-to-image generation models, and the results of this suggest that our measures conform to human intuition.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition</title>
<link>https://arxiv.org/abs/2505.04502</link>
<guid>https://arxiv.org/abs/2505.04502</guid>
<content:encoded><![CDATA[
arXiv:2505.04502v1 Announce Type: new 
Abstract: Video face detection and recognition in public places at the edge is required in several applications, such as security reinforcement and contactless access to authorized venues. This paper aims to maximize the simultaneous usage of hardware engines available in edge GPUs nowadays by leveraging the concurrency and pipelining of tasks required for face detection and recognition. This also includes the video decoding task, which is required in most face monitoring applications as the video streams are usually carried via Gbps Ethernet network. This constitutes an improvement over previous works where the tasks are usually allocated to a single engine due to the lack of a unified and automated framework that simultaneously explores all hardware engines. In addition, previously, the input faces were usually embedded in still images or within raw video streams that overlook the burst delay caused by the decoding stage. The results on real-life video streams suggest that simultaneously using all the hardware engines available in the recent NVIDIA edge Orin GPU, higher throughput, and a slight saving of power consumption of around 300 mW, accounting for around 5%, have been achieved while satisfying the real-time performance constraint. The performance gets even higher by considering several video streams simultaneously. Further performance improvement could have been obtained if the number of shuffle layers that were created by the tensor RT framework for the face recognition task was lower. Thus, the paper suggests some hardware improvements to the existing edge GPU processors to enhance their performance even higher.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation</title>
<link>https://arxiv.org/abs/2505.04512</link>
<guid>https://arxiv.org/abs/2505.04512</guid>
<content:encoded><![CDATA[
arXiv:2505.04512v1 Announce Type: new 
Abstract: Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model</title>
<link>https://arxiv.org/abs/2505.04522</link>
<guid>https://arxiv.org/abs/2505.04522</guid>
<content:encoded><![CDATA[
arXiv:2505.04522v1 Announce Type: new 
Abstract: Generating 3D CT volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research. In this paper, we introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual descriptions using the diffusion model. Unlike previous methods that rely on fixed-format text input, Text2CT employs a novel prompt formulation that enables generation from diverse, free-text descriptions. The proposed framework encodes medical text into latent representations and decodes them into high-resolution 3D CT scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3D framework. Our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text. Extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration</title>
<link>https://arxiv.org/abs/2505.04524</link>
<guid>https://arxiv.org/abs/2505.04524</guid>
<content:encoded><![CDATA[
arXiv:2505.04524v1 Announce Type: new 
Abstract: Cost-effective machine vision systems dedicated to real-time and accurate face detection and recognition in public places are crucial for many modern applications. However, despite their high performance, which could be reached using specialized edge or cloud AI hardware accelerators, there is still room for improvement in throughput and power consumption. This paper aims to suggest a combined hardware-software approach that optimizes face detection and recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX Orin. First, it leverages the simultaneous usage of all its hardware engines to improve processing time. This offers an improvement over previous works where these tasks were mainly allocated automatically and exclusively to the CPU or, to a higher extent, to the GPU core. Additionally, the paper suggests integrating a face tracker module to avoid redundantly running the face recognition algorithm for every frame but only when a new face appears in the scene. The results of extended experiments suggest that simultaneous usage of all the hardware engines that are available in the Orin GPU and tracker integration into the pipeline yield an impressive throughput of 290 FPS (frames per second) on 1920 x 1080 input size frames containing in average of 6 faces/frame. Additionally, a substantial saving of power consumption of around 800 mW was achieved when compared to running the task on the CPU/GPU engines only and without integrating a tracker into the Orin GPU\'92s pipeline. This hardware-codesign approach can pave the way to design high-performance machine vision systems at the edge, critically needed in video monitoring in public places where several nearby cameras are usually deployed for a same scene.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once</title>
<link>https://arxiv.org/abs/2505.04526</link>
<guid>https://arxiv.org/abs/2505.04526</guid>
<content:encoded><![CDATA[
arXiv:2505.04526v1 Announce Type: new 
Abstract: Visible and infrared image fusion is one of the most crucial tasks in the field of image fusion, aiming to generate fused images with clear structural information and high-quality texture features for high-level vision tasks. However, when faced with severe illumination degradation in visible images, the fusion results of existing image fusion methods often exhibit blurry and dim visual effects, posing major challenges for autonomous driving. To this end, a Darkness-Free network is proposed to handle Visible and infrared image disentanglement and fusion all at Once (DFVO), which employs a cascaded multi-task approach to replace the traditional two-stage cascaded training (enhancement and fusion), addressing the issue of information entropy loss caused by hierarchical data transmission. Specifically, we construct a latent-common feature extractor (LCFE) to obtain latent features for the cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised to acquire high-frequency semantic information. Secondly, we design a hyper cross-attention module (HCAM) to extract low-frequency information and preserve texture features from source images. Finally, a relevant loss function is designed to guide the holistic network learning, thereby achieving better image fusion. Extensive experiments demonstrate that our proposed approach outperforms state-of-the-art alternatives in terms of qualitative and quantitative evaluations. Particularly, DFVO can generate clearer, more informative, and more evenly illuminated fusion results in the dark environments, achieving best performance on the LLVIP dataset with 63.258 dB PSNR and 0.724 CC, providing more effective information for high-level vision tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAFT: Robust Augmentation of FeaTures for Image Segmentation</title>
<link>https://arxiv.org/abs/2505.04529</link>
<guid>https://arxiv.org/abs/2505.04529</guid>
<content:encoded><![CDATA[
arXiv:2505.04529v1 Announce Type: new 
Abstract: Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.
  To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real "SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Registration of 3D Point Sets Using Exponential-based Similarity Matrix</title>
<link>https://arxiv.org/abs/2505.04540</link>
<guid>https://arxiv.org/abs/2505.04540</guid>
<content:encoded><![CDATA[
arXiv:2505.04540v1 Announce Type: new 
Abstract: Point cloud registration is a fundamental problem in computer vision and robotics, involving the alignment of 3D point sets captured from varying viewpoints using depth sensors such as LiDAR or structured light. In modern robotic systems, especially those focused on mapping, it is essential to merge multiple views of the same environment accurately. However, state-of-the-art registration techniques often struggle when large rotational differences exist between point sets or when the data is significantly corrupted by sensor noise. These challenges can lead to misalignments and, consequently, to inaccurate or distorted 3D reconstructions. In this work, we address both these limitations by proposing a robust modification to the classic Iterative Closest Point (ICP) algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP), integrates a Gaussian-inspired exponential weighting scheme to construct a similarity matrix that dynamically adapts across iterations. This matrix facilitates improved estimation of both rotational and translational components during alignment. We demonstrate the robustness of ESM-ICP in two challenging scenarios: (i) large rotational discrepancies between the source and target point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show that ESM-ICP outperforms traditional geometric registration techniques as well as several recent learning-based methods. To encourage reproducibility and community engagement, our full implementation is made publicly available on GitHub. https://github.com/aralab-unr/ESM_ICP
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Componential Prompt-Knowledge Alignment for Domain Incremental Learning</title>
<link>https://arxiv.org/abs/2505.04575</link>
<guid>https://arxiv.org/abs/2505.04575</guid>
<content:encoded><![CDATA[
arXiv:2505.04575v1 Announce Type: new 
Abstract: Domain Incremental Learning (DIL) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge. Although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. This arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.To address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a novel prompt-based DIL method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. KA-Prompt operates in two phases: (1) Initial Componential Structure Configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts. (2) Online Alignment Preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve. Extensive experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt. Our source code is available at https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Sampling for MRI-based Sequential Decision Making</title>
<link>https://arxiv.org/abs/2505.04586</link>
<guid>https://arxiv.org/abs/2505.04586</guid>
<content:encoded><![CDATA[
arXiv:2505.04586v1 Announce Type: new 
Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</title>
<link>https://arxiv.org/abs/2505.04594</link>
<guid>https://arxiv.org/abs/2505.04594</guid>
<content:encoded><![CDATA[
arXiv:2505.04594v1 Announce Type: new 
Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning</title>
<link>https://arxiv.org/abs/2505.04601</link>
<guid>https://arxiv.org/abs/2505.04601</guid>
<content:encoded><![CDATA[
arXiv:2505.04601v1 Announce Type: new 
Abstract: OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastMap: Revisiting Dense and Scalable Structure from Motion</title>
<link>https://arxiv.org/abs/2505.04612</link>
<guid>https://arxiv.org/abs/2505.04612</guid>
<content:encoded><![CDATA[
arXiv:2505.04612v1 Announce Type: new 
Abstract: We propose FastMap, a new global structure from motion method focused on speed and simplicity. Previous methods like COLMAP and GLOMAP are able to estimate high-precision camera poses, but suffer from poor scalability when the number of matched keypoint pairs becomes large. We identify two key factors leading to this problem: poor parallelization and computationally expensive optimization steps. To overcome these issues, we design an SfM framework that relies entirely on GPU-friendly operations, making it easily parallelizable. Moreover, each optimization step runs in time linear to the number of image pairs, independent of keypoint pairs or 3D points. Through extensive experiments, we show that FastMap is one to two orders of magnitude faster than COLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait</title>
<link>https://arxiv.org/abs/2505.04616</link>
<guid>https://arxiv.org/abs/2505.04616</guid>
<content:encoded><![CDATA[
arXiv:2505.04616v1 Announce Type: new 
Abstract: We address the problem of whole-body person recognition in unconstrained environments. This problem arises in surveillance scenarios such as those in the IARPA Biometric Recognition and Identification at Altitude and Range (BRIAR) program, where biometric data is captured at long standoff distances, elevated viewing angles, and under adverse atmospheric conditions (e.g., turbulence and high wind velocity). To this end, we propose FarSight, a unified end-to-end system for person recognition that integrates complementary biometric cues across face, gait, and body shape modalities. FarSight incorporates novel algorithms across four core modules: multi-subject detection and tracking, recognition-aware video restoration, modality-specific biometric feature encoding, and quality-guided multi-modal fusion. These components are designed to work cohesively under degraded image conditions, large pose and scale variations, and cross-domain gaps. Extensive experiments on the BRIAR dataset, one of the most comprehensive benchmarks for long-range, multi-modal biometric recognition, demonstrate the effectiveness of FarSight. Compared to our preliminary system, this system achieves a 34.1% absolute gain in 1:1 verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set identification (Rank-20), and a 34.3% reduction in open-set identification errors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE Face in Video Evaluation (FIVE), which conducts standardized face recognition testing on the BRIAR dataset. These results establish FarSight as a state-of-the-art solution for operational biometric recognition in challenging real-world conditions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Path to Multimodal Generalist: General-Level and General-Bench</title>
<link>https://arxiv.org/abs/2505.04620</link>
<guid>https://arxiv.org/abs/2505.04620</guid>
<content:encoded><![CDATA[
arXiv:2505.04620v1 Announce Type: new 
Abstract: The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation</title>
<link>https://arxiv.org/abs/2505.03757</link>
<guid>https://arxiv.org/abs/2505.03757</guid>
<content:encoded><![CDATA[
arXiv:2505.03757v1 Announce Type: cross 
Abstract: Coordinate transformation models often fail to account for nonlinear and spatially dependent distortions, leading to significant residual errors in geospatial applications. Here we propose a residual-based neural correction strategy, in which a neural network learns to model only the systematic distortions left by an initial geometric transformation. By focusing solely on residual patterns, the proposed method reduces model complexity and improves performance, particularly in scenarios with sparse or structured control point configurations. We evaluate the method using both simulated datasets with varying distortion intensities and sampling strategies, as well as under the real-world image georeferencing tasks. Compared with direct neural network coordinate converter and classical transformation models, the residual-based neural correction delivers more accurate and stable results under challenging conditions, while maintaining comparable performance in ideal cases. These findings demonstrate the effectiveness of residual modelling as a lightweight and robust alternative for improving coordinate transformation accuracy.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding</title>
<link>https://arxiv.org/abs/2505.03788</link>
<guid>https://arxiv.org/abs/2505.03788</guid>
<content:encoded><![CDATA[
arXiv:2505.03788v1 Announce Type: cross 
Abstract: We introduce a novel approach for calibrating uncertainty quantification (UQ) tailored for multi-modal large language models (LLMs). Existing state-of-the-art UQ methods rely on consistency among multiple responses generated by the LLM on an input query under diverse settings. However, these approaches often report higher confidence in scenarios where the LLM is consistently incorrect. This leads to a poorly calibrated confidence with respect to accuracy. To address this, we leverage cross-modal consistency in addition to self-consistency to improve the calibration of the multi-modal models. Specifically, we ground the textual responses to the visual inputs. The confidence from the grounding model is used to calibrate the overall confidence. Given that using a grounding model adds its own uncertainty in the pipeline, we apply temperature scaling - a widely accepted parametric calibration technique - to calibrate the grounding model's confidence in the accuracy of generated responses. We evaluate the proposed approach across multiple multi-modal tasks, such as medical question answering (Slake) and visual question answering (VQAv2), considering multi-modal models such as LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework achieves significantly improved calibration on both tasks.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Design description of Wisdom Computing Persperctive</title>
<link>https://arxiv.org/abs/2505.03800</link>
<guid>https://arxiv.org/abs/2505.03800</guid>
<content:encoded><![CDATA[
arXiv:2505.03800v1 Announce Type: cross 
Abstract: This course design aims to develop and research a handwriting matrix recognition and step-by-step visual calculation process display system, addressing the issue of abstract formulas and complex calculation steps that students find difficult to understand when learning mathematics. By integrating artificial intelligence with visualization animation technology, the system enhances precise recognition of handwritten matrix content through the introduction of Mamba backbone networks, completes digital extraction and matrix reconstruction using the YOLO model, and simultaneously combines CoordAttention coordinate attention mechanisms to improve the accurate grasp of character spatial positions. The calculation process is demonstrated frame by frame through the Manim animation engine, vividly showcasing each mathematical calculation step, helping students intuitively understand the intrinsic logic of mathematical operations. Through dynamically generating animation processes for different computational tasks, the system exhibits high modularity and flexibility, capable of generating various mathematical operation examples in real-time according to student needs. By innovating human-computer interaction methods, it brings mathematical calculation processes to life, helping students bridge the gap between knowledge and understanding on a deeper level, ultimately achieving a learning experience where "every step is understood." The system's scalability and interactivity make it an intuitive, user-friendly, and efficient auxiliary tool in education.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Facilitating Video Story Interaction with Multi-Agent Collaborative System</title>
<link>https://arxiv.org/abs/2505.03807</link>
<guid>https://arxiv.org/abs/2505.03807</guid>
<content:encoded><![CDATA[
arXiv:2505.03807v1 Announce Type: cross 
Abstract: Video story interaction enables viewers to engage with and explore narrative content for personalized experiences. However, existing methods are limited to user selection, specially designed narratives, and lack customization. To address this, we propose an interactive system based on user intent. Our system uses a Vision Language Model (VLM) to enable machines to understand video stories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent System (MAS) to create evolving characters and scene experiences. It includes three stages: 1) Video story processing, utilizing VLM and prior knowledge to simulate human understanding of stories across three modalities. 2) Multi-space chat, creating growth-oriented characters through MAS interactions based on user queries and story stages. 3) Scene customization, expanding and visualizing various story scenes mentioned in dialogue. Applied to the Harry Potter series, our study shows the system effectively portrays emergent character social behavior and growth, enhancing the interactive experience in the video story world.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data</title>
<link>https://arxiv.org/abs/2505.03808</link>
<guid>https://arxiv.org/abs/2505.03808</guid>
<content:encoded><![CDATA[
arXiv:2505.03808v1 Announce Type: cross 
Abstract: Harmful algal blooms are a growing threat to inland water quality and public health worldwide, creating an urgent need for efficient, accurate, and cost-effective detection methods. This research introduces a high-performing methodology that integrates multiple open-source remote sensing data with advanced artificial intelligence models. Key data sources include Copernicus Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently retrieved using platforms like Google Earth Engine (GEE) and Microsoft Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the altitude from the elevation model, the temperature and wind from NOAA as well as the longitude and latitude were the most important features. The approach combines two types of machine learning models, tree-based models and a neural network, into an ensemble for classifying algal bloom severity. While the tree models performed strongly on their own, incorporating a neural network added robustness and demonstrated how deep learning models can effectively use diverse remote sensing inputs. The method leverages high-resolution satellite imagery and AI-driven analysis to monitor algal blooms dynamically, and although initially developed for a NASA competition in the U.S., it shows potential for global application. The complete code is available for further adaptation and practical implementation, illustrating the convergence of remote sensing data and AI to address critical environmental challenges (https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Dynamic Data Selection Meets Data Augmentation</title>
<link>https://arxiv.org/abs/2505.03809</link>
<guid>https://arxiv.org/abs/2505.03809</guid>
<content:encoded><![CDATA[
arXiv:2505.03809v1 Announce Type: cross 
Abstract: Dynamic data selection aims to accelerate training with lossless performance. However, reducing training data inherently limits data diversity, potentially hindering generalization. While data augmentation is widely used to enhance diversity, it is typically not optimized in conjunction with selection. As a result, directly combining these techniques fails to fully exploit their synergies. To tackle the challenge, we propose a novel online data training framework that, for the first time, unifies dynamic data selection and augmentation, achieving both training efficiency and enhanced performance. Our method estimates each sample's joint distribution of local density and multimodal semantic consistency, allowing for the targeted selection of augmentation-suitable samples while suppressing the inclusion of noisy or ambiguous data. This enables a more significant reduction in dataset size without sacrificing model generalization. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches on various benchmark datasets and architectures, e.g., reducing 50\% training costs on ImageNet-1k with lossless performance. Furthermore, our approach enhances noise resistance and improves model robustness, reinforcing its practical utility in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery</title>
<link>https://arxiv.org/abs/2505.03836</link>
<guid>https://arxiv.org/abs/2505.03836</guid>
<content:encoded><![CDATA[
arXiv:2505.03836v1 Announce Type: cross 
Abstract: Oracle Bone Inscription (OBI) is the earliest systematic writing system in China, while the identification of Oracle Bone (OB) duplicates is a fundamental issue in OBI research. In this work, we design a progressive OB duplicate discovery framework that combines unsupervised low-level keypoints matching with high-level text-centric content-based matching to refine and rank the candidate OB duplicates with semantic awareness and interpretability. We compare our approach with state-of-the-art content-based image retrieval and image matching methods, showing that our approach yields comparable recall performance and the highest simplified mean reciprocal rank scores for both Top-5 and Top-15 retrieval results, and with significantly accelerated computation efficiency. We have discovered over 60 pairs of new OB duplicates in real-world deployment, which were missed by OBI researchers for decades. The models, video illustration and demonstration of this work are available at: https://github.com/cszhangLMU/OBD-Finder/.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification</title>
<link>https://arxiv.org/abs/2505.03838</link>
<guid>https://arxiv.org/abs/2505.03838</guid>
<content:encoded><![CDATA[
arXiv:2505.03838v1 Announce Type: cross 
Abstract: Precise and effective processing of cardiac imaging data is critical for the identification and management of the cardiovascular diseases. We introduce IntelliCardiac, a comprehensive, web-based medical image processing platform for the automatic segmentation of 4D cardiac images and disease classification, utilizing an AI model trained on the publicly accessible ACDC dataset. The system, intended for patients, cardiologists, and healthcare professionals, offers an intuitive interface and uses deep learning models to identify essential heart structures and categorize cardiac diseases. The system supports analysis of both the right and left ventricles as well as myocardium, and then classifies patient's cardiac images into five diagnostic categories: dilated cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right ventricular abnormality, and no disease. IntelliCardiac combines a deep learning-based segmentation model with a two-step classification pipeline. The segmentation module gains an overall accuracy of 92.6\%. The classification module, trained on characteristics taken from segmented heart structures, achieves 98\% accuracy in five categories. These results exceed the performance of the existing state-of-the-art methods that integrate both segmentation and classification models. IntelliCardiac, which supports real-time visualization, workflow integration, and AI-assisted diagnostics, has great potential as a scalable, accurate tool for clinical decision assistance in cardiac imaging and diagnosis.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coverage Biases in High-Resolution Satellite Imagery</title>
<link>https://arxiv.org/abs/2505.03842</link>
<guid>https://arxiv.org/abs/2505.03842</guid>
<content:encoded><![CDATA[
arXiv:2505.03842v1 Announce Type: cross 
Abstract: Satellite imagery is increasingly used to complement traditional data collection approaches such as surveys and censuses across scientific disciplines. However, we ask: Do all places on earth benefit equally from this new wealth of information? In this study, we investigate coverage bias of major satellite constellations that provide optical satellite imagery with a ground sampling distance below 10 meters, evaluating both the future on-demand tasking opportunities as well as the availability of historic images across the globe. Specifically, forward-looking, we estimate how often different places are revisited during a window of 30 days based on the satellites' orbital paths, thus investigating potential coverage biases caused by physical factors. We find that locations farther away from the equator are generally revisited more frequently by the constellations under study. Backward-looking, we show that historic satellite image availability -- based on metadata collected from major satellite imagery providers -- is influenced by socio-economic factors on the ground: less developed, less populated places have less satellite images available. Furthermore, in three small case studies on recent conflict regions in this world, namely Gaza, Sudan and Ukraine, we show that also geopolitical events play an important role in satellite image availability, hinting at underlying business model decisions. These insights lay bare that the digital dividend yielded by satellite imagery is not equally distributed across our planet.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation</title>
<link>https://arxiv.org/abs/2505.03844</link>
<guid>https://arxiv.org/abs/2505.03844</guid>
<content:encoded><![CDATA[
arXiv:2505.03844v1 Announce Type: cross 
Abstract: The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising solution to augment this scarce data, enabling a broader range of applications. Leveraging over 15 years of ONERA's extensive archival airborn data from acquisition campaigns, we created a comprehensive training dataset of 110 thousands SAR images to exploit a 3.5 billion parameters pre-trained latent diffusion model. In this work, we present a novel approach utilizing spatial conditioning techniques within a foundation model to transform satellite SAR imagery into airborne SAR representations. Additionally, we demonstrate that our pipeline is effective for bridging the realism of simulated images generated by ONERA's physics-based simulator EMPRISE. Our method explores a key application of AI in advancing SAR imaging technology. To the best of our knowledge, we are the first to introduce this approach in the literature.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos</title>
<link>https://arxiv.org/abs/2505.03845</link>
<guid>https://arxiv.org/abs/2505.03845</guid>
<content:encoded><![CDATA[
arXiv:2505.03845v1 Announce Type: cross 
Abstract: Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with motor and non-motor symptoms. Depressive symptoms are prevalent in PD, affecting up to 45% of patients. They are often underdiagnosed due to overlapping motor features, such as hypomimia. This study explores deep learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention layers-to assess the presence and severity of depressive symptoms, as detected by the Geriatric Depression Scale (GDS), in PD patients through facial video analysis. The same parameters were assessed in a secondary analysis taking into account whether patients were one hour after (ON-medication state) or 12 hours without (OFF-medication state) dopaminergic medication. Using a dataset of 1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest performance, with up to 94% accuracy and 93.7% F1-score in binary classification (presence of absence of depressive symptoms), and 87.1% accuracy with an 85.4% F1-score in multiclass tasks (absence or mild or severe depressive symptoms).
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators</title>
<link>https://arxiv.org/abs/2505.03859</link>
<guid>https://arxiv.org/abs/2505.03859</guid>
<content:encoded><![CDATA[
arXiv:2505.03859v1 Announce Type: cross 
Abstract: Advances in multimodal machine learning have made text-to-image (T2I) models increasingly accessible and popular. However, T2I models introduce risks such as the generation of non-consensual depictions of identifiable individuals, otherwise known as deepfakes. This paper presents an empirical study exploring the accessibility of deepfake model variants online. Through a metadata analysis of thousands of publicly downloadable model variants on two popular repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily accessible deepfake models. Almost 35,000 examples of publicly downloadable deepfake model variants are identified, primarily hosted on Civitai. These deepfake models have been downloaded almost 15 million times since November 2022, with the models targeting a range of individuals from global celebrities to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux models are used for the creation of deepfake models, with 96% of these targeting women and many signalling intent to generate non-consensual intimate imagery (NCII). Deepfake model variants are often created via the parameter-efficient fine-tuning technique known as low rank adaptation (LoRA), requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this process widely accessible via consumer-grade computers. Despite these models violating the Terms of Service of hosting platforms, and regulation seeking to prevent dissemination, these results emphasise the pressing need for greater action to be taken against the creation of deepfakes and NCII.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.03912</link>
<guid>https://arxiv.org/abs/2505.03912</guid>
<content:encoded><![CDATA[
arXiv:2505.03912v1 Announce Type: cross 
Abstract: Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prototype-Based Information Compensation Network for Multi-Source Remote Sensing Data Classification</title>
<link>https://arxiv.org/abs/2505.04003</link>
<guid>https://arxiv.org/abs/2505.04003</guid>
<content:encoded><![CDATA[
arXiv:2505.04003v1 Announce Type: cross 
Abstract: Multi-source remote sensing data joint classification aims to provide accuracy and reliability of land cover classification by leveraging the complementary information from multiple data sources. Existing methods confront two challenges: inter-frequency multi-source feature coupling and inconsistency of complementary information exploration. To solve these issues, we present a Prototype-based Information Compensation Network (PICNet) for land cover classification based on HSI and SAR/LiDAR data. Specifically, we first design a frequency interaction module to enhance the inter-frequency coupling in multi-source feature extraction. The multi-source features are first decoupled into high- and low-frequency components. Then, these features are recoupled to achieve efficient inter-frequency communication. Afterward, we design a prototype-based information compensation module to model the global multi-source complementary information. Two sets of learnable modality prototypes are introduced to represent the global modality information of multi-source data. Subsequently, cross-modal feature integration and alignment are achieved through cross-attention computation between the modality-specific prototype vectors and the raw feature representations. Extensive experiments on three public datasets demonstrate the significant superiority of our PICNet over state-of-the-art methods. The codes are available at https://github.com/oucailab/PICNet.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models</title>
<link>https://arxiv.org/abs/2505.04050</link>
<guid>https://arxiv.org/abs/2505.04050</guid>
<content:encoded><![CDATA[
arXiv:2505.04050v1 Announce Type: cross 
Abstract: 3D terrain models are essential in fields such as video game development and film production. Since surface color often correlates with terrain geometry, capturing this relationship is crucial to achieving realism. However, most existing methods generate either a heightmap or a texture, without sufficiently accounting for the inherent correlation. In this paper, we propose a method that jointly generates terrain heightmaps and textures using a latent diffusion model. First, we train the model in an unsupervised manner to randomly generate paired heightmaps and textures. Then, we perform supervised learning of an external adapter to enable user control via hand-drawn sketches. Experiments show that our approach allows intuitive terrain generation while preserving the correlation between heightmaps and textures.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control</title>
<link>https://arxiv.org/abs/2505.04052</link>
<guid>https://arxiv.org/abs/2505.04052</guid>
<content:encoded><![CDATA[
arXiv:2505.04052v1 Announce Type: cross 
Abstract: Compositing human figures into scene images has broad applications in areas such as entertainment and advertising. However, existing methods often cannot handle occlusion of the inserted person by foreground objects and unnaturally place the person in the frontmost layer. Moreover, they offer limited control over the inserted person's pose. To address these challenges, we propose two methods. Both allow explicit pose control via a 3D body model and leverage latent diffusion models to synthesize the person at a contextually appropriate depth, naturally handling occlusions without requiring occlusion masks. The first is a two-stage approach: the model first learns a depth map of the scene with the person through supervised learning, and then synthesizes the person accordingly. The second method learns occlusion implicitly and synthesizes the person directly from input data without explicit depth supervision. Quantitative and qualitative evaluations show that both methods outperform existing approaches by better preserving scene consistency while accurately reflecting occlusions and user-specified poses.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Aerial GNSS Localization for Marine Robots</title>
<link>https://arxiv.org/abs/2505.04095</link>
<guid>https://arxiv.org/abs/2505.04095</guid>
<content:encoded><![CDATA[
arXiv:2505.04095v1 Announce Type: cross 
Abstract: Accurate localization is crucial for water robotics, yet traditional onboard Global Navigation Satellite System (GNSS) approaches are difficult or ineffective due to signal reflection on the water's surface and its high cost of aquatic GNSS receivers. Existing approaches, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face challenges like error accumulation and high computational complexity. Therefore, a more efficient and scalable solution remains necessary. This paper proposes an alternative approach that leverages an aerial drone equipped with GNSS localization to track and localize a marine robot once it is near the surface of the water. Our results show that this novel adaptation enables accurate single and multi-robot marine robot localization.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation</title>
<link>https://arxiv.org/abs/2505.04097</link>
<guid>https://arxiv.org/abs/2505.04097</guid>
<content:encoded><![CDATA[
arXiv:2505.04097v1 Announce Type: cross 
Abstract: A three-dimensional convolutional neural network was developed to classify T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid output. Using stochastic noise injection and five-fold cross-validation, the model achieved test set accuracy of 0.912 and area under the ROC curve of 0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity and specificity both exceeded 0.90. These results align with prior work reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate the effectiveness of simple augmentation for 3D MRI classification and motivate future exploration of advanced augmentation methods and architectures such as 3D U-Net and vision transformers.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RGB-Event Fusion with Self-Attention for Collision Prediction</title>
<link>https://arxiv.org/abs/2505.04258</link>
<guid>https://arxiv.org/abs/2505.04258</guid>
<content:encoded><![CDATA[
arXiv:2505.04258v1 Announce Type: cross 
Abstract: Ensuring robust and real-time obstacle avoidance is critical for the safe operation of autonomous robots in dynamic, real-world environments. This paper proposes a neural network framework for predicting the time and collision position of an unmanned aerial vehicle with a dynamic object, using RGB and event-based vision sensors. The proposed architecture consists of two separate encoder branches, one for each modality, followed by fusion by self-attention to improve prediction accuracy. To facilitate benchmarking, we leverage the ABCD [8] dataset collected that enables detailed comparisons of single-modality and fusion-based approaches. At the same prediction throughput of 50Hz, the experimental results show that the fusion-based model offers an improvement in prediction accuracy over single-modality approaches of 1% on average and 10% for distances beyond 0.5m, but comes at the cost of +71% in memory and + 105% in FLOPs. Notably, the event-based model outperforms the RGB model by 4% for position and 26% for time error at a similar computational cost, making it a competitive alternative. Additionally, we evaluate quantized versions of the event-based models, applying 1- to 8-bit quantization to assess the trade-offs between predictive performance and computational efficiency. These findings highlight the trade-offs of multi-modal perception using RGB and event-based cameras in robotic applications.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control</title>
<link>https://arxiv.org/abs/2505.04387</link>
<guid>https://arxiv.org/abs/2505.04387</guid>
<content:encoded><![CDATA[
arXiv:2505.04387v1 Announce Type: cross 
Abstract: Creating realistic 3D head assets for virtual characters that match a precise artistic vision remains labor-intensive. We present a novel framework that streamlines this process by providing artists with intuitive control over generated 3D heads. Our approach uses a geometry-aware texture synthesis pipeline that learns correlations between head geometry and skin texture maps across different demographics. The framework offers three levels of artistic control: manipulation of overall head geometry, adjustment of skin tone while preserving facial characteristics, and fine-grained editing of details such as wrinkles or facial hair. Our pipeline allows artists to make edits to a single texture map using familiar tools, with our system automatically propagating these changes coherently across the remaining texture maps needed for realistic rendering. Experiments demonstrate that our method produces diverse results with clean geometries. We showcase practical applications focusing on intuitive control for artists, including skin tone adjustments and simplified editing workflows for adding age-related details or removing unwanted features from scanned models. This integrated approach aims to streamline the artistic workflow in virtual character creation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization</title>
<link>https://arxiv.org/abs/2505.04590</link>
<guid>https://arxiv.org/abs/2505.04590</guid>
<content:encoded><![CDATA[
arXiv:2505.04590v1 Announce Type: cross 
Abstract: We introduce TetWeave, a novel isosurface representation for gradient-based mesh optimization that jointly optimizes the placement of a tetrahedral grid used for Marching Tetrahedra and a novel directional signed distance at each point. TetWeave constructs tetrahedral grids on-the-fly via Delaunay triangulation, enabling increased flexibility compared to predefined grids. The extracted meshes are guaranteed to be watertight, two-manifold and intersection-free. The flexibility of TetWeave enables a resampling strategy that places new points where reconstruction error is high and allows to encourage mesh fairness without compromising on reconstruction error. This leads to high-quality, adaptive meshes that require minimal memory usage and few parameters to optimize. Consequently, TetWeave exhibits near-linear memory scaling relative to the vertex count of the output mesh - a substantial improvement over predefined grids. We demonstrate the applicability of TetWeave to a broad range of challenging tasks in computer graphics and vision, such as multi-view 3D reconstruction, mesh compression and geometric texture generation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems</title>
<link>https://arxiv.org/abs/2505.04596</link>
<guid>https://arxiv.org/abs/2505.04596</guid>
<content:encoded><![CDATA[
arXiv:2505.04596v1 Announce Type: cross 
Abstract: This paper presents a novel approach for optimizing the scheduling and control of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments. The proposed method integrates Kalman filters for motion prediction with a dynamic network flow model to enhance real-time video capture efficiency. By assigning Kalman filters to tracked objects, the system predicts future locations, enabling precise scheduling of camera tasks. This prediction-driven approach is formulated as a network flow optimization, ensuring scalability and adaptability to various surveillance scenarios. To further reduce redundant monitoring, we also incorporate group-tracking nodes, allowing multiple objects to be captured within a single camera focus when appropriate. In addition, a value-based system is introduced to prioritize camera actions, focusing on the timely capture of critical events. By adjusting the decay rates of these values over time, the system ensures prompt responses to tasks with imminent deadlines. Extensive simulations demonstrate that this approach improves coverage, reduces average wait times, and minimizes missed events compared to traditional master-slave camera systems. Overall, our method significantly enhances the efficiency, scalability, and effectiveness of surveillance systems, particularly in dynamic and crowded environments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.04619</link>
<guid>https://arxiv.org/abs/2505.04619</guid>
<content:encoded><![CDATA[
arXiv:2505.04619v1 Announce Type: cross 
Abstract: Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer</title>
<link>https://arxiv.org/abs/2505.04622</link>
<guid>https://arxiv.org/abs/2505.04622</guid>
<content:encoded><![CDATA[
arXiv:2505.04622v1 Announce Type: cross 
Abstract: Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.04623</link>
<guid>https://arxiv.org/abs/2505.04623</guid>
<content:encoded><![CDATA[
arXiv:2505.04623v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence</title>
<link>https://arxiv.org/abs/2202.03482</link>
<guid>https://arxiv.org/abs/2202.03482</guid>
<content:encoded><![CDATA[
arXiv:2202.03482v3 Announce Type: replace 
Abstract: With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. To address this, we introduce pattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions. We evaluate various CAV methods in terms of their alignment with the true concept direction and their impact on CAV applications, including concept sensitivity testing and model correction for shortcut behavior caused by data artifacts. We demonstrate the benefits of pattern-based CAVs using the Pediatric Bone Age, ISIC2019, and FunnyBirds datasets with VGG, ResNet, ReXNet, EfficientNet, and Vision Transformer as model architectures.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models</title>
<link>https://arxiv.org/abs/2306.07971</link>
<guid>https://arxiv.org/abs/2306.07971</guid>
<content:encoded><![CDATA[
arXiv:2306.07971v2 Announce Type: replace 
Abstract: The latest breakthroughs in large vision-language models, such as Bard and GPT-4, have showcased extraordinary abilities in performing a wide range of tasks. Such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images. On the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis. In this paper, we introduce XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation. This alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge. To enhance the performance of LLMs in the medical context, we generate ~217k interactive and high-quality summaries from free-text radiology reports. These summaries serve to enhance the performance of LLMs through the fine-tuning process. Our approach opens up new avenues the research for advancing the automated analysis of chest radiographs. Our open-source demos, models, and instruction sets are available at: https://github.com/mbzuai-oryx/XrayGPT.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illumination and Shadows in Head Rotation: experiments with Denoising Diffusion Models</title>
<link>https://arxiv.org/abs/2308.06057</link>
<guid>https://arxiv.org/abs/2308.06057</guid>
<content:encoded><![CDATA[
arXiv:2308.06057v2 Announce Type: replace 
Abstract: Accurately modeling the effects of illumination and shadows during head rotation is critical in computer vision for enhancing image realism and reducing artifacts. This study delves into the latent space of denoising diffusion models to identify compelling trajectories that can express continuous head rotation under varying lighting conditions. A key contribution of our work is the generation of additional labels from the CelebA dataset,categorizing images into three groups based on prevalent illumination direction: left, center, and right. These labels play a crucial role in our approach, enabling more precise manipulations and improved handling of lighting variations. Leveraging a recent embedding technique for Denoising Diffusion Implicit Models (DDIM), our method achieves noteworthy manipulations, encompassing a wide rotation angle of $\pm 30$ degrees, while preserving individual distinct characteristics even under challenging illumination conditions. Our methodology involves computing trajectories that approximate clouds of latent representations of dataset samples with different yaw rotations through linear regression. Specific trajectories are obtained by analyzing subsets of data that share significant attributes with the source image, including light direction. Notably, our approach does not require any specific training of the generative model for the task of rotation; we merely compute and follow specific trajectories in the latent space of a pre-trained face generation model. This article showcases the potential of our approach and its current limitations through a qualitative discussion of notable examples. This study contributes to the ongoing advancements in representation learning and the semantic investigation of the latent space of generative models.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance</title>
<link>https://arxiv.org/abs/2311.18681</link>
<guid>https://arxiv.org/abs/2311.18681</guid>
<content:encoded><![CDATA[
arXiv:2311.18681v3 Announce Type: replace 
Abstract: Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: https://github.com/ChantalMP/RaDialog.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling</title>
<link>https://arxiv.org/abs/2406.04321</link>
<guid>https://arxiv.org/abs/2406.04321</guid>
<content:encoded><![CDATA[
arXiv:2406.04321v3 Announce Type: replace 
Abstract: In this work, we systematically study music generation conditioned solely on the video. First, we present a large-scale dataset comprising 360K video-music pairs, including various genres such as movie trailers, advertisements, and documentaries. Furthermore, we propose VidMuse, a simple framework for generating music aligned with video inputs. VidMuse stands out by producing high-fidelity music that is both acoustically and semantically aligned with the video. By incorporating local and global visual cues, VidMuse enables the creation of musically coherent audio tracks that consistently match the video content through Long-Short-Term modeling. Through extensive experiments, VidMuse outperforms existing models in terms of audio quality, diversity, and audio-visual alignment. The code and datasets are available at https://vidmuse.github.io/.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis</title>
<link>https://arxiv.org/abs/2406.18360</link>
<guid>https://arxiv.org/abs/2406.18360</guid>
<content:encoded><![CDATA[
arXiv:2406.18360v3 Announce Type: replace 
Abstract: Comprehensive testing of autonomous systems through simulation is essential to ensure the safety of autonomous driving vehicles. This requires the generation of safety-critical scenarios that extend beyond the limitations of real-world data collection, as many of these scenarios are rare or rarely encountered on public roads. However, evaluating most existing novel view synthesis (NVS) methods relies on sporadic sampling of image frames from the training data, comparing the rendered images with ground-truth images. Unfortunately, this evaluation protocol falls short of meeting the actual requirements in closed-loop simulations. Specifically, the true application demands the capability to render novel views that extend beyond the original trajectory (such as cross-lane views), which are challenging to capture in the real world. To address this, this paper presents a synthetic dataset for novel driving view synthesis evaluation, which is specifically designed for autonomous driving simulations. This unique dataset includes testing images captured by deviating from the training trajectory by $1-4$ meters. It comprises six sequences that cover various times and weather conditions. Each sequence contains $450$ training images, $120$ testing images, and their corresponding camera poses and intrinsic parameters. Leveraging this novel dataset, we establish the first realistic benchmark for evaluating existing NVS approaches under front-only and multicamera settings. The experimental findings underscore the significant gap in current approaches, revealing their inadequate ability to fulfill the demanding prerequisites of cross-lane or closed-loop simulation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image-GS: Content-Adaptive Image Representation via 2D Gaussians</title>
<link>https://arxiv.org/abs/2407.01866</link>
<guid>https://arxiv.org/abs/2407.01866</guid>
<content:encoded><![CDATA[
arXiv:2407.01866v2 Announce Type: replace 
Abstract: Neural image representations have emerged as a promising approach for encoding and rendering visual data. Combined with learning-based workflows, they demonstrate impressive trade-offs between visual fidelity and memory footprint. Existing methods in this domain, however, often rely on fixed data structures that suboptimally allocate memory or compute-intensive implicit models, hindering their practicality for real-time graphics applications.
  Inspired by recent advancements in radiance field rendering, we introduce Image-GS, a content-adaptive image representation based on 2D Gaussians. Leveraging a custom differentiable renderer, Image-GS reconstructs images by adaptively allocating and progressively optimizing a group of anisotropic, colored 2D Gaussians. It achieves a favorable balance between visual fidelity and memory efficiency across a variety of stylized images frequently seen in graphics workflows, especially for those showing non-uniformly distributed features and in low-bitrate regimes. Moreover, it supports hardware-friendly rapid random access for real-time usage, requiring only 0.3K MACs to decode a pixel. Through error-guided progressive optimization, Image-GS naturally constructs a smooth level-of-detail hierarchy. We demonstrate its versatility with several applications, including texture compression, semantics-aware compression, and joint image compression and restoration.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Sketch-Guided Diffusion with Latent Optimization</title>
<link>https://arxiv.org/abs/2409.00313</link>
<guid>https://arxiv.org/abs/2409.00313</guid>
<content:encoded><![CDATA[
arXiv:2409.00313v2 Announce Type: replace 
Abstract: Based on recent advanced diffusion models, Text-to-image (T2I) generation models have demonstrated their capabilities to generate diverse and high-quality images. However, leveraging their potential for real-world content creation, particularly in providing users with precise control over the image generation result, poses a significant challenge. In this paper, we propose an innovative training-free pipeline that extends existing text-to-image generation models to incorporate a sketch as an additional condition. To generate new images with a layout and structure closely resembling the input sketch, we find that these core features of a sketch can be tracked with the cross-attention maps of diffusion models. We introduce latent optimization, a method that refines the noisy latent at each intermediate step of the generation process using cross-attention maps to ensure that the generated images adhere closely to the desired structure outlined in the reference sketch. Through latent optimization, our method enhances the accuracy of image generation, offering users greater control and customization options in content creation.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Test Time Adaptation with Few-shot Guidance</title>
<link>https://arxiv.org/abs/2409.01341</link>
<guid>https://arxiv.org/abs/2409.01341</guid>
<content:encoded><![CDATA[
arXiv:2409.01341v2 Announce Type: replace 
Abstract: Deep neural networks often encounter significant performance drops while facing with domain shifts between training (source) and test (target) data. To address this issue, Test Time Adaptation (TTA) methods have been proposed to adapt pre-trained source model to handle out-of-distribution streaming target data. Although these methods offer some relief, they lack a reliable mechanism for domain shift correction, which can often be erratic in real-world applications. In response, we develop Few-Shot Test Time Adaptation (FS-TTA), a novel and practical setting that utilizes a few-shot support set on top of TTA. Adhering to the principle of few inputs, big gains, FS-TTA reduces blind exploration in unseen target domains. Furthermore, we propose a two-stage framework to tackle FS-TTA, including (i) fine-tuning the pre-trained source model with few-shot support set, along with using feature diversity augmentation module to avoid overfitting, (ii) implementing test time adaptation based on prototype memory bank guidance to produce high quality pseudo-label for model adaptation. Through extensive experiments on three cross-domain classification benchmarks, we demonstrate the superior performance and reliability of our FS-TTA and framework.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Question-Answering Dense Video Events</title>
<link>https://arxiv.org/abs/2409.04388</link>
<guid>https://arxiv.org/abs/2409.04388</guid>
<content:encoded><![CDATA[
arXiv:2409.04388v4 Announce Type: replace 
Abstract: This paper presents question-answering on dense video events, a novel task that answers and grounds dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events over extended periods of time. To facilitate the study, we construct DeVE-QA -- a dataset featuring 78K questions about 26K events on 10.6K long videos. Our benchmarking shows that state-of-the-art MLLMs struggle on DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.8% and 2.1% for G(round)QA accuracy on DeVE-QA~and NExT-GQA, respectively. Our data and code will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replace Anyone in Videos</title>
<link>https://arxiv.org/abs/2409.19911</link>
<guid>https://arxiv.org/abs/2409.19911</guid>
<content:encoded><![CDATA[
arXiv:2409.19911v2 Announce Type: replace 
Abstract: The field of controllable human-centric video generation has witnessed remarkable progress, particularly with the advent of diffusion models. However, achieving precise and localized control over human motion in videos, such as replacing or inserting individuals while preserving desired motion patterns, still remains a formidable challenge. In this work, we present the ReplaceAnyone framework, which focuses on localized human replacement and insertion featuring intricate backgrounds. Specifically, we formulate this task as an image-conditioned video inpainting paradigm with pose guidance, utilizing a unified end-to-end video diffusion architecture that facilitates image-conditioned video inpainting within masked regions. To prevent shape leakage and enable granular local control, we introduce diverse mask forms involving both regular and irregular shapes. Furthermore, we implement an enriched visual guidance mechanism to enhance appearance alignment, a hybrid inpainting encoder to further preserve the detailed background information in the masked video, and a two-phase optimization methodology to simplify the training difficulty. ReplaceAnyone enables seamless replacement or insertion of characters while maintaining the desired pose motion and reference appearance within a single framework. Extensive experimental results demonstrate the effectiveness of our method in generating realistic and coherent video content. The proposed ReplaceAnyone can be seamlessly applied not only to traditional 3D-UNet base models but also to DiT-based video models such as Wan2.1. The code will be available at https://github.com/ali-vilab/UniAnimate-DiT.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2410.04634</link>
<guid>https://arxiv.org/abs/2410.04634</guid>
<content:encoded><![CDATA[
arXiv:2410.04634v3 Announce Type: replace 
Abstract: Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose Concept2Concept, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user-defined distributions or empirical, real-world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool to facilitate use by non-technical end-users. A demo is available at https://tinyurl.com/Concept2ConceptDemo.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Create Cross-Modal Task Representations</title>
<link>https://arxiv.org/abs/2410.22330</link>
<guid>https://arxiv.org/abs/2410.22330</guid>
<content:encoded><![CDATA[
arXiv:2410.22330v2 Announce Type: replace 
Abstract: Autoregressive vision-language models (VLMs) can handle many tasks within a single model, yet the representations that enable this capability remain opaque. We find that VLMs align conceptually equivalent inputs into a shared task vector, which is invariant to modality (text, image) and format (examples, instruction), and may simplify VLM processing. We measure this alignment via cross-modal transfer -- the ability of a task vector derived in one modality to trigger the correct generation in another -- on a range of tasks and model architectures. Although the task vector is highly compressed, we find that this single vector outperforms prompting the model with the full task information, unique to this cross-modal case. Furthermore, we show that task vectors can be transferred from a base language model to its fine-tuned vision-language counterpart, and that they can be derived solely from instructions without the need for examples. Taken together, our findings shed light on how VLMs internally process task information, and how they map different modalities into common semantic representations. Project page: https://vlm-cross-modal-reps.github.io.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation</title>
<link>https://arxiv.org/abs/2411.04997</link>
<guid>https://arxiv.org/abs/2411.04997</guid>
<content:encoded><![CDATA[
arXiv:2411.04997v4 Announce Type: replace 
Abstract: CLIP is a foundational multimodal model that aligns image and text features into a shared representation space via contrastive learning on large-scale image-text pairs. Its effectiveness primarily stems from the use of natural language as rich supervision. Motivated by the remarkable advancements in large language models (LLMs), this work explores how LLMs' superior text understanding and extensive open-world knowledge can enhance CLIP's capability, especially for processing longer and more complex image captions. We propose an efficient post-training strategy that integrates LLMs into pretrained CLIP. To address the challenge posed by the autoregressive nature of LLMs, we introduce a caption-to-caption contrastive fine-tuning framework, significantly enhancing the discriminative quality of LLM outputs. Extensive experiments demonstrate that our approach outperforms LoRA-based methods, achieving nearly fourfold faster training with superior performance. Furthermore, we validate substantial improvements over state-of-the-art models such as CLIP, EVA02, and SigLip2 across various zero-shot multimodal retrieval tasks, cross-lingual retrieval tasks, and multimodal language model pretraining.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation</title>
<link>https://arxiv.org/abs/2411.05261</link>
<guid>https://arxiv.org/abs/2411.05261</guid>
<content:encoded><![CDATA[
arXiv:2411.05261v2 Announce Type: replace 
Abstract: Despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. This paper introduces a novel approach to identify specific image features in X-ray images that influence the outputs of report generation models. Specifically, we propose Cyclic Vision-Language Manipulator CVLM, a module to generate a manipulated X-ray from an original X-ray and its report from a designated report generator. The essence of CVLM is that cycling manipulated X-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for X-ray generation, achieving the term "cyclic manipulation". This process allows direct comparison between original and manipulated X-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. Empirical evaluations demonstrate that CVLM can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of AI-generated reports.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynamicControl: Adaptive Condition Selection for Improved Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2412.03255</link>
<guid>https://arxiv.org/abs/2412.03255</guid>
<content:encoded><![CDATA[
arXiv:2412.03255v2 Announce Type: replace 
Abstract: To enhance the controllability of text-to-image diffusion models, current ControlNet-like models have explored various control signals to dictate image attributes. However, existing methods either handle conditions inefficiently or use a fixed number of conditions, which does not fully address the complexity of multiple conditions and their potential conflicts. This underscores the need for innovative approaches to manage multiple conditions effectively for more reliable and detailed image synthesis. To address this issue, we propose a novel framework, DynamicControl, which supports dynamic combinations of diverse control signals, allowing adaptive selection of different numbers and types of conditions. Our approach begins with a double-cycle controller that generates an initial real score sorting for all input conditions by leveraging pre-trained conditional generation models and discriminative models. This controller evaluates the similarity between extracted conditions and input conditions, as well as the pixel-level similarity with the source image. Then, we integrate a Multimodal Large Language Model (MLLM) to build an efficient condition evaluator. This evaluator optimizes the ordering of conditions based on the double-cycle controller's score ranking. Our method jointly optimizes MLLMs and diffusion models, utilizing MLLMs' reasoning capabilities to facilitate multi-condition text-to-image (T2I) tasks. The final sorted conditions are fed into a parallel multi-control adapter, which learns feature maps from dynamic visual conditions and integrates them to modulate ControlNet, thereby enhancing control over generated images. Through both quantitative and qualitative comparisons, DynamicControl demonstrates its superiority over existing methods in terms of controllability, generation quality and composability under various conditional controls.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning for Sea Surface Temperature Reconstruction under Cloud Occlusion</title>
<link>https://arxiv.org/abs/2412.03413</link>
<guid>https://arxiv.org/abs/2412.03413</guid>
<content:encoded><![CDATA[
arXiv:2412.03413v2 Announce Type: replace 
Abstract: Sea Surface Temperature (SST) reconstructions from satellite images affected by cloud gaps have been extensively documented in the past three decades. Here we describe several Machine Learning models to fill the cloud-occluded areas starting from MODIS Aqua nighttime L3 images. To tackle this challenge, we employed a type of Convolutional Neural Network model (U-net) to reconstruct cloud-covered portions of satellite imagery while preserving the integrity of observed values in cloud-free areas. We demonstrate the outstanding precision of U-net with respect to available products done using OI interpolation algorithms. Our best-performing architecture show 50% lower root mean square errors over established gap-filling methods.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either Stereo or Mono Fail</title>
<link>https://arxiv.org/abs/2412.04472</link>
<guid>https://arxiv.org/abs/2412.04472</guid>
<content:encoded><![CDATA[
arXiv:2412.04472v2 Announce Type: replace 
Abstract: We introduce Stereo Anywhere, a novel stereo-matching framework that combines geometric constraints with robust priors from monocular depth Vision Foundation Models (VFMs). By elegantly coupling these complementary worlds through a dual-branch architecture, we seamlessly integrate stereo matching with learned contextual cues. Following this design, our framework introduces novel cost volume fusion mechanisms that effectively handle critical challenges such as textureless regions, occlusions, and non-Lambertian surfaces. Through our novel optical illusion dataset, MonoTrap, and extensive evaluation across multiple benchmarks, we demonstrate that our synthetic-only trained model achieves state-of-the-art results in zero-shot generalization, significantly outperforming existing solutions while showing remarkable robustness to challenging cases such as mirrors and transparencies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiency Meets Fidelity: A Novel Quantization Framework for Stable Diffusion</title>
<link>https://arxiv.org/abs/2412.06661</link>
<guid>https://arxiv.org/abs/2412.06661</guid>
<content:encoded><![CDATA[
arXiv:2412.06661v2 Announce Type: replace 
Abstract: Text-to-image generation via Stable Diffusion models (SDM) have demonstrated remarkable capabilities. However, their computational intensity, particularly in the iterative denoising process, hinders real-time deployment in latency-sensitive applications. While Recent studies have explored post-training quantization (PTQ) and quantization-aware training (QAT) methods to compress Diffusion models, existing methods often overlook the consistency between results generated by quantized models and those from floating-point models. This consistency is paramount for professional applications where both efficiency and output reliability are essential. To ensure that quantized SDM generates high-quality and consistent images, we propose an efficient quantization framework for SDM. Our framework introduces a Serial-to-Parallel pipeline that simultaneously maintains training-inference consistency and ensures optimization stability. Building upon this foundation, we further develop several techniques including multi-timestep activation quantization, time information precalculation, inter-layer distillation, and selective freezing, to achieve high-fidelity generation in comparison to floating-point models while maintaining quantization efficiency.
  Through comprehensive evaluation across multiple Stable Diffusion variants (v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over state-of-the-art approaches with shorter training times. Under W4A8 quantization settings, we achieve significant improvements in both distribution similarity and visual fidelity, while preserving a high image quality.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation</title>
<link>https://arxiv.org/abs/2412.11026</link>
<guid>https://arxiv.org/abs/2412.11026</guid>
<content:encoded><![CDATA[
arXiv:2412.11026v2 Announce Type: replace 
Abstract: Dynamic scenes contain intricate spatio-temporal information, crucial for mobile robots, UAVs, and autonomous driving systems to make informed decisions. Parsing these scenes into semantic triplets  for accurate Scene Graph Generation (SGG) is highly challenging due to the fluctuating spatio-temporal complexity. Inspired by the reasoning capabilities of Large Language Models (LLMs), we propose SceneLLM, a novel framework that leverages LLMs as powerful scene analyzers for dynamic SGG. Our framework introduces a Video-to-Language (V2L) mapping module that transforms video frames into linguistic signals (scene tokens), making the input more comprehensible for LLMs. To better encode spatial information, we devise a Spatial Information Aggregation (SIA) scheme, inspired by the structure of Chinese characters, which encodes spatial data into tokens. Using Optimal Transport (OT), we generate an implicit language signal from the frame-level token sequence that captures the video's spatio-temporal information. To further improve the LLM's ability to process this implicit linguistic input, we apply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a transformer-based SGG predictor to decode the LLM's reasoning and predict semantic triplets. Our method achieves state-of-the-art results on the Action Genome (AG) benchmark, and extensive experiments show the effectiveness of SceneLLM in understanding and generating accurate dynamic scene graphs.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling</title>
<link>https://arxiv.org/abs/2501.04666</link>
<guid>https://arxiv.org/abs/2501.04666</guid>
<content:encoded><![CDATA[
arXiv:2501.04666v3 Announce Type: replace 
Abstract: Given an isolated garment image in a canonical product view and a separate image of a person, the virtual try-on task aims to generate a new image of the person wearing the target garment. Prior virtual try-on works face two major challenges in achieving this goal: a) the paired (human, garment) training data has limited availability; b) generating textures on the human that perfectly match that of the prompted garment is difficult, often resulting in distorted text and faded textures. Our work explores ways to tackle these issues through both synthetic data as well as model refinement. We introduce a garment extraction model that generates (human, synthetic garment) pairs from a single image of a clothed individual. The synthetic pairs can then be used to augment the training of virtual try-on. We also propose an Error-Aware Refinement-based Schr\"odinger Bridge (EARSB) that surgically targets localized generation errors for correcting the output of a base virtual try-on model. To identify likely errors, we propose a weakly-supervised error classifier that localizes regions for refinement, subsequently augmenting the Schr\"odinger Bridge's noise schedule with its confidence heatmap. Experiments on VITON-HD and DressCode-Upper demonstrate that our synthetic data augmentation enhances the performance of prior work, while EARSB improves the overall image quality. In user studies, our model is preferred by the users in an average of 59% of cases.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to Sustainability Data Extraction</title>
<link>https://arxiv.org/abs/2501.18504</link>
<guid>https://arxiv.org/abs/2501.18504</guid>
<content:encoded><![CDATA[
arXiv:2501.18504v3 Announce Type: replace 
Abstract: Large Language Model (LLM) image recognition is a powerful tool for extracting data from images, but accuracy depends on providing sufficient cues in the prompt - requiring a domain expert for specialized tasks. We introduce Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a combination of LLMs and evolutionary computation to generate and optimize cues such that recognition of specialized features in images is improved. It achieves this by auto-generating a novel domain-specific representation and then using it to optimize suitable textual cues with a genetic algorithm. We apply CLEAR to the real-world task of identifying sustainability data from interior and exterior images of buildings. We investigate the effects of using a variable-length representation compared to fixed-length and show how LLM consistency can be improved by refactoring from categorical to real-valued estimates. We show that CLEAR enables higher accuracy compared to expert human recognition and human-authored prompts in every task with error rates improved by up to two orders of magnitude and an ablation study evincing solution concision.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics</title>
<link>https://arxiv.org/abs/2502.00205</link>
<guid>https://arxiv.org/abs/2502.00205</guid>
<content:encoded><![CDATA[
arXiv:2502.00205v2 Announce Type: replace 
Abstract: Sustainable agriculture plays a crucial role in ensuring world food security for consumers. A critical challenge faced by sustainable precision agriculture is weed growth, as weeds compete for essential resources with crops, such as water, soil nutrients, and sunlight, which notably affect crop yields. The adoption of automated computer vision technologies and ground agricultural consumer electronic vehicles in precision agriculture offers sustainable, low-carbon solutions. However, prior works suffer from issues such as low accuracy and precision, as well as high computational expense. This work proposes EcoWeedNet, a novel model that enhances weed detection performance without introducing significant computational complexity, aligning with the goals of low-carbon agricultural practices. The effectiveness of the proposed model is demonstrated through comprehensive experiments on the CottonWeedDet12 benchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves performance comparable to that of large models (mAP@0.5 = 95.2%), yet with significantly fewer parameters (approximately 4.21% of the parameters of YOLOv4), lower computational complexity and better computational efficiency 6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's deployability on low-power consumer hardware, lower energy consumption, and hence reduced carbon footprint, thereby emphasizing the application prospects of EcoWeedNet in next-generation sustainable agriculture. These findings provide the way forward for increased application of environmentally-friendly agricultural consumer technologies.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage Object Detection</title>
<link>https://arxiv.org/abs/2502.11178</link>
<guid>https://arxiv.org/abs/2502.11178</guid>
<content:encoded><![CDATA[
arXiv:2502.11178v2 Announce Type: replace 
Abstract: Recent 2D CNN-based domain adaptation approaches struggle with long-range dependencies due to limited receptive fields, making it difficult to adapt to target domains with significant spatial distribution changes. While transformer-based domain adaptation methods better capture distant relationships through self-attention mechanisms that facilitate more effective cross-domain feature alignment, their quadratic computational complexity makes practical deployment challenging for object detection tasks across diverse domains. Inspired by the global modeling and linear computation complexity of the Mamba architecture, we present the first domain-adaptive Mamba-based one-stage object detection model, termed DA-Mamba. Specifically, we combine Mamba's efficient state-space modeling with attention mechanisms to address domain-specific spatial and channel-wise variations. Our design leverages domain-adaptive spatial and channel-wise scanning within the Mamba block to extract highly transferable representations for efficient sequential processing, while cross-attention modules generate long-range, mixed-domain spatial features to enable robust soft alignment across domains. Besides, motivated by the observation that hybrid architectures introduce feature noise in domain adaptation tasks, we propose an entropy-based knowledge distillation framework with margin ReLU, which adaptively refines multi-level representations by suppressing irrelevant activations and aligning uncertainty across source and target domains. Finally, to prevent overfitting caused by the mixed-up features generated through cross-attention mechanisms, we propose entropy-driven gating attention with random perturbations that simultaneously refine target features and enhance model generalization.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.22676</link>
<guid>https://arxiv.org/abs/2503.22676</guid>
<content:encoded><![CDATA[
arXiv:2503.22676v2 Announce Type: replace 
Abstract: We present TranSplat, a 3D scene rendering algorithm that enables realistic cross-scene object transfer (from a source to a target scene) based on the Gaussian Splatting framework. Our approach addresses two critical challenges: (1) precise 3D object extraction from the source scene, and (2) faithful relighting of the transferred object in the target scene without explicit material property estimation. TranSplat fits a splatting model to the source scene, using 2D object masks to drive fine-grained 3D segmentation. Following user-guided insertion of the object into the target scene, along with automatic refinement of position and orientation, TranSplat derives per-Gaussian radiance transfer functions via spherical harmonic analysis to adapt the object's appearance to match the target scene's lighting environment. This relighting strategy does not require explicitly estimating physical scene properties such as BRDFs. Evaluated on several synthetic and real-world scenes and objects, TranSplat yields excellent 3D object extractions and relighting performance compared to recent baseline methods and visually convincing cross-scene object transfers. We conclude by discussing the limitations of the approach.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action Recognition and Transformer-based Sensor Fusion</title>
<link>https://arxiv.org/abs/2504.02287</link>
<guid>https://arxiv.org/abs/2504.02287</guid>
<content:encoded><![CDATA[
arXiv:2504.02287v3 Announce Type: replace 
Abstract: Multi-modal multi-view action recognition is a rapidly growing field in computer vision, offering significant potential for applications in surveillance. However, current datasets often fail to address real-world challenges such as wide-area distributed settings, asynchronous data streams, and the lack of frame-level annotations. Furthermore, existing methods face difficulties in effectively modeling inter-view relationships and enhancing spatial feature learning. In this paper, we introduce the MultiSensor-Home dataset, a novel benchmark designed for comprehensive action recognition in home environments, and also propose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF) method. The proposed MultiSensor-Home dataset features untrimmed videos captured by distributed sensors, providing high-resolution RGB and audio data along with detailed multi-view frame-level action labels. The proposed MultiTSF method leverages a Transformer-based fusion mechanism to dynamically model inter-view relationships. Furthermore, the proposed method integrates a human detection module to enhance spatial feature learning, guiding the model to prioritize frames with human activity to enhance action the recognition accuracy. Experiments on the proposed MultiSensor-Home and the existing MM-Office datasets demonstrate the superiority of MultiTSF over the state-of-the-art methods. Quantitative and qualitative results highlight the effectiveness of the proposed method in advancing real-world multi-modal multi-view action recognition. The source code is available at https://github.com/thanhhff/MultiTSF.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probability Density Geodesics in Image Diffusion Latent Space</title>
<link>https://arxiv.org/abs/2504.06675</link>
<guid>https://arxiv.org/abs/2504.06675</guid>
<content:encoded><![CDATA[
arXiv:2504.06675v2 Announce Type: replace 
Abstract: Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. In this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. We present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. Using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. Finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network for LiDAR Loop Closure Detection</title>
<link>https://arxiv.org/abs/2504.08280</link>
<guid>https://arxiv.org/abs/2504.08280</guid>
<content:encoded><![CDATA[
arXiv:2504.08280v2 Announce Type: replace 
Abstract: LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous Localization and Mapping (SLAM) but faces challenges in robustness and accuracy. Existing methods, including semantic graph approaches, often suffer from coarse geometric representations and lack temporal robustness against noise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic NDT-Enhanced Semantic Graph Attention Network, to overcome these limitations. PNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT) covariance matrices as rich, discriminative geometric node features, processed via a Graph Attention Network (GAT). Crucially, it integrates graph similarity scores into a probabilistic temporal filtering framework (modeled as an HMM/Bayes filter), incorporating uncertain odometry for motion modeling and utilizing forward-backward smoothing to effectively handle ambiguities. Evaluations on challenging KITTI sequences (00 and 08) demonstrate state-of-the-art performance, achieving Average Precision of 96.2\% and 95.1\%, respectively. PNE-SGAN significantly outperforms existing methods, particularly in difficult bidirectional loop scenarios where others falter. By synergizing detailed NDT geometry with principled probabilistic temporal reasoning, PNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing SLAM reliability in complex, large-scale environments.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opening Articulated Structures in the Real World</title>
<link>https://arxiv.org/abs/2402.17767</link>
<guid>https://arxiv.org/abs/2402.17767</guid>
<content:encoded><![CDATA[
arXiv:2402.17767v3 Announce Type: replace-cross 
Abstract: What does it take to build mobile manipulation systems that can competently operate on previously unseen objects in previously unseen environments? This work answers this question using opening of articulated structures as a mobile manipulation testbed. Specifically, our focus is on the end-to-end performance on this task without any privileged information, i.e. the robot starts at a location with the novel target articulated object in view, and has to approach the object and successfully open it. We first develop a system for this task, and then conduct 100+ end-to-end system tests across 13 real world test sites. Our large-scale study reveals a number of surprising findings: a) modular systems outperform end-to-end learned systems for this task, even when the end-to-end learned systems are trained on 1000+ demonstrations, b) perception, and not precise end-effector control, is the primary bottleneck to task success, and c) state-of-the-art articulation parameter estimation models developed in isolation struggle when faced with robot-centric viewpoints. Overall, our findings highlight the limitations of developing components of the pipeline in isolation and underscore the need for system-level research, providing a pragmatic roadmap for building generalizable mobile manipulation systems. Videos, code, and models are available on the project website: https://arjung128.github.io/opening-articulated-structures/
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-end Surface Optimization for Light Control</title>
<link>https://arxiv.org/abs/2408.13117</link>
<guid>https://arxiv.org/abs/2408.13117</guid>
<content:encoded><![CDATA[
arXiv:2408.13117v2 Announce Type: replace-cross 
Abstract: Designing a freeform surface to reflect or refract light to achieve a target distribution is a challenging inverse problem. In this paper, we propose an end-to-end optimization strategy for an optical surface mesh. Our formulation leverages a novel differentiable rendering model, and is directly driven by the difference between the resulting light distribution and the target distribution. We also enforce geometric constraints related to fabrication requirements, to facilitate CNC milling and polishing of the designed surface. To address the issue of local minima, we formulate a face-based optimal transport problem between the current mesh and the target distribution, which makes effective large changes to the surface shape. The combination of our optimal transport update and rendering-guided optimization produces an optical surface design with a resulting image closely resembling the target, while the geometric constraints in our optimization help to ensure consistency between the rendering model and the final physical results. The effectiveness of our algorithm is demonstrated on a variety of target images using both simulated rendering and physical prototypes.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian computation with generative diffusion models by Multilevel Monte Carlo</title>
<link>https://arxiv.org/abs/2409.15511</link>
<guid>https://arxiv.org/abs/2409.15511</guid>
<content:encoded><![CDATA[
arXiv:2409.15511v3 Announce Type: replace-cross 
Abstract: Generative diffusion models have recently emerged as a powerful strategy to perform stochastic sampling in Bayesian inverse problems, delivering remarkably accurate solutions for a wide range of challenging applications. However, diffusion models often require a large number of neural function evaluations per sample in order to deliver accurate posterior samples. As a result, using diffusion models as stochastic samplers for Monte Carlo integration in Bayesian computation can be highly computationally expensive, particularly in applications that require a substantial number of Monte Carlo samples for conducting uncertainty quantification analyses. This cost is especially high in large-scale inverse problems such as computational imaging, which rely on large neural networks that are expensive to evaluate. With quantitative imaging applications in mind, this paper presents a Multilevel Monte Carlo strategy that significantly reduces the cost of Bayesian computation with diffusion models. This is achieved by exploiting cost-accuracy trade-offs inherent to diffusion models to carefully couple models of different levels of accuracy in a manner that significantly reduces the overall cost of the calculation, without reducing the final accuracy. The proposed approach achieves a $4\times$-to-$8\times$ reduction in computational cost w.r.t. standard techniques across three benchmark imaging problems.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Aggregation Weights for Federated Segmentation of Pancreas MRI</title>
<link>https://arxiv.org/abs/2410.22530</link>
<guid>https://arxiv.org/abs/2410.22530</guid>
<content:encoded><![CDATA[
arXiv:2410.22530v3 Announce Type: replace-cross 
Abstract: Federated learning (FL) enables collaborative model training across institutions without sharing sensitive data, making it an attractive solution for medical imaging tasks. However, traditional FL methods, such as Federated Averaging (FedAvg), face difficulties in generalizing across domains due to variations in imaging protocols and patient demographics across institutions. This challenge is particularly evident in pancreas MRI segmentation, where anatomical variability and imaging artifacts significantly impact performance. In this paper, we conduct a comprehensive evaluation of FL algorithms for pancreas MRI segmentation and introduce a novel approach that incorporates adaptive aggregation weights. By dynamically adjusting the contribution of each client during model aggregation, our method accounts for domain-specific differences and improves generalization across heterogeneous datasets. Experimental results demonstrate that our approach enhances segmentation accuracy and reduces the impact of domain shift compared to conventional FL methods while maintaining privacy-preserving capabilities. Significant performance improvements are observed across multiple hospitals (centers).
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancements and limitations of LLMs in replicating human color-word associations</title>
<link>https://arxiv.org/abs/2411.02116</link>
<guid>https://arxiv.org/abs/2411.02116</guid>
<content:encoded><![CDATA[
arXiv:2411.02116v3 Announce Type: replace-cross 
Abstract: Color-word associations play a fundamental role in human cognition and design applications. Large Language Models (LLMs) have become widely available and have demonstrated intelligent behaviors in various benchmarks with natural conversation skills. However, their ability to replicate human color-word associations remains understudied. We compared multiple generations of LLMs (from GPT-3 to GPT-4o) against human color-word associations using data collected from over 10,000 Japanese participants, involving 17 colors and 80 words (10 word from eight categories) in Japanese. Our findings reveal a clear progression in LLM performance across generations, with GPT-4o achieving the highest accuracy in predicting the best voted word for each color and category. However, the highest median performance was approximately 50% even for GPT-4o with visual inputs (chance level of 10%). Moreover, we found performance variations across word categories and colors: while LLMs tended to excel in categories such as Rhythm and Landscape, they struggled with categories such as Emotions. Interestingly, color discrimination ability estimated from our color-word association data showed high correlation with human color discrimination patterns, consistent with previous studies. Thus, despite reasonable alignment in basic color discrimination, humans and LLMs still diverge systematically in the words they assign to those colors. Our study highlights both the advancements in LLM capabilities and their persistent limitations, raising the possibility of systematic differences in semantic memory structures between humans and LLMs in representing color-word associations.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech Recognition</title>
<link>https://arxiv.org/abs/2502.01547</link>
<guid>https://arxiv.org/abs/2502.01547</guid>
<content:encoded><![CDATA[
arXiv:2502.01547v3 Announce Type: replace-cross 
Abstract: Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio and can improve performance in noise, but most methods are trained only on English data. One limitation is the lack of large-scale multilingual video data, which makes it hard to train models from scratch. In this work, we propose mWhisper-Flamingo for multilingual AVSR which combines the strengths of a pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable better multi-modal integration and improve the noisy multilingual performance, we introduce decoder modality dropout where the model is trained both on paired audio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo achieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages. Audio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on all languages in noisy conditions.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonoForce: Learnable Image-conditioned Physics Engine</title>
<link>https://arxiv.org/abs/2502.10156</link>
<guid>https://arxiv.org/abs/2502.10156</guid>
<content:encoded><![CDATA[
arXiv:2502.10156v3 Announce Type: replace-cross 
Abstract: We propose a novel model for the prediction of robot trajectories on rough offroad terrain from the onboard camera images. This model enforces the laws of classical mechanics through a physics-aware neural symbolic layer while preserving the ability to learn from large-scale data as it is end-to-end differentiable. The proposed hybrid model integrates a black-box component that predicts robot-terrain interaction forces with a neural-symbolic layer. This layer includes a differentiable physics engine that computes the robot's trajectory by querying these forces at the points of contact with the terrain. As the proposed architecture comprises substantial geometrical and physics priors, the resulting model can also be seen as a learnable physics engine conditioned on real images that delivers $10^4$ trajectories per second. We argue and empirically demonstrate that this architecture reduces the sim-to-real gap and mitigates out-of-distribution sensitivity. The differentiability, in conjunction with the rapid simulation speed, makes the model well-suited for various applications including model predictive control, trajectory shooting, supervised and reinforcement learning or SLAM. The codes and data are publicly available.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training</title>
<link>https://arxiv.org/abs/2504.03783</link>
<guid>https://arxiv.org/abs/2504.03783</guid>
<content:encoded><![CDATA[
arXiv:2504.03783v3 Announce Type: replace-cross 
Abstract: Federated Active Learning (FAL) has emerged as a promising framework to leverage large quantities of unlabeled data across distributed clients while preserving data privacy. However, real-world deployments remain limited by high annotation costs and communication-intensive sampling processes, particularly in a cross-silo setting, when clients possess substantial local datasets. This paper addresses the crucial question: What is the best practice to reduce communication costs in human-in-the-loop learning with minimal annotator effort? Existing FAL methods typically rely on iterative annotation processes that separate active sampling from federated updates, leading to multiple rounds of expensive communication and annotation. In response, we introduce FAST, a two-pass FAL framework that harnesses foundation models for weak labeling in a preliminary pass, followed by a refinement pass focused exclusively on the most uncertain samples. By leveraging representation knowledge from foundation models and integrating refinement steps into a streamlined workflow, FAST substantially reduces the overhead incurred by iterative active sampling. Extensive experiments on diverse medical and natural image benchmarks demonstrate that FAST outperforms existing FAL methods by an average of 4.36% while reducing communication rounds eightfold under a limited 5% labeling budget.
]]></content:encoded>
<pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RESAnything: Attribute Prompting for Arbitrary Referring Segmentation</title>
<link>https://arxiv.org/abs/2505.02867</link>
<guid>https://arxiv.org/abs/2505.02867</guid>
<content:encoded><![CDATA[
<div> method, referring expression segmentation, open-vocabulary, zero-shot, Chain-of-Thoughts reasoning

Summary: 
The article introduces an open-vocabulary and zero-shot method called RESAnything for arbitrary referring expression segmentation. This method can handle more general input expressions compared to previous works, including object- and part-level labels, as well as implicit references to object/part attributes like function, design, and material. RESAnything utilizes Chain-of-Thoughts reasoning with attribute prompting to generate detailed attribute descriptions for potential segment proposals. It prompts a large language model to describe object/part attributes, enhancing deep reasoning capabilities. Unlike existing methods, RESAnything does not require part annotations for training or fine-tuning, making it the first zero-shot and LLM-based RES approach. It outperforms other zero-shot methods on traditional benchmarks and excels in scenarios involving implicit queries and complex part-level relations. The article also introduces a new benchmark dataset comprising about 3,000 carefully curated RES instances for evaluating part-level and arbitrary RES solutions. 

<br /><br />Summary: <div>
arXiv:2505.02867v1 Announce Type: new 
Abstract: We present an open-vocabulary and zero-shot method for arbitrary referring expression segmentation (RES), targeting input expressions that are more general than what prior works were designed to handle. Specifically, our inputs encompass both object- and part-level labels as well as implicit references pointing to properties or qualities of object/part function, design, style, material, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT) reasoning, where the key idea is attribute prompting. We generate detailed descriptions of object/part attributes including shape, color, and location for potential segment proposals through systematic prompting of a large language model (LLM), where the proposals are produced by a foundational image segmentation model. Our approach encourages deep reasoning about object or part attributes related to function, style, design, etc., enabling the system to handle implicit queries without any part annotations for training or fine-tuning. As the first zero-shot and LLM-based RES method, RESAnything achieves clearly superior performance among zero-shot methods on traditional RES benchmarks and significantly outperforms existing methods on challenging scenarios involving implicit queries and complex part-level relations. Finally, we contribute a new benchmark dataset to offer ~3K carefully curated RES instances to assess part-level, arbitrary RES solutions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images</title>
<link>https://arxiv.org/abs/2505.02949</link>
<guid>https://arxiv.org/abs/2505.02949</guid>
<content:encoded><![CDATA[
<div> compression, bias, neural, image, models
<br />
In this study, a framework for evaluating bias in neural image compression models is presented. Nine popular models and their variants were analyzed to investigate racial bias in neural compression algorithms. It was found that traditional distortion metrics are ineffective at capturing bias in these models. Racial bias was observed in all models, with facial phenotype degradation in reconstructed images serving as a key indicator. A trade-off between bias and realism in decoded images was identified across different models. Utilizing a racially balanced training set was shown to reduce bias but was not a complete solution. The study also revealed that bias can be attributed to both compression model bias and classification model bias. Overall, this work represents a crucial first step towards understanding and addressing bias in neural image compression models.
<br /><br />Summary: <div>
arXiv:2505.02949v1 Announce Type: new 
Abstract: Neural compression methods are gaining popularity due to their superior rate-distortion performance over traditional methods, even at extremely low bitrates below 0.1 bpp. As deep learning architectures, these models are prone to bias during the training process, potentially leading to unfair outcomes for individuals in different groups. In this paper, we present a general, structured, scalable framework for evaluating bias in neural image compression models. Using this framework, we investigate racial bias in neural compression algorithms by analyzing nine popular models and their variants. Through this investigation, we first demonstrate that traditional distortion metrics are ineffective in capturing bias in neural compression models. Next, we highlight that racial bias is present in all neural compression models and can be captured by examining facial phenotype degradation in image reconstructions. We then examine the relationship between bias and realism in the decoded images and demonstrate a trade-off across models. Finally, we show that utilizing a racially balanced training set can reduce bias but is not a sufficient bias mitigation strategy. We additionally show the bias can be attributed to compression model bias and classification model bias. We believe that this work is a first step towards evaluating and eliminating bias in neural image compression models.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Narrated Lecture Videos from Slides with Synchronized Highlights</title>
<link>https://arxiv.org/abs/2505.02966</link>
<guid>https://arxiv.org/abs/2505.02966</guid>
<content:encoded><![CDATA[
<div> automated system, video lectures, AI-generated narration, highlight alignment module, cost-effective<br />
Summary:<br />
An end-to-end system has been developed to automatically convert static slide decks into engaging video lectures by generating AI-narrated explanations and synchronized visual highlights. The system utilizes a novel highlight alignment module that accurately maps spoken phrases to locations on slides using diverse strategies and Text-to-Speech for timing synchronization. Technical evaluation shows high accuracy in location mapping, especially on complex content, with a low generation cost of under $1 per hour of video. This system offers potential savings of two orders of magnitude compared to manual production costs, making it a practical and cost-effective tool for creating visually-guided video lectures.<br /> 
Summary: <div>
arXiv:2505.02966v1 Announce Type: new 
Abstract: Turning static slides into engaging video lectures takes considerable time and effort, requiring presenters to record explanations and visually guide their audience through the material. We introduce an end-to-end system designed to automate this process entirely. Given a slide deck, this system synthesizes a video lecture featuring AI-generated narration synchronized precisely with dynamic visual highlights. These highlights automatically draw attention to the specific concept being discussed, much like an effective presenter would. The core technical contribution is a novel highlight alignment module. This module accurately maps spoken phrases to locations on a given slide using diverse strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at selectable granularities (line or word level) and utilizes timestamp-providing Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's effectiveness through a technical evaluation using a manually annotated slide dataset with 1000 samples, finding that LLM-based alignment achieves high location accuracy (F1 > 92%), significantly outperforming simpler methods, especially on complex, math-heavy content. Furthermore, the calculated generation cost averages under $1 per hour of video, offering potential savings of two orders of magnitude compared to conservative estimates of manual production costs. This combination of high accuracy and extremely low cost positions this approach as a practical and scalable tool for transforming static slides into effective, visually-guided video lectures.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.02971</link>
<guid>https://arxiv.org/abs/2505.02971</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial attacks, vision language segmentation models, medical image analysis, fine-tuning, robustness<br />
<br />
Summary: 
Adversarial attacks on vision language segmentation models (VLSMs) in the medical image analysis domain were explored in this study. VLSMs were fine-tuned with adapters for medical image segmentation, and two types of adversarial attacks, projected gradient descent (PGD) and fast gradient sign method (FGSM), were employed to assess their robustness. Results showed a significant decline in performance metrics such as DSC and IoU scores when subjected to these attacks. Additionally, the study attempted to explore universal perturbation for medical images but did not find a suitable solution. The research aims to address the high-risk scenario in medical imaging by understanding the vulnerabilities of VLSMs to adversarial attacks.<br /> 
Summary: <div>
arXiv:2505.02971v1 Announce Type: new 
Abstract: Adversarial attacks have been fairly explored for computer vision and vision-language models. However, the avenue of adversarial attack for the vision language segmentation models (VLSMs) is still under-explored, especially for medical image analysis.
  Thus, we have investigated the robustness of VLSMs against adversarial attacks for 2D medical images with different modalities with radiology, photography, and endoscopy. The main idea of this project was to assess the robustness of the fine-tuned VLSMs specially in the medical domain setting to address the high risk scenario.
  First, we have fine-tuned pre-trained VLSMs for medical image segmentation with adapters.
  Then, we have employed adversarial attacks -- projected gradient descent (PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to determine its robustness against adversaries.
  We have reported models' performance decline to analyze the adversaries' impact.
  The results exhibit significant drops in the DSC and IoU scores after the introduction of these adversaries. Furthermore, we also explored universal perturbation but were not able to find for the medical images.
  \footnote{https://github.com/anjilab/secure-private-ai}
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking</title>
<link>https://arxiv.org/abs/2505.02980</link>
<guid>https://arxiv.org/abs/2505.02980</guid>
<content:encoded><![CDATA[
<div> Spatial Transcriptomics, Visium, gene expression prediction, deep learning, SpaRED<br />
Summary:<br />
Spatial Transcriptomics integrates histology images with gene expression profiles, with Visium being a popular technique. However, Visium's high costs and gene capture inefficiencies pose challenges. To address this, SpaRED database is introduced, comprising 26 datasets for fair model evaluation. A transformer-based model, SpaCKLE, significantly reduces error in gene expression prediction compared to existing methods. The SpaRED benchmark evaluates eight models on raw and SpaCKLE-completed data, showing improved results with SpaCKLE. This benchmark is a comprehensive evaluation of gene expression prediction from histology images, advancing research in Spatial Transcriptomics. <br /> <div>
arXiv:2505.02980v1 Announce Type: new 
Abstract: Spatial Transcriptomics is a groundbreaking technology that integrates histology images with spatially resolved gene expression profiles. Among the various Spatial Transcriptomics techniques available, Visium has emerged as the most widely adopted. However, its accessibility is limited by high costs, the need for specialized expertise, and slow clinical integration. Additionally, gene capture inefficiencies lead to significant dropout, corrupting acquired data. To address these challenges, the deep learning community has explored the gene expression prediction task directly from histology images. Yet, inconsistencies in datasets, preprocessing, and training protocols hinder fair comparisons between models. To bridge this gap, we introduce SpaRED, a systematically curated database comprising 26 public datasets, providing a standardized resource for model evaluation. We further propose SpaCKLE, a state-of-the-art transformer-based gene expression completion model that reduces mean squared error by over 82.5% compared to existing approaches. Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE substantially improves the results across all the gene expression prediction models. Altogether, our contributions constitute the most comprehensive benchmark of gene expression prediction from histology images to date and a stepping stone for future research on Spatial Transcriptomics.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results</title>
<link>https://arxiv.org/abs/2505.03007</link>
<guid>https://arxiv.org/abs/2505.03007</guid>
<content:encoded><![CDATA[
<div> Challenge, UGC Video Enhancement, NTIRE 2025, User-generated content, Visual quality<br />
<br />Summary:
The paper introduces the NTIRE 2025 Challenge focused on enhancing user-generated content (UGC) videos. The challenge involved improving the visual quality of 150 videos with real-world degradations like noise and compression artifacts. Evaluation was based on subjective quality assessment with feedback from over 8000 assessors. More than 25 teams participated, with 7 successfully passing the final phase. The results offer insights into the current state of UGC video enhancement research, showcasing effective strategies and emerging trends. All data, including processed videos and assessment scores, is publicly available on GitHub, providing a valuable resource for further research and development in the field. <div>
arXiv:2505.03007v1 Announce Type: new 
Abstract: This paper presents an overview of the NTIRE 2025 Challenge on UGC Video Enhancement. The challenge constructed a set of 150 user-generated content videos without reference ground truth, which suffer from real-world degradations such as noise, blur, faded colors, compression artifacts, etc. The goal of the participants was to develop an algorithm capable of improving the visual quality of such videos. Given the widespread use of UGC on short-form video platforms, this task holds substantial practical importance. The evaluation was based on subjective quality assessment in crowdsourcing, obtaining votes from over 8000 assessors. The challenge attracted more than 25 teams submitting solutions, 7 of which passed the final phase with source code verification. The outcomes may provide insights into the state-of-the-art in UGC video enhancement and highlight emerging trends and effective strategies in this evolving research area. All data, including the processed videos and subjective comparison votes and scores, is made publicly available at https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GIF: Generative Inspiration for Face Recognition at Scale</title>
<link>https://arxiv.org/abs/2505.03012</link>
<guid>https://arxiv.org/abs/2505.03012</guid>
<content:encoded><![CDATA[
<div> Keywords: Face Recognition, Softmax, Computational cost, Structured identity codes, Logarithmic

Summary: 
The study introduces a new method to reduce the computational cost of Softmax in Face Recognition (FR) by using structured identity codes instead of atomic scalar labels. By transforming scalar labels into sequence of integers, the proposed tokenization scheme allows the FR backbone to predict codes for inputs, leading to a logarithmic decrease in computational cost as compared to linear. Experimental results on IJB-B and IJB-C datasets show performance improvements of 1.52% and 0.6% at TAR@FAR$=1e-4$ respectively, surpassing competitors. The code for the proposed method is available on GitHub for further exploration and implementation. Overall, the method demonstrates a significant enhancement in FR accuracy while efficiently managing computational resources. 

Summary:<br /><br />Keywords: Face Recognition, Softmax, Computational cost, Structured identity codes, Logarithmic <div>
arXiv:2505.03012v1 Announce Type: new 
Abstract: Aiming to reduce the computational cost of Softmax in massive label space of Face Recognition (FR) benchmarks, recent studies estimate the output using a subset of identities. Although promising, the association between the computation cost and the number of identities in the dataset remains linear only with a reduced ratio. A shared characteristic among available FR methods is the employment of atomic scalar labels during training. Consequently, the input to label matching is through a dot product between the feature vector of the input and the Softmax centroids. Inspired by generative modeling, we present a simple yet effective method that substitutes scalar labels with structured identity code, i.e., a sequence of integers. Specifically, we propose a tokenization scheme that transforms atomic scalar labels into structured identity codes. Then, we train an FR backbone to predict the code for each input instead of its scalar label. As a result, the associated computational cost becomes logarithmic w.r.t. number of identities. We demonstrate the benefits of the proposed method by conducting experiments. In particular, our method outperforms its competitors by 1.52%, and 0.6% at TAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the association between computational cost and the number of identities from linear to logarithmic. See code at https://github.com/msed-Ebrahimi/GIF
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer</title>
<link>https://arxiv.org/abs/2505.03018</link>
<guid>https://arxiv.org/abs/2505.03018</guid>
<content:encoded><![CDATA[
<div> Keywords: Contrast-Enhanced Spectral Mammography, CESM, Seg-CycleGAN, deep learning, lesion segmentation <br />
<br />
Summary: <br />
Contrast-Enhanced Spectral Mammography (CESM) is a technique that enhances lesion visibility in mammograms using an iodinated contrast agent. While CESM improves diagnostic accuracy, it comes with drawbacks such as higher radiation exposure and contrast medium side effects. The study introduces Seg-CycleGAN, a deep learning framework for Virtual Contrast Enhancement in CESM. This model generates high-quality dual-energy subtracted images from low-energy images, with the help of lesion segmentation maps. By enhancing diagnostically relevant regions through localized loss terms, Seg-CycleGAN outperforms baseline methods in terms of image quality metrics like PSNR and SSIM. The experiments on the CESM@UCBM dataset demonstrate improved lesion fidelity in the generated images. The findings suggest that segmentation-aware generative models like Seg-CycleGAN could pave the way for contrast-free alternatives in CESM. <br /> <div>
arXiv:2505.03018v1 Announce Type: new 
Abstract: Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic technique that improves lesion visibility through the administration of an iodinated contrast agent. It acquires both a low-energy image, comparable to standard mammography, and a high-energy image, which are then combined to produce a dual-energy subtracted image highlighting lesion contrast enhancement. While CESM offers superior diagnostic accuracy compared to standard mammography, its use entails higher radiation exposure and potential side effects associated with the contrast medium. To address these limitations, we propose Seg-CycleGAN, a generative deep learning framework for Virtual Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy subtracted images from low-energy images, leveraging lesion segmentation maps to guide the generative process and improve lesion reconstruction. Building upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss terms focused on lesion areas, enhancing the synthesis of diagnostically relevant regions. Experiments on the CESM@UCBM dataset demonstrate that Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while maintaining competitive MSE and VIF. Qualitative evaluations further confirm improved lesion fidelity in the generated images. These results suggest that segmentation-aware generative models offer a viable pathway toward contrast-free CESM alternatives.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices</title>
<link>https://arxiv.org/abs/2505.03039</link>
<guid>https://arxiv.org/abs/2505.03039</guid>
<content:encoded><![CDATA[
<div> wearable devices, depression, anxiety, anomaly detection, LSTM autoencoder

Summary:
In this study, researchers developed an explainable anomaly detection framework utilizing wearable device data to monitor and detect symptom escalation in depression and anxiety. The model, trained on data from over 2,000 participants with healthy baselines, focused on changes in sleep duration, step count, and resting heart rate to flag anomalies when self-reported depression or anxiety scores increased significantly. The model achieved an adjusted F1-score of 0.80, with higher performance in detecting episodes involving both depression and anxiety escalation and more pronounced symptom changes. Resting heart rate was identified as the most influential feature in detecting anomalies, followed by physical activity and sleep. These findings suggest the potential of using explainable anomaly detection for personalized and proactive mental health monitoring using consumer-grade wearable devices. 

<br /><br />Summary: <div>
arXiv:2505.03039v1 Announce Type: new 
Abstract: Continuous monitoring of behavior and physiology via wearable devices offers a novel, objective method for the early detection of worsening depression and anxiety. In this study, we present an explainable anomaly detection framework that identifies clinically meaningful increases in symptom severity using consumer-grade wearable data. Leveraging data from 2,023 participants with defined healthy baselines, our LSTM autoencoder model learned normal health patterns of sleep duration, step count, and resting heart rate. Anomalies were flagged when self-reported depression or anxiety scores increased by >=5 points (a threshold considered clinically significant). The model achieved an adjusted F1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393 symptom-worsening episodes across 341 participants, with higher performance observed for episodes involving concurrent depression and anxiety escalation (F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 = 0.85). Model interpretability was supported by SHAP-based analysis, which identified resting heart rate as the most influential feature in 71.4 percentage of detected anomalies, followed by physical activity and sleep. Together, our findings highlight the potential of explainable anomaly detection to enable personalized, scalable, and proactive mental health monitoring in real-world settings.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera</title>
<link>https://arxiv.org/abs/2505.03093</link>
<guid>https://arxiv.org/abs/2505.03093</guid>
<content:encoded><![CDATA[
<div> LiDAR, forest inventory, 360 video camera, photogrammetry, DBH <br />
Summary:<br />
The article introduces a cost-effective method for accurate forest inventory using a consumer-grade 360 video camera. The process involves dense point cloud reconstruction through Structure from Motion (SfM) software, trunk segmentation with SAM masks, and DBH estimation using RANSAC. An interactive visualization tool allows for inspection of segmented trees and their estimated DBH. The method achieves a median absolute relative error of 5-9% compared to manual measurements on 61 acquisitions of 43 trees, demonstrating close performance to LiDAR-based estimates at a fraction of the cost. This approach offers a practical and affordable alternative for forest monitoring, resource management, and carbon accounting. <br /> <div>
arXiv:2505.03093v1 Announce Type: new 
Abstract: Forest inventories rely on accurate measurements of the diameter at breast height (DBH) for ecological monitoring, resource management, and carbon accounting. While LiDAR-based techniques can achieve centimeter-level precision, they are cost-prohibitive and operationally complex. We present a low-cost alternative that only needs a consumer-grade 360 video camera. Our semi-automated pipeline comprises of (i) a dense point cloud reconstruction using Structure from Motion (SfM) photogrammetry software called Agisoft Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based technique to estimate cross section shape and DBH. We introduce an interactive visualization tool for inspecting segmented trees and their estimated DBH. On 61 acquisitions of 43 trees under a variety of conditions, our method attains median absolute relative errors of 5-9% with respect to "ground-truth" manual measurements. This is only 2-4% higher than LiDAR-based estimates, while employing a single 360 camera that costs orders of magnitude less, requires minimal setup, and is widely available.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability</title>
<link>https://arxiv.org/abs/2505.03097</link>
<guid>https://arxiv.org/abs/2505.03097</guid>
<content:encoded><![CDATA[
<div> diffusion models, U-Net, denoising, MaskUNet, image generation quality
Summary: 
The article introduces time-wise diffusion models that separate the generation of basic image structures from fine details. It identifies the importance of certain U-Net parameters for denoising, leading to the development of the MaskUNet method, which enhances generation quality with minimal parameters. MaskUNet leverages effective U-Net parameters that vary with timestep and sample, achieving impressive results in zero-shot inference on the COCO dataset. The method offers two fine-tuning strategies, including training-based and training-free approaches, utilizing tailored networks and optimization functions. MaskUNet outperforms other methods in FID score on the COCO dataset and proves its effectiveness in downstream task evaluations. The project page provides more information on MaskUNet and its applications. 
<br /><br />Summary: <div>
arXiv:2505.03097v1 Announce Type: new 
Abstract: The diffusion models, in early stages focus on constructing basic image structures, while the refined details, including local features and textures, are generated in later stages. Thus the same network layers are forced to learn both structural and textural information simultaneously, significantly differing from the traditional deep learning architectures (e.g., ResNet or GANs) which captures or generates the image semantic information at different layers. This difference inspires us to explore the time-wise diffusion models. We initially investigate the key contributions of the U-Net parameters to the denoising process and identify that properly zeroing out certain parameters (including large parameters) contributes to denoising, substantially improving the generation quality on the fly. Capitalizing on this discovery, we propose a simple yet effective method-termed ``MaskUNet''- that enhances generation quality with negligible parameter numbers. Our method fully leverages timestep- and sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer two fine-tuning strategies: a training-based approach and a training-free approach, including tailored networks and optimization functions. In zero-shot inference on the COCO dataset, MaskUNet achieves the best FID score and further demonstrates its effectiveness in downstream task evaluations. Project page: https://gudaochangsheng.github.io/MaskUnet-Page/
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Recognition with Online Lightweight Vision Transformer: A Survey</title>
<link>https://arxiv.org/abs/2505.03113</link>
<guid>https://arxiv.org/abs/2505.03113</guid>
<content:encoded><![CDATA[
<div> Efficient Component Design, Dynamic Network, Knowledge Distillation, lightweight vision transformers, ImageNet-1K benchmark
Summary: 
Efficient Component Design, Dynamic Network, and Knowledge Distillation are three key areas focused on in this survey of strategies for generating lightweight vision transformers for image recognition. The study evaluates the trade-offs in precision, parameters, throughput, and other factors on the ImageNet-1K benchmark to highlight the advantages, disadvantages, and flexibility of each approach. The paper aims to inspire further exploration and provide practical guidance for the community in the lightweighting of vision transformers. Future research directions and potential challenges in this area are also discussed. The findings suggest that leveraging online strategies can help address the computational and memory challenges faced by vision transformers in real-world applications. <br /><br />Summary: <div>
arXiv:2505.03113v1 Announce Type: new 
Abstract: The Transformer architecture has achieved significant success in natural language processing, motivating its adaptation to computer vision tasks. Unlike convolutional neural networks, vision transformers inherently capture long-range dependencies and enable parallel processing, yet lack inductive biases and efficiency benefits, facing significant computational and memory challenges that limit its real-world applicability. This paper surveys various online strategies for generating lightweight vision transformers for image recognition, focusing on three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation. We evaluate the relevant exploration for each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision, parameters, throughput, and more to highlight their respective advantages, disadvantages, and flexibility. Finally, we propose future research directions and potential challenges in the lightweighting of vision transformers with the aim of inspiring further exploration and providing practical guidance for the community. Project Page: https://github.com/ajxklo/Lightweight-VIT
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation</title>
<link>https://arxiv.org/abs/2505.03114</link>
<guid>https://arxiv.org/abs/2505.03114</guid>
<content:encoded><![CDATA[
<div> regularized approach, unpaired MRI-to-CT translation, neural ordinary differential equations, bone contours, bone structures<br />
Summary:<br />
The paper introduces a novel path- and bone-contour regularized approach for unpaired MRI-to-CT translation. Using neural ordinary differential equations, the method projects MRI and CT images to a shared latent space, minimizing transition path length for optimal mapping. A neural network generates bone contours from MRI to improve bone structure translation accuracy. The approach outperforms existing methods, achieving lower error rates and better bone structure fidelity in downstream bone segmentation tasks. The code for the method is available on GitHub for further exploration. <div>
arXiv:2505.03114v1 Announce Type: new 
Abstract: Accurate MRI-to-CT translation promises the integration of complementary imaging information without the need for additional imaging sessions. Given the practical challenges associated with acquiring paired MRI and CT scans, the development of robust methods capable of leveraging unpaired datasets is essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT translation methods, which predominantly rely on cycle consistency and contrastive learning frameworks, frequently encounter challenges in accurately translating anatomical features that are highly discernible on CT but less distinguishable on MRI, such as bone structures. This limitation renders these approaches less suitable for applications in radiation therapy, where precise bone representation is essential for accurate treatment planning. To address this challenge, we propose a path- and bone-contour regularized approach for unpaired MRI-to-CT translation. In our method, MRI and CT images are projected to a shared latent space, where the MRI-to-CT mapping is modeled as a continuous flow governed by neural ordinary differential equations. The optimal mapping is obtained by minimizing the transition path length of the flow. To enhance the accuracy of translated bone structures, we introduce a trainable neural network to generate bone contours from MRI and implement mechanisms to directly and indirectly encourage the model to focus on bone contours and their adjacent regions. Evaluations conducted on three datasets demonstrate that our method outperforms existing unpaired MRI-to-CT translation approaches, achieving lower overall error rates. Moreover, in a downstream bone segmentation task, our approach exhibits superior performance in preserving the fidelity of bone structures. Our code is available at: https://github.com/kennysyp/PaBoT.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion</title>
<link>https://arxiv.org/abs/2505.03116</link>
<guid>https://arxiv.org/abs/2505.03116</guid>
<content:encoded><![CDATA[
<div> Keywords: Video frame interpolation, event cameras, non-linear motion, continuous point tracking, motion estimation

Summary: 
The paper introduces a novel framework, TimeTracker, for video frame interpolation using event cameras. It addresses the challenge of non-linear motion in scenes by tracking continuous motion trajectories of patches through events. A Scene-Aware Region Segmentation module is used to divide the scene into patches, and a Continuous Trajectory guided Motion Estimation module tracks motion trajectories. Intermediate frames are generated through global motion optimization. The proposed method outperforms existing techniques in both motion estimation and frame interpolation quality. Additionally, a real-world dataset featuring fast non-linear motion is collected for evaluation purposes. The study contributes to enhancing video frame interpolation performance by effectively leveraging event cameras and continuous point tracking techniques.<br /><br />Summary: <div>
arXiv:2505.03116v1 Announce Type: new 
Abstract: Video frame interpolation (VFI) that leverages the bio-inspired event cameras as guidance has recently shown better performance and memory efficiency than the frame-based methods, thanks to the event cameras' advantages, such as high temporal resolution. A hurdle for event-based VFI is how to effectively deal with non-linear motion, caused by the dynamic changes in motion direction and speed within the scene. Existing methods either use events to estimate sparse optical flow or fuse events with image features to estimate dense optical flow. Unfortunately, motion errors often degrade the VFI quality as the continuous motion cues from events do not align with the dense spatial information of images in the temporal dimension. In this paper, we find that object motion is continuous in space, tracking local regions over continuous time enables more accurate identification of spatiotemporal feature correlations. In light of this, we propose a novel continuous point tracking-based VFI framework, named TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation (SARS) module to divide the scene into similar patches. Then, a Continuous Trajectory guided Motion Estimation (CTME) module is proposed to track the continuous motion trajectory of each patch through events. Finally, intermediate frames at any given time are generated through global motion optimization and frame refinement. Moreover, we collect a real-world dataset that features fast non-linear motion. Extensive experiments show that our method outperforms prior arts in both motion estimation and frame interpolation quality.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis</title>
<link>https://arxiv.org/abs/2505.03132</link>
<guid>https://arxiv.org/abs/2505.03132</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, data slicing, computer vision, visual analytics, object detection

Summary:
VISLIX is a novel visual analytics framework designed to assist in evaluating machine learning models, particularly in computer vision tasks such as object detection. It addresses challenges in data slicing, which is crucial for identifying model performance issues. Unlike traditional data slicing methods, VISLIX does not require additional image metadata or visual concepts, making it more versatile for various computer vision tasks. The framework generates natural language insights automatically, reducing the manual effort and expertise needed to interpret data slices. Additionally, VISLIX enables interactive testing of data slice hypotheses, providing a human-in-the-loop solution for model validation. Evaluation studies and use cases have shown the effectiveness of VISLIX in providing comprehensive insights for validating object detection models. The tool enhances the machine learning operations lifecycle by improving the efficiency and accuracy of model evaluation in safety-critical domains. 

<br /><br />Summary: <div>
arXiv:2505.03132v1 Announce Type: new 
Abstract: Real-world machine learning models require rigorous evaluation before deployment, especially in safety-critical domains like autonomous driving and surveillance. The evaluation of machine learning models often focuses on data slices, which are subsets of the data that share a set of characteristics. Data slice finding automatically identifies conditions or data subgroups where models underperform, aiding developers in mitigating performance issues. Despite its popularity and effectiveness, data slicing for vision model validation faces several challenges. First, data slicing often needs additional image metadata or visual concepts, and falls short in certain computer vision tasks, such as object detection. Second, understanding data slices is a labor-intensive and mentally demanding process that heavily relies on the expert's domain knowledge. Third, data slicing lacks a human-in-the-loop solution that allows experts to form hypothesis and test them interactively. To overcome these limitations and better support the machine learning operations lifecycle, we introduce VISLIX, a novel visual analytics framework that employs state-of-the-art foundation models to help domain experts analyze slices in computer vision models. Our approach does not require image metadata or visual concepts, automatically generates natural language insights, and allows users to test data slice hypothesis interactively. We evaluate VISLIX with an expert study and three use cases, that demonstrate the effectiveness of our tool in providing comprehensive insights for validating object detection models.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control</title>
<link>https://arxiv.org/abs/2505.03134</link>
<guid>https://arxiv.org/abs/2505.03134</guid>
<content:encoded><![CDATA[
<div> Keywords: industrial glass manufacturing, visual defect detection, deep learning, class imbalance, data augmentation 

Summary: 
This paper introduces a new method for detecting visual defects in industrial glass manufacturing by utilizing Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic defective glass images. The main challenge in defect detection is the imbalanced dataset, which hinders the performance of deep learning models. By using DDPMs for data augmentation, the methodology effectively addresses the class imbalance issue, leading to improved image classification performance of standard CNN architectures. Experimental results show significant enhancements in key machine learning metrics, particularly in recall for defective samples, across all tested deep neural network architectures. The ResNet50V2 architecture demonstrated the most dramatic improvement in classification accuracy, increasing from 78% to 93% when trained with augmented data. This approach offers a scalable and cost-effective solution for enhancing automated defect detection in glass manufacturing and could be applicable to other industries facing similar challenges with class imbalance. 

Summary: <br /><br /> <div>
arXiv:2505.03134v1 Announce Type: new 
Abstract: Visual defect detection in industrial glass manufacturing remains a critical challenge due to the low frequency of defective products, leading to imbalanced datasets that limit the performance of deep learning models and computer vision systems. This paper presents a novel approach using Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic defective glass product images for data augmentation, effectively addressing class imbalance issues in manufacturing quality control and automated visual inspection. The methodology significantly enhances image classification performance of standard CNN architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting anomalies by increasing the minority class representation. Experimental results demonstrate substantial improvements in key machine learning metrics, particularly in recall for defective samples across all tested deep neural network architectures while maintaining perfect precision. The most dramatic improvement was observed in ResNet50V2's overall classification accuracy, which increased from 78 percent to 93 percent when trained with the augmented data. This work provides a scalable, cost-effective approach to enhancing automated defect detection in glass manufacturing that can potentially be extended to other industrial quality assurance systems and industries with similar class imbalance challenges.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)</title>
<link>https://arxiv.org/abs/2505.03149</link>
<guid>https://arxiv.org/abs/2505.03149</guid>
<content:encoded><![CDATA[
<div> motion-compensated image reconstruction, unsupervised, cardiac MRI, low-rank model, free-breathing

Summary:
An unsupervised motion-compensated image reconstruction algorithm is proposed for free-breathing 3D cardiac MRI. The algorithm utilizes a low-rank model to jointly represent a family of diffeomorphisms corresponding to specific motion phases. By integrating a parametric velocity field, the algorithm effectively captures the motion information and reconstructs the image volume for each motion phase. The static template and low-rank motion model parameters are learned directly from k-space data in an unsupervised manner. The constrained motion model shows superior performance compared to existing motion-resolved and motion-compensated algorithms for free-breathing 3D cine MRI. <div>
arXiv:2505.03149v1 Announce Type: new 
Abstract: We introduce an unsupervised motion-compensated image reconstruction algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging (MRI). We express the image volume corresponding to each specific motion phase as the deformation of a single static image template. The main contribution of the work is the low-rank model for the compact joint representation of the family of diffeomorphisms, parameterized by the motion phases. The diffeomorphism at a specific motion phase is obtained by integrating a parametric velocity field along a path connecting the reference template phase to the motion phase. The velocity field at different phases is represented using a low-rank model. The static template and the low-rank motion model parameters are learned directly from the k-space data in an unsupervised fashion. The more constrained motion model is observed to offer improved recovery compared to current motion-resolved and motion-compensated algorithms for free-breathing 3D cine MRI.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Fairness Vision-Language Learning for Medical Image Analysis</title>
<link>https://arxiv.org/abs/2505.03153</link>
<guid>https://arxiv.org/abs/2505.03153</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models (VLMs), medical image analysis, fairness, robustness, loss function<br />
Summary:<br />
This paper introduces a framework for enhancing the robustness and fairness of Vision-Language Models (VLMs) in medical image analysis. The framework focuses on modifying the loss function during training to adjust faulty image-text pairs using a Dynamic Bad Pair Mining algorithm. Additionally, it incorporates Sinkhorn distance to ensure that the loss distributions of protected groups remain consistent with the total loss. Experimental results demonstrate improvements of up to 8.6% in equity-scaled AUC, highlighting the effectiveness of the proposed framework in enhancing VLM performance while maintaining fairness and robustness. <div>
arXiv:2505.03153v1 Announce Type: new 
Abstract: The advent of Vision-Language Models (VLMs) in medical image analysis has the potential to help process multimodal inputs and increase performance over traditional inference methods. However, when considering the domain in which these models will be implemented, fairness and robustness are important to ensure the model stays true for any patient. In this paper, we introduce a framework for ensuring robustness and fairness of VLM models. This framework modifies the loss function at training by identifying and adjusting faulty image-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing Sinkhorn distance to ensure the loss distributions of protected groups do not deviate from the total loss. Experimental testing of our framework shows up to a 8.6\% improvement when looking at equity-scaled AUC.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data</title>
<link>https://arxiv.org/abs/2505.03154</link>
<guid>https://arxiv.org/abs/2505.03154</guid>
<content:encoded><![CDATA[
<div> Motion capture, mocap, data, artifacts, cleanup  
Summary:  
StableMotion introduces a method for training motion cleanup models directly from unpaired corrupted datasets. By using motion quality indicators, the model can generate high-quality motions from raw data with mixed quality. This diffusion-based framework creates a unified model to identify and fix corrupted frames. The method is demonstrated on the SoccerMocap dataset, effectively reducing motion pops and frozen frames by 68% and 81% respectively. The approach offers a simple yet effective solution for automating the cleanup process of motion capture data, eliminating the need for laborious manual cleanup and access to high-quality training data. The results show improvement in cleaning a wide range of motion artifacts, making it suitable for production scenarios. <br /><br />Summary: <div>
arXiv:2505.03154v1 Announce Type: new 
Abstract: Motion capture (mocap) data often exhibits visually jarring artifacts due to inaccurate sensors and post-processing. Cleaning this corrupted data can require substantial manual effort from human experts, which can be a costly and time-consuming process. Previous data-driven motion cleanup methods offer the promise of automating this cleanup process, but often require in-domain paired corrupted-to-clean training data. Constructing such paired datasets requires access to high-quality, relatively artifact-free motion clips, which often necessitates laborious manual cleanup. In this work, we present StableMotion, a simple yet effective method for training motion cleanup models directly from unpaired corrupted datasets that need cleanup. The core component of our method is the introduction of motion quality indicators, which can be easily annotated through manual labeling or heuristic algorithms and enable training of quality-aware motion generation models on raw motion data with mixed quality. At test time, the model can be prompted to generate high-quality motions using the quality indicators. Our method can be implemented through a simple diffusion-based framework, leading to a unified motion generate-discriminate model, which can be used to both identify and fix corrupted frames. We demonstrate that our proposed method is effective for training motion cleanup models on raw mocap data in production scenarios by applying StableMotion to SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion artifacts. The trained model effectively corrects a wide range of motion artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively. See https://youtu.be/3Y7MMAH02B4 for more results.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph</title>
<link>https://arxiv.org/abs/2505.03173</link>
<guid>https://arxiv.org/abs/2505.03173</guid>
<content:encoded><![CDATA[
<div> Keywords: RAVU, video understanding, retrieval, spatio-temporal graph, multi-hop reasoning<br />
Summary:<br />
- RAVU (Retrieval Augmented Video Understanding) is a novel framework that enhances video understanding through retrieval with compositional reasoning over a spatio-temporal graph.
- The framework utilizes a graph representation of the video to capture spatial and temporal relationships between entities, enabling long-term memory to track objects and their actions across time.
- Complex queries are decomposed into reasoning steps and executed on the graph to retrieve relevant key information, allowing for more accurate understanding of long videos, especially for multi-hop reasoning and object tracking.
- RAVU demonstrates superior performance with limited retrieved frames (5-10) compared to other state-of-the-art methods and baselines on the NExT-QA and EgoSchema video QA datasets. <br /> <div>
arXiv:2505.03173v1 Announce Type: new 
Abstract: Comprehending long videos remains a significant challenge for Large Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to hours videos due to their lack of explicit memory and retrieval mechanisms. To address this limitation, we propose RAVU (Retrieval Augmented Video Understanding), a novel framework for video understanding enhanced by retrieval with compositional reasoning over a spatio-temporal graph. We construct a graph representation of the video, capturing both spatial and temporal relationships between entities. This graph serves as a long-term memory, allowing us to track objects and their actions across time. To answer complex queries, we decompose the queries into a sequence of reasoning steps and execute these steps on the graph, retrieving relevant key information. Our approach enables more accurate understanding of long videos, particularly for queries that require multi-hop reasoning and tracking objects across frames. Our approach demonstrate superior performances with limited retrieved frames (5-10) compared with other SOTA methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models</title>
<link>https://arxiv.org/abs/2505.03176</link>
<guid>https://arxiv.org/abs/2505.03176</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised algorithms, visual representations, joint-embedding predictive architecture, sequence processing, transformer encoder

Summary:<br /><br />
The paper introduces seq-JEPA, a novel world modeling paradigm that aims to overcome the trade-off between invariance and equivariance in self-supervised algorithms. Unlike traditional two-view paradigms, seq-JEPA utilizes architectural inductive biases to simultaneously learn equivariant and invariant representations by processing a short sequence of different views of an input image. The model concatenates each encoded view with embeddings of the relative transformation to predict the next observation. Through a transformer encoder, seq-JEPA successfully achieves strong performance on equivariant benchmarks and image classification tasks without compromising on either aspect. Furthermore, the framework demonstrates proficiency in tasks requiring sequence processing, such as path integration and predictive learning across actions. Overall, seq-JEPA offers a promising approach towards learning flexible visual representations that can adapt to a variety of downstream tasks. 

<br /><br /> <div>
arXiv:2505.03176v1 Announce Type: new 
Abstract: Current self-supervised algorithms mostly rely on transformations such as data augmentation and masking to learn visual representations. This is achieved by inducing invariance or equivariance with respect to these transformations after encoding two views of an image. This dominant two-view paradigm can limit the flexibility of learned representations for downstream adaptation by creating performance trade-offs between invariance-related tasks such as image classification and more fine-grained equivariance-related tasks. In this work, we introduce \emph{seq-JEPA}, a world modeling paradigm based on joint-embedding predictive architecture that leverages architectural inductive biases to resolve this trade-off. Without requiring an additional equivariance predictor or loss term, seq-JEPA simultaneously learns two architecturally segregated representations: one equivariant to the specified transformations and another invariant to them and suited for tasks such as classification. To do so, our model processes a short sequence of different views (observations) of an input image. Each encoded view is concatenated with embeddings corresponding to the relative transformation (action) producing the next observation in the sequence. A transformer encoder outputs an aggregate representation of this sequence, which is subsequently conditioned on the action leading to the next observation to predict its representation. Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and image classification without sacrificing one for the other. Additionally, our framework excels at tasks that inherently require aggregating a sequence of observations, such as path integration across actions and predictive learning across eye movements.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interactive Instance Annotation with Siamese Networks</title>
<link>https://arxiv.org/abs/2505.03184</link>
<guid>https://arxiv.org/abs/2505.03184</guid>
<content:encoded><![CDATA[
<div> Siamese networks, instance masks, semantic segmentation, deep learning, annotation<br />
Summary:<br />
The paper introduces SiamAnno, a framework for annotating instance masks efficiently. It utilizes Siamese networks and one-shot learning to predict object boundaries using a bounding box input. This approach enables annotating previously unseen objects without fine-tuning on different datasets, achieving state-of-the-art performance in cross-domain annotation tasks. SiamAnno showcases effectiveness in handling domain and environment shifts and provides comprehensive results compared to existing methods, setting a strong baseline for future research. The model marks the first exploration of Siamese architecture for instance annotation, showcasing its potential in reducing the time and effort required for manual annotations. <div>
arXiv:2505.03184v1 Announce Type: new 
Abstract: Annotating instance masks is time-consuming and labor-intensive. A promising solution is to predict contours using a deep learning model and then allow users to refine them. However, most existing methods focus on in-domain scenarios, limiting their effectiveness for cross-domain annotation tasks. In this paper, we propose SiamAnno, a framework inspired by the use of Siamese networks in object tracking. SiamAnno leverages one-shot learning to annotate previously unseen objects by taking a bounding box as input and predicting object boundaries, which can then be adjusted by annotators. Trained on one dataset and tested on another without fine-tuning, SiamAnno achieves state-of-the-art (SOTA) performance across multiple datasets, demonstrating its ability to handle domain and environment shifts in cross-domain tasks. We also provide more comprehensive results compared to previous work, establishing a strong baseline for future research. To our knowledge, SiamAnno is the first model to explore Siamese architecture for instance annotation.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models</title>
<link>https://arxiv.org/abs/2505.03203</link>
<guid>https://arxiv.org/abs/2505.03203</guid>
<content:encoded><![CDATA[
<div> noise selection, random noise, text-image alignment, referring mask, pixel-level masks

Summary:
PiCo (Pick-and-Control) is a novel training-free approach for text-to-image generation that addresses challenges with text-image alignment. It focuses on the quality of randomly initialized noise and the reliability of generated controlling masks. The approach includes a noise selection module that assesses the quality of random noise for a target text, using a fast sampling strategy for efficiency. Additionally, a referring mask module generates pixel-level masks to modulate cross-attention maps, guiding the interaction between text and image features. PiCo aims to enhance text-image alignment for diverse text descriptions and liberate users from random generation processes. Extensive experiments confirm the effectiveness of PiCo in improving text-to-image compositional generation. 

<br /><br />Summary: <div>
arXiv:2505.03203v1 Announce Type: new 
Abstract: Advanced diffusion models have made notable progress in text-to-image compositional generation. However, it is still a challenge for existing models to achieve text-image alignment when confronted with complex text prompts. In this work, we highlight two factors that affect this alignment: the quality of the randomly initialized noise and the reliability of the generated controlling mask. We then propose PiCo (Pick-and-Control), a novel training-free approach with two key components to tackle these two factors. First, we develop a noise selection module to assess the quality of the random noise and determine whether the noise is suitable for the target text. A fast sampling strategy is utilized to ensure efficiency in the noise selection stage. Second, we introduce a referring mask module to generate pixel-level masks and to precisely modulate the cross-attention maps. The referring mask is applied to the standard diffusion process to guide the reasonable interaction between text and image features. Extensive experiments have been conducted to verify the effectiveness of PiCo in liberating users from the tedious process of random generation and in enhancing the text-image alignment for diverse text descriptions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations</title>
<link>https://arxiv.org/abs/2505.03204</link>
<guid>https://arxiv.org/abs/2505.03204</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, breast cancer, histopathology images, limited annotated data, medical imaging <br />
Summary: 
Deep learning methods have shown promise in accurately classifying breast cancer histopathology images. However, their performance tends to decrease when there is a lack of annotated data available for training. Annotating medical imaging data is a costly and time-consuming process that requires specialized expertise, thus limiting the amount of annotated data that can be utilized. This shortage of annotated data poses a significant challenge for developing robust deep learning models for medical image analysis, including breast cancer classification. Addressing this challenge is crucial for improving the accuracy and reliability of automated histopathology image analysis tools in the medical field. The study highlights the importance of finding solutions to effectively handle limited annotated data in training deep learning models for medical imaging applications. <br /><br />Summary: <div>
arXiv:2505.03204v1 Announce Type: new 
Abstract: Deep learning methods have shown promise in classifying breast cancer histopathology images, but their performance often declines with limited annotated data, a critical challenge in medical imaging due to the high cost and expertise required for annotations.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data</title>
<link>https://arxiv.org/abs/2505.03220</link>
<guid>https://arxiv.org/abs/2505.03220</guid>
<content:encoded><![CDATA[
<div> Keywords: hyperspectral images, deep learning, transformer-based architectures, self-supervised pretraining, spatial-frequency masking

Summary: 
The article proposes Spatial-Frequency Masked Image Modeling (SFMIM) as a self-supervised pretraining strategy for hyperspectral data to overcome the limitation of scarce labeled data. SFMIM utilizes dual-domain masking mechanisms in both spatial and frequency domains to train transformer-based architectures efficiently. By reconstructing masked patches and frequency components, the model captures higher-order spectral-spatial correlations, leading to state-of-the-art performance on three HSI classification benchmarks. The approach shows rapid convergence during fine-tuning, demonstrating the effectiveness and efficiency of the pretraining strategy. <div>
arXiv:2505.03220v1 Announce Type: new 
Abstract: Hyperspectral images (HSIs) capture rich spectral signatures that reveal vital material properties, offering broad applicability across various domains. However, the scarcity of labeled HSI data limits the full potential of deep learning, especially for transformer-based architectures that require large-scale training. To address this constraint, we propose Spatial-Frequency Masked Image Modeling (SFMIM), a self-supervised pretraining strategy for hyperspectral data that utilizes the large portion of unlabeled data. Our method introduces a novel dual-domain masking mechanism that operates in both spatial and frequency domains. The input HSI cube is initially divided into non-overlapping patches along the spatial dimension, with each patch comprising the entire spectrum of its corresponding spatial location. In spatial masking, we randomly mask selected patches and train the model to reconstruct the masked inputs using the visible patches. Concurrently, in frequency masking, we remove portions of the frequency components of the input spectra and predict the missing frequencies. By learning to reconstruct these masked components, the transformer-based encoder captures higher-order spectral-spatial correlations. We evaluate our approach on three publicly available HSI classification benchmarks and demonstrate that it achieves state-of-the-art performance. Notably, our model shows rapid convergence during fine-tuning, highlighting the efficiency of our pretraining strategy.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing the Abstract: Translating the Abstract Language for Vision Language Models</title>
<link>https://arxiv.org/abs/2505.03242</link>
<guid>https://arxiv.org/abs/2505.03242</guid>
<content:encoded><![CDATA[
<div> keywords: Vision Language Models, abstract concepts, fashion domain, Abstract-to-Concrete Translator, text-to-image retrieval

Summary:
Natural language extends beyond describing visual content, encompassing abstract concepts like feelings and creativity. Current research on Vision Language Models (VLMs) lacks focus on abstract language, despite its value. In the fashion field, analyzing large-scale datasets reveals the prevalence and usefulness of abstract terms compared to concrete ones. However, existing VLMs are limited in representing abstract language due to database training deficiencies. To address this, a model-agnostic method called Abstract-to-Concrete Translator (ACT) is proposed. ACT enhances abstract representations in VLMs by shifting them towards well-represented concrete terms using pre-trained models and multimodal databases. On the text-to-image retrieval task, ACT outperforms fine-tuned VLMs, showcasing generalization capabilities. The consistent enhancement across various VLMs makes ACT a versatile solution. 

<br /><br />Summary: <div>
arXiv:2505.03242v1 Announce Type: new 
Abstract: Natural language goes beyond dryly describing visual content. It contains rich abstract concepts to express feeling, creativity and properties that cannot be directly perceived. Yet, current research in Vision Language Models (VLMs) has not shed light on abstract-oriented language. Our research breaks new ground by uncovering its wide presence and under-estimated value, with extensive analysis. Particularly, we focus our investigation on the fashion domain, a highly-representative field with abstract expressions. By analyzing recent large-scale multimodal fashion datasets, we find that abstract terms have a dominant presence, rivaling the concrete ones, providing novel information, and being useful in the retrieval task. However, a critical challenge emerges: current general-purpose or fashion-specific VLMs are pre-trained with databases that lack sufficient abstract words in their text corpora, thus hindering their ability to effectively represent abstract-oriented language. We propose a training-free and model-agnostic method, Abstract-to-Concrete Translator (ACT), to shift abstract representations towards well-represented concrete ones in the VLM latent space, using pre-trained models and existing multimodal databases. On the text-to-image retrieval task, despite being training-free, ACT outperforms the fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its effectiveness with a strong generalization capability. Moreover, the improvement introduced by ACT is consistent with various VLMs, making it a plug-and-play solution.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs</title>
<link>https://arxiv.org/abs/2505.03254</link>
<guid>https://arxiv.org/abs/2505.03254</guid>
<content:encoded><![CDATA[
<div> quantization, convolutional neural networks, depthwise-separable architectures, energy cost, storage size
Summary:
The article introduces a new approach, PROM, for quantizing modern depthwise-separable convolutional networks by selectively using ternary weights for pointwise convolutions and 8-bit weights for other modules. By also quantizing activations to 8-bit, PROM transforms pointwise convolutions with ternary weights into int8 additions, significantly reducing energy cost. Applying PROM to MobileNetV2 results in a 23.9x reduction in energy cost and a 2.7x decrease in storage size compared to the float16 baseline, while maintaining similar classification performance on ImageNet. This approach advances the Pareto frontier for energy consumption vs. top-1 accuracy for quantized convolutional models on ImageNet, addressing the challenges of quantizing depthwise-separable networks and offering a simple way to reduce energy cost and storage size. 

<br /><br />Summary: <div>
arXiv:2505.03254v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) are crucial for computer vision tasks on resource-constrained devices. Quantization effectively compresses these models, reducing storage size and energy cost. However, in modern depthwise-separable architectures, the computational cost is distributed unevenly across its components, with pointwise operations being the most expensive. By applying a general quantization scheme to this imbalanced cost distribution, existing quantization approaches fail to fully exploit potential efficiency gains. To this end, we introduce PROM, a straightforward approach for quantizing modern depthwise-separable convolutional networks by selectively using two distinct bit-widths. Specifically, pointwise convolutions are quantized to ternary weights, while the remaining modules use 8-bit weights, which is achieved through a simple quantization-aware training procedure. Additionally, by quantizing activations to 8-bit, our method transforms pointwise convolutions with ternary weights into int8 additions, which enjoy broad support across hardware platforms and effectively eliminates the need for expensive multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost by more than an order of magnitude (23.9x) and its storage size by 2.7x compared to the float16 baseline while retaining similar classification performance on ImageNet. Our method advances the Pareto frontier for energy consumption vs. top-1 accuracy for quantized convolutional models on ImageNet. PROM addresses the challenges of quantizing depthwise-separable convolutional networks to both ternary and 8-bit weights, offering a simple way to reduce energy cost and storage size.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor</title>
<link>https://arxiv.org/abs/2505.03261</link>
<guid>https://arxiv.org/abs/2505.03261</guid>
<content:encoded><![CDATA[
<div> diffusion model, Video Quality Assessment, temporal coherence, feature extractor, VQA framework

Summary:<br />
DiffVQA introduces a novel Video Quality Assessment (VQA) framework that utilizes diffusion models pre-trained on extensive datasets for assessing video quality. The framework adapts the diffusion model to reconstruct input frames through a control module, extracting semantic and distortion features. It enhances handling of temporal dynamics with a parallel Mamba module to extract temporal coherence augmented features. These features are merged with diffusion features to predict the final quality score. Experimental results show superior performance of DiffVQA on intra-dataset evaluations and high generalization across datasets. Leveraging a diffusion model as a feature extractor yields enhanced VQA performance compared to CNN and ViT backbones. The framework's ability to align closely with human perceptions in diverse real-world scenarios is attributed to its robust generalization capabilities. <div>
arXiv:2505.03261v1 Announce Type: new 
Abstract: Video Quality Assessment (VQA) aims to evaluate video quality based on perceptual distortions and human preferences. Despite the promising performance of existing methods using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), they often struggle to align closely with human perceptions, particularly in diverse real-world scenarios. This challenge is exacerbated by the limited scale and diversity of available datasets. To address this limitation, we introduce a novel VQA framework, DiffVQA, which harnesses the robust generalization capabilities of diffusion models pre-trained on extensive datasets. Our framework adapts these models to reconstruct identical input frames through a control module. The adapted diffusion model is then used to extract semantic and distortion features from a resizing branch and a cropping branch, respectively. To enhance the model's ability to handle long-term temporal dynamics, a parallel Mamba module is introduced, which extracts temporal coherence augmented features that are merged with the diffusion features to predict the final score. Experiments across multiple datasets demonstrate DiffVQA's superior performance on intra-dataset evaluations and its exceptional generalization across datasets. These results confirm that leveraging a diffusion model as a feature extractor can offer enhanced VQA performance compared to CNN and ViT backbones.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction</title>
<link>https://arxiv.org/abs/2505.03284</link>
<guid>https://arxiv.org/abs/2505.03284</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous vehicles, 3D semantic occupancy prediction, multisensor fusion, cylindrical coordinates, nuScenes dataset

Summary:
The article introduces OccCylindrical, a novel approach for 3D semantic occupancy prediction in autonomous vehicles (AVs) that utilizes sensor fusion under cylindrical coordinates. Unlike existing methods that focus on Cartesian coordinates, OccCylindrical considers the distribution of sensor readings, resulting in better preservation of fine-grained geometry detail and improved performance. Through experiments on the nuScenes dataset, including challenging scenarios like rain and night driving, OccCylindrical demonstrates state-of-the-art effectiveness. The proposed approach showcases superior performance in understanding surroundings and is a significant advancement in the field of autonomous vehicle perception technologies.<br /><br />Summary: <div>
arXiv:2505.03284v1 Announce Type: new 
Abstract: The safe operation of autonomous vehicles (AVs) is highly dependent on their understanding of the surroundings. For this, the task of 3D semantic occupancy prediction divides the space around the sensors into voxels, and labels each voxel with both occupancy and semantic information. Recent perception models have used multisensor fusion to perform this task. However, existing multisensor fusion-based approaches focus mainly on using sensor information in the Cartesian coordinate system. This ignores the distribution of the sensor readings, leading to a loss of fine-grained details and performance degradation. In this paper, we propose OccCylindrical that merges and refines the different modality features under cylindrical coordinates. Our method preserves more fine-grained geometry detail that leads to better performance. Extensive experiments conducted on the nuScenes dataset, including challenging rainy and nighttime scenarios, confirm our approach's effectiveness and state-of-the-art performance. The code will be available at: https://github.com/DanielMing123/OccCylindrical
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2505.03286</link>
<guid>https://arxiv.org/abs/2505.03286</guid>
<content:encoded><![CDATA[
<div> Keywords: Visible-infrared person re-identification, Base-Detail Feature Learning Framework, modality-specific details, detail feature extraction, correlation restriction.

Summary:<br /><br />Visible-infrared person re-identification (VIReID) faces challenges due to discrepancies between visible and infrared modalities. Existing methods focus on modality-shared information and overlook modality-specific details. To address this, the Base-Detail Feature Learning Framework (BDLF) is proposed to enhance base and detail knowledge learning. BDLF utilizes a detail feature extraction module and base embedding generation mechanism, along with a correlation restriction method to enrich features across modalities. Experiments on various datasets confirm the effectiveness of BDLF in improving person re-identification accuracy. <div>
arXiv:2505.03286v1 Announce Type: new 
Abstract: Visible-infrared person re-identification (VIReID) provides a solution for ReID tasks in 24-hour scenarios; however, significant challenges persist in achieving satisfactory performance due to the substantial discrepancies between visible (VIS) and infrared (IR) modalities. Existing methods inadequately leverage information from different modalities, primarily focusing on digging distinguishing features from modality-shared information while neglecting modality-specific details. To fully utilize differentiated minutiae, we propose a Base-Detail Feature Learning Framework (BDLF) that enhances the learning of both base and detail knowledge, thereby capitalizing on both modality-shared and modality-specific information. Specifically, the proposed BDLF mines detail and base features through a lossless detail feature extraction module and a complementary base embedding generation mechanism, respectively, supported by a novel correlation restriction method that ensures the features gained by BDLF enrich both detail and base knowledge across VIS and IR features. Comprehensive experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the effectiveness of BDLF.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach</title>
<link>https://arxiv.org/abs/2505.03299</link>
<guid>https://arxiv.org/abs/2505.03299</guid>
<content:encoded><![CDATA[
<div> earth observation, remote sensing, vision models, capabilities encoding, downstream tasks

Summary:
The article discusses the development of over 75 remote sensing vision foundation models in the past four years. These models have the capability to address various tasks after a single training phase. However, there is no consistent top performer across all downstream tasks. To address this issue, the authors propose a cost-effective method for predicting a model's performance on multiple downstream tasks without the need for fine-tuning. This approach, called "capabilities encoding," simplifies the selection of a foundation model for new tasks and offers a new perspective on existing literature. The authors provide codes for this method and suggest potential avenues for future research in the field of Earth observation and computer vision. The proposed method aims to facilitate model comparison and selection, ultimately advancing the field of remote sensing and computer vision. 

<br /><br />Summary: <div>
arXiv:2505.03299v1 Announce Type: new 
Abstract: Foundation models constitute a significant advancement in computer vision: after a single, albeit costly, training phase, they can address a wide array of tasks. In the field of Earth observation, over 75 remote sensing vision foundation models have been developed in the past four years. However, none has consistently outperformed the others across all available downstream tasks. To facilitate their comparison, we propose a cost-effective method for predicting a model's performance on multiple downstream tasks without the need for fine-tuning on each one. This method is based on what we call "capabilities encoding." The utility of this novel approach is twofold: we demonstrate its potential to simplify the selection of a foundation model for a given new task, and we employ it to offer a fresh perspective on the existing literature, suggesting avenues for future research. Codes are available at https://github.com/pierreadorni/capabilities-encoding.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation</title>
<link>https://arxiv.org/abs/2505.03300</link>
<guid>https://arxiv.org/abs/2505.03300</guid>
<content:encoded><![CDATA[
<div> semantic segmentation, 3D LiDAR, supervised learning, domain shifts, pseudo-labels
Summary:
- The article introduces a new pipeline for 3D semantic segmentation using aligned scenes and 2D segmentation methods.
- The approach generates 2D views from LiDAR scans colored by sensor intensity and applies 2D semantic segmentation using a pretrained model.
- The segmented 2D outputs are back-projected onto 3D points using a simple voting-based estimator.
- This global pipeline requires no prior 3D annotation and does not rely on additional modalities for inference.
- The generated pseudo-labels from the segmentation can be used for Unsupervised Domain Adaptation tasks. 

<br /><br />Summary: <div>
arXiv:2505.03300v1 Announce Type: new 
Abstract: Semantic segmentation of 3D LiDAR point clouds, essential for autonomous driving and infrastructure management, is best achieved by supervised learning, which demands extensive annotated datasets and faces the problem of domain shifts. We introduce a new 3D semantic segmentation pipeline that leverages aligned scenes and state-of-the-art 2D segmentation methods, avoiding the need for direct 3D annotation or reliance on additional modalities such as camera images at inference time. Our approach generates 2D views from LiDAR scans colored by sensor intensity and applies 2D semantic segmentation to these views using a camera-domain pretrained model. The segmented 2D outputs are then back-projected onto the 3D points, with a simple voting-based estimator that merges the labels associated to each 3D point. Our main contribution is a global pipeline for 3D semantic segmentation requiring no prior 3D annotation and not other modality for inference, which can be used for pseudo-label generation. We conduct a thorough ablation study and demonstrate the potential of the generated pseudo-labels for the Unsupervised Domain Adaptation task.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices</title>
<link>https://arxiv.org/abs/2505.03303</link>
<guid>https://arxiv.org/abs/2505.03303</guid>
<content:encoded><![CDATA[
<div> Evaluation, lightweight deep learning models, image classification, resource-constrained environments, MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, ShuffleNetV2 <br />
<br />
Summary: <br />
This paper evaluates the performance of lightweight deep learning models for image classification in resource-constrained environments. Five state-of-the-art architectures are benchmarked on three datasets, considering accuracy, inference time, FLOPs, and model size. Transfer learning significantly improves model accuracy and efficiency. EfficientNetV2 achieves the highest accuracy, MobileNetV3 balances accuracy and efficiency, and SqueezeNet excels in speed and compactness. Pretrained models outperform scratch-trained ones, especially on complex datasets like Tiny ImageNet. This study addresses critical trade-offs between accuracy and efficiency, providing insights for deploying lightweight models in real-world applications with limited computational resources. The research contributes to optimizing deep learning systems for edge computing and mobile platforms. <br /> <div>
arXiv:2505.03303v1 Announce Type: new 
Abstract: This paper presents a comprehensive evaluation of lightweight deep learning models for image classification, emphasizing their suitability for deployment in resource-constrained environments such as low-memory devices. Five state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using four key performance metrics: classification accuracy, inference time, floating-point operations (FLOPs), and model size. Additionally, we investigate the impact of hyperparameter tuning, data augmentation, and training paradigms by comparing pretrained models with scratch-trained counterparts, focusing on MobileNetV3 Small. Our findings reveal that transfer learning significantly enhances model accuracy and computational efficiency, particularly for complex datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest accuracy, while MobileNetV3 offers the best balance between accuracy and efficiency, and SqueezeNet excels in inference speed and compactness. This study highlights critical trade-offs between accuracy and efficiency, offering actionable insights for deploying lightweight models in real-world applications where computational resources are limited. By addressing these challenges, this research contributes to optimizing deep learning systems for edge computing and mobile platforms.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Gaussian Splatting Data Compression with Mixture of Priors</title>
<link>https://arxiv.org/abs/2505.03310</link>
<guid>https://arxiv.org/abs/2505.03310</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, data compression, Mixture of Priors, conditional entropy modeling, element-wise quantization

Summary: 
The article introduces a novel approach, the Mixture of Priors (MoP), to enhance 3D Gaussian Splatting (3DGS) data compression. The MoP strategy utilizes hyperprior information through multiple lightweight MLPs to generate diverse prior features, integrated via a gating mechanism. For lossless compression, the MoP feature improves conditional entropy modeling. In lossy compression, it guides an element-wise quantization technique known as Coarse-to-Fine Quantization (C2FQ) with a predefined quantization step value. The quantization step matrix is refined adaptively from coarse to fine granularity, guided by the MoP feature. Experimental results show superior performance on various benchmarks like Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&amp;Temples. The proposed framework outperforms existing methods in terms of compression efficiency and effectiveness. <div>
arXiv:2505.03310v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling efficient storage and transmission in 3D scene modeling. However, its development remains limited due to inadequate entropy models and suboptimal quantization strategies for both lossless and lossy compression scenarios, where existing methods have yet to 1) fully leverage hyperprior information to construct robust conditional entropy models, and 2) apply fine-grained, element-wise quantization strategies for improved compression granularity. In this work, we propose a novel Mixture of Priors (MoP) strategy to simultaneously address these two challenges. Specifically, inspired by the Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior information through multiple lightweight MLPs to generate diverse prior features, which are subsequently integrated into the MoP feature via a gating mechanism. To enhance lossless compression, the resulting MoP feature is utilized as a hyperprior to improve conditional entropy modeling. Meanwhile, for lossy compression, we employ the MoP feature as guidance information in an element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine Quantization (C2FQ) strategy with a predefined quantization step value. Specifically, we expand the quantization step value into a matrix and adaptively refine it from coarse to fine granularity, guided by the MoP feature, thereby obtaining a quantization step matrix that facilitates element-wise quantization. Extensive experiments demonstrate that our proposed 3DGS data compression framework achieves state-of-the-art performance across multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&amp;Temples.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.03318</link>
<guid>https://arxiv.org/abs/2505.03318</guid>
<content:encoded><![CDATA[
<div> Keyword: multimodal Reward Models, long chains of thought, UnifiedReward-Think, CoT reasoning, reinforcement fine-tuning<br />
<br />
Summary: 
The paper introduces a novel approach, UnifiedReward-Think, a multimodal CoT-based reward model that incorporates explicit long chains of thought for more reliable reward reasoning in vision tasks. The model leverages exploration-driven reinforcement fine-tuning to develop complex reasoning abilities. It starts by distilling GPT-4o's reasoning process with a small amount of data and then uses large-scale multimodal preference data to refine the model's reasoning across various vision tasks. Correct reasoning outputs are used for refinement, while incorrect outputs are fed into a reinforcement fine-tuning process called Group Relative Policy Optimization (GRPO). Extensive experiments demonstrate the model's superiority in various vision reward tasks. UnifiedReward-Think demonstrates enhanced reliability and robustness in reward signal delivery by incorporating CoT reasoning and implicit reasoning capabilities. <div>
arXiv:2505.03318v1 Announce Type: new 
Abstract: Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SD-VSum: A Method and Dataset for Script-Driven Video Summarization</title>
<link>https://arxiv.org/abs/2505.03319</link>
<guid>https://arxiv.org/abs/2505.03319</guid>
<content:encoded><![CDATA[
<div> Keywords: script-driven video summarization, VideoXum dataset, natural language descriptions, network architecture, cross-modal attention mechanism

Summary: 
In this work, the task of script-driven video summarization is introduced, where a summary of a full-length video is generated based on a user-provided script outlining the visual content desired. The VideoXum dataset is expanded with natural language descriptions of human-annotated summaries to support this task. A new network architecture, SD-VSum, uses a cross-modal attention mechanism to align and fuse information from visual and text modalities for video summarization. Experimental results show that SD-VSum outperforms state-of-the-art methods for query-driven and generic summarization, producing customized video summaries tailored to user needs. <div>
arXiv:2505.03319v1 Announce Type: new 
Abstract: In this work, we introduce the task of script-driven video summarization, which aims to produce a summary of the full-length video by selecting the parts that are most relevant to a user-provided script outlining the visual content of the desired summary. Following, we extend a recently-introduced large-scale dataset for generic video summarization (VideoXum) by producing natural language descriptions of the different human-annotated summaries that are available per video. In this way we make it compatible with the introduced task, since the available triplets of ``video, summary and summary description'' can be used for training a method that is able to produce different summaries for a given video, driven by the provided script about the content of each summary. Finally, we develop a new network architecture for script-driven video summarization (SD-VSum), that relies on the use of a cross-modal attention mechanism for aligning and fusing information from the visual and text modalities. Our experimental evaluations demonstrate the advanced performance of SD-VSum against state-of-the-art approaches for query-driven and generic (unimodal and multimodal) summarization from the literature, and document its capacity to produce video summaries that are adapted to each user's needs about their content.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2505.03327</link>
<guid>https://arxiv.org/abs/2505.03327</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, forest mapping, TanDEM-X, self-supervised learning, high resolution<br />
Summary:<br />
- Deep learning models trained with TanDEM-X SAR data have shown promise in mapping forests at medium resolution.
- High-resolution capabilities of TanDEM-X can be exploited to map forests at 6 m resolution, overcoming limitations of mid-resolution products.
- Self-supervised learning techniques are investigated to extract informative representations from input features and improve forest mapping with limited labels.
- Testing in Pennsylvania, USA, shows the self-supervised framework outperforms fully-supervised methods in forest classification accuracy.
- Application in the Amazon rainforest with limited labeled data at high resolution demonstrates the effectiveness of the proposed self-supervised framework for large-scale forest mapping with TanDEM-X data.<br /> 

Summary: <br />Deep learning models using TanDEM-X SAR data show promise in mapping medium-resolution forests. Exploiting TanDEM-X's high-resolution capabilities enhances mapping at 6m, overcoming mid-resolution limitations. Self-supervised learning improves forest mapping with limited labels. Testing in Pennsylvania reveals superior performance compared to fully-supervised methods. Application in the Amazon showcases the effectiveness of the self-supervised framework for large-scale forest mapping with TanDEM-X data. <div>
arXiv:2505.03327v1 Announce Type: new 
Abstract: Deep learning models have shown encouraging capabilities for mapping accurately forests at medium resolution with TanDEM-X interferometric SAR data. Such models, as most of current state-of-the-art deep learning techniques in remote sensing, are trained in a fully-supervised way, which requires a large amount of labeled data for training and validation. In this work, our aim is to exploit the high-resolution capabilities of the TanDEM-X mission to map forests at 6 m. The goal is to overcome the intrinsic limitations posed by midresolution products, which affect, e.g., the detection of narrow roads within vegetated areas and the precise delineation of forested regions contours. To cope with the lack of extended reliable reference datasets at such a high resolution, we investigate self-supervised learning techniques for extracting highly informative representations from the input features, followed by a supervised training step with a significantly smaller number of reliable labels. A 1 m resolution forest/non-forest reference map over Pennsylvania, USA, allows for comparing different training approaches for the development of an effective forest mapping framework with limited labeled samples. We select the best-performing approach over this test region and apply it in a real-case forest mapping scenario over the Amazon rainforest, where only very few labeled data at high resolution are available. In this challenging scenario, the proposed self-supervised framework significantly enhances the classification accuracy with respect to fully-supervised methods, trained using the same amount of labeled data, representing an extremely promising starting point for large-scale, very high-resolution forest mapping with TanDEM-X data.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing</title>
<link>https://arxiv.org/abs/2505.03329</link>
<guid>https://arxiv.org/abs/2505.03329</guid>
<content:encoded><![CDATA[
<div> Keywords: scene text editing, multilingual, glyph conditioning, text embedding, FLUX-Text

Summary: 
FLUX-Text is a new multilingual scene text editing framework that addresses challenges in accurately editing and adding text to images. The framework is based on FLUX-Fill and focuses on glyph conditioning, taking into account both visual and textual modalities. By introducing lightweight glyph and text embedding modules, FLUX-Text enhances its understanding and generation of glyphs while maintaining generative capabilities. Despite being trained with only 100K examples, FLUX-Text outperforms current popular methods trained with 2.9M examples, achieving state-of-the-art performance in text editing tasks. Qualitative and quantitative experiments on public datasets confirm the superior text fidelity of FLUX-Text compared to previous works. <div>
arXiv:2505.03329v1 Announce Type: new 
Abstract: The task of scene text editing is to modify or add texts on images while maintaining the fidelity of newly generated text and visual coherence with the background. Recent works based on latent diffusion models (LDM) show improved text editing results, yet still face challenges and often generate inaccurate or unrecognizable characters, especially for non-Latin ones (\eg, Chinese), which have complex glyph structures. To address these issues, we present FLUX-Text, a simple and advanced multilingual scene text editing framework based on FLUX-Fill. Specifically, we carefully investigate glyph conditioning, considering both visual and textual modalities. To retain the original generative capabilities of FLUX-Fill while enhancing its understanding and generation of glyphs, we propose lightweight glyph and text embedding modules. Owning to the lightweight design, FLUX-Text is trained only with $100K$ training examples compared to current popular methods trained with 2.9M ones. With no bells and whistles, our method achieves state-of-the-art performance on text editing tasks. Qualitative and quantitative experiments on the public datasets demonstrate that our method surpasses previous works in text fidelity.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection</title>
<link>https://arxiv.org/abs/2505.03334</link>
<guid>https://arxiv.org/abs/2505.03334</guid>
<content:encoded><![CDATA[
<div> Keywords: language-guided aerial object detection, large-scale dataset, open-set detection, vision-language model, annotation pipeline

Summary:
A new large-scale language-guided open-set aerial detection dataset has been proposed to address the limitations of existing datasets. The Multi-instance Open-set Aerial Dataset (MI-OAD) contains 163,023 images and 2 million image-caption pairs, significantly larger than comparable datasets. The dataset includes three levels of language guidance, from words to sentences, enabling fine-grained open-world detection. The OS-W2S Label Engine, an automatic annotation pipeline, integrates image preprocessing with BERT-based postprocessing to provide rich textual annotations for aerial images. State-of-the-art open-set methods from the natural image domain, when trained on MI-OAD, show improved open-set detection capabilities. For instance, Grounding DINO achieves significant improvements in performance metrics under zero-shot transfer conditions. The dataset and label engine will be publicly released to support further research in language-guided aerial object detection. 

<br /><br />Summary: <div>
arXiv:2505.03334v1 Announce Type: new 
Abstract: In recent years, language-guided open-world aerial object detection has gained significant attention due to its better alignment with real-world application needs. However, due to limited datasets, most existing language-guided methods primarily focus on vocabulary, which fails to meet the demands of more fine-grained open-world detection. To address this limitation, we propose constructing a large-scale language-guided open-set aerial detection dataset, encompassing three levels of language guidance: from words to phrases, and ultimately to sentences. Centered around an open-source large vision-language model and integrating image-operation-based preprocessing with BERT-based postprocessing, we present the OS-W2S Label Engine, an automatic annotation pipeline capable of handling diverse scene annotations for aerial images. Using this label engine, we expand existing aerial detection datasets with rich textual annotations and construct a novel benchmark dataset, called Multi-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of current remote sensing grounding data and enabling effective open-set aerial detection. Specifically, MI-OAD contains 163,023 images and 2 million image-caption pairs, approximately 40 times larger than comparable datasets. We also employ state-of-the-art open-set methods from the natural image domain, trained on our proposed dataset, to validate the model's open-set detection capabilities. For instance, when trained on our dataset, Grounding DINO achieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs under zero-shot transfer conditions. Both the dataset and the label engine will be released publicly.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Vision-Language Model for Focal Liver Lesion Classification</title>
<link>https://arxiv.org/abs/2505.03350</link>
<guid>https://arxiv.org/abs/2505.03350</guid>
<content:encoded><![CDATA[
<div> Classification, focal liver lesions, Vision-Language models, Liver-VLM, multimodal learning<br />
<br />
Summary: <br />
Accurate classification of focal liver lesions is essential in hepatology. Traditional supervised deep learning models require large annotated datasets, posing limitations in medical imaging. Liver-VLM, inspired by CLIP, integrates class information into the text encoder without increasing inference overhead. By optimizing with cross-entropy loss and cosine similarities, Liver-VLM aligns image features with text features, outperforming standard CLIP and MedCLIP models in accuracy and AUC on the MPCT-FLLs dataset. Using a lightweight ResNet18 backbone improves classification performance, especially with limited data. Liver-VLM's multimodal approach enhances learning efficiency and effectiveness in focal liver lesion classification. <br /> <div>
arXiv:2505.03350v1 Announce Type: new 
Abstract: Accurate classification of focal liver lesions is crucial for diagnosis and treatment in hepatology. However, traditional supervised deep learning models depend on large-scale annotated datasets, which are often limited in medical imaging. Recently, Vision-Language models (VLMs) such as Contrastive Language-Image Pre-training model (CLIP) has been applied to image classifications. Compared to the conventional convolutional neural network (CNN), which classifiers image based on visual information only, VLM leverages multimodal learning with text and images, allowing it to learn effectively even with a limited amount of labeled data. Inspired by CLIP, we pro-pose a Liver-VLM, a model specifically designed for focal liver lesions (FLLs) classification. First, Liver-VLM incorporates class information into the text encoder without introducing additional inference overhead. Second, by calculating the pairwise cosine similarities between image and text embeddings and optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively aligns image features with class-level text features. Experimental results on MPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the standard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve (AUC). Further analysis shows that using a lightweight ResNet18 backbone enhances classification performance, particularly under data-constrained conditions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUAVA: Generalizable Upper Body 3D Gaussian Avatar</title>
<link>https://arxiv.org/abs/2505.03351</link>
<guid>https://arxiv.org/abs/2505.03351</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D human avatar, facial expression, animatable, Gaussian avatar, real-time animation<br />
Summary:<br />
- The article introduces a new framework called GUAVA for reconstructing high-quality, animatable 3D human avatars from a single image, focusing on facial and hand motions. 
- It addresses the complexity and time-consuming nature of existing methods by introducing an expressive human model (EHM) and accurate tracking techniques. 
- GUAVA is the first framework for fast animatable upper-body 3D Gaussian avatar reconstruction, leveraging inverse texture mapping and projection sampling to infer Ubody Gaussians from a single image. 
- The framework also includes a neural refiner to enhance the rendering quality of the images. 
- Experimental results demonstrate that GUAVA outperforms previous methods in rendering quality, offers significant speed improvements with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.<br /> 
Summary: <div>
arXiv:2505.03351v1 Announce Type: new 
Abstract: Reconstructing a high-quality, animatable 3D human avatar with expressive facial and hand motions from a single image has gained significant attention due to its broad application potential. 3D human avatar reconstruction typically requires multi-view or monocular videos and training on individual IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's expressiveness, these methods often focus on body motion but struggle with facial expressions. To address these challenges, we first introduce an expressive human model (EHM) to enhance facial expression capabilities and develop an accurate tracking method. Based on this template model, we propose GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar reconstruction. We leverage inverse texture mapping and projection sampling techniques to infer Ubody (upper-body) Gaussians from a single image. The rendered images are refined through a neural refiner. Experimental results demonstrate that GUAVA significantly outperforms previous methods in rendering quality and offers significant speed improvements, with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Zero-shot Learning with Infinite Class Concepts</title>
<link>https://arxiv.org/abs/2505.03361</link>
<guid>https://arxiv.org/abs/2505.03361</guid>
<content:encoded><![CDATA[
<div> Zero-shot learning, Large-scale Language Models, Transferability, Discriminability, Infinite Class Concepts
<br />
<br />
Zero-shot learning (ZSL) typically aligns images with class semantics to recognize unseen classes. This study introduces a new framework, InfZSL, that leverages Large-scale Language Models (LLMs) to generate a wide variety of phrase-level class concepts dynamically. The framework addresses challenges with transparency and the hallucination problem in LLMs by introducing an entropy-based scoring process that selects only the most transferable and discriminative concepts. InfZSL shows significant improvements on benchmark datasets and produces interpretable, image-grounded concepts. This novel approach redefines class semantics in ZSL, focusing on transferability and discriminability, offering a promising solution for accurate and interpretable zero-shot learning. 
Summary: <br /><br />Zero-shot learning is redefined with a novel framework, InfZSL, leveraging Large-scale Language Models (LLMs) to generate a wide array of class concepts dynamically. An entropy-based scoring process selects transferable and discriminative concepts, improving classification performance and producing interpretable, image-grounded results. <div>
arXiv:2505.03361v1 Announce Type: new 
Abstract: Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images with intermediate class semantics, like human-annotated concepts or class definitions. An emerging alternative leverages Large-scale Language Models (LLMs) to automatically generate class documents. However, these methods often face challenges with transparency in the classification process and may suffer from the notorious hallucination problem in LLMs, resulting in non-visual class semantics. This paper redefines class semantics in ZSL with a focus on transferability and discriminability, introducing a novel framework called Zero-shot Learning with Infinite Class Concepts (InfZSL). Our approach leverages the powerful capabilities of LLMs to dynamically generate an unlimited array of phrase-level class concepts. To address the hallucination challenge, we introduce an entropy-based scoring process that incorporates a ``goodness" concept selection mechanism, ensuring that only the most transferable and discriminative concepts are selected. Our InfZSL framework not only demonstrates significant improvements on three popular benchmark datasets but also generates highly interpretable, image-grounded concepts. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D Surface Reconstruction with Enhanced High-Frequency Details</title>
<link>https://arxiv.org/abs/2505.03362</link>
<guid>https://arxiv.org/abs/2505.03362</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural implicit 3D reconstruction, high-frequency details, surface reconstruction, FreNeuS, volume rendering

Summary: 
FreNeuS is a new method designed to improve surface detail reconstruction in neural implicit 3D reconstruction without 3D supervision. By utilizing high-frequency information and pixel gradient changes, FreNeuS can accurately capture fine surface details that are often missed by current methods. The method dynamically samples rays based on high-frequency regions in the image, allowing for a more targeted approach to surface reconstruction. Additionally, a high-frequency weighting method is implemented to focus on representing high-frequency details during the reconstruction process. Results demonstrate that FreNeuS outperforms existing methods in terms of surface reconstruction quality and detail preservation. Furthermore, this approach is flexible and can be easily applied to other neural implicit 3D reconstruction techniques. <div>
arXiv:2505.03362v1 Announce Type: new 
Abstract: Neural implicit 3D reconstruction can reproduce shapes without 3D supervision, and it learns the 3D scene through volume rendering methods and neural implicit representations. Current neural surface reconstruction methods tend to randomly sample the entire image, making it difficult to learn high-frequency details on the surface, and thus the reconstruction results tend to be too smooth. We designed a method (FreNeuS) based on high-frequency information to solve the problem of insufficient surface detail. Specifically, FreNeuS uses pixel gradient changes to easily acquire high-frequency regions in an image and uses the obtained high-frequency information to guide surface detail reconstruction. High-frequency information is first used to guide the dynamic sampling of rays, applying different sampling strategies according to variations in high-frequency regions. To further enhance the focus on surface details, we have designed a high-frequency weighting method that constrains the representation of high-frequency details during the reconstruction process. Qualitative and quantitative results show that our method can reconstruct fine surface details and obtain better surface reconstruction quality compared to existing methods. In addition, our method is more applicable and can be generalized to any NeuS-based work.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.03374</link>
<guid>https://arxiv.org/abs/2505.03374</guid>
<content:encoded><![CDATA[
<div> vision language models, discriminative models, physical activity behaviours, wearable devices, computer vision<br />
<br />
Summary:<br />
The article evaluates the performance of vision language models (VLM) and discriminative models (DM) in predicting physical activity behaviors from images captured by wearable cameras. In two validation studies with participants in the UK and China, VLM and DM performed well in identifying sedentary behavior. However, their accuracy decreased for light and moderate-to-vigorous intensity physical activity. While VLM and DM showed promising results in the UK study, their performance dropped when applied to the external study in China. The findings suggest that freely available computer vision models could assist in annotating sedentary behavior from wearable camera images, potentially reducing the burden of manual annotations. <div>
arXiv:2505.03374v1 Announce Type: new 
Abstract: Introduction: Data from wearable devices collected in free-living settings, and labelled with physical activity behaviours compatible with health research, are essential for both validating existing wearable-based measurement approaches and developing novel machine learning approaches. One common way of obtaining these labels relies on laborious annotation of sequences of images captured by cameras worn by participants through the course of a day. Methods: We compare the performance of three vision language models and two discriminative models on two free-living validation studies with 161 and 111 participants, collected in Oxfordshire, United Kingdom and Sichuan, China, respectively, using the Autographer (OMG Life, defunct) wearable camera. Results: We found that the best open-source vision-language model (VLM) and fine-tuned discriminative model (DM) achieved comparable performance when predicting sedentary behaviour from single images on unseen participants in the Oxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86, 0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63, 0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53, 0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study, performance fell across all intensity categories, with median Cohen's kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM, and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely available computer vision models could help annotate sedentary behaviour, typically the most prevalent activity of daily living, from wearable camera images within similar populations to seen data, reducing the annotation burden.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant</title>
<link>https://arxiv.org/abs/2505.03380</link>
<guid>https://arxiv.org/abs/2505.03380</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical AI assistant, multimodal alignment, hierarchical vision-language grounding, self-reinforcing correlation mechanism, cell segmentation

Summary: 
RCMed is a full-stack AI assistant designed to enhance multimodal alignment in medical image analysis. It utilizes a hierarchical vision-language grounding approach to improve anatomical delineation and diagnosis accuracy through a self-reinforcing correlation mechanism. By allowing visual features to inform language context and vice versa, RCMed achieves precise localization and reliable diagnosis through a closed-loop refinement process. The integration of a color region description strategy further enhances the correlation by translating anatomical structures into semantically rich text. Trained on a large dataset of image-mask-description triplets, RCMed demonstrates state-of-the-art performance in contextualizing irregular lesions and subtle anatomical boundaries across multiple clinical tasks and modalities. Additionally, it excels in cell segmentation from microscopy images, achieving a 23.5% relative improvement over previous methods. RCMed's strong vision-language alignment enables exceptional generalization, with superior performance in external validation across various cancer types, showcasing the potential of integrated multimodal models in advancing AI healthcare.<br /><br />Summary: <div>
arXiv:2505.03380v1 Announce Type: new 
Abstract: Medical AI assistants support doctors in disease diagnosis, medical image analysis, and report generation. However, they still face significant challenges in clinical use, including limited accuracy with multimodal content and insufficient validation in real-world settings. We propose RCMed, a full-stack AI assistant that improves multimodal alignment in both input and output, enabling precise anatomical delineation, accurate localization, and reliable diagnosis through hierarchical vision-language grounding. A self-reinforcing correlation mechanism allows visual features to inform language context, while language semantics guide pixel-wise attention, forming a closed loop that refines both modalities. This correlation is enhanced by a color region description strategy, translating anatomical structures into semantically rich text to learn shape-location-text relationships across scales. Trained on 20 million image-mask-description triplets, RCMed achieves state-of-the-art precision in contextualizing irregular lesions and subtle anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It achieved a 23.5% relative improvement in cell segmentation from microscopy images over prior methods. RCMed's strong vision-language alignment enables exceptional generalization, with state-of-the-art performance in external validation across 20 clinically significant cancer types, including novel tasks. This work demonstrates how integrated multimodal models capture fine-grained patterns, enabling human-level interpretation in complex scenarios and advancing human-centric AI healthcare.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples</title>
<link>https://arxiv.org/abs/2505.03383</link>
<guid>https://arxiv.org/abs/2505.03383</guid>
<content:encoded><![CDATA[
<div> transfer-based attack, deep learning models, face recognition, adversarial examples, attention-aggregated attack 

Summary: 
The study focuses on enhancing the transferability of adversarial examples in face recognition models. It explores the unique facial features that contribute to embedding learning in face recognition models, distinguishing between decisive and auxiliary features specific to each model. Drawing inspiration from attention divergence, the proposed method, Attention-aggregated Attack (AAA), aims to disrupt critical facial features by mimicking the attention patterns of other face recognition models. Experimental results demonstrate the effectiveness and robustness of AAA compared to existing methods, highlighting its superiority in attacking fine-grained vision tasks like face recognition. <div>
arXiv:2505.03383v1 Announce Type: new 
Abstract: Adversarial examples have revealed the vulnerability of deep learning models and raised serious concerns about information security. The transfer-based attack is a hot topic in black-box attacks that are practical to real-world scenarios where the training datasets, parameters, and structure of the target model are unknown to the attacker. However, few methods consider the particularity of class-specific deep models for fine-grained vision tasks, such as face recognition (FR), giving rise to unsatisfactory attacking performance. In this work, we first investigate what in a face exactly contributes to the embedding learning of FR models and find that both decisive and auxiliary facial features are specific to each FR model, which is quite different from the biological mechanism of human visual system. Accordingly we then propose a novel attack method named Attention-aggregated Attack (AAA) to enhance the transferability of adversarial examples against FR, which is inspired by the attention divergence and aims to destroy the facial features that are critical for the decision-making of other FR models by imitating their attentions on the clean face images. Extensive experiments conducted on various FR models validate the superiority and robust effectiveness of the proposed method over existing methods.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EOPose : Exemplar-based object reposing using Generalized Pose Correspondences</title>
<link>https://arxiv.org/abs/2505.03394</link>
<guid>https://arxiv.org/abs/2505.03394</guid>
<content:encoded><![CDATA[
<div> reposing, objects, images, e-commerce, framework
Summary:
EOPose is a novel end-to-end framework for generic object reposing in images. Leveraging unsupervised keypoint correspondence detection, the method uses a target pose-guidance image to warp and re-render source object images into the desired pose with fine-grained details preserved. A new dataset of paired objects was prepared for training and testing the network, demonstrating high-quality reposing output based on image quality metrics. The method's efficacy is further supported by detailed ablation studies and user studies. <div>
arXiv:2505.03394v1 Announce Type: new 
Abstract: Reposing objects in images has a myriad of applications, especially for e-commerce where several variants of product images need to be produced quickly. In this work, we leverage the recent advances in unsupervised keypoint correspondence detection between different object images of the same class to propose an end-to-end framework for generic object reposing. Our method, EOPose, takes a target pose-guidance image as input and uses its keypoint correspondence with the source object image to warp and re-render the latter into the target pose using a novel three-step approach. Unlike generative approaches, our method also preserves the fine-grained details of the object such as its exact colors, textures, and brand marks. We also prepare a new dataset of paired objects based on the Objaverse dataset to train and test our network. EOPose produces high-quality reposing output as evidenced by different image quality metrics (PSNR, SSIM and FID). Besides a description of the method and the dataset, the paper also includes detailed ablation and user studies to indicate the efficacy of the proposed method
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation</title>
<link>https://arxiv.org/abs/2505.03401</link>
<guid>https://arxiv.org/abs/2505.03401</guid>
<content:encoded><![CDATA[
<div> Keywords: Radiology Report Generation, Longitudinal Radiology Report Generation, dynamic difference-aware temporal residual network, feature extraction, spatial and temporal correlations.<br />
Summary:
Radiology Report Generation (RRG) automates the creation of radiology reports, while Longitudinal Radiology Report Generation (LRRG) tracks temporal changes in clinical findings. Existing LRRG methods struggle to capture spatial and temporal correlations effectively during feature extraction. To address this, a dynamic difference-aware temporal residual network (DDaTR) was developed. DDaTR includes modules to capture multi-level spatial correlations and align prior features for clinical information integrity. The network also identifies relationships across exams to capture favorable difference information. By unidirectionally transmitting longitudinal information, DDaTR effectively models temporal correlations. Extensive experiments showed superior performance on three benchmarks, demonstrating efficacy in both RRG and LRRG tasks.<br /> <div>
arXiv:2505.03401v1 Announce Type: new 
Abstract: Radiology Report Generation (RRG) automates the creation of radiology reports from medical imaging, enhancing the efficiency of the reporting process. Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating the ability to compare current and prior exams, facilitating the tracking of temporal changes in clinical findings. Existing LRRG approaches only extract features from prior and current images using a visual pre-trained encoder, which are then concatenated to generate the final report. However, these methods struggle to effectively capture both spatial and temporal correlations during the feature extraction process. Consequently, the extracted features inadequately capture the information of difference across exams and thus underrepresent the expected progressions, leading to sub-optimal performance in LRRG. To address this, we develop a novel dynamic difference-aware temporal residual network (DDaTR). In DDaTR, we introduce two modules at each stage of the visual encoder to capture multi-level spatial correlations. The Dynamic Feature Alignment Module (DFAM) is designed to align prior features across modalities for the integrity of prior clinical information. Prompted by the enriched prior features, the dynamic difference-aware module (DDAM) captures favorable difference information by identifying relationships across exams. Furthermore, our DDaTR employs the dynamic residual network to unidirectionally transmit longitudinal information, effectively modelling temporal correlations. Extensive experiments demonstrated superior performance over existing methods on three benchmarks, proving its efficacy in both RRG and LRRG tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.03412</link>
<guid>https://arxiv.org/abs/2505.03412</guid>
<content:encoded><![CDATA[
<div> dataset, anomaly detection, X-ray, internal defects, industrial components  
Summary:  
- The article introduces a new dataset called CXR-AD focusing on X-ray images for internal defect detection in industrial components.  
- The dataset includes 653 normal samples and 561 defect samples with precise annotations.  
- Challenges identified include complex internal structures, low contrast, high noise interference in X-ray imaging, and variations in defect scales and morphologies.  
- Three state-of-the-art anomaly detection frameworks were benchmarked on CXR-AD showing a 29.78% average performance degradation compared to existing datasets.  
- CXR-AD aims to advance algorithm development and improve precision in internal defect inspection technologies.  

<br /><br />Summary: <div>
arXiv:2505.03412v1 Announce Type: new 
Abstract: Internal defect detection constitutes a critical process in ensuring component quality, for which anomaly detection serves as an effective solution. However, existing anomaly detection datasets predominantly focus on surface defects in visible-light images, lacking publicly available X-ray datasets targeting internal defects in components. To address this gap, we construct the first publicly accessible component X-ray anomaly detection (CXR-AD) dataset, comprising real-world X-ray images. The dataset covers five industrial component categories, including 653 normal samples and 561 defect samples with precise pixel-level mask annotations. We systematically analyze the dataset characteristics and identify three major technical challenges: (1) strong coupling between complex internal structures and defect regions, (2) inherent low contrast and high noise interference in X-ray imaging, and (3) significant variations in defect scales and morphologies. To evaluate dataset complexity, we benchmark three state-of-the-art anomaly detection frameworks (feature-based, reconstruction-based, and zero-shot learning methods). Experimental results demonstrate a 29.78% average performance degradation on CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms in handling internal defect detection tasks. To the best of our knowledge, CXR-AD represents the first publicly available X-ray dataset for component anomaly detection, providing a real-world industrial benchmark to advance algorithm development and enhance precision in internal defect inspection technologies.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Target-unspecific Tasks through a Features Matrix</title>
<link>https://arxiv.org/abs/2505.03414</link>
<guid>https://arxiv.org/abs/2505.03414</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt learning, vision-language models, Features Matrix, general knowledge, target-unspecific tasks 

Summary: 
The article discusses the limitations of current prompt optimization methods in large vision-language models when it comes to handling target-unspecific tasks effectively. It proposes a Features Matrix (FM) regularization approach to address this issue by capturing and leveraging general knowledge in the model. The FM extracts diverse input semantics deeply and finely, preserving essential general knowledge to prevent overfitting during training. Results show that the FM can be easily integrated into existing frameworks as a versatile module and significantly enhances performance in target-unspecific tasks, achieving state-of-the-art results. Overall, the FM regularization approach offers a promising solution to improving the generalizability of vision-language models and addressing the challenges faced in handling diverse tasks effectively. 

<br /><br />Summary: <div>
arXiv:2505.03414v1 Announce Type: new 
Abstract: Recent developments in prompt learning of large vision-language models have significantly improved performance in target-specific tasks. However, these prompt optimizing methods often struggle to tackle the target-unspecific or generalizable tasks effectively. It may be attributed to the fact that overfitting training causes the model to forget its general knowledge having strong promotion on target-unspecific tasks. To alleviate this issue, we propose a novel Features Matrix (FM) regularization approach designed to enhance these models on target-unspecific tasks. Our method extracts and leverages general knowledge, shaping a Features Matrix (FM). Specifically, the FM captures the semantics of diverse inputs from a deep and fine perspective, preserving essential general knowledge, which mitigates the risk of overfitting. Representative evaluations demonstrate that: 1) the FM is compatible with existing frameworks as a generic and flexible module, and 2) the FM significantly showcases its effectiveness in enhancing target-unspecific tasks, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiftFeat: 3D Geometry-Aware Local Feature Matching</title>
<link>https://arxiv.org/abs/2505.03422</link>
<guid>https://arxiv.org/abs/2505.03422</guid>
<content:encoded><![CDATA[
arXiv:2505.03422v1 Announce Type: new 
Abstract: Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called \textit{LiftFeat}, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications</title>
<link>https://arxiv.org/abs/2505.03426</link>
<guid>https://arxiv.org/abs/2505.03426</guid>
<content:encoded><![CDATA[
arXiv:2505.03426v1 Announce Type: new 
Abstract: Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for diagnosing heart diseases and evaluating cardiac health. However, the limited availability of large-scale, high-quality CMR datasets poses a major challenge to the effective application of artificial intelligence (AI) in this domain. Even the amount of unlabeled data and the health status it covers are difficult to meet the needs of model pretraining, which hinders the performance of AI models on downstream tasks. In this study, we present Cardiac Phenotype-Guided CMR Generation (CPGG), a novel approach for generating diverse CMR data that covers a wide spectrum of cardiac health status. The CPGG framework consists of two stages: in the first stage, a generative model is trained using cardiac phenotypes derived from CMR data; in the second stage, a masked autoregressive diffusion model, conditioned on these phenotypes, generates high-fidelity CMR cine sequences that capture both structural and functional features of the heart in a fine-grained manner. We synthesized a massive amount of CMR to expand the pretraining data. Experimental results show that CPGG generates high-quality synthetic CMR data, significantly improving performance on various downstream tasks, including diagnosis and cardiac phenotypes prediction. These gains are demonstrated across both public and private datasets, highlighting the effectiveness of our approach. Code is availabel at https://anonymous.4open.science/r/CPGG.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution</title>
<link>https://arxiv.org/abs/2505.03431</link>
<guid>https://arxiv.org/abs/2505.03431</guid>
<content:encoded><![CDATA[
arXiv:2505.03431v1 Announce Type: new 
Abstract: The fusion of low-spatial-resolution hyperspectral images (HSIs) with high-spatial-resolution conventional images (e.g., panchromatic or RGB) has played a significant role in recent advancements in HSI super-resolution. However, this fusion process relies on the availability of precise alignment between image pairs, which is often challenging in real-world scenarios. To mitigate this limitation, we propose a single-image super-resolution model called the Fusion-Guided Inception Network (FGIN). Specifically, we first employ a spectral-spatial fusion module to effectively integrate spectral and spatial information at an early stage. Next, an Inception-like hierarchical feature extraction strategy is used to capture multiscale spatial dependencies, followed by a dedicated multi-scale fusion block. To further enhance reconstruction quality, we incorporate an optimized upsampling module that combines bilinear interpolation with depthwise separable convolutions. Experimental evaluations on two publicly available hyperspectral datasets demonstrate the competitive performance of our method.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks</title>
<link>https://arxiv.org/abs/2505.03435</link>
<guid>https://arxiv.org/abs/2505.03435</guid>
<content:encoded><![CDATA[
arXiv:2505.03435v1 Announce Type: new 
Abstract: The rapid advancement of generative image technology has introduced significant security concerns, particularly in the domain of face generation detection. This paper investigates the vulnerabilities of current AI-generated face detection systems. Our study reveals that while existing detection methods often achieve high accuracy under standard conditions, they exhibit limited robustness against adversarial attacks. To address these challenges, we propose an approach that integrates adversarial training to mitigate the impact of adversarial examples. Furthermore, we utilize diffusion inversion and reconstruction to further enhance detection robustness. Experimental results demonstrate that minor adversarial perturbations can easily bypass existing detection systems, but our method significantly improves the robustness of these systems. Additionally, we provide an in-depth analysis of adversarial and benign examples, offering insights into the intrinsic characteristics of AI-generated content. All associated code will be made publicly available in a dedicated repository to facilitate further research and verification.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polar Coordinate-Based 2D Pose Prior with Neural Distance Field</title>
<link>https://arxiv.org/abs/2505.03445</link>
<guid>https://arxiv.org/abs/2505.03445</guid>
<content:encoded><![CDATA[
arXiv:2505.03445v1 Announce Type: new 
Abstract: Human pose capture is essential for sports analysis, enabling precise evaluation of athletes' movements. While deep learning-based human pose estimation (HPE) models from RGB videos have achieved impressive performance on public datasets, their effectiveness in real-world sports scenarios is often hindered by motion blur, occlusions, and domain shifts across different pose representations. Fine-tuning these models can partially alleviate such challenges but typically requires large-scale annotated data and still struggles to generalize across diverse sports environments. To address these limitations, we propose a 2D pose prior-guided refinement approach based on Neural Distance Fields (NDF). Unlike existing approaches that rely solely on angular representations of human poses, we introduce a polar coordinate-based representation that explicitly incorporates joint connection lengths, enabling a more accurate correction of erroneous pose estimations. Additionally, we define a novel non-geodesic distance metric that separates angular and radial discrepancies, which we demonstrate is better suited for polar representations than traditional geodesic distances. To mitigate data scarcity, we develop a gradient-based batch-projection augmentation strategy, which synthesizes realistic pose samples through iterative refinement. Our method is evaluated on a long jump dataset, demonstrating its ability to improve 2D pose estimation across multiple pose representations, making it robust across different domains. Experimental results show that our approach enhances pose plausibility while requiring only limited training data. Code is available at: https://github.com/QGAN2019/polar-NDF.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD)</title>
<link>https://arxiv.org/abs/2505.03463</link>
<guid>https://arxiv.org/abs/2505.03463</guid>
<content:encoded><![CDATA[
arXiv:2505.03463v1 Announce Type: new 
Abstract: Dynamic computed tomography (CT) reconstruction faces significant challenges in addressing motion artifacts, particularly for nonperiodic rapid movements such as cardiac imaging with fast heart rates. Traditional methods struggle with the extreme limited-angle problems inherent in nonperiodic cases. Deep learning methods have improved performance but face generalization challenges. Recent implicit neural representation (INR) techniques show promise through self-supervised deep learning, but have critical limitations: computational inefficiency due to forward-warping modeling, difficulty balancing DVF complexity with anatomical plausibility, and challenges in preserving fine details without additional patient-specific pre-scans. This paper presents a novel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It addresses these challenges through four key contributions: (1) backward-warping deformation that enables direct computation of each dynamic voxel with significantly reduced computational cost, (2) diffeomorphism-based DVF regularization that ensures anatomically plausible deformations while maintaining representational capacity, (3) motion-compensated analytical reconstruction that enhances fine details without requiring additional pre-scans, and (4) dimensional-reduction design for efficient 4D coordinate encoding. Through various simulations and practical studies, including digital and physical phantoms and retrospective patient data, we demonstrate the effectiveness of our approach for nonperiodic dynamic CT reconstruction with enhanced details and reduced motion artifacts. The proposed framework enables more accurate dynamic CT reconstruction with potential clinical applications, such as one-beat cardiac reconstruction, cinematic image sequences for functional imaging, and motion artifact reduction in conventional CT scans.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Blending 3D Geometry and Machine Learning for Multi-View Stereopsis</title>
<link>https://arxiv.org/abs/2505.03470</link>
<guid>https://arxiv.org/abs/2505.03470</guid>
<content:encoded><![CDATA[
arXiv:2505.03470v1 Announce Type: new 
Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric and geometric consistency constraints. In contrast, modern learning-based algorithms often rely on the plane sweep algorithm to infer 3D geometry, applying explicit geometric consistency (GC) checks only as a post-processing step, with no impact on the learning process itself. In this work, we introduce GC MVSNet plus plus, a novel approach that actively enforces geometric consistency of reference view depth maps across multiple source views (multi view) and at various scales (multi scale) during the learning phase (see Fig. 1). This integrated GC check significantly accelerates the learning process by directly penalizing geometrically inconsistent pixels, effectively halving the number of training iterations compared to other MVS methods. Furthermore, we introduce a densely connected cost regularization network with two distinct block designs simple and feature dense optimized to harness dense feature connections for enhanced regularization. Extensive experiments demonstrate that our approach achieves a new state of the art on the DTU and BlendedMVS datasets and secures second place on the Tanks and Temples benchmark. To our knowledge, GC MVSNet plus plus is the first method to enforce multi-view, multi-scale supervised geometric consistency during learning. Our code is available.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion</title>
<link>https://arxiv.org/abs/2505.03494</link>
<guid>https://arxiv.org/abs/2505.03494</guid>
<content:encoded><![CDATA[
arXiv:2505.03494v1 Announce Type: new 
Abstract: Background: Brain tumor segmentation has a significant impact on the diagnosis and treatment of brain tumors. Accurate brain tumor segmentation remains challenging due to their irregular shapes, vague boundaries, and high variability. Objective: We propose a brain tumor segmentation method that combines deep learning with prior knowledge derived from a region-growing algorithm. Methods: The proposed method utilizes a multi-scale feature fusion (MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale features and capture global contextual information. To enhance the model's robustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout) strategy is employed for uncertainty estimation. Results: Extensive experiments demonstrate that the proposed method achieves superior performance on Brain Tumor Segmentation (BraTS) datasets, significantly outperforming various state-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are 89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT) segmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019 validation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for ET, WT, and TC segmentation, respectively. Ablation studies further confirmed the contribution of each module to segmentation accuracy, indicating that each component played a vital role in overall performance improvement. Conclusion: This study proposed a novel 3D brain tumor segmentation network based on the U-Net architecture. By incorporating the prior knowledge and employing the uncertainty estimation method, the robustness and performance were improved. The code for the proposed method is available at https://github.com/chenzhao2023/UPMAD_Net_BrainSeg.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRI motion correction via efficient residual-guided denoising diffusion probabilistic models</title>
<link>https://arxiv.org/abs/2505.03498</link>
<guid>https://arxiv.org/abs/2505.03498</guid>
<content:encoded><![CDATA[
arXiv:2505.03498v1 Announce Type: new 
Abstract: Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly degrade image quality and impair quantitative analysis. Conventional mitigation strategies, such as repeated acquisitions or motion tracking, are costly and workflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising diffusion probabilistic model tailored for MRI motion artifact correction. Methods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in the forward diffusion process, aligning the noise distribution with motion-corrupted data and enabling an efficient four-step reverse diffusion. A U-net backbone enhanced with Swin-Transformer blocks conventional attention layers, improving adaptability across resolutions. Training employs a combined l1+l2 loss, which promotes image sharpness and reduces pixel-level errors. Res-MoCoDiff was evaluated on synthetic dataset generated using a realistic motion simulation framework and on an in-vivo dataset. Comparative analyses were conducted against established methods, including CycleGAN, Pix2pix, and MT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and normalized mean squared error (NMSE). Results: The proposed method demonstrated superior performance in removing motion artifacts across all motion severity levels. Res-MoCoDiff consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling time was reduced to 0.37 seconds per batch of two image slices, compared with 101.74 seconds for conventional approaches.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking</title>
<link>https://arxiv.org/abs/2505.03507</link>
<guid>https://arxiv.org/abs/2505.03507</guid>
<content:encoded><![CDATA[
arXiv:2505.03507v1 Announce Type: new 
Abstract: To reduce the reliance on large-scale annotations, self-supervised RGB-T tracking approaches have garnered significant attention. However, the omission of the object region by erroneous pseudo-label or the introduction of background noise affects the efficiency of modality fusion, while pseudo-label noise triggered by similar object noise can further affect the tracking performance. In this paper, we propose GDSTrack, a novel approach that introduces dynamic graph fusion and temporal diffusion to address the above challenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the modalities of neighboring frames, treats them as distractor noise, and leverages the denoising capability of a generative model. Specifically, by constructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the proposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic adjacency matrix to guide graph attention, focusing on and fusing the object's coherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features from neighboring frames as interference, and thus improving robustness against similar-object noise. Extensive experiments conducted on four public RGB-T tracking datasets demonstrate that GDSTrack outperforms the existing state-of-the-art methods. The source code is available at https://github.com/LiShenglana/GDSTrack.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks</title>
<link>https://arxiv.org/abs/2505.03522</link>
<guid>https://arxiv.org/abs/2505.03522</guid>
<content:encoded><![CDATA[
arXiv:2505.03522v1 Announce Type: new 
Abstract: Deep learning has substantially advanced the Single Image Super-Resolution (SISR). However, existing researches have predominantly focused on raw performance gains, with little attention paid to quantifying the transferability of architectural components. In this paper, we introduce the concept of "Universality" and its associated definitions which extend the traditional notion of "Generalization" to encompass the modules' ease of transferability, thus revealing the relationships between module universality and model generalizability. Then we propose the Universality Assessment Equation (UAE), a metric for quantifying how readily a given module could be transplanted across models. Guided by the UAE results of standard residual blocks and other plug-and-play modules, we further design two optimized modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB). Through comprehensive experiments on natural-scene benchmarks, remote-sensing datasets, extreme-industrial imagery and on-device deployments, we demonstrate that networks embedded with the proposed plug-and-play modules outperform several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or enabling a 71.3% reduction in parameters with negligible loss in reconstruction fidelity.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication</title>
<link>https://arxiv.org/abs/2505.03528</link>
<guid>https://arxiv.org/abs/2505.03528</guid>
<content:encoded><![CDATA[
arXiv:2505.03528v1 Announce Type: new 
Abstract: Cooperative perception, leveraging shared information from multiple vehicles via vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous driving to alleviate the limitation of single-vehicle perception. Existing works have explored the effects of V2V communication impairments on perception precision, but they lack generalization to different levels of impairments. In this work, we propose a joint weighting and denoising framework, Coop-WD, to enhance cooperative perception subject to V2V channel impairments. In this framework, the self-supervised contrastive model and the conditional diffusion probabilistic model are adopted hierarchically for vehicle-level and pixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is proposed to selectively deactivate denoising to reduce processing overhead. Rician fading, non-stationarity, and time-varying distortion are considered. Simulation results demonstrate that the proposed Coop-WD outperforms conventional benchmarks in all types of channels. Qualitative analysis with visual examples further proves the superiority of our proposed method. The proposed Coop-WD-eco achieves up to 50% reduction in computational cost under severe distortion while maintaining comparable accuracy as channel conditions improve.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT</title>
<link>https://arxiv.org/abs/2505.03538</link>
<guid>https://arxiv.org/abs/2505.03538</guid>
<content:encoded><![CDATA[
arXiv:2505.03538v1 Announce Type: new 
Abstract: Semi-supervised learning has become a compelling approach for 3D tooth segmentation from CBCT scans, where labeled data is minimal. However, existing methods still face two persistent challenges: limited corrective supervision in structurally ambiguous or mislabeled regions during supervised training and performance degradation caused by unreliable pseudo-labels on unlabeled data. To address these problems, we propose Region-Aware Instructive Learning (RAIL), a dual-group dual-student, semi-supervised framework. Each group contains two student models guided by a shared teacher network. By alternating training between the two groups, RAIL promotes intergroup knowledge transfer and collaborative region-aware instruction while reducing overfitting to the characteristics of any single model. Specifically, RAIL introduces two instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller improves supervised learning by instructing predictions only within areas where student outputs diverge from both ground truth and the best student, thereby concentrating supervision on structurally ambiguous or mislabeled areas. In the unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces agreement in regions with high model certainty while reducing the effect of low-confidence predictions during training. This helps prevent our model from learning unstable patterns and improves the overall reliability of pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets show that RAIL surpasses state-of-the-art methods under limited annotation. Our code will be available at https://github.com/Tournesol-Saturday/RAIL.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Panoramic Out-of-Distribution Segmentation</title>
<link>https://arxiv.org/abs/2505.03539</link>
<guid>https://arxiv.org/abs/2505.03539</guid>
<content:encoded><![CDATA[
arXiv:2505.03539v1 Announce Type: new 
Abstract: Panoramic imaging enables capturing 360{\deg} images with an ultra-wide Field-of-View (FoV) for dense omnidirectional perception. However, current panoramic semantic segmentation methods fail to identify outliers, and pinhole Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the panoramic domain due to background clutter and pixel distortions. To address these issues, we introduce a new task, Panoramic Out-of-distribution Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the first solution, POS, which adapts to the characteristics of panoramic images through text-guided prompt distribution learning. Specifically, POS integrates a disentanglement strategy designed to materialize the cross-domain generalization capability of CLIP. The proposed Prompt-based Restoration Attention (PRA) optimizes semantic decoding by prompt guidance and self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL) refines the manifold of per-pixel mask embeddings via semantic prototype supervision. Besides, to compensate for the scarcity of PanOoS datasets, we establish two benchmarks: DenseOoS, which features diverse outliers in complex environments, and QuadOoS, captured by a quadruped robot with a panoramic annular lens system. Extensive experiments demonstrate superior performance of POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS, outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves leading closed-set segmentation capabilities. Code and datasets will be available at https://github.com/MengfeiD/PanOoS.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment</title>
<link>https://arxiv.org/abs/2505.03554</link>
<guid>https://arxiv.org/abs/2505.03554</guid>
<content:encoded><![CDATA[
arXiv:2505.03554v1 Announce Type: new 
Abstract: The Equine Facial Action Coding System (EquiFACS) enables the systematic annotation of facial movements through distinct Action Units (AUs). It serves as a crucial tool for assessing affective states in horses by identifying subtle facial expressions associated with discomfort. However, the field of horse affective state assessment is constrained by the scarcity of annotated data, as manually labelling facial AUs is both time-consuming and costly. To address this challenge, automated annotation systems are essential for leveraging existing datasets and improving affective states detection tools. In this work, we study different methods for specific ear AU detection and localization from horse videos. We leverage past works on deep learning-based video feature extraction combined with recurrent neural networks for the video classification task, as well as a classic optical flow based approach. We achieve 87.5% classification accuracy of ear movement presence on a public horse video dataset, demonstrating the potential of our approach. We discuss future directions to develop these systems, with the aim of bridging the gap between automated AU detection and practical applications in equine welfare and veterinary diagnostics. Our code will be made publicly available at https://github.com/jmalves5/read-my-ears.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID</title>
<link>https://arxiv.org/abs/2505.03557</link>
<guid>https://arxiv.org/abs/2505.03557</guid>
<content:encoded><![CDATA[
arXiv:2505.03557v1 Announce Type: new 
Abstract: The personalization of Stable Diffusion for generating professional portraits from amateur photographs is a burgeoning area, with applications in various downstream contexts. This paper investigates the impact of augmentations on improving facial resemblance when using two prominent personalization techniques: DreamBooth and InstantID. Through a series of experiments with diverse subject datasets, we assessed the effectiveness of various augmentation strategies on the generated headshots' fidelity to the original subject. We introduce FaceDistance, a wrapper around FaceNet, to rank the generations based on facial similarity, which aided in our assessment. Ultimately, this research provides insights into the role of augmentations in enhancing facial resemblance in SDXL-generated portraits, informing strategies for their effective deployment in downstream applications.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Person Image Synthesis Using a Flow Matching Model</title>
<link>https://arxiv.org/abs/2505.03562</link>
<guid>https://arxiv.org/abs/2505.03562</guid>
<content:encoded><![CDATA[
arXiv:2505.03562v1 Announce Type: new 
Abstract: Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images conditioned on a target pose and a source image. This task plays a key role in various real-world applications, such as sign language video generation, AR/VR, gaming, and live streaming. In these scenarios, real-time PGPIS is critical for providing immediate visual feedback and maintaining user immersion.However, achieving real-time performance remains a significant challenge due to the complexity of synthesizing high-fidelity images from diverse and dynamic human poses. Recent diffusion-based methods have shown impressive image quality in PGPIS, but their slow sampling speeds hinder deployment in time-sensitive applications. This latency is particularly problematic in tasks like generating sign language videos during live broadcasts, where rapid image updates are required. Therefore, developing a fast and reliable PGPIS model is a crucial step toward enabling real-time interactive systems. To address this challenge, we propose a generative model based on flow matching (FM). Our approach enables faster, more stable, and more efficient training and sampling. Furthermore, the proposed model supports conditional generation and can operate in latent space, making it especially suitable for real-time PGPIS applications where both speed and quality are critical. We evaluate our proposed method, Real-Time Person Image Synthesis Using a Flow Matching Model (RPFM), on the widely used DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves near-real-time sampling speeds while maintaining performance comparable to the state-of-the-art models. Our methodology trades off a slight, acceptable decrease in generated-image accuracy for over a twofold increase in generation speed, thereby ensuring real-time performance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images</title>
<link>https://arxiv.org/abs/2505.03567</link>
<guid>https://arxiv.org/abs/2505.03567</guid>
<content:encoded><![CDATA[
arXiv:2505.03567v1 Announce Type: new 
Abstract: Text-based pedestrian search (TBPS) in full images aims to locate a target pedestrian in untrimmed images using natural language descriptions. However, in complex scenes with multiple pedestrians, existing methods are limited by uncertainties in detection and matching, leading to degraded performance. To address this, we propose UPD-TBPS, a novel framework comprising three modules: Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts multi-granularity queries to identify potential targets and assigns confidence scores to reduce early-stage uncertainty. PUD leverages visual context decoupling and prototype mining to extract features of the target pedestrian described in the query. It separates and learns pedestrian prototype representations at both the coarse-grained cluster level and the fine-grained individual level, thereby reducing matching uncertainty. ReID evaluates candidates with varying confidence levels, improving detection and retrieval accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the effectiveness of our framework.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models</title>
<link>https://arxiv.org/abs/2505.03569</link>
<guid>https://arxiv.org/abs/2505.03569</guid>
<content:encoded><![CDATA[
arXiv:2505.03569v1 Announce Type: new 
Abstract: Backgrounds in images play a major role in contributing to spurious correlations among different data points. Owing to aesthetic preferences of humans capturing the images, datasets can exhibit positional (location of the object within a given frame) and size (region-of-interest to image ratio) biases for different classes. In this paper, we show that these biases can impact how much a model relies on spurious features in the background to make its predictions. To better illustrate our findings, we propose a synthetic dataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images with various backgrounds, object positions, and object sizes. By evaluating the dataset on different pretrained models, we find that most models rely heavily on spurious features in the background when the region-of-interest (ROI) to image ratio is small and the object is far from the center of the image. Moreover, we also show that current methods that aim to mitigate harmful spurious features, do not take into account these factors, hence fail to achieve considerable performance gains for worst-group accuracies when the size and location of core features in an image change.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning</title>
<link>https://arxiv.org/abs/2505.03575</link>
<guid>https://arxiv.org/abs/2505.03575</guid>
<content:encoded><![CDATA[
arXiv:2505.03575v1 Announce Type: new 
Abstract: Recycling textile fibers is critical to reducing the environmental impact of the textile industry. Hyperspectral near-infrared (NIR) imaging combined with advanced deep learning algorithms offers a promising solution for efficient fiber classification and sorting. In this study, we investigate supervised and unsupervised deep learning models and test their generalization capabilities on different textile structures. We show that optimized convolutional neural networks (CNNs) and autoencoder networks achieve robust generalization under varying conditions. These results highlight the potential of hyperspectral imaging and deep learning to advance sustainable textile recycling through accurate and robust classification.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes</title>
<link>https://arxiv.org/abs/2505.03581</link>
<guid>https://arxiv.org/abs/2505.03581</guid>
<content:encoded><![CDATA[
arXiv:2505.03581v1 Announce Type: new 
Abstract: The analysis of events in dynamic environments poses a fundamental challenge in the development of intelligent agents and robots capable of interacting with humans. Current approaches predominantly utilize visual models. However, these methods often capture information implicitly from images, lacking interpretable spatial-temporal object representations. To address this issue we introduce DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates compressed spatial-temporal structural observation representation with the cognitive capabilities of large language models. The purpose of this integration is to enable advanced question answering based on a sequence of textual scene graphs. Extended evaluations on the STAR and AGQA datasets indicate that DyGEnc outperforms existing visual methods by a large margin of 15-25% in addressing queries regarding the history of human-to-object interactions. Furthermore, the proposed method can be seamlessly extended to process raw input images utilizing foundational models for extracting explicit textual scene graphs, as substantiated by the results of a robotic experiment conducted with a wheeled manipulator platform. We hope that these findings will contribute to the implementation of robust and compressed graph-based robotic memory for long-horizon reasoning. Code is available at github.com/linukc/DyGEnc.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fixed-Length Dense Fingerprint Representation</title>
<link>https://arxiv.org/abs/2505.03597</link>
<guid>https://arxiv.org/abs/2505.03597</guid>
<content:encoded><![CDATA[
arXiv:2505.03597v1 Announce Type: new 
Abstract: Fixed-length fingerprint representations, which map each fingerprint to a compact and fixed-size feature vector, are computationally efficient and well-suited for large-scale matching. However, designing a robust representation that effectively handles diverse fingerprint modalities, pose variations, and noise interference remains a significant challenge. In this work, we propose a fixed-length dense descriptor of fingerprints, and introduce FLARE-a fingerprint matching framework that integrates the Fixed-Length dense descriptor with pose-based Alignment and Robust Enhancement. This fixed-length representation employs a three-dimensional dense descriptor to effectively capture spatial relationships among fingerprint ridge structures, enabling robust and locally discriminative representations. To ensure consistency within this dense feature space, FLARE incorporates pose-based alignment using complementary estimation methods, along with dual enhancement strategies that refine ridge clarity while preserving the original fingerprint modality. The proposed dense descriptor supports fixed-length representation while maintaining spatial correspondence, enabling fast and accurate similarity computation. Extensive experiments demonstrate that FLARE achieves superior performance across rolled, plain, latent, and contactless fingerprints, significantly outperforming existing methods in cross-modality and low-quality scenarios. Further analysis validates the effectiveness of the dense descriptor design, as well as the impact of alignment and enhancement modules on the accuracy of dense descriptor matching. Experimental results highlight the effectiveness and generalizability of FLARE as a unified and scalable solution for robust fingerprint representation and matching. The implementation and code will be publicly available at https://github.com/Yu-Yy/FLARE.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2505.03599</link>
<guid>https://arxiv.org/abs/2505.03599</guid>
<content:encoded><![CDATA[
arXiv:2505.03599v1 Announce Type: new 
Abstract: Deep learning-based medical image-to-mesh reconstruction has rapidly evolved, enabling the transformation of medical imaging data into three-dimensional mesh models that are critical in computational medicine and in silico trials for advancing our understanding of disease mechanisms, and diagnostic and therapeutic techniques in modern medicine. This survey systematically categorizes existing approaches into four main categories: template models, statistical models, generative models, and implicit models. Each category is analysed in detail, examining their methodological foundations, strengths, limitations, and applicability to different anatomical structures and imaging modalities. We provide an extensive evaluation of these methods across various anatomical applications, from cardiac imaging to neurological studies, supported by quantitative comparisons using standard metrics. Additionally, we compile and analyze major public datasets available for medical mesh reconstruction tasks and discuss commonly used evaluation metrics and loss functions. The survey identifies current challenges in the field, including requirements for topological correctness, geometric accuracy, and multi-modality integration. Finally, we present promising future research directions in this domain. This systematic review aims to serve as a comprehensive reference for researchers and practitioners in medical image analysis and computational medicine.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model</title>
<link>https://arxiv.org/abs/2505.03603</link>
<guid>https://arxiv.org/abs/2505.03603</guid>
<content:encoded><![CDATA[
arXiv:2505.03603v1 Announce Type: new 
Abstract: Audio-driven human animation technology is widely used in human-computer interaction, and the emergence of diffusion models has further advanced its development. Currently, most methods rely on multi-stage generation and intermediate representations, resulting in long inference time and issues with generation quality in specific foreground regions and audio-motion consistency. These shortcomings are primarily due to the lack of localized fine-grained supervised guidance. To address above challenges, we propose PAHA, an end-to-end audio-driven upper-body human animation framework with diffusion model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss weights based on pose confidence scores, effectively improving visual quality. PCE constructs and trains diffusion-based regional audio-visual classifiers to improve the consistency of motion and co-speech audio. Afterwards, we design two novel inference guidance methods for the foregoing classifiers, Sequential Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality respectively. Additionally, we build CNAS, the first public Chinese News Anchor Speech dataset, to advance research and validation in this field. Extensive experimental results and user studies demonstrate that PAHA significantly outperforms existing methods in audio-motion alignment and video-related evaluations. The codes and CNAS dataset will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection</title>
<link>https://arxiv.org/abs/2505.03610</link>
<guid>https://arxiv.org/abs/2505.03610</guid>
<content:encoded><![CDATA[
arXiv:2505.03610v1 Announce Type: new 
Abstract: 3D mask presentation attack detection is crucial for protecting face recognition systems against the rising threat of 3D mask attacks. While most existing methods utilize multimodal features or remote photoplethysmography (rPPG) signals to distinguish between real faces and 3D masks, they face significant challenges, such as the high costs associated with multimodal sensors and limited generalization ability. Detection-related text descriptions offer concise, universal information and are cost-effective to obtain. However, the potential of vision-language multimodal features for 3D mask presentation attack detection remains unexplored. In this paper, we propose a novel knowledge-based prompt learning framework to explore the strong generalization capability of vision-language models for 3D mask presentation attack detection. Specifically, our approach incorporates entities and triples from knowledge graphs into the prompt learning process, generating fine-grained, task-specific explicit prompts that effectively harness the knowledge embedded in pre-trained vision-language models. Furthermore, considering different input images may emphasize distinct knowledge graph elements, we introduce a visual-specific knowledge filter based on an attention mechanism to refine relevant elements according to the visual context. Additionally, we leverage causal graph theory insights into the prompt learning process to further enhance the generalization ability of our method. During training, a spurious correlation elimination paradigm is employed, which removes category-irrelevant local image patches using guidance from knowledge-based text features, fostering the learning of generalized causal prompts that align with category-relevant local patches. Experimental results demonstrate that the proposed method achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images</title>
<link>https://arxiv.org/abs/2505.03611</link>
<guid>https://arxiv.org/abs/2505.03611</guid>
<content:encoded><![CDATA[
arXiv:2505.03611v1 Announce Type: new 
Abstract: Face anti-spoofing is a critical technology for ensuring the security of face recognition systems. However, its ability to generalize across diverse scenarios remains a significant challenge. In this paper, we attribute the limited generalization ability to two key factors: covariate shift, which arises from external data collection variations, and semantic shift, which results from substantial differences in emerging attack types. To address both challenges, we propose a novel approach for learning unknown spoof prompts, relying solely on real face images from a single source domain. Our method generates textual prompts for real faces and potential unknown spoof attacks by leveraging the general knowledge embedded in vision-language models, thereby enhancing the model's ability to generalize to unseen target domains. Specifically, we introduce a diverse spoof prompt optimization framework to learn effective prompts. This framework constrains unknown spoof prompts within a relaxed prior knowledge space while maximizing their distance from real face images. Moreover, it enforces semantic independence among different spoof prompts to capture a broad range of spoof patterns. Experimental results on nine datasets demonstrate that the learned prompts effectively transfer the knowledge of vision-language models, enabling state-of-the-art generalization ability against diverse unknown attack types across unseen target domains without using any spoof face images.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing</title>
<link>https://arxiv.org/abs/2505.03621</link>
<guid>https://arxiv.org/abs/2505.03621</guid>
<content:encoded><![CDATA[
arXiv:2505.03621v1 Announce Type: new 
Abstract: Remote photoplethysmography (rPPG) enables non-contact physiological measurement but remains highly susceptible to illumination changes, motion artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at capturing long-range dependencies, offering a potential solution but struggle with the continuous, noise-sensitive nature of rPPG signals due to their text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative optimization framework that synergizes LLMs with domain-specific rPPG components. Specifically, the Text Prototype Guidance (TPG) strategy is proposed to establish cross-modal alignment by projecting hemodynamic features into LLM-interpretable semantic space, effectively bridging the representational gap between physiological signals and linguistic tokens. Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for resolving signal instability through adaptive time-frequency domain feature re-weighting. Finally, rPPG task-specific cues systematically inject physiological priors through physiological statistics, environmental contextual answering, and task description, leveraging cross-modal learning to integrate both visual and textual information, enabling dynamic adaptation to challenging scenarios like variable illumination and subject movements. Evaluation on four benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness, demonstrating superior generalization across lighting variations and motion scenarios.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map</title>
<link>https://arxiv.org/abs/2505.03623</link>
<guid>https://arxiv.org/abs/2505.03623</guid>
<content:encoded><![CDATA[
arXiv:2505.03623v1 Announce Type: new 
Abstract: Synthetic dataset generation in Computer Vision, particularly for industrial applications, is still underexplored. Industrial defect segmentation, for instance, requires highly accurate labels, yet acquiring such data is costly and time-consuming. To address this challenge, we propose a novel diffusion-based pipeline for generating high-fidelity industrial datasets with minimal supervision. Our approach conditions the diffusion model on enriched bounding box representations to produce precise segmentation masks, ensuring realistic and accurately localized defect synthesis. Compared to existing layout-conditioned generative methods, our approach improves defect consistency and spatial accuracy. We introduce two quantitative metrics to evaluate the effectiveness of our method and assess its impact on a downstream segmentation task trained on real and synthetic data. Our results demonstrate that diffusion-based synthesis can bridge the gap between artificial and real-world industrial data, fostering more reliable and cost-efficient segmentation models. The code is publicly available at https://github.com/covisionlab/diffusion_labeling.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision</title>
<link>https://arxiv.org/abs/2505.03631</link>
<guid>https://arxiv.org/abs/2505.03631</guid>
<content:encoded><![CDATA[
arXiv:2505.03631v1 Announce Type: new 
Abstract: Video quality assessment (VQA) is essential for quantifying perceptual quality in various video processing workflows, spanning from camera capture systems to over-the-top streaming platforms. While recent supervised VQA models have made substantial progress, the reliance on manually annotated datasets -- a process that is labor-intensive, costly, and difficult to scale up -- has hindered further optimization of their generalization to unseen video content and distortions. To bridge this gap, we introduce a self-supervised learning framework for VQA to learn quality assessment capabilities from large-scale, unlabeled web videos. Our approach leverages a \textbf{learning-to-rank} paradigm to train a large multimodal model (LMM) on video pairs automatically labeled via two manners, including quality pseudo-labeling by existing VQA models and relative quality ranking based on synthetic distortion simulations. Furthermore, we introduce a novel \textbf{iterative self-improvement training strategy}, where the trained model acts an improved annotator to iteratively refine the annotation quality of training data. By training on a dataset $10\times$ larger than the existing VQA benchmarks, our model: (1) achieves zero-shot performance on in-domain VQA benchmarks that matches or surpasses supervised models; (2) demonstrates superior out-of-distribution (OOD) generalization across diverse video content and distortions; and (3) sets a new state-of-the-art when fine-tuned on human-labeled datasets. Extensive experimental results validate the effectiveness of our self-supervised approach in training generalized VQA models. The datasets and code will be publicly released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Smart Point-and-Shoot Photography</title>
<link>https://arxiv.org/abs/2505.03638</link>
<guid>https://arxiv.org/abs/2505.03638</guid>
<content:encoded><![CDATA[
arXiv:2505.03638v1 Announce Type: new 
Abstract: Hundreds of millions of people routinely take photos using their smartphones as point and shoot (PAS) cameras, yet very few would have the photography skills to compose a good shot of a scene. While traditional PAS cameras have built-in functions to ensure a photo is well focused and has the right brightness, they cannot tell the users how to compose the best shot of a scene. In this paper, we present a first of its kind smart point and shoot (SPAS) system to help users to take good photos. Our SPAS proposes to help users to compose a good shot of a scene by automatically guiding the users to adjust the camera pose live on the scene. We first constructed a large dataset containing 320K images with camera pose information from 4000 scenes. We then developed an innovative CLIP-based Composition Quality Assessment (CCQA) model to assign pseudo labels to these images. The CCQA introduces a unique learnable text embedding technique to learn continuous word embeddings capable of discerning subtle visual quality differences in the range covered by five levels of quality description words {bad, poor, fair, good, perfect}. And finally we have developed a camera pose adjustment model (CPAM) which first determines if the current view can be further improved and if so it outputs the adjust suggestion in the form of two camera pose adjustment angles. The two tasks of CPAM make decisions in a sequential manner and each involves different sets of training samples, we have developed a mixture-of-experts model with a gated loss function to train the CPAM in an end-to-end manner. We will present extensive results to demonstrate the performances of our SPAS system using publicly available image composition datasets.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant</title>
<link>https://arxiv.org/abs/2505.03654</link>
<guid>https://arxiv.org/abs/2505.03654</guid>
<content:encoded><![CDATA[
arXiv:2505.03654v1 Announce Type: new 
Abstract: Recent advances in personalized MLLMs enable effective capture of user-specific concepts, supporting both recognition of personalized concepts and contextual captioning. However, humans typically explore and reason over relations among objects and individuals, transcending surface-level information to achieve more personalized and contextual understanding. To this end, existing methods may face three main limitations: Their training data lacks multi-object sets in which relations among objects are learnable. Building on the limited training data, their models overlook the relations between different personalized concepts and fail to reason over them. Their experiments mainly focus on a single personalized concept, where evaluations are limited to recognition and captioning tasks. To address the limitations, we present a new dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard graph prompting methods are designed to align KGs within the model's semantic space. We establish the ReGraP Benchmark, which contains diverse task types: multiple-choice, fill-in-the-blank, True/False, and descriptive questions in both open- and closed-ended settings. The proposed benchmark is designed to evaluate the relational reasoning and knowledge-connection capability of personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and other competitive MLLMs. Results show that the proposed model not only learns personalized knowledge but also performs relational reasoning in responses, achieving the SoTA performance compared with the competitive methods. All the codes and datasets are released at: https://github.com/xyfyyds/ReGraP.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models</title>
<link>https://arxiv.org/abs/2505.03662</link>
<guid>https://arxiv.org/abs/2505.03662</guid>
<content:encoded><![CDATA[
arXiv:2505.03662v1 Announce Type: new 
Abstract: Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are essential for evaluating white matter integrity and structural connectivity in neuroimaging. However, the spatial misalignment between FA maps and tractography atlases hinders their effective integration into predictive models. To address this issue, we propose a CycleGAN based approach for generating FA maps directly from T1-weighted MRI scans, representing the first application of this technique to both healthy and tumour-affected tissues. Our model, trained on unpaired data, produces high fidelity maps, which have been rigorously evaluated using Structural Similarity Index (SSIM) and Peak Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in tumour regions. Radiological assessments further underscore the model's potential to enhance clinical workflows by providing an AI-driven alternative that reduces the necessity for additional scans.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution-Conditional Generation: From Class Distribution to Creative Generation</title>
<link>https://arxiv.org/abs/2505.03667</link>
<guid>https://arxiv.org/abs/2505.03667</guid>
<content:encoded><![CDATA[
arXiv:2505.03667v1 Announce Type: new 
Abstract: Text-to-image (T2I) diffusion models are effective at producing semantically aligned images, but their reliance on training data distributions limits their ability to synthesize truly novel, out-of-distribution concepts. Existing methods typically enhance creativity by combining pairs of known concepts, yielding compositions that, while out-of-distribution, remain linguistically describable and bounded within the existing semantic space. Inspired by the soft probabilistic outputs of classifiers on ambiguous inputs, we propose Distribution-Conditional Generation, a novel formulation that models creativity as image synthesis conditioned on class distributions, enabling semantically unconstrained creative generation. Building on this, we propose DisTok, an encoder-decoder framework that maps class distributions into a latent space and decodes them into tokens of creative concept. DisTok maintains a dynamic concept pool and iteratively sampling and fusing concept pairs, enabling the generation of tokens aligned with increasingly complex class distributions. To enforce distributional consistency, latent vectors sampled from a Gaussian prior are decoded into tokens and rendered into images, whose class distributions-predicted by a vision-language model-supervise the alignment between input distributions and the visual semantics of generated tokens. The resulting tokens are added to the concept pool for subsequent composition. Extensive experiments demonstrate that DisTok, by unifying distribution-conditioned fusion and sampling-based synthesis, enables efficient and flexible token-level generation, achieving state-of-the-art performance with superior text-image alignment and human preference scores.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting</title>
<link>https://arxiv.org/abs/2505.03679</link>
<guid>https://arxiv.org/abs/2505.03679</guid>
<content:encoded><![CDATA[
arXiv:2505.03679v1 Announce Type: new 
Abstract: Segmenting objects in an environment is a crucial task for autonomous driving and robotics, as it enables a better understanding of the surroundings of each agent. Although camera sensors provide rich visual details, they are vulnerable to adverse weather conditions. In contrast, radar sensors remain robust under such conditions, but often produce sparse and noisy data. Therefore, a promising approach is to fuse information from both sensors. In this work, we propose a novel framework to enhance camera-only baselines by integrating a diffusion model into a camera-radar fusion architecture. We leverage radar point features to create pseudo-masks using the Segment-Anything model, treating the projected radar points as point prompts. Additionally, we propose a noise reduction unit to denoise these pseudo-masks, which are further used to generate inpainted images that complete the missing information in the original images. Our method improves the camera-only segmentation baseline by 2.63% in mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the Waterscenes dataset. This demonstrates the effectiveness of our approach for semantic segmentation using camera-radar fusion under adverse weather conditions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration</title>
<link>https://arxiv.org/abs/2505.03692</link>
<guid>https://arxiv.org/abs/2505.03692</guid>
<content:encoded><![CDATA[
arXiv:2505.03692v1 Announce Type: new 
Abstract: Multiview point cloud registration plays a crucial role in robotics, automation, and computer vision fields. This paper concentrates on pose graph construction and motion synchronization within multiview registration. Previous methods for pose graph construction often pruned fully connected graphs or constructed sparse graph using global feature aggregated from local descriptors, which may not consistently yield reliable results. To identify dependable pairs for pose graph construction, we design a network model that extracts information from the matching distance between point cloud pairs. For motion synchronization, we propose another neural network model to calculate the absolute pose in a data-driven manner, rather than optimizing inaccurate handcrafted loss functions. Our model takes into account geometric distribution information and employs a modified attention mechanism to facilitate flexible and reliable feature interaction. Experimental results on diverse indoor and outdoor datasets confirm the effectiveness and generalizability of our approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning</title>
<link>https://arxiv.org/abs/2505.03703</link>
<guid>https://arxiv.org/abs/2505.03703</guid>
<content:encoded><![CDATA[
arXiv:2505.03703v1 Announce Type: new 
Abstract: Vision-language models (VLMs) allow to embed texts and images in a shared representation space. However, it has been shown that these models are subject to a modality gap phenomenon meaning there exists a clear separation between the embeddings from one modality and another in the embedding space. While this misalignment is detrimental for downstream tasks such as multimodal retrieval, multimodal clustering or zero-shot classification, etc. no generic and practical methods have so far been proposed to assess it precisely and even reduce it. We therefore propose novel measures and effective techniques (spectral- and optimal transport-based methods) to achieve this goal. Extensive experiments conducted on several image-text datasets and models demonstrate their effectiveness and beneficial effects on downstream tasks. Our code is available at the URL provided in the paper's abstract.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DISARM++: Beyond scanner-free harmonization</title>
<link>https://arxiv.org/abs/2505.03715</link>
<guid>https://arxiv.org/abs/2505.03715</guid>
<content:encoded><![CDATA[
arXiv:2505.03715v1 Announce Type: new 
Abstract: Harmonization of T1-weighted MR images across different scanners is crucial for ensuring consistency in neuroimaging studies. This study introduces a novel approach to direct image harmonization, moving beyond feature standardization to ensure that extracted features remain inherently reliable for downstream analysis. Our method enables image transfer in two ways: (1) mapping images to a scanner-free space for uniform appearance across all scanners, and (2) transforming images into the domain of a specific scanner used in model training, embedding its unique characteristics. Our approach presents strong generalization capability, even for unseen scanners not included in the training phase. We validated our method using MR images from diverse cohorts, including healthy controls, traveling subjects, and individuals with Alzheimer's disease (AD). The model's effectiveness is tested in multiple applications, such as brain age prediction (R2 = 0.60 \pm 0.05), biomarker extraction, AD classification (Test Accuracy = 0.86 \pm 0.03), and diagnosis prediction (AUC = 0.95). In all cases, our harmonization technique outperforms state-of-the-art methods, showing improvements in both reliability and predictive accuracy. Moreover, our approach eliminates the need for extensive preprocessing steps, such as skull-stripping, which can introduce errors by misclassifying brain and non-brain structures. This makes our method particularly suitable for applications that require full-head analysis, including research on head trauma and cranial deformities. Additionally, our harmonization model does not require retraining for new datasets, allowing smooth integration into various neuroimaging workflows. By ensuring scanner-invariant image quality, our approach provides a robust and efficient solution for improving neuroimaging studies across diverse settings. The code is available at this link.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios</title>
<link>https://arxiv.org/abs/2505.03730</link>
<guid>https://arxiv.org/abs/2505.03730</guid>
<content:encoded><![CDATA[
arXiv:2505.03730v1 Announce Type: new 
Abstract: Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure, such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at https://shiyi-zh0408.github.io/projectpages/FlexiAct/
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent System for Comprehensive Soccer Understanding</title>
<link>https://arxiv.org/abs/2505.03735</link>
<guid>https://arxiv.org/abs/2505.03735</guid>
<content:encoded><![CDATA[
arXiv:2505.03735v1 Announce Type: new 
Abstract: Recent advancements in AI-driven soccer understanding have demonstrated rapid progress, yet existing research predominantly focuses on isolated or narrow tasks. To bridge this gap, we propose a comprehensive framework for holistic soccer understanding. Specifically, we make the following contributions in this paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer knowledge base, integrating rich domain knowledge about players, teams, referees, and venues to enable knowledge-driven reasoning; (ii) we present SoccerBench, the largest and most comprehensive soccer-specific benchmark, featuring around 10K standardized multimodal (text, image, video) multi-choice QA pairs across 13 distinct understanding tasks, curated through automated pipelines and manual verification; (iii) we introduce SoccerAgent, a novel multi-agent system that decomposes complex soccer questions via collaborative reasoning, leveraging domain expertise from SoccerWiki and achieving robust performance; (iv) extensive evaluations and ablations that benchmark state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our proposed agentic system. All data and code are publicly available at: https://jyrao.github.io/SoccerAgent/.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers</title>
<link>https://arxiv.org/abs/2505.02843</link>
<guid>https://arxiv.org/abs/2505.02843</guid>
<content:encoded><![CDATA[
arXiv:2505.02843v1 Announce Type: cross 
Abstract: Artificial intelligence in medical imaging has seen unprecedented growth in the last years, due to rapid advances in deep learning and computing resources. Applications cover the full range of existing medical imaging modalities, with unique characteristics driven by the physics of each technique. Yet, artificial intelligence professionals entering the field, and even experienced developers, often lack a comprehensive understanding of the physical principles underlying medical image acquisition, which hinders their ability to fully leverage its potential. The integration of physics knowledge into artificial intelligence algorithms enhances their trustworthiness and robustness in medical imaging, especially in scenarios with limited data availability. In this work, we review the fundamentals of physics in medical images and their impact on the latest advances in artificial intelligence, particularly, in generative models and reconstruction algorithms. Finally, we explore the integration of physics knowledge into physics-inspired machine learning models, which leverage physics-based constraints to enhance the learning of medical imaging features.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Floating Car Observers in Intelligent Transportation Systems: Detection Modeling and Temporal Insights</title>
<link>https://arxiv.org/abs/2505.02845</link>
<guid>https://arxiv.org/abs/2505.02845</guid>
<content:encoded><![CDATA[
arXiv:2505.02845v1 Announce Type: cross 
Abstract: Floating Car Observers (FCOs) extend traditional Floating Car Data (FCD) by integrating onboard sensors to detect and localize other traffic participants, providing richer and more detailed traffic data. In this work, we explore various modeling approaches for FCO detections within microscopic traffic simulations to evaluate their potential for Intelligent Transportation System (ITS) applications. These approaches range from 2D raytracing to high-fidelity co-simulations that emulate real-world sensors and integrate 3D object detection algorithms to closely replicate FCO detections. Additionally, we introduce a neural network-based emulation technique that effectively approximates the results of high-fidelity co-simulations. This approach captures the unique characteristics of FCO detections while offering a fast and scalable solution for modeling. Using this emulation method, we investigate the impact of FCO data in a digital twin of a traffic network modeled in SUMO. Results demonstrate that even at a 20% penetration rate, FCOs using LiDAR-based detections can identify 65% of vehicles across various intersections and traffic demand scenarios. Further potential emerges when temporal insights are integrated, enabling the recovery of previously detected but currently unseen vehicles. By employing data-driven methods, we recover over 80% of these vehicles with minimal positional deviations. These findings underscore the potential of FCOs for ITS, particularly in enhancing traffic state estimation and monitoring under varying penetration rates and traffic conditions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition</title>
<link>https://arxiv.org/abs/2505.02877</link>
<guid>https://arxiv.org/abs/2505.02877</guid>
<content:encoded><![CDATA[
arXiv:2505.02877v1 Announce Type: cross 
Abstract: Plant disease is a critical factor affecting agricultural production. Traditional manual recognition methods face significant drawbacks, including low accuracy, high costs, and inefficiency. Deep learning techniques have demonstrated significant benefits in identifying plant diseases, but they still face challenges such as inference delays and high energy consumption. Deep learning algorithms are difficult to run on resource-limited embedded devices. Offloading these models to cloud servers is confronted with the restriction of communication bandwidth, and all of these factors will influence the inference's efficiency. We propose a collaborative inference framework for recognizing plant diseases between edge devices and cloud servers to enhance inference speed. The DNN model for plant disease recognition is pruned through deep reinforcement learning to improve the inference speed and reduce energy consumption. Then the optimal split point is determined by a greedy strategy to achieve the best collaborated inference acceleration. Finally, the system for collaborative inference acceleration in plant disease recognition has been implemented using Gradio to facilitate friendly human-machine interaction. Experiments indicate that the proposed collaborative inference framework significantly increases inference speed while maintaining acceptable recognition accuracy, offering a novel solution for rapidly diagnosing and preventing plant diseases.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Prompting for Diverse Count-level PET Denoising</title>
<link>https://arxiv.org/abs/2505.03037</link>
<guid>https://arxiv.org/abs/2505.03037</guid>
<content:encoded><![CDATA[
arXiv:2505.03037v1 Announce Type: cross 
Abstract: The to-be-denoised positron emission tomography (PET) volumes are inherent with diverse count levels, which imposes challenges for a unified model to tackle varied cases. In this work, we resort to the recently flourished prompt learning to achieve generalizable PET denoising with different count levels. Specifically, we propose dual prompts to guide the PET denoising in a divide-and-conquer manner, i.e., an explicitly count-level prompt to provide the specific prior information and an implicitly general denoising prompt to encode the essential PET denoising knowledge. Then, a novel prompt fusion module is developed to unify the heterogeneous prompts, followed by a prompt-feature interaction module to inject prompts into the features. The prompts are able to dynamically guide the noise-conditioned denoising process. Therefore, we are able to efficiently train a unified denoising model for various count levels, and deploy it to different cases with personalized prompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly selected 13-22\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies. It shows our dual prompting can largely improve the performance with informed count-level and outperform the count-conditional model.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sim2Real Transfer for Vision-Based Grasp Verification</title>
<link>https://arxiv.org/abs/2505.03046</link>
<guid>https://arxiv.org/abs/2505.03046</guid>
<content:encoded><![CDATA[
arXiv:2505.03046v1 Announce Type: cross 
Abstract: The verification of successful grasps is a crucial aspect of robot manipulation, particularly when handling deformable objects. Traditional methods relying on force and tactile sensors often struggle with deformable and non-rigid objects. In this work, we present a vision-based approach for grasp verification to determine whether the robotic gripper has successfully grasped an object. Our method employs a two-stage architecture; first YOLO-based object detection model to detect and locate the robot's gripper and then a ResNet-based classifier determines the presence of an object. To address the limitations of real-world data capture, we introduce HSR-GraspSynth, a synthetic dataset designed to simulate diverse grasping scenarios. Furthermore, we explore the use of Visual Question Answering capabilities as a zero-shot baseline to which we compare our model. Experimental results demonstrate that our approach achieves high accuracy in real-world environments, with potential for integration into grasping pipelines. Code and datasets are publicly available at https://github.com/pauamargant/HSR-GraspSynth .
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis</title>
<link>https://arxiv.org/abs/2505.03123</link>
<guid>https://arxiv.org/abs/2505.03123</guid>
<content:encoded><![CDATA[
arXiv:2505.03123v1 Announce Type: cross 
Abstract: We propose a multimodal spatiotemporal graph neural network (STG) framework to predict colorectal cancer liver metastasis (CRLM) progression. Current clinical models do not effectively integrate the tumor's spatial heterogeneity, dynamic evolution, and complex multimodal data relationships, limiting their predictive accuracy. Our STG framework combines preoperative CT imaging and clinical data into a heterogeneous graph structure, enabling joint modeling of tumor distribution and temporal evolution through spatial topology and cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal neighborhood information and leverages supervised and contrastive learning strategies to enhance the model's ability to capture temporal features and improve robustness. A lightweight version of the model reduces parameter count by 78.55%, maintaining near-state-of-the-art performance. The model jointly optimizes recurrence risk regression and survival analysis tasks, with contrastive loss improving feature representational discriminability and cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a time-adjacent accuracy of 85% and a mean absolute error of 1.1005, significantly outperforming existing methods. The innovative heterogeneous graph construction and spatiotemporal decoupling mechanism effectively uncover the associations between dynamic tumor microenvironment changes and prognosis, providing reliable quantitative support for personalized treatment decisions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Data Curation Using GPS &amp; NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets</title>
<link>https://arxiv.org/abs/2505.03174</link>
<guid>https://arxiv.org/abs/2505.03174</guid>
<content:encoded><![CDATA[
arXiv:2505.03174v1 Announce Type: cross 
Abstract: Instruction-Action (IA) data pairs are valuable for training robotic systems, especially autonomous vehicles (AVs), but having humans manually annotate this data is costly and time-inefficient. This paper explores the potential of using mobile application Global Positioning System (GPS) references and Natural Language Processing (NLP) to automatically generate large volumes of IA commands and responses without having a human generate or retroactively tag the data. In our pilot data collection, by driving to various destinations and collecting voice instructions from GPS applications, we demonstrate a means to collect and categorize the diverse sets of instructions, further accompanied by video data to form complete vision-language-action triads. We provide details on our completely automated data collection prototype system, ADVLAT-Engine. We characterize collected GPS voice instructions into eight different classifications, highlighting the breadth of commands and referentialities available for curation from freely available mobile applications. Through research and exploration into the automation of IA data pairs using GPS references, the potential to increase the speed and volume at which high-quality IA datasets are created, while minimizing cost, can pave the way for robust vision-language-action (VLA) models to serve tasks in vision-language navigation (VLN) and human-interactive autonomous systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization</title>
<link>https://arxiv.org/abs/2505.03186</link>
<guid>https://arxiv.org/abs/2505.03186</guid>
<content:encoded><![CDATA[
arXiv:2505.03186v1 Announce Type: cross 
Abstract: The inherent synchronization between a speaker's lip movements, voice, and the underlying linguistic content offers a rich source of information for improving speech processing tasks, especially in challenging conditions where traditional audio-only systems falter. We introduce CoGenAV, a powerful and data-efficient model designed to learn versatile audio-visual representations applicable across a wide range of speech and audio-visual tasks. CoGenAV is trained by optimizing a dual objective derived from natural audio-visual synchrony, contrastive feature alignment and generative text prediction, using only 223 hours of labeled data from the LRS2 dataset. This contrastive-generative synchronization strategy effectively captures fundamental cross-modal correlations. We showcase the effectiveness and versatility of the learned CoGenAV representations on multiple benchmarks. When utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these representations contribute to achieving a state-of-the-art Word Error Rate (WER) of 1.27. They also enable strong performance in Visual Speech Recognition (VSR) with a WER of 22.0 on LRS2, and significantly improve performance in noisy environments by over 70%. Furthermore, CoGenAV representations benefit speech reconstruction tasks, boosting performance in Speech Enhancement and Separation, and achieve competitive results in audio-visual synchronization tasks like Active Speaker Detection (ASD). Our model will be open-sourced to facilitate further development and collaboration within both academia and industry.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Image Captioning Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.03420</link>
<guid>https://arxiv.org/abs/2505.03420</guid>
<content:encoded><![CDATA[
arXiv:2505.03420v1 Announce Type: cross 
Abstract: Hallucinations in vision-language models (VLMs) hinder reliability and real-world applicability, usually stemming from distribution shifts between pretraining data and test samples. Existing solutions, such as retraining or fine-tuning on additional data, demand significant computational resources and labor-intensive data collection, while ensemble-based methods incur additional costs by introducing auxiliary VLMs. To address these challenges, we propose a novel test-time adaptation framework using reinforcement learning to mitigate hallucinations during inference without retraining or any auxiliary VLMs. By updating only the learnable parameters in the layer normalization of the language model (approximately 0.003% of the model parameters), our method reduces distribution shifts between test samples and pretraining samples. A CLIP-based hallucination evaluation model is proposed to provide dual rewards to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in hallucination rates on LLaVA and InstructBLIP, respectively. Our approach outperforms state-of-the-art baselines with a 68.3% improvement in hallucination mitigation, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition</title>
<link>https://arxiv.org/abs/2505.03510</link>
<guid>https://arxiv.org/abs/2505.03510</guid>
<content:encoded><![CDATA[
arXiv:2505.03510v1 Announce Type: cross 
Abstract: In this paper, we introduce a novel paradigm for reservoir computing (RC) that leverages a pool of cultured biological neurons as the reservoir substrate, creating a biological reservoir computing (BRC). This system operates similarly to an echo state network (ESN), with the key distinction that the neural activity is generated by a network of cultured neurons, rather than being modeled by traditional artificial computational units. The neuronal activity is recorded using a multi-electrode array (MEA), which enables high-throughput recording of neural signals. In our approach, inputs are introduced into the network through a subset of the MEA electrodes, while the remaining electrodes capture the resulting neural activity. This generates a nonlinear mapping of the input data to a high-dimensional biological feature space, where distinguishing between data becomes more efficient and straightforward, allowing a simple linear classifier to perform pattern recognition tasks effectively. To evaluate the performance of our proposed system, we present an experimental study that includes various input patterns, such as positional codes, bars with different orientations, and a digit recognition task. The results demonstrate the feasibility of using biological neural networks to perform tasks traditionally handled by artificial neural networks, paving the way for further exploration of biologically-inspired computing systems, with potential applications in neuromorphic engineering and bio-hybrid computing.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders</title>
<link>https://arxiv.org/abs/2505.03646</link>
<guid>https://arxiv.org/abs/2505.03646</guid>
<content:encoded><![CDATA[
arXiv:2505.03646v1 Announce Type: cross 
Abstract: Despite the extensive use of deep autoencoders (AEs) in critical applications, their adversarial robustness remains relatively underexplored compared to classification models. AE robustness is characterized by the Lipschitz bounds of its components. Existing robustness evaluation frameworks based on white-box attacks do not fully exploit the vulnerabilities of intermediate ill-conditioned layers in AEs. In the context of optimizing imperceptible norm-bounded additive perturbations to maximize output damage, existing methods struggle to effectively propagate adversarial loss gradients throughout the network, often converging to less effective perturbations. To address this, we propose a novel layer-conditioning-based adversarial optimization objective that effectively guides the adversarial map toward regions of local Lipschitz bounds by enhancing loss gradient information propagation during attack optimization. We demonstrate through extensive experiments on state-of-the-art AEs that our adversarial objective results in stronger attacks, outperforming existing methods in both universal and sample-specific scenarios. As a defense method against this attack, we introduce an inference-time adversarially trained defense plugin that mitigates the effects of adversarial examples.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach</title>
<link>https://arxiv.org/abs/2505.03702</link>
<guid>https://arxiv.org/abs/2505.03702</guid>
<content:encoded><![CDATA[
arXiv:2505.03702v1 Announce Type: cross 
Abstract: Automating leaf manipulation in agricultural settings faces significant challenges, including the variability of plant morphologies and deformable leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf grasping that combines traditional computer vision with neural networks through self-supervised learning. Our method integrates YOLOv8 for instance segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf representations, which feed into both a geometric feature scoring pipeline and a neural refinement module (GraspPointCNN). The key innovation is our confidence-weighted fusion mechanism that dynamically balances the contribution of each approach based on prediction certainty. Our self-supervised framework uses the geometric pipeline as an expert teacher to automatically generate training data. Experiments demonstrate that our approach achieves an 88.0% success rate in controlled environments and 84.7% in real greenhouse conditions, significantly outperforming both purely geometric (75.3%) and neural (60.2%) methods. This work establishes a new paradigm for agricultural robotics where domain expertise is seamlessly integrated with machine learning capabilities, providing a foundation for fully automated crop monitoring systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Imitation Enables Contextual Humanoid Control</title>
<link>https://arxiv.org/abs/2505.03729</link>
<guid>https://arxiv.org/abs/2505.03729</guid>
<content:encoded><![CDATA[
arXiv:2505.03729v1 Announce Type: cross 
Abstract: How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infrared Image Deturbulence Restoration Using Degradation Parameter-Assisted Wide &amp; Deep Learning</title>
<link>https://arxiv.org/abs/2305.18708</link>
<guid>https://arxiv.org/abs/2305.18708</guid>
<content:encoded><![CDATA[
arXiv:2305.18708v2 Announce Type: replace 
Abstract: Infrared images captured under turbulent conditions are degraded by complex geometric distortions and blur. We address infrared deturbulence as an image restoration task, proposing DparNet, a parameter-assisted multi-frame network with a wide & deep architecture. DparNet learns a degradation prior (key parameter matrix) directly from degraded images without external knowledge. Its wide & deep architecture uses these learned parameters to directly modulate restoration, achieving spatially and intensity adaptive results. Evaluated on dedicated infrared deturbulence (49,744 images) and visible image denoising (109,536 images) datasets, DparNet significantly outperforms State-of-the-Art (SOTA) methods in restoration performance and efficiency. Notably, leveraging these parameters improves PSNR by 0.6-1.1 dB with less than 2% increase in model parameters and computational complexity. Our work demonstrates that degraded images hide key degradation information that can be learned and utilized to boost adaptive image restoration.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-supervised Underwater Image Enhancement Using A Physics-Aware Triple-Stream Network</title>
<link>https://arxiv.org/abs/2307.11470</link>
<guid>https://arxiv.org/abs/2307.11470</guid>
<content:encoded><![CDATA[
arXiv:2307.11470v4 Announce Type: replace 
Abstract: Underwater images normally suffer from degradation due to the transmission medium of water bodies. Both traditional prior-based approaches and deep learning-based methods have been used to address this problem. However, the inflexible assumption of the former often impairs their effectiveness in handling diverse underwater scenes, while the generalization of the latter to unseen images is usually weakened by insufficient data. In this study, we leverage both the physics-based Image Formation Model (IFM) and deep learning techniques for Underwater Image Enhancement (UIE). To this end, we propose a novel Physics-Aware Triple-Stream Underwater Image Enhancement Network, i.e., PATS-UIENet, which comprises a Direct Signal Transmission Estimation Steam (D-Stream), a Backscatter Signal Transmission Estimation Steam (B-Stream) and an Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE task by explicitly estimating the degradation parameters of a revised IFM. We also adopt an IFM-inspired semi-supervised learning framework, which exploits both the labeled and unlabeled images, to address the issue of insufficient data. To our knowledge, such a physics-aware deep network and the IFM-inspired semi-supervised learning framework have not been used for the UIE task before. Our method performs better than, or at least comparably to, sixteen baselines across six testing sets in the degradation estimation and UIE tasks. These promising results should be due to the fact that the proposed method can not only model the degradation but also learn the characteristics of diverse underwater scenes.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware Learning</title>
<link>https://arxiv.org/abs/2310.04306</link>
<guid>https://arxiv.org/abs/2310.04306</guid>
<content:encoded><![CDATA[
arXiv:2310.04306v2 Announce Type: replace 
Abstract: Group-level emotion recognition (GER) is an inseparable part of human behavior analysis, aiming to recognize an overall emotion in a multi-person scene. However, the existing methods are devoted to combing diverse emotion cues while ignoring the inherent uncertainties under unconstrained environments, such as congestion and occlusion occurring within a group. Additionally, since only group-level labels are available, inconsistent emotion predictions among individuals in one group can confuse the network. In this paper, we propose an uncertainty-aware learning (UAL) method to extract more robust representations for GER. By explicitly modeling the uncertainty of each individual, we utilize stochastic embedding drawn from a Gaussian distribution instead of deterministic point embedding. This representation captures the probabilities of different emotions and generates diverse predictions through this stochasticity during the inference stage. Furthermore, uncertainty-sensitive scores are adaptively assigned as the fusion weights of individuals' face within each group. Moreover, we develop an image enhancement module to enhance the model's robustness against severe noise. The overall three-branch model, encompassing face, object, and scene component, is guided by a proportional-weighted fusion strategy and integrates the proposed uncertainty-aware method to produce the final group-level output. Experimental results demonstrate the effectiveness and generalization ability of our method across three widely used databases.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Data Selection with Performance: Performance-driven Reinforcement Learning for Active Learning in Object Detection</title>
<link>https://arxiv.org/abs/2310.08387</link>
<guid>https://arxiv.org/abs/2310.08387</guid>
<content:encoded><![CDATA[
arXiv:2310.08387v3 Announce Type: replace 
Abstract: Active learning strategies aim to train high-performance models with minimal labeled data by selecting the most informative instances for labeling. However, existing methods for assessing data informativeness often fail to align directly with task model performance metrics, such as mean average precision (mAP) in object detection. This paper introduces Mean-AP Guided Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness for deep detection networks, directly optimizing the sampling strategy using mAP. MGRAL employs a reinforcement learning agent based on LSTM architecture to efficiently navigate the combinatorial challenge of batch sample selection and the non-differentiable nature between performance and selected batches. The agent optimizes selection using policy gradient with mAP improvement as the reward signal. To address the computational intensity of mAP estimation with unlabeled samples, we implement fast look-up tables, ensuring real-world feasibility. We evaluate MGRAL on PASCAL VOC and MS COCO benchmarks across various backbone architectures. Our approach demonstrates strong performance, establishing a new paradigm in reinforcement learning-based active learning for object detection.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paragraph-to-Image Generation with Information-Enriched Diffusion Model</title>
<link>https://arxiv.org/abs/2311.14284</link>
<guid>https://arxiv.org/abs/2311.14284</guid>
<content:encoded><![CDATA[
arXiv:2311.14284v3 Announce Type: replace 
Abstract: Text-to-image (T2I) models have recently experienced rapid development, achieving astonishing performance in terms of fidelity and textual alignment capabilities. However, given a long paragraph (up to 512 words), these generation models still struggle to achieve strong alignment and are unable to generate images depicting complex scenes. In this paper, we introduce an information-enriched diffusion model for paragraph-to-image generation task, termed ParaDiffusion, which delves into the transference of the extensive semantic comprehension capabilities of large language models to the task of image generation. At its core is using a large language model (e.g., Llama V2) to encode long-form text, followed by fine-tuning with LORA to alignthe text-image feature spaces in the generation task. To facilitate the training of long-text semantic alignment, we also curated a high-quality paragraph-image pair dataset, namely ParaImage. This dataset contains a small amount of high-quality, meticulously annotated data, and a large-scale synthetic dataset with long text descriptions being generated using a vision-language model. Experiments demonstrate that ParaDiffusion outperforms state-of-the-art models (SD XL, DeepFloyd IF) on ViLG-300 and ParaPrompts, achieving up to 15% and 45% human voting rate improvements for visual appeal and text faithfulness, respectively. The code and dataset will be released to foster community research on long-text alignment.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback</title>
<link>https://arxiv.org/abs/2404.05046</link>
<guid>https://arxiv.org/abs/2404.05046</guid>
<content:encoded><![CDATA[
arXiv:2404.05046v2 Announce Type: replace 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. To tackle this issue, existing methods mainly utilize Reinforcement Learning (RL) to align modalities in LVLMs. However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive. To handle these limitations, we propose an innovative method to align modalities in LVLMs through Fine-Grained Artificial Intelligence Feedback (FGAIF), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and Reinforcement Learning with Fine-grained Reward. Specifically, We first utilize AI tools to predict the types of hallucination for each segment in the response and obtain a collection of fine-grained feedback. Then, based on the collected reward data, three specialized reward models are trained to produce dense rewards. Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm. Extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method. Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3D-HGS: 3D Half-Gaussian Splatting</title>
<link>https://arxiv.org/abs/2406.02720</link>
<guid>https://arxiv.org/abs/2406.02720</guid>
<content:encoded><![CDATA[
arXiv:2406.02720v4 Announce Type: replace 
Abstract: Photo-realistic image rendering from 3D scene reconstruction has advanced significantly with neural rendering techniques. Among these, 3D Gaussian Splatting (3D-GS) outperforms Neural Radiance Fields (NeRFs) in quality and speed but struggles with shape and color discontinuities. We propose 3D Half-Gaussian (3D-HGS) kernels as a plug-and-play solution to address these limitations. Our experiments show that 3D-HGS enhances existing 3D-GS methods, achieving state-of-the-art rendering quality without compromising speed.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph</title>
<link>https://arxiv.org/abs/2406.07113</link>
<guid>https://arxiv.org/abs/2406.07113</guid>
<content:encoded><![CDATA[
arXiv:2406.07113v4 Announce Type: replace 
Abstract: Locating objects described in natural language presents a significant challenge for autonomous agents. Existing CLIP-based open-vocabulary methods successfully perform 3D object grounding with simple (bare) queries, but cannot cope with ambiguous descriptions that demand an understanding of object relations. To tackle this problem, we propose a modular approach called BBQ (Beyond Bare Queries), which constructs 3D scene graph representation with metric and semantic spatial edges and utilizes a large language model as a human-to-agent interface through our deductive scene reasoning algorithm. BBQ employs robust DINO-powered associations to construct 3D object-centric map and an advanced raycasting algorithm with a 2D vision-language model to describe them as graph nodes. On the Replica and ScanNet datasets, we have demonstrated that BBQ takes a leading place in open-vocabulary 3D semantic segmentation compared to other zero-shot methods. Also, we show that leveraging spatial relations is especially effective for scenes containing multiple entities of the same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks, our deductive approach demonstrates a significant improvement, enabling objects grounding by complex queries compared to other state-of-the-art methods. The combination of our design choices and software implementation has resulted in significant data processing speed in experiments on the robot on-board computer. This promising performance enables the application of our approach in intelligent robotics projects. We made the code publicly available at https://linukc.github.io/BeyondBareQueries/.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMORE: Simultaneous Map and Object REconstruction</title>
<link>https://arxiv.org/abs/2406.13896</link>
<guid>https://arxiv.org/abs/2406.13896</guid>
<content:encoded><![CDATA[
arXiv:2406.13896v4 Announce Type: replace 
Abstract: We present a method for dynamic surface reconstruction of large-scale urban scenes from LiDAR. Depth-based reconstructions tend to focus on small-scale objects or large-scale SLAM reconstructions that treat moving objects as outliers. We take a holistic perspective and optimize a compositional model of a dynamic scene that decomposes the world into rigidly-moving objects and the background. To achieve this, we take inspiration from recent novel view synthesis methods and frame the reconstruction problem as a global optimization over neural surfaces, ego poses, and object poses, which minimizes the error between composed spacetime surfaces and input LiDAR scans. In contrast to view synthesis methods, which typically minimize 2D errors with gradient descent, we minimize a 3D point-to-surface error by coordinate descent, which we decompose into registration and surface reconstruction steps. Each step can be handled well by off-the-shelf methods without any re-training. We analyze the surface reconstruction step for rolling-shutter LiDARs, and show that deskewing operations common in continuous time SLAM can be applied to dynamic objects as well, improving results over prior art by an order of magnitude. Beyond pursuing dynamic reconstruction as a goal in and of itself, we propose that such a system can be used to auto-label partially annotated sequences and produce ground truth annotation for hard-to-label problems such as depth completion and scene flow. Please see https://anishmadan23.github.io/smore/ for more visual results.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioSyntax: end-to-end SYNTAX score prediction -- dataset, benchmark and method</title>
<link>https://arxiv.org/abs/2407.19894</link>
<guid>https://arxiv.org/abs/2407.19894</guid>
<content:encoded><![CDATA[
arXiv:2407.19894v2 Announce Type: replace 
Abstract: The SYNTAX score has become a widely used measure of coronary disease severity, crucial in selecting the optimal mode of the revascularization procedure. This paper introduces a new medical regression and classification problem - automatically estimating SYNTAX score from coronary angiography. Our study presents a comprehensive CardioSYNTAX dataset of 3,018 patients for the SYNTAX score estimation and coronary dominance classification. The dataset features a balanced distribution of individuals with zero and non-zero scores. This dataset includes a first-of-its-kind, complete coronary angiography samples captured through a multi-view X-ray video, allowing one to observe coronary arteries from multiple perspectives. Furthermore, we present a novel, fully automatic end-to-end method for estimating the SYNTAX. For such a difficult task, we have achieved a solid coefficient of determination R2 of 0.51 in score value prediction and 77.3% accuracy for zero score classification.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Guided Self-Questioning and Answering for Video-Language Alignment</title>
<link>https://arxiv.org/abs/2410.02768</link>
<guid>https://arxiv.org/abs/2410.02768</guid>
<content:encoded><![CDATA[
arXiv:2410.02768v2 Announce Type: replace 
Abstract: The development of multi-modal models has been rapidly advancing, with some demonstrating remarkable capabilities. However, annotating video-text pairs remains expensive and insufficient. Take video question answering (VideoQA) tasks as an example, human annotated questions and answers often cover only part of the video, since the corresponding text is often short and monotonous, leading to underutilization of video. To address this, we propose a Bootstrapping Video-Language Alignment framework (BoViLA), a self-training method that augments question samples during training process through LLM-based self-questioning and answering, which help model exploit video information and the internal knowledge of LLMs more thoroughly to improve modality alignment. However, low-quality self-generated questions may instead contaminate the performance, especially in the early stages of training, as we have observed in our experiments. To filter bad self-generated questions, we introduce Evidential Deep Learning (EDL) to estimate uncertainty and assess the quality of self-generated questions by evaluating the modality alignment within the context. To the best of our knowledge, this work is the first to explore LLM-based self-training frameworks for modality alignment. We evaluate BoViLA on five strong VideoQA benchmarks, where it outperforms several state-of-the-art methods and demonstrate its effectiveness and generality. Additionally, we provide extensive analyses of the self-training framework and the EDL-based uncertainty filtering mechanism. The code will be made available.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality</title>
<link>https://arxiv.org/abs/2411.02179</link>
<guid>https://arxiv.org/abs/2411.02179</guid>
<content:encoded><![CDATA[
arXiv:2411.02179v2 Announce Type: replace 
Abstract: High-quality environment lighting is essential for creating immersive mobile augmented reality (AR) experiences. However, achieving visually coherent estimation for mobile AR is challenging due to several key limitations in AR device sensing capabilities, including low camera FoV and limited pixel dynamic ranges. Recent advancements in generative AI, which can generate high-quality images from different types of prompts, including texts and images, present a potential solution for high-quality lighting estimation. Still, to effectively use generative image diffusion models, we must address two key limitations of content quality and slow inference. In this work, we design and implement a generative lighting estimation system called CleAR that can produce high-quality, diverse environment maps in the format of 360{\deg} HDR images. Specifically, we design a two-step generation pipeline guided by AR environment context data to ensure the output aligns with the physical environment's visual context and color appearance. To improve the estimation robustness under different lighting conditions, we design a real-time refinement component to adjust lighting estimation results on AR devices. Through a combination of quantitative and qualitative evaluations, we show that CleAR outperforms state-of-the-art lighting estimation methods on both estimation accuracy, latency, and robustness, and is rated by 31 participants as producing better renderings for most virtual objects. For example, CleAR achieves 51% to 56% accuracy improvement on virtual object renderings across objects of three distinctive types of materials and reflective properties. CleAR produces lighting estimates of comparable or better quality in just 3.2 seconds -- over 110X faster than state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Fine Detail and Global Geometry for Compressed Depth Map Super-Resolution</title>
<link>https://arxiv.org/abs/2411.03239</link>
<guid>https://arxiv.org/abs/2411.03239</guid>
<content:encoded><![CDATA[
arXiv:2411.03239v4 Announce Type: replace 
Abstract: Recovering high-quality depth maps from compressed sources has gained significant attention due to the limitations of consumer-grade depth cameras and the bandwidth restrictions during data transmission. However, current methods still suffer from two challenges. First, bit-depth compression produces a uniform depth representation in regions with subtle variations, hindering the recovery of detailed information. Second, densely distributed random noise reduces the accuracy of estimating the global geometric structure of the scene. To address these challenges, we propose a novel framework, termed geometry-decoupled network (GDNet), for compressed depth map super-resolution that decouples the high-quality depth map reconstruction process by handling global and detailed geometric features separately. To be specific, we propose the fine geometry detail encoder (FGDE), which is designed to aggregate fine geometry details in high-resolution low-level image features while simultaneously enriching them with complementary information from low-resolution context-level image features. In addition, we develop the global geometry encoder (GGE) that aims at suppressing noise and extracting global geometric information effectively via constructing compact feature representation in a low-rank space. We conduct experiments on multiple benchmark datasets, demonstrating that our GDNet significantly outperforms current methods in terms of geometric consistency and detail recovery. In the ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st place award. Our codes are available at: https://github.com/Ian0926/GDNet.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>About Time: Advances, Challenges, and Outlooks of Action Understanding</title>
<link>https://arxiv.org/abs/2411.15106</link>
<guid>https://arxiv.org/abs/2411.15106</guid>
<content:encoded><![CDATA[
arXiv:2411.15106v2 Announce Type: replace 
Abstract: We have witnessed impressive advances in video action understanding. Increased dataset sizes, variability, and computation availability have enabled leaps in performance and task diversification. Current systems can provide coarse- and fine-grained descriptions of video scenes, extract segments corresponding to queries, synthesize unobserved parts of videos, and predict context across multiple modalities. This survey comprehensively reviews advances in uni- and multi-modal action understanding across a range of tasks. We focus on prevalent challenges, overview widely adopted datasets, and survey seminal works with an emphasis on recent advances. We broadly distinguish between three temporal scopes: (1) recognition tasks of actions observed in full, (2) prediction tasks for ongoing partially observed actions, and (3) forecasting tasks for subsequent unobserved action(s). This division allows us to identify specific action modeling and video representation challenges. Finally, we outline future directions to address current shortcomings.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers</title>
<link>https://arxiv.org/abs/2411.18673</link>
<guid>https://arxiv.org/abs/2411.18673</guid>
<content:encoded><![CDATA[
arXiv:2411.18673v4 Announce Type: replace 
Abstract: Numerous works have recently integrated 3D camera control into foundational text-to-video models, but the resulting camera control is often imprecise, and video generation quality suffers. In this work, we analyze camera motion from a first principles perspective, uncovering insights that enable precise 3D camera manipulation without compromising synthesis quality. First, we determine that motion induced by camera movements in videos is low-frequency in nature. This motivates us to adjust train and test pose conditioning schedules, accelerating training convergence while improving visual and motion quality. Then, by probing the representations of an unconditional video diffusion transformer, we observe that they implicitly perform camera pose estimation under the hood, and only a sub-portion of their layers contain the camera information. This suggested us to limit the injection of camera conditioning to a subset of the architecture to prevent interference with other video features, leading to a 4x reduction of training parameters, improved training speed, and 10% higher visual quality. Finally, we complement the typical dataset for camera control learning with a curated dataset of 20K diverse, dynamic videos with stationary cameras. This helps the model distinguish between camera and scene motion and improves the dynamics of generated pose-conditioned videos. We compound these findings to design the Advanced 3D Camera Control (AC3D) architecture, the new state-of-the-art model for generative video modeling with camera control.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing</title>
<link>https://arxiv.org/abs/2412.04280</link>
<guid>https://arxiv.org/abs/2412.04280</guid>
<content:encoded><![CDATA[
arXiv:2412.04280v2 Announce Type: replace 
Abstract: We present HumanEdit, a high-quality, human-rewarded dataset specifically designed for instruction-guided image editing, enabling precise and diverse image manipulations through open-form language instructions. Previous large-scale editing datasets often incorporate minimal human feedback, leading to challenges in aligning datasets with human preferences. HumanEdit bridges this gap by employing human annotators to construct data pairs and administrators to provide feedback. With meticulously curation, HumanEdit comprises 5,751 images and requires more than 2,500 hours of human effort across four stages, ensuring both accuracy and reliability for a wide range of image editing tasks. The dataset includes six distinct types of editing instructions: Action, Add, Counting, Relation, Remove, and Replace, encompassing a broad spectrum of real-world scenarios. All images in the dataset are accompanied by masks, and for a subset of the data, we ensure that the instructions are sufficiently detailed to support mask-free editing. Furthermore, HumanEdit offers comprehensive diversity and high-resolution $1024 \times 1024$ content sourced from various domains, setting a new versatile benchmark for instructional image editing datasets. With the aim of advancing future research and establishing evaluation benchmarks in the field of image editing, we release HumanEdit at https://huggingface.co/datasets/BryanW/HumanEdit.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data</title>
<link>https://arxiv.org/abs/2501.15326</link>
<guid>https://arxiv.org/abs/2501.15326</guid>
<content:encoded><![CDATA[
arXiv:2501.15326v2 Announce Type: replace 
Abstract: We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gathers 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks, respectively, in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. Code, model, and demo are available at https://ntlm1686.github.io/raso.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deformable Beta Splatting</title>
<link>https://arxiv.org/abs/2501.18630</link>
<guid>https://arxiv.org/abs/2501.18630</guid>
<content:encoded><![CDATA[
arXiv:2501.18630v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has advanced radiance field reconstruction by enabling real-time rendering. However, its reliance on Gaussian kernels for geometry and low-order Spherical Harmonics (SH) for color encoding limits its ability to capture complex geometries and diverse colors. We introduce Deformable Beta Splatting (DBS), a deformable and compact approach that enhances both geometry and color representation. DBS replaces Gaussian kernels with deformable Beta Kernels, which offer bounded support and adaptive frequency control to capture fine geometric details with higher fidelity while achieving better memory efficiency. In addition, we extended the Beta Kernel to color encoding, which facilitates improved representation of diffuse and specular components, yielding superior results compared to SH-based methods. Furthermore, Unlike prior densification techniques that depend on Gaussian properties, we mathematically prove that adjusting regularized opacity alone ensures distribution-preserved Markov chain Monte Carlo (MCMC), independent of the splatting kernel type. Experimental results demonstrate that DBS achieves state-of-the-art visual quality while utilizing only 45% of the parameters and rendering 1.5x faster than 3DGS-MCMC, highlighting the superior performance of DBS for real-time radiance field rendering. Interactive demonstrations and source code are available on our project website: https://rongliu-leo.github.io/beta-splatting/.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel computational workflows for natural and biomedical image processing based on hypercomplex algebras</title>
<link>https://arxiv.org/abs/2502.07758</link>
<guid>https://arxiv.org/abs/2502.07758</guid>
<content:encoded><![CDATA[
arXiv:2502.07758v5 Announce Type: replace 
Abstract: Hypercomplex image processing extends conventional techniques in a unified paradigm encompassing algebraic and geometric principles. This work leverages quaternions and the two-dimensional orthogonal planes split framework (splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D planes) for natural/biomedical image analysis through the following computational workflows and outcomes: natural/biomedical image re-colorization, natural image de-colorization, natural/biomedical image contrast enhancement, computational re-staining and stain separation in histological images, and performance gains in machine/deep learning pipelines for histological images. The workflows are analyzed separately for natural and biomedical images to showcase the effectiveness of the proposed approaches. The proposed workflows can regulate color appearance (e.g. with alternative renditions and grayscale conversion) and image contrast, be part of automated image processing pipelines (e.g. isolating stain components, boosting learning models), and assist in digital pathology applications (e.g. enhancing biomarker visibility, enabling colorblind-friendly renditions). Employing only basic arithmetic and matrix operations, this work offers a computationally accessible methodology - in the hypercomplex domain - that showcases versatility and consistency across image processing tasks and a range of computer vision and biomedical applications. The proposed non-data-driven methods achieve comparable or better results (particularly in cases involving well-known methods) to those reported in the literature, showcasing the potential of robust theoretical frameworks with practical effectiveness. Results, methods, and limitations are detailed alongside discussion of promising extensions, emphasizing the potential of feature-rich mathematical/computational frameworks for natural and biomedical images.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instance Segmentation of Scene Sketches Using Natural Image Priors</title>
<link>https://arxiv.org/abs/2502.09608</link>
<guid>https://arxiv.org/abs/2502.09608</guid>
<content:encoded><![CDATA[
arXiv:2502.09608v2 Announce Type: replace 
Abstract: Sketch segmentation involves grouping pixels within a sketch that belong to the same object or instance. It serves as a valuable tool for sketch editing tasks, such as moving, scaling, or removing specific components. While image segmentation models have demonstrated remarkable capabilities in recent years, sketches present unique challenges for these models due to their sparse nature and wide variation in styles. We introduce InkLayer, a method for instance segmentation of raster scene sketches. Our approach adapts state-of-the-art image segmentation and object detection models to the sketch domain by employing class-agnostic fine-tuning and refining segmentation masks using depth cues. Furthermore, our method organizes sketches into sorted layers, where occluded instances are inpainted, enabling advanced sketch editing applications. As existing datasets in this domain lack variation in sketch styles, we construct a synthetic scene sketch segmentation dataset, InkScenes, featuring sketches with diverse brush strokes and varying levels of detail. We use this dataset to demonstrate the robustness of our approach.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement</title>
<link>https://arxiv.org/abs/2502.17648</link>
<guid>https://arxiv.org/abs/2502.17648</guid>
<content:encoded><![CDATA[
arXiv:2502.17648v5 Announce Type: replace 
Abstract: Accurate multi-sensor calibration is essential for deploying robust perception systems in applications such as autonomous driving and intelligent transportation. Existing LiDAR-camera calibration methods often rely on manually placed targets, preliminary parameter estimates, or intensive data preprocessing, limiting their scalability and adaptability in real-world settings. In this work, we propose a fully automatic, targetless, and online calibration framework, CalibRefine, which directly processes raw LiDAR point clouds and camera images. Our approach is divided into four stages: (1) a Common Feature Discriminator that leverages relative spatial positions, visual appearance embeddings, and semantic class cues to identify and generate reliable LiDAR-camera correspondences, (2) a coarse homography-based calibration that uses the matched feature correspondences to estimate an initial transformation between the LiDAR and camera frames, serving as the foundation for further refinement, (3) an iterative refinement to incrementally improve alignment as additional data frames become available, and (4) an attention-based refinement that addresses non-planar distortions by leveraging a Vision Transformer and cross-attention mechanisms. Extensive experiments on two urban traffic datasets demonstrate that CalibRefine achieves high-precision calibration with minimal human input, outperforming state-of-the-art targetless methods and matching or surpassing manually tuned baselines. Our results show that robust object-level feature matching, combined with iterative refinement and self-supervised attention-based refinement, enables reliable sensor alignment in complex real-world conditions without ground-truth matrices or elaborate preprocessing. Code is available at https://github.com/radar-lab/Lidar_Camera_Automatic_Calibration
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers</title>
<link>https://arxiv.org/abs/2503.03307</link>
<guid>https://arxiv.org/abs/2503.03307</guid>
<content:encoded><![CDATA[
arXiv:2503.03307v2 Announce Type: replace 
Abstract: For event cameras, current sparse geometric solvers for egomotion estimation assume that the rotational displacements are known, such as those provided by an IMU. Thus, they can only recover the translational motion parameters. Recovering full-DoF motion parameters using a sparse geometric solver is a more challenging task, and has not yet been investigated. In this paper, we propose several solvers to estimate both rotational and translational velocities within a unified framework. Our method leverages event manifolds induced by line segments. The problem formulations are based on either an incidence relation for lines or a novel coplanarity relation for normal vectors. We demonstrate the possibility of recovering full-DoF egomotion parameters for both angular and linear velocities without requiring extra sensor measurements or motion priors. To achieve efficient optimization, we exploit the Adam framework with a first-order approximation of rotations for quick initialization. Experiments on both synthetic and real-world data demonstrate the effectiveness of our method. The code is available at https://github.com/jizhaox/relpose-event.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction</title>
<link>https://arxiv.org/abs/2503.15661</link>
<guid>https://arxiv.org/abs/2503.15661</guid>
<content:encoded><![CDATA[
arXiv:2503.15661v2 Announce Type: replace 
Abstract: Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate tasks like document editing and file management can greatly enhance computer workflows. While existing research focuses on online settings, desktop environments, critical for many professional and everyday tasks, remain underexplored due to data collection challenges and licensing issues. We introduce UI-Vision, the first comprehensive, license-permissive benchmark for offline, fine-grained evaluation of computer use agents in real-world desktop environments. Unlike online benchmarks, UI-Vision provides: (i) dense, high-quality annotations of human demonstrations, including bounding boxes, UI labels, and action trajectories (clicks, drags, and keyboard inputs) across 83 software applications, and (ii) three fine-to-coarse grained tasks-Element Grounding, Layout Grounding, and Action Prediction-with well-defined metrics to rigorously evaluate agents' performance in desktop environments. Our evaluation reveals critical limitations in state-of-the-art models like UI-TARS-72B, including issues with understanding professional software, spatial reasoning, and complex actions like drag-and-drop. These findings highlight the challenges in developing fully autonomous computer use agents. By releasing UI-Vision as open-source, we aim to advance the development of more capable agents for real-world desktop tasks.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2504.11739</link>
<guid>https://arxiv.org/abs/2504.11739</guid>
<content:encoded><![CDATA[
arXiv:2504.11739v2 Announce Type: replace 
Abstract: The evolution of Text-to-video (T2V) generative models, trained on large-scale datasets, has been marked by significant progress. However, the sensitivity of T2V generative models to input prompts highlights the critical role of prompt design in influencing generative outcomes. Prior research has predominantly relied on Large Language Models (LLMs) to align user-provided prompts with the distribution of training prompts, albeit without tailored guidance encompassing prompt vocabulary and sentence structure nuances. To this end, we introduce RAPO, a novel Retrieval-Augmented Prompt Optimization framework. In order to address potential inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO refines the naive prompts through dual optimization branches, selecting the superior prompt for T2V generation. The first branch augments user prompts with diverse modifiers extracted from a learned relational graph, refining them to align with the format of training prompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive prompt using a pre-trained LLM following a well-defined instruction set. Extensive experiments demonstrate that RAPO can effectively enhance both the static and dynamic dimensions of generated videos, demonstrating the significance of prompt optimization for user-provided prompts.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization</title>
<link>https://arxiv.org/abs/2504.13460</link>
<guid>https://arxiv.org/abs/2504.13460</guid>
<content:encoded><![CDATA[
arXiv:2504.13460v3 Announce Type: replace 
Abstract: Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model's ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DreamO: A Unified Framework for Image Customization</title>
<link>https://arxiv.org/abs/2504.16915</link>
<guid>https://arxiv.org/abs/2504.16915</guid>
<content:encoded><![CDATA[
arXiv:2504.16915v2 Announce Type: replace 
Abstract: Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different types of condition. Developing a unified framework for image customization remains an open challenge. In this paper, we present DreamO, an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions. Specifically, DreamO utilizes a diffusion transformer (DiT) framework to uniformly process input of different types. During training, we construct a large-scale training dataset that includes various customization tasks, and we introduce a feature routing constraint to facilitate the precise querying of relevant information from reference images. Additionally, we design a placeholder strategy that associates specific placeholders with conditions at particular positions, enabling control over the placement of conditions in the generated results. Moreover, we employ a progressive training strategy consisting of three stages: an initial stage focused on simple tasks with limited data to establish baseline consistency, a full-scale training stage to comprehensively enhance the customization capabilities, and a final quality alignment stage to correct quality biases introduced by low-quality data. Extensive experiments demonstrate that the proposed DreamO can effectively perform various image customization tasks with high quality and flexibly integrate different types of control conditions.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step1X-Edit: A Practical Framework for General Image Editing</title>
<link>https://arxiv.org/abs/2504.17761</link>
<guid>https://arxiv.org/abs/2504.17761</guid>
<content:encoded><![CDATA[
arXiv:2504.17761v3 Announce Type: replace 
Abstract: In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLUM: Improving Inference Efficiency By Leveraging Repetition-Sparsity Trade-Off</title>
<link>https://arxiv.org/abs/2312.01581</link>
<guid>https://arxiv.org/abs/2312.01581</guid>
<content:encoded><![CDATA[
arXiv:2312.01581v2 Announce Type: replace-cross 
Abstract: Efficient inference of Deep Neural Networks (DNNs) on resource-constrained edge devices is essential. Quantization and sparsity are key techniques that translate to repetition and sparsity within tensors at the hardware-software interface. This paper introduces the concept of repetition-sparsity trade-off that helps explain computational efficiency during inference. We propose PLUM, a unified co-design framework that integrates DNN inference systems and quantization (forward and backward pass) to leverage the repetition-sparsity trade-off to improve inference efficiency. Our results demonstrate that PLUM's quantization method is more accurate than binary quantization with the same number of non-zero weights. Detailed analysis indicates that signed binarization generates a smaller distribution of effectual (non-zero) parameters nested within a larger distribution of total parameters of latent full-precision weights for a DNN block. Finally, the proposed PLUM framework achieves a 26% speedup on real hardware, doubles energy efficiency, and reduces density by 2.8x compared to binary methods while retaining top-1 accuracy when compared to prior-art methods for ResNets on ImageNet (by achieving 66.2% top-1 accuracy), presenting an alternative solution for deploying efficient models in resource-limited environments.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Momentum-SAM: Sharpness Aware Minimization without Computational Overhead</title>
<link>https://arxiv.org/abs/2401.12033</link>
<guid>https://arxiv.org/abs/2401.12033</guid>
<content:encoded><![CDATA[
arXiv:2401.12033v2 Announce Type: replace-cross 
Abstract: The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss. While significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam. We evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization. Code is available at https://github.com/MarlonBecker/MSAM.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records</title>
<link>https://arxiv.org/abs/2409.07012</link>
<guid>https://arxiv.org/abs/2409.07012</guid>
<content:encoded><![CDATA[
arXiv:2409.07012v2 Announce Type: replace-cross 
Abstract: Chest X-ray (CXR) is an important diagnostic tool widely used in hospitals to assess patient conditions and monitor changes over time. Recently, generative models, specifically diffusion-based models, have shown promise in generating realistic synthetic CXRs. However, these models mainly focus on conditional generation using single-time-point data, i.e., generating CXRs conditioned on their corresponding reports from a specific time. This limits their clinical utility, particularly for capturing temporal changes. To address this limitation, we propose a novel framework, EHRXDiff, which predicts future CXR images by integrating previous CXRs with subsequent medical events, e.g., prescriptions, lab measures, etc. Our framework dynamically tracks and predicts disease progression based on a latent diffusion model, conditioned on the previous CXR image and a history of medical events. We comprehensively evaluate the performance of our framework across three key aspects, including clinical consistency, demographic consistency, and visual realism. Results show that our framework generates high-quality, realistic future images that effectively capture potential temporal changes. This suggests that our framework could be further developed to support clinical decision-making and provide valuable insights for patient monitoring and treatment planning in the medical field. The code is available at https://github.com/dek924/EHRXDiff.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Meta-Learning from a Learning Lens</title>
<link>https://arxiv.org/abs/2409.08474</link>
<guid>https://arxiv.org/abs/2409.08474</guid>
<content:encoded><![CDATA[
arXiv:2409.08474v3 Announce Type: replace-cross 
Abstract: Meta-learning seeks to learn a well-generalized model initialization from training tasks to solve unseen tasks. From the "learning to learn" perspective, the quality of the initialization is modeled with one-step gradient decent in the inner loop. However, contrary to theoretical expectations, our empirical analysis reveals that this may expose meta-learning to underfitting. To bridge the gap between theoretical understanding and practical implementation, we reconsider meta-learning from the "Learning" lens. We propose that the meta-learning model comprises two interrelated components: parameters for model initialization and a meta-layer for task-specific fine-tuning. These components will lead to the risks of overfitting and underfitting depending on tasks, and their solutions, fewer parameters vs. more meta-layer, are often in conflict. To address this, we aim to regulate the task information the model receives without modifying the data or model structure. Our theoretical analysis indicates that models adapted to different tasks can mutually reinforce each other, highlighting the effective information. Based on this insight, we propose TRLearner, a plug-and-play method that leverages task relation to calibrate meta-learning. It first extracts task relation matrices and then applies relation-aware consistency regularization to guide optimization. Extensive theoretical and empirical evaluations demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Autonomous Virtualization System for Two-Dimensional Semiconductor Quantum Dot Arrays</title>
<link>https://arxiv.org/abs/2411.12516</link>
<guid>https://arxiv.org/abs/2411.12516</guid>
<content:encoded><![CDATA[
arXiv:2411.12516v2 Announce Type: replace-cross 
Abstract: Arrays of gate-defined semiconductor quantum dots are among the leading candidates for building scalable quantum processors. High-fidelity initialization, control, and readout of spin qubit registers require exquisite and targeted control over key Hamiltonian parameters that define the electrostatic environment. However, due to the tight gate pitch, capacitive crosstalk between gates hinders independent tuning of chemical potentials and interdot couplings. While virtual gates offer a practical solution, determining all the required cross-capacitance matrices accurately and efficiently in large quantum dot registers is an open challenge. Here, we establish a modular automated virtualization system (MAViS) -- a general and modular framework for autonomously constructing a complete stack of multilayer virtual gates in real time. Our method employs machine learning techniques to rapidly extract features from two-dimensional charge stability diagrams. We then utilize computer vision and regression models to self-consistently determine all relative capacitive couplings necessary for virtualizing plunger and barrier gates in both low- and high-tunnel-coupling regimes. Using MAViS, we successfully demonstrate accurate virtualization of a dense two-dimensional array comprising ten quantum dots defined in a high-quality Ge/SiGe heterostructure. Our work offers an elegant and practical solution for the efficient control of large-scale semiconductor quantum dot systems.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator for Exposure Correction</title>
<link>https://arxiv.org/abs/2411.15255</link>
<guid>https://arxiv.org/abs/2411.15255</guid>
<content:encoded><![CDATA[
arXiv:2411.15255v2 Announce Type: replace-cross 
Abstract: Exposure correction is a fundamental problem in computer vision and image processing. Recently, frequency domain-based methods have achieved impressive improvement, yet they still struggle with complex real-world scenarios under extreme exposure conditions. This is due to the local convolutional receptive fields failing to model long-range dependencies in the spectrum, and the non-generative learning paradigm being inadequate for retrieving lost details from severely degraded regions. In this paper, we propose Omnidirectional Spectral Mamba (OSMamba), a novel exposure correction network that incorporates the advantages of state space models and generative diffusion models to address these limitations. Specifically, OSMamba introduces an omnidirectional spectral scanning mechanism that adapts Mamba to the frequency domain to capture comprehensive long-range dependencies in both the amplitude and phase spectra of deep image features, hence enhancing illumination correction and structure recovery. Furthermore, we develop a dual-domain prior generator that learns from well-exposed images to generate a degradation-free diffusion prior containing correct information about severely under- and over-exposed regions for better detail restoration. Extensive experiments on multiple-exposure and mixed-exposure datasets demonstrate that the proposed OSMamba achieves state-of-the-art performance both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Editable-DeepSC: Reliable Cross-Modal Semantic Communications for Facial Editing</title>
<link>https://arxiv.org/abs/2411.15702</link>
<guid>https://arxiv.org/abs/2411.15702</guid>
<content:encoded><![CDATA[
arXiv:2411.15702v2 Announce Type: replace-cross 
Abstract: Real-time computer vision (CV) plays a crucial role in various real-world applications, whose performance is highly dependent on communication networks. Nonetheless, the data-oriented characteristics of conventional communications often do not align with the special needs of real-time CV tasks. To alleviate this issue, the recently emerged semantic communications only transmit task-related semantic information and exhibit a promising landscape to address this problem. However, the communication challenges associated with Semantic Facial Editing, one of the most important real-time CV applications on social media, still remain largely unexplored. In this paper, we fill this gap by proposing Editable-DeepSC, a novel cross-modal semantic communication approach for facial editing. Firstly, we theoretically discuss different transmission schemes that separately handle communications and editings, and emphasize the necessity of Joint Editing-Channel Coding (JECC) via iterative attributes matching, which integrates editings into the communication chain to preserve more semantic mutual information. To compactly represent the high-dimensional data, we leverage inversion methods via pre-trained StyleGAN priors for semantic coding. To tackle the dynamic channel noise conditions, we propose SNR-aware channel coding via model fine-tuning. Extensive experiments indicate that Editable-DeepSC can achieve superior editings while significantly saving the transmission bandwidth, even under high-resolution and out-of-distribution (OOD) settings.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSynthCT-Brain: A Federated Learning Framework for Multi-Institutional Brain MRI-to-CT Synthesis</title>
<link>https://arxiv.org/abs/2412.06690</link>
<guid>https://arxiv.org/abs/2412.06690</guid>
<content:encoded><![CDATA[
arXiv:2412.06690v2 Announce Type: replace-cross 
Abstract: The generation of Synthetic Computed Tomography (sCT) images has become a pivotal methodology in modern clinical practice, particularly in the context of Radiotherapy (RT) treatment planning. The use of sCT enables the calculation of doses, pushing towards Magnetic Resonance Imaging (MRI) guided radiotherapy treatments. Deep learning methods for MRI-to-sCT have shown promising results, but their reliance on single-centre training dataset limits generalisation capabilities to diverse clinical settings. Moreover, creating centralised multi-centre datasets may pose privacy concerns. To address the aforementioned issues, we introduced FedSynthCT-Brain, an approach based on the Federated Learning (FL) paradigm for MRI-to-sCT in brain imaging. This is among the first applications of FL for MRI-to-sCT, employing a cross-silo horizontal FL approach that allows multiple centres to collaboratively train a U-Net-based deep learning model. We validated our method using real multicentre data from four European and American centres, simulating heterogeneous scanner types and acquisition modalities, and tested its performance on an independent dataset from a centre outside the federation. In the case of the unseen centre, the federated model achieved a median Mean Absolute Error (MAE) of $102.0$ HU across 23 patients, with an interquartile range of $96.7-110.5$ HU. The median (interquartile range) for the Structural Similarity Index (SSIM) and the Peak Signal to Noise Ratio (PNSR) were $0.89 (0.86-0.89)$ and $26.58 (25.52-27.42)$, respectively. The analysis of the results showed acceptable performances of the federated approach, thus highlighting the potential of FL to enhance MRI-to-sCT to improve generalisability and advancing safe and equitable clinical applications while fostering collaboration and preserving data privacy.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Liver Cirrhosis Stage Estimation from MRI with Deep Learning</title>
<link>https://arxiv.org/abs/2502.18225</link>
<guid>https://arxiv.org/abs/2502.18225</guid>
<content:encoded><![CDATA[
arXiv:2502.18225v2 Announce Type: replace-cross 
Abstract: We present an end-to-end deep learning framework for automated liver cirrhosis stage estimation from multi-sequence MRI. Cirrhosis is the severe scarring (fibrosis) of the liver and a common endpoint of various chronic liver diseases. Early diagnosis is vital to prevent complications such as decompensation and cancer, which significantly decreases life expectancy. However, diagnosing cirrhosis in its early stages is challenging, and patients often present with life-threatening complications. Our approach integrates multi-scale feature learning with sequence-specific attention mechanisms to capture subtle tissue variations across cirrhosis progression stages. Using CirrMRI600+, a large-scale publicly available dataset of 628 high-resolution MRI scans from 339 patients, we demonstrate state-of-the-art performance in three-stage cirrhosis classification. Our best model achieves 72.8% accuracy on T1W and 63.8% on T2W sequences, significantly outperforming traditional radiomics-based approaches. Through extensive ablation studies, we show that our architecture effectively learns stage-specific imaging biomarkers. We establish new benchmarks for automated cirrhosis staging and provide insights for developing clinically applicable deep learning systems. The source code will be available at https://github.com/JunZengz/CirrhosisStage.
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases</title>
<link>https://arxiv.org/abs/2504.07606</link>
<guid>https://arxiv.org/abs/2504.07606</guid>
<content:encoded><![CDATA[
arXiv:2504.07606v2 Announce Type: replace-cross 
Abstract: Heart diseases constitute the main cause of international human defunction. According to the World Health Organization (WHO), approximately 18 million deaths happen each year due to precisely heart diseases. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid, and effective prediction. This work presents an automatic system based on a novel deep learning framework which analyses in real-time echocardiography video sequences for the challenging and more specific task of heart failure time prediction. This system works in two stages. The first one transforms the data from a database of echocardiography video sequences into a machine learning-compatible collection of annotated images which can be used in the training phase of any machine learning-based framework, including a deep learning-based one. This stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage builds and trains a Vision Transformer (ViT). Self-supervised learning (SSL) methods, so far barely explored in the literature about heart failure prediction, are adopted to effectively train the ViT from scratch, even with scarce databases. The designed neural network analyses images from echocardiography sequences to estimate the time in which a heart failure will happen. The results obtained show the efficacy of the HODMD algorithm and the superiority of the proposed system with respect to several established ViT and Convolutional Neural Network (CNN) architectures. The source code will be incorporated into the next version release of the ModelFLOWs-app software (https://github.com/modelflows/ModelFLOWs-app).
]]></content:encoded>
<pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects</title>
<link>https://arxiv.org/abs/2504.18468</link>
<guid>https://arxiv.org/abs/2504.18468</guid>
<content:encoded><![CDATA[
<div> Gaussian surfel representation, inverse rendering, relighting, deferred shading pipeline, high-quality reconstruction

Summary:
RGS-DR introduces a novel method for inverse rendering of glossy and reflective objects, addressing view-dependent issues by utilizing a 2D Gaussian surfel representation for accurate geometry and normals estimation. The approach incorporates learnable primitives in a deferred shading pipeline to reduce rendering artifacts and preserve sharp reflections. The use of multi-level cube mipmaps aids in approximating environment lighting integrals for high-quality reconstruction and relighting. Additionally, a residual pass with spherical-mipmap-based directional encoding refines appearance modeling. Experimentation shows that RGS-DR excels in reconstructing and rendering shiny objects, surpassing existing methods in quality and allowing for relighting capabilities. <br /><br />Summary: <div>
arXiv:2504.18468v3 Announce Type: replace 
Abstract: We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D Gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. Our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. By employing a multi-level cube mipmap, RGS-DR accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. A residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-party Collaborative Attention Control for Image Customization</title>
<link>https://arxiv.org/abs/2505.01428</link>
<guid>https://arxiv.org/abs/2505.01428</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, image generation, attention control, subject localization, customization

Summary:
MCA-Ctrl is introduced as a method for high-quality image customization using text and visual conditions. It addresses limitations of current methods by allowing customization with both text and complex visual conditions, coordinating multiple diffusion processes and guiding image generation. MCA-Ctrl captures specific subjects' content and appearance while maintaining semantic consistency. A Subject Localization Module is introduced to extract precise subject and editable image layers based on user instructions, reducing subject leakage and confusion in complex visual scenarios. Extensive experiments show MCA-Ctrl outperforms existing methods in zero-shot image customization, resolving issues related to inconsistent backgrounds and high computational costs. MCA-Ctrl offers a tuning-free approach for customized image generation with improved quality and efficiency.<br /><br />Summary: <div>
arXiv:2505.01428v1 Announce Type: new 
Abstract: The rapid advancement of diffusion models has increased the need for customized image generation. However, current customization methods face several limitations: 1) typically accept either image or text conditions alone; 2) customization in complex visual scenarios often leads to subject leakage or confusion; 3) image-conditioned outputs tend to suffer from inconsistent backgrounds; and 4) high computational costs. To address these issues, this paper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a tuning-free method that enables high-quality image customization using both text and complex visual conditions. Specifically, MCA-Ctrl leverages two key operations within the self-attention layer to coordinate multiple parallel diffusion processes and guide the target image generation. This approach allows MCA-Ctrl to capture the content and appearance of specific subjects while maintaining semantic consistency with the conditional input. Additionally, to mitigate subject leakage and confusion issues common in complex visual scenarios, we introduce a Subject Localization Module that extracts precise subject and editable image layers based on user instructions. Extensive quantitative and human evaluation experiments show that MCA-Ctrl outperforms existing methods in zero-shot image customization, effectively resolving the mentioned issues.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable AI-Driven Detection of Human Monkeypox Using Deep Learning and Vision Transformers: A Comprehensive Analysis</title>
<link>https://arxiv.org/abs/2505.01429</link>
<guid>https://arxiv.org/abs/2505.01429</guid>
<content:encoded><![CDATA[
<div> zoonotic viral illness, mpox, deep learning, medical imaging, transfer learning <br />
Summary: <br />Since mpox is a zoonotic viral illness with symptoms similar to measles and chickenpox, early clinical diagnosis is challenging. This study explores the use of deep learning and vision transformer-based models to analyze skin lesion images for disease detection. Training models from scratch with limited datasets proved to be a drawback, leading to the use of transfer learning with pre-trained models for better classification. The MobileNet-v2 model showed the highest accuracy at 93.15%, followed by ViT B16 at 92.12% and ResNet-50 at 86.21%. Explainable AI techniques were used to validate model performance, demonstrating the potential of using deep learning in medical image analysis for disease detection and diagnosis. <br /> <div>
arXiv:2505.01429v1 Announce Type: new 
Abstract: Since mpox can spread from person to person, it is a zoonotic viral illness that poses a significant public health concern. It is difficult to make an early clinical diagnosis because of how closely its symptoms match those of measles and chickenpox. Medical imaging combined with deep learning (DL) techniques has shown promise in improving disease detection by analyzing affected skin areas. Our study explore the feasibility to train deep learning and vision transformer-based models from scratch with publicly available skin lesion image dataset. Our experimental results show dataset limitation as a major drawback to build better classifier models trained from scratch. We used transfer learning with the help of pre-trained models to get a better classifier. The MobileNet-v2 outperformed other state of the art pre-trained models with 93.15% accuracy and 93.09% weighted average F1 score. ViT B16 and ResNet-50 also achieved satisfactory performance compared to already available studies with accuracy 92.12% and 86.21% respectively. To further validate the performance of the models, we applied explainable AI techniques.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models</title>
<link>https://arxiv.org/abs/2505.01430</link>
<guid>https://arxiv.org/abs/2505.01430</guid>
<content:encoded><![CDATA[
<div> metric, cultural biases, image generation, data imbalance, AI systems
Summary:
The paper introduces the Component Inclusion Score (CIS) as a metric to evaluate cultural biases in text-to-image (T2I) models. Through analysis of 2,400 images, biases in compositional fragility and contextual misalignment for Western and non-Western cultural prompts are identified. The study highlights the impact of data imbalance, attention entropy, and embedding superposition on model fairness. By benchmarking models like Stable Diffusion with CIS, potential interventions for enhancing cultural inclusivity in AI-generated imagery are explored. The research aims to diagnose and mitigate biases in T2I generation, advocating for more equitable AI systems. <div>
arXiv:2505.01430v1 Announce Type: new 
Abstract: The transformative potential of text-to-image (T2I) models hinges on their ability to synthesize culturally diverse, photorealistic images from textual prompts. However, these models often perpetuate cultural biases embedded within their training data, leading to systemic misrepresentations. This paper benchmarks the Component Inclusion Score (CIS), a metric designed to evaluate the fidelity of image generation across cultural contexts. Through extensive analysis involving 2,400 images, we quantify biases in terms of compositional fragility and contextual misalignment, revealing significant performance gaps between Western and non-Western cultural prompts. Our findings underscore the impact of data imbalance, attention entropy, and embedding superposition on model fairness. By benchmarking models like Stable Diffusion with CIS, we provide insights into architectural and data-centric interventions for enhancing cultural inclusivity in AI-generated imagery. This work advances the field by offering a comprehensive tool for diagnosing and mitigating biases in T2I generation, advocating for more equitable AI systems.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZS-VCOS: Zero-Shot Outperforms Supervised Video Camouflaged Object Segmentation</title>
<link>https://arxiv.org/abs/2505.01431</link>
<guid>https://arxiv.org/abs/2505.01431</guid>
<content:encoded><![CDATA[
<div> Keywords: camouflaged object segmentation, zero-shot approach, optical flow, vision-language model, MoCA-Mask dataset 

Summary:
Camouflaged object segmentation is a challenging task due to the similarity between objects and backgrounds. Existing methods have focused on supervised or unsupervised pre-training, neglecting zero-shot approaches. This study proposes a novel method that integrates optical flow, vision-language models, and the Segment Anything Model (SAM) 2 in a sequential pipeline. The approach achieves significant performance improvements on the MoCA-Mask dataset, surpassing existing zero-shot and supervised methods in terms of F-measure. Evaluation on the MoCA-Filter dataset also demonstrates superior results compared to the FlowSAM method. A thorough ablation study confirms the individual contributions of each component. The proposed method is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2505.01431v1 Announce Type: new 
Abstract: Camouflaged object segmentation presents unique challenges compared to traditional segmentation tasks, primarily due to the high similarity in patterns and colors between camouflaged objects and their backgrounds. Effective solutions to this problem have significant implications in critical areas such as pest control, defect detection, and lesion segmentation in medical imaging. Prior research has predominantly emphasized supervised or unsupervised pre-training methods, leaving zero-shot approaches significantly underdeveloped. Existing zero-shot techniques commonly utilize the Segment Anything Model (SAM) in automatic mode or rely on vision-language models to generate cues for segmentation; however, their performances remain unsatisfactory, likely due to the similarity of the camouflaged object and the background. Optical flow, commonly utilized for detecting moving objects, has demonstrated effectiveness even with camouflaged entities. Our method integrates optical flow, a vision-language model, and SAM 2 into a sequential pipeline. Evaluated on the MoCA-Mask dataset, our approach achieves outstanding performance improvements, significantly outperforming existing zero-shot methods by raising the F-measure ($F_\beta^w$) from 0.296 to 0.628. Remarkably, our approach also surpasses supervised methods, increasing the F-measure from 0.476 to 0.628. Additionally, evaluation on the MoCA-Filter dataset demonstrates an increase in the success rate from 0.628 to 0.697 when compared with FlowSAM, a supervised transfer method. A thorough ablation study further validates the individual contributions of each component. More details can be found on https://github.com/weathon/vcos.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos</title>
<link>https://arxiv.org/abs/2505.01481</link>
<guid>https://arxiv.org/abs/2505.01481</guid>
<content:encoded><![CDATA[
<div> VideoHallu, synthetic video generation, foundation models, common sense, physical laws <br />
Summary:
The article introduces VideoHallu, a benchmark for evaluating synthetic videos generated by foundation models like Veo2, Sora, and Kling. Existing metrics like VideoScore do not capture abnormalities in synthetic videos. Multi-modal large language models (MLLMs) are used as interpretable evaluators to detect violations of common sense and physical laws. State-of-the-art MLLMs like GPT-4o and Gemini-2.5-Pro still struggle with basic reasoning tasks in synthetic videos. Fine-tuning these models using Group Relative Policy Optimization (GRPO) on real and synthetic data improves their accuracy, especially with the integration of counterexamples. The study highlights the challenge of hallucination in synthetic video generation and showcases the potential for enhancing MLLMs' reasoning abilities in this context.<br /><br />Summary: <div>
arXiv:2505.01481v1 Announce Type: new 
Abstract: Synthetic video generation with foundation models has gained attention for its realism and wide applications. While these models produce high-quality frames, they often fail to respect common sense and physical laws, resulting in abnormal content. Existing metrics like VideoScore emphasize general quality but ignore such violations and lack interpretability. A more insightful approach is using multi-modal large language models (MLLMs) as interpretable evaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities in synthetic videos remains underexplored. To address this, we introduce VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora, and Kling, paired with expert-designed QA tasks solvable via human-level reasoning across various categories. We assess several SoTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat, these models still hallucinate on basic commonsense and physics tasks in synthetic settings, underscoring the challenge of hallucination. We further fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real and synthetic commonsense/physics data. Results show notable accuracy gains, especially with counterexample integration, advancing MLLMs' reasoning capabilities. Our data is available at https://github.com/zli12321/VideoHallu.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldGenBench: A World-Knowledge-Integrated Benchmark for Reasoning-Driven Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.01490</link>
<guid>https://arxiv.org/abs/2505.01490</guid>
<content:encoded><![CDATA[
<div> Keyword: text-to-image generation, world knowledge, implicit reasoning, benchmark, diffusion models<br />
Summary:<br />
The article introduces a new benchmark called WorldGenBench for evaluating text-to-image generation models based on their world knowledge grounding and implicit inferential capabilities. The benchmark covers various domains such as humanities and nature. The Knowledge Checklist Score is proposed as a metric to measure how well generated images meet key semantic expectations. Experiments involving 21 state-of-the-art models show that diffusion models perform well among open-source methods, but proprietary auto-regressive models like GPT-4o show stronger reasoning and knowledge integration. The findings emphasize the importance of deeper understanding and inference capabilities in enhancing the performance of text-to-image generation systems.<br /> <div>
arXiv:2505.01490v1 Announce Type: new 
Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive results, yet existing models still struggle with prompts that require rich world knowledge and implicit reasoning: both of which are critical for producing semantically accurate, coherent, and contextually appropriate images in real-world scenarios. To address this gap, we introduce \textbf{WorldGenBench}, a benchmark designed to systematically evaluate T2I models' world knowledge grounding and implicit inferential capabilities, covering both the humanities and nature domains. We propose the \textbf{Knowledge Checklist Score}, a structured metric that measures how well generated images satisfy key semantic expectations. Experiments across 21 state-of-the-art models reveal that while diffusion models lead among open-source methods, proprietary auto-regressive models like GPT-4o exhibit significantly stronger reasoning and knowledge integration. Our findings highlight the need for deeper understanding and inference capabilities in next-generation T2I systems. Project Page: \href{https://dwanzhang-ai.github.io/WorldGenBench/}{https://dwanzhang-ai.github.io/WorldGenBench/}
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer</title>
<link>https://arxiv.org/abs/2505.01530</link>
<guid>https://arxiv.org/abs/2505.01530</guid>
<content:encoded><![CDATA[
<div> Keywords: structured information extraction, deep learning, engineering drawings, oriented bounding box detection, transformer-based document parsing<br />
Summary:<br />
- Accurate extraction of key information from engineering drawings is vital for high-precision manufacturing. Manual extraction is time-consuming and error-prone, while traditional OCR techniques face challenges with complex layouts.
- A hybrid deep learning framework is proposed, integrating an OBB detection model with a transformer-based document parsing model for structured information extraction.
- YOLOv11 is trained to detect key categories like GD&amp;T and Tolerances, which are then labeled and used to fine-tune Donut for structured JSON output.
- The single model approach outperforms category-specific models, achieving high precision, recall, and F1 score while reducing hallucination.
- The framework enhances accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.<br />  
Summary: <div>
arXiv:2505.01530v1 Announce Type: new 
Abstract: Accurate extraction of key information from 2D engineering drawings is crucial for high-precision manufacturing. Manual extraction is time-consuming and error-prone, while traditional Optical Character Recognition (OCR) techniques often struggle with complex layouts and overlapping symbols, resulting in unstructured outputs. To address these challenges, this paper proposes a novel hybrid deep learning framework for structured information extraction by integrating an oriented bounding box (OBB) detection model with a transformer-based document parsing model (Donut). An in-house annotated dataset is used to train YOLOv11 for detecting nine key categories: Geometric Dimensioning and Tolerancing (GD&amp;T), General Tolerances, Measures, Materials, Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are cropped into images and labeled to fine-tune Donut for structured JSON output. Fine-tuning strategies include a single model trained across all categories and category-specific models. Results show that the single model consistently outperforms category-specific ones across all evaluation metrics, achieving higher precision (94.77% for GD&amp;T), recall (100% for most), and F1 score (97.3%), while reducing hallucination (5.23%). The proposed framework improves accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation</title>
<link>https://arxiv.org/abs/2505.01548</link>
<guid>https://arxiv.org/abs/2505.01548</guid>
<content:encoded><![CDATA[
<div> Sparse Event Camera, RGB-Event Fusion, Event Representation, Temporal Correlations, Semantic Segmentation
Summary:
The paper introduces a novel approach for RGB-Event fusion using a Motion-enhanced Event Tensor (MET) representation to address temporal, spatial, and modal misalignments. The MET transforms sparse event voxels into a dense and temporally coherent form by incorporating dense optical flows and event temporal features. The proposed Frequency-aware Bidirectional Flow Aggregation Module (BFAM) utilizes the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms handle spatiotemporal misalignment. Experimental results on large-scale datasets demonstrate superior performance compared to existing approaches in RGB-Event semantic segmentation tasks. The code for the framework is publicly available at the provided GitHub repository. 
<br /><br />Summary: <div>
arXiv:2505.01548v1 Announce Type: new 
Abstract: Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning</title>
<link>https://arxiv.org/abs/2505.01558</link>
<guid>https://arxiv.org/abs/2505.01558</guid>
<content:encoded><![CDATA[
<div> satellite technology, remote sensing, segmentation models, domain adaptation, geospatial

Summary:<br />
Remote sensing plays a crucial role in various applications such as land cover mapping and crop yield prediction. However, the accuracy of segmentation models is often limited by the scarcity and variability of labeled data across different sources and conditions. To address this issue, this study proposes a domain generalization approach that combines soft-alignment pseudo-labeling with generative pre-training from the source to the target domain. The research also introduces mathematical insights into generative learning based on Mean Absolute Error for domain-invariant feature learning. Experimental results using hyperspectral and multispectral remote sensing datasets demonstrate the effectiveness of the proposed method in improving adaptability and segmentation accuracy. <div>
arXiv:2505.01558v1 Announce Type: new 
Abstract: Remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. Advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. Domain adaptation offers a promising solution to improve model generalization. This paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. We further provide new mathematical insights into MAE-based generative learning for domain-invariant feature learning. Experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PainFormer: a Vision Foundation Model for Automatic Pain Assessment</title>
<link>https://arxiv.org/abs/2505.01571</link>
<guid>https://arxiv.org/abs/2505.01571</guid>
<content:encoded><![CDATA[
<div> Keywords: PainFormer, multi-task learning, vision foundation model, automatic pain assessment, multimodal inputs

Summary:
PainFormer is introduced as a vision foundation model for automatic pain assessment, based on multi-task learning principles and trained on 14 tasks/datasets with 10.9 million samples. It serves as an embedding extractor for various input modalities, providing feature representations to the Embedding-Mixer for final pain assessment. The model was extensively tested on behavioral and physiological modalities, showing effective extraction of high-quality embeddings. Evaluations on BioVid and AI4Pain datasets compared PainFormer to 73 other methodologies, demonstrating state-of-the-art performance across modalities. The framework shows promise for general-purpose models in pain assessment, aiming to improve monitoring and decision-making processes for pain management protocols. <br /><br />Summary: <div>
arXiv:2505.01571v1 Announce Type: new 
Abstract: Pain is a manifold condition that impacts a significant percentage of the population. Accurate and reliable pain evaluation for the people suffering is crucial to developing effective and advanced pain management protocols. Automatic pain assessment systems provide continuous monitoring and support decision-making processes, ultimately aiming to alleviate distress and prevent functionality decline. This study introduces PainFormer, a vision foundation model based on multi-task learning principles trained simultaneously on 14 tasks/datasets with a total of 10.9 million samples. Functioning as an embedding extractor for various input modalities, the foundation model provides feature representations to the Embedding-Mixer, a transformer-based module that performs the final pain assessment. Extensive experiments employing behavioral modalities-including RGB, synthetic thermal, and estimated depth videos-and physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that PainFormer effectively extracts high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 73 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Task Assistance with Multimodal Cues from a Single Demonstration</title>
<link>https://arxiv.org/abs/2505.01578</link>
<guid>https://arxiv.org/abs/2505.01578</guid>
<content:encoded><![CDATA[
<div> Assistance, Multimodal, Contextual, Gaze, Speech
<br />
Summary: 
The article introduces MICA, a framework for enhancing conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into sub-tasks, extracting keyframes and captions to provide richer contextual grounding for visual question answering. Evaluations show that multimodal cues significantly improve response quality over frame-based retrieval. Gaze cues alone achieve 93% of speech performance, with the combination yielding the highest accuracy. The effectiveness of implicit (gaze) vs. explicit (speech) cues depends on the task type, highlighting the need for adaptable multimodal models. The results showcase the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance. 
<br /> <div>
arXiv:2505.01578v1 Announce Type: new 
Abstract: A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action</title>
<link>https://arxiv.org/abs/2505.01583</link>
<guid>https://arxiv.org/abs/2505.01583</guid>
<content:encoded><![CDATA[
<div> masked event prediction, causal reasoning, video temporal understanding, video segmentation, dense captioning

Summary:
TEMPURA is a new framework designed to improve video temporal understanding by incorporating causal reasoning and fine-grained event segmentation. The two-stage training approach of TEMPURA involves masked event prediction reasoning to reconstruct missing events and generate causal explanations, followed by video segmentation and dense captioning to decompose videos into non-overlapping events with detailed descriptions. Trained on the VER dataset with temporally aligned event descriptions and reasoning steps, TEMPURA outperforms baseline models in temporal grounding and highlight detection benchmarks. By integrating causal reasoning with fine-grained temporal segmentation, TEMPURA demonstrates enhanced video understanding capabilities. <div>
arXiv:2505.01583v1 Announce Type: new 
Abstract: Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation</title>
<link>https://arxiv.org/abs/2505.01615</link>
<guid>https://arxiv.org/abs/2505.01615</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal sensor fusion, autonomous marine navigation, cross attention transformer, LiDAR point clouds, birds eye view <br />
Summary: <br />
The study introduces a novel approach for multimodal sensor fusion using a cross attention transformer to enhance autonomous marine navigation safety. The method combines RGB images, long wave infrared images, and sparse LiDAR point clouds to create a comprehensive birds eye view of a vessel's surroundings. Training incorporates X band radar and electronic chart data for more informed predictions. The resulting view offers a detailed and reliable representation of the scene, improving navigational accuracy and robustness. Real-world sea trials demonstrate the effectiveness of the method, even in challenging weather conditions and complex maritime environments. <div>
arXiv:2505.01615v1 Announce Type: new 
Abstract: We propose a cross attention transformer based method for multimodal sensor fusion to build a birds eye view of a vessels surroundings supporting safer autonomous marine navigation. The model deeply fuses multiview RGB and long wave infrared images with sparse LiDAR point clouds. Training also integrates X band radar and electronic chart data to inform predictions. The resulting view provides a detailed reliable scene representation improving navigational accuracy and robustness. Real world sea trials confirm the methods effectiveness even in adverse weather and complex maritime settings.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Onboard AI-Enabled Solutions to Space Object Detection for Space Sustainability</title>
<link>https://arxiv.org/abs/2505.01650</link>
<guid>https://arxiv.org/abs/2505.01650</guid>
<content:encoded><![CDATA[
<div> Keywords: low-Earth orbit satellites, space object detection, deep learning models, collision assessment, vision sensors<br />
Summary:<br />
The paper delves into the importance of effective space object detection (SOD) for collision assessment and avoidance in the rapidly expanding low-Earth orbit (LEO) satellite field. It explores utilizing vision sensors and deep learning models, such as Squeeze-and-Excitation (SE), Vision Transformer (ViT), and Generalized Efficient Layer Aggregation Network (GELAN), for SOD tasks. Experimental results showcase promising performance metrics, with the proposed models achieving high mean average precision scores. The GELAN-ViT-SE model stands out by enhancing mAP50 and mAP50:95 scores compared to the baseline model, while also reducing GFLOPs and peak power consumption. This research highlights the potential of advanced technologies in enhancing SOD capabilities and optimizing satellite operations for future space missions. <br /><br /> <div>
arXiv:2505.01650v1 Announce Type: new 
Abstract: The rapid expansion of advanced low-Earth orbit (LEO) satellites in large constellations is positioning space assets as key to the future, enabling global internet access and relay systems for deep space missions. A solution to the challenge is effective space object detection (SOD) for collision assessment and avoidance. In SOD, an LEO satellite must detect other satellites and objects with high precision and minimal delay. This paper investigates the feasibility and effectiveness of employing vision sensors for SOD tasks based on deep learning (DL) models. It introduces models based on the Squeeze-and-Excitation (SE) layer, Vision Transformer (ViT), and the Generalized Efficient Layer Aggregation Network (GELAN) and evaluates their performance under SOD scenarios. Experimental results show that the proposed models achieve mean average precision at intersection over union threshold 0.5 (mAP50) scores of up to 0.751 and mean average precision averaged over intersection over union thresholds from 0.5 to 0.95 (mAP50:95) scores of up to 0.280. Compared to the baseline GELAN-t model, the proposed GELAN-ViT-SE model increases the average mAP50 from 0.721 to 0.751, improves the mAP50:95 from 0.266 to 0.274, reduces giga floating point operations (GFLOPs) from 7.3 to 5.6, and lowers peak power consumption from 2080.7 mW to 2028.7 mW by 2.5\%.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel WaveInst-based Network for Tree Trunk Structure Extraction and Pattern Analysis in Forest Inventory</title>
<link>https://arxiv.org/abs/2505.01656</link>
<guid>https://arxiv.org/abs/2505.01656</guid>
<content:encoded><![CDATA[
<div> Keywords: tree structure extraction, instance segmentation, wavelet transform, phenotypic dataset, precision forestry

Summary: 
The study introduces a novel WaveInst instance segmentation framework for accurate tree structure extraction, utilizing a discrete wavelet transform to enhance multi-scale edge information. The method outperforms existing techniques on various datasets, including the newly introduced PoplarDataset for phenotypic analysis of artificial forests. The proposed model achieves superior performance in extracting tree structures, surpassing the state-of-the-art method. By integrating the segmentation model with a regression model, accurate tree growth parameters such as tree location, diameter-at-breast-height, and plant height can be obtained directly from 2D images. This research contributes valuable data for tree structure analysis in phenotype research, with applications in precision forestry, ecological monitoring, and intelligent breeding.<br /><br />Summary: <div>
arXiv:2505.01656v1 Announce Type: new 
Abstract: The pattern analysis of tree structure holds significant scientific value for genetic breeding and forestry management. The current trunk and branch extraction technologies are mainly LiDAR-based or UAV-based. The former approaches obtain high-precision 3D data, but its equipment cost is high and the three-dimensional (3D) data processing is complex. The latter approaches efficiently capture canopy information, but they miss the 3-D structure of trees. In order to deal with the branch information extraction from the complex background interference and occlusion, this work proposes a novel WaveInst instance segmentation framework, involving a discrete wavelet transform, to enhance multi-scale edge information for accurately improving tree structure extraction. Experimental results of the proposed model show superior performance on SynthTree43k, CaneTree100, Urban Street and our PoplarDataset. Moreover, we present a new Phenotypic dataset PoplarDataset, which is dedicated to extract tree structure and pattern analysis from artificial forest. The proposed method achieves a mean average precision of 49.6 and 24.3 for the structure extraction of mature and juvenile trees, respectively, surpassing the existing state-of-the-art method by 9.9. Furthermore, by in tegrating the segmentation model within the regression model, we accurately achieve significant tree grown parameters, such as the location of trees, the diameter-at-breast-height of individual trees, and the plant height, from 2D images directly. This study provides a scientific and plenty of data for tree structure analysis in related to the phenotype research, offering a platform for the significant applications in precision forestry, ecological monitoring, and intelligent breeding.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.01664</link>
<guid>https://arxiv.org/abs/2505.01664</guid>
<content:encoded><![CDATA[
<div> domain adaptation, partial domain adaptation, optimal transport, neural network, class-conditional distribution matching 

Summary: 
The paper introduces a Soft-masked Semi-dual Optimal Transport (SSOT) method to address the challenges of partial domain adaptation (PDA) in visual domain adaptation. The method estimates class weights of domains to construct a reweighed source domain for class-conditional distribution matching with the target domain. A soft-masked transport distance matrix enhances the representation ability of optimal transport in the shared feature space. The semi-dual formulation of the entropy-regularized Kantorovich problem is utilized for large-scale optimal transport problems, optimized using gradient-based algorithms. A neural network is used to approximate the Kantorovich potential, allowing generalization of the dual variable. The SSOT model, based on neural networks, can be optimized in an end-to-end manner. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed SSOT method in partial domain adaptation scenarios. 

<br /><br />Summary: <div>
arXiv:2505.01664v1 Announce Type: new 
Abstract: Visual domain adaptation aims to learn discriminative and domain-invariant representation for an unlabeled target domain by leveraging knowledge from a labeled source domain. Partial domain adaptation (PDA) is a general and practical scenario in which the target label space is a subset of the source one. The challenges of PDA exist due to not only domain shift but also the non-identical label spaces of domains. In this paper, a Soft-masked Semi-dual Optimal Transport (SSOT) method is proposed to deal with the PDA problem. Specifically, the class weights of domains are estimated, and then a reweighed source domain is constructed, which is favorable in conducting class-conditional distribution matching with the target domain. A soft-masked transport distance matrix is constructed by category predictions, which will enhance the class-oriented representation ability of optimal transport in the shared feature space. To deal with large-scale optimal transport problems, the semi-dual formulation of the entropy-regularized Kantorovich problem is employed since it can be optimized by gradient-based algorithms. Further, a neural network is exploited to approximate the Kantorovich potential due to its strong fitting ability. This network parametrization also allows the generalization of the dual variable outside the supports of the input distribution. The SSOT model is built upon neural networks, which can be optimized alternately in an end-to-end manner. Extensive experiments are conducted on four benchmark datasets to demonstrate the effectiveness of SSOT.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study</title>
<link>https://arxiv.org/abs/2505.01680</link>
<guid>https://arxiv.org/abs/2505.01680</guid>
<content:encoded><![CDATA[
<div> Keywords: ARAT, upper extremity assessment, stroke rehabilitation, automated scoring system, multimodal video analysis

Summary:
An automated scoring system for the Action Research Arm Test (ARAT) in stroke rehabilitation has been developed in this study. The system integrates multimodal video analysis using advanced models like SlowFast, I3D, and Transformer-based models, incorporating OpenPose keypoints and object locations. It utilizes multi-view data and applies early and late fusion techniques to combine features across views and models. Hierarchical Bayesian Models (HBMs) are employed to infer movement quality components, enhancing interpretability. The system includes a clinician dashboard displaying task scores, execution times, and quality assessments. Clinicians reviewed the system's generated video ratings and provided feedback on its accuracy and usability. Validation on a stroke rehabilitation dataset showed the framework achieving 89.0% accuracy with late fusion, and the HBMs closely aligned with manual assessments. This work offers a scalable and interpretable solution for automated rehabilitation assessments, advancing the field with clinical validation. 

<br /><br />Summary: <div>
arXiv:2505.01680v1 Announce Type: new 
Abstract: Manual scoring of the Action Research Arm Test (ARAT) for upper extremity assessment in stroke rehabilitation is time-intensive and variable. We propose an automated ARAT scoring system integrating multimodal video analysis with SlowFast, I3D, and Transformer-based models using OpenPose keypoints and object locations. Our approach employs multi-view data (ipsilateral, contralateral, and top perspectives), applying early and late fusion to combine features across views and models. Hierarchical Bayesian Models (HBMs) infer movement quality components, enhancing interpretability. A clinician dashboard displays task scores, execution times, and quality assessments. We conducted a study with five clinicians who reviewed 500 video ratings generated by our system, providing feedback on its accuracy and usability. Evaluated on a stroke rehabilitation dataset, our framework achieves 89.0% validation accuracy with late fusion, with HBMs aligning closely with manual assessments. This work advances automated rehabilitation by offering a scalable, interpretable solution with clinical validation.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topology-Aware CLIP Few-Shot Learning</title>
<link>https://arxiv.org/abs/2505.01694</link>
<guid>https://arxiv.org/abs/2505.01694</guid>
<content:encoded><![CDATA[
<div> Topological information, Vision-Language Models, Few-shot learning, Representation Topology Divergence, Task Residual<br />
<br />
Summary: 
In this new article, the authors propose a topology-aware tuning approach for adapting large Vision-Language Models (VLMs) like CLIP for few-shot learning. By integrating Representation Topology Divergence (RTD) into the Task Residual (TR) framework, they align the topological structures of visual and text representations while freezing base VLM encoders. This method aims to balance pre-trained knowledge retention and task-specific adaptation, optimizing only lightweight Task Residual parameters. Across 6 benchmark datasets, the approach shows significant gains in few-shot settings, with an average accuracy improvement of 1-2% over baseline methods. This strategy effectively boosts VLM few-shot capabilities by incorporating topological alignment.<br /><br />Summary: <div>
arXiv:2505.01694v1 Announce Type: new 
Abstract: Efficiently adapting large Vision-Language Models (VLMs) like CLIP for few-shot learning poses challenges in balancing pre-trained knowledge retention and task-specific adaptation. Existing methods often overlook valuable structural information within the VLM's latent space. We introduce a topology-aware tuning approach integrating Representation Topology Divergence (RTD) into the Task Residual (TR) framework. By explicitly aligning the topological structures of visual and text representations using a combined RTD and Cross-Entropy loss, while freezing base VLM encoders, our method enhances few-shot performance. We optimize only lightweight Task Residual parameters, effectively leveraging topological information. Across 6 diverse benchmark datasets, our approach demonstrates significant gains, achieving an average accuracy improvement of 1-2\% over relevant baseline methods in few-shot settings. This work presents an effective strategy to boost VLM few-shot capabilities by incorporating topological alignment.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning</title>
<link>https://arxiv.org/abs/2505.01699</link>
<guid>https://arxiv.org/abs/2505.01699</guid>
<content:encoded><![CDATA[
<div> Keywords: face recognition, fairness, bias mitigation, Bayesian Network, meta-learning

Summary:
In a new study, researchers focus on face component fairness, a concept that addresses bias in individual biological face features. They introduce a novel approach called Bayesian Network-informed Meta Reweighting (BNMR) to mitigate bias in face attribute prediction at the biological feature level. The study identifies challenges such as attribute label scarcity and inter-dependencies, which limit the effectiveness of bias mitigation. BNMR incorporates a Bayesian Network calibrator to guide a meta-learning-based sample reweighting process, dynamically tracking model bias and encoding prior probabilities for face component attributes. Experimental results show that BNMR outperforms recent bias mitigation baselines and positively impacts demographic fairness. The findings suggest that face component fairness could be a surrogate objective for demographic fairness, opening up new research avenues in this area. The code for the approach is publicly available for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.01699v1 Announce Type: new 
Abstract: The widespread integration of face recognition technologies into various applications (e.g., access control and personalized advertising) necessitates a critical emphasis on fairness. While previous efforts have focused on demographic fairness, the fairness of individual biological face components remains unexplored. In this paper, we focus on face component fairness, a fairness notion defined by biological face features. To our best knowledge, our work is the first work to mitigate bias of face attribute prediction at the biological feature level. In this work, we identify two key challenges in optimizing face component fairness: attribute label scarcity and attribute inter-dependencies, both of which limit the effectiveness of bias mitigation from previous approaches. To address these issues, we propose \textbf{B}ayesian \textbf{N}etwork-informed \textbf{M}eta \textbf{R}eweighting (BNMR), which incorporates a Bayesian Network calibrator to guide an adaptive meta-learning-based sample reweighting process. During the training process of our approach, the Bayesian Network calibrator dynamically tracks model bias and encodes prior probabilities for face component attributes to overcome the above challenges. To demonstrate the efficacy of our approach, we conduct extensive experiments on a large-scale real-world human face dataset. Our results show that BNMR is able to consistently outperform recent face bias mitigation baselines. Moreover, our results suggest a positive impact of face component fairness on the commonly considered demographic fairness (e.g., \textit{gender}). Our findings pave the way for new research avenues on face component fairness, suggesting that face component fairness could serve as a potential surrogate objective for demographic fairness. The code for our work is publicly available~\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings</title>
<link>https://arxiv.org/abs/2505.01711</link>
<guid>https://arxiv.org/abs/2505.01711</guid>
<content:encoded><![CDATA[
<div> Framework, medical imaging, language models, clinical reasoning, dataset 

Summary: 
The paper introduces CXR-TextInter, a framework that utilizes text-centric large language models for automated interpretation of chest X-rays (CXR). By operating on structured textual representations of image content, generated by an image analysis pipeline, the framework enhances clinical reasoning with an integrated medical knowledge module. The development of the MediInstruct-CXR dataset, containing structured image representations paired with diverse instruction-response examples, facilitates training and evaluation. The CXR-TextInter framework achieves state-of-the-art performance in pathology detection, report generation, and visual question answering on the CXR-ClinEval benchmark, surpassing existing multimodal models. Ablation studies confirm the importance of the knowledge integration module, and blinded human evaluation by radiologists shows a preference for the clinical quality of CXR-TextInter outputs. This work validates a novel approach in medical image AI, demonstrating the effectiveness of leveraging advanced large language models when visual information is structured and domain knowledge is integrated.<br /><br /> <div>
arXiv:2505.01711v1 Announce Type: new 
Abstract: Automated interpretation of chest X-rays (CXR) is a critical task with the potential to significantly improve clinical workflow and patient care. While recent advances in multimodal foundation models have shown promise, effectively leveraging the full power of large language models (LLMs) for this visual task remains an underexplored area. This paper introduces CXR-TextInter, a novel framework that repurposes powerful text-centric LLMs for CXR interpretation by operating solely on a rich, structured textual representation of the image content, generated by an upstream image analysis pipeline. We augment this LLM-centric approach with an integrated medical knowledge module to enhance clinical reasoning. To facilitate training and evaluation, we developed the MediInstruct-CXR dataset, containing structured image representations paired with diverse, clinically relevant instruction-response examples, and the CXR-ClinEval benchmark for comprehensive assessment across various interpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that CXR-TextInter achieves state-of-the-art quantitative performance across pathology detection, report generation, and visual question answering, surpassing existing multimodal foundation models. Ablation studies confirm the critical contribution of the knowledge integration module. Furthermore, blinded human evaluation by board-certified radiologists shows a significant preference for the clinical quality of outputs generated by CXR-TextInter. Our work validates an alternative paradigm for medical image AI, showcasing the potential of harnessing advanced LLM capabilities when visual information is effectively structured and domain knowledge is integrated.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision and Intention Boost Large Language Model in Long-Term Action Anticipation</title>
<link>https://arxiv.org/abs/2505.01713</link>
<guid>https://arxiv.org/abs/2505.01713</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-term action anticipation, Vision-language model, Intention-conditioned, Multi-modality fusion, Example selection strategy

Summary:
The study introduces a novel Intention-Conditioned Vision-Language (ICVL) model for long-term action anticipation. By combining visual data with the reasoning capabilities of large language models (LLMs), the ICVL model extracts behavioral intentions from video inputs using a vision-language model. These intentions are fused with visual features to enhance visual representations, which are then inputted into LLM for future action anticipation. Additionally, an effective example selection strategy that considers visual and textual similarities is proposed to improve in-context learning. Experimental results on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+ datasets demonstrate the superiority of the ICVL model in terms of performance. The novel approach addresses limitations of single-modality methods by leveraging the rich semantic information of visual data and the powerful reasoning capabilities of LLMs, thereby enhancing the accuracy of long-term action anticipation. 

<br /><br />Summary: <div>
arXiv:2505.01713v1 Announce Type: new 
Abstract: Long-term action anticipation (LTA) aims to predict future actions over an extended period. Previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. Recent researches leverage large language models (LLMs) by utilizing text-based inputs which suffer severe information loss. To tackle these limitations single-modality methods face, we propose a novel Intention-Conditioned Vision-Language (ICVL) model in this study that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of LLMs. Considering intention as a high-level concept guiding the evolution of actions, we first propose to employ a vision-language model (VLM) to infer behavioral intentions as comprehensive textual features directly from video inputs. The inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. These enhanced visual representations, along with textual prompts, are fed into LLM for future action anticipation. Furthermore, we propose an effective example selection strategy jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. Extensive experiments with state-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+ datasets fully demonstrate the effectiveness and superiority of the proposed method.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes</title>
<link>https://arxiv.org/abs/2505.01726</link>
<guid>https://arxiv.org/abs/2505.01726</guid>
<content:encoded><![CDATA[
<div> probabilistic framework, 3D segmentation, interactive segmentation, Neural Processes, uncertainty estimation
Summary:<br />
- NPISeg3D introduces a probabilistic framework for interactive 3D segmentation that effectively generalizes from sparse user clicks to produce accurate segmentation. <br />
- It incorporates a hierarchical latent variable structure with scene-specific and object-specific latent variables to capture global context and object-specific characteristics for enhanced few-shot generalization. <br />
- The model utilizes a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables to improve object-aware context understanding and quantify predictive uncertainty. <br />
- Experimental results on four 3D point cloud datasets demonstrate that NPISeg3D outperforms existing methods in segmentation performance, requiring fewer clicks while providing reliable uncertainty estimations. <br /> <div>
arXiv:2505.01726v1 Announce Type: new 
Abstract: Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model's ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth</title>
<link>https://arxiv.org/abs/2505.01729</link>
<guid>https://arxiv.org/abs/2505.01729</guid>
<content:encoded><![CDATA[
<div> framework, camera pose control, generative world models, self-supervised depth estimation, viewpoint synthesis <br />
Summary: <br />
The paper introduces PosePilot, a framework that enhances camera pose control in generative world models. It leverages self-supervised depth estimation principles to tightly couple camera pose and video generation. By incorporating self-supervised depth and pose readouts, the model can infer depth and relative camera motion, driving pose-aware frame warping with a photometric warping loss. A reverse warping step and pose regression loss further refine camera pose estimation. PosePilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. It sets a new benchmark for pose controllability by steering camera pose with self-supervised depth, enabling physically consistent and reliable viewpoint synthesis. Extensive experiments on autonomous driving and general-domain video datasets demonstrate the effectiveness of PosePilot in achieving precise and flexible camera pose control. <div>
arXiv:2505.01729v1 Announce Type: new 
Abstract: Recent advancements in autonomous driving (AD) systems have highlighted the potential of world models in achieving robust and generalizable performance across both ordinary and challenging driving conditions. However, a key challenge remains: precise and flexible camera pose control, which is crucial for accurate viewpoint transformation and realistic simulation of scene dynamics. In this paper, we introduce PosePilot, a lightweight yet powerful framework that significantly enhances camera pose controllability in generative world models. Drawing inspiration from self-supervised depth estimation, PosePilot leverages structure-from-motion principles to establish a tight coupling between camera pose and video generation. Specifically, we incorporate self-supervised depth and pose readouts, allowing the model to infer depth and relative camera motion directly from video sequences. These outputs drive pose-aware frame warping, guided by a photometric warping loss that enforces geometric consistency across synthesized frames. To further refine camera pose estimation, we introduce a reverse warping step and a pose regression loss, improving viewpoint precision and adaptability. Extensive experiments on autonomous driving and general-domain video datasets demonstrate that PosePilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. By steering camera pose with self-supervised depth, PosePilot sets a new benchmark for pose controllability, enabling physically consistent, reliable viewpoint synthesis in generative world models.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Multi-frame and Monocular Prior for Estimating Geometry in Dynamic Scenes</title>
<link>https://arxiv.org/abs/2505.01737</link>
<guid>https://arxiv.org/abs/2505.01737</guid>
<content:encoded><![CDATA[
<div> Keywords: monocular videos, dynamic scenes, 3D geometry, feed-forward prediction, Siamese architecture

Summary: 
MMP, a new model for estimating 3D geometry in monocular videos of dynamic scenes, is introduced. Existing models struggle with object motion, providing only partial attributes over a pair of frames. These attributes are often noisy and require global optimizations during test time, leading to potential failure and heavy inference costs. MMP addresses this challenge by predicting geometry in a feed-forward manner, evolving a dynamic pointmap representation over multiple frames. The model incorporates a trajectory encoding module within a Siamese architecture, enhancing expressiveness for dynamic scenes. Experimental results show MMP achieves state-of-the-art quality in feed-forward pointmap prediction, with a 15.1% improvement in regression error. <br /><br />Summary: <div>
arXiv:2505.01737v1 Announce Type: new 
Abstract: In monocular videos that capture dynamic scenes, estimating the 3D geometry of video contents has been a fundamental challenge in computer vision. Specifically, the task is significantly challenged by the object motion, where existing models are limited to predict only partial attributes of the dynamic scenes, such as depth or pointmaps spanning only over a pair of frames. Since these attributes are inherently noisy under multiple frames, test-time global optimizations are often employed to fully recover the geometry, which is liable to failure and incurs heavy inference costs. To address the challenge, we present a new model, coined MMP, to estimate the geometry in a feed-forward manner, which produces a dynamic pointmap representation that evolves over multiple frames. Specifically, based on the recent Siamese architecture, we introduce a new trajectory encoding module to project point-wise dynamics on the representation for each frame, which can provide significantly improved expressiveness for dynamic scenes. In our experiments, we find MMP can achieve state-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1% enhancement in the regression error.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding</title>
<link>https://arxiv.org/abs/2505.01743</link>
<guid>https://arxiv.org/abs/2505.01743</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision Language Models, Low-resolution data, Human Behavior Understanding, Contrastive-Oriented Data Labeler, Physical-Knowledge Guided Captioner

Summary:
The paper introduces a novel system, Llambda, aimed at enhancing the understanding of low-resolution human behavior in vision systems. The system leverages limited labeled data and a large amount of unlabeled data to guide Large Vision Language Models (LVLMs) in generating informative captions for low-resolution videos. The system comprises two main components: the Contrastive-Oriented Data Labeler, which generates high-quality pseudo labels for unlabeled data, and the Physical-Knowledge Guided Captioner, which improves LVLMs' understanding of sequential data and generates high-quality video captions. To ensure deployability on devices, efficient fine-tuning techniques are employed using LoRA-based methods. Experimental results demonstrate that Llambda outperforms existing LVLM systems by up to 40.03% on average Bert-Score when tested on real-world datasets. 

<br /><br />Summary: 
1. Introduction of Llambda system for enhancing low-resolution human behavior understanding in vision systems
2. Leveraging limited labeled data and a large amount of unlabeled data to guide LVLMs in generating informative captions
3. Components of Llambda including Contrastive-Oriented Data Labeler and Physical-Knowledge Guided Captioner
4. Efficient fine-tuning techniques using LoRA-based methods for on-device deployability
5. Experimental results showing Llambda outperforming existing LVLM systems by up to 40.03% on average Bert-Score <div>
arXiv:2505.01743v1 Announce Type: new 
Abstract: The rapid advancements in Large Vision Language Models (LVLMs) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (HBU) in low-resolution vision systems, such as depth, thermal, and infrared. However, existing large vision language model (LVLM) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as RGB images. A quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. In this paper, we propose a novel, labor-saving system, Llambda, designed to support low-resolution HBU. The core idea is to leverage limited labeled data and a large amount of unlabeled data to guide LLMs in generating informative captions, which can be combined with raw data to effectively fine-tune LVLM models for understanding low-resolution videos in HBU. First, we propose a Contrastive-Oriented Data Labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. Second, we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. Therefore, it can improve LLMs' understanding of sequential data and then generate high-quality video captions. Finally, to ensure on-device deployability, we employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data. We evaluate Llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that Llambda outperforms several state-of-the-art LVLM systems up to $40.03\%$ on average Bert-Score.
]]></content:encoded>
<pubDate>Tue, 06 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>